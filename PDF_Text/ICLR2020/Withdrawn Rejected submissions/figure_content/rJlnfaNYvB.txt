Figure 1:	Statistics of activation gradients collected from training SSD (Liu et al., 2016) by FP32.
Figure 2:	Comparison between the standard backpropagation algorithm and the loss scaled one.
Figure 3: An example of how the adaptive loss scaling method works based on a 3-layer MLP (biasand activation functions are omitted). Black and red arrows represent forward and backward propa-gation respectively, and terms beside red arrows are gradients. Section 3.1 explains the symbols inthis figure, specifically, β1, β2, and β3 denote the loss scale values calculated locally for each layer,and α1, α2, and α3 stand for accumulated scales. α4 is an optional initial scale.
Figure 4: Examples that show the benefit from using adaptive loss scaling. Note that the underflowrate in Figure 4b is collected by subtracting the percentage of zeros of the FP16 result and the cast-to-FP32 result. The higher ∆ is, the more effective that adaptive loss scaling can mitigate underflow.
