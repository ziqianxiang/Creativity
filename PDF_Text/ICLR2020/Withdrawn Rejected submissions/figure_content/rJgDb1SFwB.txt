Figure 1: Proposed model architecture.
Figure 2: Interpretation of the different attention weights in our model.
Figure 3: Performance of different models. It can be seen that our proposed labels are harder to fitthan the ones by Moor et al. (2019). Moreover, our proposed model outperforms the baselines onboth label sets, especially for earlier prediction horizons.
Figure 4: HeatmaPS of the learned MGP covariance matrices between the data features for the twodifferent smoothness clusters.
Figure 5: Visualization of the journey of an exemplary patient trajectory through our proposed modelarchitecture. The raw features (row 1), measured at irregular time points, are interpolated by anMGP (row 2). Samples from the MGP posterior can then be aggregated into means and variances foreach feature on a fixed, regularly-spaced time grid (row 3). These values are then attended to by theTCN (row 4), where positive attention weights are yellow and negative ones blue. Row 5 shows theattention weights separated by features (x-axis) and time point (y-axis).
