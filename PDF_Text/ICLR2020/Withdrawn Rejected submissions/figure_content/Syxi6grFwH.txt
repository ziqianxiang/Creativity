Figure 1: Experimental setup and neuronal data. (A) Experimental task setup. (i) The experimentalresults are based on a plus maze which can be converted to a T-maze by temporarily blockingthe opposite start arm (yellow box) after pseudo-random selection of a start location during anallocentric or egocentric spatial task. There are two possible start points (North and South) and tworeward arms (West and East). During a training session the rules (or tasks) are switched multipletimes. (ii) Representation of the allocentric task in which the animal has to navigate the west armwhere a reward is located. (iii) Representation of the egocentric task in which the animal has toturn right to get a reward irrespective of start point. Figure adapted from Ciocchi et al. (2015). (B)Example of trial-averaged z-scored firing rates of 30 randomly selected CA1 neurons out of 612around the reward point (vertical dashed line).
Figure 2: Demixed principal component analysis (dPCA) reveals hidden features from CA1 hip-pocampal recordings. (A) Cumulative variance explained by dPCA and PCA components. (B)Demixed components for each experimental variable: decision (incorrect in black and correct inbrown), strategy (allocentric in solid line and egocentric in dashed line), time and mixed deci-sion/strategy. Vertical dashed line represents the crossing of the reward sensor. We expect thatthe actual reward to be consumed 〜1 afterwards. (C) The variance explained by each demixedprincipal component and total variance per experimental condition (pie chart). Figure follows samestructure as in the original dPCA (Kobak et al. (2016)).
Figure 3: Demixed PCA for earlier and later training sessions. The first decision, strategy andinteraction components along with the first three temporal components are plotted from dPCA onneuronal firing rates from the initial (A) and final (B) experimental sessions. The components fromlater days are cleaner and highlight the influence of learning the tasks over time. Vertical dashed linerepresents the crossing of the reward sensor.
Figure 4: Comparing RL simulation results with low-dimensional dPCA components. (Ai) Valueand eligibility trace from a TD(λ) model simulated for a 1D environment. (Bi) Firing activity patternin dPCA component 1 is akin to the simulated eligibility trace pattern. (Aii) TD errors before(light brown) and after learning (dark brown) from a TD(λ) model simulation. The model uses alook-ahead strategy for error estimation. (Bii) Weak reward prediction signals around the rewarddelivery point. Error prediction signals signalling trial outcomes after reward point. (C) Separationof strategies based on Q-value estimates. (D) Distinct firing activity in dPCA component 4 pertainingto allocentric and egocentric tasks.
Figure 5: Behavioural performance in animals and Q-learning model during task switching. (A)Performance accuracy for animal SC03. (B) Performance accuracy for simulated model. A perfor-mance of 1 represents correct and 0 incorrect for both egocentric (blue) and allocentric (red). Soliddarker lines represent moving averages. Abrupt changes in both model and animal occur withina given strategy because of changes in the start location (e.g. around the 80th trial). Color-codedvertical dashed lines indicate moments of task switches. Dashed yellow lines denote a change ofreward locations while following a particular strategy. Dashed purple lines denote the change fromallocentric to egocentric task or vice versa.
Figure 6: Task switching simulation using a deep Q network. Solid darker lines represent movingaverages. Task-specific information is added in the input maze to facilitate differentiation of theallocentric and egocentric rules by the network. Mean performance in both tasks is above the chancelevel. The neural network achieves improved continual learning of both tasks.
Figure 7: Comparing mean performance of all Q-learning model simulations with the animal’s meanperformance. Re-learning performance on allocentric and egocentric tasks are also plotted from theQ-learning models and the animal. Color-coded dashed lines at trials 106-109 and 206-209 displayaverage performance of the networks and animal in the last few trials of the previous strategy itwas learning. (A) Even the animal is unable to achieve perfect learning of both tasks (orange bar).
Figure 8: T-maze layout in a 5 x 5 matrix grid for the online Q-learning simulation. Grid layoutsmimic the original allocentric and egocentric rules in the behavioural experiment. The value -1denotes un-walkable cells because it is an elevated plus maze (convertible to a T-maze). The value 0denotes walkable cells. The middle row has a 1 at either of its extremes to represent a reward. Idealtrajectories that should be chosen by the animal/agent are marked with color-coded arrows.
Figure 9: Task switching simulation using two separate deep Q networks. Total number of trialsover which the networks learn the tasks independently is 250. Solid darker lines represent movingaverages. Dashed lines denote change of reward locations during a type of task and can be called astrategy switch.(A) Performance accuracy of a DQN learning the allocentric task. (B) Performanceaccuracy of a DQN learning the egocentric task.
