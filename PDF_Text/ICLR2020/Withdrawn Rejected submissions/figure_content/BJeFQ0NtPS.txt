Figure 1: (a) Autoregressive text-to-spectrogram model. The dashed line depicts the autoregressivedecoding of mel spectrogram at inference. (b) Non-autoregressive ParaNet model, which distills theattention from a pretrained autoregressive model.
Figure 2: Architecture of ParaNet. Its encoder provides key and value as the textual representation.
Figure 3: Our ParaNet iteratively refines the attention alignment in a layer-by-layer way. One can seethe 1st layer attention is mostly dominated by the positional encoding prior. It becomes more andmore confident about the alignment in the subsequent layers.
