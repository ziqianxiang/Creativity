Figure 1: Using RVAE, action sequences are encoded w.r.t the L2 distance in action space. As aresult, although a(01..).3 and a(03..).3have exactly the same consequence, their codes will be far away inthe option space. This is not a desired property.
Figure 2: System architecture. E, D, F , P and RL are implemented with neural networks. Thegradients of three losses Lrvae, Ladv and Lpred will be back-propagated through the encoder E, andregularize the option. Note the gradient of Ladv will change sign before back-propagating to E.
Figure 3: Termination conditions. a) The decoder D outputs a termination signal ct at each timestep t. b) The option terminates when the predictor P fails to predict the next state.
Figure 4: Evaluation tasks. a) 2D navigation task. b) MoJoCo tasks including Ant, HalfCheetah,Hopper and Walker2d. c) Ant tasks including AntTurn, AntMaze, AntPush and AntFall.
Figure 5: 2D navigation task. a) Interpolation in option space. b) Disturbing one dimension inthe option space. c) Comparing the flat RL and TAIC on the ability of recovering from suddenenvironment change (average over 10 trials).
Figure 6: Visualization of option space w.r.t. state change. a) Option obtained by RVAE wo/ con-straints. b) Option w/ constraints and Term-output. c) Option w/ constraints and Term-predict. Thepoints in (c) are most ordered w.r.t. the color. This suggests the option distribution in (c) is mostcorrelated with state change.
Figure 7: Evaluating the performance of TAIC by HRL training on MuJoCo tasks. The proposedframework with different termination conditions are compared with the flat RL. Each case is anaverage over 5 repeated training.
Figure 8: Transfer capability. We compare the proposed TAIC framework with two baseline methodson more challenging tasks. The TAIC framework efficiently transfers the options learned in simplerAnt-v1 task to these novel and more complex tasks.
