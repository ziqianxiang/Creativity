Figure 1: Overview framework OfMaskConvNet for filter pruning on a typical bundled block structurecomposed of Conv - BN - ReLU.
Figure 2: Testing/Validation accuracy (Top-1) and sparsity pattern as the function of training epochsfor MaskConvNet with WideResNet-28-10 on CIFAR-10 (left) and ResNet-34 on ImageNet (right).
Figure 3: Results of MaskConvNet-P (parameter counts as target metric) and MaskConvNet-F (FLOPas target metric) with various sparsity budgets on CIFAR-10, using VGG-19 and WideResNet-28-10as baseline architectures. Text labels on plot points denote ‚ÄùAccuracy% (Actual / Budget Sparsity%)Figure 4: Layer-wise parameter sparsities throughout training for VGG-19 (left) and WideResnet-28-10 (right) on CIFAR-10, with warmup training till epoch 10. The brigher the color, the higher thesparsity.
Figure 4: Layer-wise parameter sparsities throughout training for VGG-19 (left) and WideResnet-28-10 (right) on CIFAR-10, with warmup training till epoch 10. The brigher the color, the higher thesparsity.
Figure 5: Mask Histogram after training.
Figure 6: Layer-wise parameter sparsities throughout training for VGG-19 on CIFAR-10, withdifferent mask decay and warmup training. The brigher the color, the higher the sparsity.
