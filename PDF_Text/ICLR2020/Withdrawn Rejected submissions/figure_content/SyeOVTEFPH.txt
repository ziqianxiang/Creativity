Figure 1: Overview of instance adaptive adversarial training. Samples close to the decision boundary(bird on the left) have nearby samples from a different class (deer) within a small Lp ball, makingthe constraints imposed by PGD-8 / PGD-16 adversarial training infeasible. Samples far from thedecision boundary (deer on the right) can withstand large perturbations well beyond = 8. Ouradaptive adversarial training correctly assigns the perturbation radius (shown in dotted line) so thatsamples within each Lp ball maintain the same class.
Figure 2: Visualizing training samples and their perturbations. The left panel shows samples thatare assigned small (displayed below images) during adaptive training. These images are close toclass boundaries, and change class when perturbed with ≥ 8. The right panel show images that areassigned large . These lie far from the decision boundary, and retain class information even withvery large perturbations. All live in the range [0, 255](b) Samples from top 1%Algorithm 2 selection algorithmRequire: i: Sample index, j : Epoch indexRequire: β: Smoothing constant, γ: Discretization for search.
Figure 3: Tradeoffs between accuracy and robustness: Each blue dot denotes an adversarially trainedmodel with a different used at training (Training is marked next to blue dots). Models trainedusing instance adaptive adversarial training are shown in red. Adaptive training breaks through thePareto frontier achieved by plain adversarial training with a fixed . For all models, adversarialaccuracy is reported on PGD-1000 attacks with a fixed test = 8.
Figure 4: Plot of adversarial robustness over a sweep of test6Under review as a conference paper at ICLR 2020Table 3: Robustness results on other attacks for models trained using PGD-10 for WRN-32-10 modelon CIFAR-10 dataset. Accuracies are reported in %Algorithm	Natural acc.	PGD-1000	DeePFool	MIFGSM	CW40Adversarial training	8685	-44.84-	-65.28-	-54.66-	55.62	IAAT	91.34	46.54	66.58	53.99	56.80Table 4: Robustness experiments on Imagenet. All adversarial attacks are generated with PGD-1000.
Figure 5: Visualizing training samples with their corresponding perturbation. All live in the range[0, 255]12Under review as a conference paper at ICLR 2020Figure 6: Visualizations of samples for which low ’s are assigned by instance adaptive adversarialtraining. These samples are close to the decision boundary and change class when perturbed with≥ 8. Perturbing them with assigned by IAAT retains the class information.
Figure 6: Visualizations of samples for which low ’s are assigned by instance adaptive adversarialtraining. These samples are close to the decision boundary and change class when perturbed with≥ 8. Perturbing them with assigned by IAAT retains the class information.
Figure 7: Visualizing progress of instance adaptive adversarial trianing. Plot on the left showsaverage of samples over epochs, while the plot on the right shows progress of three randomlychosen samples.
Figure 8: Histogram of of training samples at different training epochs13Under review as a conference paper at ICLR 2020Figure 9: Imagenet robustness of IAAT over the number of PGD iterationsTable 9: Sensitivity of IAAT performance to hyperparameters β and γ. Models are trained onCIFAR-10 dataset using Wideresnet-32-10.
Figure 9: Imagenet robustness of IAAT over the number of PGD iterationsTable 9: Sensitivity of IAAT performance to hyperparameters β and γ. Models are trained onCIFAR-10 dataset using Wideresnet-32-10.
