Figure 2: The learning dynamics of ResNet-56 on CIFAR-10, i.e., training and testing accuraciesalong with training iterations. The legend in the top left is shared by all subfigures. ‘xxx: yyy’ means‘method: emphasis focus’. The results of r = 80% are shown in Figure 6 in the supplementarymaterial. We have two key observations: 1) When noise rate increases, better generalisation isobtained with higher emphasis focus, i.e., focusing on relatively easier examples; 2) Both overfittingand underfitting lead to bad generalisation. For example, ‘CCE: 0’ fits training data much betterthan the others while ‘GR: None’ generally fits it unstably or a lot worse. Better viewed in colour.
Figure 3: Diverse semantically abnormal training examples highlighted by red boxes. The 1st rowshows synthetic abnormal examples from corrupted CIFAR-10 (Krizhevsky, 2009). The 2nd and3rd rows present realistic abnormal examples from video person re-identification benchmark MARS(Zheng et al., 2016).
Figure 4: The training and test accuracies on clean CIFAR-10 along with training iterations. Thetraining labels are clean. We fix λ = 0 to focus on harder examples while changing emphasis spreadcontroller β. The backbone is ResNet-20. The results of ResNet-56 are shown in Figure 5. Betterviewed in colour.
Figure 5: The training and test accuracies on clean CIFAR-10 along with training iterations. Thetraining labels are clean. We fix λ = 0 to focus on more difficult examples while changing emphasisspread controller β . The backbone is ResNet-56. The results of ResNet-20 are shown in Figure 4.
Figure 6: The learning dynamics on CIFAR-10 (r = 80%) With ResNet-56, i.e., training and testingaccuracies along With training iterations. The legend in the top left is shared by tWo subfigures. ‘xxx:yyy’ means ‘method: emphasis focus’. The results of r = 20%, 40%, 60% are shoWn in Figure 2 inthe paper.
Figure 7: ResNet-56 on CIFAR-10 (r = 20%). From left to right, the results of four emphasisfocuses 0, 0〜0.5, 0.5, 0.57 With different emphasis spreads are displayed in each column respec-tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :1) When	λ	= 0:	We	tried β	=	0.5, 1, 2, 4;2)	When	λ	= 0.5: We tried	β	= 4, 8, 12, 16;3)	When	λ	= 1:	We	tried β	=	8, 12, 16, 20;4)	When	λ	= 2:	We	tried β	=	12, 16, 20, 24.
Figure 8: ResNet-56 on CIFAR-10 (r = 40%). From left to right, the results of four emphasisfocuses 0, 0~0.5, 0.5, 0.5~1 With different emphasis spreads are displayed in each column respec-tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :1) When	λ	= 0:	We	tried β	=	0.5, 1, 2, 4;2) When	λ	= 0.5: We tried	β	= 4, 8, 12, 16;3) When	λ	= 1:	We	tried β	=	8, 12, 16, 20;4) When	λ	= 2:	We	tried β	=	12, 16, 20, 24.
Figure 9: ResNet-56 on CIFAR-10 (r = 60%). From left to right, the results of four emphasisfocuses 0, 0~0.5, 0.5, 0.5~1 with different emphasis spreads are displayed in each column respec-tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :1) when	λ	= 0:	we	tried β	=	0.5, 1, 2, 4;2) when	λ	= 0.5: we tried	β	= 4, 8, 12, 16;3) when	λ	= 1:	we	tried β	=	8, 12, 16, 20;4) when	λ	= 2:	we	tried β	=	12, 16, 20, 24.
Figure 10: ResNet-56 on CIFAR-10 (r = 80%). From left to right, the results of four emphasisfocuses 0, 0〜0.5, 0.5, 0.57 With different emphasis spreads are displayed in each column respec-tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :1) When λ	=	0:	We	tried β =	0.5, 1, 2, 4;2) When λ	=	0.5: We tried β	= 4, 8, 12, 16;3) When λ	=	1:	We	tried β =	8, 12, 16, 20;4) When λ	=	2:	We	tried β =	12, 16, 20, 24.
