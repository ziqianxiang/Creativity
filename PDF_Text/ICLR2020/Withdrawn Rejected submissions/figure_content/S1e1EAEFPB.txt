Figure 1: Perceptual regularization: we stack a decoder on top of the feature map and jointly train itwith the classifier.
Figure 2: Left: Original images from SVHN. Right: The model’s perception of the images.
Figure 3:	Visualizing the model’s attention. Acc is the accuracy of the classifier on the test set.
Figure 4:	First row: by adding an imperceptibly small vector whose elements are obtained fromperforming a PGD attack, we can change the classification of the image. Second row: by applyingperception, we can understand the model’s “incorrect” prediction.
Figure 5: Images after applying perception on the task of classifying smiling face, we can understandthe model’s “incorrect” prediction. The corresponding original images can be found in Appendix A.
Figure 6: First row: smiling to not smiling. Second row: no glasses to glasses. Third row: female tomale. Each column corresponds sequentially to α ∈ {0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2}.
Figure 7: First row: Xadv for random noise X. Second row: P (Xadv). Even for what is essentiallyrandom noise, the model insists on perceiving something that semantically looks like a number.
Figure 8: Visualization of adversarial examples on CelebA dataset with Smile labels. The adversarialexamples are obtained by applying 100 iterations of PGD attack With '∞ perturbation and e = 0.03.
Figure 9: Visualization of adversarial examples on CelebA dataset with Smile labels. The adversarialexamples are obtained by applying 100 iterations of PGD attack With '∞ perturbation and e = 0.03.
Figure 10: Visualization of adversarial examples on CelebA dataset with eyeglasses labels. Theadversarial examples are obtained by applying 100 iterations of PGD attack With '∞ perturbationand = 0.03. The regularization parameter is set to λ = 10-3.
Figure 11: Visualization of adversarial examples on CelebA dataset with Gender labels. Theadversarial examples are obtained by applying 100 iterations of PGD attack With '∞ perturbationand = 0.03. The regularization parameter is set to λ = 10-3.
Figure 12: Visualization of adversarial examples on CelebA dataset with Eyeglasses labels. Theadversarial examples are obtained by applying 100 iterations of PGD attack With '∞ perturbationand = 0.03. The regularization parameter is set to λ = 10-3.
Figure 13: Visualization of adversarial examples on CelebA dataset with Eyeglasses labels. Theadversarial examples are obtained by applying 100 iterations of PGD attack With '∞ perturbationand = 0.03. The regularization parameter is set to λ = 10-3.
Figure 14: Visualization of adversarial examples on MNIST dataset with e = 0.2 ('∞ perturbation).
Figure 15: Visualization of adversarial examples on CelebA dataset with Eyeglasses labels. Theadversarial examples are obtained by applying 100 iterations of PGD attack With '∞ perturbationand = 0.03. The regularization parameter is set to λ = 10-3.
Figure 16: The percentage indicates the proportion of positive labels among the dataset.
