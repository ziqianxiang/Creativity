Published as a conference paper at ICLR 2020
Disagreement-Regularized Imitation
Learning
Kiante Brantley *
University of Maryland
kdbrant@cs.umd.edu
Wen Sun
Microsoft Research
sun.wen@microsoft.com
Mikael Henaff
Microsoft Research
mihenaff@microsoft.com
Ab stract
We present a simple and effective algorithm designed to address the covariate shift
problem in imitation learning. It operates by training an ensemble of policies on
the expert demonstration data, and using the variance of their predictions as a cost
which is minimized with RL together with a supervised behavioral cloning cost.
Unlike adversarial imitation methods, it uses a fixed reward function which is easy
to optimize. We prove a regret bound for the algorithm which is linear in the time
horizon multiplied by a coefficient which we show to be low for certain problems
on which behavioral cloning fails. We evaluate our algorithm empirically across
multiple pixel-based Atari environments and continuous control tasks, and show
that it matches or significantly outperforms behavioral cloning and generative ad-
versarial imitation learning.
1	Introduction
Training artificial agents to perform complex tasks is essential for many applications in robotics,
video games and dialogue. If success on the task can be accurately described using a reward or
cost function, reinforcement learning (RL) methods offer an approach to learning policies which
has proven to be successful in a wide variety of applications (Mnih et al., 2015; 2016; Lillicrap
et al., 2016; Hessel et al., 2018). However, in other cases the desired behavior may only be roughly
specified and it is unclear how to design a reward function to characterize it. For example, training
a video game agent to adopt more human-like behavior using RL would require designing a reward
function which characterizes behaviors as more or less human-like, which is difficult.
Imitation learning (IL) offers an elegant approach whereby agents are trained to mimic the demon-
strations of an expert rather than optimizing a reward function. Its simplest form consists of training
a policy to predict the expert’s actions from states in the demonstration data using supervised learn-
ing. While appealingly simple, this approach suffers from the fact that the distribution over states
observed at execution time can differ from the distribution observed during training. Minor errors
which initially produce small deviations become magnified as the policy encounters states further
and further from its training distribution. This phenomenon, initially noted in the early work of
(Pomerleau, 1989), was formalized in the work of (Ross & Bagnell, 2010) who proved a quadratic
O(T 2) bound on the regret and showed that this bound is tight. The subsequent work of (Ross
et al., 2011) showed that if the policy is allowed to further interact with the environment and make
queries to the expert policy, it is possible to obtain a linear bound on the regret. However, the ability
to query an expert can often be a strong assumption.
In this work, we propose a new and simple algorithm called DRIL (Disagreement-Regularized Im-
itation Learning) to address the covariate shift problem in imitation learning, in the setting where
the agent is allowed to interact with its environment. Importantly, the algorithm does not require
any additional interaction with the expert. It operates by training an ensemble of policies on the
demonstration data, and using the disagreement in their predictions as a cost which is optimized
through RL together with a supervised behavioral cloning cost. The motivation is that the policies
in the ensemble will tend to agree on the set of states covered by the expert, leading to low cost, but
are more likely to disagree on states not covered by the expert, leading to high cost. The RL cost
* Work done while at Microsoft Research.
1
Published as a conference paper at ICLR 2020
thus guides the agent back towards the distribution of the expert, while the supervised cost ensures
that it mimics the expert within the expert’s distribution.
Our theoretical results show that, subject to realizability and optimization oracle assumptions1, our
algorithm obtains a O(κT) regret bound, where κ is a measure which quantifies a tradeoff between
the concentration of the demonstration data and the diversity of the ensemble outside the demon-
stration data. We evaluate DRIL empirically across multiple pixel-based Atari environments and
continuous control tasks, and show that it matches or significantly outperforms behavioral cloning
and generative adversarial imitation learning, often recovering expert performance with only a few
trajectories.
2	Preliminaries
We consider episodic finite horizon MDP in this work. Denote by S the state space, A the action
space, and Π the class of policies the learner is considering. Let T denote the task horizon and π?
the expert policy whose behavior the learner is trying to mimic. For any policy π, let dπ denote
the distribution over states induced by following π. Denote C(s, a) the expected immediate cost of
performing action a in state s, which we assume is bounded in [0, 1]. In the imitation learning setting,
we do not necessarily know the true costs C(s, a), and instead we observe expert demonstrations.
Our goal is to find a policy π which minimizes an observed surrogate loss ` between its actions and
the actions of the expert under its induced distribution of states, i.e.
∏ = arg min Es〜dπ ['(∏(s), n?(s))]
(1)
For the following, We will assume ' is the total variation distance (denoted by ∣∣ ∙ ∣∣), which is an
upper bound on the 0 - 1 loss. Our goal is thus to minimize the following quantity, which represents
the distance between the actions taken by our policy π and the expert policy π?:
JexP⑺=Es〜d∏ 卜忻《|S)- π*(IS)∣
(2)
Denote Qtπ (S, a) as the standard Q-function of the policy π, which is defined as Qtπ(S, a)
E [PT=t C(ST,aτ)∣(st,at) = (s,a),aτ 〜∏ .
The following result shows that if ` is an upper
bound on the 0 - 1 loss and C satisfies certain smoothness conditions, then minimizing this loss
within E translates into an O(ET) regret bound on the true task cost Jc(∏) = Es,a〜dπ [C(s, a)]:
Theorem 1. (Ross etal., 2011)If π satisfies Jexp(∏) = e, and Qγ-t+ι(s, a) 一 Q∏-t+ι(s, ∏?) ≤ U
for all time steps t, actions a and states S reachable by π, then JC(π) ≤ JC(π?) + uTE.
Unfortunately, it is often not possible to optimize JexP directly, since it requires evaluating the expert
policy on the states induced by following the current policy. The supervised behavioral cloning cost
JBC, which is computed on states induced by the expert, is often used instead:
Jbc(∏) = Es 〜d∏Jk∏?(∙∣s)-∏(∙∣s)∣]	⑶
Minimizing this loss within E yields a quadratic regret bound on regret:
Theorem 2. (Ross & Bagnell, 2010) Let JBC(π) = E, then JC(π) ≤ JC(π?) + T2E.
Furthermore, this bound is tight: as we will discuss later, there exist simple problems which match
the worst-case lower bound.
3	Algorithm
Our algorithm is motivated by two criteria: i) the policy should act similarly to the expert within
the expert’s data distribution, and ii) the policy should move towards the expert’s data distribution
1We assume for the analysis the action space is discrete, but the state space can be large or infinite.
2
Published as a conference paper at ICLR 2020
Algorithm 1 Disagreement-Regularized Imitation Learning (DRIL)
1:	Input: Expert demonstration data D = {(si , ai)}iN=1
2:	Initialize policy π and policy ensemble ΠE = {π1, ..., πE}
3:	for e = 1, E do
4:	Sample De 〜D with replacement, with |De| = |D|.
5:	Train πe to minimize JBC(πe) on De to convergence.
6:	end for
7:	for i = 1, ... do
8:	Perform one gradient update to minimize JBC (π) using a minibatch from D.
9:	Perform one step of policy gradient to minimize Es〜dπ ,a〜∏(∙∣s)[CUhpGa)].
10:	end for
if it is outside of it. These two criteria are addressed by combining two losses: a standard behavior
cloning loss, and an additional loss which represents the variance over the outputs of an ensemble
ΠE = {π1, ..., πE} of policies trained on the demonstration data D. We call this the uncertainty
cost, which is defined as:
1E	1E	2
Cu(s,a) = Varn〜∏e(∏(a∣s)) = E X (∏i(a∣s) - E X∏i(a∣s))
i=1
i=1
The motivation is that the variance over plausible policies is high outside the expert’s distribution,
since the data is sparse, but it is low inside the expert’s distribution, since the data there is dense.
Minimizing this cost encourages the policy to return to regions of dense coverage by the expert.
Intuitively, this is what we would expect the expert policy π? to do as well. The total cost which the
algorithm optimizes is given by:
Jaig(π) = Es〜d∏? [∣∣π?(∙∣s) - ∏(∙∣s)k]+ Es〜d∏,a〜∏(∙∣s)[Cu(s, a)]
、---------------------{z------------} 、------------V------------/
JBC(π)	JU(π)
The first term is a behavior cloning loss and is computed over states generated by the expert policy,
of which the demonstration data D is a representative sample. The second term is computed over
the distribution of states generated by the current policy and can be optimized using policy gradient.
Note that the demonstration data is fixed, and this ensemble can be trained once offline. We then
interleave the supervised behavioral cloning updates and the policy gradient updates which minimize
the variance of the ensemble. The full algorithm is shown in Algorithm 1. We also found that dropout
(Srivastava et al., 2014), which has been proposed as an approximate form of ensembling, worked
well (see Appendix D).
In practice, for the supervised loss we optimize the KL divergence between the actions predicted
by the policy and the expert actions, which is an upper bound on the total variation distance due to
Pinsker’s inequality. We also found it helpful to use a clipped uncertainty cost:
CUclip(s, a) =	-+11
if CU(s, a) ≤ q
else
where the threshold q is a top quantile of the raw uncertainty costs computed over the demonstration
data. The threshold q defines a normal range of uncertainty based on the demonstration data, and
values above this range incur a positive cost (or negative reward).
The RL cost can be optimized using any policy gradient method. In our experiments we used
advantage actor-critic (A2C) (Mnih et al., 2016) or PPO (Schulman et al., 2017), which estimate the
expected cost using rollouts from multiple parallel actors all sharing the same policy (see Appendix
C for details). We note that model-based RL methods could in principle be used as well if sample
efficiency is a constraint.
3
Published as a conference paper at ICLR 2020
4 Analysis
4.1	Coverage Coefficient
We now analyze DRIL for MDPs with discrete action spaces and potentially large or infinite state
spaces. We will show that, subject to assumptions that the policy class contains an optimal policy and
that we are able to optimize costs within of their global minimum, our algorithm obtains a regret
bound which is linear in κT , where κ is a quantity which depends on the environment dynamics,
the expert distribution d∏, and our learned ensemble. Intuitively, K represents a tradeoff between
how concentrated the demonstration data is and how high the variance of the ensemble is outside
the expert distribution.
Assumption 1. (Realizability) π? ∈ Π.
Assumption 2. (Optimization Oracle) For any given cost function J, our minimization procedure
returns a policy π ∈ Π such that J(∏) ≤ argmin∏∈∏ J(π) + 匕
The motivation behind our algorithm is that the policies in the ensemble agree inside the expert’s
distribution and disagree outside of it. This defines a reward function which pushes the learner back
towards the expert’s distribution if it strays away. However, what constitutes inside and outside the
distribution, or sufficient agreement or disagreement, is ambiguous. Below we introduce quantities
which makes these ideas precise.
Definition 1. For any set U ⊆ S, define the concentrability inside of U as α(U)
maχ∏∈∏ suPs∈u d∏?(S)).
The notion of concentrability has been previously used to give bounds on the performance of value
iteration (Munos & Szepesvari, 2008). For a set U, α(U) will be low if the expert distribution has
high mass at the states in U that are reachable by policies in the policy class.
Definition 2. Define the minimum variance of the ensemble outside of U as β(U) =
mins/u ,a∈A Varn 〜∏e [π(a∣s)].
We now define the κ coefficient as the minimum ratio of these two quantities over all possible subsets
ofS.
Definition 3. We define K = minu⊆s O(Uj.
We can view κ as the quantity which minimizes the tradeoff over different subsets U between cov-
erage by the expert policy inside of U, and variance of the ensemble outside of U.
4.2	Regret Bound
We now establish a relationship between the K coefficient just defined, the cost our algorithm opti-
mizes, and Jexp defined in Equation (2) which we would ideally like to minimize and which trans-
lates into a regret bound. All proofs can be found in Appendix A.
Lemma 1. For any π ∈ Π, we have Jexp (π) ≤ KJalg(π).
This result shows that if K is not too large, and we are able to make our cost function Jalg(π) small,
then we can ensure Jexp(π) is also small. This result is only useful if our cost function can indeed
achieve a small minimum. The next lemma shows that this is the case.
Lemma 2. minπ∈Π Jalg (π) ≤ 2.
Here is the threshold specified in Assumption 2. Combining these two lemmas with the previous
result of Ross et al. (2011), we get a regret bound which is linear in KT.
Theorem 3. Let π be the result of minimizing Jalg using our optimization oracle, and assume that
Qτ-t+ι(s, a) — QT-t+ι(s, ∏?) ≤ U for all actions a, time steps t andstates S reachable by π. Then
π satisfies Jc(Π) ≤ Jc(∏?) + 3uκeT.
Our bound is an improvement over that of behavior cloning if K is less than O(T). Note that
DRIL does not require knowledge of K. The quantity K is problem-dependent and depends on the
4
Published as a conference paper at ICLR 2020
Figure 1: Example of a problem where behavioral cloning incurs quadratic regret.
environment dynamics, the expert policy and the policies in the learned ensemble. We next compute
κ exactly for a problem for which behavior cloning is known to perform poorly, and show that it is
independent of T .
Example 1. Consider the tabular MDP given in (Ross & Bagnell, 2010) as an example of a prob-
lem where behavioral cloning incurs quadratic regret, shown in Figure 1. There are 3 states
S = (s0, s1, s2) and two actions (a1, a2). Each policy π can be represented as a set of proba-
bilities π(a1 |s) for each state s ∈ S 2. Assume the models in our ensemble are drawn from a
posterior p(π(a1 |s)|D) given by a Beta distribution with parameters Beta(n1 + 1, n2 + 1) where
n1 , n2 are the number of times the pairs (s, a1) and (s, a2) occur, respectively, in the demon-
Stration data D. The agent always starts in s0 and the expert's policy is given by π?(a1 ∣s0) =
1,π夫(a1 ∣s1) = 0,π夫(a1 ∣s2) = 1. For any (s, a) pair, the task cost is C(s, a) = 0 if a = π*(S)
and 1 otherwise. Here d∏ = (*, T-1,0). For any π, dπ (s0) = * and dπ (s1) ≤ T-1 due
to the dynamics Ofthe MDP, so [：(；) ≤ 1 for S ∈ {s0,s1}. Writing out α({s0,s1}), we get:
α({s0,s J) =maxπ∈Π suPs∈{so,sι}第 ≤ 1.
Furthermore, since S2 is never visited in the demonstration data, for each policy πi in the ensemble
we have πi(a1 ∣s2),πi(a2∣s2)〜Beta(1,1) = Uniform(0,1) Itfollows that Varn〜πE(∏(a∣s2))
is approximately equal 3 to the variance of a uniform distribution over [0,1], i.e. 击.Therefore:
α(U)	α({S0, S1})	1
K = m⊆⊆S β(U) ≤ β({S0,S1}).舌=12
Applying our result from Theorem 3, we see that our algorithm obtains an O(T ) regret bound on
this problem, in contrast to the O(T 2) regret of behavioral cloning4.
5	Related Work
The idea of learning through imitation dates back at least to the work of (Pomerleau, 1989), who
trained a neural network to imitate the steering actions of a human driver using images as input. The
problem of covariate shift was already observed, as the author notes: “the network must not solely
be shown examples of accurate driving, but also how to recover once a mistake has been made”.
This issue was formalized in the work of (Ross & Bagnell, 2010), who on one hand proved an
O(T2) regret bound, and on the other hand provided an example showing this bound is tight. The
subsequent work (Ross et al., 2011) proposed the DAGGER algorithm which obtains linear regret,
provided the agent can both interact with the environment, and query the expert policy. Our approach
also requires environment interaction, but importantly does not need to query the expert. Also of
2Note that π(a2 |s) = 1 - π(a1 |s).
3Via Hoeffding's inequality, with probability 1 一 δ the two differ by at most O(Plog(I∕δ)∕∣∏E|).
4Observe that a policy with π(aι |so) = 1 — eT, π(a2 ∣sι) = eT,π(a2 ∣s2) = 1 has a behavioral cloning
cost of but a regret of T2 .
5
Published as a conference paper at ICLR 2020
note is the work of (Venkatraman et al., 2015), which extended DAGGER to time series prediction
problems by using the true targets as expert corrections.
Imitation learning has been used within the context of modern RL to help improve sample efficiency
(Chang et al., 2015; Ross & Bagnell, 2014; Sun et al., 2017; Hester et al., 2018; Le et al., 2018;
Cheng & Boots, 2018) or overcome exploration (Nair et al., 2017). These settings assume the
reward is known and that the policies can then be fine-tuned with reinforcement learning. In this
case, covariate shift is less of an issue since it can be corrected using the reinforcement signal.
The work of (Luo et al., 2019) also proposed a method to address the covariate shift problem when
learning from demonstrations when the reward is known, by conservatively extrapolating the value
function outside the training distribution using negative sampling. This addresses a different setting
from ours, and requires generating plausible states which are off the manifold of training data, which
may be challenging when the states are high dimensional such as images. The work of (Reddy
et al., 2019) proposed to treat imitation learning within the Q-learning framework, setting a positive
reward for all transitions inside the demonstration data and zero reward for all other transitions in
the replay buffer. This rewards the agent for repeating (or returning to) the expert’s transitions. The
work of (Sasaki et al., 2019) also incorporates a mechanism for reducing covariate shift by fitting
a Q-function that classifies whether the demonstration states are reachable from the current state.
Random Expert Distillation (Wang et al., 2019) uses Random Network Distillation (RND) (Burda
et al., 2019) to estimate the support of the expert’s distribution in state-action space, and minimizes
an RL cost designed to guide the agent towards the expert’s support. This is related to our method,
but differs in that it minimizes the RND prediction error rather than the ensemble variance and does
not include a behavior cloning cost. The behavior cloning cost is essential to our theoretical results
and avoids certain failure modes, see Appendix B for more discusion.
Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) is a state-of-the-art algo-
rithm which addresses the same setting as ours. It operates by training a discriminator network to
distinguish expert states from states generated by the current policy, and the negative output of the
discriminator is used as a reward signal to train the policy. The motivation is that states which are
outside the training distribution will be assigned a low reward while states which are close to it will
be assigned a high reward. This encourages the policy to return to the expert distribution if it strays
away from it. However, the adversarial training procedure means that the reward function is chang-
ing over time, which can make the algorithm unstable or difficult to tune. In contrast, our approach
uses a simple fixed reward function. We include comparisons to GAIL in our experiments.
Using disagreement between models in an ensemble to represent uncertainty has recently been ex-
plored in several contexts. The works of (Shyam et al., 2018; Pathak et al., 2019; Henaff, 2019)
used disagreement between different dynamics models to drive exploration in the context of model-
based RL. Conversely, (Henaff et al., 2019) used variance across different dropout masks to prevent
policies from exploiting error in dynamics models. Ensembles have also been used to represent un-
certainty over Q-values in model-free RL in order to encourage exploration (Osband et al., 2016).
Within the context of imitation learning, the work of (Menda et al., 2018) used the variance of the
ensemble together with the DAGGER algorithm to decide when to query the expert demonstrator
to minimize unsafe situations. Here, we use disagreement between different policies trained on
demonstration data to address covariate shift in the context of imitation learning.
6	Experiments
6.1	Tabular MDPs
As a first experiment, we applied DRIL to the tabular MDP of (Ross & Bagnell, 2010) shown in
Figure 1. We computed the posterior over the policy parameters given the demonstration data using
a separate Beta distribution for each state s with parameters determined by the number of times each
action was performed in s. For behavior cloning, we sampled a single policy from this posterior. For
DRIL, we sampled an ensemble of 5 policies and used their negative variance to define an additional
reward function. We combined this with a reward which was the probability density function of a
given state-action pair under the posterior distribution, which corresponds to the supervised learning
loss, and used tabular Q-learning to optimize the sum of these two reward functions. This experiment
6
Published as a conference paper at ICLR 2020
120-
N=I demonstration
N=5 demonstrations
N=IO demonstrations
120-
120-
IOO-
IOO-
80-
BO-
TO-
Ncl
—BehavkjrCIoning
DRIL
—Behsvior Cloning
T- DRIL
—BehaviorCIoning
T- DRIL
σ> β0
40-
1∞-
40
Time Horizon
Time Horizon
Time Horizon
Figure 2: Results on tabular MDP from (Ross & Bagnell, 2010). Shaded region represents range
between 5th and 95th quantiles, computed across 500 trials. Behavior cloning exhibits poor worst-
case regret, whereas DRIL has low regret across all trials.
was repeated 500 times for time horizon lengths up to 500 and N = 1, 5, 10 expert demonstration
trajectories.
Figure 2 shows plots of the regret over the 500 different trials across different time horizons. Al-
though BC achieves good average performance, it exhibits poor worst-case performance with some
trials incurring very high regret, especially when using fewer demonstrations. Our method has low
regret across all trials, which stays close to constant independantly of the time horizon, even with a
single demonstration. This performance is better than that suggested by our analysis, which showed
a worst-case linear bound with respect to time horizon.
6.2	Atari Environments
We next evaluated our approach on six different Atari environments. We used pretrained PPO
(Schulman et al., 2017) agents from the stable baselines repository (Hill et al., 2018) to generate
N = {1, 3, 5, 10, 15, 20} expert trajectories. We compared against two other methods: standard be-
havioral cloning (BC) and Generative Adversarial Imitation Learning (GAIL). Results are shown in
Figure 3a. DRIL outperforms behavioral cloning across most environments and numbers of demon-
strations, often by a substantial margin. In many cases, our method is able to match the expert’s
performance using a small number of trajectories. Figure 3b shows the evolution of the uncertainty
cost and the policy reward throughout training. In all cases, the reward improves while the uncer-
tainty cost decreases.
We were not able to obtain meaningful performance for GAIL on these domains, despite performing
a hyperparameter search across learning rates for the policy and discriminator, and across different
numbers of discriminator updates. We additionally experimented with clipping rewards in an effort
to stabilize performance. These results are consistent with those of (Reddy et al., 2019), who also
reported negative results when running GAIL on images. While improved performance might be
possible with more sophisticated adversarial training techniques, we note that this contrasts with our
method which uses a fixed reward function obtained through simple supervised learning.
In Appendix D we provide ablation experiments examining the effects of the cost function clipping
and the role of the BC loss. We also compare the ensemble approach to a dropout-based approxima-
tion and show that DRIL works well in both cases.
6.3	Continuous Control
We next report results of running our method on 6 different continuous control tasks from the Py-
Bullet5 and OpenAI Gym (Brockman et al., 2016) environments. We again used pretrained agents
to generate expert demonstrations, and compared to Behavior Cloning and GAIL.
Results for all methods are shown in Figure 4. In these environments we found Behavior Cloning
to be a much stronger baseline than for the Atari environments: in several tasks it was able to
match expert performance using as little as 3 trajectories, suggesting that covariate shift may be
less of an issue. Our method performs similarly to Behavior Cloning on most tasks, except on
Walker2D, where it yields improved performance for N = 1, 3, 5 trajectories. GAIL performs
5https://github.com/bulletphysics/bullet3/tree/master/examples/
pybullet/gym/pybullet_envs/examples
7
Published as a conference paper at ICLR 2020
MsPacman
3	5	10	15	20
Expert Trajectories
BeamRider
peM*
Spacelnvaders
10	15
Expert Trajectories
Pong
PJeM第
Breakout
35«
3	5	10	15	20
ExpertTriajectories
Qbert
1	3	5	10	15	20
Expert Trajectories
PJeM第
3	5	10	15
Expert Trajectories
a)
30000
250∞
WMMO
α>
M 150C®
S
IOOOO
50∞
o∙
1	3	5	10	15	20
ExpertTriajectories
Spacelnvaders
MsPacman
-soɔ"u∙lJ8un
-0.95
-O.
96
■ VW
15∞
IOOO
PJMMatl。PoS-duj
0.25 0.50 0.75 LOo 1.25
steps
Breakout
1.50
1.75	2.8
ie7
摹BOO
S 68
-O 400
3
0.200
0 l ，
0.8 0.25 0.5。
0.75 1.00	1.25	1.5。	1.75 2.00
steps	1*7
BeamRider
W -0.7 ∙
田-0.8 -
g
8 -09
U
n
-1.0
? 3∞
M
Z 200
W
8 ια)
0.
0
0.00
0.25 0.50 0.75 LOo 1.25
steps
1.50
1.75 2.00
⅛7
-0.90
-0.92
-0.94
-0.96
-098
P 3000
雪
■ 2000
I »<»
s,8 XIU-CiJ8un
0.25	0 50
0.75 1.00	1.25	1.5。	1.75 2.00
steps	1*7
b)
Figure 3: Results on Atari environments. a) Median final policy performance for different numbers
of expert trajectories, taken over 4 seeds (shaded regions are min/max performance) b) Evolution of
policy reward and uncertainty cost during training with N = 3 trajectories.
somewhat better than DRIL on HalfCheetah and Walker2D, but performs worse than both DRIL
and BC on LunarLander and BipedalWalkerHardcore. The fact that DRIL is competitive across all
tasks provides evidence of its robustness.
7	Conclusion
Addressing covariate shift has been a long-standing challenge in imitation learning. In this work,
we have proposed a new method to address this problem by penalizing the disagreement between an
ensemble of different policies trained on the demonstration data. Importantly, our method requires
no additional labeling by an expert. Our experimental results demonstrate that DRIL can often
match expert performance while using only a small number of trajectories across a wide array of
tasks, ranging from tabular MDPs to pixel-based Atari games and continuous control tasks. On
the theoretical side, we have shown that our algorithm can provably obtain a low regret bound for
problems in which the κ parameter is low.
8
Published as a conference paper at ICLR 2020
13	5	10	15
ExpertTraIectorIes
BlpedaIWaIkerHardcore-VZ
S S
PjeMdH
3	5	10	15	30
ExpertTraJectorIes
ExpertTraJectorIes
Figure 4: Results on continuous control tasks.
There are multiple directions for future work. On the theoretical side, characterizing the κ param-
eter on a larger array of problems would help to better understand the settings where our method
can expect to do well. Empirically, there are many other settings in structured prediction (DaUme
et al., 2009) where covariate shift is an issue and where our method could be applied. For example,
in dialogue and language modeling it is common for generated text to become progressively less
coherent as errors push the model off the manifold it was trained on. Our method could potentially
be used to fine-tune language or translation models (Cho et al., 2014; Welleck et al., 2019) after
training by applying our uncertainty-based cost function to the generated text.
9
Published as a conference paper at ICLR 2020
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=H1lJJnR5Ym.
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Hal Daume III. Learn-
ing to search better than your teacher. 2015.
Ching-An Cheng and Byron Boots. Convergence of value aggregation for imitation learning. arXiv
preprint arXiv:1801.07292, 2018.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-
decoder for statistical machine translation. In Proceedings of the 2014 Conference on Em-
Pirical Methods in Natural Language Processing (EMNLP), pp. 1724-1734, Doha, Qatar, Oc-
tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL
https://www.aclweb.org/anthology/D14-1179.
Hal Daume, John Langford, and Daniel Marcu. Search-based structured prediction. CoRR,
abs/0907.0786, 2009. URL http://arxiv.org/abs/0907.0786.
Mikael Henaff. Explicit explore-exploit algorithms in continuous state spaces. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems 32, pp. 9377-
9387. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
9135-explicit-explore-exploit-algorithms-in-continuous-state-spaces.
pdf.
Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive policy learning with un-
certainty regularization for driving in dense traffic. In International Conference on Learning
RePresentations, 2019. URL https://openreview.net/forum?id=HygQBn0cYm.
Matteo Hessel, Joseph Modayil, Hado P. van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In AAAI, 2018.
Todd Hester, Matej Vecerlk, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Hor-
gan, John Quan, Andrew Sendonaris, Ian Osband, Gabriel Dulac-Arnold, John Agapiou, Joel Z.
Leibo, and Audrunas Gruslys. Deep q-learning from demonstrations. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative AP-
Plications of Artificial Intelligence (IAAI-18), and the 8th AAAI SymPosium on Educational
Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018, pp. 3223-3230, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/
AAAI18/paper/view/16976.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Rene Traore, Prafulla Dhariwal,
Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schul-
man, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/hill-a/
stable-baselines, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 29, pp. 4565-4573. Curran Associates, Inc., 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL
http://arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a con-
ference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.
Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
10
Published as a conference paper at ICLR 2020
Hoang M Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, and Hal DaUme III. Hierar-
chical imitation and reinforcement learning. arXiv preprint arXiv:1803.00590, 2018.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR,
abs/1509.02971, 2016.
Yuping Luo, Huazhe Xu, and Tengyu Ma. Learning self-correctable policies and value functions
from demonstrations with negative sampling. CoRR, abs/1907.05634, 2019. URL http://
arxiv.org/abs/1907.05634.
Kunal Menda, Katherine Rose Driggs-Campbell, and Mykel J. Kochenderfer. Ensembledagger: A
bayesian approach to safe imitation learning. ArXiv, abs/1807.08364, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 1928-1937, New York, New York, USA, 20-22 Jun 2016. PMLR. URL http:
//proceedings.mlr.press/v48/mniha16.html.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. J. Mach. Learn.
Res., 9:815857, June 2008. ISSN 1532-4435.
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Over-
coming exploration in reinforcement learning with demonstrations. 2018 IEEE International
Conference on Robotics and Automation (ICRA), pp. 6292-6299, 2017.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. CoRR, abs/1602.04621, 2016. URL http://arxiv.org/abs/1602.
04621.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In ICML, 2019.
Dean A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network.
In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems 1,
pp. 305-313. Morgan-Kaufmann, 1989. URL http://papers.nips.cc/paper/
95-alvinn-an- autonomous- land- vehicle- in-a-neural-network.pdf.
Siddharth Reddy, Anca D. Dragan, and Sergey Levine. SQIL: imitation learning via regularized
behavioral cloning. CoRR, abs/1905.11108, 2019. URL http://arxiv.org/abs/1905.
11108.
Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Yee Whye Teh
and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 661-668,
Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR. URL http://proceedings.
mlr.press/v9/ross10a.html.
Stephane Ross andJ Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret
learning. arXiv preprint arXiv:1406.5979, 2014.
11
Published as a conference paper at ICLR 2020
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav
Dudk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence
and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 627-635, Fort
Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL http://Proceedings .mlr.press/
v15/ross11a.html.
Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample efficient imitation learning for
continuous control. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=BkN5UoAqF7.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Pranav Shyam, Wojciech Jaskowski, and Faustino Gomez. Model-based active exploration. CoRR,
abs/1810.12162, 2018.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014. URL http://www.cs.toronto.edu/~rsalakhu/
papers/srivastava14a.pdf.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 3309-3318. JMLR. org, 2017.
Arun Venkatraman, Martial Hebert, and J. Andrew Bagnell. Improving multi-step prediction of
learned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial
Intelligence, AAAI’15, pp. 3024-3030. AAAI Press, 2015. ISBN 0-262-51129-0. URL http:
//dl.acm.org/citation.cfm?id=2888116.2888137.
Ruohan Wang, Carlo Ciliberto, Pierlugi Amadori, and Yiannis Demiris. Random expert distilla-
tion: Imitation learning via expert policy support estimation. In Proceedings of International
Conference on Machine Learning. ACM, 2019.
Sean Welleck, Kiante Brantley, Hal DaUme III, and KyUnghyUn Cho. Nοn-mοnοtοnic sequential text
generation. CoRR, abs/1902.02192, 2019. URL http://arxiv.org/abs/1902.02192.
12
Published as a conference paper at ICLR 2020
A	Proofs
Lemma 1. For any π ∈ Π we have Jexp (π) ≤ κJalg (π)
Proof. We will first show that for any ∏ ∈ Π and U ⊆ S, We have Jexp (∏) ≤ OU Jaig (∏). We can
rewrite this as:
Jexp(π) = Es〜d∏ hkπ3S)- π*3S)II]
=Es〜d∏ [l(s ∈ U)k∏(∙∣s) - π?(∙∣s)k] + Es〜d∏ [l(s ∈ U)k∏(∙∣s) - π?(∙∣s)k]
We begin by bounding the first term:
Es〜dn [l(s ∈ U)k∏(∙∣s) - ∏?(∙∣s)k] = X d∏(s)k∏(∙∣s) - π?(∙∣s)k
π	s∈U =X -dπ(s)d∏*(S)kπ(dS) - π*(dS)k s∈U dn*(S) ≤ X (m0axsuP7π7sτ) dn*(S)kn(1S) - n*(1S)k π0∈Π s∈U dπ* (S) s ∈U	^} α7U ) =α(U) X d∏*(S)∣∣π(∙∣S)-π?(∙∣S)k s∈U ≤ α(U) Xd∏*(S)∣∣π(∙∣S)-π?(∙∣S)k s∈S =α(U)Es 〜d∏*[k∏(∙∣S)-π?(∙∣S)ki = α(U)JBC (π)
We next bound the second term:
Es〜d∏ [l(s ∈ U)k∏(∙∣s) - ∏*(∙∣s)k] ≤ Es〜d∏ [l(s ∈ U)]
	≤ EinhI(S ∈ U)mina∈A VarniUE [ni(a|S)]i β (U ) =β(U)Es〜d∏ [i(s ∈ U) X n(a|S)Varni〜nE[ni(a|S)]i =β(U) X dn(S) X n(a|S)Varni〜nE[ni(a|S)] β( ) s∈U	a∈A '	{z	'
A(π)
Now observe we can decompose the RL cost as follows:
13
Published as a conference paper at ICLR 2020
JU(π) = Es〜d∏ ,a〜π(∙∣s) [Var∏i〜∏eni(aIs)]
=X d∏(s) X∏(a∣s) IVarni〜∏E∏i(a∣s)]
=Xd∏(s) X∏(a∣s) IVarni〜∏e∏i(a∣s)] + X d∏(s) X∏(a∣s)[
s∈U	a
V-------------------V-----------
B(n)
Putting these together, we get the following:
Varni 〜∏e ∏i(a∣s)]
s∈U	a
X--------------------
A(π)
}
Jexp(π) ≤ α(U)JBC(π) + §(U)A(π)
≤
F JBC(∏) + 工 A(∏)
瑞 JBC(∏) + 就 A(∏)
(π) + A(π)
(π) + JU(π)
≤
≤
α(U) T 外
My Jalg ⑺
Here We have Used the fact that β(U) ≤ 1 since 0 ≤ ∏(a∣s) ≤ 1 and α(U) ≥ sups∈u d∏(∣) = 1
hence OIU) ≤ 1. Taking the minimum over subsets U ⊆ S, we get Jexp(∏) ≤ κJalg(∏).
□
Lemma 2. minn∈Π Jalg (π) ≤ 2
Proof. PlUgging the optimal policy into Jalg , We get:
Jalg(π?) = JBC(π?) + JU(π?)
=0 + Es〜d∏? ,a〜n? (∙∣ s) [Var∏i〜ΠE [πi (a| s)]]
=Es 〜d∏* ,a 〜n* (∙∣s) h E X 卜 i(a|s) - n(a|S)) i
i=1
1E	2	2
≤ Es〜dπ*,a〜n*(∙∣s) [e X (∏i(a∣s) - ∏*(a∣s)) + (∏(a∣s) - ∏*(a∣s))]
i=1
Es〜d∏* ,a〜n* (∙∣ s)
1E	2
[e X 卜i(a|s) - n*(a|S)) ] + Es〜d∏* ,a〜n*(∙∣s)
"i=1	S--------------
_ - /
{z
[(π(als) -
^z'‰,"^^^^^^^^
n*(a|S)) i
Term1
Term2
We will first bound Term 1:
14
Published as a conference paper at ICLR 2020
1 E	2
ES〜dπ*,α〜π*(∙∣s) [e X 卜i(α∣s) — π*(α∣s))]
i=1
α∈A
i=1
E
≤
E ES〜或
Q* [ X π*(α∣s) X 卜i(α∣s) — π*(α∣s)∣]
≤
EES
α∈A
E
i=1
~%*[XX∣∏i(α∣s) — π*(α∣s) ∣]
i=1a∈A
≤
1E
E X Es"Jk∏i(∙∣s)-π*(∙∣s)∣∣]
i=1
1 E
-E x
≤
€
i=1
€
We will next bound Term 2:
「/	、2]	「/	1、石、	ʌ 2 -I
ES〜dπ*,a〜π*(∙∣S) [(π(a∣s) — π*(a∣s)) ] = ES〜dπ*,a〜π*(∙∣S) [(π*(a∣s) — E X∏i(a∣s))]
_	「/ 1 二，―	1 X , , A2I
=ES 〜d∏* ,a 〜π*(∙∣S) [(e X π (OIS)- E X πi(OIS))]
i=1
i=1
1E	2
=ES〜d∏*,α〜π*(∙∣s) [(e X(π*(a∣s) — πi(a∣S)))]
i=1
≤ ES 〜d∏* ,a-π*(∙∣S)[ E E X"(a∣s)—∏i(a∣s)) ] (Cauchy
i=1
Schwarz)
1E	2
=E XES〜d∏*,a〜π*(∙∣s)[(π (α∣s) — πi(α∣s))]
i=1
1E
≤ E XES〜dπ*,α〜π*(∙∣s)[卜*(α∣s) — ∏i(α∣s) I ]
i=1
1E
≤ E X Es%J∣∣π*(∙∣s)-∏i(∙∣s)∣∣]
i=1
1E
=E X jBc(πi)
i=1
≤ €
The last step follows from our optimization oracle assumption: 0 ≤ minπ∈∏ Jbc (∏) ≤ Jbc (∏?)=
0, hence JBc(∏i) ≤ 0 + e = e. Combining the bounds on the two terms, We get Jaig (∏*) ≤ 2e.
Since π? ∈ Π, the result follows.
□
15
Published as a conference paper at ICLR 2020
Theorem 1. Let π be the result of minimizing Jalg using our optimization oracle, and assume that
QT-t+ι(s, a) — QT-t+ι(s, ∏?) ≤ U for all a ∈ A, t ∈ {1, 2,…，T}, d∏(S) > 0. Then π satisfies
J(π) ≤ J(π?) + 3uκeT.
Proof. By our optimization oracle and Lemma 2, we have
Jalg (∏) ≤ min Jalg (∏) + e
π∈Π
≤ 2 +
= 3
Combining with Lemma 1, we get:
Jexp (∏) ≤ κJalg(Π)
≤ 3κ
Applying Theorem 1 from (Ross et al., 2011), We get J(π) ≤ J(π?) + 3uκeT.
□
B	Importance of B ehavior Cloning Cost
The folloWing example shoWs hoW minimizing the uncertainty cost alone Without the BC cost can
lead to highly sub-optimal policies if the demonstration data is generated by a stochastic policy
Which is only slightly suboptimal. Consider the folloWing deterministic chain MDP:
(zi	ɑɪ	ɑɪ
ɑo αo aɑ
The agent alWays starts in s1, and gets a reWard of 1 in s3 and 0 elseWhere. The optimal policy is
given by:
π?(IsO) = (Oj)
∏?(∙∣sι) = (0,1)
∏?(∙∣S2) = (0,1)
∏?(∙∣S3) = (0,1)
Assume the demonstration data is generated by the folloWing policy, Which is only slightly subopti-
mal:
πdemo (Jsθ) = (0, 1)
πdemo (1sl) = (0, 1)
∏demo (∙∣s2) = (0.1, 0.9)
πdemo (1s3)= (0, 1)
Let us assume realizability and perfect optimization for simplicity. If both transitions (s2, a0) and
(s2, a1) appear in the demonstration data, then Random Expert Distillation (RED) Will assign zero
16
Published as a conference paper at ICLR 2020
cost to both transitions. If we do not use bootstrapped samples to train the ensemble, then DRIL
without the BC cost (we will call this UO-DRIL for Uncertainty-Only DRIL) will also assign zero
cost to both transitions since all models in the ensemble would recover the Bayes optimal solution
given the demonstration data. If we are using bootstrapped samples, then the Bayes optimal solu-
tion for each bootstrapped sample may differ and thus the different policies in the ensemble might
disagree in their predictions, although given enough demonstration data we would expect these dif-
ferences (and thus the uncertainty cost) to be small.
Note also that since no samples at the state s0 occur in the demonstration data, both RED and UO-
DRIL will likely assign high uncertainty costs to state-action pairs at (s0, a0), (s0, a1) and thus avoid
highly suboptimal policies which get stuck at s0 .
Now consider policies ∏1,∏2 given by:
πι(ISO) = (OJ)
πι(ISI) = (OJ)
π1(∙ls2) = (1, 0)
π1(∙ls3) = (0, 1)
and
π2(∙ls0) = (0, 1)
π2(∙ls1) = (0, 1)
∏2(∙∣S2) = (0.2, 0.8)
π2(∙ls3) = (0, 1)
Both of these policies only visit state-action pairs which are visited by the demonstration policy.
In the case described above, both RED and UO-DRIL will assign ∏ι and ∏2 similarly low costs.
However, ∏ι will cycle forever between si and s2, never collecting reward, while ∏2 will with high
probability reach S3 and stay there, thus achieving high reward. This shows that minimizing the
uncertainty cost alone does not necessarily distinguish between good and bad policies. However, ∏ι
will incur a higher BC cost than ∏2, since ∏2 more closely matches the demonstration data at s2.
This shows that including the BC cost can be important for further disambiguating between policies
which all stay within the distribution of the demonstration data, but have different behavior within
that distribution.
C Experimental Details
C.1 Atari Environments
All behavior cloning models were trained to minimize the negative log-likelihood classification loss
on the demonstration data for 500 epochs using Adam (Kingma & Ba, 2014) and a learning rate of
2.5 ∙ 10-4. We stopped training once the validation error did not improve for 20 epochs. For our
method, we initially performed a hyperparameter search on Space Invaders over the values shown
in Table 1
17
Published as a conference paper at ICLR 2020
Table 1: Hyperparameters for DRIL
Hyperparameter	Values Considered	Final Value
Policy Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3
Quantile cutoff	0.8, 0.9, 0.95, 0.98	0.98
Number of supervised updates	1,5	1
Number of policies in ensemble	5	5
Gradient clipping	0.1	0.1
Entropy coefficient	0.01	0.01
Value loss coefficient	0.5	0.5
Number of steps	128	128
Parallel Environments	16	16
We then chose the best values and kept those hyperparameters fixed for all other environments. All
other A2C hyperparameters follow the default values in the repo (Kostrikov, 2018): policy networks
consisted of 3-layer convolutional networks with 8-32-64 feature maps followed by a single-layer
MLP with 512 hidden units.
For GAIL, we used the implementation in (Kostrikov, 2018) and replaced the MLP discriminator
by a CNN discriminator with the same architecture as the policy network. We initially performed
a hyperparameter search on Breakout with 10 demonstrations over the values shown in Table 2.
However, we did not find any hyperparameter configuration which performed better than behavioral
cloning.
Table 2: Hyperparameters for GAIL
Hyperparameter	Values Considered	Final Value
Policy Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3
Discriminator Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3
Number of discriminator updates	1,5,10	5
Gradient clipping	0.1	0.1
Entropy coefficient	0.01	0.01
Value loss coefficient	0.5	0.5
Number of steps	128	128
Parallel Environments	16	16
C.2 Continuous Control
All behavior cloning and ensemble models were trained to minimize the mean-squared error regres-
sion loss on the demonstration data for 500 epochs using Adam (Kingma & Ba, 2014) and a learning
rate of 2.5 ∙ 10-4. Policy networks were 2-layer fully-connected MLPs with tanh activations and 64
hidden units.
Table 3: Hyperparameters (our method)
Hyperparameter	Values Considered	Final Value
Policy Learning rate	2.5 ∙ 10-3, 2.5 ∙1 0-4,1 ∙ 10-4, 5 ∙ 10-5	2.5 ∙ 10-5
Quantile cutoff	0.98	0.98
Number of supervised updates	1	1
Number of policies in ensemble	5	5
Gradient clipping	0.1	0.1
Entropy coefficient	0.01	0.01
Value loss coefficient	0.5	0.5
Number of steps	128	128
Parallel Environments	16	16
18
Published as a conference paper at ICLR 2020
D Ablation Experiments
In this section we provide ablation experiments examining the effects of the cost function clipping
and the role of the BC loss. We also compare the ensemble approach to a dropout-based approxima-
tion and show that DRIL works well in both cases.
Table 4: Ablation Experiments with 3 expert trajectories
Environment	SpaceInvaders	Breakout	BeamRider
DRIL (ensemble)	-355^	286.7	2033.4
DRIL (dropout)	581.4	205.4	2124.5
DRIL (raw cost)	421.8	70.9	1265.5
DRIL (no BC cost)	102.1	78.3	538.4
BC		257.0	2.7	689.7
Results are shown in Figure 4. First, switching from the clipped cost in {-1, +1} to the the raw
cost causes a drop in performance. One explanation may be that since the raw costs are always
positive (which corresponds to a reward which is always negative), the agent may learn to terminate
the episode early in order to minimize the total cost incurred. Using a cost/reward which has both
positive and negative values avoids this behavior.
Second, optimizing the pure BC cost performs better than the pure uncertainty cost for some envi-
ronments (SpaceInvaders, BeamRider) while optimizing the pure uncertainty cost performs better
than BC in Breakout. DRIL, which optimizes both, has robust performance and performs the best
over all environments.
For the dropout approximation we trained a single policy network with a dropout rate of 0.1 applied
to all layers except the last, and estimated the variance for each state-action pair using 5 different
dropout masks. Similarly to the ensemble approach, we computed the 98th quantile of the variance
on the demonstration data and used this value in our clipped cost. MC-dropout performs similarly
to the ensembling approach, which shows that our method can be paired with different approaches
to posterior estimation.
19