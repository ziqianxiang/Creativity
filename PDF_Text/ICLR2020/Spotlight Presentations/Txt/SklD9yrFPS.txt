Published as a conference paper at ICLR 2020
Neural Tangents:
Fast and Easy Infinite Neural Networks in Python
Roman Novak； LeChao Xiao； Jiri Hron*, Jaehoon Lee,
Alexander A. Alemi, Jascha Sohl-Dickstein,	Samuel S. SChoenholz；
Google Brain, *University of Cambridge
{romann, xlc}@google.com, jh2084@cam.ac.uk, {jaehlee, alemi, jaschasd, schsam}@google.com
Ab stract
Neural Tangents is a library for working with infinite-width neural networks.
It provides a high-level API for specifying complex and hierarchical neural network
architectures. These networks can then be trained and evaluated either at finite-
width as usual or in their infinite-width limit. Infinite-width networks can be
trained analytically using exact Bayesian inference or using gradient descent via
the Neural Tangent Kernel. Additionally, Neural Tangents provides tools
to study gradient descent training dynamics of wide but finite networks in either
function space or weight space.
The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations
can be automatically distributed over multiple accelerators with near-linear scaling
in the number of devices. Neural Tangents is available at
www.github.com/google/neural-tangents
We also provide an accompanying interactive Colab notebook* 1.
1	Introduction
Deep neural networks (DNNs) owe their success in part to the broad availability of high-level, flexible,
and efficient software libraries like Tensorflow (Abadi et al., 2015), Keras (Chollet et al., 2015),
PyTorch.nn (Paszke et al., 2017), Chainer (Tokui et al., 2015; Akiba et al., 2017), JAX (Bradbury
et al., 2018a), and others. These libraries enable researchers to rapidly build complex models by
constructing them out of smaller primitives. The success of new machine learning approaches will
similarly depend on developing sophisticated software tools to support them.
1.1	Infinite-width Bayesian neural networks
Recently, a new class of machine learning models has attracted significant attention, namely, deep
infinitely wide neural networks. In the infinite-width limit, a large class of Bayesian neural networks
become Gaussian Processes (GPs) with a specific, architecture-dependent, compositional kernel;
these models are called Neural Network Gaussian Processes (NNGPs). This correspondence was
first established for shallow fully-connected networks by Neal (1994) and was extended to multi-
layer setting in (Lee et al., 2018; Matthews et al., 2018b). Since then, this correspondence has
been expanded to a wide range of nonlinearities (Matthews et al., 2018a; Novak et al., 2019) and
architectures including those with convolutional layers (Garriga-Alonso et al., 2019; Novak et al.,
2019), residual connections (Garriga-Alonso et al., 2019), pooling (Novak et al., 2019), as well as
graph neural networks (Du et al., 2019). The results for individual architectures have subsequently
been generalized, and it was shown that a GP correspondence holds for a general class of networks
that can be mapped to so-called tensor programs in (Yang, 2019). The recurrence relationship defining
* Equal contribution. ^ Work done during an internship at Google Brain.
1colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb
1
Published as a conference paper at ICLR 2020
the NNGP kernel has additionally been extensively studied in the context of mean field theory and
initialization (Cho & Saul, 2009; Daniely et al., 2016; Poole et al., 2016; Schoenholz et al., 2016;
Yang & Schoenholz, 2017; Xiao et al., 2018; Li & Nguyen, 2019; Pretorius et al., 2018; Hayou et al.,
2018; Karakida et al., 2018; Blumenfeld et al., 2019; Hayou et al., 2019).
1.2	Infinite-width neural networks trained by gradient descent
In addition to enabling a closed form description of Bayesian neural networks, the infinite-width
limit has also very recently provided insights into neural networks trained by gradient descent. In the
last year, several papers have shown that randomly initialized neural networks trained with gradient
descent are characterized by a distribution that is related to the NNGP, and is described by the
so-called Neural Tangent Kernel (NTK) (Jacot et al., 2018; Lee et al., 2019; Chizat et al., 2019), a
kernel which was implicit in some earlier papers (Li & Liang, 2018; Allen-Zhu et al., 2018; Du et al.,
2018a;b; Zou et al., 2019). In addition to this “function space” perspective, a dual, “weight space”
view on the wide network limit was proposed in Lee et al. (2019) which showed that networks under
gradient descent were well-described by the first-order Taylor series about their initial parameters.
1.3	Promise and practical barriers to working with infinite-width networks
Combined, these discoveries established infinite-width networks as useful theoretical tools to under-
stand a wide range of phenomena in deep learning. Furthermore, the practical utility of these models
has been proven by achieving state-of-the-art performance on image classification benchmarks among
GPs without trainable kernels (Garriga-Alonso et al., 2019; Novak et al., 2019; Arora et al., 2019a),
and by their ability to match or exceed the performance of finite width networks in some situations,
especially for fully- and locally-connected model families (Lee et al., 2018; Novak et al., 2019; Arora
et al., 2019b).
However, despite their utility, using NNGPs and NTK-GPs is arduous and can require weeks-to-
months of work by seasoned practitioners. Kernels corresponding to neural networks must be
derived by hand on a per-architecture basis. Overall, this process is laborious and error prone, and
is reminiscent of the state of neural networks before high quality Automatic Differentiation (AD)
packages proliferated.
1.4	Summary of contributions
In this paper, we introduce a new open-source software library called Neural Tangents targeting
JAX (Bradbury et al., 2018a) to accelerate research on infinite limits of neural networks. The main
features of Neural Tangents are:2
•	A high-level neural network API for specifying complex, hierarchical, models. Networks
specified using this API can have their infinite-width NNGP kernel and NTK evaluated
analytically (§2.1, Listings 1, 2, 3, §B.2).
•	Functions to approximate infinite-width kernels by Monte Carlo sampling for networks
whose kernels cannot be constructed analytically. These methods are agnostic to the neural
network library used to build the network and are therefore quite versatile (§2.2, Figure 2,
§B.5).
•	An API to analytically perform inference using infinite-width networks either by computing
the Bayesian posterior or by computing the result of continuous gradient descent with an
MSE loss. The API additionally includes tools to perform inference by numerically solving
the ODEs corresponding to: continuous gradient descent, with-or-without momentum, on
arbitrary loss functions, at finite or infinite time (§2.1, Figure 1, §B.4).
•	Functions to compute arbitrary-order Taylor series approximations to neural networks about
a given setting of parameters to explore the weight space perspective on the infinite-width
limit (§B.6, Figure 6).
•	Leveraging XLA, our library runs out-of-the-box on CPU, GPU, or TPU. Kernel computa-
tions can automatically be distributed over multiple accelerators with near-perfect scaling
(§3.2, Figure 5, §B.3).
2See §A for Neural Tangents comparison against specific relevant prior works.
2
Published as a conference paper at ICLR 2020
We begin with three short examples (§2) that demonstrate the ease, efficiency, and versatility of
performing calculations with infinite networks using Neural Tangents. With a high level view of
the library in hand, we then dive into a number of technical aspects of our library (§3).
1.5	Background
We briefly describe the NNGP (§1.1) and NTK (§1.2). NNGP. Neural networks are often structured as
affine transformations followed by pointwise applications of nonlinearities. Let zil (x) describe the ith
pre-activation following a linear transformation in lth layer of a neural network. At initialization, the
parameters of the network are randomly distributed and so central-limit theorem style arguments can
be used to show that the pre-activations become Gaussian distributed with mean zero and are therefore
described entirely by their covariance matrix K(x, x0) = E[zil (x)zil (x0)]. This describes a NNGP
with the kernel, K(x, x0). One can use the NNGP to make Bayesian posterior predictions at a test
point, x, which are Gaussian distributed with with mean μ(χ) = K(χ, X)K(X, X)-1 Y and variance
σ2 (x) = K(x, x) - K(x, X)K(X, X)-1K(X, x), where (X, Y) is the training set of inputs and
targets respectively. NTK. When neural networks are optimized using continuous gradient descent
with learning rate η on mean squared error (MSE) loss, the function evaluated on training points
evolves as ∂tft(X) = -ηJt(X)Jt(X)T (ft(X) - Y) where Jt(X) is the Jacobian of the output ft
evaluated at X and Θt(X, X) = Jt (X)Jt(X)T is the NTK. In the infinite-width limit, the NTK
remains constant (Θt = Θ) throughout training and the time-evolution of the outputs can be solved
in closed form as a Gaussian with mean ft(x) = Θ(x, X)Θ(X, X)-1 (I - exp [-ηΘ(X, X)t]) Y.
2	Examples
We begin by applying Neural Tangents to several example tasks. While these tasks are designed
for pedagogy rather than research novelty, they are nonetheless emblematic of problems regularly
faced in research. We emphasize that without Neural Tangents, it would be necessary to derive
the kernels for each architecture by hand.
2.1	Inference with an Infinitely Wide Neural Network
We begin by training an infinitely wide neural network with gradient descent and comparing the
result to training an ensemble of wide-but-finite networks. This example is worked through in detail
in the Colab notebook.3
We train on a synthetic dataset with training data drawn from the process yi = sin(xi) + i with
Xi 〜 UnifOrm(-π,π) and ∈i 〜 N(0,σ2) independently and identically distributed. To train an
infinite neural network with Erf activations4 on this data using gradient descent and an MSE loss we
write the following:
from neural_tangents import predict, Stax
init_fn, apply_fn, kernel_fn = stax.serial(
stax.Dense(2048, W_std=1.5, b_std=0.05), stax.Erf(),
stax.Dense(2048, W_std=1.5, b_std=0.05), stax.Erf(),
stax.Dense(1, W_std=1.5, b_std=0.05))
y_mean, y_var = Predict.gp_inference(kernel_fn, x_train, y_train, x_test, 'ntk',
diag_reg=1e-4, compute_cov=TrUe)
The above code analytically generates the predictions that would result from performing gradient
descent for an infinite amount of time. However, it is often desirable to investigate finite-time learning
dynamics of deep networks. This is also supported in Neural Tangents as illustrated in the
following snippet:
3
www.colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb
4Error function, a nonlinearity similar to tanh; see §D for other implemented nonlinearities, including Relu.
3
Figure 1: Training dynamics for an ensemble of finite-width networks compared with an infi-
nite network. Left: Mean and variance of the train and test MSE loss evolution throughout training.
Right: Comparison between the predictions of the trained infinite network and the respective ensem-
ble of finite-width networks. The shaded region and the dashed lines denote two standard deviations
of uncertainty in the predictions for the infinite network and the ensemble respectively.
Predict_fn = Predict.gradient_descent_mse_gp(kernel_fn, x_train, y_train, x_test, 'ntk',
diag_reg=1e-4, compute_cov=TrUe)
y_mean, y_var = Predict_fn(t=100) # Predict the distribution at t = 100.
The above specification set the hidden layer widths to 2048, which has no effect on the infinite
width network inference, but the init_fn and apply_fn here correspond to ordinary finite width
networks. In Figure 1 We compare the result of this exact inference with training an ensemble of
one-hundred of these finite-width networks by looking at the training curves and output predictions
of both models. We see excellent agreement between exact inference using the infinite-width model
and the result of training an ensemble using gradient descent.
2.2	An Infinitely WideResNet
The above example considers a relatively simple network on a synthetic task. In practice we may
want to consider real-world architectures, and see how close they are to their infinite-width limit. For
this task we study a variant of an infinite-channel Wide Residual Network (Zagoruyko & Komodakis,
2016) (WRN-28-∞). We first define both finite and infinite models within Listing 1.
We now study how quickly the kernel of the finite-channel WideResNet approaches its infinite channel
limit. We explore two different axes along which convergence takes place: first, as a function of
the number of channels (as measured by the widening factor, k) and second as a function of the
number of finite-network Monte Carlo samples we average over. Neural Tangents makes it easy
to compute MC averages of finite kernels using the following snippet:
kernel_fn = nt.monte_carlo_kernel_fn(init_fn, apply_fn, rng_key, n_samples)
SamPIed_kernel = kernel_fn(x, x)
The convergence is shown in Figure 2. We see that as both the number of samples is increased or
the network is made wider, the empirical kernel approaches the kernel of the infinite network. As
noted in Novak et al. (2019), for any finite widening factor the MC estimate is biased. Here, however,
the bias is small relative to the variance and the distance to the empirical kernel decreases with the
number of samples.
2.3	Comparison of Neural Network architectures and training set sizes
The above examples demonstrate how one might construct a complicated architecture and perform
*
inference using Neural TangentsNext we train a range of architectures on CIFAR-10 and compare
their performance as a function of dataset size. In particular, we compare a fully-connected network,
4
Published as a conference paper at ICLR 2020
a convolutional network whose penultimate layer vectorizes the image, and the wide-residual network
described above. In each case, we perform exact infinite-time inference using the analytic infinite-
width NNGP or NTK. For each architecture we perform a hyperparameter search over the depth of
the network, selecting the depth that maximizes the marginal log likelihood on the training set.
def WideReSNetBloCk(Channels, strides=。， 1), Channel_mismatCh=False):
Main = stax.serial(stax.Relu(), stax.Conv(channels, (3, 3), strides, padding='SAME'),
stax.Relu(), stax.Conv(channels, (3, 3), padding='SAME'))
Shortcut = (stax.Identity() if not Channel_mismatch else
stax.Conv(channels, (3, 3), strides, padding='SAME'))
return stax.serial(stax.FanOut(2), stax.parallel(Main, Shortcut), stax.FanInSum())
def WideReSNetGroup(n, channels, strides=(1, 1)):
blocks = [WideResNetBlock(channels, strides, Channel_mismatch=True)]
for _ in range(n - 1):
blocks += [WideResNetBlock(channels, (1, 1))]
return stax.serial(*blocks)
def WideReSNet(block_size, k, num_classes):
return stax.serial(stax.Conv(16, (3, 3), padding='SAME'),
WideReSNetGroup(block_size, int(16 * k)),
WideReSNetGroup(block_size, int(32 * k), (2, 2)),
WideReSNetGroup(block_size, int(64 * k), (2, 2)),
stax.GlobalAvgPool(), stax.Dense(num_classes))
init_fn, apply_fn, kernel_fn = WideReSNet(block_size=4, k=1, num_cIaSSeS=10)
Listing 1: Definition of an infinitely WideResNet. This snippet simultaneously defines a finite
(init_fn, apply_fn ) and an infinite ( kernel_fn ) model. This model is used in Figures and 3.
NNGP
5
NTK
(J0sjsmcicφpiM)ZMOi 4
log2(#Samples) 10
(J0sjsmcicφpiM)ZMOi4
O log2(#Samples)
O
Figure 2: Convergence of the Monte Carlo (MC) estimates of the WideResNet WRN-28-k (where
k is the widening factor) NNGP and NTK kernels (computed with monte_carlo_kernel_fn )
to their analytic values (WRN-28-∞, computed with kerneLfn ), as the network gets wider by
increasing the widening factor (vertical axis) and as more random networks are averaged over
(horizontal axis). Experimental detail. The kernel is computed in 32-bit precision on a 100 × 50
batch of 8 × 8-downsampled CIFAR10 (Krizhevsky, 2009) images. For sampling efficiency, for
NNGP the output of the penultimate layer was used, and for NTK the output layer was assumed to
be of dimension 1 (all logits are i.i.d. conditioned on a given input). The displayed distance is the
relative Frobenius norm squared, i.e. kK - Kk,nkF2 / kKkF2 , where k is the widening factor and n is
the number of samples.
5
Published as a conference paper at ICLR 2020
0.
0.80-..........................-
0.75-
⅛ 0.70-
ω 0.65-
o 0.60 -
8 0.55 -
K 0.50-
π>
Q 0.45-
0.40-
2® 29 21。2"2122"2好
Training Set Size
0.060-∣θ 2》2、 2⅛ 2^02^12^22^32^42^5
Training Set Size
θ'^26 27 28 29 2^02^12^22^32^42^5
Training Set Size
Figure 3: CIFAR-10 classification with varying neural network architectures. NEURAL TAN-
gents simplify experimentation with architectures. Here we use infinite time NTK inference and
full Bayesian NNGP inference for CIFAR-10 for Fully Connected (FC, Listing 3), Convolutional
network without pooling (CONV, Listing 2), and Wide Residual Network w/ pooling (WRES-
NET, Listing 1). As is common in prior work (Lee et al., 2018; Novak et al., 2019), the classification
task is treated as MSE regression on zero-mean targets like (-0.1, . . . , -0.1, 0.9, -0.1, . . . , -0.1) .
For each training set size, the best model in the family is selected by minimizing the mean negative
marginal log-likelihood (NLL, right) on the training set.
The results are shown in Figure 3. We see that in each case the performance of the model increases
approximately logarithmically in the size of the dataset. Moreover, we observe a clear hierarchy of
performance, especially at large dataset size, in terms of architecture (FC < CONV < WRESNET w/
pooling).
3	Implementation: Transforming Tensor Ops to Kernel Ops
Neural networks are compositions of basic tensor operations such as: dense or convolutional affine
transformations, application of pointwise nonlinearities, pooling, or normalization. For most networks
without weight tying between layers the kernel computation can also be written compositionally
and there is a direct correspondence between tensor operations and kernel operations (see §3.1 for
an example). The core logic of Neural Tangents is a set of translation rules, that sends each
tensor operation acting on a finite-width layer to a corresponding transformation of the kernel for an
infinite-width network. This is illustrated in Figure 4 for a simple convolutional architecture. In the
associated table, we compare tensor operations (second column) with corresponding transformations
of the NT and NNGP kernel tensors (third and fourth column respectively). See §D for a list of all
tensor operations for which translation rules are currently implemented.
One subtlety to consider when designing networks is that most infinite-width results require nonlinear
transformations to be preceded by affine transformations (either dense or convolutional). This is be-
cause infinite-width results often assume that the pre-activations of nonlinear layers are approximately
Gaussian. Randomness in weights and biases causes the output of infinite affine layers to satisfy
this Gaussian requirement. Fortunately, prefacing nonlinear operations with affine transformations is
common practice when designing neural networks and Neural Tangents will raise an error if this
requirement is not satisfied.
3.1	A taste of Tensor-to-Kernel Ops Translation
To get some intuition behind the translation rules, we consider the case of a nonlinearity followed by
a dense layer. Let z = z (X, θ) ∈ Rd×n be the preactivations resulting from d distinct inputs at a
node in some hidden layer of a neural network. Suppose z has NNGP kernel and NTK given by
Kz = Eθ LizT] , θz	= Eθ d∂θ	(d∂θ)	⑴
where zi	∈ Rd	is the ith neuron andθ are the	parameters	in the network	up until z . Here d
is the cardinality of the network inputs X and n is the number of neurons in the z node. We
6
Published as a conference paper at ICLR 2020
10 classes
per input
①。UEμE>oo
d°.NNO
affine transform
㊁110
4x4 covariance
per class
4x4x10 input covariance
(not always necessary to track the whole 4x4x10x10 covariance；
Layer	Tensor Op	NNGP Op	NTK Op
0 (input)	y0 = X	Co = XXT	Θo=0
0 (pre-activations)	Z0 = Conv (y0)	KC0 = A (CO)	θ0 = C0 + A (θ0)
1 (activations)	y1 = φ (Z 0)	C1 = T (C0)	Θ1 = T (Co) Θ Θ0
1 (pre-activations)	z1 = Conv (y1)	C1 = A (CI)	Θ1 = C1 + A (Θ1)
2 (activations)	y2 = Φ (Z1)	C2 = T (C1)	Θ2 = T (C1 )θ θ 1
2 (readout) 		z 2 = DenSe ◦ FIatten (y2)	CC2 = Tr (C2)	Θ2 = C2 + Tr (Θ2)
Figure 4: An example of the translation of a convolutional neural network into a sequence of
kernel operations. We demonstrate how the compositional nature of a typical NN computation on its
inputs induces a Corresponding compositional computation on the NNGP and NT kernels. Presented
is a 2-hidden-layer 1D CNN with nonlinearity φ, performing regression on the 10-dimensional
outputs Z2 for each of the 4 (1, 2, 3, 4) inputs X from the dataset X. To declutter notation, unit weight
and zero bias variances are assumed in all layers. Top: recursive output (z2) computation in the CNN
(toP) induces a respective recursive nngp kernel (KC2 ③ Iio) computation (NTK computation being
similar, not shown). Bottom: explicit listing of tensor and corresponding kernel ops in each layer.
See Table 1 for operation definitions. Illustration and description adapted from Figure 3 in Novak
et al (2019).
.	.
assume Z is a mean zero multivariate Gaussian∙ We wish to compute the kernel corresponding
to h = Dense (。“, σb) (φ(z)) by computing the kernels of y = φ(z) and h = Dense (。“, σb)(y)
separately Here
h = DenSeSω,σb Xy) ≡ (1A∕n) σωWy + σβ	⑵
and the variables Wij and βi are i.i.d. Gaussian N(0,1). We will compute kernel operations -
denoted φ* and Dense(σω, σb厂-induced bythe tensor OPeratiOnS φ and Dense(σω, σ尸.Finally,
we will compute the kernel operation associated with the composition (Dense(σω, σb) ◦ φ) =
DenSe(Jσb)* ◦ φ*.
First we compute the NNGP and NT kernels for y . To compute K note that from its definition,
y
Cy= Kφ(z) = Eθ [φ(z)i φ(z)T] = Eθ [Φ(Zi) Φ(Zi)T] = T(Cz).	(3)
Since φ does not introduce any new variables Θy can be computed as,
Tif ∖∖T"I	∖T
θy=Eθ[ *( *)]=Eθ X(Zi))看(磊)diag(φ(T=T(Cz) θ θz.
Taken together these equations imply that,
(Ky, Θy) = Φ*(Cz, Θz) ≡ (T(Kz), T(Cz)。Θz)	(4)
—
5T (Σ) ≡ E [φ(u)φ(u)T] , T(Σ) ≡ E [φ0(u)φ0(U)T] ,u 〜N (0, Σ), as in (Leeetal., 2019).
7
Published as a conference paper at ICLR 2020
will be the translation rule for a pointwise nonlinearity. Note that Equation Equation 4 only has an
analytic expression for a small set of activation functions φ.
Next we consider the case of a dense operation. Using the independence between the weights, the
biases, and h it follows that,
Kh = EW,β,θ [hi hiT] = σω2Eθ[yiyiT] + σb2 = σω2 Ky + σb2.	(5)
Finally, the NTK of h can be computed as a sum of two terms:
θh = EW,β,θ ∂(W⅛ (∂(W⅛)	+ EWMθ dh (d∂θi)	= σ2Ky+ σ2+ σωθy .⑹
This gives the translation rule for the dense layer in terms of Ky and Θy as,
(Kh, Θh) = Dense(σω,σfe)* (Ky, Θy) ≡ (σ2jKy + σ2, σ±Ky + σ2 + σ22Θy) .	(7)
3.2 Performance
Our library performs a number of automatic performance optimizations without sacrificing flexibility.
Leveraging block-diagonal covariance structure. A common computational challenge with GPs
is inverting the training set covariance matrix. Naively, for a classification task with C classes and
training set X, NNGP and NTK covariances have the shape of |X| C × |X| C. For CIFAR-10, this
would be 500, 000 × 500, 000. However, if a fully-connected readout layer is used (which is an
extremely common design in classification architectures), the C logits are i.i.d. conditioned on the
input x. This results in outputs that are normally distributed with a block-diagonal covariance matrix
of the form Σ 0 Ic, where Σ has shape |X| X |X| and IC is the C X C identity matrix. This reduces
the computational complexity and storage in many common cases by an order of magnitude, which
makes closed-form exact inference feasible in these cases.
Automatically tracking only the smallest necessary subset of intermediary covariance entries.
For most architectures, especially convolutional, the main computational burden lies in constructing
the covariance matrix (as opposed to inverting it). Specifically for a convolutional network of depth
l, constructing the |X | X |X | output covariance matrix, Σ, involves computing l intermediate layer
covariance matrices, Σl , of size |X | d X |X | d (see Listing 1 for a model requiring this computation)
where d is the total number of pixels in the intermediate layer outputs (e.g. d = 1024 in the case
of CIFAR-10 with SAME padding). However, as Xiao et al. (2018); Novak et al. (2019); Garriga-
Alonso et al. (2019) remarked, if no pooling is used in the network the output covariance Σ can be
computed by only using the stack of d |X| X |X |-blocks of Σl, bringing the time and memory cost
from O(|X |2 d2) down to O(|X|2d) per layer (see Figure 4 and Listing 2 for models admitting this
optimization). Finally, if the network has no convolutional layers, the cost further reduces to O(|X |2)
(see Listing 3 for an example). These choices are performed automatically by Neural Tangents
to achieve efficient computation and minimal memory footprint.
Expressing covariance computations as 2D convolutions with optimal layout. A key insight to
high performance in convolutional models is that the covariance propagation operator for convo-
lutional layers A can be expressed in terms of 2D convolutions when it operates on both the full
|X| d X |X| d covariance matrix Σ, and on the d diagonal |X| X |X |-blocks. This allows utilization
of modern hardware accelerators, many of which target 2D convolutions as their primary machine
learning application.
Simultaneous NNGP and NT kernel computations. As NTK computation requires the NNGP
covariance as an intermediary computation, the NNGP covariance is computed together with the
NTK at no extra cost. This is especially convenient for researchers looking to investigate similarities
and differences between these two infinite-width NN limits.
Automatic batching and parallelism across multiple devices. In most cases as the dataset or
model becomes large, it is impossible to perform the entire kernel computation at once. Additionally,
in many cases it is desirable to parallelize the kernel computation across devices (CPUs, GPUs, or
TPUs). Neural Tangents provides an easy way to perform both of these common tasks using a
single batch decorator shown below:
8
Published as a conference paper at ICLR 2020
Figure 5: Performance scaling with batch size (left) and number of GPUs (right). Shows time
per entry needed to compute the analytic NNGP and NTK covariance matrices (using kerneLfn )
in a 21-layer ReLU network with global average pooling. Left: Increasing the batch size when
computing the covariance matrix in blocks allows for a significant performance increase until a
certain threshold when all cores in a single GPU are saturated. Simpler models are expected to have
better scaling with batch size. Right: Time-per-sample scales linearly with the number of GPUs,
demonstrating near-perfect hardware utilization.
batched_kernel_fn = nt.batch(kernel_fn, batch_size)
batched_kernel_fn(x, x) == kernel_fn(x, x) # True!
This code works with either analytic kernels or empirical kernels. By default, it automatically shares
the computation over all available devices. We plot the performance as a function of batch size and
number of accelerators when computing the theoretical NTK of a 21-layer convolutional network in
Figure 5, observing near-perfect scaling with the number of accelerators.
Op fusion. JAX and XLA allow end-to-end compilation of the whole kernel computation and/or
inference. This enables the XLA compiler to fuse low-level ops into custom model-specific accelerator
kernels, as well as eliminating overhead from op-by-op dispatch to an accelerator. In similar vein,
we allow the covariance tensor to change its order of dimensions from layer to layer, with the order
tracked and parsed as additional metadata under the hood. This eliminates redundant transpositions6
by adjusting the computation performed by each layer based on the input metadata.
4 Conclusion
We believe Neural Tangents will enable researchers to quickly and easily explore infinite-width
networks. By democratizing this previously challenging model family, we hope that researchers
will begin to use infinite neural networks, in addition to their finite counterparts, when faced with a
new problem domain (especially in cases that are data-limited). In addition, we are excited to see
novel uses of infinite networks as theoretical tools to gain insight and clarity into many of the hard
theoretical problems in deep learning. Going forward, there are significant additions to Neural
Tangents that we are exploring. There are more layers we would like to add in the future (§D)
that will enable an even larger range of infinite network topologies. Additionally, there are further
performance improvements we would like to implement, to allow experimenting with larger models
and datasets. We invite the community to join our efforts by contributing new layers to the library
(§B.7), or by using it for research and providing feedback!
Acknowledgments
We thank Yasaman Bahri for frequent discussion and useful feedback on the manuscript. We
additionally appreciate both Yasaman Bahri and Greg Yang for the ongoing contributions to improve
the library. We thank Sergey Ioffe for feedback on the text, as well as Ravid Ziv, and Jeffrey
Pennington for discussion and feedback on early versions of the library.
6These transpositions could not be automatically fused by the XLA compliler.
9
Published as a conference paper at ICLR 2020
References
Mardn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh LeVenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from
tensorflow.org.
Takuya Akiba, Keisuke Fukuda, and Shuji Suzuki. ChainerMN: Scalable Distributed Deep Learning
Framework. In Proceedings of Workshop on ML Systems in The Thirty-first Annual Conference on
Neural Information Processing Systems (NIPS), 2017. URL http://learningsys.org/nips17/
assets/papers/paper_25.pdf.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, 2018.
Anonymous. Infinite attention: Nngp and ntk for deep attention networks. In International Conference
on Machine Learning (ICML), 2020. submission under review.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances In Neural Information Processing
Systems, 2019a.
Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu.
Harnessing the power of infinitely wide deep nets on small-data tasks, 2019b.
Yaniv Blumenfeld, Dar Gilboa, and Daniel Soudry. A mean field theory of quantized deep networks:
The quantization-depth trade-off. arXiv preprint arXiv:1906.00771, 2019.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy programs,
2018a. URL http://github.com/google/jax.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, and Skye Wanderman-Milne. Stax, a flexible neural net specification library in jax, 2018b.
URL https://github.com/google/jax/blob/master/jax/experimental/stax.py.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2019.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances In Neural
Information Processing Systems, 2009.
Frangois Chollet et al. Keras. https://keras.io, 2015.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems,pp. 2253-2261, 2016.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang,
and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with
graph kernels. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
10
Published as a conference paper at ICLR 2020
5724-5734. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
8809-graph-neural-tangent-kernel-fusing-graph-neural-networks-with-graph-kernels.
pdf.
Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances
in Neural Information Processing Systems, 2018.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional
networks as shallow gaussian processes. In International Conference on Learning Representations,
2019.
GPy. GPy: A gaussian process framework in python. http://github.com/SheffieldML/GPy,
2012.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activation
function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-field behaviour of neural tangent kernel
for deep neural networks, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, 2018.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in
deep neural networks: mean field approach. 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha
Sohl-dickstein. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, 2019.
Ping Li and Phan-Minh Nguyen. On random deep weight-tied autoencoders: Exact asymptotic
analysis, phase transitions, and implications to training. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HJx54i05tX.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-8166,
2018.
Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke. Fujii, Alexis Boukouvalas,
Pablo Le‘on-Villagr‘a, Zoubin Ghahramani, and James Hensman. GPflow: A Gaussian process
library using TensorFlow. Journal of Machine Learning Research, 18(40):1-6, apr 2017. URL
http://jmlr.org/papers/v18/16-537.html.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018a.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
Representations, 2018b.
Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto, 1994.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2019.
11
Published as a conference paper at ICLR 2020
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. In Advances In Neural Information
Processing Systems, 2016.
Arnu Pretorius, Elan van Biljon, Steve Kroon, and Herman Kamper. Critical initialisation for deep sig-
nal propagation in noisy rectifier neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems
31, pp. 5717-5726. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7814- critical- initialisation- for- deep- signal- propagation- in- noisy- rectifier- neural- networks.
pdf.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv preprint arXiv:1611.01232, 2016.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open
source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems
(LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems
(NIPS), 2015. URL http://learningsys.org/papers/LearningSys_2015_paper_33.pdf.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018.
Lechao Xiao, Jeffrey Pennington, and Samuel S Schoenholz. Disentangling trainability and general-
ization in deep learning. arXiv preprint arXiv:1912.13053, 2019.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
In Neural Information Processing Systems, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British
Machine Vision Conference (BMVC), 2016.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, Oct 2019. ISSN 1573-0565. doi:
10.1007/s10994-019-05839-6. URL https://doi.org/10.1007/s10994-019-05839-6.
Appendix
A Neural Tangents and prior work
Here we briefly discuss the differences between Neural Tangents and the relevant prior work.
1.	Prior benchmarks in the domain of infinitely wide neural networks. Various prior
works have evaluated convolutional and fully-connected models on certain datasets (Lee
et al., 2018; Matthews et al., 2018b;a; Novak et al., 2019; Garriga-Alonso et al., 2019;
Arora et al., 2019a). While these efforts must have required implementing certain parts
of our library, to our knowledge such prior efforts were either not open-sourced or not
comprehensive / user-friendly / scalable enough to be used as a user-facing library. In
addition, all of the works above used their own separate implementation, which further
highlights a need for a more general approach.
12
Published as a conference paper at ICLR 2020
2.	Code released by Lee et al. (2019). Lee et al. (2019) have released code along with
their paper submission, which is a strict and minor subset of our library. More specif-
ically, at the time of the submission, Lee et al. (2019) have released code equivalent
to nt.linearize , nt.empirical_ntk_fn , nt.predict.gradient_descent_mse ,
nt. Predict.gradient_descent , and nt. predict. momentum . Every other part of the
library (most notably, nt. Stax ) is new in this submission and was not used by Lee et al.
(2019) or any other prior work. At the time of writing, Neural Tangents differs from
the code released by Lee et al. (2019) by about +9, 500/ - 2, 500 lines of code.
3.	GPy (2012), GPFlow (Matthews et al., 2017), GPyTorch (Gardner et al., 2018), and
other GP packages. While various packages allowing for kernel construction, optimization,
and inference with Gaussian Processes exist, none of them allow easy construction of the
very specific kernels corresponding to infinite neural networks (NNGP/NTK; nt. Stax ),
nor do they provide the tools and convenience for studying wide but finite networks and their
training dynamics ( nt. taylor_expand , nt. predict , nt.monte_carlo_kernel_fn ).
On the other hand, Neural Tangents does not provide any tools for approximate
inference with these kernels.
B Library description
Neural Tangents provides a high-level interface for specifying analytic, infinite-width, Bayesian
and gradient descent trained neural networks as Gaussian Processes. This interface closely follows
the stax API (Bradbury et al., 2018b) in JAX.
B.1	Neural networks with JAX
stax represents each component of a network as two functions: init_fn and apply_fn . These
components can be composed in serial or in parallel to produce new network components with
their own init_fn and apply_fn . In this way, complicated neural network architectures can be
specified hierarchically.
Calling init_fn on a random seed and an input shape generates a random draw of trainable
parameters for a neural network. Calling apply_fn on these parameters and a batch of inputs
returns the outputs of the given finite neural network.
from jax.experimental import stax
init_fn, apply_fn = stax.serial(stax.Dense(512), stax.Relu, stax.Dense(10))
_, params = init_fn(key, (-1, 32 * 32 * 3))
fx_train, fx_test = apply_fn(Params, x_train), apply_fn(Params, x_test)
B.2	Infinite neural networks with Neural Tangents
We extend stax layers to return a third function kernel_fn , which represents the covariance
functions of the infinite NNGP and NTK networks of the given architecture (recall that since infinite
networks are GPs, they are fully defined by their covariance functions, assuming 0 mean as is common
in the literature).
from neural_tangents import stax
init_fn, apply_fn, kernel_fn = stax.serial(stax.Dense(512), stax.Relu(), stax.Dense(10))
We demonstrate a specification of a more complicated architecture (WideResNet) in Listing 1.
kernel_fn accepts two batches of inputs x1 and x2 and returns their NNGP covariance and
NTK matrices as kernel_fn(x1, x2) . nngp and kernel_fn(x1, x2) . ntk respectively, which
13
Published as a conference paper at ICLR 2020
can then be used to make posterior test set predictions as the mean of a conditional multivariate
normal Ytest = K (Xtest, Xtrain) K (Xrain, Xtrain)T Ytrai「
from jax.numpy.linalg import inv
y_test = kernel_fn(x_test, x_train).ntk @ inv(kernel_fn(x_train, x_train).ntk) @ y_train
Note that the above code does not do Cholesky decomposition and is presented merely to show the
mathematical expression. We provide efficient GP inference method in the predict submodule:
import neural_tangents as nt
y_test = nt.predict.gp_inference(kernel_fn, x_train, y_train, x_test,
get='ntk', diag_reg=1e-4, COmPUte_cov=FalSe)
B.3	Computing infinite network kernels in batches and in parallel
Naively, the kernel_fn will compute the whole kernel in a single call on one device. However,
for large datasets or complicated architectures, it is often necessary to distribute the calculation in
some way. To do this, we introduce a batch decorator that takes a kernel_fn and returns a new
kernel_fn with the exact same signature. The new function computes the kernel in batches and
automatically parallelizes the calculation over however many devices are available, with near-perfect
speedup scaling with the number of devices (Figure 5, right).
import neural_tangents as nt
kernel_fn = nt.batch(kernel_fn, batch_size=32)
Note that batching is often used to compute large covariance matrices that may not even fit on a
GPU/TPU device, and require to be stored and used for inference using CPU RAM. This is easy
to achieve by simply specifying nt.batch(..., store_on_deviCe=FaISe) . Once the matrix is
stored in RAM, inference will be performed with a CPU when nt. predict methods are called. As
mentioned in §3.2, for many (notably, convolutional, and especially pooling) architectures, inference
cost can be small relative to kernel construction, even when running on CPU (for example, it takes less
than 3 minutes to execute jax.scipy.linalg.solve(. . ., sym_pos=TrUe) on a 45,000 × 45, 000
training covariance matrix and a 45,000 X 10 training target matrix).
B.4	Training dynamics of infinite networks
In addition to closed form multivariate Gaussian posterior prediction, it is also interesting to consider
network predictions following continuous gradient descent. To facilitate this we provide several
functions to compute predictions following gradient descent with an MSE loss, for gradient descent
with arbitrary loss, or for momentum with arbitrary loss. The first case is handled analytically, while
the latter two are computed by numerically integrating the differential equation. For example, the
following code will compute the function evaluation on train and test points following gradient
descent for some time training_time .
import neural_tangents as nt
predictor = nt.predict.gradient_descent_mse(kernel_fn(x_train, x_train), y_train,
fx_train, fx_test = PrediCtor(training_time, fx_train, fx_test)
B.5	Infinite networks of any architecture through sampling
Of course, there are cases where the analytic kernel cannot be computed. To support these situations,
we provide utility functions to efficiently compute Monte Carlo estimates of the NNGP covariance
and NTK. These functions work with neural networks constructed using any neural network library.
14
Published as a conference paper at ICLR 2020
山 PaJEnbS UE①W IPE
---- Oth order (constant)
---- 1st order (linear)
---- 2nd order (quadratic)
---- Original function
Figure 6: Training a neural network and its various approximations using nt. taylor_expand .
Presented is a 5-layer Erf-neural network of width 512 trained on MNIST using SGD with mo-
mentum, along with its constant (0th order), linear (1st order), and quadratic (2nd order) Taylor
expansions about the initial parameters. As training progresses (left to right), lower-order expansions
deviate from the original function faster than higher-order ones.
from jax import random
from jax.experimental import stax
import neural_tangents as nt
init_fn, apply_fn = stax.serial(stax.Dense(64), stax.BatchNorm(), stax.Sigmoid, stax.Dense(1))
kernel_fn = nt.monte_carlo_kernel_fn(init_fn, apply_fn, key=random.PRNGKey(1), n_samples=128)
kernel = kernel_fn(x_train, x_train)
We demonstrate convergence of the Monte Carlo kernel estimates to the closed-form analytic kernels
in the case of a WideResNet in Figure 2.
B.6	Weights of wide but finite networks
While most of Neural Tangents is devoted to a function-space perspective—describing the
distribution of function values on finite collections of training and testing points—we also provide
tools to investigate a dual weight space perspective described in Lee et al. (2019). Convergence
of dynamics to NTK dynamics coincide with networks being described by a linear approximation
about their initial set of parameters. We provide decorators linearize and taylor_expand to
approximate functions to linear order and to arbitrary order respectively. Both functions take an
apply_fn and returns a new apply_fn that computes the series approximation.
import neural_tangents as nt
taylor_apply_fn = nt.taylor_expand(apply_fn, params, order)
fx_train_approx = taylor_apply_fn(new_params, x_train)
These act exactly like normal JAX functions and, in particular, can be plugged into gradient descent,
which we demonstrate in Figure 6.
B.7	Extending Neural Tangents
Many neural network layers admit a sensible infinite-width limit behavior in the Bayesian and
continuous gradient descent regimes as long as the multivariate central limit theorem applies to their
outputs conditioned on their inputs. To add such layer to Neural Tangents, one only has to
implement it as a method in nt.stax with the following signature:
15
Published as a conference paper at ICLR 2020
@_layer # an internal decorator taking care of certain boilerplate.
NewLayer(layer_params： Any) -> (init_fn： function, apply_fn： function, kernel_fn： function)
Here init_fn and apply_fn are initialization and the forward pass methods of the finite
width layer implementation (see §B.1). If the layer of interest already exists in JAX, there
is no need to implement these methods and the user can simply return the respective methods
from jax.experimental. stax (see nt.stax.Flatten for an example; in fact the majority of
nt.stax layers call the original jax.experimental. stax layers for finite width layer methods).
In this case what remains is to implement the kernel_fn method with signature
kernel_fn(input_kernel: nt.utils.Kernel) -> output_kernel: nt.utils.Kernel
Here both input_kernel and output_kernel are namedtuple s containing the NNGP and NTK
covariance matrices, as well as additional metadata useful for computing the kernel propagation
operation. The specific operation to be performed should be derived by the user in the context of the
particular operation that the finite width layer performs. This transformation could be as simple as an
affine map on the kernel matrices, but could also be analytically intractable.
Once implemented, the correctness of the implementation can be very easily tested by extending the
nt. tests. stax_test with the new layer, to test the agreement with large-widths empirical NNGP
and NTK kernels.
C Architecture specifications
from neural_tangents import stax		
def ConvolutionalNetwork(depth, W_std=1.0, layers =[] for _ in range(depth):	b_std=0.0):	
layers += [stax.Conv(1, (3, 3), W_std,	b_std,	Padding='SAME'), stax.Relu()]
layers += [stax.Flatten(), stax.Dense(1, return stax.serial(*layers)	W_std,	b_std)]
Listing 2: All-convolutional model (ConvOnly) definition used in Figure 3.
from neural_tangents import stax
def FullyConnectedNetwork(depth, W_std=1.0, b_std=0.0):
layers = [stax.Flatten()]
for _ in range(depth):
layers += [stax.Dense(1, W_std, b_std), stax.Relu()]
layers += [stax.Dense(1, W_std, b_std)]
return stax.serial(*layers)
Listing 3: Fully-connected (FC) model definition used in Figure 3.
16
Published as a conference paper at ICLR 2020
D Implemented and coming soon functionality
The following layers7 are currently implemented, with translation rules given in Table 1:
•	serial
•	parallel
•	FanOUt
•	FanInSum
•	FanInConcat
•	Dense
•	Conv with arbitrary filter shapes, strides, dimension numbers, and padding8
•	RelU
•	LeakyRelU
•	Abs
•	ABRelu 9
•	Erf
•	Identity
•	Flatten
•	AvgPool
•	GlobalAvgPool
•	SUmPool
•	GlobalSumPool
•	Dropout
•	LayerNorm
•	GlobalSelfAttention (Anonymous, 2020)
The following is in our near-term plans:
•	Exp , Elu , Selu , Gelu
•	Apache Beam support.
The following layers do not have known closed-form expressions for infinite network covariances, and
respective infinite networks have to be estimated empirically (via nt. monte_carlo_kernel_fn ) or
using other approximations (not currently implemented):
•	Sigmoid , Tanh,10 Swish ,11 Softmax , LogSoftMax, Softplus , MaxPool .
7 Abs , ABRelu , Erf , GlobalAvgPool , GlobalSumPool , LayerNorm , and GlobalSelfAttention
are only available in our library nt.stax and not in jax. experimental. stax .
8In addition to SAME and VALID , we support CIRCULAR padding, which is especially convenient for
theoretical analysis and was used by Xiao et al. (2018) and Novak et al. (2019).
9 ABRelu is a more flexible extension of LeakyRelu , computed as a min (x, 0) + b max (x, 0).
10 Sigmoid and Tanh are similar to Erf which does have a covariance expression and is implemented.
11 Swish is similar to Gelu .
17
Published as a conference paper at ICLR 2020
Tensor Op	NNGP Op	NTK Op
X	K	Θ
Dense(σw, σb)	σw2 K + σb2	(σw2 K + σb2) +σw2 Θ
φ	T(K)	T(K) Θ Θ
Dropout(ρ)	K +(1 - 1)Diag(K)	Θ+ (P - 1)Diag(Θ)
Conv(σw , σb)	σw2 A (K) +σb2	σw2 A (K) + σb2 +σw2A(Θ)
Flatten	Tr(K)	Tr(K + Θ)
AvgPool(s, q, p)	AvgPool(s, q, p)(K)	AvgPool(s, q, p)(K + Θ)
GlobalAvgPool	GlobalAvgPool(K)	GlobalAvgPool(K + Θ)
SumPool(s, q, p)	SumPool(s, q, p)(K)	SumPool(s, q, p)(K + Θ)
GlobalSumPool	GlobalSumPool(K)	GlobalSumPool(K + Θ)
Attn(σQK, σOV ) (Anonymous, 2020)	Attn(σQK, σOV )(K)	2Attn(σQK, σOV )(K)+ Attn(σQK, σOV )(Θ)
FanInSum(X1, . . . , Xn)	n j=1Kj	Pjn=1 Θj
FanOut(n)	[K] * n	[Θ] * n
Table 1: Translation rules (§3) converting tensor operations into operations on NNGP and
NTK kernels. Here the input tensor X is assumed to have shape |X | × H × W × C (dataset size,
height, width, number of channels), and the full NNGP and NT kernels K and T are considered to
be of shape (|X| × H X W)×2 (in practice shapes of ∣X∣×2 X H X W and ∣X∣×2 are also possible,
depending on which optimizations in §3.2 are applicable). Notation details. The Tr, GlobalAvgPool,
and GlobalSumPool ops are assumed to act on all spatial axes (with sizes H and W in this example),
producing a ∣X∣×2-kernel. Similarly, the AvgPool and SumPool ops is assumed to act on all spatial
axes as well, applying the specified strides s, pooling window sizes p and padding strategy p to
the respective axes pairs in K and T (acting as 4D pooling with replicated parameters of the 2D
version). T and T are defined identically to Lee et al. (2019) as T (Σ) = E [φ(u)φ(u)T] , T (Σ)=
E [φ0(u)φ0(u)T] , u 〜 N (0, Σ). These expressions can be evaluated in closed form for many
nonlinearities, and preserve the shape of the kernel. The A op is defined similarly to Novak et al.
(2019); Xiao et al. (2018) as [A (Σ)]hw,,hw00 (x, x0) = Pdh,dw [Σ]hw++ddhw,h,w0+0+ddhw (x, x0) /q2, where the
summation is performed over the convolutional filter receptive field with q pixels (we assume unit
strides and circular padding in this expression, but generalization to other settings is trivial and
supported by the library). [Σ] * n = [Σ,..., Σ] (n-fold replication). For LayerNorm, FanInConcat,
and Attn (Anonymous, 2020) translation rules we refer the reader to our code at https://github.
com/google/neural-tangents, as these ops are challenging to express concisely using current
notation. See Figure 4 for an example of applying the translation rules to a specific model, and §3.1
for deriving a sample translation rule. See §D for the full list of currently implemented translations.
18
Published as a conference paper at ICLR 2020
3.0-1
2.5-
3.0-1
Fμll Test; Coyaηanςe
4 2.0-
N
S
H
1.5-
lpdepenldeηt l l
-FC-NTK
27 2® 2® 2io 2ɪɪ 2ɪ2 2ɪɜ 22ɪʒ
Training Set Size
,,,NTK1 l l c
2.5-
2 7 2 8 2 9 2 ɪɑ 2 ɪɪ 212 213 2 2 ɪʒ
Training Set Size
..NNG? .	. l
6
-I 2
.0
L
6
-2
.0
L
9
1
8
1
1 1
J≡二二--------
JBqEnN uo≡puo□
10
1
1 1
6 5 4
Ooo
6
--....-12
3 2
O O
1 1
Training Set Size
7 6 5 4 3 2
Oooooo
Illlll
JBqEnN uo⅛puo□
—■ Conv-Test-Cov _
WResNet	≡
-- WResNet-Test-Cov 二
2 7 28 29 2 io 2 ɪɪ 212 2 ɪɜ
Training Set Size
Figure 7: Predictive negative log-likelihoods and condition numbers. Top. Test negative log-
likelihoods for NNGP posterior and Gaussian predictive distribution for NTK at infinite training
time for CIFAR-10 (test set of 2000 points). Fully Connected (FC, Listing 3) and Convolutional
network without pooling (CONV, Listing 2) models are selected based on train marginal negative
log-likelihoods in Figure 3. Bottom. Condition numbers for covariance matrices corresponding to
NTK/NNGP as well as respective predictive covaraince on the test set. Ill-conditioning of Wide
Residual Network kernels due to pooling layers (Xiao et al., 2019) could be the cause of numerical
issues when evaluating predictive NLL for this kernels.
19