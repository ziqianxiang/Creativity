Published as a conference paper at ICLR 2020
Understanding Why Neural Networks Gener-
alize Well Through GSNR of Parameters
Jinlong Liu1, Guo-qing Jiang1, YUnzhi Bai1, Ting Chen2, and HUayan Wang1
IYtech - KWAI incorporation
{liujinlong,jiangguoqing,baiyunzhi,wanghuayan}@kuaishou.com
2Samsung Research China - Beijing (SRC-B)
ting11.chen@samsung.com
Ab stract
As deep neural networks (DNNs) achieve tremendous success across many appli-
cation domains, researchers tried to explore in many aspects on why they gener-
alize well. In this paper, we provide a novel perspective on these issues using
the gradient signal to noise ratio (GSNR) of parameters during training process
of DNNs. The GSNR of a parameter is defined as the ratio between its gradi-
ent’s squared mean and variance, over the data distribution. Based on several ap-
proximations, we establish a quantitative relationship between model parameters’
GSNR and the generalization gap. This relationship indicates that larger GSNR
during training process leads to better generalization performance. Moreover, we
show that, different from that of shallow models (e.g. logistic regression, support
vector machines), the gradient descent optimization dynamics of DNNs naturally
produces large GSNR during training, which is probably the key to DNNs’ re-
markable generalization ability.
1	Introduction
Deep neural networks typically contain far more trainable parameters than training samples, which
seems to easily cause a poor generalization performance. However, in fact they usually exhibit
remarkably small generalization gaps. Traditional generalization theories such as VC dimension
(Vapnik & Chervonenkis, 1991) or Rademacher complexity (Bartlett & Mendelson, 2002) cannot
explain its mechanism. Extensive research focuses on the generalization ability of DNNs (Neyshabur
et al., 2017; Arora et al., 2018; Keskar et al., 2016; Dinh et al., 2017; Hoffer et al., 2017; Novak
et al., 2018; Dziugaite & Roy, 2017; Jakubovitz et al., 2019; Kawaguchi et al., 2017; Advani &
Saxe, 2017).
Unlike that of shallow models such as logistic regression or support vector machines, the global
minimum of high-dimensional and non-convex DNNs cannot be found analytically, but can only
be approximated by gradient descent and its variants (Zeiler, 2012; Kingma & Ba, 2014; Graves,
2013). Previous work (Zhang et al., 2016; Hardt et al., 2015; Dziugaite & Roy, 2017) suggests that
the generalization ability of DNNs is closely related to gradient descent optimization. For example,
Hardt et al. (2015) claims that any model trained with stochastic gradient descent (SGD) for reason-
able epochs would exhibit small generalization error. Their analysis is based on the smoothness of
loss function. In this work, we attempt to understand the generalization behavior of DNNs through
GSNR and reveal how GSNR affects the training dynamics of gradient descent. Stanislav Fort
(2019) studied a new gradient alignment measure called stiffness in order to understand generaliza-
tion better and stiffness is related to our work.
The GSNR of a parameter is defined as the ratio between its gradient’s squared mean and variance
over the data distribution. Previous work tried to use GSNR to conduct theoretical analysis on
deep learning. For example, Rainforth et al. (2018) used GSNR to analyze variational bounds in
* corresponding author
1
Published as a conference paper at ICLR 2020
unsupervised DNNs such as variational auto-encoder (VAE). Here we focus on analyzing the relation
between GSNR and the generalization gap.
Intuitively, GSNR measures the similarity of a parameter’s gradients among different training sam-
ples. Large GSNR implies that most training samples agree on the optimization direction of this
parameter, thus the parameter is more likely to be associated with a meaningful “pattern” and we
assume its update could lead to a better generalization. In this work, we prove that the GSNR is
strongly related to the generalization performance, and larger GSNR means a better generalization.
To reveal the mechanism of DNNs’ good generalization ability, we show that the gradient descent
optimization dynamics of DNN naturally leads to large GSNR of model parameters and therefore
good generalization. Furthermore, we give a complete analysis and a detailed interpretation to this
phenomenon. We believe this is probably the key to DNNs remarkable generalization ability.
In the remainder of this paper we first analyze the relation between GSNR and generalization (Sec-
tion 2). We then show how the training dynamics lead to large GSNR of model parameters experi-
mentally and analytically in Section 3.
2	Larger GSNR Leads to B etter Generalization
In this section, we establish a quantitative relation between the GSNR of model parameters and
generalization gap, showing that larger GSNR during training leads to better generalization.
2.1	Gradients Signal to Noise Ratio
Consider a data distribution Z = X × Y, from which each sample (x, y) is drawn; a model y =
f(x, θ) parameterized by θ; and a loss function L.
The parameters’ gradient w.r.t. L and sample (xi, yi) is denoted by
z Q、 小	∂L (yi,f( Xi,θ))
g(xi,yi,θ) or gi(θ):= --------∂θ-------	(1)
whose j -th element is gi(θj). Note that throughout this paper we always use i to index data examples
and j to index model parameters.
Given the data distribution Z, we have the (sample-wise) mean and variance of gi (θ). We denote
them as g(θ) = E(χ,y)〜Z(g(x,y,θ)) and P2(θ) = Var(χ,y)〜Z(g(x,y,θ)), respectively.
The gradient signal to noise ratio (GSNR) of one model parameter θj is defined as:
r(θj)
g1 2( θj)
P 2( θj)
(2)
At a particular point of the parameter space, GSNR measures the consistency of a parameter’s gra-
dients across different data samples. Figure 1 intuitively shows that if GSNR is large, the parameter
gradient space tends to be distributed in the similar direction and if GSNR is small, the gradient
vectors are then scatteredly distributed.
2.2	One-Step Generalization Ratio
In this section we introduce a new concept to help measure the generalization performance dur-
ing gradient descent optimization, which we call one-step generalization ratio (OSGR). Consider
training set D = {(xι,y 1),…，(Xn,yn)} 〜 Zn with n samples drawn from Z, and a test set
D0 = {(x01,y10), ..., (x0n0,yn00)} 〜 Zn . In practice We use the loss on D0 to measure generalization.
For simplicity, we assume the sizes of training and test datasets are equal, i.e. n = n0 . We denote
the empirical training and test loss as:
1n	1n
L [ D ] = - X L (yi,f( χi,θ)),	L [ D ] = - X L (yi,f( χi ,θ)),	⑶
n i=1	n i=1
respectively. Then the empirical generalization gap is given by L[D0] - L[D].
2
Published as a conference paper at ICLR 2020
个 Smaller GSNR
乂
---Gradient vector of single sample
— Gradient mean	、
Gradient (θi)
Figure 1: Schematic diagram of the sample-wise parameter gradient distribution corresponding to
greater (Left) and smaller (Right) GSNR. Pink arrows denote the gradient vectors for each sample
while the blue arrow indicates their mean.
OSGR(t) = O	0< OSGR(t) < 1	OSGR(t) ≈ 1
Figure 2: Schematic diagram of the training behavior satisfies OSGR(t) = 0 (Left), 0 <
OSGR(t) < 1 (Middle) and OSGR(t) ≈ 1 (Right). Note that the Middle scenario most com-
monly happens in regular tasks.
In gradient descent optimization, both the training and test loss would decrease step by step. We
use ∆L[D] and ∆L[D0] to denote the one-step training and test loss decrease during training, re-
spectively. Let’s consider the ratio between the expectations of ∆L[D0] and ∆L[D] of one single
training step, which we denote as R(Z, n).
R(Z, n) :=
ED2D0^znA∆L∖D^
ED 〜Z n(∆ L [ D])
(4)
Note that this ratio also depends on current model parameters θ and learning rate λ. We are not
including them in the above notation as we will not explicitly model these dependencies, but rather
try to quantitatively characterize R for very small λ and for θ at the early stage of training (satisfying
Assumption 2.3.1).
Also note that the expectation of ∆L[D 0] is over D and D 0. This is because the optimization step is
performed on D . We refer to R(Z, n) as OSGR of gradient descent optimization. Statistically the
training loss decreases faster than the test loss and 0 < OSGR(t) < 1 (Middle panel of Figure 2),
which usually results in a non-zero generalization gap at the end of training. If OSGR(t) is large
(≈ 1) in the whole training process (Right panel of Figure 2), generalization gap would be small
when training completes, implying good generalization ability of the model. If OSGR(t) is small
(= 0), the test loss will not decrease while the training loss normally drops (Left panel of Figure 2),
corresponding to a large generalization gap.
2.3 Relation between GSNR and OSGR
In this section, we derive a relation between the OSGR during training and the GSNR of model pa-
rameters. This relation indicates that, for the first time as far as we know, the sample-wise gradient
distribution of parameters is related to the generalization performance of gradient descent optimiza-
tion.
3
Published as a conference paper at ICLR 2020
In gradient descent optimization, we take the average gradient over training set D, which we denote
as gD(θ). Note that We have used gi(θ) to denote gradient evaluated on one data sample and g(θ) to
denote its expectation over the entire data distribution. Similarly we define gD0 (θ) to be the average
gradient over test set D0 .
gD(θ) = 1 Xg(Xi,yi,θ) = dL∂D , gD0(θ) = 1 Xg(X0,yi,θ) = dLD	⑸
n	∂θ	n	∂θ
i=1	i=1
Both the training and test dataset are randomly generated from the same distribution Zn, so We
can treat gD(θ) and gD0 (θ) as random variables. At the beginning of the optimization process, θ
is randomly initialized thus independent ofD, so gD (θ) and gD0 (θ) Would obey the same distribu-
tion. After a period of training, the model parameters begin to fit the training dataset and become
a function ofD, i.e. θ = θ(D), therefore distributions of gD(θ(D)) and gD0 (θ(D)) become differ-
ent. HoWever We choose not to model this dependency and make the folloWing assumption for our
analysis:
Assumption 2.3.1 (Non-overfitting limit approximation) The average gradient over the training
dataset and test dataset gD (θ) and gD0 (θ) obey the same distribution.
Obviously the mean of gD (θ) and gD (θ) is just the mean gradient over the data distribution g(θ).
ED^Zn [gD (θ)] = Ed,d~zn [gD0 (θ)] = g(θ)	(6)
We denote their variance as σ2 (θ), i.e.
VarD〜Zn [gD(θ)] = Va^rDD0〜Zn [gdo(θ)] = σ2(θ)	(7)
It is straightforWard to shoW that:
1n	1
σ 2( θ) = Var D〜Z n [— Eg i (θ)] = -f( 2( θ)	(8)
nn
i=1
Where σ2 (θ) is the variance of the average gradient over the dataset of size n, and ρ2(θ) is the
variance of the gradient of a single data sample.
In one gradient descent step, the model parameter is updated by ∆θ = θt+1 - θt = -λgD(θ)
Where λ is the learning rate. If λ is small enough, the one-step training and test loss decrease can be
approximated by
δ L[ D] ≈	- δ θ	∙	—J	] +	O ( λ 2) = λ g D ( θ )	∙	g D ( θ )	+ O ( λ 2)	(9)
∂θ
∆L[D] ≈  ∆θ ∙ A?, ] + O(λ2) = λgD(θ) ∙ gD0(θ) + O(λ2)	(10)
∂θ
Usually there are some differences betWeen the directions of gD (θ) and gD0 (θ), so statistically
∆L[D] tends to be larger than ∆L[D0] and the generalization gap Would increase during training.
When λ → 0, in one single training step the empirical generalization gap increases by ∆L[D] -
∆L[D0], for simplicity We denote this quantity as 5:
▽ :=∆L[D] - ∆L[D] ≈ λgD(θ) ∙ gD(θ) - λgD(θ) ∙ gdo (θ)	(11)
=λ (g( θ) + E )(g( θ)+ — g( θ …)	(12)
=λ (g( θ) + E)(1)	(13)
Here we replaced the random variables by gD (θ) = g(θ) + E and gd,(θ) = g(θ) + E, where E and
E0 are random variables With zero mean and variance σ2(θ). Since E(E0) = E(E) = 0, E and E0 are
independent, the expectation of ▽ is
Ed,do 〜Zn (▽) = E(λE ∙ E) + O(λ2) = λXσ2(θj) + O(λ2)	(14)
j
where σ2(θj) is the variance the of average gradient of the parameter θj.
4
Published as a conference paper at ICLR 2020
For simplicity, when it involves a single model parameter θj , we will use only a subscript j instead
of the full notation. For example, we use σj2, rj, and gD,j to denote σ2(θj), r(θj), and gD (θj)
respectively.
Consider the expectation of ∆L[D] and ∆L[D0] when λ → 0
ED~Zn。L [ D ]) ≈ λED~Zn (g D ( θ ) ∙ g D ( θ )) = λ X ED~Zn (gD,j )	(15)
j
ED,D0~zn (∆L[D!]) = ED,D0~zn (∆L[D] - 5)	(16)
≈ λ X(Ed~Z" (g2D,j) - σ2)	(17)
j
=λ X(eD3 (g2D,j) - ρj/n)	(18)
j
Substituting (18) and (15) into (4) we have:
R(Z, n) = 1 — L FPP ( 2、	(19)
n j ED~Zn (gD ,j )
Although we derived eq. (19) from simplified assumptions, we can empirically verify it by estimating
two sides of the equation on real data. We will elaborate on this estimation method in section 2.4.
We can rewrite eq. (19) as:
R(Z,n)
1	ED^Zn (gD,j)	ρj
n 彳 P j0 ED~Zn (gD,j0) ED~Zn (g2D,j )
1 '^X ED~Zn (gDj)	1
n j P j0 ED~Zn (gD,j0) rj + n
(20)
(21)
where Ed~Z" (gDj) = VarD~zn (gD,j) + ED~z~ (gD,j) = 1 ρj + g j
We define ∆Lj [D] to be the training loss decrease caused by updating θj . We can show that when
λ is very small ∆Lj [D] = λgD2 ,j + O(λ2). Therefore when λ → 0, we have
R(Z,n) =	1 - 1 X Wj -^τ,	where Wj	:= ED~Z n'、Lj [D])	With X Wj	= 1	(22)
n j 3 rj + #	j Ed~zn(∆L[D]) j	'，
Eq. (22) shows that the GSNR rj plays a crucial role in the model’s generalization ability—the one-
step generalization ratio in gradient descent equals one minus the weighted average of +1 over
rj十n
all model parameters divided by n. The weight is proportional to the expectation of the training loss
decrease resulted from updating that parameter. This implies that larger GSNR of model parameters
during training leads to smaller generalization gap growth thus better generalization performance of
the trained model. Also note when n → ∞, we have R(Z, n) → 1, meaning that training on more
data helps generalization.
2.4 Experimental verification of the relation between GSNR and OSGR
The relation between GSNR and OSGR, i.e. eq. (19) or (22) can be empirically verified using any
dataset if: (1) The dataset includes enough samples to construct many training sets and a large
enough test set so that we can reliably estimate ρj, Ed^zn (gD j∙) and OSGR. (2) The learning rate
is small enough. (3) In the early training stage of gradient descent.
To empirically verify eq. (19), we show how to estimate its left and right hand sides, i.e. OSGR by
definition and OSGR as a function of GSNR. Suppose we have M training sets each with size n,
and a test set of size n0 . We initialize a model and train it separately on the M training sets and test
it with the same test set. For the t-th training iteration, we denote the training loss and test loss of
the model trained on the m-th training dataset as Lt(m) and L0t(m), respectively. Then the left hand
5
Published as a conference paper at ICLR 2020
(UOQ≡J3P Aq H9S0) 61 .b3 Jo SHI
Figure 3: Left hand (LHS or OSGR by definition) and right side (RHS or OSGR as a function of
GSNR) of eq. (19). Points are drawn under different experiment settings. Left: LHS vs RHS at
epoch 20, 100, 500, 2500. Each point is drawn by LHS and RHS computed at the given epoch under
different model structure (number of channels) or training data size; red dotted line is the line of
best fit computed by least squares; blue dotted line is the line of reference representing LHS = RHS;
the value of c in each title represents the Pearson correlation coefficient between LHS and RHS
computed by points in figure. Right: The legend. Different symbols and colors stand for different
number of channels and training data size. Different random noise levels are not distinguished.
side, i.e. OSGR by definition, of the t-th iteration can be estimated by
Rt(Z, n) ≈
M	0(m)	0(m)
乙 m =1 L t +1 — L t
PM ( (m) (mm)
m=1 Lt+1 - Lt
(23)
For the model trained on the m-th training set, we can compute the t-th step average gradient and
sample-wise gradient variance of θj on the corresponding training set, denoted as gm,j,t and ρ2m,j,t,
respectively. Therefore the right hand side of eq. (19) can be estimated by
2	1M2
ED〜Zn (gD,j,t) ≈ M Σ ggm,j,t,
m=1
ρj2,t ≈
1M
1 X 2
M 乙 ρm,j,t
m=1
(24)
We performed the above estimations on MNIST with a simple CNN structure consists of 2
Conv-Relu-MaxPooling blocks and 2 fully-connected layers. First, to estimate eq. (24) with
M = 10, we randomly sample 10 training sets with size n and a test set with size 10,000.
To cover different conditions, we (1) choose n ∈ {1000, 2000, 4000, 6000, 8000, 10000, 15000},
respectively; (2) inject noise by randomly changing the labels with probability prandom ∈
{0.0, 0.1, 0.2, 0.3, 0.5}; (3) change the model structure by varying number of channels in the lay-
ers, ch ∈ {6, 8, 10, 12, 14, 16, 18, 20}. See Appendix A for more details of the setup. We use the
gradient descent training (not SGD), with a small learning rate of 0.001. The left and right hand sides
of 19 at different epochs are shown in Figure 3, where each point represents one specific choice of
the above settings.
At the beginning of training, the data points are closely distributed along the dashed line correspond-
ing to LHS=RHS. This shows that eq. (19) fits quite well under a variety of different settings. As
training proceeds, the points become more scattered as the non-overfitting limit approximation no
longer holds, but correlation between the LHS and RHS remains high even when the training con-
verges (at epoch 2,500). We also conducted the same experiment on CIFAR10 A.2 and a toy dataset
A.3 observed the same behavior. See Appendix for these experiments.
The empirical evidence together with our previous derivation of eq. (19) clearly show the relation
between GSNR and OSGR and its implication in the model’s generalization ability. 3
3	Training dynamics of DNNs naturally leads to large GSNR
In this section, we analyze and explain one interesting phenomenon: the parameters’ GSNR of
DNNs rises in the early stages of training, whereas the GSNR of shallow models such as logistic
regression or support vector machines declines during the entire training process. This difference
gives rise to GSNR’s large practical values during training, which in turn is associated with good
6
Published as a conference paper at ICLR 2020
generalization. We analyze the dynamics behind this phenomenon both experimentally and theoret-
ically.
3.1	GSNR behavior of DNNs training
For shallow models, the GSNR of parameters decreases in the whole training process because gra-
dients become small as learning converges. But for DNNs it is not the case. We trained DNNs
on the CIFAR datasets and computed the GSNR averaged over all model parameters. Because
ED〜Zn (gD,j) = nρj + gj and We assume n is large, EDzZn (gD,j) ≈ gj. In the case of only one
large training datasets, we estimate GSNR of t-th iteration by
rj,t ≈ gDj,j,t /ρjD,j,t	(25)
As shoWn in Figure 4, the GSNR starts out loW With randomly initialized parameters. As learning
progresses, the GSNR increases in the early training stage and stays at a high level in the Whole
learning process. For each model parameter, We also computed the proportion of the samples With
the same gradient sign, denoted as psame_sign . In Figure 4c, We plot the mean of time series of this
proportion for all the parameters. This value increases from about 50% (half positive half negetive
due to random initialization) to about 56% finally, Which indicates that for most parameters, the
gradient signs on different samples become more consistent. This is because meaningful features
begin to emerge in the learning process and the gradients of the Weights on these features tend to
have the same sign among different samples.
Previous research (Zhang et al., 2016) shoWed that DNNs achieved zero training loss by memorizing
training samples even if the labels Were randomized. We also plot the average GSNR for model
trained using data With randomized labels in Figure 4 and find that the GSNR stays at a loW level
throughout the training process. Although the training loss of both the original and randomized
labels go to zero (not shoWn), the GSNR curves clearly distinguish betWeen these tWo cases and
reveal the lack of meaningful patterns in the latter one. We believe this is the reason Why DNNs
trained on real and random data lead to completely different generalization behaviors.
EPOCh	Epoch	Epoch
Figure 4: (a): GSNR curves generated by a simple netWork based on real and random data. An
obvious upWard process in the early training stage Was observed for real data only. (b): Same plot
for ResNet18. (c): Average of psame_sign for the same model as in (a).
3.2	Training Dynamics behind the GSNR behavior
In this section We shoW that the feature learning ability of DNNs is the key reason Why the GSNR
curve behavior of DNNs is different from that of shalloW models during the gradient descent training.
To demonstrate this, a simple tWo-layer perceptron regression model is constructed. A synthetic
dataset is generated as folloWing. Each data point is constructed i.i.d. using y = x0x1 + , Where
x0 and x1 are draWn from uniform distribution [-1, 1] and is draWn from uniform distribution
[-0.01, 0.01]. The training set and test set sizes are 200 and 10,000, respectively. We use a very
simple tWo-layer MLP structure With 2 inputs, 20 hidden neurons and 1 output.
We randomly initialized the model parameters and trained the model on the synthetic training dataset.
As a control setup We also tried to freeze model Weights in the first layer to prevent it from learning
features. Note that a tWo layer MLP With the first layer frozen is equivalent to a linear regression
model. That is, regression Weights are learned on the second layer using fixed features extracted by
the first layer. We plot the average GSNR of the second layer parameters for both the frozen and
non-frozen cases. Figure 5 shoWs that in the non-frozen case, the average GSNR over parameters of
7
Published as a conference paper at ICLR 2020
Figure 5: Average GSNR (a) and loss (b) curves for the frozen and non-frozen case. (c): GSNR
curves of individual parameters for the non-frozen case.
the second layer shows a significant upward process, whereas in the frozen case the average GSNR
decreases in the beginning and remains at a low level during the whole training process.
In the non-frozen case, GSNR curve of individual parameters of the second layer are shown in Figure
5. The GSNR for some parameters show a significant upward process. To measure the quality of
these features, we computed the Pearson correlation between them and the target output y, both at
the beginning of training and at the maximum point of their GSNR curves. We can see that the
learning process learns “good” features (high correlation value, i.e. with stronger correlation with y)
from random initialized ones, as shown in Table 1. This shows that the GSNR increasing process is
related to feature learning.
3.3	Analysis of training dynamics behind DNNs’ GSNR behavior
In this section, we will investigate the training dynamics behind the GSNR curve behavior. In the
case of fully connected network structure, we can analytically show that the numerator of GSNR,
i.e. the squared gradient mean of model parameters, tends to increase in the early training stage
through feature learning.
Consider a fully connected network, whose parameters are θ = {W(1) , b(1) , ..., W(lmax) , b(lmax)},
where W(1) , b(1) are the weight matrix and bias of the first layer, and so on. We denote the ac-
tivations of the l-th layer as a(l) = {a(sl) (θ(l-))}, where s is the index for nodes/channels of
this layer, and θ(l-) is the collection of model parameters in the layers before l, i.e. θ(l-) =
{W(1) , b(1) , ..., W(l-1) , b(l-1)}. In the forward pass on data sample i, {als(θ(l-))} is multiplied
by the weight matrix W(l) :
oi(,lc) = XWc(,ls)ai(,ls)(θ(l-))	(26)
s
where o(l) = {oi(,lc)} is the output of the matrix multiplication, for the i-th data sample, on the l-th
layer, c = {1, 2, ..., C} is the index of nodes/channels in the (l + 1)-th layer. We use gD(l) to denote
the average gradient of weights of the l-th layer W(I), i.e. gD) = 1 P乙 ∂Wil), where Li is the
loss of the i-th sample.
Here we show that the feature learning ability of DNNs plays a crucial role in the GSNR increasing
process. More precisely, we show that the learning of features a(l) (θ(l-)), i.e. the learning of pa-
rameters θ(l-) tends to increase the absolute value of gD(l). Consider the one-step change of gradient
mean ∆gD(l) = gD(l),t+1 - gD(l),t with the learning rate λ → 0. In one training step, θ is updated by
∆θ = θt+1 - θt = -λgD (θ). Using linear approximation with λ → 0, we have
∆gDl,s.c≈Xdg∂θsc∆θj= X *∆θj+ X	*∆θj
j	jj
j	θj ∈θ (l-)	θj ∈θ(l+)
(27)
where θ(l-) and θ(l+) denote model parameters before and after the l-the layer (including the l-th),
respectively.
8
Published as a conference paper at ICLR 2020
We focus on the first term of eq. (27), i.e. the one-step change of gD(l) caused by learning θ(l-) .
Substituting gD) = n Pn=1 dwLii) and δθj = (-λn Pn=1 dj) into eq.(27), we have
△gD)s,cc=-n X W(lc((X * daθr)2+other terms	(28)
n θj∈θ(l-)	i=1 ∂oi,c ∂θj
The detailed derivation of eq. (28) can be found in Appendix B. We can see the first term (which is a
summation over parameters in θ(l-)) in eq. (28) has opposite sign with Ws(l, )c. This term will make
∆gD(l),s,c negatively correlated with Ws(l, )c . We plot the correlation between ∆gD(l),s,c with Ws(l, )c for a
model trained on MNIST for 200 epochs in Figure 6a. In the early training stage, they are indeed
negatively correlated. For top-10% weights with larger absolute values, the negative correlation is
even more significant.
Here we show that this negative correlation between ∆gD(l),s,c and Ws(l, )c tends to increase the ab-
solute value of gD(l) through an interesting mechanism. Consider the weights Ws(l, )c with {Ws(l, )c >
0, gD(l),s,c < 0}. Learning θl- would decrease gD(l),s,c and thus increase its absolute value because
the first term in eq. (28) is negative. On the other hand, learning Ws(l, )c would increase Ws(l, )c and its
absolute value because ∆Ws(l, )c = -λgD(l),s,c is positive. This will form a positive feedback process,
in which the numerator of GSNR, (gD(l),s,c)2, would increase and so is the GSNR. Similar analysis
can be done for the case with {Ws(l, )c < 0, gD(l),s,c > 0}.
On the other hand, when {Ws(l, )cgD(l),s,c > 0}, we show that the weights tend to change into the earlier
case, i.e. {Ws(l, )cgD(l),s,c < 0} during training. Consider the caseof {Ws(l,)c > 0, gD(l),s,c > 0}, the first
term in eq. (28) is negative, learning θ(l-) tends to decrease gD(l),s,c or even change its sign. Another
posibility is that learning Ws(l, )c changes the sign of Ws(l, )c because ∆Ws(l, )c = -λgD(l),s,c is negative.
In both cases the weights change into the earlier case with {Ws(l, )cgD(l),s,c < 0}. Similar analysis can
be done for the case of {Ws(l, )c < 0,gD(l),s,c < 0}.
Therefore {Ws(l, )cgD(l),s,c < 0} is a more stable state in the training process. For a simple model
trained on MNIST, We plot the proportion of weights satisfying {Ws(l, )cgD(l),s,c < 0} in Figure 6b
and find that there are indeed more weights with {Ws(l, )cgD(l),s,c < 0} than the opposite. Because
weights with small absolute value easily change sign during training, we also plot this proportion
for the top-10% weights with larger absolute values. We can see that for the weights with large
absolute values, nearly 80% of them have opposite signs with their gradient mean, confirming our
earlier analysis. For these weights, the numerator of GSNR, (gD(l),s,c)2, tends to increase through the
positive feedback process as discussed above.
Figure 6: MNIST experiments. Left: Correlation between ∆gD(l),s,c
and Ws(l, )c . Right : Ratio of weights that have opposite signs with
their gradient mean.
Table 1: Pearson correlation
between features and target
output y, where ct0 and ctmax
are correlations at the begin-
ning of training and maxi-
mum of GSNR curve respec-
tively.
feature id	ct ο	ctmax
0	-0.11	0.47
5	0.11	0.44
13	0.07	0.40
14	-0.21	-0.27
17	-0.33	0.53
9
Published as a conference paper at ICLR 2020
4 Summary
In this paper, we performed a series of analysis on the role of model parameters’ GSNR in deep
neural networks’ generalization ability. We showed that large GSNR is a key to small generalization
gap, and gradient descent training naturally incurs and exploits large GSNR as the model discovers
useful features in learning.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach, 2018. arXiv:1802.05296.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal ofMachine Learning Research, 3:463-482, 2002.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1019-1028. JMLR. org, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Alex Graves. Agenerating sequences with recurrent neural networks, 2013. arXiv:1308.0850v5.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Daniel Jakubovitz, Raja Giryes, and Miguel RD Rodrigues. Generalization error in deep learning,
2019.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: An empirical study. arXiv:1802.08760, 2018.
Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. arXiv preprint
arXiv:1802.04537, 2018.
Stanislaw Jastrzebski Srini Narayanan Stanislav Fort, Pawe Krzysztof Nowak. Stiffness: A new
perspective on generalization in neural networks, 2019. arXiv:1901.09491.
Vladimir N Vapnik and A Ja Chervonenkis. The necessary and sufficient conditions for consistency
of the method of empirical risk. Pattern Recognition and Image Analysis, 1(3):284-305, 1991.
10
Published as a conference paper at ICLR 2020
Matthew D. Zeiler. Adadelta: An adaptive learning rate method, 2012. arXiv:1212.5701.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11
Published as a conference paper at ICLR 2020
A Appendix A
A.1 Model S tructure in Section 2.4
As shown in Table 2, all models in the experiment consist of 2 Conv-Relu-MaxPooling blocks and
2 fully-connected layers, but they are different in the number of channels. We choose the number of
channels p from {6, 8, 10, 12, 14, 16, 18, 20}.
Table 2: Model structure On MNIST in Section 2.4. P is the number of channels and q = int (2.5 *P)
Layer	input #Channels	output #channels
conv + relu + maxpooling	1	P
conv + relu + maxpooling	P	q
flatten	-	-
fc + relu	16*q	10*q
fc + relu	10*q	10
softmax	-	-
A.2 Experiment on CIFAR 1 0
Different from the experiment on MNIST, we use a deeper network on CIFAR10. We also include
the Batch Normalization (BN) layer, because we find that it’s difficult for the network to converge
in the absence of it. The network consists of 4 Conv-BN-Relu-Conv-BN-Relu-MaxPooling blocks
and 3 fully-connected layers. More details are shown in Table 3.
Table 3: Model structure on CIFAR10. P is the number of channels.
Layer	input #Channels	output #Channels
conv + bn + relu	3	P
conv + bn + relu	P	P
maxpooling	-	-
conv + bn + relu	P	2P
conv + bn + relu	2P	2P
maxpooling		
conv + bn + relu	^^2p	4P
conv + bn + relu	4P	4P
maxpooling	-	-
conv + bn + relu	4p	8P
conv + bn + relu	8P	8P
maxpooling	-	-
flatten	-	-
fc + relu	32 * q	8*q
fc + relu	8*q	8*q
fc	8*q	10
softmax	-	-
The experiment is conducted under a similar setting as that of MNIST in section 2.4. We choose
n ∈ {2000, 4000, 6000, 8000, 10000}, Prandom ∈ {0.0, 0.2, 0.4}, ch ∈ {6, 8, 10, 12, 14, 16, 18}.
We use the gradient descent training (Not SGD), with a small learning rate of 0.001. The left and
right hand sides of 19 at different epochs are shown in Figure 7, where each point represents one
specific combination of the above settings. Note that at the evaluation step of every epoch, we use
12
Published as a conference paper at ICLR 2020
Figure 7: Left hand (LHS) and right side (RHS) of eq. (19). Points are drawn under different
experiment settings. Left figure: LHS vs RHS relation at epoch 20, 100, 500, 1000.
Figure 8: Similar with Fig. 3, but for a toy regression model discussed in in Appendix A.3.
the same mean and variance inside the BN layers as the training dataset. That’s to ensure that the
network and loss function are consistent between training and test.
At the beginning of training, compared to that of MNIST, the data points no longer perfectly resides
on the diagonal dashed line. We suppose that’s beacuse of the presence of BN layer, whose internal
parameters, i.e. running mean and running variance, are not regular learnable parameters in the
optimization process, but change their values in a different way. Their change affects the OSGR,
yet we could not include them in the estimation of OSGR. However, the strong positive correlation
between the left and right hand sides of eq. (19) can always be observed until the training begins to
converge.
A.3 Experiment on Toy Dataset
In this section we show a simple two-layer regression model consists of a FC-Relu structure with
only 2 inputs, 1 hidden layer with N neurons and 1 output. A similar synthetic dataset with the
training data used in the experiment of Section 3.2 is generated as follows. Each data point is
constructed i.i.d. using y = x0x1 + , where x0 and x1 are drawn from uniform distribution of
[-1, 1] and is drawn from uniform distribution of [-ηnoise, ηnoise].
To estimate eq. (24), we randomly generate 100 training sets with n samples each, i.e. M =100,
and a test set with 20,000 samples. To cover different conditions, we (1) choose n ∈
{50, 100, 300, 600, 1000, 2000, 6000}; (2) inject noise with ηnoise ∈ {0.2, 2, 4, 6, 8}; (3) perturb
model structures by choosing N ∈ {6, 8, 10, 12, 14, 16, 18, 20}. We use gradient descent with learn-
ing rate of 0.001.
Figure 8 shows a similar behavior as Fig. 3. During the early training stages, the LHS and RHS of
eq. (19) are very close. Their highly correlated relation remains until training converges, whereas
the RHS of eq. (19) decreases significantly.
13
Published as a conference paper at ICLR 2020
B Appendix B
Derivation of eq. (28)
Σ
θj ∈θ(l-)
Σ
θj ∈θ(l-)
∂ n P:
n
i=1
∂θj
∂Li
∆θj + other terms
∂ W S (,c
∂θj
)	1 n ∂Li
-(—A— 2. )+0 + other terms
n i=1 ∂θj
(29)
(30)
(31)
(32)
(33)
(l)
∂(1 Pn	∂Li dθi,c )
In 2^i =1 ∂C(I) ∂W(l)	A W__ _
∂oi,c ∂Ws,c	A	i	i,c0	i,s0
F- Q n S 三还菖西T)+ otherterms
d(Pn=ι 第ails) "	∂Li	(l) da?)0
——j— (E	W QO J ) + Other terms
∂θj	i=1 s0,c0 ∂oi(,c)0	∂θj
A n ∂Li ∂ai(,ls) ∂2 Li (l)	(l) n ∂Li ∂ai(,ls)0
n2 θjX- X(懑C 'dθ +	∂θais)(X " X EF)
Σ
θj ∈θ(l-)
	
AX
n2
θj ∈θ(l-)
	
+ other terms
∂ o(l)0	(l)
Above We used	=W(o)。，
∂ a( ) 0	s ,c
i,s0
the first term of eq. (33). When
and
s0 =
∂o(l)
i,c
∂ Wslc
∂Li ∂oi(,lc)0 ∂ai(,ls)0
ai(,ls) that can both be derived from eq. (26). Consider
s, c0 = c, We have
∆g(,C = -n X WC(X dL) dis)2 + other- terms	(34)
n θj∈θ(l-)	i=1 ∂oi,C	j
Note that the term related to RlLL a(l) and the terms when s = S or C = C in eq. (33) are merged
∂oi,c∂θj ,s
into other terms of eq. (34).
14
Published as a conference paper at ICLR 2020
C Appendix C	Notations
Z s or (x, y) D D0 θ gs (θ) or gi(θ) g( θ)	A data distribution satisfies X × Y A single data sample Training set consists of n samples drawn from Z Test set consists of n0 samples drawn from Z Model parameters, whose components are denoted as θj Parameters’ gradient w.r.t. a single data sample s or (xi, yi) Mean values of parameters’ gradient over a total data distribu- tion, i.e., ES〜Z(gs(θ))
gD(θ) gD0 (θ)	Average gradient over the training dataset, i.e., 1 Pn=1 gi(θ) n0 Average gradient over the test dataset, i.e.,熹 £分=1 gi(θ). Note that, in eq. (5), we assume n0 = n
gD,j ρ2(θ)	Same as gD(θj) Variance of parameters’ gradient of a single sample, i.e., Var S〜Z (g s (θ))
ρj2 σ2(θ)	Same as ρ2(θj) Variance of the average gradient over a training dataset of size n, i.e., VarD〜Zn [gD (θ)]
σj2 rj or r(θj ) L[D] L[D0] ∆L[D] ∆Lj[D]	Same as σ2(θj) Gradient signal to noise ratio (GSNR) of model parameter θj Empirical training loss, i.e., 1 Pn=1 L(yi, f (xi, θ)) Empirical test loss, i.e., * P n= 1 L Wi ,f( χi, θ))) One-step training loss decrease One-step training loss decrease caused by updating one param- eter θj
R(Z , n)	One-step generalization ratio (OSGR) for the training and test sets of size n sampled from data distribution Z, i.e., ED,D0~Zn <δ L [ D0]) ED〜Zn (∆L [D])
λ	Learning rate
5 W(l) and b(l) θ(l-)	One-step generalization gap increment, i.e., ∆L[D] - ∆L[D0] Random variables with zero mean and variance σ2 (θ) Model parameters (weight matrix and bias) of the l-th layer Collection of model parameters over all the layers before the l-th layer
(l) gD θ(l+)	Average gradient of W(l) over the training dataset Collection of model parameters over all the layers after the l-th layer, including the l-th layer
a(l) = {a(sl) (θ(l-))}	Activations of the l-th layer, where s = {1, 2, ..., S} is the index of nodes/channels in the l-th layer.
o(l) = {o(cl) }	Outputs of matrix multiplication of the l-th layer, where c = {1, 2, ..., C} is index of nodes/channels in the (l + 1)-th layer.
(l) d (l) ai,s and oi,c	a(Sl) and o(cl) evaluated on data sample i
15