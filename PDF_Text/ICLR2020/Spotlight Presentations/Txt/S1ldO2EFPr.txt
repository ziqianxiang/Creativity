Published as a conference paper at ICLR 2020
Graph Neural Networks Exponentially Lose
Expressive Power for Node Classification
Kenta Oono1, 2, Taiji Suzuki1, 3
{kenta_oono, taiji}@mist.i.u-tokyo.ac.jp
1	The University of Tokyo 2Preferred Networks, Inc.
3	RIKEN Center for Advanced Intelligence Project (AIP)
Ab stract
Graph Neural Networks (graph NNs) are a promising deep learning approach for
analyzing graph-structured data. However, it is known that they do not improve
(or sometimes worsen) their predictive performance as we pile up many layers and
add non-lineality. To tackle this problem, we investigate the expressive power of
graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our
strategy is to generalize the forward propagation of a Graph Convolutional Net-
work (GCN), which is a popular graph NN variant, as a specific dynamical sys-
tem. In the case of a GCN, we show that when its weights satisfy the conditions
determined by the spectra of the (augmented) normalized Laplacian, its output ex-
ponentially approaches the set of signals that carry information of the connected
components and node degrees only for distinguishing nodes. Our theory enables
us to relate the expressive power of GCNs with the topological information of the
underlying graphs inherent in the graph spectra. To demonstrate this, we charac-
terize the asymptotic behavior of GCNs on the Erdos - Renyi graph. We show
that when the Erdos - Renyi graph is sufficiently dense and large, a broad range
of GCNs on it suffers from the “information loss” in the limit of infinite layers
with high probability. Based on the theory, we provide a principled guideline for
weight normalization of graph NNs. We experimentally confirm that the proposed
weight scaling enhances the predictive performance of GCNs in real data1.
1	Introduction
Motivated by the success of Deep Learning (DL), several attempts have been made to apply DL mod-
els to non-Euclidean data, particularly, graph-structured data such as chemical compounds, social
networks, and polygons. Recently, Graph Neural Networks (graph NNs) (Duvenaud et al., 2015; Li
et al., 2016; Gilmer et al., 2017; Hamilton et al., 2017; Kipf & Welling, 2017; Nguyen et al., 2017;
Schlichtkrull et al., 2018; Battaglia et al., 2018; Xu et al., 2019; Wu et al., 2019a) have emerged as a
promising approach. However, despite their practical popularity, theoretical research of graph NNs
has not been explored extensively.
The characterization ofDL model expressive power, i.e., to identify what function classes DL mod-
els can (approximately) represent, is a fundamental question in theoretical research of DL. Many
studies have been conducted for Fully Connected Neural Networks (FNNs) (Cybenko, 1989; Hornik,
1991; Hornik et al., 1989; Barron, 1993; Mhaskar, 1993; Sonoda & Murata, 2017; Yarotsky, 2017)
and Convolutional Neural Networks (CNNs) (Petersen & Voigtlaender, 2018; Zhou, 2018; Oono &
Suzuki, 2019). For such models, we have theoretical and empirical justification that deep and non-
linear architectures can enhance representation power (Telgarsky, 2016; Chen et al., 2018b; Zhou
& Feng, 2018). However, for graph NNs, several papers have reported that node representations go
indistinguishable (known as over-smoothing) and prediction performances severely degrade when
we stack many layers (Kipf & Welling, 2017; Wu et al., 2019b; Li et al., 2018). Besides, Wu et al.
(2019a) reported that graph NNs achieved comparable performance even if they removed interme-
diate non-linear functions. These studies posed a question about the current architecture and made
us aware of the need for the theoretical analysis of the graph NN expressive power.
1Code is available at https://github.com/delta2323/gnn-asymptotics.
1
Published as a conference paper at ICLR 2020
In this paper, we investigate the expressive power of graph NNs by analyzing their asymptotic be-
haviors as the layer size goes to infinity. Our theory gives new theoretical conditions under which
neither layer stacking nor non-linearity contributes to improving expressive power. We consider a
specific dynamics that includes a transition defining a Markov process and the forward propaga-
tion of a Graph Convolutional Network (GCN) (Kipf & Welling, 2017), which is one of the most
popular graph NN variants, as special cases. We prove that under certain conditions, the dynamics
exponentially approaches a subspace that is invariant under the dynamics. In the case of GCN, the
invariant space is a set of signals that correspond to the lowest frequency of graph spectra and that
have “no information” other than connected components and node degrees for a node classification
task whose goal is to predict the nodes’ properties in a graph. The rate of the distance between the
output and the invariant space is O((sλ)L) where s is the maximum singular values of weights, λ
is typically a quantity determined by the spectra of the (augmented) normalized Laplacian, and L is
the layer size. See Sections 3.3 (general case) and 4 (GCN case) for precise statements.
We can interpret our theorem as the generalization of the well-known property that if a finite and dis-
crete Markov process is irreducible and aperiodic, it exponentially converges to a unique equilibrium
and the eigenvalues of its transition matrix determine the convergence rate (see, e.g., Chung & Gra-
ham (1997)). Different from the Markov process case, which is linear, the existence of intermediate
non-linear functions complicates the analysis. We overcame this problem by leveraging the combi-
nation of the ReLU activation function (Krizhevsky et al., 2012) and the positivity of eigenvectors
of the Laplacian associated with the smallest positive eigenvalues.
Our theory enables us to investigate asymptotic behaviors of graph NNs via the spectral distribution
of the underlying graphs. To demonstrate this, We take GCNs defined on the Erdos - Renyi graph
GN,p, which has N nodes and each edge appears independently with probability p, for an example.
We prove that if IopgNNN = o(1) as a function of N, any GCN whose weights have maximum singular

values at most C
NP
log(N∕ε)
approaches the “information-less” invariant space with probability at
least 1 - ε, where C is a universal constant. Intuitively, if the graph on which we define graph NNs
is sufficiently dense, graph-convolution operations mix signals on nodes fast and hence the feature
maps lose information for distinguishing nodes quickly.
Our contributions are as follows:
•	We relate asymptotic behaviors of graph NNs with the topological information of underly-
ing graphs via the spectral distribution of the (augmented) normalized Laplacian.
•	We prove that if the weights of a GCN satisfy conditions determined by the graph spectra,
the output of the GCN carries no information other than the node degrees and connected
components for discriminating nodes when the layer size goes to infinity (Theorems 1, 2).
•	We apply our theory to ErdoS - Renyi graphs as an example and show that when the graph
is sufficiently dense and large, many GCNs suffer from the information loss (Theorem 3).
•	We propose a principled guideline for weight normalization of graph NNs and empirically
confirm it using real data.
2	Related Work
MPNN-type Graph NNs. Since many graph NN variants have been proposed, there are several
unified formulations of graph NNs (Gilmer et al., 2017; Battaglia et al., 2018). Our approach is
the closest to the formulation of Message Passing Neural Network (MPNN) (Gilmer et al., 2017),
which unified graph NNs in terms of the update and readout operations. Many graph NNs fall into
this formulation such as Duvenaud et al. (2015), Li et al. (2θ16), and Velickovic et al. (2018).
Among others, GCN (Kipf & Welling, 2017) is an important application of our theory because it
is one of the most widely used graph NNs. In addition, GCNs are interesting from a theoretical
research perspective because, in addition to an MPNN-type graph NN, we can interpret GCNs as a
simplification of spectral-type graph NNs (Henaff et al., 2015; Defferrard et al., 2016), that make
use of the graph Laplacian.
Our approach, which considers the asymptotic behaviors graph NNs as the layer size goes to infinity,
is similar to Scarselli et al. (2009), one of the earliest works about graph NNs. They obtained node
2
Published as a conference paper at ICLR 2020
representations by iterating message passing between nodes until convergence. Their formulation is
general in that we can use any local aggregation operation as long as it is a contraction map. Our
theory differs from theirs in that we proved that the output of a graph NN approaches a certain space
even if the local aggregation function is not necessarily a contraction map.
Expressive Power of Graph NNs. Several studies have focused on theoretical analysis and the im-
provement of graph NN expressive power. For example, Xu et al. (2019) proved that graph NNs are
no more powerful than the Weisfeiler - Lehman (WL) isomorphism test (Weisfeiler & A.A., 1968)
and proposed a Graph Isomorphism Network (GIN), that is approximately as powerful as the WL
test. Although they experimentally showed that GIN has improved accuracy in supervised learning
tasks, their analysis was restricted to the graph isomorphism problem. Xu et al. (2018) analyzed
the non-asymptotic properties of GCNs through the lens of random walk theory. They proved the
limitations of GCNs in expander-like graphs and proposed a Jumping Knowledge Network (JK-
Net) to address the issue. To handle the non-linearity, they linearized networks by a randomization
assumption (Choromanska et al., 2015). We take a different strategy and make use of the interpre-
tation of ReLU as a projection onto a cone. Recently, NT & Maehara (2019) showed that a GCN
approximately works as a low-pass filter plus an MLP in a certain setting. Although they analyzed
finite-depth GCNs, our theory has similar spirits with theirs because our “information-less” space
corresponds to the lowest frequency of a graph Laplacian. Another point is that they imposed as-
sumptions that input signals consist of low-frequent true signals and high-frequent noise, whereas
we need not such an assumption.
Role of Deep and Non-linear Structures. For ordinal DL models such as FNNs and CNNs, we
have both theoretical and empirical justification of deep and non-linear architectures for enhancing
of the expressive power (e.g., Telgarsky (2016); Petersen & Voigtlaender (2018); Oono & Suzuki
(2019)). In contrast, several studies have witnessed severe performance degradation when stacking
many layers on graph NNs (Kipf & Welling, 2017; Wu et al., 2019b). Li et al. (2018) reported that
feature vectors on nodes in a graph go indistinguishable as we increase layers in several tasks. They
named this phenomenon over-smoothing. Regarding non-linearity, Wu et al. (2019a) empirically
showed that graph NNs achieve comparable performance even ifwe omit intermediate non-linearity.
These observations gave us questions about the current models of deep graph NNs in terms of their
expressive power. Several studies gave theoretical explanations of the over-smoothing phenomena
for linear GNNs (Li et al., 2018; Zhang, 2019; Zhao & Akoglu, 2020). We can think of our theory
as an extension of their results to non-linear GNNs.
3	Problem Setting and Main Res ult
3.1	Notation
Let N+ be the set of positive integers. For N ∈ N+, we denote [N] := {1, . . . , N}. For a vector
v ∈ RN, we write v ≥ 0 if and only if vn ≥ 0 for all n ∈ [N]. Similarly, for a matrix X ∈ RN×C,
we write X ≥ 0 if and only if Xnc ≥ 0 for all n ∈ [N] and c ∈ [C]. We say such a vector and
matrix is non-negative. 〈•, •)denotes the inner product of vectors or matrices, depending on the
context: hu, vi := u>v for u, v ∈ RN and hX, Y i := tr(XT Y ) for X, Y ∈ RN×C. 1P equals to 1
if the proposition P is true else 0. For vectors V ∈ RN and W ∈ RC, V 0 W ∈ RN×C denotes the
Kronecker product of V and W defined by (V 0 w)nc := VnWc. For X ∈ RN×C, ∣∣X∣∣f :=(X,X”
denotes the Frobenius norm of X. For a vector V ∈ RN, diag(V) := (Vnδnm)n,m∈[N] ∈ RN×N
denotes the diagonalization of V. IN ∈ RN×N denotes the identity matrix of size N . For a linear
operator P : RN → RM and a subset V ⊂ RN, we denote the restriction ofP to V by P|V .
3.2	Dynamical System
Although we are mainly interested in GCNs, we develop our theory more generally using dynamical
systems. We will specialize to the GCNs in Section 4.
For N, C, Hl	∈	N+	(l	∈	N+),	let P ∈	RN×N be a symmetric matrix and	Wlh	∈	RC×C for
l ∈ N+ and h ∈ [Hl]. We define fl : RN×C → RN×C by fl(X) := MLPl(PX). Here,
MLPl : RN×C → RN×C is the l-th multi-layer perceptron common to all nodes (Xu et al., 2019)
and is defined by MLPl(X) := σ(…σ(σ(X)W11)W12 …Wih1 ), where σ : RN×C → RN×C
3
Published as a conference paper at ICLR 2020
is an element-wise ReLU function (Krizhevsky et al., 2012) defined by σ(X)nc := max(Xnc, 0)
for n ∈ [N], c ∈ [C]. We consider the dynamics X(l+1) := fl (X(l)) with some initial value
X (0) ∈ RN ×C . We are interested in the asymptotic behavior of X (l) as l → ∞.
For M ≤ N, let U be a M -dimensional subspace of RN . We assume that U and P satisfy the
following properties that generalize the situation where U is the eigenspace associated with the
smallest eigenvalue of a (normalized) graph Laplacian ∆ (that is, zero) and P is a polynomial of ∆.
Assumption 1. U has an orthonormal basis (em)m∈[M] that consists of non-negative vectors.
Assumption 2. U is invariant under P, i.e., if u ∈ U, then Pu ∈ U.
We endow RN with the ordinal inner product and denote the orthogonal complement of U by U⊥ :=
{u ∈ RN | hu, vi = 0, ∀v ∈ U}. By the symmetry of P , we can show that U⊥ is invariant under
P, too (Appendix E.1, Proposition 2). Therefore, We can regard P as a linear mapping P|u⊥ :
U⊥ → U⊥. We denote the operator norm of P|u⊥ by λ. When U is the eigenspace associated
With the smallest eigenvalue of ∆ and P is g(∆) Where g is a polynomial, then, λ corresponds to
λ = sup* ∣g(μ) | where sup ranges over all eigenvalues except the smallest one.
3.3	Main Result
We define the subspace M of RN×C by M := U 0 RC = {PM=ι em 0 Wm | Wm ∈ RC} where
(em)m∈[M] is the orthonormal basis of U appeared in Assumption 1. For X ∈ RN×C, we denote
the distance between X and M by dM(X) := inf{kX -Y kF | Y ∈ M}. We denote the maximum
singular value of Wlh by slh and set sl := QhH=l 1 slh. With these preparations, we introduce the
main theorem of the paper.
Theorem 1. Under Assumptions 1 and 2, we have dM(fl(X)) ≤ slλdM(X) for any X ∈ RN×C.
The proof key is that the non-linear operation σ decreases the distance dM, that is, dM (σ(X)) ≤
dM(X). We use the non-negativity of em to prove this claim. See Appendix A for the complete
proof. We also discuss the strictness of Theorem 1 in Appendix E.3.
By setting dM (X) = 0, this theorem implies that M is invariant under fl. In addition, if the
maximum value of singular values are small, X(l) asymptotically approaches M in the sense of
Johnson (1973) for any initial value X(0). That is, the followings hold under Assumptions 1 and 2.
Corollary 1. M is invariant under fl for any l ∈ N+, that is, ifX ∈ M, then we have fl(X) ∈ M.
Corollary 2. Let s := supl∈N+ sl. We have dM(X(l)) = O((sλ)l). In particular, if sλ < 1, then
Xl exponentially approaches M as l → ∞ for any initial value X(0).
Suppose the operator norm of P |U : U → U is no larger than λ, then, under the assumption of
sλ < 1, X(l) converges to 0, the trivial fixed point (see Appendix E.2, Proposition 3). Therefore,
we are mainly interested in the case where the operator norm of P |U is strictly larger than λ (see
Proposition 1). Finally, we restate Theorem 1 specialized to the situation where U is the direct sum
of eigenspaces associated with the largest M eigenvalues of P. Note that the eigenvalues of P is
real since P is symmetric.
Corollary 3. Let λι ≤ •…≤ λN be the eigenvalue of P, sorted in ascending order. Suppose the
multiplicity of the largest eigenvalue λN is M (≤ N), i.e., λN-M < λN-M+ι = •…=λN. We
define λ := maXn∈[N-M ] ∣λn∣. We denote U by the eigenspace associated with λN and assume that
U satisfies Assumption 1. Then, we have dM (X (l+1)) ≤ slλdM(X(l)).
Remark 1. It is known that any Markov process on finite states converges to a unique distribution
(equilibrium) if it is irreducible and aperiodic (see e.g., Norris (1998)). Theorem 1 includes this
proposition as a special case with M = 1, C = 1, and Wl = 1for all l ∈ N+. This is essentially
the direct ConSeqUenCe ofPerron - Frobenius' theorem (see e.g., Meyer (2000)). See Appendix F.
4	Application to GCN
We formulate a GCN (Kipf & Welling, 2017) without readout operations (Gilmer et al., 2017) using
the dynamical system in the previous section and derive a sufficient condition in terms of the spectra
of underlying graphs in which layer stacking nor non-linearity are not helpful for node classification.
4
Published as a conference paper at ICLR 2020
Let G = (V, E) be an undirected graph where V is a set of nodes and E is a set of edges. We denote
the number of nodes in G by N = |V | and identify V with [N] by fixing an order of V . We associate
a C dimensional signal to each node. X in the previous section corresponds to concatenation of the
signals. GCNs iteratively update signals on V using the connection information and weights.
Let A := (1{(i,j)∈E})i,j∈[N] ∈ RN×N be the adjacency matrix and D := diag(deg(i)i∈[N]) ∈
RN×N be the degree matrix of G where deg(i) := |{j ∈ V | (i, j) ∈ E}| is the degree of node
i. Let A := A + IN, D := D + IN be the adjacent and degree matrix of graph G augmented
with self-loops. We define the augmented normalized Laplacian (WU et al., 2019a) of G by ∆ :=
1	1∖ 一 1 ^Λ 1Λ ― 1	1 , 7-»	T	ʌ T . 7^	一 RT F ,11	11	1 ∙	. ∙ 1
IN - D 2 AD 2 and set P := IN - ∆. Let L,C ∈ N+ be the layer and channel sizes, respectively.
For weights Wl ∈ RC × C (l ∈ [L]), We define a GCN2 associated with G by f = /l ◦•••◦ fι where
fl : RN×C → RN×C is defined by fl (X) := σ(PXWl). We are interested in the asymptotic
behavior of the oUtpUt X(L) of the GCN as L → ∞.
Suppose G has M connected components and let V = Vi t ∙∙∙ t VM be the decomposition of
the node set V into connected components. We denote an indicator vector of the m-th connected
component by um := (1{n∈Vm})n∈[N] ∈ RN. The following proposition shows that GCN satisfies
the assumption of Corollay 3 (see Appendix B for proof).
Proposition 1. Let λι ≤ •… ≤ Xn be the eigenvalue of P sorted in ascending order. Then, we
have —1 < λι, Xn-M < 1, and Xn-M +i = •…=Xn = 1. In particular, we have λ :=
maxn=ι,…,n-M ∣λn | < 1. Further, em, := D2 Um for m ∈ [M] are the basis of the eigenspace
associated with the eigenvalue 1.
Theorem 2. For any initial value X(0), the output of l-th layer X(l) satisfies dM(X(l)) ≤
(sX)l dM (X (0)). In particular, dM(X(l)) exponentially converges to 0 when sX < 1.
In the context of node classification tasks, we can interpret this corollary as the “information loss” of
GCNs in the limit of infinite layers. For any X ∈ M, if two nodes i, j ∈ V are in a same connected
component and their degrees are identical, then, the column vectors of X that correspond to nodes i
and j are identical. It means that we cannot distinguish these nodes using X . In this sense, M only
has information about connected components and node degrees and we can interpret this theorem
as the exponential information loss of GCNs in terms of the layer size. Similarly to the discussion
in the previous section, X(l) converges to the trivial fixed point 0 when s < 1 (remember XN = 1).
An interesting point is that even if s ≥ 1, X(l) can suffer from this information loss when s < X-1.
We note that the rate sX in Theorem 2 depends on the spectra of the augmented normalized Lapla-
cian, which is determined by the topology of the underlying graph G. Hence, our result explicitly
relates the topological information of graphs and asymptotic behaviors of graph NNs.
Remark 2. The old preprint (version 2) of Luan et al. (2019) formulated a theorem that explains the
over-smoothing of non-linear GNNs. Specifically, it claimed that ifa graph does not have a bipartite
component and the input distribution is continuous, the rank of the output of a GCN converges to the
number of connected components of the underlying graph as the layer size goes to infinity almost
surely. However, it is not true in general as we give a counterexample in Appendix C.
5	Asymptotic Behavior of GCN on ERDOS — RENYI Graph
Theorem 2 gives us a way to characterize the asymptotic behaviors of GCNs via the spectral distri-
butions of the underlying graphs. To demonstrate this, we consider an Erdos - Renyi graph Gn,p
(Erdos & Renyi, 1959; Gilbert, 1959), which is a random graph that has N nodes and whose edges
between two distinct nodes appear independently with probability p ∈ [0, 1], as an example. First,
consider a (non-random) graph G with M connected components. Let 0 = μι = …=μM <
μM+ι ≤ ∙∙∙ ≤ μN < 2 be eigenvalues of the augmented normalized Laplacian of G (see, Propo-
sition 1) and set λ := minm=M+i,...,n |1 一 μm∣(< 1). By Theorem 2, the output of GCN “loses
information” as the layer size goes to infinity when the largest singular values of weights are strictly
smaller than λ-i. Therefore, the closer the positive eigenvalues μm, are to 1, the broader range of
GCNs satisfies the assumption of Theorem 2.
2Following the original paper (Kipf & Welling, 2017), we use one-layer MLps (i.e., Hl = 1 for all l ∈ N+.).
However, our result holds for the multi-layer case
5
Published as a conference paper at ICLR 2020
Figure 1: Visualization of vector field V (X) := f(X) -X induced by the one-step transition. Color
maps indicate the absolute value |V (X)| at the point X. Dotted lines are the subspace M. Left:
Case 1. Right: Case 2. Best view in color.
For an Erdos - Renyi graph Gn,p, ChUng & Radcliffe (2011) showed that when IoNN = o(1), the
eigenvalues of the (usual) normalized Laplacian except for the smallest one converge to 1 with high
probability (see Theorem 2 therein)3. We can interpret this theorem as the convergence of ErdOs-
Renyi graphs to the complete graph in terms of graph spectra. We can prove that the augmented
normalized Laplacian behaves similarly (Lemma 6). By combining this fact with the discUssion in
the previous paragraph, We obtain the asymptotic behavior of GCNS on the Erdos - Renyi graph.
See Appendix D for the complete proof.
Theorem 3. Consider a GCN on the Erdos-Renyi graph Gn,p such that IoNN = o(1) as afunction
of N. For any ε > 0, if the supremum S of the maximum singular values of weights in the GCN
satisfies S < so :=之 C iN(-N+9), then, for SUficiently large N, the GCN satisfies the condition of
Theorem 2 with probability at least 1 - ε.
Theorem 3 requires that an underlying graph is not extremely sparse. For example, suppose the
node size is N = 20, 000, which is the approximately the maximum node size of datasets we use in
experiments, and the edge probability is p = log N/N. Then, each node has the order of Np ≈ 4.3
adjacent nodes.
Under the condition of Theorem 3, the upper bound S0 → ∞ as N → ∞. It means that if the graph
is sufficiently large and not extremely sparse, most GCNs suffer from the information loss. For the
dependence on the edge probability p, S0 is an increasing function of p, which means the denser a
graph is, the more quickly graph convolution operations mix signals on nodes and move them close
to each other.
Theorem 3 implies that graph NNs perform poorly on dense NNs. More aggressively, we can hy-
pothesize that the sparsity of practically available graphs is one of the reasons for the success of
graph NNs in node classification tasks. To confirm this hypothesis, we artificially add edges to ci-
tation networks to make them dense in the experiments and observe the failure of graph NNs as
expected (see Section 6.3).
6	Experiment
6.1	Synthesis Data: One-step Transition
We numerically investigate how the transition f(X) := σ(PXW) changes inputs using the vector
field V (X) := f(X) - X4. For this purpose, we set N = 2, M = 1, and C = 1. Let λ1 ≤ λ2
be the eigenvalues of P. We choose W as ∣λ2∣-1 ≤ W < ∣λ1∣-1 so that Theorem 1 is applicable
but is not reduced to the trivial situation (see, Appendix E.2). We choose the eigenvector e ∈ R2
3Chung et al. (2004) and Coja-Oghlan (2007) proved similar theorems.
4Since we consider the one-step transition only, we omit the subscript l from fl , Xl, and Wl .
6
Published as a conference paper at ICLR 2020
ωuujsaαω>ce-ωɑ 60-1
6 4 2 0 2 4
- -
us>α CTOJ
p=0.1, s=10.Of Iambda=O.195
O 2	4	6	8	10
Layer Index
Figure 2: The actual distances to the invariant space M and their upper bounds. Solid lines are the
log relative distance defined by y(l) = log(dM(X(l))/dM(X(0))) and dotted lines are upper bound
y(l) = l log(sλ), where X(0) is the input signal and X(l) is the output of the l-th layer.
4	6	8
Layer Size
***
s = 0.5
s = 1.05
s = 3.0
s = 10.0
Unnormalized
Figure 3: Node prediction results on Noisy Cora. Left: Effect of the maximum singular values on
weights on model performance. The horizontal dotted line indicates the chance rate (30.2%). The
error bar is the standard deviation of 3 trials. Right: Transition of maximum singular values during
training. See Appendix I.3 for results using other datasets. Best view in color.
associated with λ2 in two ways as described below. See Appendix H.1 for the concrete values of P,
e, and W . Figure 1 shows the visualization of V . First, we choose the non-negative eigenvector e
so that it satisfies Assumption 1 (Case 1). We see that the transition function f uniformly decreases
the distance from M. This is consistent with the consequence of Theorem 1. Next, we choose
the eigenvector e = [e1 e2]> such that the signs of e1 and e2 differ (Case 2), which violates
Assumption 1. We see that M is not invariant under f and f does not uniformly decrease the
distance from M. Therefore, we cannot remove the non-negativity assumption from Theorem 1.
6.2	Synthesis Data: Distance to Invariant Space
We evaluate the distance to the invariant space M using synthesis data. We randomly generate an
Erdos - Renyi graph, a GCN on it, and an input signal X ⑼.We compute the distance between the l-
th intermediate output X (l) and the invariant space M for various edge probability p and maximum
SingUlar value s. Figure 2 plots the logarithm of the relative distance y(l) = log dM(X(o)) with
respect to the layer index l. From Theorem 1, we know that it is upper bounded by y(l) = l log(sλ).
We see that this bound well approximates the actual value when sλ is small. On the other hand, it is
loose for large sλ. We leave tighter bounds for dM in such a case for future research.
6.3	Real Data: Effect of Maximum Singular Values on Performance
Theorem 2 implies that ifs is smaller than the threshold λ-1, we cannot expect deep GCN to achieve
good prediction accuracy. Conversely, if we can successfully train the model, s should avoid the
region s ≤ λ-1. We empirically confirm these hypotheses using real datasets.
7
Published as a conference paper at ICLR 2020
We use Cora, CiteSeer, and PubMed (Sen et al., 2008), which are standard citation network datasets.
The task is to classify the genre of papers using word occurrences and citation relationships. We
regard each paper as a node and citation relationship as an edge. Due to space constraints, we focus
on Cora in the main article. See Appendix H.3 and I.3 for the other datasets. The discussion in
Section 5 implies that Theorem 2 can support a wide range of GCNs when the underlying graph
is relatively dense. However, the citation networks are too sparse to examine the aforementioned
hypotheses — Theorem 2 gives a non-trivial result only when 1 ≤ s < λ-1 ≈ 1 + 3.62 × 10-3. To
circumvent this, we make noisy versions of citation networks by randomly adding edges to graphs.
Through this manipulation, we can increase the value of λ-1 to 1.11.
Figure 3 (left) shows the accuracy for the test dataset in terms of the maximum singular values
and the number of graph convolution layers. We can observe that when GCNs whose maximum
singular value s is out of the region s < λ-1 outperform those inside the region in almost all
configurations. Furthermore, the accuracy of GCNs with s = 10 are better than those without
normalization (unnormalized). Figure 3 (right) shows the transition of the maximum singular values
of the weights during training when we use a three-layered GCN. We can observe that the maximum
singular value s does not shrink to the region s ≤ λ-1. In addition, when the layer size is small and
predictive accuracy is high, GCNs gradually increase s from the initial value and avoid the region.
In conclusion, the experiment results are consistent with the theorems.
6.4	Real Data: Effect of Signal Component Perpendicular To Invariant Space
We can decompose the output X of a model as X = X0 + X1
(X0 ∈ M, X1 ∈ M⊥). According to the theory, X0 has
limited information for node classification. We hypothesize
that the model emphasizes the perpendicular component X1
to perform good predictions. To quantitatively evaluate it, we
define the relative magnitude of the perpendicular component
of the output X by t(X) := X1/X0. Figure 4 compares this
quantity and the prediction accuracy on the noisy version of
Cora (see Appendix I.4 for other datasets). We observe that
these two quantities are correlated (R = 0.545). If we remove
GCNs have only one layer (corresponding to right points in the
figure), the correlation coefficient is 0.827. This result does not
contradict to the hypothesis above 5.
7 Discussion
0.65
0.60
0.55
0.50
h.45
0.35
0.30
0.25
Log of Relative Perpendicular Component
NOiSy Cora (R=O.545, p=4.880e-03)
• s=0.5
▲ s=1.05
♦ s=3.0
■ s=10.0
× unnormalized
Figure 4: log t(X ) and prediction
accuracy on Noisy Cora.
0
Applicability to Graph NNs on Sparse Graphs. We have theoretically and empirically shown that
when the underlying graph is sufficiently dense and large, the threshold λ-1 is large (Theorem 2
and Section 6.3), which means many graph CNNs are eligible. However, real-world graphs are not
often dense, which means that Theorem 2 is applicable to very limited GCNs. In addition, Coja-
Oghlan (2007) theoretically proved that if the expected average degree of GN,p is bounded, the
smallest positive eigenvalue of the normalized Laplacian of GN,p is o(1) with high probability. The
asymptotic behaviors of graph NNs on sparse graphs are left for future research.
Remedy for Over-smoothing. Based on our theory, we can propose several techniques for mitigat-
ing the over-smoothing phenomena. One idea is to (randomly) sample edges in an underlying graph.
The sparsity of practically available graphs could be a factor in the success of graph NNs. Assum-
ing this hypothesis is correct, there is a possibility that we can relive the effect of over-smoothing
by sparsification. Since we can never restore the information in pruned edges if we remove them
permanently, random edge sampling could work better as FastGCN (Chen et al., 2018a) and Graph-
SAGE (Hamilton et al., 2017) do. Another idea is to scale node representations (i.e., intermediate
or final output of graph NNs) appropriately so that they keep away from the invariant space M. Our
proposed weight scaling mechanism takes this strategy. Recently, Zhao & Akoglu (2020) has pro-
5We cannot conclude that large perpendicular components are essential for good performance, since the
maximum singular value s is correlated to the accuracy, too.
8
Published as a conference paper at ICLR 2020
posed PairNorm to alleviate the over-smoothing phenomena. Although the scaling target is different
-they rescaled signals whereas We normalized weights - theirs and ours have similar spirits.
Graph NNs with Large Weights. Our theory suggests that the maximum singular values of weights
in a GCN should not be smaller than a threshold λ-1 because it suffers from information loss for
node classification. On the other hand, if the scale of weights are very large, the model complexity
of the function class represented by graph NNs increases, which may cause large generalization
errors. Therefore, from a statistical learning theory perspective, we conjecture that the graph NNs
with too-large weights perform poorly, too. A trade-off should exist between the expressive power
and model complexity and there should be a “sweet spot” on the weight scale that balances the two.
Relation to Double Descent Phenomena. Belkin et al. (2019) pointed out that modern deep mod-
els often have double descent risk curves: when a model is under-parameterized, a classical bias-
variance trade-off occurs. However, once the model has a large capacity and perfectly fits the training
data, the test error decreases as we increase the number of parameters. To the best of our knowl-
edge, no literature reported the double descent phenomena for graph NNs (it is consistent with the
picture of the classical U-shaped risk curve in the previous paragraph). It is known that double
descent phenomena do not occur in some situations, especially depending on regularization types.
For example, while Belkin et al. (2019) employed the interpolating hypothesis with the minimum
norm, Mei & Montanari (2019) found that the double descent was alleviated or disappeared when
they used Ridge-type regularization techniques. Therefore, one can hypothesize the over-smoothing
is a cause or consequence of regularization that is more like a Ridge-type rather than minimum-norm
inductive bias.
Limitations in Graph NN Architectures. Our analysis is limited to graph NNs with the ReLU
activation function because we implicitly use the property that ReLU is a projection onto the cone
{X ≥ 0} (Appendix A, Lemma 3). This fact enables the ReLU function to get along with the non-
negativity of eigenvectors associated with the largest eigenvalues. Therefore, it is far from trivial to
extend our results to other activation functions such as the sigmoid function or Leaky ReLU (Maas
et al., 2013). Another point is that our formulation considers the update operation (Gilmer et al.,
2017) of graph NNs only and does not take readout operations into account. In particular, we cannot
directly apply our theory to graph classification tasks in which each sample is a graph.
Over-smoothing of Residual GNNs. Considering the correspondence of graph NNs and Markov
processes (see Appendix F), one can imagine that residual links do not contribute to alleviating
the over-smoothing phenomena because adding residual connections to a graph NN corresponds
to converting a Markov process to its lazy version. When a Markov process converges to a stable
distribution, the corresponding lazy process also converges eventually under certain conditions. It
implies that residual links might not be helpful. However, Li et al. (2019) reported that graph NNs
with as many as 56 layers performed well if they added residual connections. Considering that,
the situation could be more complicated than our intuitions. The analysis of the role of residual
connections in graph NNs is a promising direction for future research.
8 Conclusion
In this paper, to understand the empirically observed phenomena that deep non-linear graph NNs
do not perform well, we analyzed their asymptotic behaviors by interpreting them as a dynamical
system that includes GCN and Markov process as special cases. We gave theoretical conditions
under which GCNs suffer from the information loss in the limit of infinite layers. Our theory directly
related the expressive power of graph NNs and topological information of the underlying graphs via
spectra of the Laplacian. It enabled us to leverage spectral and random graph theory to analyze the
expressive power of graph NNs. To demonstrate this, we considered GCN on the ErdOs - Renyi
graph as an example and showed that when the underlying graph is sufficiently dense and large,
a wide range of GCNs on the graph suffer from information loss. Based on the theory, we gave
a principled guideline for how to determine the scale of weights of graph NNs and empirically
showed that the weight normalization implied by our theory performed well in real datasets. One
promising direction of research is to analyze the optimization and statistical properties such as the
generalization power (Verma & Zhang, 2019) of graph NNs via spectral and random graph theories.
9
Published as a conference paper at ICLR 2020
Acknowledgments
We thank Katsuhiko Ishiguro for providing a part of code for the experiments, Kohei Hayashi and
Haru Negami Oono for giving us feedback and comments on the draft, Keyulu Xu and anonymous
reviewers for fruitful discussions via OpenReview, and Ryuta Osawa for pointing out errors and
suggesting improvements of the paper. TS was partially supported by JSPS KAKENHI (15H05707,
18K19793, and 18H03201), Japan Digital Design, and JST CREST.
References
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, pp.
2623-2631, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-6201-6. doi: 10.1145/3292500.
3330701.
Dana Angluin and Leslie G Valiant. Fast probabilistic algorithms for hamiltonian circuits and match-
ings. Journal of Computer and system Sciences, 18(2):155-193, 1979.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945, 1993.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
James S. Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs KegL Algorithms for hyper-
parameter optimization. In Advances in Neural Information Processing Systems 24, pp. 2546-
2554. Curran Associates, Inc., 2011.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks
via importance sampling. In International Conference on Learning Representations, 2018a. URL
https://openreview.net/forum?id=rytstxWAW.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 873-882. PMLR, 10-15 Jul 2018b.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the
loss surfaces of multilayer networks. In Peter Grunwald, Elad Hazan, and Satyen Kale (eds.),
Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine
Learning Research, pp. 1756-1760. PMLR, 2015.
Fan Chung and Linyuan Lu. Connected components in random graphs with given expected degree
sequences. Annals of combinatorics, 6(2):125-145, 2002.
Fan Chung and Mary Radcliffe. On the spectra of general random graphs. the electronic journal of
combinatorics, 18(1):215, 2011.
Fan Chung, Linyuan Lu, and Van Vu. The spectra of random graphs with given expected degrees.
Internet Mathematics, 1(3):257-275, 2004.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92 in CBMS Regional
Conference Series in Mathematics. American Mathematical Soc., 1997.
Amin Coja-Oghlan. On the laplacian eigenvalues of Gn,p. Combinatorics, Probability and Com-
puting, 16(6):923-946, 2007.
10
Published as a conference paper at ICLR 2020
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and Systems, 2(4):303-314,1989.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems 29, pp. 3844-3852. Curran Associates, Inc., 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in Neural Information Processing Systems 28, pp. 2224-2232. Curran
Associates, Inc., 2015.
Paul Erdos and AIfred Renyi. On random graphs I. PubUcationes Mathematicae (Debrecen), 6:
290-297, 1959.
Edgar N Gilbert. Random graphs. The Annals of Mathematical Statistics, 30(4):1141-1144, 1959.
C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: an automatic citation indexing system.
In Proceedings of the third ACM conference on Digital libraries, pp. 89-98. ACM, 1998.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263-1272.
PMLR, 2017.
Torben Hagerup and Christine Rub. A guided tour of Chernoff bounds. Information processing
letters, 33(6):305-308, 1990.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems 30, pp. 1024-1034. Curran Associates,
Inc., 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
CD Johnson. Stabilization of linear dynamical systems with respect to arbitrary linear subspaces.
Journal of Mathematical Analysis and Applications, 44(1):175-186, 1973.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097-
1105. Curran Associates, Inc., 2012.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as
cnns? In Proceedings of the IEEE International Conference on Computer Vision, pp. 9267-9276,
2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
11
Published as a conference paper at ICLR 2020
Yujia Li, Richard Zemel, and Marc Brockschmidt. Gated graph sequence neural networks. In
International Conference on Learning Representations, 2016.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. In Advances in Neural Information Processing Systems
32, pp.10943-10953. Curran Associates, Inc., 2019.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language
Processing, 2013.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163,
2000.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Carl D Meyer. Matrix analysis and applied linear algebra, volume 71. Siam, 2000.
Hrushikesh Narhar Mhaskar. Approximation properties of a multilayered feedforward artificial neu-
ral network. Advances in Computational Mathematics, 1(1):61-80, 1993.
Hai Nguyen, Shinichi Maeda, and Kenta Oono. Semi-supervised learning of hierarchical represen-
tations of molecules using neural message passing. arXiv preprint arXiv:1711.10168, 2017.
James R Norris. Markov chains. Number 2 in Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge university press, 1998.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019.
Kenta Oono and Taiji Suzuki. Approximation and non-parametric estimation of ResNet-type con-
volutional neural networks. arXiv preprint arXiv:1903.10047, 2019.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2010.
Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural
networks and fully-connected networks. arXiv preprint arXiv:1809.00973, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
Matus Telgarsky. Benefits of depth in neural networks. In 29th Annual Conference on Learning
Theory, volume 49 of Proceedings of Machine Learning Research, pp. 1517-1539. PMLR, 23-
26 Jun 2016.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open
source framework for deep learning. In Proceedings of Workshop on Machine Learning Sys-
tems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing
Systems (NIPS), 2015.
12
Published as a conference paper at ICLR 2020
Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji
Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki Yamazaki Vincent. Chainer: A deep learn-
ing framework for accelerating the research cycle. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 2002-2011. ACM, 2019.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
SaUrabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolUtional neUral net-
works. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, 2019.
Boris Weisfeiler and Lehman A.A. A redUction of a graph to a canonical form and an algebra arising
dUring this redUction. Nauchno-Technicheskaya Informatsia, 2(9):12-16, 1968.
Felix WU, Tianyi Zhang, AmaUri Holanda de SoUza Jr, Christopher Fifty, Tao YU, and Kilian Q
Weinberger. Simplifying graph convolUtional networks. arXiv preprint arXiv:1902.07153, 2019a.
Zonghan WU, ShirUi Pan, Fengwen Chen, GUodong Long, Chengqi Zhang, and Philip S YU. A
comprehensive sUrvey on graph neUral networks. arXiv preprint arXiv:1901.00596, 2019b.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. In Proceedings of
the 35th International Conference on Machine Learning, volUme 80 of Proceedings of Machine
Learning Research, pp. 5453-5462. PMLR, 2018.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In International Conference on Learning Representations, 2019.
Dmitry Yarotsky. Error boUnds for approximations with deep ReLU networks. Neural Networks,
94:103-114, 2017.
Jiawei Zhang. Gresnet: Graph residUals for reviving deep graph neUral nets from sUspended anima-
tion. arXiv preprint arXiv:1909.05729, 2019.
Lingxiao Zhao and Leman AkoglU. Pairnorm: Tackling oversmoothing in {gnn}s. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=rkecl1rtwB.
Ding-XUan ZhoU. Universality of deep convolUtional neUral networks. arXiv preprint
arXiv:1805.10769, 2018.
Pan ZhoU and Jiashi Feng. Understanding generalization and optimization performance of deep
CNNs. In Proceedings of the 35th International Conference on Machine Learning, volUme 80 of
Proceedings of Machine Learning Research, pp. 5960-5969. PMLR, 2018.
A Proof of Theorem 1
As we wrote in the main article, it is enoUgh to show the following lemmas (definition of mis-
Cellaneous variables are as in Section 3.2). Remember that λ = supn∈[N-M] ∣λn∣ and Slh is the
maximUm singUlar valUe of Wlh
Lemma 1.	For any X	∈	RN×C,	we	have dM (PX) ≤ λdM (X).
Lemma 2.	For any X	∈	RN ×C,	we	have dM (XWlh) ≤ slh dM (X).
Lemma 3.	For any X	∈	RN×C,	we	have dM (σ(X)) ≤ dM (X).
Proof of Lemma 1. Since	P is a symmetric linear operator on U⊥, we can choose the orthonormal
basis (em)m=M +i,...,n of U⊥ consisting of the eigenvalue of P|u⊥. Let λm be the eigenvalue of
P to which em is associated (m = M + 1,..., N). Note that since the operator norm of P|u⊥ is
λ, We have ∣λm∣ ≤ λ for all m = M +1,...,N. Since (em)m∈[N] forms the orthonormal basis of
13
Published as a conference paper at ICLR 2020
RN, We can uniquely write X ∈ RN×C as X = Pm=I em 0 Wm for some Wm ∈ RC. Then, We
have d2M(X) = PN=M+ι k Wm ∣∣2 where k ∙ k is the 2-norm of a vector. On the other hand, we have
N
PX = Pem 0 Wm
m=1
MN
= Pem 0 Wm +	Pem 0 Wm
m=1	m=M +1
MN
= Pem 0 Wm +	em 0 (λmWm)
m=1	m=M +1
Since U is invariant under P, for any m ∈ [M], we can write Pem as a linear combination of
en(n ∈ [M]). Therefore, we have d2M(PX) = PNm=M+1 ∣λmWm∣2. Then, we obtain the desired
inequality as follows:
N
d2M (PX) = X ∣λmWm ∣2
m=M +1
N
≤ λ2 X ∣Wm ∣2
m=M +1
N
≤ λ2 X ∣Wm ∣2
m=M +1
= λ2d2M (X).
□
Proof of Lemma 2. Using the same decomposition ofX as the proof in Lemma 1, we have
N
XWlh = X em 0 (Wl>hWm)
m=1
MN
= em0(Wl>hWm)+	em0(Wl>hWm).
m=1	m=M +1
Therefore, we have
N
d2M(XWlh) = X ∣Wl>h Wm ∣2
m=M +1
N
≤ sl2h X ∣Wm ∣2
m=M +1
= sl2h d2M (X).
□
Proof of Lemma 3. We choose (em)m=N-M+1,...,N as in the proof of Lemma 1. We denote X =
(Xnc)n∈[N],c∈[C] and en = (emn)m∈[N], respectively. Let (e0c)c∈[C] be the standard basis of RC.
Then, (en 0 e0c)n∈[N],c∈[C] is the orthonormal basis of RN×C, endowed with the standard inner
product as a Euclid space. Therefore, we can decompose X as X = PnN=1 PcC=1 ancen 0 e0c where
anc = hX, en 0 e0ci = PNm=1 Xmcemn. Then, we have d2M(X) = PnN=M+1∣ PcC=1ance0c∣2,
14
Published as a conference paper at ICLR 2020
which is further transformed as
N
d2M (X) = X
n=M +1
ance0c
c=1
NC
X	Xa2n
n=M +1 c=1
CN	M
=X Xa2nc-Xa2nc
c=1 n=1	n=1
CM
=X kX∙ck2- Xh"ni2
c=1	n=1
where Xc is the c-th column vector of X. Similarly, We have
CM
dM(σ(X)) = X kX++ k2 - XhX++, eni2 ,
c=1	n=1
where we denote σ(X) = (Xn+c)n∈[N],c∈[C] in shorthand. Therefore, the inequality follow from the
following lemma.	□
Lemma 4. Let x ∈ RN and v1, . . . , vM ∈ RN be orthonormal vectors (i.e., hvm, vni = δmn) satis-
fying vm ≥ 0forallm ∈ [M]. Then, we have kxk2 -PmM=1hx,vmi2 ≥ kx+ k2 - PmM=1 hx+ , vmi2
where x+ := max(x, 0) for x ∈ R.
Proof. The value kyk2 - PmM=1 hy, umi2 is invariant under simultaneous coordinate permutation of
y and um’s. Therefore, we can assume without loss of generality that the coordinate ofx are sorted:
xι ≤ ... ≤ XL < 0 ≤ xl+i ≤ ∙∙∙ ≤ XN for some L ≤ N. Then, we have
L
kXk2 - kX+ k2 = XX2n.
n=1
When L = 0, the sum in the right hand side is treated as 0. On the other hand, writing as vm
(vnm )n∈[N] , direct calculation shows
(1)
—
M / L L	∖ 2 L N
hX+ ,vmi2=	Xnvnm	- 2 Σ Σ XnXlvnmvlm
m=1 n=1	n=1 l=L+1
(2)
Let Im := {n ∈ [N] | vnm > 0} be the support of vm for m ∈ [M]. We note that ifm 6= m0 ∈ [M],
we have Im ∩ Im，= 0 since if there existed n ∈ Im ∩ Im，, we have
0 = hvm, vm0i ≥ vnmvnm0 > 0,
which is contradictory. Therefore,
NL	2N
X X Xnvnm =X( X
m=1 n=1	m=1 n∈Im∩[L]
2
xnvnm
≤ £ ( E	xn )( E Vnm )	(∙.∙ CaUChy-SChWarZ inequality)
m=1 n∈Im ∩[L]	n∈Im ∩[L]
≤ X ( X	Xn) C.∙kvmk2 = 1)
m=1	n∈Im ∩[L]
L
≤ X X2n .
n=1
(3)
15
Published as a conference paper at ICLR 2020
We used the fact that Im’s are disjoint and vnm = 0 if n 6∈ ∪mIm in the first equality above.
Further, we have xnxlvnmvlm ≤ 0 for 1 ≤ n ≤ L and L + 1 ≤ l ≤ N by the definition of L and
non-negativity of vm . By combining (1), (2), and (3), we have
ML
Xhx,vmi2 - hx+,vmi2 ≤ Xx2n = kxk2 - kx+k2.
m=1	n=1
□
Proof of Theorem 1. By Lemma 1, 2, and 3, we have
dM(力(X)) = dM(σ(…σ(σ( PX)W11)W12 …WlHl))
'-------------------{---}
H times
≤ dM(σ(…σ(σ( PX)Wl1)Wl2 …)WlHl)
'--------{----}
H -1 times
≤ slHl-idM(σ(…σ(σ( PX)Wl1)Wl2 …)WlHl-i))
'--------------{---}
H-1 times
• • •
≤ YHl slh dM (PX)
≤ sldM (PX)
≤ slλdM(X).
□
B Proof of Proposition 1
Proof. Let μι ≤ •… ≤ "n be the eigenvalue of the augmented normalized Laplacian ∆, sorted
in ascending order. Since P = IN - ∆, it is enough to show μι = •…="m = 0, "m +1 > 0,
and μN < 2. For the first two, the statements are equivalent to that ∆ is positive semi-definite and
that the multiplicity of the eigenvalue 0 is same as the number of connected components 6. This
is well-known for Laplacian or its normalized version (see, e.g., Chung & Graham (1997)) and the
proof for ∆ is similar. By direct calculation, we have
x>∆ X
1N
2 X aij
i,j=1
—
for any X = [xi •…	XN]> ∈ RN. Therefore, ∆ is positive semi-definite and hence μι ≥ 0.
Suppose temporally that G is connected. If x ∈ RN is an eigenvector associated to 0, then, by
the aformentioned calculation, √x+τ and √j^ must be same for all pairs (i,j) such that aj >
0. However, since G is connected, -^== must be same value for all i ∈ [N]. That means the
di+1
multiplicity of the eigenvalue 0 is 1 and any eigenvector associated to 0 must be proportional to
KIT TL T	1	Tl ʃ	,1	. T 7-	TT-	T . ʌ	1 .t	. 1
D 21. Now, suppose G has M connected components V1,..., VM. Let ∆m be the augmented
normalized Laplacians corresponding to each connected component Vm for m ∈ [M]. By the
aformentioned discussion, ∆m has the eigenvalue 0 with multiplicity 1. Since ∆ is the direct sum
of ∆ms, the eigenvalue of ∆ is the union of those for ∆m s. Therefore, ∆ has the eigenvalue 0 with
1
multiplicity M and em = D 2 ImSare the orthogonal basis of the eigenspace.
6The former statement is identical to Lemma 1 and latter one is the extension of Lemma 2 of Wu et al.
(2019a).
16
Published as a conference paper at ICLR 2020
Finally, We prove M < 2. Let μN be the largest eigenvalue of the normalized Laplacian ∆
D- 1 (D 一 A)D-2, where D- 1 ∈ RN×N is the diagonal matrix defined by
D-2 _ ʃdeg(i)-1
Dii = 0
(if deg(i) 6=0)
(if deg(i) =0)
Note that D-2 D 1 nor D 1 D- 1 are not equal to the identity matrix IN in general. However, we
have
L = D 2 D-1LD-1D 2
(4)
where L = D 一 A is the (unnormalized) Laplacian. Therefore, we have
x>∆ X
μN = maχ -Cj~77
N	x6=0	kxk
maχ
x6=0
x>D - 2 LD - 2 x
maχ
x6=0
x>D - 1 D 1 D- 1 LD-2 D1D -1 x
kxk
J (4))
maχ
x6=0
(D 2 D -1 x)>∆(D 2 D - 2 x)
maχ
x6=0
1 ~	1
D 2 D - 2 x=0
(D 2 D -1 x)>∆(D 2 D - 2 x)
kxk
(D 2 D - 2 x)>∆(D 2 D - 2 x) IID 2 D - 2 Xk
max -----------——1------------∏-∏---
I x=0	∣∣D2D-2Xk	IlXll
D 2 D - 2 x = 0
max
x6=0
1 ~ _ 1
D 2 D - 2 χ=0
(D 2 D -1 x)>∆(D 2 D - 2 x)
1 ~	1	..
∣∣D 2 D - 2 Xk
max
x6=0
1 ~ _ 1
D 2 D- 2 χ=0
11
∣∣D 2 D - 2 Xk
≤ max y>∆y
y6=0 IyI
max
x6=0
11
∣∣D 2 D - 2 Xk
≤
1
d deg⑺ V
Hn max ----------
n∈[N] 'deg(i) + 1)
≤ μN.
Therefore, we have μN ≤ μN7. Since maXi∈[N] ^^)?]) 2 < 1, the equality μN = μN holds
if and only if μN = 0, that is, G has N connected components. On the other hand, it is known
that μN ≤ 2 and the equality holds if and only if G has non-trivial bipartite graph as a connected
component (see, e.g., Chung & Graham (1997)). Therefore, μN = μN and μN = 2 does not hold
simultaneously and we obtain μN < 2.	□
C Counterexample of Previous Study on Over-smoothing for
Non-linear GNNs
We restate Theorem 1 of the preprint (version2) of Luan et al. (2019)7 8. Let G be a simple undirected
graph with N nodes and k connected components such that it does not have a bipartite component.
Let L = D-1/2AD-1/2 ∈ RN×N be the augmented normalized Laplacian of G. Let F ∈ N? and
7Theorem 1 of Wu et al. (2019a) showed that this inequality strictly holds when G is simple and connected.
We do not require this assumption.
8https://arxiv.org/abs/1906.02174v2
17
Published as a conference paper at ICLR 2020
Wn ∈ RF×F be the weight of the n-th layer for n ∈ N+. For the input X ∈ RN×F, we define
the output Yn ∈ RN×F of the n-th layer of a GCN by Yn = σ(L …σ(LXW0)…Wn) where σ
is the ReLU function. We assume the input X is drawn from a continuous distribution on RN×F .
Then, the theorem claims that we have limn→∞ rank(Yn) = k almost surely with respect to the
distribution of X .
We construct a conterexample. Consider a graph G consisting of N = 4 nodes whose adjacency
matrix is
1
1
1
1
A
111
110
110
001
Note that G is connected (i.e., k = 1) and is not bipartite. We make a GCN with F = 3 channels
and whose weight matrices are Wn = I3 (the identity matrix of size 3) for all n ∈ N. For the
distribution of the input X , we consider an absolutely continuous distribution with respect to the
Lebesgue measure on R4×3 such that P (X ≥ 0) > 0 (here, X ≥ 0 means the element-wise
comparison). For example, the standard Gaussian distribution satisfies the condition.
Since L ≥ 0, we have Yn = LnX if X ≥ 0. Let L = P >ΛP be the diagonalization of L where
P ∈ O(4) is an orthogonal matrix of size 4. Since rank(L) = 3, we have rank(Λn) = 3 for any n
(we can assume that Λ44 = 0 without loss of generality). Therefore, under the condition X ≥ 0, we
have
rank(Yn) = 3 o rank(P>ΛnPX) = 3
^⇒ X ∈ {P-1 [B v]> | B ∈ R3×3 is invertible, v ∈ R3}.
Note that the last condition is independent of n. Since the set of invertible matrices is dense in the
set of all matrices of the same size (with respect to the standard topology of the Euclidean space),
we have P({rank(Yn) = 3 for all n ∈ N}) > 0. Therefore, we have limn→∞ rank(Yn) = 3 with a
non-zero probability.	□
D	Proof of Theorem 3
We follow the proof of Theorem 2 of Chung & Radcliffe (2011). The idea is to relate the spectral
distribution of the normalized Laplacian with that of its expected version. Since we can compute
the latter one explicitly for the Erdos-Renyi graph, We can derive the convergence of spectra. We
employ this technique and derive similar conclusion for the augmented normalized Laplacian.
First, we consider genral random graphs not restricted to ErdOs-Renyi graphs. Let N ∈ N+, and
P = (pij)i,j∈[N] be a non-negative symmetric matrix (meaning that pij ≥ 0 for any i, j ∈ [N]).
Let G be an undirected random graph with N nodes such that an edge between i and j is inde-
pendently present with probability pij . Let A and D be the adjacency and the degree matrices of
G, respectively (that is, Aij 〜Ber(pifj), i.i.d.). Define the expected node degree of node i by
ti := PN=I Pij. Let A := A + IN, D := D + IN and define A := E[A] = P + IN and
D := E[D] = diag(t1, . . . , tN) + IN correspondingly. We define the augmented normalized Lapla-
cian ∆ of G by ∆ := IN - D- 1 AD- 1 and its expected version by ∆ := IN - D- 1 AD- 1
9. For a symmetric matrix X ∈ RN, we define its eigenvalues, sorted in ascending order by
λι(X) ≤ ∙∙∙ ≤ λN (X) and its operator norm by ∣∣X ∣∣ = maXn∈[N ] ∣λn(X )|.
Lemma 5 (Ref. Chung & Radcliffe (2011) Theorem 2). Let δ := minn∈[N] tn be the minimum
expected degree of G. Set k(ε) := 3(1 + log(4∕ε)). Then, for any ε > 0, if δ +1 > k(ε) log N, we
have
ma^λn (∆) - λn(A)l ≤ 4rUog≡U
n∈[N]	δ + 1
with probability at least 1 - ε.
9
9 Note that E[∆] 6= ∆ in general due to the dependence between A and D.
18
Published as a conference paper at ICLR 2020
Proof. By Weyrs theorem, We have maXn∈[N]卜n(∆) - λn(Z∆)∣ ≤ ∣∣2∆ - A∣∣. Therefore, it is
enough to bound ∣∆ - A∣∣. Let C := IN — D- 1 AD. By the triangular inequality, we have
∣∣A - Ak ≤ ∣∣A - Ck + ∣∣C - A∣∣. We will bound these terms respectively.
First, we bound ∣∣C-A ∣∣. Direct calculation shows C-A = -D - 1 (A-P )D - 2. Let Eij ∈ RN ×n
be a matrix defined by
(Eij )kl =	10
if (i = k and i = l) or (i = l and j = k),
otherwise.
We define the random variable Yij by
Y-. ：=_Aij - Pij_Eij
ij:	√mpjn	.
Then, Yij's are independent and we have C - A = PNj=I Yij. To apply Theorem 5 of Chung &
Radcliffe (2011) to Yij ’s, we bound ∣Yij - E[Yij ]∣ and ∣ PiN,j=1 E[Yi2j ]∣. First, we have
Since
we have
∣Yij - E[Yij ]∣ = ∣Yij ∣ ≤
∣E ij∣
√ti + Iptj +1
≤ (δ + 1)-1.
E[Y2 ]=	Pij- P2j	[e ii + Ejj
[ ij] = (ti + 1)(tj + 1) IEii
(ifi 6= j),
(ifi=j),
N
X E[Yi2j]
i,j=1
N
X
i,j=1
〃 Pij — Pj ,、Eii
(ti + 1)(tj + 1)
max
i∈[N]
Pij - P2j
(ti + 1)(tj + 1)
≤ max
i∈[N]
Pij
(ti + 1)(tj + 1)
≤(δ+1)-1.
By letting a J J3log+NN/ε, M J (δ + 1)-1, v2 J (δ + 1)-1 and applying Theorem 5 of Chung
& Radcliffe (2011), we have
Pr(∣C - AIl >a) ≤ 2Nexp
a2
2(δ + 1)-1 + 2(δ + 1)-1a/3
—
≤ 2N exp
-_ 3log(4N∕ε))
I- 2(1 + a∕3))
By the definition of k(ε), we have a < 1 if δ + 1 > k(ε) log n. For such δ, we have
Pr(∣C - A∣ >a) ≤ 2Nexp (-^4^)
2(1 + a∕3)
≤ 2Nexp(- log(4N∕ε))	(√ a < 1)
ε
=-
2
(5)
19
Published as a conference paper at ICLR 2020
Next, We bound ∣∣z∆ - C∣∣. First, since a < 1, by Chernoff bound (see, e.g. Angluin & Valiant
(1979); Hagerup & RUb (1990))), we have
Pr(Idi - ti| > α(ti + 1)) ≤ 2exp
a2(ti + I)
3
≤ 2exP 卜∖)
ε
=----.
2N
Therefore, if ∣di - ti∣ ≤ a(t + 1), then we have
(√ ∣√x — 1∣ ≤ ∣x — 1∣ for x ≥ 0)
Therefore, by union bound, we have
怔-2 力 1-in ∣=mf nm-1 ≤a
with probability at least 1 - ε∕2. Further, since the eigenvalue of the augmented normalized Lapla-
cian is in [0,2] by the proof of Proposition 1, we have ∣∣In 一 A∣∣ ≤ 1. By combining them, we
have
IlA - Cll = Il(D-2力2 - In)(In - A)D2D-2 + (IN - A)(I -力1D-2)||
≤ Il(D-2D 1 - InIlIlD2D-11∣ + ∣∣I - D 1 D-21|
≤ a(a + 1) + a.	(6)
From (5) and (6), we have
~ 一 ~ 一
∣A - A∣≤∣A - Ck + IlC - A∣
≤ a + a(a +1) + a
≤ a + 3a
≤ 4a 「a < 1)
with probability at least 1 - ε by union bound.
□
Let N ∈ N+ and p > 0. In the case of the ErdOS-Renyi graph Gn,p, we should set P = P(JN - IN)
where JN ∈ RNXN are the all-one matrix. Then, we have A = PJN + (1 - P)IN, D = (Np -
p + 1)In, and A = NP-P+ι (NIN - JN). Since the eigenvalue of JN is N (with multiplicity 1)
and 0 (with multiplicity N - 1), the eigenvalue of A is 0 (with multiplicity 1) and NPNP+ι (with
multiplicity N - 1). For Gn,p, δ is the expected average degree (N - 1)p. Hence, we have the
following lemma from Lemma 5:
Lemma 6. Let A be its augmented normalized Laplacian ofthe ErdoS-Renyi graph Gn,p∙ For any
ε > 0, if Np- P+1 > k(ε) := 3(1 + log(4∕ε)) ,then, with probability at least 1 — ε, we have
ʌ / x ∖
max λi(A)一
i=2,...,N
Np
Np — p + 1
≤4
/3log(4N∕ε)
N Np — p + 1
Corollary 4. Consider GCN on Gn,p . Let Wi be the weight ofthe l -th layer of GCN and Sl be
the maximum singular value of Wi for l ∈ N+. Set S := SUPieN十.Let ε > 0. We define k(ε):=
3(1+log(4∕ε)) and l(N,p,ε) = n⅛ + 4√1∣∣NF. If NP-NI > ⅛(≡) and S ≤ l(N,ε)-1,
then, GCN on Gn,p satisfies the assumption of Theorem 2 with probability at least 1 — ε.
20
Published as a conference paper at ICLR 2020
ProofofTheorem 3. Since IoNpN = o(1), for fixed ε, We have
Np - p + 1 Np
log N > logN > k(ε)
for sufficiently large N. Further, Np → ∞ as N → ∞ when IoNN
(1 - p)2
Np - p + 1
for sufficiently large N. Hence.
= o(1). Therefore, we have
4N)
≤ Np ≤(7- 4√3)2 log
1-p
Np-p+ 1
≤ (7-4√3) Jz⅛
Np - p + 1
Therefore, we have l(N,p,ε) ≤ 7 JNp-N+? ∙ Therefore, if S ≤ IjINp-N；：；, then We have
S ≤ l(N,p, ε)-1.	□
E Miscellaneous Propositions
E.1	Invariance of Orthogonal Complement Space
Proposition 2. Let P ∈ RN×N be a symmetric matrix, treated as a linear operator P : RN → RN.
Ifa subspace U ⊂ RN is invariant under P (i.e., ifu ∈ U, then Pu ∈ U), then, U⊥ is invariant
under P, too.
Proof. For any u ∈ U⊥ and v ∈ U, by symmetry of P, we have
hPu, vi = (Pu)>v = u>P >v = u>Pv = hu, Pvi.
Since U is an invariant space ofP, we have Pv ∈ U. Hence, we have hu, Pvi = 0 because u ∈ U⊥.
We obtain Pu ∈ U⊥ by the definition of U⊥.	□
E.2 Convergence to Trivial Fixed Point
Let P ∈ RN×N be a symmetric matrix, Wl ∈ RC×C, Sl be the maximum singular value of Wl for
l ∈ N+. We define fl : RN×C → RN×C by fl(X) := σ(PXWl) where σ is the element-wise
ReLU function.
Proposition 3. Suppose further that the operator norm of P is no larger than λ, then we have
kfl (X)kF ≤ Sl λkX kF for any l ∈ N+. In particular, let S := supl∈N+ Sl. If Sλ < 1, then, Xl
exponentially approaches 0 as l → ∞.
Proof. Since λ is the operator norm of P|u⊥, the assumption implies that the operator norm of
P itself is no larger than λ. Therefore, we have kPXWl kF ≤ λkXWlkF ≤ SlλkXkF. On the
other hand, since σ(x)2 ≤ x2 for any x ∈ R, we have kσ(X)kF ≤ kXkF for any X ∈ RN×C.
Combining the two, we have Ilfl(X)∣∣f ≤ kPXWι∣∣F ≤ sιλ∣∣X∣∣f.	□
E.3 Strictness of Theorem 1
Theorem 1 implies that if Sλ ≤ 1, then, one-step transition fl does not increase the distance to M.
In this section, we first prove that this theorem is strict in the sense that, there exists a situation in
which Slλ > 1 holds and the distance dM increases by one-step transition fl at some point X.
Set N - 2, C — 1, and M - 1 in Section 3.2. For μ,λ > 0, we set
P- μ λ，« [0] ,U- {]χ |y=0}.
Then, by definition, we can check that the 3-tuple (P, e, U) satisfies the Assumptions 1 and 2. Set
M := U 0 R = U and choose W ∈ R so that W > λ-1. Finally define f : RN×c → RN×c by
f(X) := σ(PXW) where σ is the element-wise ReLU function.
21
Published as a conference paper at ICLR 2020
Proposition 4. We have dM(f(X)) > dM(X) for any X = [x1 x2]> ∈ R2 such that x2 > 0.
Proof. By definition, we have dM(X) = |x2|. On the other hand, direct calculation shows that
fι(X) = [(WμXι)+ (WλX2)+]> and dg(力(X)) = (WλX2)+ where x+ := max(x,0) for
X ∈ R. Since W > λ-1 and x2 > 0, we have dM(fι(X)) > "m(X).	□
Next, we prove the non-strictness of Theorem 1 in the sense that there exists a situation in which
sιλ > 1 holds and the distance d，M uniformly decreases by 力.Again, We set Set N J 2, C J 1,
and M J 1. Let λ ∈ (1, 2) and set
λ
2
PJ
1
-1
-1
1
ej√2[1]
|x=y
Then, we can directly show that 3-tuple (P, e, U) satisfies the Assumptions 1 and 2. Set W J 1.
Proposition 5. We have Wλ > 1 and dM(fl(X)) < dM(X) for all X ∈ R2.
Proof. First, note that e0 :=*[1 -1]> is the eigenvector of P associated to λ: Pe0 = λe0. For
X = ae + be0 (a, b Z 0), the distance between X and M is dM(X) = |b|. On the other hand, by
direct computation, we have
f(X) = σ(PXW)
0	√√2 i>
嗡0i>
(ifb≥0),
(if b < 0).
Therefore, the distance between f (X) and M is dM(f (X)) = λ∣b∣∕2. Since λ < 2, we have
dM(f (X)) < dM(X) for any X ∈ R2.	□
We have shown that the non-negativity of e (Assumption 1) is not a redundant condition in Section
6.1.
F Relation to Markov Process
It is known that any Markov process on finite states converges to a unique distribution (equilibrium)
ifit is irreducible and aperiodic (see, e.g., Norris (1998)). As we see in this section, this theorem is
the special case of Corollary 3.
Let S := {1, . . . , N} be a finite discrete state space. Consider a Markov process on S characterized
by a symmetric transition matrix P = (pij )i,j∈[N] ∈ RN×N such that P ≥ 0 and P 1 = 1 where 1
is the all-one vector. We interpret pij as the transition probability from a state i to j . We associate
P with a graph GP = (VP , EP ) by VP = [N] and (i, j) ∈ EP if and only if pij > 0. Since P
is symmetric, we can regard GP as an undirected graph. We assume P is irreducible and aperiodic
10. Perron - Frobenius, theorem (see, e.g., Meyer (2000)) implies that P satisfy the assumption of
Corollary 3 with M = 1.
Proposition 6 (Perron - Frobenius). Let the eigenvalues of P be λι ≤ •…≤ Xn. Then, we have
-1 < λ1, λN-1 < 1, and λN = 1. Further, there exists unique vector e ∈ RN such that e ≥ 0,
kek = 1, and e is the eigenvector for the eivenvalue 1.
Corollary 5. Let λ := maxn=ι,…,n-i ∣λn∣(< 1) and M := {e 0 W | W ∈ RC}. If sλ < 1, then,
for any initial valueX1 , Xl exponentially approaches M as l → ∞.
If we set C = 1 and Wl = 1 for all l ∈ N+, then, we can inductively show that Xl ≥ 0 for any
l ≥ 2. Therefore, we can interpret Xl as a measure on S. Suppose further that we take the initial
value X1 as X1 ≥ 0 and X1> 1 = 1 so that we can interpret X1 as a probability distribution on
S. Then, we can inductively show that Xl ≥ 0, Xl> 1 = 1 (i.e., Xl is a probability distribution on
10A symmetric matrix A is called irreducible if and only if GA is connected. We say a graph G is aperiodic
if the greatest common divisor of length of all loops in G is 1. A symmetric matrix A is aperiodic if the graph
GA induced by A is aperiodic.
22
Published as a conference paper at ICLR 2020
S), and Xl+1 = σ(PXlWl) = PXl for all l ∈ N+. In conclusion, the corollary is reduced to the
fact that if a finite and discrete Markov process is irreducible and aperiodic, any initial probability
distribution converges exponentially to an equibrilium. In addition, the the rate λ corresponds to the
mixing time of the Markov process.
G GCN Defined By Normalized Laplacian
In Section 4, we defined P using the augmented normalized Laplacian ∆ by P = IN - ∆. We
can alternatively use the usual normalized Laplacian ∆ instead of the augmented one to define P
and want to apply the theory developed in Section 3.2. We write the normalized Laplacian version
as P∆ := IN - ∆. The only obstacle is that the smallest eigenvalue λ1 of P∆ can be equal to
-1, while that of P is strictly larger than -1 (see, Proposition 1). This corresponds to that fact the
largest eigenvalue of ∆ is strictly smaller than 2, while that for ∆ can be 2. It is known that the
largest eigenvalue of ∆ is 2 if and only if the graph has a non-trivial bipartite connected component
(see, e.g., Chung & Graham (1997)). Therefore, we can develop a theory using the normalized
Laplacian instead of the augmented one in parallel for such a graph G.
In Section 5, we characterized the asymptotic behavior of GCN defined by the augmented normal-
ized Laplacian via its spectral distribution (Lemma 6 of Appendix D). We can derive a similar claim
for GCN defined via the normalized Laplacian using the original theorem for the normalized Lapla-
cian in Chung & Radcliffe (2011) (Theorem 7 therein). The normalized Laplacian version of GCN is
advantegeous over the one made from the augmented one because we know its spectral distribution
for broader range of random graphs. For example, Chung & Radcliffe (2011) proved the conver-
gence of the spectral distribution of the normalized Laplacian for Chung-Lu’s model (Chung & Lu,
2002), which includes power law graphs as a special case (see, Theorem 4 of Chung & Radcliffe
(2011)).
H Details of Experiment Settings
H.1 Experiment of Section 6.1
We set the eigenvalue ofP to λ1 = 0.5 and λ2 = 1.0 and randomly generated P until the eigenvector
e associated to λ2 satisfies the condition of each case described in the main article. We set W = 1.2
and used the following values for each case as P and e.
H.1.1 CASE 1
P
0.7469915
0.2499819
0.2499819
0.7530085
0.7028392
-0.71134876
e
H.1.2 CASE 2
P
0.6899574
-0.2426827
-0.2426827
0.8100426
0.61637234
-0.78745485
e
H.2 Experiment of Section 6.2
We randomly generated an ErdoS — Renyi graph Gn,p with N = 1000 and randomly generated a
one-of-K hot vector for each node and embed it to a C-dimensional vector using a random matrix
whose elements were randomly sampled from the standard Gaussian distribution. Here, K = 10
and C = 32. We treated the resulting single as the input signal X(0) ∈ RN×C. We constructed a
GCN with L = 10 layers and C channels. All parameters were i.i.d. sampled from the Gaussian
distribution whose standard deviation is same as the one used in LeCun et al. (2012)11 and multiplied
a scalar to each weight matrix so that the largest singular value equals to a specified value s. We
used three configurations (p, s) = (0.1, 0.1), (0.5, 1.0), (0.5, 10.0). λ of the generated GCNs are
0.063, 0.197, 0.194, respectively. See Appendix 6.2 for the results of other configurations of (p, s).
11This is the default initialization method for weight matrices in Chainer and Chainer Chemistry.
23
Published as a conference paper at ICLR 2020
Table 1: Dataset specifications. The threshold λ-1 in the table indicates the upper bound of Corol-
lary 2.	__________________________________________________________________________________
	#Node	#Edge	#Class	Chance Rate	Threshold λ-1
Cora	2708	5429	6	30.2%	1+3.62 X 10-3
CiteSeer	3312	4732	7	21.1%	1+1.25 X 10-3
PubMed	19717	44338	3	39.9%	1+9.57 X 10-3
Table 2: Dataset specifications for noisy citation networks. The threshold λ-1 in the table indicates
the upper bound of Corollary 2.
	Original Dataset	#Edge Added	Threshold λ-1
Noisy Cora 2500	Cora	2495	1.11
Noisy Cora 5000	Cora	4988	1.15
Noisy CiteSeer	CiteSeer	4991	1.13
Noisy PubMed	PubMed	24993	1.17
H.3 Experiment of Section 6.3
H.3.1 Dataset
We used the Cora (McCallum et al., 2000; Sen et al., 2008), CiteSeer (Giles et al., 1998; Sen et al.,
2008), and PubMed(Sen et al., 2008) datasets for experiments. We obtained the preprocessed dataset
from the code repository of Kipf & Welling (2017)12. Table 1 summarizes specifications of datasets
and their noisy version (explained in the next section).
The Cora dataset is a citation network dataset consisting of 2708 papers and 5429 links. Each paper
is represented as the occurence of 1433 unique words and is associated to one of 7 genres (Case
Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule
Learning, Theory). The graph made from the citation links has 78 connected components and the
smallest positive eigenvalue of the augmented Normalized Laplacian is approximately μ = 3.62 X
10-3. Therefore, the upper bound of Theorem 2 is λ-1 = (1 一 μ)-1 ≈ 1 + 3.62 × 1θ-3. 818 out
of 2708 samples are labelled as “Probabilistic Methods”, which is the largest proportion. Therefore,
the chance rate is 818/2708 = 30.2%.
The CiteSeer dataset is a citation network dataset consisting of 3312 papers and 4732 links. Each
paper is represented as the occurence of 3703 unique words and is associated to one of 6 genres
(Agents, AI, DB, IR, ML, HCI). The graph made from the citation links has 438 connected compo-
nents and the smallest positive eigenvalue of the augmented Normalized Laplacian is approximately
μ = 1.25 X 10-3. Therefore, the upper bound of Theorem 2 is λ-1 = (1 - μ)-1 ≈ 1 + 1.25 X 10-3.
701 out of 2708 samples are labelled as “IR”, which is the largest proportion. Therefore, the chance
rate is 701/3312 = 21.1%.
The PubMed dataset is a citation network dataset consisting of 19717 papers and 44338 links. Each
paper is represented as the occurence of 500 unique words and is associated to one of 3 genres
(“Diabetes Mellitus, Experimental”, “Diabetes Mellitus Type 1”, “Diabetes Mellitus Type 2”). The
graph made from the citation links has 438 connected components and the smallest positive eigen-
value of the augmented Normalized Laplacian is approximately μ = 9.48 X 10-3. Therefore, the
upper bound of Theorem 2 is λ-1 = (1 - μ)-1 ≈ 1 + 9.57 X 10-3. 7875 out of 19717 samples are
labelled as “Diabetes Mellitus Type 2”, which is the largest proportion. Therefore, the chance rate
is 7875/19717 = 39.9%.
H.3.2 Noisy Citation Networks
We summarize the properties of noisy citation networks in Table 2.
12https://github.com/tkipf/gcn
24
Published as a conference paper at ICLR 2020
∙0∙5∙0∙5∙0
NLL00
ΦD-π>cφo-ω
∙0∙5∙0∙5
NLL0
ΦD-π>cφo-ω
0.0 ι ι ι ι ι ι
0	500 1000 1500 2000 2500
Index
0.0
9595
2 110
①n-BAU①9山
95959
2 110 0
①n-BAU①6一山
Figure 5: Spectral distribution of Laplacian for the citation network datasets. Left: normalized
Laplacian. Right: augmented normalized Laplacian. Top: Cora and Noisy Cora (2500, 5000).
Bottom: CiteSeer and Noisy CiteSeer.
We created two datasets from the Cora dataset: Noisy Cora 2500 and Noisy Cora 5000. Noisy
Cora 2500 is made from the Cora dataset by uniformly randomly adding 2500 edges, respectively.
Since some random edges are overlapped with existing edges, the number of newly-added edges
is 2495 in total. We only changed the underlying graph from the Cora dataset and did not change
word occurences (feature vectors) and genres (labels). The underlying graph of the Noisy Cora
dataset has two connected components and the smallest positive eigenvalue is μ ≈ 9.62 X 10-2.
Therefore, the threshold of the maximum singular values of in Theorem 2 has been increased to
λ-1 = (1 一 μ)-1 ≈ 1.11. Similarly, Noisy Cora 5000 was made by adding 5000 edges uniformaly
randomly. The number of newly added edges is 4988 and the graph is connected (i.e., it has only 1
connected component). μ and λ are μ ≈ 1.32 × 10-1 and λ =(1 一 μ)-1 ≈ 1.15, respectively.
We made the noisy version of CiteSeer (Noisy CiteSeer) and PubMed (Noisy PubMed), in the sim-
ilar way, by adding 5000 and 25000 edges uniformly randomly to the datasets. Since some random
edges were overlapped with existing edges, 4991 and 24993 edges are newly added, respectively.
This manipulation reduced the number of connected component of the graph to 3. μ is approxi-
mately 1.11 × 10-1 (Noisy CiteSeer) and 1.43 × 10-1 (Noisy PUbMed) and λ-1 = (1 一 μ)-1 is
approximately 1.13 (Noisy CiteSeer) and 1.17 (Noisy PubMed), respectively. Figure 5 (right) shows
the spectral distribution of the augmented normalized Laplacian For comparison, we show in Figure
5 (left) the spectral distribution of the normalized Laplacian for these datasets13.
H.3.3 Model Architecture
We used a GCN consisting of a single node embedding layer, one to nine graph convolution layers,
and a readout operation (Gilmer et al., 2017), which is a linear transformation common to all nodes
in our case. We applied softmax function to the output of GCN. The output dimension of GCN
is same as the number of classes (i.e., seven for Noisy Cora 2500/5000, six for Noisy CiteSeer,
and three for Noisy PubMed). We treated the number of units in each graph convolution layer as
a hyperparameter. Optionally, we specified the maximum singular values s of graph convolution
13Due to computational resource problems, we cannot compute the spectral distributions for PubMed and
Noisy Pubmed.
25
Published as a conference paper at ICLR 2020
Table 3: Hyperparameters of the experiment in Section 6.3. X 〜LogUnif [10a, 10b] denotes the
random variable log10 X obeys the uniform distribution over [a, b]. “Learning rate” corresponds to
α when “Optimization algorithm” is Adam (Kingma & Ba, 2015).
Name	Value
Unit size	{10, 20,..., 500}
Epoch	{10,20,...,100}
Optimization algorithm	{SGD, MomentumSGD, Adam}
Learning rate	LogUnif [10-5, 10-2]
layers. The choice of s is either 0.5 (smaller than 1), s1 (in the interval {1 ≤ s < λ-1}), 3 and 10
(larger than λ-1). We used s1 = 1.05 for Noisy Cora 2500, Noisy CiteSeer, and Noisy PubMed,
and s1 = 1.1 for Noisy Cora 5000 and Noisy CiteSeer so that s1 is not close to the edges of the the
interval {1 ≤ s < λ-1}.
H.3.4 Performance Evaluation Procedure
We split all nodes in a graph (either Noisy Cora 2500/5000 or Noisy CiteSeer) into training, val-
idation, and test sets. Data split is the same as the one done by Kipf & Welling (2017). This
is a transductive learning (Pan & Yang, 2010) setting because we can use node properties of the
validation and test data during training. We trained the model three times for each choice of hyper-
paremeters using the training set and defined the objective function as the average accuracy on the
validation set. We chose the combination of hyperparameters that achieves the best value of objec-
tive function. We evaluate the accuracy of the test dataset three times using the chosen combination
of hyperparameters and computed their average and the standard deviation.
H.3.5 Training
At initialization, we sampled parameters from the i.i.d. Gaussian distribution. If the scale of maxi-
mum singular values s was specified, we subsequently scaled weight matrices of graph convolution
layers so that their maximum singular values were normalized to s. The loss function was defined
as the sum of the cross entropy loss for all training nodes. We train the model using the one of
gradient-based optimization methods described in Table 3.
H.3.6 Hyperprameters
Table 3 shows the set of hyperparameters from which we chose. Since we compute the repre-
sentations of all nodes at once at each iteration, each epoch consists of 1 iteration. We employ
Tree-structured Parzen Estimator (Bergstra et al., 2011) for hyperparameter optimization.
H.3.7 Implementation
We used Chainer Chemistry14, which is an extension library for the deep learning framework
Chainer (Tokui et al., 2015; 2019), to implement GCNs and Optuna (Akiba et al., 2019) for hy-
perparameter tuning. We conducted experiments in a signel machine which has 2 Intel(R) Xeon(R)
Gold 6136 CPU@3.00GHz (24 cores), 192 GB memory (DDR4), and 3 GPGPUs (NVIDIA Tesla
V100). Our implementation achieved 68.1% with Dropout (Srivastava et al., 2014) (2 graph convo-
lution layers) and 64.2% without Dropout (1 graph convolution layer) on the test dataset. These are
slightly worse than the accuracy reported in Kipf & Welling (2017), but are still comparable with it.
H.4 Experiment of Section 6.4
The experiment settings are almost same as the experiment in Section 6.3. The only difference is that
we did not train the node embedding layer, which we put before convolution layers of a GCN, while
we did in Section 6.3. This is because we wanted to see the the effect of convolution operations
14https://github.com/pfnet-research/chainer-chemistry
26
Published as a conference paper at ICLR 2020
on the perpendicular component of signals, while we interested in the prediction accuracy in real
training settings in the previous experiment.
I Additional Experiment Results
I.1	Experiment of Section 6.1
We show the vector field V for various W in Figure 6 (Case 1) and Figure 7 (Case 2). Parameters
other than W are same as experiments in Section 6.1 (detail values are available in Appendix H.1).
I.2	Experiment of Section 6.2
Figure 8 shows the relative log distance of signals and their upper bound for various edge probability
p and the maximum singular value s. Note that we generate a new graph for each configuration of
(p, s). Therefore, different configurations may have different graphs and hence different λ even they
have a same edge probability p in common.
I.3	Experiment of Section 6.3
I.3.1	Predictive Accuracy
Figure 9 shows the comparison of predictive performance in terms the maximum singular value
and layer size when the dataset is Noisy Cora 5000 (left) and Noisy Citeseer (right), respectively.
Concrete values are available in Table 4.
I.3.2	Transition of Maximum Singular Values
Figure 10-13 show the transition of weight of graph convolution layers during training when the
dataset is Noisy Cora 2500, Noisy Cora 5000, and Noisy CiteSeer, respectively. We note that the
result of 3-layered GCN from the Noisy Cora 2500 is identical to Figure 3 (right) of the main article.
I.4	Experiment of Section 6.4
Figure 14 shows the logarithm of relative perpendicular component and prediction accuracy on
Noisy Cora, Noisy CiteSeer, and Noisy PubMed datasets. We use Pearson R as a correlation co-
efficient. If GCNs have only one layer, it has more large relative perpendicular components (cor-
responding to right points in the figures) than GCNs which have other number of layers. The cor-
relation between the logarithm of relative perpendicular components and prediction accuracies are
0.827(p = 6.890 × 10-6) for Noisy Cora, 0.524(p = 1.771 × 10-2) for Noisy CiteSeer, and
0.679(p = 1.002 × 10-3) for Noisy PubMed, if we treat the one-layer case as outliers and remove
them.
27
Published as a conference paper at ICLR 2020
Figure 6: Vector field V for various W for Case 1. Top left: W = 0.5. Top right: W = 1.0.
Middle left: W = 1.2 (same as Figure 1 in the main article). Middle right: W = 1.5. Bottom left:
W = 2.0. Bottom right: W = 4.0. Best view in color.
-4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
28
Published as a conference paper at ICLR 2020
-∖-1.4
1.2
1.0
0.8
0.6
0.4
0.2
-∖-1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.8
1.2
1.0
0.6
D.4
D.2
Figure 7: Vector field V for various W for Case 2. Top left: W = 0.5. Top right: W = 1.0.
Middle left: W = 1.2 (same as Figure 1 in the main article). Middle right: W = 1.5. Bottom left:
W = 2.0. Bottom right: W = 4.0. Best view in color.
29
Published as a conference paper at ICLR 2020
Table 4: Comparison of performance in terms of maximum singular value of weights and layer size.
“U” in the right most column indicates the accuracy of GCN without weight normalization.
Noisy Cora 2500
Maximum Singular Value
Depth	1	1.05	3	10	U
1	0.389 ± 0.101	0.429 ± 0.090	0.552 ± 0.014	0.632 ± 0.007	0.587 ± 0.008
3	0.273 ± 0.051	0.309 ± 0.017	0.580 ± 0.058	0.661 ± 0.003	0.494 ± 0.041
5	0.319 ± 0.000	0.267 ± 0.059	0.462 ± 0.065	0.602 ± 0.004	0.326 ± 0.029
7	0.261 ± 0.076	0.262 ± 0.080	0.407 ± 0.021	0.501 ± 0.017	0.279 ± 0.129
9	0.261 ± 0.080	0.319 ± 0.000	0.284 ± 0.109	0.443 ± 0.014	0.319 ± 0.000
Noisy Cora 5000
Maximum Singular Value
Depth	1	1.1	3	10	U
1	0.301 ± 0.080	0.333 ± 0.099	0.557 ± 0.004	0.561 ± 0.019	0.555 ± 0.016
3	0.245 ± 0.066	0.247 ± 0.076	0.370 ± 0.041	0.587 ± 0.009	0.286 ± 0.066
5	0.274 ± 0.048	0.237 ± 0.070	0.257 ± 0.076	0.535 ± 0.031	0.319 ± 0.000
7	0.263 ± 0.080	0.297 ± 0.031	0.260 ± 0.074	0.339 ± 0.060	0.319 ± 0.000
9	0.262 ± 0.081	0.258 ± 0.064	0.262 ± 0.080	0.261 ± 0.082	0.318 ± 0.002
Noisy CiteSeer
Maximum Singular Value
Depth	0.5	1.1	3	10	U
1	0.461 ± 0.018	0.467 ± 0.012	0.490 ± 0.016	0.494 ± 0.006	0.495 ± 0.009
3	0.438 ± 0.027	0.436 ± 0.010	0.450 ± 0.019	0.462 ± 0.007	0.417 ± 0.061
5	0.285 ± 0.008	0.371 ± 0.016	0.373 ± 0.011	0.425 ± 0.007	0.380 ± 0.024
7	0.213 ± 0.006	0.282 ± 0.011	0.309 ± 0.012	0.385 ± 0.007	0.308 ± 0.012
9	0.182 ± 0.005	0.242 ± 0.030	0.303 ± 0.021	0.325 ± 0.003	0.229 ± 0.033
Noisy PUbmed
Maximum Singular Value
Depth	0.5	1.1	3	10	U
1	0.488 ± 0.039	0.636 ± 0.006	0.641 ± 0.010	0.632 ± 0.002	0.631 ± 0.010
3	0.442 ± 0.027	0.426 ± 0.026	0.658 ± 0.004	0.661 ± 0.005	0.631 ± 0.013
5	0.431 ± 0.033	0.431 ± 0.034	0.561 ± 0.083	0.641 ± 0.004	0.424 ± 0.093
7	0.428 ± 0.032	0.443 ± 0.051	0.449 ± 0.035	0.619 ± 0.011	0.440 ± 0.041
9	0.413 ± 0.009	0.438 ± 0.039	0.539 ± 0.052	0.569 ± 0.042	0.473 ± 0.031
30
Published as a conference paper at ICLR 2020
p=0.01, s=0.1, lambda=0.643
0
12 3
- - -
uus>460~i
iuuubs°iu>4-iuk60~i
-40
0.0	2.5	5.0	7.5	10.0
Layer Index
p=0.1, s=0.1, Iambda=O.195
O
Ooooo
1 2 3 4 5
- - - - -
8us3>6oη
0.0	2.5	5.0	7.5	10.0
Layer Index
p=0.9, s=0.1, lambda=0.021
Oooo
2 4 6
- - -
3uus-α 3≥a6o~∣
p=O.01, s=1.0, Iambda=O.649
0.0
-2.5
-5.0
-7.5
-10.0
0.0	2.5	5.0	7.5	10.0
Layer Index
p=0.1, s=1.0, Iambda=O.195
0 5 0 5 0 5
--1T
8us3>α 6
0.0	2.5	5.0	7.5	10.0
Layer Index
p=0.9, s=1.0, lambda=0.021
3uuas-α 3≥a-luκ6oη
Oooo
12 3 4
8uas≡3>=-luα CTB
p=0.01, s=10.0, Iambda=O.640
p=0.1, s=10.0, Iambda=O.196
6 4 2 0 2 4
- -
8us3>α CTB
0.0	2.5	5.0	7.5	10.0
Layer Index
p=0.9, s=10.0, lambda=0.021
T101520
0.0	2.5	5.0	7.5	10.0	0.0	2.5	5.0	7.5	10.0	0.0	2.5	5.0	7.5	10.0
Layer Index	Layer Index	Layer Index
Figure 8: The actual distance dM to the invariant space M and the upper bound inferred by Theorem
1. The edge probability p takes 0.01(top), 0.1, 0.9(bottom) and the maximum singular value s takes
0.1(left), 1.0,10(right). Blue lines are the log relative distance defined by y(l) := log dM(X；0))and
orange dotted lines are upper bound y(l) := l log(sλ), where X(0) is the input signal and X(l) is the
output of the l-th layer. Best view in color.
31
Published as a conference paper at ICLR 2020
♦本** ♦本**
s = 0.5
s = 1.1
s = 3.0
s = 10.0
Unnormalized
s = 0.5
s = 1.1
s = 3.0
s = 10.0
Unnormalized
Layer Size
♦ s = 0.5
一▲・ s = 1.05
-⅜- s = 3.0
"t" s = 10.0
Unnormalized
2	4	6	8
Layer Size
Figure 9: Effect of the maximum singular values of weights on predictive performance. Horizontal
dotted lines indicate the chance rates (30.2% for Noisy Cora 5000, 21.2% for Noisy CiteSeer, and
39.9% for Noisy PubMed). The error bar is the standard deviation of 3 trials. Left: Noisy Cora
5000. Right: Noisy CiteSeer. Bottom: Noisy Pubmed. Best view in color.
32
Published as a conference paper at ICLR 2020
Maximum Singular Value Maximum Singular Value
ω-πj> J-n6u∞EnE-Xπjw
2.000
1.975
1.950
1.925
1.900
---Layer 1
r: r: r: r: r: r: "	Layer	2
MMMHMMuuuwMu*a'' Lsyer3
一■ Layer 4
---Layer 5
---Layer 6
Layer 7
0	10	20	30	40
Iteration
二 Q 9 6
2 2 11
> J6uEnE-Xw
— — — — — — — — — — —------- Layer 1
——Layer 2
■ ■ ■ ■ Layer 3
一■ Layer 4
_------ Layer	5
----Layer 6
Layer 7
----Layer 8
Layer 9
2	4	6	8
Iteration
Figure 10: Transition of maximum singular values of GCN during training using Noisy Cora 2500.
Top left: 1 layer. Top right: 5 layers. Bottom left: 7 layers. Bottom right: 9 layers.
33
Published as a conference paper at ICLR 2020
二 Q 9
2 2 1
> Jn6uEnE-Xw
---Layer 1
——Layer 2
■■■■ Layer 3
2	4	6	8
Iteration
----Layer 1
——Layer 2
Layer 3
—" Layer 4
----Layer 5
2	4	6	8
Iteration
ω-πj> J-n6u∞EnE-XEW
Q 6∙6
2 11
---Layer 1
——Layer 2
Layer 3
一■ Layer 4
---Layer 5
——Layer 6
Layer 7
4	6	8
Iteration
E
/6
X
f 1.5
Q 6∙6
2 11
> J6uEnE-Xw
---Layer 1
—F Layer 2
---— — — — — — — — — — ―-- ■ ≡ Layer 3
_______________________— ■ Layer 4
.......................—Layer 5
---Layer 6
Layer 7
-Layer 8
Layer 9
4	6	8
Iteration
Figure 11: Transition of maximum singular values of GCN during training using Noisy Cora 5000.
Top left: 1 layer. Top right: 3 layers. Middle left: 5 layers. Middle right: 7 layers. Bottom: 9 layers.
34
Published as a conference paper at ICLR 2020
Noisy CiteSeer
Maximum Singular Value Maximum Singular Value
1.98
1.96
1.94
80
4 2 Q
2 2 2
①A Jn6u 访 EnLU-Xiλi
20	40	60
Iteration
O
—Layer 1
——Layer 2
Layer 3
—■ Layer 4
---Layer 5
0
20	40
Iteration
60
ω-πj> J-n6u∞EnE-Xπjw
2.00
1.95-
1.90
---Layer 1
——Layer 2
■■■■ Layer 3
一■ Layer 4
---Layer 5
---Layer 6
Layer 7
0
25	50	75	100
Iteration
Φ
n
To
> 2.5
tα
"5
6
C
in
E
n
E
'×
to
W
2.0
1.5
0
80
20	40	60
Iteration
Figure 12:	Transition of maximum singular values of GCN during training using Noisy CiteSeer.
Top left: 1 layer. Top right: 3 layers. Middle left: 5 layers. Middle right: 7 layers. Bottom: 9 layers.
35
Published as a conference paper at ICLR 2020
Noisy PubMed
O
4 3 2
> Jn6uEnE-Xw
20
O
4
O
6
6 5 4 3 2
①-πA J-n6u∞EnLU-Xπiλi
O
20
O
4
60
Iteration
Iteration
ω-πj> J-n6u∞EnE-Xπjw
1.90-
1.85
1.80
1.75
0	20	40
Iteration
----Layer 1
——Layer 2
Layer 3
—■ Layer 4
----Layer 5
60
Q 6∙6
2 11
> Jn6uEnE-Xw
----Layer 1
—F Layer 2
-----.-.-.-.-.-.-.-------.I---- Layer 3
一■ Layer 4
----Layer 5
---Layer 6
-------------------------------- Layer	7
20	40	60	80
Iteration
⅛ 2.05
---Layer 1
——Layer 2
- J - - ■ ≡ Layer 3
一■ Layer 4
Layer 5
一 ■ 一 ■ 一 ■ 一 ■ 一 ■ 一 '” — — Layer 6
7jπ-π- ■ ■ Layer 7
-------Layer 8
Layer 9
0	20	40	60
Iteration
Figure 13:	Transition of maximum singular values of GCN during training using Noisy PubMed.
Top left: 1 layer. Top right: 3 layers. Middle left: 5 layers. Middle right: 7 layers. Bottom: 9 layers.
36
Published as a conference paper at ICLR 2020
Noisy CiteSeer (R=0.580, p=2.367e-03)
0.45
NoiSy PUbMed (R=O.416, p=3.860e-02)
0.40
I
0.60
0.35
A
u
2
§ 0.30
<
0.25
× ×
×
0.20
• s=0.5
▲ s=1.05
♦	s=3.0
■ s=10.0
× unnormalized
0.55
A
u
2
n
⅛0.50
0.45-
*
0.40	▲
• s=0.5
▲ s=1.05
♦	s=3.0
■ s=10.0
× unnormalized
0
1
Log of Relative Perpendicular Component
-2.0 -1.5 -1.0 -0.5 0.0	0.5	1.0	1.5
Log of Relative Perpendicular Component
Figure 14:	Logarithm of relative perpendicular component and prediction accuracy. Left: Noisy
CiteSeer. Right: Noisy PubMed. p in the title represents the p-value for the Pearson R coefficients.
37