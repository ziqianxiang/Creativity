Published as a conference paper at ICLR 2020
Influence-Based Multi-Agent Exploration
Tonghan Wang； Jianhao Wang*, Yi Wu & Chongjie Zhang
Institute for Interdisciplinary Information Sciences
Tsinghua University
Beijing, China
wangth18@mails.tsinghua.edu.cn, wjh720.eric@gmail.com
jxwuyi@openai.com, chongjie@tsinghua.edu.cn
Ab stract
Intrinsically motivated reinforcement learning aims to address the exploration
challenge for sparse-reward tasks. However, the study of exploration methods
in transition-dependent multi-agent settings is largely absent from the literature.
We aim to take a step towards solving this problem. We present two exploration
methods: exploration via information-theoretic influence (EITI) and exploration
via decision-theoretic influence (EDTI), by exploiting the role of interaction in
coordinated behaviors of agents. EITI uses mutual information to capture the
interdependence between the transition dynamics of agents. EDTI uses a novel
intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the
influence of one agent’s behavior on expected returns of other agents. By optimiz-
ing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate
their exploration and learn policies to optimize the team performance. We show
how to optimize these regularizers so that they can be easily integrated with pol-
icy gradient reinforcement learning. The resulting update rule draws a connection
between coordinated exploration and intrinsic reward distribution. Finally, we
empirically demonstrate the significant strength of our methods in a variety of
multi-agent scenarios.
1	Introduction
Reinforcement learning algorithms aim to learn a policy that maximizes the accumulative reward
from an environment. Many advances of deep reinforcement learning rely on a dense shaped reward
function, such as distance to the goal (Mirowski et al., 2016; Wu et al., 2018), scores in games (Mnih
et al., 2015) or expert-designed rewards (Wu & Tian, 2016; OpenAI, 2018), but they tend to strug-
gle in many real-world scenarios with sparse rewards (Burda et al., 2019). Therefore, many recent
works propose to introduce additional intrinsic incentives to boost exploration, including pseudo-
counts (Bellemare et al., 2016; Tang et al., 2017; Ostrovski et al., 2017), model-learning improve-
ments (Burda et al., 2019; Pathak et al., 2017; Burda et al., 2018), and information gain (Florensa
et al., 2017; Gupta et al., 2018; Hyoungseok Kim, 2019). These works result in significant progress
in many challenging tasks such as Montezuma Revenge (Burda et al., 2018), robotic manipula-
tion (Pathak et al., 2018; Riedmiller et al., 2018), and Super Mario games (Burda et al., 2019;
Pathak et al., 2017).
Notably, most of the existing breakthroughs on sparse-reward environments have been focusing on
single-agent scenarios and leave the exploration problem largely unstudied for multi-agent settings -
it is common in real-world applications that multiple agents are required to solve a task in a coordi-
nated fashion (Cao et al., 2012; NoWe et al., 2012; Zhang & Lesser, 2011). This problem has recently
attracted attention and several exploration strategies have been proposed for transition-independent
cooperative multi-agent settings (Dimakopoulou & Van Roy, 2018; Dimakopoulou et al., 2018; Bar-
giacchi et al., 2018; Iqbal & Sha, 2019b). Nevertheless, hoW to explore effectively in more general
scenarios With complex reWard and transition dependency among cooperative agents remains an
open research problem.
* Equal Contribution.
1
Published as a conference paper at ICLR 2020
This paper aims to take a step towards this goal. Our basic idea is to coordinate agents’ exploration
by taking into account their interactions during their learning processes. Configurations where in-
teraction happens (interaction points) lie at critical junctions in the state-action space, through these
critical configurations can transit to potentially important under-explored regions. To exploit this
idea, we propose exploration strategies where agents start with decentralized exploration driven by
their individual curiosity, and are also encouraged to visit interaction points to influence the ex-
ploration processes of other agents and help them get more extrinsic and intrinsic rewards. Based
on how to quantify influence among agents, we propose two exploration methods. Exploration via
information-theoretic influence (EITI) uses mutual information (MI) to capture the interdependence
between the transition dynamics of agents. Exploration via decision-theoretic influence (EDTI) goes
further and uses a novel measure called value of interaction (VoI) to disentangle the effect of one
agent’s state-action pair on the expected (intrinsic) value of other agents. By optimizing MI or VoI
as a regularizer to the value function, agents are encouraged to explore state-action pairs where they
can exert influences on other agents for learning sophisticated multi-agent cooperation strategies.
To efficiently optimize MI and VoI, we propose augmented policy gradient formulations so that the
gradients can be estimated purely from trajectories. The resulting update rule draws a connection
between coordinated exploration and the distribution of individual intrinsic rewards among team
members, which further explains why our methods are able to facilitate multi-agent exploration.
We demonstrate the effectiveness of our methods on a variety of sparse-reward cooperative multi-
agent tasks. Empirical results show that both EITI and EDTI allow for the discovery of influential
states and EDTI further filter out interactions that have no effects on the performance. Our results
also imply that these influential states are implicitly discovered as subgoals in search space that
guide and coordinate exploration. The video of experiments is available at https://sites.
google.com/view/influence-based-mae/.
2	Settings
In our work, we consider a fully cooperative multi-agent task that can be modelled by a factored
multi-agent MDP G = hN, S, A, T, r, h, ni, where N ≡ {1, 2, ..., n} is the finite set of agents,
S ≡ ×i∈N Si is the finite set of joint states and Si is the state set of agent i. At each timestep,
each agent selects an action ai ∈ Ai at state s, forming a joint action a ∈ A ≡ ×i∈N Ai , resulting
in a shared extrinsic reward r(s, a) for each agent and the next state s0 according to the transition
function T(s0|s, a).
The objective of the task is that each agent learns a policy ∏i(a∕si), jointly maximizing team
performance. The joint policy π=hπ1 , . . . , πni induces an action-value function, Qext,π(s, a)=
Eτ [Pth=0 rt|s0=s, a0=a, π], and a value function V ext,π(s)= maxa Qext,π(s, a), where τ is the
episode trajectory and h is the horizon.
We adopt a centralized training and decentralized execution paradigm, which has been widely used
in multi-agent deep reinforcement learning (Foerster et al., 2016; Lowe et al., 2017; Foerster et al.,
2018; Rashid et al., 2018). During training, agents are granted access to the states, actions, (intrinsic)
rewards, and value functions of other agents, while decentralized execution only requires individual
states.
3	Influence-Based Coordinated Multi-Agent Exploration
Efficient exploration is critical for reinforcement learning, particularly in sparse-reward tasks. In-
trinsic motivation (Oudeyer & Kaplan, 2009) is a crucial mechanism for behaviour learning since
it provides the driver of exploration. Therefore, to trade off exploration and exploitation, it is com-
mon for an RL agent to maximize an objective of the expected extrinsic reward augmented by the
expected intrinsic reward. Curiosity is one of the extensively-studied intrinsic rewards to encourage
an agent to explore according to its uncertainty about the environment, which can be measured by
model prediction error (Burda et al., 2019; Pathak et al., 2017; Burda et al., 2018) or state visitation
count (Bellemare et al., 2016; Tang et al., 2017; Ostrovski et al., 2017).
While such an intrinsic motivation as curiosity drives effective individual exploration, it is often
not sufficient enough for learning in collaborative multi-agent settings, because it does not take
2
Published as a conference paper at ICLR 2020
into account agent interactions. To encourage interactions, we propose an influence value aims to
quantify one agent’s influence on the exploration processes of other agents. Maximizing this value
will encourage agents to visit interaction points more often through which the agent team can reach
configurations that are rarely visited by decentralized exploration. In next sections, we will provide
two ways to formulate the influence value with such properties, leading to two exploration strategies.
Thus, for each agent i, our overall optimization objective is:
Jθi [∏i∣∏-i,po] ≡ Vext,π(so) + Viint,π(so) + β ∙ Iπi∣i,	⑴
where p0(s0) is the initial state distribution, π-i is the joint policy excluding that of agent i, and
Viint,π (S) is the intrinsic value function of agent i, I-即 is the influence value, β > 0 is a weighting
term. In this paper, we use the following notations:
ri(s, a) = r(s, a) + Ui(si, a)	(2)
Viπ(s) = Vext,π(s) +Viint,π(s),	(3)
Qn(s, a) = ri(s, a) + X T(s0∣s, a)V-(s0),	(4)
where Ui(s%, a# is a curiosity-derived intrinsic reward, r%(s, a) is a sum of intrinsic and extrinsic
rewards, Viπ(s) and Qiπ (s, a) here contain both intrinsic and extrinsic rewards.
3.1	Exploration via Information-Theoretic Influence
One critical problem in our learning framework presented above is to define the influence value
I. For simplicity, we start with a two-agent case. The first method we propose is to use mutual
information between agents’ trajectories to measure one agent’s influence on other agents’ learning
processes. Such mutual information can be defined as information gain of one agent’s state transition
given the other’s state and action. Without loss of generality, we define it from the perspective of
agent 1:
MInI(S2； Si, A1∣S2, A2) =	X	pπ(s, a, s2) [logP-(s2∣s, a) — logpπ(s2∣s2,a2)],
s,a,s02 ∈(S,A,S2)
(5)
where s = (s1, s2) is the joint state, a = (a1, a2) is the joint action, and Si and Ai are the random
variables of state and action of agent i subject to the distribution induced by the joint policy π .
So We define I-ι as MI-ι(S2; S1,A1∣S2,A2) that captures transition interactions between agents.
Optimizing this objective encourages agent 1 to visited critical points where it can influence the
transition probability of agent 2. We call such an exploration method exploration via information-
theoretic influence (EITI).
Optimizing MI-1 with respect to the policy parameters θi of agent 1 is a little bit challenging,
because it is an expectation with respect to a distribution that depends on θ1. The gradient consists
of two terms:
VθιMIF(S2; S1,A1∣S2,A2)
Σ
s,a,s02 ∈(S,A,S2)
Vθ1 (p- (s, a, s02)) log
p(s2∣s, a)
p- (S2|s2 ,a2 )
+	p- (s, a,
s,a,s02 ∈(S,A,S2)
s02)Vθ1 log
p(s2∣s, a)
p- (S2|s2,a2)
(6)
While the second term is an expectation over the trajectory and can be shown to be zero (see Ap-
pendix B.1), it is unwieldy to deal with the first term because it requires the gradient of the stationary
distribution, which depends on the policies and the dynamics of the environment. Fortunately, the
gradient can still be estimated purely from sampled trajectories by drawing inspiration from the
proof of the policy gradient theorem (Sutton et al., 2000).
The resulting policy gradient update is:
Vθι Jθι (t) = (Ri- V-(St)) Vθι log∏θι (a1∣s1)	(7)
3
Published as a conference paper at ICLR 2020
where 号(St) is an augmented value function of R = Ph0=t rt{ and
ri = rt + u1 + β log p(s2+1t+1ι, s2,at,F .	(8)
p(st2+1|st2,at2)
The third term, which we call EITI reward, is 0 when the agents are transition-independent, i.e.,
when p(st2+1 |st1 , st2, at1 , at2) = p(st2+1 |st2, at2), and is positive when st1 , at1 increase the probability
of agent 2 translating to st2+1 . Therefore, the EITI reward is an intrinsic motivation that encourages
agent 1 to visit more frequently the state-action pairs where it can influence the trajectory of agent 2.
The estimation of p(st2+1 |st1 , st2, at1 , at2) and p(st2+1 |st2, at2) are discussed in Appendix C. We assume
that agents know the states and actions of other agents, but this information is only available during
centralized training. When execution, agents only have access to their local observations.
3.2	Exploration via Decision-Theoretic Influence
Mutual information characterizes the influence of one agent’s trajectory on that of the other and
captures interactions between the transition functions of the agents. However, it does not provide
the value of these interactions to identify interactions related to more internal and external rewards
(r). To address this issue, We propose exploration via decision-theoretic influence (EDTI) based on a
decision-theoretic measure ofI, called Value of Interaction (VoI), which disentangles both transition
and reward influences. VoI is defined as the expected difference between the action-value function
of one agent (e.g., agent 2) and its counterfactual action-value function without considering the state
and action of the other agent (e.g., agent 1):
VoInI(S2; S1,A1 lS2 ,A2) = X	pπ (S, a, s2) hQπ (S, a, s2) - Qπ,:(S2,a2,s2R ,
s,a,s02 ∈(S,A,S2)
(9)
where Q2π(S, a, S02) is the expected rewards (including intrinsic rewards) of agent 2 defined as:
Qn(s, a, s2) = r2(s, a) + Y Ep(SI |s, a, s2)V2π(s'),
s01
(10)
and the CoUnterfactUal action-value function Q∏,* (also includes intrinsic and extrinsic rewards) can
be obtained by marginalizing out the state and action of agent 1:
Q∏,1*(s2,a2,s2) = E pn(s；,a；|s2,a2)[r2(s；,s2,a1,a2)+ YEp(S1|s；,s2 ,a^,a2,s2 )V2n (s0)].
s'1,a't	si
(11)
Note that the definition of VoI is analogous to that of MI and the difference lies in that logp(∙)
measures the amount of information while Q measures the action value. Although VoI can be ob-
tained by learning Q2n(s, a) and Q2n(S2, a2) and calculating the difference, we propose to explicitly
marginalize out s； and a； utilizing the estimated model transition probability pn(s2∣S2, a2) and
p(S02 |s, a) to get a more accurate value estimate (Feinberg et al., 2018). The performance of these
two formulations are compared in the experiments.
Value functions Q and V used in VoI contains both expected external rewards and internal rewards,
which will not only encourage coordinated exploration by the influence between intrinsic rewards
but also filter out meaningless interactions which can not lead to extrinsic reward after intrinsic
reward diminishes. To facilitate the optimization of VoI, we rewrite it as an expectation over state-
action trajectories.
VoInI(S2; S1,A1∣S2,A2) = Eτ k(s, a) - r∏(s2,a2) + γ(1 - Ppn：s	Vn(s'),
p(S2 |s, a)
(12)
where rp (s2 ,a2) is the counterfactual immediate reward. The detailed proof is deferred to Appendix
B.2. From this definition, we can intuitively see how VoI reflects the value of interactions.彳2 (s, a)-
rp(s2,a2) and 1 - pπ(s2∣S2, a2)∕p(s2∣s, a) measure the influence of agent 1 on the immediate
reward and the transition function of agent 2, and V2n(s') serves as a scale factor in terms of future
value. Only when agent 1 and agent 2 are both transition- and reward-independent, i.e., when
pn(S'2 |S2, a2) = p(S'2|s, a) and r2n(S2, a2) = r2(s, a) will VoI equal to 0. In particular, maximizing
4
Published as a conference paper at ICLR 2020
VoI with respect to policy parameters θ1 will lead agent 1 to meaningful interaction points, where
V2π(s0) is high and s1, a1 can increase the probability that s0 is reached.
In this learning framework, agents initially explore the environment individually driven by its own
curiosity, during which process they will discover potentially valuable interaction points where they
can influence the transition function and (intrinsic) rewarding structure of each other. VoI highlights
these points and encourages agents to visit these configurations more frequently. As intrinsic re-
ward diminishes, VoI can gradually distinguish those interaction points which are necessary to get
extrinsic rewards.
3.2.1 Policy Optimization with VoI
We want to optimize Jθi with respect to the policy parameters θi , where the most cumbrous term
is Vθi VoI-i∣i. For brevity, We can consider a two-agent case, e.g., optimizing VoI2∣1 with respect
to the policy parameters θ∖. Directly computing the gradient Vθι VoI2∣1 is not stable, because
VoI2∣1 contains policy-dependent functions r∏(s2,a2),pπ(s2∣s2,a2), and Vn(s0) (see Eq. 12). To
stabilize training , we use target functions to approximate these policy-dependent functions, which
is a commonly used technique in deep RL (Mnih et al., 2015). With this approximation, we denote
g2(s, a) = r (s, a) — r-(s2,a2)+ YXT(s0∣s, a) (l — P (s2|：2,a2)) V—(s；, s2).	(13)
2	s0	p(s02|s,a)	2	1 2
where r2- , p- , and V2- are corresponding target functions. As these target functions are only period-
ically updated during the learning, their gradients over θ1 can be approximately ignored. Therefore,
from Eq. 12, we have
Vθι VoInI(S2; S1,A1∣S2,A2) ≈	X	(VθιPπ(s, a)) g2(s, a).	(14)
s,a∈(S,A)
Similar to the calculation of Vθi MI, we get the gradient at every step (see Appendix B.3 for proof):
Vθι Jθι (t) ≈ (RI- Vln(St)) Vθι log∏θι (a；|s1),	(15)
where Vn(St) is an augmented value function regressed towards R = phh= r； and
r1 = rt + u1 + β [u2 + Y (1 ― M+fSS2%) ) VP N' s2+l .	(16)
WA rail Qit-I-〜(1 — P (s2+i|s2，a2) 、V- t et+1 et+1'i the FnTT rzcrA
WeCall u2 + Y 11	p(st+%t ,s' ,at ,at)J V2 (SI ,s2 ) the EDTIreward.
3.3 Discussions
Scale to Large Settings: For cases with more than two agents, the VoI of agent i on other agents
can be defined similarly to Eq. 9, which is annotated with VoI-ni|i(S-0 i; Si, Ai|S-i, A-i), where
S-i and A-i are the state and action sets of all agents other than agent i. In practice, agents in-
teraction can often be decomposed to pairwise interaction so VoI-ni|i(S-0 i; Si, Ai|S-i, A-i) is well
approximated by the sum of values of pairwise value of interaction:
VoI-ni|i(S-0i;Si,Ai|S-i,A-i) ≈ X VoIjn|i(Sj0;Si,Ai|S-i,A-i).	(17)
j∈N,j6=i
Relationship between EITI and EDTI: EITI and EDTI gradient updates are obtained by
information- and decision-theoretical influence respectively. Therefore, it is nontrivial to derive
that part of the EDTI reward is a lower bound of the EITI reward:
p(S0-i|S-i,a-i)	p(S0-i|s,a)	0
P(S0-i|s, a)	— θg P(S-i|S-i,a-i),	8,α, S-
(18)
which easily follows given that log x ≥ 1 — 1/x for ∀x > 0. This draws a connection between EITI
and EDTI reward.
5
Published as a conference paper at ICLR 2020
Table 1: Baseline algorithms. The third column is the reward used to train the value func-
tion of PPO. ui and ucen are curiosity about individual state si and global state s, T1 =
log (p(s0-i|s,a)/p(s0-i|s-i,a-i)), T2 = 1 - p(s0-i|s-i, a-i)/p(s0-i|s, a), and ∆Q-i(s, a) =
Q-i(s, a) - Q-i(s-i, a-i). Social influence (Jaques et al., 2018) and COMA (Foerster et al., 2018)
are augmented with curiosity.
	Alg.		Reward	Description
Ours	EITI EDTI	r + Ui + βT r + Ui + β(u-i + γT2 V-i)	Influence-theoretic influence Decision-theoretic influence
Other Exploration Methods	random cen dec cen_control	r r + Ucen r+Ui r + ucen	Pure PPO Decentralized PPO with cen curiosity Decentralized PPO with dec curiosity Centralized PPO with cen curiosity
Ablations	r influence plusV shared_critic Q-Q	r + Ui + βU-i r+Ui + βV-i r + Ucen r + Ui + β∆Q-i(s, a)	Disentangle reward interaction Use other agents’ value functions PPO with shared V and cen curiosity EDTI without explicit counterfactual
Related Works	social COMA Multi	— — —	By Jaques et al. (2018) By Foerster et al. (2018) By Iqbal & Sha (2019b)
Comparing EDTI to Centralized Methods: Different from a centralized method which directly
includes value functions of other agents in the optimization objective, (i.e., by setting total reward
^ = r + Ui + β(u-i + γV-i), which is called PlusV henceforth), the EDTI reward for agent i
disentangles its contributions to values of another agents using a counterfactual formulation. This
difference is important for quantifying influence because the value of another agent does not just
contain the contributions from agent i, but also those of itself and third-party agents. Therefore,
EDTI is a kind of intrinsic reward assignment. Our experiments in the next section will compare
the performance of plusV against our methods, which verify the importance of the intrinsic reward
assignment.
4	Experimental Results
Our experiments aim to answer the following questions: (1) Can EITI and EDTI rewards capture
interaction points? If they can, how do these points change throughout exploration? (2) Can exploit-
ing these interaction points facilitate exploration and learning performance? (3) Can EDTI filter out
interaction points that are not related to environmental rewards? (4) What if only reward influence
between agents are disentangled? We evaluate our approach on a set of multi-agent tasks with sparse
rewards based on a discrete version of multi-agent particle world environment (Lowe et al., 2017).
PPO (Schulman et al., 2017) is used as the underlying algorithm. For evaluation, all experiments
are carried out with 5 different random seeds and results are shown with 95% confidence interval.
Demonstrative videos1 are available online.
Baselines We compare our methods with various baselines shown in Table 1. In particular, we carry
out the following ablation studies: i) r.influence disentangles immediate reward influence between
agents, (derivation of the associated augmented reward can be found in Appendix B.4. Reward influ-
ence in long term is not considered because it inevitably involves transition interactions) ii) PlusV as
described in Sec. 3.3. iii) Sharedxritic uses decentralized PPO agents with shared centralized value
function and thus is a cooperative version of MADDPG (Lowe et al., 2017) augmented with intrin-
sic reward of curiosity. iv) Q-Q is similar to EDTI but without explicit counterfactual formulation,
as described in Sec. 3.2. We also note that EITI is an ablation of EDTI which considers transition
interactions. PIUSY Shared_critic, Q-Q, and cen_control have access to global or other agents, value
functions during training. When execution, all the methods except cen_control only require local
state.
1 https://sites.google.com/view/influence- based- ma- exploration/
6
Published as a conference paper at ICLR 2020
Figure 1: Didactic examples. Left: task Pass. Two agents starting at the upper-left corner are only
rewarded when both of them reach the other room through the door, which will open only when at
least one of the switches is occupied by one or more agents. Middle: Secret-Room. An extension
of Pass with 4 rooms and switches. When the switch 1 is occupied, all the three doors turn open.
And the three switches on the right only control the door of its room. The agents need to reach the
upper right room to achieve any reward. Right: comparison of our methods with ablations on Pass.
Performance of Ablations on pass
Updates
4.1	Didactic Examples
We present two didactic examples of multi-agent cooperation tasks with sparse reward to explain
how EITI and EDTI work. The first didactic example consists of a 30 × 30 maze with two rooms
and a door with two switches (Fig. 1 left). In the optimal strategy, one agent should first step on
switch 1 to help the other agent pass the door, and then the agent that has already reached the right
half should further go to switch 2 to bring the remaining agent in. There are two pairs of interaction
points in this task: (switch 1, door) and (switch 2, door), i.e., transition probability of the agent near
door is determined by whether another agent is on one of the switch.
Fig. 1-right and Fig. 2-top show the learning curves of our methods and all the baselines, among
which EITL EDTL [—influence, Multi, and centralized control can learn the winning strategy and
ours learn much more efficiently. Fig. 2-bottom gives a possible explanation why our methods
work. EITI and EDTI rewards successfully highlight the interaction points (before 100 and 2100
updates, respectively). Agents are encouraged to explore these configurations more frequently and
thus have better chance to learn the goal strategy. EDTI reward considers the value function of the
other agent, so it converges slower than the EITI reward. In contrast, directly adding the other agent’s
intrinsic rewards and value functions is noisy (see ”plusV reward”) and confuses the agent because
these contain the effect of the other agent’s exploration. As for centralized control, global curiosity
encourages agents to try all possible configurations, so it can find environmental rewards in most
tasks. However, visiting all configurations without bias renders it inefficient - external rewards begin
to dominate the behaviors of agents after 7000 updates even with the help of centralized learning
algorithm. Our methods use the same information as centralized exploration but take advantages of
agents’ interactions to accelerate exploration.
In order to evaluate whether EDTI can help filter out noisy interaction points and accelerate explo-
ration, we conduct experiments in a second didactic task (see Fig. 1 middle). It is also a navigation
task in a 25 × 25 maze where agents are rewarded for being in a goal room. However, in this exper-
iment, we consider a case where there are four rooms and the upper right one is attached to reward.
This task contains 6 pairs of interaction points (switch 1 with each of the doors, each switch with
the door of the same room), but only two of them are related to external rewards, i.e., (switch 1,
door 1) and (switch 2, door 1). As Fig. 3-right shows, EITI agents treat three doors equally even
after 7400 updates (see Fig. 3 right, 7400 updates, top row). In comparison, although EDTI reward
suffers from noise in the beginning, it clearly highlight two pairs of valuable interaction points (see
Fig. 3 right, 7400 updates, bottom row) as intrinsic reward diminishes. This can explain why EDTI
outperforms EITI (Fig. 3 left).
7
Published as a conference paper at ICLR 2020
Team
Reward
Phase 1 Learning
Basic Dynamics
Phase 2 Exploration by exploiting interaction points
Phase 3 Optimal Policy
1000
800
600
400
200
----EDTI
----日Tl
----random
----cen
—dec
cen-control
socialinfluence
----COMA
—Multi
-200
Updates
100
2100
Agent 1
Received
IAgent 2
Received
Agent 1
Received
IAgent 2
Received
IAgent 2
Received
0.06
日Tl
Reward
EDTI
Reward
pl us V
Reward
0.03
∣0.00
9000
7000
Agent 1
Received



0
心

Figure 2: Development of performance of our methods compared to baselines and intrinsic reward
terms of EITI, EDTI, and plusV over the training period of 9000 PPO updates segmented into three
phases. ”Team Reward” shows averaged team reward gained in a episode, with a maximum of 1000.
It shows that only EITI, EDTI, and centralized control and Multi can learn the strategy during this
stage. ”EITI reward”, ”EDTI reward”, and ”plusV reward” demonstrate the evolving of correspond-
ing intrinsic rewards.
100 Updates
Agent 1
Agent 2
Agent 2
Agent 2
Agent 1
Agent 1
2900 Updates
7400 Updates
2
1
Figure 3: Left: performance comparison between EDTI and EITI on Secret-Room over 7400 PPO
updates. Right: EITI and EDTI terms of two agents after 100, 2900, and 7400 updates.
push-box
BuuellLlotBd ESH
0	2500 5000 7500
Updates
island
0	2500 5000 7500
Updates
large-island
BuuellLlotBd ESH
——EDTI
——EITI
rjnfluence
plusV
---shared critic
—
---Q-Q
0	1000 2000 3000 4000
Updates
5
4
3
Figure 4: Comparison of our methods against ablations for Push-Box, Island, and Large-Island.
Comparison with baselines is shown in Fig. 8 in Appendix D.
8
Published as a conference paper at ICLR 2020
4.2	Exploration in Complex Tasks
Next, we evaluate the performance of our methods on more complex tasks. To this end, we use three
sparse reward cooperative multi-agent tasks depicted in Fig. 7 of Appendix D and analyzed below.
Details of implementation and experiment settings are also described in Appendix D.
Push-Box: A 15 × 15 room is populated with 2 agents and 1 box. Agents need to push the box to
the wall in 300 environment steps to get a reward of 1000. However, the box is so heavy that only
when two agents push it in the same direction at the same time can it be moved a grid. Agents need
to coordinate their positions and actions for multiple steps to earn a reward. The purpose of this task
is to demonstrate that EITI and EDTI can explore long-term cooperative strategy.
Island: This task is a modified version of the classic Stag Hunt game (Peysakhovich & Lerer, 2018)
where two agents roam a 10 × 10 island populated with 9 treasures and a random walking beast for
300 environment steps. Agents can collect a treasure by stepping on it to get a team reward of 10 or,
by attacking the beast within their attack range, capture it for a reward of 300. The beast would also
attack the agents when they are too close. The beast and agent have a maximum energy of 8 and 5
respectively, which will be subtracted by 1 every time attacked. Therefore, an agent is too weak to
beat the beast alone and they have to cooperate. In order to learn optimal strategy in this task, one
method has to keep exploring after sub-optimal external rewards are found.
Large-Island: Similar to Island but with more agents (4), more treasures (16), and a beast with more
energy (16) and a higher reward (600) for being caught. This task aims to demonstrate feasibility of
our methods in cases with more than 2 agents.
Push-Box requires agents to take coordinated actions at certain positions for multiple steps to get
rewarded. Therefore, this task is particularly challenging and all the baselines struggle to earn
any reward (Fig. 4 left and Fig. 8 left). Our methods are considerably more successful because
interaction happens when the box is moved - agents remain unmoved when they push the box
alone but will move by a grid if push it together. In this way, EITI and EDTI agents are rewarded
intrinsically to move the box and thus are able to quickly find the optimal policy.
In the Island task, collecting treasures is a easily-attainable local optimal. However, efficient trea-
sures collecting requires the agents to spread on the island. This leads to a situation where attempting
to attack the beast seems a bad choice since it is highly possible that agents will be exposed to the
beast’s attack alone. They have to give up profitable spreading strategy and take the risk of being
killed to discover that if they attack the beast collectively for several timesteps, they will get much
more rewards. Our methods help solve this challenge by giving agents intrinsic incentives to appear
together in the attack range of the beast, where they have indirect interactions (health is part of the
state and it decreases slower when the two are attacked alternatively). Fig. 9 in Appendix D demon-
strates that our methods learn to catch the beast quickly, and thus have better performance (Fig. 8
right).
Finally, outperformance of our methods on Large-Island proves that they can successfully handle
cases with more than two agents.
In summary, both of our methods are able to facilitate effective exploration on all the tasks by
exploiting interactions. EITI outperforms EDTI in scenarios where all interaction points align with
extrinsic rewards. On other tasks, EDTI performs better than EITI due to its ability to filter out
interaction points that can not lead to more values.
We also study EDTI with only intrinsic rewards, discussion and results are included in Appendix A.
5	Related Works
Single-agent exploration achieves conspicuous success recently. Provably efficient methods are pro-
posed, such as upper confidence bound (UCB) (Jaksch et al., 2010; Azar et al., 2017; Jin et al.,
2018) and posterior sampling for reinforcement learning (PSRL) (Strens, 2000; Osband et al., 2013;
Osband & Van Roy, 2016; Agrawal & Jia, 2017). Given that these methods do not scale well to
large or continuous settings, another line of research has been focusing on curiosity-driven explo-
ration (Schmidhuber, 1991; Chentanez et al., 2005; Oudeyer et al., 2007; Barto, 2013; Bellemare
et al., 2016; Pathak et al., 2017; Ostrovski et al., 2017), and have shown impressive results (Burda
9
Published as a conference paper at ICLR 2020
et al., 2019; 2018; Hyoungseok Kim, 2019). In addition, methods based on variational informa-
tion maximization (Houthooft et al., 2016; Barron et al., 2018) and mutual information (Rubin et al.,
2012; Still & Precup, 2012; Salge et al., 2014; Mohamed & Rezende, 2015; Hyoungseok Kim, 2019)
have been proposed for single-agent intrinsically motivated exploration.
Although multi-agent reinforcement learning (MARL) has been making significant progresses in
recent years (Foerster et al., 2018; Lowe et al., 2017; Wen et al., 2019; Iqbal & Sha, 2019a; Sune-
hag et al., 2018; Son et al., 2019; Rashid et al., 2018), less attention has been drawn to multi-agent
exploration. Dimakopoulou & Van Roy (2018) and Dimakopoulou et al. (2018) propose poste-
rior sampling methods for exploration of concurrent reinforcement learning in coverage problems,
Bargiacchi et al. (2018) presents a multi-agent upper confidence exploration method for repeated
single-stage problems, and Iqbal & Sha (2019b) investigates methods to combine several decentral-
ized curiosity-driven exploration strategies. All these works focus on transition-independent set-
tings. Another Bayesian exploration approach has been proposed for learning in stateless repeated
games (Chalkiadakis & Boutilier, 2003). In contrast, this paper focuses on more general multi-agent
sequential decision making problems with complex reward dependencies and transition interactions
among agents.
In the literature of MARL, COMA (Foerster et al., 2018) shares some similarity with our decision-
theoretic EDTI approach in that both of them use the idea of counterfactual formulations. However,
they are quite different in terms of definition and optimization: (1) conceptually, EDTI measures
the influence of one agent on the value functions of other agents, while COMA quantifies individual
contribution to the team value; (2) EDTI is defined on counterfactual Q-value over state-action
pairs of other agents given its own state-action pair, while COMA uses the counterfactual Q-value
just over its own action without considering state information, which is critical for exploration;
(3) we explicitly derive the gradients for optimizing EDTI influence for coordinated exploration
in the policy gradient framework, which provides more accurate feedback, while COMA uses the
counterfactual Q value as a critic. Another line of relevant works (Oliehoek et al., 2012; de Castro
et al., 2019) propose influence-based abstraction to predict influence sources to help local decision
making of agents. In contrast, this paper presents two novel approaches that quantify and maximize
the influence between agents for enabling coordinated multi-agent exploration.
In addition, some previous MARL work has also studied intrinsic rewards. One notably relevant
work is Jaques et al. (2018), which models the social influence of one agent on other agents’ policies.
In contrast, EITI measures the influence of one agent on the transition dynamics of other agents.
Accompanying this distinction, EITI includes states of agents in the calculation of influence while
social influence dos not. Apart from that, the optimization methods are also different - We directly
derive the gradients of mutual information and incorporate its optimization in the policy gradient
frameWork, While Jaques et al. (2018) adds social influence reWard to the immediate environmental
reWard for training policies. Hughes et al. (2018) proposes an inequality aversion reWard for learning
in intertemporal social dilemmas. Strouse et al. (2018) uses mutual information betWeen goal and
states or actions as an intrinsic reWard to train the agent to share or hide their intentions.
6	Closing Remarks
In this paper, We study the multi-agent exploration problem and propose tWo influence-based meth-
ods that exploits the interaction structure. These methods are based on tWo interaction measures,
MI and Value of Interaction (VoI), Which respectively measure the amount and value of one agent’s
influence on the other agents’ exploration processes. These tWo measures can be best regraded
as exploration bonus distribution. We also propose an optimization method in the policy gradient
frameWork, Which enables agents to achieve coordinated exploration in a decentralized manner and
optimize team performance.
References
Shipra AgraWal and Randy Jia. Optimistic posterior sampling for reinforcement learning: Worst-case
regret bounds. In Advances in Neural Information Processing Systems, pp. 1184-1194, 2017.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
10
Published as a conference paper at ICLR 2020
70,pp. 263-272. JMLR. org, 2017.
EUgenio Bargiacchi, Timothy Verstraeten, Diederik Roijers, Ann Nowe, and Hado Hasselt. Learning
to coordinate with coordination graphs in repeated single-stage multi-agent decision problems. In
International Conference on Machine Learning, pp. 491-499, 2018.
Trevor Barron, Oliver Obst, and Heni Ben Amor. Information maximizing exploration with a latent
dynamics model. arXiv preprint arXiv:1804.01238, 2018.
Andrew G Barto. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learn-
ing in natural and artificial systems, pp. 17-47. Springer, 2013.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom SchaUl, David Saxton, and Remi MUnos.
Unifying coUnt-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
YUri BUrda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018.
YUri BUrda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-scale stUdy of cUriosity-driven learning. International Conference on Learning Represen-
tations, 2019.
Yongcan Cao, WenwU YU, Wei Ren, and GUanrong Chen. An overview of recent progress in the
stUdy of distribUted mUlti-agent coordination. IEEE Transactions on Industrial informatics, 9(1):
427-438, 2012.
Georgios Chalkiadakis and Craig BoUtilier. Coordination in mUltiagent reinforcement learning:
A bayesian approach. In Proceedings of the Second International Joint Conference on Au-
tonomous Agents and Multiagent Systems, AAMAS ’03, pp. 709-716, New York, NY, USA,
2003. ACM. ISBN 1-58113-683-8. doi: 10.1145/860575.860689. URL http://doi.acm.
org/10.1145/860575.860689.
NUttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement
learning. In Advances in neural information processing systems, pp. 1281-1288, 2005.
MigUel SUaU de Castro, Elena CongedUti, Rolf AN Starre, Aleksander Czechowski, and Frans A
Oliehoek. InflUence-based abstraction in deep reinforcement learning. In Adaptive, learning
agents workshop (Vol. 34)., 2019.
PrafUlla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John SchUlman, Szymon Sidor, YUhUai WU, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Maria DimakopoUloU and Benjamin Van Roy. Coordinated exploration in concUrrent reinforcement
learning. In International Conference on Machine Learning, pp. 1270-1278, 2018.
Maria DimakopoUloU, Ian Osband, and Benjamin Van Roy. Scalable coordinated exploration in
concUrrent reinforcement learning. In Advances in Neural Information Processing Systems, pp.
4219-4227, 2018.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based valUe estimation for efficient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.
Carlos Florensa, Yan DUan, and Pieter Abbeel. Stochastic neUral networks for hierarchical rein-
forcement learning. arXiv preprint arXiv:1704.03012, 2017.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
commUnicate with deep mUlti-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 2137-2145, 2016.
Jakob N Foerster, Gregory FarqUhar, Triantafyllos AfoUras, Nantas Nardelli, and Shimon Whiteson.
CoUnterfactUal mUlti-agent policy gradients. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
11
Published as a conference paper at ICLR 2020
Charles W Fox and Stephen J Roberts. A tutorial on variational bayesian inference. Artificial
intelligence review, 38(2):85-95, 2012.
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
reinforcement learning of structured exploration strategies. In Advances in Neural Information
Processing Systems, pp. 5302-5311, 2018.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Duenez-Guzman, Anto-
nio Garcla Castaneda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, et al. Inequity
aversion improves cooperation in intertemporal social dilemmas. In Advances in Neural Informa-
tion Processing Systems, pp. 3330-3340, 2018.
Yeonwoo Jeong Sergey Levine Hyun Oh Song Hyoungseok Kim, Jaekyeom Kim. Emi: Explo-
ration with mutual information. In Proceedings of the 36th International Conference on Machine
Learning. JMLR. org, 2019.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 2961-2970, 2019a.
Shariq Iqbal and Fei Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforce-
ment learning. arXiv preprint arXiv:1905.12127, 2019b.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro A Ortega, DJ Strouse,
Joel Z Leibo, and Nando de Freitas. Intrinsic social motivation via causal influence in multi-agent
rl. arXiv preprint arXiv:1810.08647, 2018.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Computer Science,
2014.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino,
Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in
complex environments. arXiv preprint arXiv:1611.03673, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. In Advances in neural information processing systems,
pp. 2125-2133, 2015.
Ann Nowe, Peter Vrancx, and Yann-MichaeI De Hauwere. Game theory and multi-agent reinforce-
ment learning. In Reinforcement Learning, pp. 441-470. Springer, 2012.
Frans Adriaan Oliehoek, Stefan J Witwicki, and Leslie Pack Kaelbling. Influence-based abstraction
for multiagent systems. In Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.
OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
12
Published as a conference paper at ICLR 2020
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems, pp. 3003-3011,2013.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based ex-
ploration with neural density models. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 2721-2730. JMLR. org, 2017.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computa-
tional approaches. Frontiers in neurorobotics, 1:6, 2009.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286,
2007.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787,
2017.
Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan
Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pp. 2050-2053, 2018.
Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve generalized stag hunts
better than selfish ones. In Proceedings of the 17th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 2043-2044. International Foundation for Autonomous
Agents and Multiagent Systems, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent rein-
forcement learning. In International Conference on Machine Learning, pp. 4292-4301, 2018.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de
Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing-
solving sparse reward tasks from scratch. arXiv preprint arXiv:1802.10567, 2018.
Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in mdps. In
Decision Making with Imperfect Decision Makers, pp. 57-74. Springer, 2012.
Christoph Salge, Cornelius Glackin, and Daniel Polani. Changing the environment based on em-
powerment as intrinsic motivation. Entropy, 16(5):2789-2819, 2014.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neu-
ral controllers. In Proc. of the international conference on simulation of adaptive behavior: From
animals to animats, pp. 222-227, 1991.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 5887-5896, 2019.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforce-
ment learning. Theory in Biosciences, 131(3):139-148, 2012.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000, pp.
943-950, 2000.
DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, and David J Schwab. Learning
to share and hide intentions using information regularization. In Advances in Neural Information
Processing Systems, pp. 10249-10259, 2018.
13
Published as a conference paper at ICLR 2020
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085-2087. Inter-
national Foundation for Autonomous Agents and Multiagent Systems, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-
man, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for
deep reinforcement learning. In Advances in neural information processing systems, pp. 2753-
2762, 2017.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. In International Conference on Learning Representations,
2019.
Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a
realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018.
Yuxin Wu and Yuandong Tian. Training agent for first-person shooter game with actor-critic cur-
riculum learning. ICLR, 2016.
Chongjie Zhang and Victor Lesser. Coordinated multi-agent reinforcement learning in networked
distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
14
Published as a conference paper at ICLR 2020
Appendix
A Intrinsic EDTI
Value of interaction (VoI) captures both transition and reward influence among agents, and it facil-
itates coordinated exploration by encouraging interactions. VoI contains influence of both intrinsic
and extrinsic rewards. Since single-agent literature has studied purely curiosity-driven learning and
gets cutting-edge performance (Burda et al., 2019), it is interesting to investigate the performance of
VoI given only intrinsic rewards.
Intuitively, intrinsic VoI distributes individual curiosity among team members and facilitates explo-
ration by encouraging agents to help each other to reach under-explored states. Specifically, we use
the following objective:
Jθi[∏il∏-i,po] ≡ Vext,π(so)+ Viint,π(So) + β ∙ VOWin	(19)
The corresponding augmented reward is:
rt = rt + u1 + β [u2 + Y(1 - U;2+11;2,71) Vint,-(s1+1,s2+1)l	(20)
p(st2+1|st1, st2, at1, at2)
We use this method (intrinsic EDTI) to train the agents on Pass, Secret-Room, Push-Box, and Island
and show the results in Fig. 5.
B	Mathematical Details
B.1	Gradient of Mutual Information
To encourage agents to exert influence on transitions of other agents, we optimize mutual informa-
tion between agent’s trajectories. In particular, in the following, we show that term 2 in Eq. 6 is
always zero.
T2
X	pπ(s, a, s2)Vθι log P(S2ls, a)
s,a,s2∈(S,A,S2)	P (S2|S2,a2)
— E Pπ(s, a,s2)Vθι logpπ(；2|；2,a2)
s,a,s02
- Pπ (s,
s,a,s02
C o CeI (Pn (S2ls2,a2D
, 2) Pn(s2∣S2,a2)
-
s,a,s02
Pn (S, a, s2) ▽
Pn(S2|s2,a2)	θ1
士	士 ∖	/ 士 I	∖	/ 士 |士\
S2,a2, s1,a1)P(s1∣s2, a2)∏θ1 (a1∣S1)
-
s,a,s02
Pn (s, a, ；2)
Pn (S2|s2,a2)
E P(S2|S2,a2,Sl,a1)P(Sl|S2,a2)Veiπθι (a；|S1)
s'1,a't
-
s2 ,a2 ,s02
Pn (S2,a2,S2)
Pn (S2|S2,a2)
E P(S2|S2, a2, S1, a"P(S"S2, a2)Vθ1πθ1 (a"s"
s'1,a'1
-	E	Pn(S2,a2) EP(S2|S2,a2,S；,a1)P(S；|S2,a2)Vei πθι	(a；|S1)
s2,a2,s'2	s3af
—	E Pn(S2, a2) E P(s1|s2,a2)vθιπθι (a1|Sl) EP(S2 |S2, a2,Sl,a1 )
s2,a2	sɪ,aɪ	s2
—	X Pn(s2, a2) X P(s"s2,a2)vθ1πθ1 (al|Sl) XP(S2|S2, a2,Sl,a1 )
s2,a2	sɪ,aɪ	s2
、---------{z--------}
=1
(21)
(22)
(23)
(24)
(25)
(26)
(27)
(28)
(29)
15
Published as a conference paper at ICLR 2020
①UUeEJoted EroφH
①UUeEJoted EroφH
Ooooo
0 5 0 5
2 11
φuluJOJJΦd EroφH
push-box
0	2500	5000	7500
Updates
0 5 0 5 0
0 7 5 2
əuuelujojjəd EroφH
——EITI
——EDTI
---EDTI-intrinsic
0	2500	5000	7500
Updates
Figure 5: Performance of intrinsic EDTI in comparison with EITI and EDTI on Pass, Secret-Room,
Push-Box, and Island.
=-Epn(s2,a2) Ep(S"s2,a2Wθ1∑>θι ㈤⑸)	(30)
s2,a2	sɪ	aɪ
=-X pπ (s2,a2) X P(SHS2,adVθ[1	(31)
S2,a2	sɪ
= 0	(32)
B.2	DEFINITION OF Value of Interaction
To capture both transition and reward interactions between agents and thereby achieve intrinsic re-
ward distribution, we propose a decision-theoretic measure called Value of Interaction. We start
from 2-agent cases and the following theorem gives the definition of V oI2|1 in the form of an ex-
pectation over trajectories, which is especially helpful in the derivation of the EDTI policy gradient
update shown Eq. 15.
Theorem 1. Value of Interaction of agent 1 on agent 2 is:
Voi∏∣ι(s2;S1,A1∣S2,A2) = Eτ k(s,a) -r∏(s2,a2) + Y(1 - Pp(s2lS2,a2)[ Vπ(s0),
p(S2|s,a)
(33)
where rp (s2,a2) is the CounterfactuaI immediate reward.
V oI2|1 can be defined similarly. To lighten notation in the proof, we define
Vn(S2|s1, s2,a1,a2) = Ep(S1∣s1, s2, a1, a2, s2)V2p(S1, s2)	(34)
sɪ
16
Published as a conference paper at ICLR 2020
rπ(s2,a2)= E Pπ(£；,。；|£2,。2犷2国，£2,。；,。2),	(35)
s；,a；
V2π,*(s2ls2,a2) = X pπ ⑸,a1ls2,ad X P(s1ls^,s2,aɪ,a2,s2)V2π (s1, s2).	(36)
s't,a'1	si
We first prove Lemma 1, which is used in the proof of Theorem 1.
Lemma 1.
E	Pπ(S1, s2,a1,adYEp(S2|s1, s2,a1,a2)V2π(s2|s2,a2)	(37)
s1,s2,a1 ,a2	s02
E pπ(s1,s2,a1,a2)γ E τ(si,s2∣s1,s2,a1,a2) ∙
s1 ,s2 ,a1 ,a2	s01 ,s02
(TS2,a2))Vn(s1,s2).
p(S02|S1, S2, a1, a2)
Proof.
E	pπ(s1, s2,a1,a2)Yɪ2p(s2ls1, s2,a1,a2)V2π(s2|s2,a2)	(38)
s1,s2,a1 ,a2	s02
E pπ(Si, S2,a1,a2)YEp(S2∣S1, S2,a1,a2) ∙	(39)
s1,s2,a1 ,a2	s02	
X Pπ(s1, al|S2,a2) Xp(S1|S；, s2, a；, a2, S2)Vn(S1, S2) s3a;	si	(40)
E pπ(Si, S2,ai,a2)YEp(S2∣Si, S2,ai,a2) ∙	(41)
si ,s2,ai ,a2	s02	
π ； ；	T(S0i, S02|Si；, S2, ai；, a2) π 0 0 X p (Si,al|S2,a2) > —	J~~V2 (Si,S2) %	V p(S2|Sl,S2,al,a2) si ,ai	si	(42)
π	T(S0i, S02 |Si, S2, ai, a2) 〉, p (Si,S2,ai,a2)Y >	-	∙ si,s2,ai,a2	s0i,s02 p(S02|Si, S2, ai, a2)	(43)
V2π(s1,s2) X pπ(Si,al|S2,a2)p(S2|Sl,S2,a；,a2) s't,a't	(44)
E pπ(Si, S2,ai,a2)γ E T(s1, s2∣si, S2, ai, a2) ∙	(45)
si ,s2,ai ,a2	s0i ,s02	
/；(干丁) [ Vn(Si,S2). p(S2 |Si, S2, ai, a2)	(46)
	□
We now give the proof of Theorem 1:
Proof.
VoI∏ι(S2; S1,A1∣S2,A2)	(47)
= X	pπ(s, a, s2) ∣Qπ(s, a, s2)- Q∏,1*(s2, a2, s2)]	(48)
s,a,s02 ∈(S,A,S2)
= E	pπ(si, S2, aι, a2)(r2(s1, S2,a1,a2)-注(S2,a2) +	(49)
s1,s2,	a1,a2
YEp(S2 |s1, s2,a1,a2)(Vπ(S2|s1, s2,a1,a2) - V2π,*(s2ls2,a2))	(50)
s02
= E	pπ(S1,S2,a1,a2)(r2(S1, S2,a1,a2)-道(s2,a2) +	(51)
s1,s2,a1,a2
17
Published as a conference paper at ICLR 2020
Y X T(s1,s2∣sι,S2,aι,a2)(1 - (P (s2ls2,a2) )½π(s；, s2)) (Lemma 1) (52)
0 0	p(s02 |s1 , s2, a1, a2)
s1,s2
ETh(S, a)-r∏(s2,a2)+ Y (1- Pn(s2 ls2,a2)) Vn(s0)l .	(53)
P(s2 |s, a)
□
B.3 Calculating Gradient of VoI
In order to optimize V oI with respect to the parameters of agent policy, in Sec. 3.2.1, we propose to
use target function and get:
Vθι VoInI(S2; S1,A1∣S2,A2) ≈ X	(Vθιpπ(s, a))[r2(s, a) - r-(s2,a2)+
s,a∈(S,A)
Y X T (s0∣s, a) (1 - p-(s2 ∣s2,a2) ) V-(s1,s2)].
s0	P(s02 |s, a)	2	1 2
(54)
We prove that Es a (Vθιpπ(s, a)) r-(s2,a2) is 0 in the following lemma.
Lemma 2.
(Vθ	(Vθι Pπ (s1,s2,a1,a2)) r-(s2,a2)=0.	(55)
s1,s2,a1,a2
Proof. Similar to the way that policy gradient theorem was proved by Sutton et al. (2000),
E	(Vθι Pn (si, s2,a1,a2)) r- (s2,a2)
s1,s2,a1,a2
Vθι	E Pn(si, s2, ai, a2)r-(s2,a2)
s1 ,s2,a1 ,a2
∞
X do(s1, SO) Y (vθi π(a1, a2 |s1, s2))T (s1+i, s2+1 |s1, a1, s2, a2)r2 (s2, a2)
s01,s02
t=o
∞
E do(s0,s2)∏ [∏(a1 ,a2∣s1, s2) (Vθι logn(a；, a2|s；,s2))
s01,s02
t=o
T(S1+i, s2+1ls1, a1, s2, a2)r2(S2, a2)]
E Pn (si, S2,ai,a2) (Vθι log ∏(ai,a2∣Sl, S2)) r-(s2, a2)
s1,s2,a1,a2
E Pn(si, S2,a1,a2) (Vθι log∏(aι∣sι, S2)) r-(s2, a2)
s1,s2,a1,a2
E Pn(s2,a2) E Pn(s1,a1∣s2,a2) (Vθι log∏(a1∣s1,s2)) r-(s2,a2)
s2,a2
s1,a1
E Pn(s2,a2)r-(s2,a2) E Pn(si,ai∣s2,a2)(Vθ1 log∏(a1∣s1,s2))
s2,a2	s1,a1
n	-	Pn(si, ai|s2, a2)
Pn P (s2,a2)r2 (s2,a2) 工-----/-1-----^(Vθιπ(a1ls1, s2))
Ea	S↑X π(ailsi,s2)
(56)
(57)
(58)
(59)
(60)
(61)
(62)
(63)
(64)
(65)
EPn(S2,adr-(s2,ad E
s2,a2	s1 ,a1
Pn(s1∣s2, a2)Pn(aι∣sι, S2,a2)
∏(aι∣Si, S2)
(Vθιπ(aι∣sι, S2))
(66)
EPn(S2,adr-(s2,ad E
s2,a2	s1 ,a1
Pn(si ∣S2, a2)∏(a1∣S1, S2)
∏(ai∣Sl,S2)
(Vθι∏(ai∣Si, S2))
(67)
18
Published as a conference paper at ICLR 2020
E pπ(s2, a2)r-(s2,a2) E pπ(si∣s2, a2)(Vθ1 ∏(aι∣sι, s2)) sa	sa	(68)
2 , 2	1 , 1 E Pπ(s2,a2)r-(s2,a2) EPn(s1∣s2,a2)^(Vθ1 ∏(aι∣s1,s2)) s2 ,a2	s1	a1	(69)
E pn(s2,a2)r-(s2,a2) EPn(s1∣s2,a2) Vθι E∏(aι∣sι,S2) s2 ,a2	s1	a1 ,	(70)
E Pn(s2,a2)r-(s2,a2) EPn(S1|s2,a2)(vΘi 1) sa	s	(71)
s2 ,a2	s1 0	(72)
□
B.4 Immediate Reward Influence
Similar to MI and V oI, we can define influence of agent 1 on the immediate rewards of agent 2 as:
RInI(S2； Si, Aι∣S2, A2) =	X pπ(s, a)[r2(s, a)-千2(s2,a2)].
s,a∈(S,A)
Use Lemma 2, we can get:
VθιRI∏ι(S2； S1,A1∣S2,A2)=	X	Vθι (pπ(s, a))r2(s, a).
s,a∈(S,A)
Now we have
Vθι Jθι (t) ≈ (Ri - VT(St)) Vθι log∏θι (a1∣s1),
where VIn(St) is an augmented value function of R = Ph,= r1( and
rt = rt + u1 + βu2∙
(73)
(74)
(75)
(76)
C Estimation of Conditional Probabilities
To quantify interdependence among exploration processes of agents, we use mutual information and
value of interaction. Calculations of MI and VoI need estimation of p(S02 |S2, a2) and p(S02 |s, a). In
practice, we track the empirical frequencies pemp(S02 |S2, a2) andpemp(S02|s, a) and substitute them
for the corresponding terms in Eq. 8 and 16.
Estimating pemp(S02 |S2, a2) and pemp(S02 |s, a) is one obstacle to the scalability of our method, we
now discuss how to solve this problem. When the state and action space is small, we can use
hash table to implement Monte Carlo method (MC) for estimating the distributions accurately. In
the MC sampling, We count from the samples the state frequencies p(s2∣s2,a2) ≡ NNjss2,：? and
p(s2∣s, a) ≡ NN2ss：a：a：2), where N (∙) is the number of times each state-action pair was visited
during the learning process. When the problem space becomes large, MC consumes large memory in
practice. As an alternative, we adopt variational inference (Fox & Roberts, 2012) to learn variational
distributions qξ1 (S02 |S2, a2) and qξ2 (S02 |s, a), parameterized via neural networks with parameters ξi
and ξ2 , by optimizing the evidence lower bound. In Fig. 6, we show the performance of EDTI
estimated using variational inference and the changing of associated EDTI rewards on Pass during
9000 PPO updates. Variational inference introduces some noise in EDTI rewards estimation and thus
requires slightly more steps to learn the true probability and the strategy. However, estimating using
MC sampling consumes 1.6G memory to save the hash table with 100M items each agent while
variational inference needs a three-layer fully connected network with 74800 parameters occupying
about 0.60M memory. This results highlights the feasibility of estimating EITI and EDTI rewards
using variational inference in problem with large state-action space.
19
Published as a conference paper at ICLR 2020
Figure 6: Left: Performance of EDTI (vi) (EDIT estimated using variational inference) compared
with EITI and EDTI estimated using MC sampling. Others: Development of EDTI (vi) rewards
during exploration process. Top row: EDTI (vi) rewards of agent 1; bottom row: EDTI (vi) rewards
of agent 2.
Ullll ll∣

IOO Updates 2100 Updates 3800 Updates 7000 Updates
Table 2: The scaling weights for different intrinsic reward terms in various tasks. βT is the weight
of term Ti (See Table 1). 3血 and βeχt are scaling factors to combine r and Ui in r. u— in r.influence
is scaled by βr while Vint and Vext in plusV are respectively scaled by βpUsV and βpXusV.
Task	η	βT	βint	βext	βr	e帮W	βPXUsV
Pass	10.
Secret-Room	10.
Push-Box	1.
Island	1.
Large-Island	1.
1—0.0.
.1.1.1.5
0000
1.1.000.
11
000.0
0.—0.0.
1 11
.0—.0.0
0. 0.0.
D	Implementation Details
D. 1 Network Architecture, Hyperparameters, and Infrastructure
We base our framework on OpenAI implementation of PPO2 (Dhariwal et al., 2017) and use its
default parameters to carry out all the experiments. We train our models on an NVIDIA RTX
2080TI GPU using experience sampled from 32 parallel environments. We use visitation count to
calculate the intrinsic reward, for its provable effectiveness (Azar et al., 2017; Jin et al., 2018). For
all our methods and baselines, We use η/，N(S) as the exploration bonus for N(s)-th visit to state
s. Specific values ofη and scaling weights can be found in Table 2.
As for variational inference, the inference network is a 3-layer fully-connected network coupled
with a 64-dimensional reparameterization estimator. ReLU is used as the activation function for the
first two layers and the sum of negative log-likelihood and negative Evidence Lower Bound is used
as loss. We use Adam optimizer (Kingma & Ba, 2014) with learning rate 1 × 10-3 and batchsize
2048. To speed up the learning of variational distributions estimation, we equip the learning with
proportional prioritized experience replay (Schaul et al., 2015).
D.2 Task S tructure
In this section, we describe the detailed settings of the experimental tasks.
Pass: There are two agents and two switches to open the door in a 30 × 30 grid. Only when at least
one of the switches are occupied will the door open. The agents need navigate from left to right and
the team reward, which is 1000, is only provided when all agents reach the target zone. Agents can
observe the position of another agents.
Secret-Room: This is an extension of the Pass task with 4 rooms and4 switches locating in different
rooms. The size of the grid is 25 × 25. When the left switch is occupied, all the three doors are open.
And the three switches in each room on the right only control the door of its room. The agents need
20
Published as a conference paper at ICLR 2020
• agent
A
■ box
$
■
agent A
・ agent B
agent
B
:	wolf
$
gu"5uuoEd ESF
push-box
200
150
100
50
0
0	2500	5000	7500
Updates
① uu"5uuo七0 d ESF
Figure 7: Task Push-Box, Island, and Large-Island
0	2500	5000	7500
Updates
----EDTI
----EITI
----random
----cen
dec
cen_control
----infl_MOA
Multi
----COMA
Figure 8:	Comparison of our methods against baselines on Push-Box (left), Island (right).
to navigate towards the desired room (in light red of Fig. 1 middle) to achieve the extrinsic team
reward 1000. Agents can observe the position of the other agents.
Push-Box: There are two agents and one box in a 15 × 15 grid. Agents need to push the box to the
wall. However, the box is so heavy that only when two agents push it in the same direction at the
same time can itbe moved a grid. The only team reward, 1000, is given when the box is placed right
against the wall. Agents can observe the coordinates of their teammate and the location of the box.
Island: A group of two agents are hunting for treasure on an island. However, a random walking
beast may attack the agents when they are too close. The agents can also attack the beast within their
attack range. This hurt doubles when more than one agent attack at the same time. Each agent has
a maximum health of 5 and will lose 1/n health per step when there are n agents within the attack
range of the beast. Island is a modified version of the classic coordination scenario Stag-Hunt with
local optimal, because finding each treasure (9 in total) will trigger a team reward of 10 but catching
the beast gives a higher team reward of 300. Agents can observe the position and health of each
other, and the coordinates of the beast. Fig. 9 shows the development of the probability of catching
the beast and the averaged number of treasures found in an episode during 9000 PPO updates.
Large-Island: Settings are similar to that of Island but with more agents (4), more treasures (16),
and a beast with more energy (16) and a higher reward (600) for being caught.
The horizon of one episode is set to 300 timesteps in all these tasks.
E	Comparis on with S ingle-Agent Exploration Methods
In this paper, we study the exploration problem in multi-agent settings from a decentralized per-
SPective. Alternatively, exploration can be carried out in a centralized manner - treating agents as
a joint one and using single-agent exploration algorithms. In this section, we compare our methods
with centralized exploration strategies using RND (Burda et al., 2018) and EMI (Hyoungseok Kim,
2019), which are among the most cutting-edge exploration algorithms driven by curiosity and based
on mutual information, respectively. We use codes published by their authors and carry out a modest
21
Published as a conference paper at ICLR 2020
2 」'°
至=qeqoq8
0	2500	5000	7500
Updates
2500	5000	7500
Updates
*.20
l°15
1 0.10
§0.05
e
0.00
/ ∖/ ,
O 2500	5000	7500
Updates
4 2 0
PUnoL SaJnSEJ. pω6ejω>v
2500	5000	7500
Updates
Figure 9:	Comparison of our methods against baselines and ablations on Island in terms of the
probability of catching the beast and the averaged treasures collected in an episode.
grid search over hyperparameters. For RND, we search intrinsic reward coefficient in the range of
[0.005, 1.0] and extrinsic reward coefficient in range [0.05, 2.0]. For EMI, we test difference com-
binations of loss weights. Results averaged over four random seeds with the best found parameters
are shown below.
pass	secret-room	push-box
O 2500	5000	7500
Updates
O 2000	4000	6000
Updates
O 2500	5000	7500
Updates
Figure 10: Comparison of our methods against centralized single-agent exploration algorithms on
Pass (left), Secret-Room (middle), and Push-Box (right).
Performance comparisons on problems of Pass, Secret-Room, and Push-Box are illustrated in
Fig. 10. We can observe that our methods significantly outperform centralized exploration strategies
using RND or EMI. To better understand this observation, we plot visitation heatmaps over time for
RND and EMI, respectively, in Fig. 11 and 12.
Fig. 11 shows visitation heatmaps of RND on the Pass problem. From Fig. 11 (b), we can see that
RND seems finding good policies for agents to pass the door in the first 4671 updates. However,
agents’ policies seem to collapse quickly after that and their visits scatter around rooms again, which
explains its learning curve in Fig. 10. From the evolution of its visitation heatmaps, we hypothesize
that after visiting the center of the room for many times, agents’ curiosity models overfit on a par-
ticular set of states and they start to be curious about the relatively unfamiliar transition dynamics
around the wall. As the result, the RND intrinsic reward drags the agents to the walls, as shown in
Fig. 11(c) and (d), and their performance quickly drops within several updates (i.e., update 4671-
4677 shown by Fig. 11(b-d)). After a while, agents then leave from the walls and visit around in the
22
Published as a conference paper at ICLR 2020
(a) 2500 Updates (b) 4671 Updates
Figure 11: Visitation heatmap of RND agents on Pass of most recent 1k episodes. The brighter the
yellow color, the higher the visitation frequency. Top: agent 1, bottom: agent 2.
(c) 4673 Updates (d) 4677 Updates
(e) 5500 Updates
(a) 10 Updates
Figure 12: Visitation heatmap in most recent 1k episodes of EMI agents on Pass. The brighter the
yellow color, the higher the visitation frequency. Top: agent 1, bottom: agent 2.
room again, as shown in Fig. 11(e). The whole exploration process repeated. Similar behaviors are
also observed on the Secret-Room problem.
We also analyze the exploration behaviors of EMI agents on Pass, as illustrated by visitation
heatmaps in Fig. 12. EMI tends to explore the state-action pairs where the transition dynamics
is relatively complex, such as the edges and corners of the room (Fig. 12(a-c)). For problems where
these state-action pairs do not lead to goals, EMI is not very effective. As the (centralized) transition
dynamics of the Pass problem is relatively simple, EMI intrinsic reward quickly diminishes, which
results in the behaviors of agents keeping unchanged after 500 updates (Fig. 12(d-e)).
In summary, centralized single-agent exploration methods encode some heuristics to facilitate ex-
ploration, but they typically do not place a great emphasis on interactions among agents and are thus
not very efficient for multi-agent exploration with sparse interactions.
23