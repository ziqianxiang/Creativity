Published as a conference paper at ICLR 2020
Explanation by Progressive Exaggeration
Sumedha Singla
Department of Computer Science
University of Pittsburgh
Brian Pollack, Junxiang Chen
Department of Biomedical Informatics
University of Pittsburgh
Kayhan Batmanghelich
Department of Biomedical Informatics
Department of Computer Science
Intelligent Systems Program
University of Pittsburgh
Ab stract
As machine learning methods see greater adoption and implementation in high
stakes applications such as medical image diagnosis, the need for model inter-
pretability and explanation has become more critical. Classical approaches that
assess feature importance (e.g., saliency maps) do not explain how and why a par-
ticular region of an image is relevant to the prediction. We propose a method
that explains the outcome of a classification black-box by gradually exaggerat-
ing the semantic effect of a given class. Given a query input to a classifier, our
method produces a progressive set of plausible variations of that query, which
gradually changes the posterior probability from its original class to its negation.
These counter-factually generated samples preserve features unrelated to the clas-
sification decision, such that a user can employ our method as a “tuning knob”
to traverse a data manifold while crossing the decision boundary. Our method is
model agnostic and only requires the output value and gradient of the predictor
with respect to its input.
1	Introduction
With the explosive adoption of deep learning for real-world applications, explanation and model
interpretability have received substantial attention from the research community (Kim, 2015; Doshi-
Velez & Kim, 2017; Molnar, 2019; Guidotti et al., 2019). Explaining an outcome of a model in high
stake applications, such as medical diagnosis from radiology images, is of paramount importance to
detect hidden biases in data (Cramer et al., 2018), evaluate the fairness of the model (Doshi-Velez
& Kim, 2017), and build trust in the system (Glass et al., 2008). For example, consider evaluating
a computer-aided diagnosis of Alzheimer’s disease from medical images. The physician should be
able to assess whether or not the model pays attention to age-related or disease-related variations
in an image in order to trust the system. Given a query, our model provides an explanation that
gradually exaggerates the semantic effect of one class, which is equivalent to traversing the decision
boundary from side to another.
Although not always clear, there are subtle differences between interpretability and explana-
tion (Turner, 2016). While the former mainly focuses on building or approximating models that
are locally or globally interpretable (Ribeiro et al., 2016), the latter aims at explaining a predictor a-
posteriori. The explanation approach does not compromise the prediction performance. However, a
rigorous definition for what is a good explanation is elusive. Some researchers focused on providing
feature importance (e.g., in the form of a heatmap (Selvaraju et al., 2017)) that influence the outcome
of the predictor. In some applications (e.g., diagnosis with medical images) the causal changes are
spread out across a large number of features (i.e., large portions of the image are impacted by a
disease). Therefore, a heatmap may not be informative or useful, as almost all image features are
highlighted. Furthermore, those methods do not explain why a predictor returns an outcome. Others
have introduced local occlusion or perturbations to the input (Zhou et al., 2014; Fong & Vedaldi,
2017) by assessing which manipulations have the largest impact on the predictors. There is also
1
Published as a conference paper at ICLR 2020
recent interest in generating counterfactual inputs that would change the black box classification de-
cision with respect to the query inputs (Goyal et al., 2019; Liu et al., 2019). Local perturbations of
a query are not guaranteed to generate realistic or plausible inputs, which diminishes the usefulness
of the explanation, especially for end users (e.g., physicians). We argue that the explanation should
depend not only on the predictor function but also on the data. Therefore, it is reasonable to train a
model that learns from data as well as the black-box classifier (e.g., (Chang et al., 2019; Dabkowski
& Gal, 2017; Fong & Vedaldi, 2017)).
Our proposed method falls into the local explanation paradigm. Our approach is model agnostic
and only requires access to the predictor values and its gradient with respect to the input. Given a
query input to a black-box, we aim at explaining the outcome by providing plausible and progressive
variations to the query that can result in a change to the output. The plausibility property ensures that
perturbation is natural-looking. A user can employ our method as a “tuning knob” to progressively
transform inputs, traverse the decision boundary from one side to the other, and gain understanding
about how the predictor makes a decision. We introduce three principles for an explanation function
that can be used beyond our application of interest. We evaluate our method on a set of benchmarks
as well as real medical imaging data. Our experiments show that the counterfactually generated
samples are realistic-looking and in the real medical application, satisfy the external evaluation. We
also show that the method can be used to detect bias in training of the predictor.
2	Method
Consider a black box classifier that maps an input space X (e.g., images) to an output space Y (e.g.,
labels). In this paper, we consider binary classification problems where Y = {-1, +1}. To model
the black-box, we use f(x) = P(y|x) to denote the posterior probability of the classification. We
assume that f is a differentiable function and we have access to its value as well as its gradient with
respect to the input Vχf (x).
Figure 1: (a) The schematic of the method: f is the black-box function producing the posterior probability. δ is
the required change in black-box’s output f (x). If (x, δ) is an explainer function for f, which shifts the value
of f (x) by δ. The E(∙) is an encoder that maps the data manifold Mx to the embedding manifold Mz. Xδ is
an abbreviation for If (x, δ). (b) The architecture of our model: E is the encoder, Gδf denotes the conditional
generator G(∙, Cf (x, δ)), f is the black-box and D is the discriminator. The circles denote loss functions.
We view the (visual) explanation of the black-box as a generative process that produces an input
for the black-box that slightly perturbs current prediction (f (x) + δ) while remaining plausible and
realistic. By repeating this process towards each end of the binary classification spectrum, we can
traverse the prediction space from one end to the other and exaggerate the underlying effect. We
conceptualize the traversal from one side of the decision boundary to the other as walking across a
data manifold, Mx . We assume the walk has a fixed step size and each step of the walk makes δ
change to the posterior probability of the the classifier, f. Since the output of f is bounded between
2
Published as a conference paper at ICLR 2020
[0,1], Wecantakeat-most [ 1C steps. Each positive (negative) step increases (decreases) the posterior
probability of the previous step. We assume that there is a low-dimensional embedding space (Mz)
that encodes the Walk. An encoder, E : Mx → Mz, maps an input, x, from the data manifold,
Mx , to the embedding space. A generator, G : Mz → My, takes both the embedding coordinate
and the number of steps and maps it back to the data manifold (see Figure1).
We use If(∙,∙) to denote the explainer function. Formally, Zf (x, δ) : (X,R) → X is a function
that takes tWo arguments: a query image x and the desired perturbation δ. This function generates a
perturbed image Which is then passed through function f. The difference betWeen the outputs of f
given the original image and the perturbed image should be the desired change i.e., f(xδ) - f(x) =
δ. We use xδ to denote If (x, δ). This formulation enables us to use δ as a knob to exaggerate the
visual explanations of the query sample While it is crossing the decision boundary given by function
f. Our proposed interpretability function If should satisfy the folloWing properties:
1.	Data Consistency: perturbed samples generated by If should lie on the data manifold,
Mx, to be consistent With real data. In other Words, the generated samples should look
realistic When compared to other samples.
2.	Compatibility with f: changing the second argument in If (x, ∙) should produce the de-
sired outcome from classifier f, i.e., f (If (x, δ)) ≈ f(x) + δ.
3.	Self Consistency: Applying reverse perturbation should bring x back to its original form
i.e., If (If (x, δ), -δ) = x. Also, applying setting δ to zero should return the query, i.e.,
If (x, 0) = x.
Each criterion is enforced via a loss function Which are discussed in the folloWing sections.
2.1	Data Consistency
We adopt the Generative Adversarial NetWorks (GANs) frameWork for our model (GoodfelloW et al.,
2014). The GANs implicitly model the underlying data distribution by setting up a min-max game
betWeen generative (G) and discriminative (D) netWorks:
Lgan(D, G)= Ex”(x) [log (D(x))] + Ez〜Pz [log(1 - D(G(Z)))],
Where z and Pz are the noise distribution and the corresponding canonical distribution. There has
been significant progress toWard improving GANs stability as Well as sample quality (Brock et al.,
2019; Karras et al., 2019). The advantage of GANs is that they produce realistic-looking samples
Without an explicit likelihood assumption about the underlying probability distribution. This prop-
erty is appealing for our application.
Furthermore, We need to provide the desired amount of perturbation to the black-box, f . Hence,
We use a Conditional GAN (cGAN) that alloWs the incorporation of a context as a condition to the
GAN (Mirza & Osindero, 2014; Miyato & Koyama, 2018). To define the condition, We fix the step
size, δ, and descritize the Walk Which effectively cuts the posterior probability range of the predictor
(i.e., [0,1]) into [ 1C equally-sized bins. Hence, one can view the perturbation from f (x) to f (x) + δ
as changing the bin index from the current value cf (x, 0) to cf (x, δ) Where cf (x, δ) returns the bin
index of f(x) + δ. We use cf (x, δ) as a condition to the cGAN.
The cGAN optimizes the following loss function:
LCGAN(D, G)= Eχ,sP(x,c)[log (D(x, c))] + Ez〜Pz,c〜Pc [log (1 - D(G(Z c), c))],⑴
where c denotes a condition. Instead of generating random samples from Pz , we use the output of
an encoder, E(x), as input to the generator. Finally, the explainer function is defined as:
If(x,δ) = G(E(x), cf (x, δ)).	(2)
Our architecture is based on Projection GAN (Miyato & Koyama, 2018), a modification of cGAN.
An advantage of the Projection GAN is that it scales well with the number of classes allowing δ → 0.
The Projection GAN imposes the following structure on the discriminator loss function:
LCGAN(D,G)(x, c) = log Pdata(Cx) + log Pdata(X) := r(c∣x) + ψ(φ(G(z))),	⑶
q(c|x)	q(x)
ʌ ʌ
where LCGAN(D, G) indicates the loss function in Eq. 1 when G is fixed, φ(∙) and ψ(∙) are networks
producing vector (feature) and scalar outputs respectively. The r(c|x) is a conditional ratio function
which will be discussed in Section 2.2.
3
Published as a conference paper at ICLR 2020
2.2	Compatibility with the black box
In our model, the condition c is an ordered variable i.e., cf (x, δ1) < cf (x, δ2) when δ1 < δ2. There-
fore, we adapt the first term in Eq. 3 to account for ordinal multi-class regression by transforming
Cf (x, δ) into b 1C - 1 binary classification terms (Frank & Hall, 2001):
r(c = k|x) := X viT φ(x),	(4)
i<k
where φ(∙) is the feature network in Eq. 3 and v/s are parameters. We also need to ensure that
plugging xδ into f (∙) yields f (x) + δ (i.e., compatible with f). This condition is enforce by a
KullbackLeibler (KL) divergence loss term. Adding the KL loss and the conditional ratio function
we arrive at the following loss:
Lf (D, G) := r(c|x) + DKL (f (x) + δkf (If (x, δ))) .
While the first term is a function of both G and D, the second term influences only the generator G.
2.3	Self Consistency
We use a reconstruction loss term to enforce encoder-decoder consistency and satisfy the identity
constraint of x = If (x, 0),
Lrec(G) = ||x - G (E(x), cf (x, 0)) ||1,	(5)
We also require that the perturbation is reversible (i.e., If (If (x, δ), -δ) = x). We use a cycle-
consistency (Zhu et al., 2017) loss to reconstruct the input from its corresponding perturbed image,
Lcyc(G) = ||x - G(E(xδ),cf(x,0))∣∣ι.	(6)
Note that the conditions for the generators in Eq. 5 and 6 are the same. However, in the former,
we are reconstructing the input x from its latent space, but in the latter, we perturb xδ from the bin
index cf (x, δ) back to original bin index cf (x, 0).
2.4	Objective Functions
We adapted the hinge version of the adversarial loss for LcGAN(G, D).
LCGAN(D) = -Ex〜PdMmin(0,-1 + D(x, Cf(x, 0)))]
-Ex 〜Pdata,Cf (x,δ)∈[0,1 ] [ min(0, -1 - D(G(E(X), Cf(X δ)) Cf(X δ)))]
LCGAN(G, E) = -Ex 〜Pdata ,Cf (x,δ)∈[0, δ] [D(G(E(x), Cf(x, δ)), Cf(x, δ))]
The overall objective function is
min max λcGANLcGAN(D, G) + λf Lf (D, G) + λrecLrec(G) + λrecLcyc(G)
E,G D
where λcGAN, λf, λrec are the hyper-parameters that balance the importance of the loss terms.
(7)
(8)
(9)
3	Related Work
Our work broadly relates to literature in interpretation methods that are designed to provide a visual
explanation of the decisions made by a black-box function f, for a given query sample x.
Perturbation-based methods: These methods provide interpretation by showing what minimal
changes are required in x to induce a desirable output of f. Some methods employed image manip-
ulation via the removal of image patches (Zhou et al., 2014) or the occlusion of image regions (Zhou
et al., 2014) to change the classification score. Recently, the use of influence function, as proposed
by (Koh & Liang, 2017) are applied as a form of data perturbation to modify a classifier’s response.
The authors in (Fong & Vedaldi, 2017) proposed the use of optimal perturbation, defined as remov-
ing the smallest possible image region in x that results in the maximum drop in classification score.
In another approach, (Chang et al., 2019) proposed a generative process to find and fill the image
4
Published as a conference paper at ICLR 2020
regions that correspond to the largest change in the decision output of a classifier. To switch the
decision of a classifier, (Goyal et al., 2019) suggested generating counterfactuals by replacing the
regions of x with patches from images with a different class label. All of the aforementioned works
perform pixel- or patch-level manipulation to x, which may not result in natural-looking images.
In contrast, our model enforces that the perturbed data be consistent with the unperturbed data to
ensure that the perturbation is plausible. Furthermore, our method can be applied to general data
and is not restricted to the imaging domain.
Saliency map-based methods: Saliency maps explains the decision of f on x by highlighting the
relevant regions of x. Some earlier work in this direction(Simonyan et al., 2013; Springenberg et al.,
2015; Bach et al., 2015) focuses on computing the gradient of the target class with respect to x
and considers the image regions with large gradients as most informative. Building on this work,
the class activation map (CAM) (Zhou et al., 2016) and its generalized version Grad-CAM Selvaraju
et al. (2017) and other variants such as LPR (Bach et al., 2015) use a linear or non-linear combination
of the activation layers to derive relevance score for every pixel in an image. These gradient-based
methods are not model-agnostic and require access to intermediate layers. Recently, Adebayo et al.
(2018) have shown that some saliency methods are independent both of the model and of the data
generating process. We used their propose evaluation to validate our interpretation model. The
saliency maps are also prone to adversarial attacks as shown by Ghorbani et al. (2019) and Kinder-
mans et al. (2017). Furthermore, if the causal effect of a class is distributed across an image, which
is the case in radiology images, the saliency approaches highlight large sections of the image, which
greatly reduce the usefulness of the interpretation.
Generative explanation-based methods: These are interpretation models that uses a generative
process to produce visual explanations. The contrastive explanations method (CEM) (Dhurandhar
et al., 2018) generates explanations that show minimum regions in x which must be present/absent
for a particular classification decision. In another work, (Liu et al., 2019; Joshi et al., 2019; Saman-
gouei et al., 2018) generates explanations that highlight what features should be changed in x so
that the classifier confidence in the prediction is strengthen (prototype) or weakened (counterfac-
tual). Our approach is aligned with these latter lines of work, although our method and model
architecture is different. Our method allows for the gradual change of the class effect, and our con-
sistency criteria result in high-quality feasible perturbation in x. We rigorously evaluate our method
on real medical imaging applications, in addition to the curated computer vision datasets.
4	Experiments
We set up four experiments to evaluate our method. First, we assess if our method satisfies the three
criteria of the explainer function introduced in Section 2. We report both qualitative and quantitative
results. Second, we apply our method on a medical image diagnosis task. We use external domain
knowledge about the disease to perform a quantitative evaluation of the explanation. Third, we
train two classifiers on biased and unbiased data and examine the performance of our method in
identifying the bias. While our method does not produce a saliency map, in our last experiment, we
use the two counterfactual samples on the boundary [0, 1] to generate a saliency map and compare
it with the other methods. In Appendix A, we show further experiments to evaluate our model in
human experiments, to demonstrate its compatibility with a multi-label classifier and, an ablation
study, to show the relative importance of each of the three criteria of the explainer function.
Our experiments are conducted on the CelebA (Liu et al., 2015) and CheXpert (Irvin et al., 2019)
datasets. CelebA contains 200K celebrity face images, each with forty attribute labels. We consid-
ered binary classifier trained on the “smiling” and “young” attributes. CheXpert is a medical dataset
containing 224K chest x-ray images from 65K patients and has labels for fourteen radio-graphic ob-
servations. We considered Cardiomegaly as the target class for generating explanations. All images
are re-sized to 128 × 128 before processing.
4.1	Evaluating the Criteria of the Explainer
Figure 2 reports the qualitative results on three datasets. Given a query image x at inference time, our
model generates a series of images xδ as visual explanations, which gradually increase the posterior
probability f (xδ) (top label). We show results for three prediction tasks: smiling or not-smiling,
5
Published as a conference paper at ICLR 2020
young or old, and Cardiomegaly or healthy. The values on the top of each figure report the f (xδ)'s.
For Cardiomegaly, we show the outlines of the heart as well as its normalized size (values inside the
parenthesis), which is indicative of the disease.
[0.7-0.8) [0.9-01.0]
Smiling
Generated Visual Explanations
[0.3-0.4)	[0.5-0.6)	[0.6-0.7)
Query Image
Desired f[×):	[0.0-0.1)
Not-smiling
0.96
0.18
0.28
0.46
0.45
0.72
Old
0.11
Young
0.97
MUnOA<q ①-①ɔ
∣≡ ≡B≡≡≡1≡
0.16
0.07
0.41
0.38
0.56
0.36
0.79
0.93
0.81(1.07)
Healthy
0.45(LO) 0.47(Lo2) 0.47(LlI
0.63(Ll) 0.65(1.14)
Cardiomegaly
0.7(Ll7) 0∙78(1.25)
>ωbn① IUo 一
Figure 2: Visual explanations generated for three prediction tasks: smiling/not-smiling face (first two rows),
young/old face (middle two rows) and Cardiomegaly/healthy chest x-ray (bottom two rows). The first column
shows the query image, followed by the corresponding generated explanations. The values above each image
are the output of the classifier f . For Cardiomegaly, we show the segmentation of the heart (yellow edge) and
report normalized heart size (values in parenthesis), which is indicative of the disease.
J
K
Data Consistency: The generated explanations are synthesized variations of the query image. To
quantitatively compare their visual quality, We consider Frechet Inception Distance (FID) (HeUsel
et al., 2017). We compared our results against the counterfactual explanations produced by
xGEM (Joshi et al., 2018). The details of the xGEM model are given in appendix A.2. We di-
vided the real and fake (i.e., generated explanations) images into tWo groups (on either boundary
of f(x) ∈ [0, 1]) and reported the FID for each group and the overall score. Our method signifi-
cantly outperforms xGEM, producing crisper and more realistic-looking images. xGEM is based on
variational autoencoder (VAE) Which are knoWn to produce blurry images (see Figure 7).
Compatibility with the black-box f: To quantify Whether the generation process is aligned With
the desire perturbation δ, We plotted the expected outcome f(x) + δ against the actual response of
the classifier for the generated explanations, f(xδ). Figure 3 shoWs hoW our model performs When
generating a series of explanations starting from a Wide range of initial query images. The perfor-
mance is almost perfect for Young/Old, but less so for more challenging classification problems such
as Smiling or Cardiomegaly. The plot also validates that We are producing perturb images covering
the entire classification range, [0, 1]. Appendix A.3 shoWs additional result from CelebA dataset.
6
Published as a conference paper at ICLR 2020
CelebA:Smiling CelebA:Young Xray:Cardiomegaly
Target Class	xGEM	OUrS	xGEM	Ours	xGEM	Ours
Present (f(Xδ) ∈ [0.9, 1])	111.0	46.9	115.2	67.6	368.6	82.9
Absent (f(Xδ) ∈ [0, 0.1])	112.9	56.3	170.3	74.4	394.6	84.8
Overall (f (Xδ) ∈ [0, 1])	106.3	358~	117.9	53.4	326.3	58.1
Table 1: The Frechet Inception Distance (FID) score, measuring the quality of the generated explanations for
the three prediction tasks. Lower FID corresponds to better image quality. Top (bottom) row corresponds the
top (bottom) 10% of the decision interval.
Figure 3: Plot of the expected outcome from the classifier, f(x)+δ, against the actual response of the classifier
on generated explanations, f (xδ). The monotonically increasing trend shows a positive correlation between
f(x) + δ and f(xδ), and thus the generated explanations are consistent with the expected condition.
Identity preservation: The generated explanations should differ only in semantic features associ-
ated with the target class, while retaining the identity of the query image. We extracted the latent
embedding for real images (E(x)) and their corresponding explanations (E(xδ)), for different val-
ues of δ. We calculated latent space closeness as the percentage of the times, xδ is closest to the
query image x as compared to other generated explanations
∀δ, X ∈ X,	||E(X)- E(Xδ)||2 <	min	||E(Xδ) - E(m)||2,	(Io)
m∈If (X-{x},δ)
where, m ∈ If (X -{X}, δ)) is the set of explanations generated for all the real images excluding the
query image X. Another, popular approach to quantify identity of two face images, is to perform face
verification. We used state-of-the-art face recognition model trained on VGGFace2 dataset (Cao
et al., 2018) as feature extractor for both real images and their corresponding fake explanations.
For face verification, we calculated the closeness between real and fake image as cosine distance
between their feature vectors. The faces were considered as verified i.e., fake explanation have same
identity as real image, if the distance is below 0.5. Table 2 summarizes the results.
Our method achieved high performance on localized attribute “smiling”, which alters a relatively
small region of the face image as compare to attribute “age” which affects the entire face. Medical
images like chest x-ray have very fine grain details which are difficult to preserve in the generative
process of GAN. Our explainer function preserves the high level features like shape and size of the
lung, but it struggles to retain the low level features like anatomy of the breast and shape of the collar
bones. Also, it should be noted that both the datasets have multiple images for same person, but we
ignore this information in our analysis and treat each image as a different identity. We compared our
performance against xGEM (Joshi et al., 2018). VAE explicitly minimizes for latent space closeness.
The generated explanation by xGEM were blurry version of the query image. Hence, although they
were close to query image in latent space, but they didn’t preserve the identity of the individual as
shown in face verification task and is evident in Figure 7 in appendix A.2. In comparison, our model
achieved good performance on both the tasks.
4.2	Counterfactual evaluation on medical data
Cardiomegaly refers to an abnormal enlargement of the heart (Brakohiapa et al., 2017). To un-
derstand the explanations derived for Cardiomegaly target class, we overlaid the heart segmentation
7
Published as a conference paper at ICLR 2020
CelebA:Smiling CelebA:Young Xray:Cardiomegaly
XGEM	OUrS	XGEM	OUrS	XGEM	OUrS
Latent Space Closeness	882	88.0	895	816	2.2	27.9
Face Verification Accuracy 0.0	85.3	0.0	72.2	-	-
Table 2: Identity preServing performance on three prediction taSkS.
over the X-ray image and viSUalize the gradUal change in heart Size. The heart Segmentation iS Shown
aS oUtlineS in FigUre 2, with their correSponding heart Size (top valUeS in parentheSeS). The heart
Segmentation iS derive by training a UNet (Ronneberger et al., 2015) model on the Segmentation in
cheSt radiograph (SCR) dataSet (van Ginneken et al., 2006). We regiStered x with itS aSSociated xδ
and applied the reSUlting tranSformation to the heart maSkS of x to derive the heart maSkS for xδ.
For popUlation-level analySiS, we plotted the average heart Size of xδ vS the condition USed for
generation (f (x) + δ) in FigUre 4 (a). The plot ShowS a poSitive correlation between the heart Size
and the reSponSe of the claSSifier f (x), which agreeS with the definition of Cardiomegaly. To better
UnderStand the reSUltS, we divided the popUlation into two groUpS, the firSt groUp (xh; f (xh) < 0.1)
conSiStS of real imageS of healthy X-rayS, and the Second groUp (xc; f (xc) > 0.9) containS real
imageS of abnormal X-rayS poSitive for Cardiomegaly. For xh we generated coUnterfactUal aS xcδ
SUch that f (xδc) > 0.9. Similarly, coUnterfactUalS for xc are derived aS xδh SUch that f (xδh) <
0.1. In FigUre 4 (b), we Show the diStribUtion of heart Size in the foUr groUpS. We reported the
dependent t-teSt StatiSticS for paired SampleS xh and xcδ , xc and xδh . A Significant p-valUe
0.001 rejected the nUll hypotheSiS (i.e., that the two groUpS have Similar diStribUtionS). We alSo
reported the independent two-Sample t-teSt StatiSticS for healthy xh and xδh, p-valUe > 0.01 and
abnormal (xc and xcδ, p-valUe < 0.01) popUlationS. Given higher p-valUeS, we cannot reject the nUll
hypotheSiS of identical average diStribUtionS with high confidence. OUr model derived eXplanationS
SUcceSSfUlly captUred the change in heart Size while generating coUnterfactUal eXplanationS.
Figure 4: Cardiomegaly disease is associated with large heart size. In (a) we show the positive correlation
between the heart size and the response of the classifier f (x). (b) Comparison of the distribution of the heart
size in the four groups. (c) Plot to show the drop in accuracy of the classifier as we perturb the most relevant
pixels (relevance calculated from saliency map) in the image.
flx) + δ
(a)
Real Counterfactual Real Counterfactual
Healthy ×h Abnormal x⅛ Abnormal ×c Healthy
(b)
----Grad-Cam
----EpsiIon-LRP
----DeepLIFT (RescaIe)
----Ours
----Random
5	10	15	20
% of the most relevent pixels pertrubed
(C)
4.3	Saliency Map
Saliency maps show the importance of each pixel of an image in the context of classification. Our
method is not designed to produce saliency maps as a continuous score for every feature of the
input. We extract an approximate saliency map by quantifying the regions that changed the most
when comparing explanations at the opposing ends of the classification spectrum. For each query
image, we generated two visual explanations corresponding to the two extremes of the decision
boundary f (xδ) = 0 and f (xδ) = 1 . The absolute difference between these explanations is our
saliency map. Figure 5 shows the saliency map obtain from our method and its comparison with
popular gradient based methods. We restricted the saliency maps obtained from different methods
to have positive values and normalize them to range [0,1]. Subjective, the saliency maps produced
by our method are very localized and are comparable to the other methods.
8
Published as a conference paper at ICLR 2020
We adapted the metric introduced in (Samek et al., 2016) to compare the different saliency maps.
In an iterative procedure, we progressively replace a percentage of the most relevant pixels in an
image (as given by the saliency map) with random values sampled from a uniform distribution. We
observe the corresponding change in the classification performance as shown in Figure 4 (c). All the
methods experienced a drop in the accuracy of the classifier with increase in the fraction of perturb
pixels. The saliency maps produced by our model is significantly better than random maps and are
comparable to the other saliency map methods. It should be noted that, there are many ways to
quantify important regions in a image, using the series of explanations generated by our method.
We didn’t optimize to find the best saliency map and showed results for one such method.
Figure 5: Our comparison with popular gradient-based saliency map producing methods on the prediction task
of identifying smiling faces in CelebA dataset.
4.4	Bias Detection
Our model can discover confounding bias in the data used for training the black-box classifier. Con-
founding bias provides an alternative explanation for an association between the data and the target
label. For example, a classifier trained to predict the presence of a disease may make decisions
based on hidden attributes like gender, race, or age. In a simulated experiment, we trained two clas-
sifiers to identify smiling vs not-smiling images in the CelebA dataset. The first classifier fBiased is
trained on a biased dataset, confounded with gender such that all smiling images are of male faces.
We train a second classifier fNo-biased on an unbiased dataset, with data uniformly distributed with
respect to gender. Note that we evaluate both the classifiers on the same validation set. Addition-
ally, we assume access to a proxy Oracle classifier fGender that perfectly classifies the confounding
attribute i.e., gender. As shown in Cohen et al. (2018), if the training data for the GAN is biased,
then the inference would reflect that bias. In Figure 6, we compare the explanations generated for
the two classifiers. The visual explanations for the biased classifier change gender as it increases the
amount of smile. We adapted the confounding metric proposed in Joshi et al. (2018) to summarize
our results in Table 3. Given the data D = {(xi, yi, ai), xi ∈ X , yi , ai ∈ Y}, we quantify that a
classifier is confounded by an attribute a if the generated explanation Xδ has a different attribute a,
as compared to query image x, when processed through the Oracle classifier fGender. The metric is
formally defined as ED[1(g*(xδ) = a)]∕∖D∖. For a biased classifier, the Oracle function predicted
the female class for the majority of the images, while the unbiased classifier is consistent with the
true distribution of the validation set for gender. Thus, we the fraction of generated explanations that
changed the confounding attribute “gender’ was found to be high for the biased classifier.
9
Published as a conference paper at ICLR 2020
Target Label
Black-box classifier	Smiling	Not-Smiling
fBiased	Male: 0.52- Female: 0.48 Overall: 0.12	Male: 0.18 Female: 0.82 Overall: 0.35
fNo-biased	Male: 0.48- Female: 0.52 Overall: 0.07	Male: 0.47 Female: 0.53 Overall: 0.08
Table 3: Confounding metric for biased detection. For target label “Smiling” and “Not-Smiling”, the explana-
tions are generated using condition f(x) + δ > 0.9 and f(x) + δ < 0.1 respectively. The Male and Female
values quantifies the fraction of the generated explanations classifier as male or female, respectively by oracle
classifier fGender. The overall value quantifies the fraction of the generated explanations who have different
gender as compared to the query image. A small overall value shows least bias.
Query Image	Generated Visual Explanations
Desired f(x):	[0.0-0.1)	[0.2-0.3)	[0.3-0.4)	[0.5-0.6)	[0.6-0.7)
Not-smiling/Female
Hx)：o.o 仪)：0.0	0.23_________032__________038__________0.49
[0.7-0.8) [0.9-01.0]
Smiling/Male
0.69_________0.94
Figure 6: The visual explanations for two classifiers, both trained to classify “Smiling” attribute on CelebA
dataset. For each example, the top row shows results from “Biased” classifier whose data distribution is con-
founded with “Gender”. The bottom row shows explanations from “No-Biased” classifier with uniform data
distribution w.r.t gender. The top label indicates output of the classifier and the bottom label is the output of
an oracle classifier for the con-founding attribute gender. The visual explanations for the “Biased” classifier
changes the gender as it adds smile on the face.
5 Conclusion
In this paper, we proposed a novel interpretation method that explains the decision of a black-box
classifier by producing natural-looking, gradual perturbations of the query image, resulting in an
equivalent change in the output of the classifier. We evaluated our model on two very different
datasets, including a medical imaging dataset. Our model produces high-quality explanations while
preserving the identity of the query image. Our analysis shows that our explanations are consistent
with the definition of the target disease without explicitly using that information. Our method can
also be used to generate a saliency map in a model agnostic setting. In addition to the interpretability
advantages, our proposed method can also identify plausible confounding biases in a classifier.
10
Published as a conference paper at ICLR 2020
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp.
9505-9515, 2018.
Sebastian Bach, Alexander Binder, Grgoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.
Edmund Kwadwo Kwakye Brakohiapa, Benard Ohene Botwe, Benjamin Dabo Sarkodie, Eric Kwesi
Ofori, and Jerry Coleman. Radiographic determination of cardiomegaly using cardiotho-
racic ratio and transverse cardiac diameter: can one size fit all? Part one. The Pan
African medical journal, 27:201, 2017. ISSN 1937-8688. doi: 10.11604/pamj.2017.27.201.
12017. URL http://www.ncbi.nlm.nih.gov/pubmed/28904726http://www.
pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5579422.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity
Natural Image Synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Q Cao, L Shen, W Xie, O M Parkhi, and A Zisserman. VGGFace2: A dataset for recognising faces
across pose and age. In International Conference on Automatic Face and Gesture Recognition,
2018.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining Image Clas-
sifiers by Counterfactual Generation, 2019. URL https://openreview.net/forum?id=
B1MXz20cYQ.
Joseph Paul Cohen, Margaux Luck, and Sina Honari. Distribution matching losses can hal-
lucinate features in medical image translation. In Lecture Notes in Computer Science (in-
cluding subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformat-
ics), volume 11070 LNCS, pp. 529-536. Springer Verlag, 2018. ISBN 9783030009274. doi:
10.1007/978-3-030-00928-1{\_}60.
Henriette Cramer, Jean Garcia-Gathright, Aaron Springer, and Sravana Reddy. Assessing and ad-
dressing algorithmic bias in practice. interactions, 25(6):58-63, 2018.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in
Neural Information Processing Systems, pp. 6967-6976, 2017.
Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shan-
mugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations
with pertinent negatives. In Advances in Neural Information Processing Systems, pp. 592-603,
2018.
Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017.
Ruth C. Fong and Andrea Vedaldi. Interpretable Explanations of Black Boxes by Meaningful Per-
turbation. In Proceedings of the IEEE International Conference on Computer Vision, 2017. ISBN
9781538610329. doi: 10.1109/ICCV.2017.371.
Eibe Frank and Mark Hall. A simple approach to ordinal classification. In European Conference on
Machine Learning, pp. 145-156, 2001.
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681-3688, 2019.
Alyssa Glass, Deborah L McGuinness, and Michael Wolverton. Toward establishing trust in adaptive
agents. In Proceedings of the 13th international conference on Intelligent user interfaces, pp.
227-236. ACM, 2008.
11
Published as a conference paper at ICLR 2020
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Z Ghahramani, M Welling,
C Cortes, N D Lawrence, and K Q Weinberger (eds.), Advances in Neural Information Processing
Systems27,pp. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/
paper/5423- generative- adversarial- nets.pdf.
Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual Visual
Explanations. In International Conference on Machine Learning, pp. 2376-2384, 2019. URL
http://arxiv.org/abs/1904.07451.
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino
Pedreschi. A survey of methods for explaining black box models. ACM computing surveys
(CSUR), 51(5):93, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong,
Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N.
Patel, Matthew P. Lungren, and Andrew Y. Ng. CheXpert: A Large Chest Radiograph Dataset
with Uncertainty Labels and Expert Comparison, 1 2019. URL http://arxiv.org/abs/
1901.07031.
Shalmali Joshi, Oluwasanmi Koyejo, Been Kim, and Joydeep Ghosh. xGEMs: Generating Exam-
plars to Explain Black-Box Models. arXiv preprint arXiv:1806.08867, 2018.
Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. To-
wards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making
Systems. CoRR, abs/1907.09615, 2019. URL http://arxiv.org/abs/1907.09615.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Been Kim. Interactive and interpretable machine learning models for human machine collabora-
tion. PhD thesis, Massachusetts Institute of Technology, 2015.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, SVen
Dahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. Explainable AI:
Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280, 2017.
Pang Wei Koh and Percy Liang. Understanding black-box predictions Via influence functions. In
34th International Conference on Machine Learning, ICML 2017, 2017. ISBN 9781510855144.
Shusen Liu, BhaVya Kailkhura, Donald LoVeland, and Yong Han. GeneratiVe Counterfactual Intro-
spection for Explainable Deep Learning. arXiv, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Mehdi Mirza and Simon Osindero. Conditional GeneratiVe AdVersarial Nets. CoRR, abs/1411.1784,
2014. URL http://arxiv.org/abs/1411.1784.
Takeru Miyato and Masanori Koyama. cGANs with Projection Discriminator. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ.
Christoph Molnar. Interpretable Machine Learning. Github, 2019. https://christophm.
github.io/interpretable-ml-book/.
12
Published as a conference paper at ICLR 2020
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp.1135-1144. ACM, 2016.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomed-
ical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention
(MICCAI), 2015. URL http://arxiv.org/abs/1505.04597.
Pouya Samangouei, Ardavan Saeedi, Liam Nakagawa, and Nathan Silberman. Explaingan: Model
explanation via decision boundary crossing transformations. In Proceedings of the European
Conference on Computer Vision (ECCV), pp. 666-681, 2018.
Wojciech Samek, Alexander Binder, Grgoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of What a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660-2673, 2016.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618-626,
2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. Computing Research Repository,
2013.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A Riedmiller. Striving for
Simplicity: The All Convolutional Net. In ICLR (workshop track), 2015.
Ryan Turner. A model explanation system. In 2016 IEEE 26th International Workshop on Machine
Learning for Signal Processing (MLSP), pp. 1-6. IEEE, 2016.
B van Ginneken, M B Stegmann, and M Loog. Segmentation of anatomical structures in chest
radiographs using supervised methods: a comparative study on a public database. Medical Image
Analysis, 10(1):19-40, 2006.
Anthony J Viera, Joanne M Garrett, and others. Understanding interobserver agreement: the kappa
statistic. Fam med, 37(5):360-363, 2005.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. Computing Research Repository, 2014.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2921-2929, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired Image-to-Image Transla-
tion using Cycle-Consistent Adversarial Networks. In Proceedings of the IEEE international con-
ference on computer vision, pp. 2223-2232, 2017. URL http://arxiv.org/abs/1703.
10593.
A Appendix
A. 1 Implementation Details
The architecture for the generator and discriminator is adapted from Miyato & Koyama (2018).
The image encoding learned by encoder E(x) is fed into the generator. The condition cf (x, δ) is
passed to each resnet block in the generator, using conditional batch normalization. The generator
has five resnet blocks, where each block consists of BN-ReLU-Conv3-BN-ReLU-Conv3. BN is
batch normalization, ReLU is activation function, and Conv3 is the convolution filter. The encoder
function uses the same structure but downsamples the image. The discriminator function has five
resnet blocks, each of which has the form ReLU-Conv3-ReLU-Conv3.
13
Published as a conference paper at ICLR 2020
A.2 xGEM implementation
We refers to Joshi et al. (2019) for the implementation of xGEM. First, VAE is trained to generate
face images. The VAE used is available at:https://github.com/LynnHo/VAE-Tensorflow. All settings
and architectures were set to default values. The original code generates an image of dimension
64x64. We extended the given network to produce an image with dimensions 128x128. The pre-
trained VAE is then extended to incorporate the cross-entropy loss for flipping the label of the query
image. The model evaluates the cross-entropy loss by passing the generated image through the
classifier. Figure 7 shows the qualitative difference between the explanations generated by our
proposed method and xGEM.
A.3 Extended results for evaluating the criteria of the explainer
Here, we provide results for four more prediction tasks on celebA dataset: no-beard or beard, heavy
makeup or light makeup, black hair or not back hair, and bangs or no-bangs. Figure 8 shows the
qualitative results, an extended version of results in Figure 2. We evaluated the results from these
prediction tasks for compatibility with black-box f (see Figure 9), data consistency and self consis-
tency (see Table 4).
Prediction Task	Data Consistency (FID)			Self Consistency	
	Present	Absent	Overall	LSC	FVA
Smiling vs Not-smiling	46.9^^	56.3	-^35.8	88.0	85.3
Young vs Old	67.5	74.4	53.4	81.6	72.2
No beard vs Beard	79.2	72.3	45.4	89.6	83.3
Heavy makeup vs Light makeup	64.9	98.2	39.2	89.2	75.3
Black hair vs Not black hair	55.8	72.8	34.8	79.4	81.6
Bangs vs No bangs	54.1	57.8	40.6	76.5	87.3
Table 4: Our model results for six prediction tasks on CelebA dataset. FID (Frechet Inception
Distance) score measures the quality of the generated explanations. Lower FID is better. LSC
(Latent Space Closeness) quantifies the fraction of the population where generated explanation is
nearest to the query image than any other generated explanation in embedding space. FVA (Face
verification accuracy) measures percentage of the times the query image and generated explanation
have same face identity as per model trained on VGGFace2. Higher LSC and FVA is better.
A.4 Human Evaluation
We used Amazon Mechanical Turk (AMT) to conduct human experiments to demonstrate that the
progressive exaggeration produced by our model is visually perceivable to humans. We presented
AMT workers with three tasks. In the first task, we evaluated if humans can detect the relative
order between two explanations produced for a given image. We ask the AMT workers, “Given
two images of the same person, in which image is the person younger (or smiling more)?” (see
Figure 10). We experimented with 200 query images and generated two pairs of explanations for
each query image (i.e., 400 hits). The first pair (easy) imposed the two images are samples from
opposite ends of the explanation spectrum (counterfactuals), while the second pair (hard) makes no
such assumption.
In the second task, we evaluated if humans can identify the target class for which our model has
provided the explanations. We ask the AMT workers, “What is changing in the images? (age, smile,
hair-style or beard)”. We experimented with 100 query images from each of the four attributes (i.e.,
400 hits). In the third task, we demonstrate that our model can help the user to identify problems like
possible bias in the black-box training. Here, we used the same setting as in the second task but also
showed explanations generated for a biased classifier. We ask the AMT workers, “What is changing
in the images? (smile or smile and gender)” (see Figure 10). We generated explanations for 200
query images each, from a biased-classifier (fBiased) explainer from Section 4.4 and an unbiased
classifier (fNo-biased) explainer (i.e., 400 hits). In all the three tasks, we collected eight votes for each
task, evaluated against the ground truth, and used the majority vote for calculating accuracy.
14
Published as a conference paper at ICLR 2020
We summarize our results in Table 5. In the first task, the annotators achieved high accuracy for the
easy pair when there was a significant difference among the two explanation images, as compared
to the hard pair when the two explanations can have very subtle differences. Overall, the annotators
were successful in identifying the relative order between the two explanation images.
In the second task, the annotators were generally successful in correctly identifying the target class.
The target class “bangs” proved to be the most difficult to identify, which was expected. The gen-
erated images for “bangs” were qualitatively, the most subtle. For the third task, the correct answer
was always the target class i.e., “smile”. In the case of biased classifier explainer, the annotators
selected “Smile and Gender” 12.5% of the times. The gradual progression made by the explainer
for a biased classifier was very subtle and was changing large regions of the face as compared to the
unbiased explainer. The difference is much more visible when we compare the explanation gener-
ated for the same query image for a biased and no-biased classifier, as in Figure 6. But in a realistic
scenario, the no-biased classifier would not be available to compare against. Nevertheless, the an-
notators detected bias at roughly the same level of accuracy as our classifier (Table 3). Future work
could improve upon bias detection.
Annotation Task	Overall		Sub categories		
	Accuracy	κ-statistic	Category	Accuracy	κ-statistic
Task-1 (Age)	83.5%	0.41 (Moderate)	Hard	73%	0.31 (Fair)
			Easy	94%	0.51 (Moderate)
Task-1 (Smile)	77.5%	0.28 (Fair)	Hard	66%	0.23 (Fair)
			Easy	89.5%	0.32 (Fair)
Task-2 (Identify Target Class)	77%	0.35 (Fair)	Age	72%	-
			Smile	99%	-
			Bangs	50%	-
			Beard	87%	-
Task-3 (Bias Detection)	93.75%	0.14 (Slight)	fBiased	87.5%	0.09 (Slight)
			fNo-biased	100%	0.02 (Slight)
Table 5: Summarizing the results of human evaluation. The κ -statistics measure inter-rater agree-
ment for qualitative classification of items into some mutually exclusive categories. One possible
interpretation of κ as given in Viera et al. (2005) is < 0.0: Poor, 0.01 - 0.2: Slight, 0.21 - 0.40:
Fair, 0.41 - 0.60: Moderate, 0.61 - 0.80: Substantial and 0.81 - 1.00: Almost perfect agreement.
A.5 Evaluating Class Discrimination
In multi-label settings, multiple labels can be true for a given image. In this test, we evaluated the
sensitivity of our generated explanations to the class being explained. We consider a classifier trained
to identify multiple attributes: young, smiling, black-hair, no-beard and bangs in face images from
CelebA dataset. We used our model to generate explanations while considering one of the attributes
as the target. Ideally, an explanation model trained to explain a target attribute should produce
explanations consistent with the query image on all the attributes beside the target. Figure 11 plots
the fraction of the generated explanations, that have flipped in source attribute as compared to the
query image. Each column represents one source attribute. Each row is one run of our method to
explain a given target attribute.
A.6 Ablation Study
Our proposed model has three types of loss functions: adversarial loss from cGAN, KL loss, and
reconstruction loss. The three losses enforce the three properties of our proposed explainer function:
data consistency, compatibility with f, and self-consistency, respectively. In the ablation study, we
quantify the importance of each of these components by training different models, which differ in
one hyper-parameter while rest are equivalent (λcGAN = 1, λf = 1 and λrec = 100). For data
consistency, We evaluate Frechet Inception Distance (FID). FID score measures the visual quality
of the generated explanations by comparing them with the real images. We show results for two
15
Published as a conference paper at ICLR 2020
groups. In the first group, we consider real and fake images where the classifier has high confidence
in presence of the target label i.e., f(xδ), f(x) ∈ [0.9, 1.0]. In second group, the target label is
absent i.e., f(xδ), f(x) ∈ [0.0, 0.1). We also report an overall score by considering all the real and
generated explanations together. For compatability with f we plotted the desired output of the clas-
sifier i.e., f(x) + δ against the actual output of the classifier f(xδ) for the generated explanations.
For self consistency, we calculated the Latent Space Closeness (LSC) measure and Face verification
accuracy (FVA). LSC quantifies the fraction of the population in which the generated explanation
is nearest to the query image than any other generated explanation in embedding space. FVA mea-
sures the percentage of the instances in which the query image and generated explanation have the
same face identity as per the model trained on VGGFace2. For the ablation study, we consider the
prediction task of young vs old on the CelebA dataset. Figure 12 shows the results for compatibility
with f . Table 6 summarizes the results for data consistency and self-consistency.
Configuration Data Consistency (FID) Self Consistency
λcGAN	λf	λrec	Present	Absent	OVerall	LSC	FVA
0-	1	100	69.7^^	105.7	67.2	96.1	99.8
1	1	100	67.5	74.4	53.4	81.6	72.2
10	1	100	89.4	105.2	63.0	68.0	82.7
100	1	100	71.6	80.6	44.26	75.3	18.0
1	0~(Γ~	100	66.2^^	66.2	44.9	77.2	99.4
1	1	100	67.5	74.4	53.4	81.6	72.2
1	10	100	95.5	90.4	62.4	71.83	96.8
1	100	100	77.4	73.1	71.2	55.4	42.23
1	^^1 ^^	^^0~~	116.2	118.9	72.2	16.6	0.0
1	1	1	63.0	78.6	61.6	32.2	5.5
1	1	10	87.6	83.6	65.7	71.5	88.8
1	1	100	67.5	74.4	53.4	81.6	72.2
Table 6: Our model with ablation on prediction task of young Vs old on CelebA dataset. FID (Frechet
Inception Distance) score measures the quality of the generated explanations. Lower FID is better.
LSC (Latent Space Closeness) quantifies the fraction of the population where generated explanation
is nearest to the query image than any other generated explanation in embedding space. FVA (Face
Verification accuracy) measures percentage of the times the query image and generated explanation
haVe same face identity as per model trained on VGGFace2. Higher LSC and FVA is better.
16
Published as a conference paper at ICLR 2020
Query Image
houno>-‹q ①一① ɔ
A-a① IUo-PJBɔ>BHX

∞⅛0 IAIaX
Figure 7: Visual explanations generated for three prediction tasks on CelebA dataset. The first column shows
the query image, followed by the corresponding generated explanations.
17
Published as a conference paper at ICLR 2020
Query Image
Generated Visual Explanations
Desired f(x)
[0.0-0.1) [0.1-0.2) [0.2-0.3) [0.3-0.4) [0.4-0.5) [0.5-0.6) [0.6-0.7) [0.7-0.8) [0.8-0.9) [0.9-01.0]
Not-smiling	Smiling
f(x2=⅞p4 Q2 乙	£3 上“旦4 乙	旦61 “ 0.8 匕 “ 0.8乙旦9> .、&〜工 0_
P」e3m ON U
dnəaelʌl >>eθ 工<qa3
Sω>ues<qαa
NX) = LO
Figure 8: Visual explanations generated for six prediction tasks on CelebA dataset. The first column shows
the query image, followed by the corresponding generated explanations. The values above each image are the
output of the classifier f .

18
Published as a conference paper at ICLR 2020
Figure 9: Plot of the expected outcome from the classifier, f (x) + δ, against the actual response of
the classifier on generated explanations, f(xδ). The monotonically increasing trend shows a positive
correlation between f(x) + δ and f(xδ), and thus the generated explanations are consistent with the
expected condition.
Task-I (Age)
Task-2 (Identify Target Class)
In which image the person is smiling more?
Task-3 Bias Detection
Figure 10: The interface for the human evaluation done using Amazon Mechanical Turk (AMT).
Task-1 evaluated if humans can detect the relative order between two explanations. Task-2 evaluated
if humans can identify the target class for which our model has provided the explanations. Task-3
demonstrated that our model can help the user to identify problems like possible bias in the black-
box training.
Task-I (Smile)
Young	0.92	0.08	0.07	0.04	0.08	^ x.v 1-0.8 -0.6 ∏ Λ
Smiling	0.11	0.89	0.08	0.04	0.07	
BIackHair	0.16	0.18	0.58	0.08	0.16	
No-Beard	0.13	0.1	0.07	0.46	0.04	-U.4 -0.2 I ∙o.o
Bangs	0.15	0.08	0.17	0.07	0.32	
	Young	Smiling	BIackHair	No-Beard	Bangs	
Figure 11: Each cell is the fraction of the generated explanations, that have flipped in source attribute
as compared to the query image. The x-axis is source attribute and y-axis is the target attribute for
which explanation is generated. Note: This is not a confusion matrix.
19
Published as a conference paper at ICLR 2020
Figure 12: Ablation study to show the effect of KL loss term. Plot of the expected outcome from the
classifier, f(x) + δ, against the actual response of the classifier on generated explanations, f(xδ).
20