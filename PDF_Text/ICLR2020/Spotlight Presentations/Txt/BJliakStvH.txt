Published as a conference paper at ICLR 2020
Maximum Likelihood Constraint Inference
for Inverse Reinforcement Learning
Dexter R.R. Scobee & S. Shankar Sastry
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
{dscobee, sastry}@eecs.berkeley.edu
Ab stract
While most approaches to the problem of Inverse Reinforcement Learning (IRL)
focus on estimating a reward function that best explains an expert agent’s policy
or demonstrated behavior on a control task, it is often the case that such behavior
is more succinctly represented by a simple reward combined with a set of hard
constraints. In this setting, the agent is attempting to maximize cumulative rewards
subject to these given constraints on their behavior. We reformulate the problem of
IRL on Markov Decision Processes (MDPs) such that, given a nominal model of
the environment and a nominal reward function, we seek to estimate state, action,
and feature constraints in the environment that motivate an agent’s behavior. Our
approach is based on the Maximum Entropy IRL framework, which allows us to
reason about the likelihood of an expert agent’s demonstrations given our knowl-
edge of an MDP. Using our method, we can infer which constraints can be added
to the MDP to most increase the likelihood of observing these demonstrations. We
present an algorithm which iteratively infers the Maximum Likelihood Constraint
to best explain observed behavior, and we evaluate its efficacy using both simulated
behavior and recorded data of humans navigating around an obstacle.
1	Introduction
Advances in mechanical design and artificial intelligence continue to expand the horizons of robotic
applications. In these new domains, it can be difficult to design a specific robot behavior by hand.
Even manually specifying a task for a reinforcement-learning-enabled agent is notoriously difficult
(Ho et al., 2015; Amodei et al., 2016). Inverse Reinforcement Learning (IRL) techniques can help
alleviate this burden by automatically identifying the objectives driving certain behavior. Since first
being introduced as Inverse Optimal Control by Kalman (1964), much of the work on IRL has focused
on learning environmental rewards to represent the task of interest (Ng & Russell, 2000; Abbeel &
Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008). While these types of IRL algorithms have proven
useful in a variety of situations (Abbeel et al., 2007; Vasquez et al., 2014; Ziebart, 2010; Scobee et al.,
2018), their basis in assuming that reward functions fully represent task specifications makes them ill
suited to problem domains with hard constraints or non-Markovian objectives.
Recent work has attempted to address these pitfalls by using demonstrations to learn a rich class of
possible specifications that can represent a task (Vazquez-Chanlatte et al., 2018). Others have focused
specifically on learning constraints, that is, behaviors that are expressly forbidden or infeasible
(PardoWitz et al., 2005; Perez-D'Arpino & Shah, 2017; Subramani et al., 2018; McPherson et al.,
2018; Chou et al., 2018). Such constraints arise in safety-critical systems, where requirements such
as an autonomous vehicle avoiding collisions With pedestrians are more naturally expressed as hard
constraints than as soft reWard penalties. It is toWards the problem of inferring such constraints that
We turn our attention.
In this Work, We present a novel method for inferring constraints, draWing primarily from the
Maximum Entropy approach to IRL described by Ziebart et al. (2008). We use this frameWork to
reason about the likelihood of observing a set of demonstrations given a nominal task description, as
Well as about their likelihood if We imposed additional constraints on the task. This knoWledge alloWs
us to select a constraint, or set of constraints, Which maximizes the demonstrations’ likelihood and
1
Published as a conference paper at ICLR 2020
best explains the differences between expected and demonstrated behavior. Our method improves on
prior work by being able to simultaneously consider constraints on states, actions and features in a
Markov Decision Process (MDP) to provide a principled ranking of all options according to their
effect on demonstration likelihood.
2	Related Work
2.1	Inverse Reinforcement Learning
A formulation of the IRL problem was first proposed by Kalman (1964) as the Inverse problem of
Optimal Control (IOC). Given a dynamical system and a control law, the author sought to identify
which function(s) the control law was designed to optimize. This problem was brought into the
domain of MDPs and Reinforcement Learning (RL) by Ng & Russell (2000), who proposed IRL
as the task of, given an MDP and a policy (or trajectories sampled according to that policy), find a
reward function with respect to which that policy is optimal.
One of the chief difficulties in the problem of IRL is the fact that a policy can be optimal with respect
to a potentially infinite set of reward functions. The most trivial example of this is the fact that all
policies are optimal with respect to a null reward function that always returns zero. Much of the work
in IRL has been devoted to developing approaches that address this ambiguity by imposing additional
structure to make the problem well-posed (Abbeel & Ng, 2004; Ratliff et al., 2006). Ziebart et al.
(2008) approach the problem by employing the principle of maximum entropy (Jaynes, 1957), which
allows the authors to develop an IRL algorithm that produces a single stochastic policy that matches
feature counts without adding any additional constraints to the produced behavior. This so called
Maximum Entropy IRL (MaxEnt) provides a framework for reasoning about demonstrations from
experts who are noisily optimal. The induced probability distribution over trajectories forms the basis
for our efforts in identifying the most likely behavior-modifying constraints.
2.2	Beyond Reward Functions
While Markovian rewards do often provide a succinct and expressive way to specify the objectives of
a task, they cannot capture all possible task specifications. Vazquez-Chanlatte et al. (2018) highlight
the utility of non-Markovian Boolean specifications which can describe complex objectives (e.g. do
this before that) and compose in an intuitive way (e.g. avoid obstacles and reach the goal). The
authors of that work draw inspiration from the MaxEnt framework to develop their technique for
using demonstrations to calculate the posterior probability that an agent is attempting to satisfy a
Boolean specification.
A subset of these types of specifications that is of particular interest to us is the specification of
constraints, which are states, actions, or features of the environment that must be avoided. Chou et al.
(2018) explore how to infer trajectory feature constraints given a nominal model of the environment
(lacking the full set of constraints) and a set of demonstrated trajectories. The core of their approach is
to sample from the set of trajectories which have better performance than the demonstrated trajectories.
They then infer that the set of possible constraints is the subset of the feature space that contains the
higher-reward sampled trajectories, but not the demonstrated trajectories. Intuitively, they reason that
if the demonstrator could have passed through those features to earn a higher reward, but did not,
then there must have been a previously unknown constraint preventing that behavior. However, while
their approach does allow for a cost function to rank elements from the set of possible constraints, the
authors do not offer a mechanism for determining what cost function will best order these constraints.
Our approach to constraint inference from demonstrations addresses this open question by providing
a principled ranking of the likelihood of constraints. We adapt the MaxEnt framework to allow us to
reason about how adding a constraint will affect the likelihood of demonstrated behaviors, and we can
then select the constraints which maximize this likelihood. We consider feature-space constraints as in
Chou et al. (2018), and we explicitly augment the feature space with state- and action-specific features
to directly compare the impacts of state-, action-, and feature-based constraints on demonstration
likelihood.
2
Published as a conference paper at ICLR 2020
0 oftΦeK >0,"
0 ofthe=wSse,…
Ti /	、
φΦi (s,a)
3	Maximum Likelihood Constraint Inference
3.1	Problem Formulation
Following the formulation presented by Ziebart et al. (2008), we base our work in the set-
ting of a (finite-state) Markov Decision Process (MDP). We define an MDP M as a tuple
(S, {As }, {Ps,a} , D0 , φ, R) where S is a finite set of discrete states; {As } is a set of the sets
of actions available to be taken for each state s, such that As ⊆ A, where A is a finite set of discrete
actions; {Ps,a} is a set of state transition probability distributions such that Ps,a(s0) = P(s0|s, a) is
the probability of transitioning to state s0 after taking action a from state s; D0 : S → [0, 1] is an
initial state distribution; φ : S × A → Rk+ is a mapping to a k-dimensional space of non-negative
features; and R : S × A → R is a reward function. A trajectory ξ through this MDP is a sequence
of states St and actions at such that s° 〜D° and state Si+ι 〜Psa,a「Actions are chosen by an
agent navigating the MDP according to a, potentially time-varying, policy π such that π(∙∣s, t) is a
probability distribution over actions in As . We denote a finite-time trajectory of length T + 1 by
ξ = {s0:T, a0:T}.
At every time step t, a trajectory will accumulate features equal to φ(st, at). We use the notation
φi(∙, ∙) to refer to the i-th element of the feature map, and We use the label φi to denote the i-th
feature itself. We also introduce an augmented indicator feature mapping φ1 : S X A → {0,1}nφ,
Where nφ = k + |S | + |A|. This augmented feature map uses binary variables to indicate the presence
of a feature and expands the feature space by adding binary features to track occurrences of each state
and action, such that
1 if a = ai
0 otherWise .
(1)
Typically, agents are modeled as trying to maximize, or approximately maximize, the total reWard
earned for a trajectory ξ, given by R(ξ) = PtT=0 γtR(st, at), Where γ ∈ (0, 1] is a discount factor.
Therefore, an agent’s policy π is closely tied to the form of the MDP’s reWard function.
Conventional IRL focuses on inferring a reWard function that explains an agent’s policy, revealed
through the behavior observed in a set of demonstrated trajectories D. HoWever, our method for
constraint inference poses a different challenge: given an MDP M, including a reWard function, and
a set of demonstrations D, find the most likely set of constraints C * that could modify M to explain
these demonstrations. We define our notion of constraints in the folloWing section.
3.2	Constraints for MDPs
Constraints are those behaviors that are not disalloWed explicitly by the structure of the MDP, but
Which Would be infeasible or prohibited for the underlying system being modeled by the MDP. This
sort of discrepancy can occur When a generic or simplified MDP is designed Without exact knoWledge
of specific constraints for the modeled system. For instance, for a generic MDP modeling the behavior
of cars, We might Want to include states for speeds up to 500km/h and actions for accelerations up to
12m/s2. HoWever, for a specific car on a specific roadWay, the set of states Where the vehicle travels
above 100km/h may be prohibited because of a speed limit, and the set of actions Where the vehicle
accelerates above 4m/s2 may be infeasible because of the physical limitations of the vehicle’s engine.
Therefore, any MDP trajectory of this specific car system Would not contain a state-action pair Which
violates these legal and physical limits. Figure 1 shoWs an example of constraints driving behavior.
We define a constraint set Ci ⊆ S X A as a set of state-action pairs that violate some specification
of the modeled system. We consider three general classes of constraints: state constraints, action
constraints, and feature constraints. A state constraint set Csi = {(s, a) | s = si} includes all state-
action pairs such that the state component is si. An action constraint set Cai = {(s, a) | a = ai}
includes all state-action pairs such that the action component is ai . A feature constraint set
Cφi = {(s, a) | φi (s, a) > 0} includes all state-action pairs that produce a non-zero value for
feature φi .
If We augment the set of features as described in (1), it is straightforWard to see that state
and action constraints become special cases of feature constraints, With constraint sets given by
3
Published as a conference paper at ICLR 2020
Ci = {(s, a) | φi (s, a) = 1}. It is also evident that We can obtain compound constraints, respecting
two or more conditions, by taking the union of constraint sets Ci to obtain C = i Ci .
3.2.1	Adding Constraints to an MDP
We need to be able to reason about hoW adding a constraint to an
MDP Will influence the behavior of agents navigating that envi-
ronment. If We impose a constraint on an MDP, then none of the
state-action pairs in that constraint set may appear in a trajectory of
the constrained MDP. To enforce this condition, We must restrict the
actions available in each state so that it is not possible for an agent
to produce one of the constrained state-action pairs. For a given
constraint C, We can replace the set of available actions As in every
state s With an alternative set AsC given by
AsC = As \ {a ∈ As | (s, a) ∈ C} .	(2)
Performing such substitutions for an MDP M Will lead to a modified
MDP MC such that MC = (S,{AsC},{Ps,a},D0,φ,R).
The question then arises as to the hoW We should treat states With
empty action sets AC = 0. Since an agent arriving in such an empty
state Would have no valid action to select, any trajectory visiting an
empty state must be deemed invalid. Indeed, such empty action sets
Will be produced for any state si such that Csi ⊆ C .
For MDPs With deterministic transitions, agents knoW precisely
Which state they Will arrive in folloWing a certain action. Therefore,
any agent respecting constraints Will not take an action that leads to
an empty state, since doing so Will lead to constraint violations. If We
consider the set of empty states Sempty, then for the purposes of rea-
soning about an agent’s behavior, We can impose an additional con-
Figure 1: Illustration of tra-
jectories likely to be produced
by noisily optimal agents nav-
igating an MDP. (a) Expected
behavior on a generic, nom-
inal MDP. (b) Demonstrated
behavior from a specific, con-
strained MDP. Numbers repre-
sent state-based reWards, and
the red-shaded tiles represent
state constraints.
straint set Cempty = {(s, a) | ∃ sempty ∈
Sempty : Ps,a (sempty) = 1}.
In this Work, We Will alWays implicitly add this constraint set, such
that MC Will be equivalent to MC∪Cempty , and We recursively add
these constraints until reaching a fixed point.
For MDPs With stochastic transitions, the semantics of an empty state
are less obvious and could lend themselves to multiple interpretations
depending on the nature of the system being modeled. We offer a
possible treatment in the appendix.
3.2.2	Nominal MDPs
The nominal MDPs to Which We Will add constraints fall into tWo broad categories, Which We denote
as generic and baseline. The car MDP described at the beginning of Section 3.2 is an example of
a generic nominal model: its state and action spaces are broad enough to encompass a Wide range
of car models, and We can use this nominal model to infer constraints that specialize the MDP to a
specific car and task. For a generic nominal MDP, the reWard function may also come from a generic,
simplified task, such as “minimize time to the goal” or “minimize energy usage.”
A baseline nominal MDP is a snapshot of a system from a point in time Where it Was Well characterized.
With a baseline nominal MDP, the constraints that We infer Will represent changes to the system With
respect to this baseline. In this case, the nominal reWard function can be learned using existing IRL
techniques With demonstrated behavior from the baseline model. We take this approach in our human
obstacle avoidance example in Section 4.2: We use demonstrations of humans Walking through the
empty space to learn a nominal reWard, then We can detect the presence of a neW obstacle in the space
from subsequent demonstrations.
4
Published as a conference paper at ICLR 2020
3.3	Demonstration Likelihood Maximization
Our goal is to find the constraints C * which are most likely to have been added to a nominal MDP
M, given a set of demonstrations D from an agent navigating the constrained MDP. Let us define
PM to denote probabilities given that we are considering MDP M. Our problem then becomes
to select the constraints that maximize PM (C | D). If we assume a uniform prior over possible
constraints, then we know from Bayes, Rule that PM (C | D) 8 PM (D | C). Therefore, in order to
find the constraints that maximize PM(C | D), we can solve the equivalent problem of finding which
constraints maximize the likelihood of the given demonstrations. In this section, we present our
approach to solving maximum likelihood constraint inference via solving demonstration likelihood
maximization.
Under the maximum entropy model presented by Ziebart et al. (2008), the probability of a cer-
tain finite-length trajectory ξ being executed by an agent traversing a deterministic MDP M is
exponentially proportional to the reward earned by that trajectory.
PM(ξ) = ɪ eβR(ξ) lM(ξ),	⑶
Z
where Z is the partition function, lM(ξ) indicates if the trajectory is feasible for this MDP, and
β ∈ [0, ∞) is a parameter describing how closely an agent adheres to the task of optimizing the
reward function (as β → ∞, the agent becomes a perfect optimizer, and as β → 0, the agent’s actions
become perfectly random). In the sequel, we assume that a given reward function will appropriately
capture the role of β , so we omit β from our notation without loss of generality.
In the case of finite horizon planning, the partition function will be the sum of the exponentially
weighted rewards for all feasible trajectories on MDP M of length no greater than the planning
horizon. We denote this set of trajectories by ΞM. Because adding constraints C modifies the set of
feasible trajectories, we express this dependence as
Z(C)= X eR(ξ) IMC (ξ).	(4)
ξ∈ΞM
Assuming independence among demonstrated trajectories, the probability of observing a set D of N
demonstrations is given by the product
PMC (D) = ZCN Y eR(ξ) IMC (ξ).
Z(C) ξ∈D
(5)
Our goal is to maximize the demonstration probability given by (5). Because we take the reward
function and demonstrations as given, our only available decision variable in this maximization is the
constraint set C which alters the indicator IM and partition function Z(C).
C* = arg max PMC (D),
C∈C
(6)
where C ⊆ 2S ×A is the hypothesis space of possible constraints.
From the form of (5), it is clear that to solve (6), we must choose a constraint set that does not
invalidate any demonstrated trajectory while simultaneously minimizing the value of Z(C). Consider
the set of trajectories that would be made infeasible by augmenting the MDP with constraint C,
which we denote as 三二。={ξ ∈ Ξm | IM (ξ) = 0}. The value of Z(C) is minimized when we
maximize the sum of exponentiated rewards of these infeasible trajectories. Considering the form of
the trajectory probability given by (3), we can see that this sum is proportional to the total probability
of observing a trajectory from Ξ-MC
on the original MDP M
E eR(G (X E PM(ξ) = Pm(Ξmc).
ξ∈Ξ-MC	ξ∈Ξ-MC
This insight leads us to the final form of the optimization
C* = arg max PM(Ξ-MC)
C∈C
s.t. D ∩ ΞMc = 0
(7)
(8)
5
Published as a conference paper at ICLR 2020
Figure 2: Selecting constraints to maximize demonstration likelihood. Trajectories that are likely to
be observed on a given MDP are shown as dashed, angular arrows, and a provided demonstration is
shown as a solid, curved arrow. Adding C1 in (b) does little to align the expected trajectories with the
demonstration. On the other hand, adding C2 in (c) makes the original expected trajectories infeasible
and causes the new expected trajectories to agree with the demonstration, greatly increasing the
likelihood of the demonstration on this constrained MDP.
In order to solve (8), we must reason about the probability distribution of trajectories on the original
MDP M, then find the constraint C such that Ξ-MC contains the most probability mass while
not containing any demonstrated trajectories. Figure 2 provides a graphical representation of this
reasoning. We highlight here that the fact that the chosen C must not conflict with any demonstration
is an important condition: all provided demonstrations must perfectly respect a constraint in order for
it to be learned, otherwise a less restrictive set of constraints may be learned instead. Future work
will look to relax this requirement in order to learn about constraints that are generally respected
by a set of demonstrations, without needing to first isolate just the successful, constraint-respecting
demonstrations.
While equation (8) is derived for deterministic MDPs, if we can assume, as proposed by Ziebart
et al. (2008), that for a given stochastic MDP, the stochastic outcomes have little effect on an
agent’s behavior and the partition function, then the solution to (8) will also approximate the optimal
constraint selection for that MDP. However, in order to fully address the stochastic case, we would
need to reformulate our approach based on maximum causal entropy (Ziebart, 2010). We save this
extension for future work.
3.3.1 Constraint Hypothesis Space
In order for the solutions to (8) to be meaningful, we must be careful with our choice of the constraint
hypothesis space C. For instance, if we let C = 2S×A, then the optimal solution will always be to
choose the most restrictive C to constrain all state-action pairs not observed in the demonstration set.
One approach to avoid this trivial solution is to use domain knowledge of the modeled system to
restrict C to a library of plausible or common constraints. McPherson et al. (2018) construct such a
library by using reachability theory to calculate a family of likely unsafe sets.
We could also potentially address this problem by adding a regularization term to the optimization
that would penalize constraints based on some notion of “size,” which would encourage the selection
of “smaller” constraints. The size of a constraint set could be defined by the number of minimal
constraints that it contains. These minimal constraint sets constrain a single state, action, or feature,
and were introduced in Section 3.2 as Csi, Cai, and Cφi, respectively. While penalizing this definition
of size would effectively discourage overfitting, considering every possible combination of minimal
constraints causes the hypothesis space to grow exponentially in the number of states, actions, and
features of the MDP, which may make directly solving this formulation intractable.
Another approach, which avoids this combinatorial explosion, is to use the minimal constraint
sets themselves, not their combinations, as our hypothesis space, and to select from among these
constraints in an iterative, greedy manner. By iteratively selecting individual minimal constraint sets,
and choosing a proper stopping condition, it is possible to gradually grow the full estimated constraint
6
Published as a conference paper at ICLR 2020
set and avoid overfitting to the demonstrations. It is this method that we will utilize in this work.
Section 3.4 details our approach for selecting the most likely minimal constraint, and Section 3.5
details our approach for iteratively growing the estimated constraint set.
3.4 Probability Mass for Minimal Constraints
As detailed in Section 3.3, the most
likely constraint set is the one whose
eliminated trajectories Ξ-MC have the
highest probability of being demon-
strated on the original, unconstrained
MDP. Therefore, to find the most
likely of the minimal constraints, we
must find the expected proportion of
trajectories which will contain any
state or action, or accrue any feature.
By using our augmented indicator fea-
ture map from (1), we can reduce this
problem to only examine feature ac-
cruals. Ziebart et al. (2008) present
their forward-backward algorithm for
calculating expected feature counts
for an agent following a policy in the
maximum entropy setting. This al-
gorithm nearly suffices for our pur-
poses, but it computes the expectation
of the total number of times a feature
will be accrued (i.e. how often will
this feature be observed per trajec-
tory), rather than the expectation of
the number of trajectories that will ac-
crue that feature at any point in time.
To address this problem, we present a
modified form of the “forward” pass
as Algorithm 1. Our algorithm tracks
state visitations as well as feature ac-
cruals at each state, which allows us
to produce the same maximum en-
Algorithm 1 Feature Accrual History Calculation
Input: an MDP M, a policy π(a∣s,t), a time horizon T
Output: expected feature accrual history Φ[1,T]
/* Initialize state visitation and feature accrual history */
1: for s ∈ S do
2：	Ds,0 - DO(S)
3:	φ s,0 - 0nφ×1
4： end for
/* Track feature accruals over the time horizon */
5:	for t ∈ [0, T - 1] do
6:	for s ∈ S do
7:	for a ∈ As do
8:	/*	New feature accruals */
9:	/*	“” denotes element-wise multiplication	*/
10:	δφs,MoO - φ (S, a) Θ IDs,t1nφ×1 - φs,tJ
11:	end	for
12:	end for
13:	for S0 ∈ S do
14:	Ds0,t+1 - P P Ds,t∏(a∖s,-t)P(s0∣s,α)
15:	φs0,t+1 -
P P	Φe s,t + ∆Φe s,t(a) π(a∖S, t)P (S0∖S, a)
s∈S a∈As
16:	end for
17:	Φt+1 - P Φs,t+1
s∈S
18:	end for
19:	Return Φ[1,T]
tropy distribution over trajectories as Ziebart et al. (2008) while not counting additional accruals for
trajectories that have already accrued a feature.
The input of Algorithm 1 includes the MDP itself, a time horizon, and a time-varying policy. This
policy should capture the expected behavior of the demonstrator on the nominal MDP M, and it
can be computed via the “backward” part of the algorithm from Ziebart et al. (2008). The output of
Algorithm 1, Φ[1,T], is an nφ × T array such that the t-th column Φt is a vector whose i-th entry is
the expected proportion of trajectories to have accrued the i-th feature by time t. In particular, the
i-th element of ΦT is equal to PM(Ξ-MCi ), which allows us to now directly select the most likely
constraint according to (8).
3.5 Maximum-Coverage-Based Iterative Constraint Inference
When using minimal constraint sets as the constraint hypothesis space, it is possible that the most
likely constraint still does not provide a satisfactory explanation for the demonstrated behavior. In
this case, it can be beneficial to combine minimal constraints. If the task of solving (8) is framed as
finding the combination of constraint sets that “covers” the most probability mass, then the problem
becomes a direct analog for the classic maximum coverage problem. While this problem is known to
be NP-hard, there exist a simple greedy algorithm with known suboptimality bounds (Hochbaum &
Pathria, 1998).
7
Published as a conference paper at ICLR 2020
We present Algorithm 2 as our approach for adapting this greedy heuristic to solve the problem of
constraint inference. At each iteration, we grow our estimated constraint set by augmenting it with
the constraint set in our hypothesis space that covers the most currently uncovered probability mass.
By analogy to the maximum coverage problem (Hochbaum & Pathria, 1998), we derive the following
bound on the suboptimality of our approach.
Theorem 1. Let Cnc be the set of all constraints Cnc such that Cnc = in=c 1 Ci for Ci ∈ C, and let
CnC be the solution to (8) using Cnc as the constraint hypothesis space. Itfollows, then, that at the
end of every iteration i of Algorithm 2,
P (ξmc*
P工以
This bound is directly analogous
to the suboptimality bound for the
greedy solution to the maximum cov-
erage problem proven by Hochbaum
& Pathria (1998). For space, the proof
is included in the appendix.
Rather than selecting the number of
constraints nc to be used ahead of
time, we check a stopping condition
to decide if we should continue to add
constraints. Because we are attempt-
ing to maximize PMC (D), it might
seem natural to use a probability-
based stopping condition. However,
choosing a stopping criterion based
on probability is problematic because
Algorithm 2 Greedy Iterative Constraint Inference
1
2
3
4
5
6
7
8
9
10
Input: MDP M, constraint hypothesis space C,
empirical probability distribution PD , threshold dDKL
Output: estimated constraint set C*
for i ∈ [1, |C| ] do
Ci J solution to (8) using M C*, C, and D
δDKL = DKL(PD || PMC* ) - DKL(PD || PMC*∪cj
if ∆DKL ≤ dDKL then
break
end if
^ ^
Cb* J Cb * ∪ Ci
end for
Return Cb*
the probability of observing a set of demonstrations is dependent on the number of demonstrations
in the set, even if each individual demonstration has the same probability. We instead base our
stopping condition on KL divergence, which depends only on the distribution of trajectories in D and
not on the number of demonstrations. The quantity DKL PD || PMCb * provides a measure of how
well the distribution over trajectories induced by our inferred constraints, PMCb * , agrees with the
empirical probability distribution over trajectories observed in the demonstrations, PD . Using this KL
divergence in the stopping condition actually preserves a link to the probability of the demonstration
set, since the KL divergence will decrease monotonically as PMC (D) increases. The threshold
parameter dDKL is chosen to avoid overfitting to the demonstrations, combating the tendency to select
additional constraints that may only marginally better align our predictions with the demonstrations.
C* 一 0
4	Examples
4.1	Synthetic Grid World
We consider the grid world MDP presented in Figure 3. The environment consists of a 9-by-9 grid of
states, and the actions are to move up, down, left, right, or diagonally by one cell. The objective is to
move from the starting state in the bottom-left corner (s0) to the goal state in the bottom-right corner
(sG). Every state-action pair produces a distance feature, and the MDP reward is negative distance,
which encourages short trajectories. There are additionally two more features, denoted green and
blue, which are produced by taking actions from certain states, as shown in Figure 3.
The true MDP, from which agents generate trajectories, is shown in Figure 3a, including its constraints.
The nominal, more generic MDP shown in Figure 3b is what we take as M for applying the iterative
maximum likelihood constraint inference in Algorithm 2, with feature accruals estimated using
Algorithm 1. While Figures 3c through 3e show the iteratively estimated constraints, which align
with the true constraints, it is interesting to note that not all constraints present in the true MDP are
identified. For instance, it is so unlikely that an agent would ever select the up-left diagonal action,
that the fact that demonstrated trajectories did not contain that action is unsurprising and does not
make that action an estimated constraint.
8
Published as a conference paper at ICLR 2020
(a) True MDP
Features
(C) Cb*, nc = 1
(d) C*, nc = 2
(e) Cb*, nc = 6
Figure 3: Algorithm performanCe on a synthetiC grid world MDP. EaCh subfigure represents the MDP
by showing (CloCkwise from left) its states, aCtions, and features. EaCh element is shaded aCCording to
the proportion of trajeCtories that are expeCted to aCCrue the respeCtive augmented feature, Computed
via Algorithm 1. Constraints are marked with a red “X,” and bright bounding boxes mark the green
and blue feature-produCing states. The result here are shown for a set of 100 demonstrations sampled
aCCording to the expeCtation for the True MDP (a). We begin with the nominal MDP shown in (b),
and produCe (C), (d), and (e) by applying Algorithm 2. Note that (C), (d), and (e) show the seleCtions
of feature, aCtion, and state Constraints, respeCtively.
Figure 4 shows how the performanCe of our approaCh
varies based on the number of available demonstrations
and the seleCtion for the threshold dDKL . The false positive
rate shown in Figure 4a is the proportion of seleCted Con-
straints whiCh are not Constraints of the true system. We
Can observe two trends in this data that we would expeCt.
First, lower values of dDKL lead to greater false positive
rates sinCe they allow Algorithm 2 to Continue iterating
and aCCept Constraints that do less to align expeCtations
and demonstrations. SeCond, having more demonstrations
available provides more information and reduCes the rate
of false positives. Further, Figure 4b shows that more
demonstrations also allows the behavior prediCted by Con-
straints to better align with the observations. It is interest-
ing to note, however, that with fewer than 10 demonstra-
tions and a very low dDKL , we may produCe very low KL
divergenCe, but at the Cost of a high false positive rate. This
phenomenon highlights the role of seleCting dDKL to avoid
over-fitting. The threshold dDKL = 0.1 aChieves a good
balanCe of produCing few false positives with suffiCient
examples while also produCing lower KL divergenCes, and
we used this threshold to produCe the results in Figures 3
and 5.
4.2	Human Obstacle Avoidance
In our seCond example, we analyze trajeCtories from hu-
mans as they navigate around an obstaCle on the floor. We
map these Continuous trajeCtories into trajeCtories through
a grid world where eaCh Cell represents a 1ft-by-1ft area
(a) False positives
(b) DKL
Figure 4: Algorithm performanCe on the
synthetiC grid world. EaCh data point
represents the mean result of 10 indepen-
dent trajeCtory draws, and the margins
show ±1 standard error.
9
Published as a conference paper at ICLR 2020
on the ground. The human agents are attempting to reach a fixed goal state (sG) from a given initial
state (s0), as shown in Figure 5. We performed MaxEnt IRL on human demonstrations of the task
without the obstacle to obtain the nominal distance-based reward function. We restrict ourselves to
estimating only state constraints, as we do not supply our algorithm with knowledge of any additional
features in the environment and we assume that the humans’ motion is unrestrained.
Demonstrations were collected from 16 volunteers, and the
results of performing constraint inference are shown in Figure
5. Our method is able to successfully predict the existence ofa
central obstacle. While we do not estimate every constrained
state, the constraints that we do estimate make all of the
obstacle states unlikely to be visited. In order to identify those
states as additional constraints, we would have to decrease
our dDKL threshold, which could also lead to more spurious
constraint selections, such as the three shown in Figure 5.
5	Conclusion and Future Work
We have presented our novel technique for learning con-
straints from demonstrations. We improve upon previous
work in constraint-learning IRL by providing a principled
framework for identifying the most likely constraint(s), and
we do so in a way that explicitly makes state, action, and
feature constraints all directly comparable to one another. We
believe that the numerical results presented in Section 4 are
promising and highlight the usefulness of our approach.
Despite its benefits, one drawback of our approach is that
the formulation is based on (3), which only exactly holds for
deterministic MDPs. As mentioned in Section 3.3, we plan
to investigate the use of a maximum causal entropy approach
Figure 5: Human trajectories overlaid
on a grid world MDP. The shaded re-
gion represents an obstacle in the hu-
man’s environment, and the red “X”s
represent learned constraints.
to address this issue and fully handle stochastic MDPs. Additionally, the methods presented here
require all demonstrations to contain no violations of the constraints we will estimate. We believe
that softening this requirement, which would allow reasoning about the likelihood of constraints that
are occasionally violated in the demonstration set, may be beneficial in cases where trajectory data is
collected without explicit labels of success or failure. Finally, the structure of Algorithm 1, which
tracks the expected features accruals of trajectories over time, suggests that we may be able to reason
about non-Markovian constraints by using this historical information to our advantage.
Overall, we believe that our formulation of maximum likelihood constraint inference for IRL shows
promising results and presents attractive avenues for further investigation.
Acknowledgments
This work is supported by the National Science Foundation through grant CNS-1545126 (VeHICaL).
We would like to thank Jaime Fisac, Andrea Bajcsy, Sylvia Herbert, and David Fridovich-Keil for
their insightful discussions and for sharing their collected data.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of reinforcement
learning to aerobatic helicopter flight. In Advances in neural information processing systems, pp.
1-8, 2007.
Dario Amodei, Chris Olah, Jacob Steinhardt, PaUl F. Christiano, John Schulman, and Dan Mane.
Concrete problems in AI safety. arXiv, arXiv:1606.06565, 2016.
10
Published as a conference paper at ICLR 2020
Glen Chou, Dmitry Berenson, and Necmiye Ozay. Learning constraints from demonstrations. In
Springer Proceedings in Advanced Robotics (SPAR) series on 2018 Workshop on the Algorithmic
Foundations of Robotics (WAFR 2018). Springer, 2018.
Mark K Ho, Michael L Littman, Fiery Cushman, and Joseph L Austerweil. Teaching with rewards
and punishments: Reinforcement or communication? In CogSci, 2015.
Dorit S Hochbaum and Anu Pathria. Analysis of the greedy approach in problems of maximum
k-coverage. Naval Research Logistics (NRL), 45(6):615-627,1998.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
Rudolf Emil Kalman. When is a linear control system optimal? Journal of Basic Engineering, 86(1):
51-60, 1964.
David L. McPherson, Dexter R.R. Scobee, Joseph Menke, Allen Y Yang, and S. Shankar Sastry.
Modeling supervisor safe sets for improving collaboration in human-robot teams. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 861-868, Oct 2018. doi:
10.1109/IROS.2018.8593865.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of
the Seventeenth International Conference on Machine Learning, pp. 663-670. Morgan Kaufmann
Publishers Inc., 2000.
Michael Pardowitz, Raoul Zollner, and Rudiger Dillmann. Learning sequential constraints of tasks
from user demonstrations. In 5th IEEE-RAS International Conference on Humanoid Robots, 2005.,
pp. 424-429. IEEE, 2005.
Claudia PereZ-D'Arpino and Julie A Shah. C-learn: Learning geometric constraints from demonstra-
tions for multi-step manipulation in shared autonomy. In 2017 IEEE International Conference on
Robotics and Automation (ICRA), pp. 4058-4065. IEEE, 2017.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In
Proceedings of the 23rd international conference on Machine learning, pp. 729-736. ACM, 2006.
Dexter R R Scobee, Vicenc Rubies Royo, Claire J Tomlin, and S Shankar Sastry. Haptic assistance
via inverse reinforcement learning. In 2018 IEEE International Conference on Systems, Man, and
Cybernetics (SMC), pp. 1510-1517. IEEE, 2018.
Guru Subramani, Michael Zinn, and Michael Gleicher. Inferring geometric constraints in human
demonstrations. In Conference on Robot Learning, pp. 223-236, 2018.
Dizan Vasquez, Billy Okal, and Kai O Arras. Inverse reinforcement learning algorithms and features
for robot navigation in crowds: an experimental comparison. In 2014 IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp. 1341-1346. IEEE, 2014.
Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K Ho, and Sanjit Seshia. Learning task
specifications from demonstrations. In Advances in Neural Information Processing Systems, pp.
5367-5377, 2018.
Brian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, Carnegie Mellon University, 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
11
Published as a conference paper at ICLR 2020
Appendix
A Adding constraints to Stochastic MDPs
For MDPs with stochastic transitions, the semantics of an empty state are less obvious and could lend
themselves to multiple interpretations depending on the nature of the system being modeled. In our
context, we use constraints to describe how observed behaviors from demonstrations differ from pos-
sible behaviors allowed by the nominal MDP structure. We therefore assume that any demonstrations
provided are, by the fact that they were selected to be provided, consistent with the system’s con-
straints, including avoiding empty states. This assumption implies that any stochastic state transitions
that would have led to an empty state will not be observed in trajectories from the demonstration set.
The omission of these transitions means that, for a given (s, a), if Ps,a(Sempty) = p, then a proportion
p of these (s, a) pairs which occur as an agent navigates the environment will be excluded from
demonstrations. Therefore, as we modify the MDP to reason about demonstrated behavior, we need
updated transition probabilities which eliminate the probability mass of transitioning to empty states,
an event which will never be observed in a demonstration. Such modified probabilities can be given
as
PSCa(S') = 10 P…
I 1- PS,a( Sempty )
if s ∈ Sempty
otherwise
(9)
We must also capture the change to observed state-action pair frequencies by understanding that any
observed policy πC will be related to an agent’s actual policy π according to
πC (a|s, t)
n(a|s,t)(I - Ps,a ( Sempty ))
P ∏(a0∣S,t)(1 - Ps,a(Sempty))
a0∈AsC
(10)
It is important to note that the modifications presented in (9) and (10) for non-deterministic MDPs
are not meant to directly reflect the reality of the underlying system (we wouldn’t expect the actual
transition dynamics to change, for instance), but to reflect the apparent behavior that we would expect
to observe in the subset of trajectories that would be selected as demonstrations. We further note
that applying these modifications to deterministic MDPs will result in the same expected behavior as
augmenting the constraint set with Cempty.
B Proof for Theorem 1
Theorem 1. Let Cnc be the set of all constraints Cnc such that Cnc = in=c 1 Ci for Ci ∈ C, and let
Cnc be the solution to (8) using Cnc as the constraint hypothesis space. Itfollows, then, that at the
end of every iteration i of Algorithm 2,
P (ξmc*
P工以
Proof. The problem of finding CnC is analogous to solving the maximum coverage problem, dis-
cussed by Hochbaum & Pathria (1998), where the set of elements to be covered is the set of trajectories
{ξ | ∃ C ∈ C : ξ ∈ Ξ'mc and D ∩ Ξ~mc = 0} and the weight of each element ξ is PM(ξ). Because
Algorithm 2 constructs C * iteratively by taking the union of the previous value of C * and the set
Ci ∈ C which solves (8), the value of C* at the end of the i-th iteration is analogous to the greedy
solution of the maximum coverage problem with nc = i. Therefore, we can directly apply the
suboptimality bound for the greedy solution proven in Hochbaum & Pathria (1998) to arrive at our
given bound on eliminated probability mass.
12