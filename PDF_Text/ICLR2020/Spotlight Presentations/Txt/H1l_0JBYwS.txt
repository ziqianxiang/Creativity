Published as a conference paper at ICLR 2020
Spectral Embedding of Regularized Block
Models
Nathan De Lara & Thomas Bonald
Institut Polytechnique de Paris
Paris, France
{nathan.delara, thomas.bonald}@telecom-paris.fr
Ab stract
Spectral embedding is a popular technique for the representation of graph data.
Several regularization techniques have been proposed to improve the quality of
the embedding with respect to downstream tasks like clustering. In this paper, we
explain on a simple block model the impact of the complete graph regularization,
whereby a constant is added to all entries of the adjacency matrix. Specifically, we
show that the regularization forces the spectral embedding to focus on the largest
blocks, making the representation less sensitive to noise or outliers. We illustrate
these results on both on both synthetic and real data, showing how regularization
improves standard clustering scores.
1 Introduction
Spectral embedding is a standard technique for the representation of graph data (Ng et al., 2002;
Belkin & Niyogi, 2002). Given the adjacency matrix A ∈ Rn+×n of the graph, it is obtained by
solving either the eigenvalue problem:
LX = XΛ, with XTX = I,	(1)
or the generalized eigenvalue problem:
LX = DXΛ, with XT DX = I,	(2)
where D = diag(A1n) is the degree matrix, with 1n the all-ones vector of dimension n, L = D - A
is the Laplacian matrix of the graph, Λ ∈ Rk×k is the diagonal matrix of the k smallest (generalized)
eigenvalues of L and X ∈ Rn×k is the corresponding matrix of (generalized) eigenvectors. In this
paper, we only consider the generalized eigenvalue problem, whose solution is given by the spectral
decomposition of the normalized Laplacian matrix Lnorm = I - D-1/2AD-1/2 (Luxburg, 2007).
The spectral embedding can be interpreted as equilibrium states of some physical systems (Snell &
Doyle, 2000; Spielman, 2007; Bonald et al., 2018), a desirable property in modern machine learning.
However, it tends to produce poor results on real datasets if applied directly on the graph (Amini
et al., 2013). One reason is that real graphs are most often disconnected due to noise or outliers in
the dataset.
In order to improve the quality of the embedding, two main types of regularization have been pro-
posed. The first artificially increases the degree of each node by a constant factor (Chaudhuri et al.,
2012; Qin & Rohe, 2013), while the second adds a constant to all entries of the original adjacency
matrix (Amini et al., 2013; Joseph et al., 2016; Zhang & Rohe, 2018). In the practically interesting
case where the original adjacency matrix A is sparse, the regularized adjacency matrix is dense but
has a so-called sparse + low rank structure, enabling the computation of the spectral embedding on
very large graphs (Lara, 2019).
While (Zhang & Rohe, 2018) explains the effects of regularization through graph conductance and
(Joseph et al., 2016) through eigenvector perturbation on the Stochastic Block Model, there is no
simple interpretation of the benefits of graph regularization. In this paper, we show on a simple
block model that the complete graph regularization forces the spectral embedding to separate the
blocks in decreasing order of size, making the embedding less sensitive to noise or outliers in the
data.
1
Published as a conference paper at ICLR 2020
Indeed, (Zhang & Rohe, 2018) identified that, without regularization, the cuts corresponding to the
first dimensions of the spectral embedding tend to separate small sets of nodes, so-called dangling
sets, loosely connected to the rest of the graph. Our work shows more explicitly that regularization
forces the spectral embedding to focus on the largest clusters. Moreover, our analysis involves some
explicit characterization of the eigenvalues, allowing us to quantify the impact of the regularization
parameter.
The rest of this paper is organized as follows. Section 2 presents block models and an important
preliminary result about their aggregation. Section 3 presents the main result of the paper, about
the regularization of block models, while Section 4 extends this result to bipartite graphs. Section 5
presents the experiments and Section 6 concludes the paper.
2	Aggregation of Block Models
Let A ∈ Rn+×n be the adjacency matrix of an undirected, weight graph, that is a symmetric matrix
such that Aij > 0 if and only if there is an edge between nodes i and j , with weight Aij . Assume
that the n nodes of the graph can be partitioned into K blocks of respective sizes n1 , . . . , nK so
that any two nodes of the same block have the same neighborhood, i.e., the corresponding rows (or
columns) of A are the same. Without any loss of generality, we assume that the matrix A has rank
K . We refer to such a graph as a block model.
Let Z ∈ Rn×K be the associated membership matrix, with Zij = 1 if index i belongs to block j
and 0 otherwise. We denote by W = ZTZ ∈ RK×K the diagonal matrix of block sizes.
Now define A = ZT AZ ∈ RK × K. This is the adjacency matrix of the aggregate graph, where each
block of the initial graph is replaced by a single node; two nodes in this graph are connected by an
edge of weight equal to the total weight of edges between the corresponding blocks in the original
graph. We denote by D = diag(AlK) the degree matrix and by L = DD - A the Laplacian matrix
of the aggregate graph.
The following result shows that the solution to the generalized eigenvalue problem (2) follows from
that of the aggregate graph:
Proposition 1. Let x be a solution to the generalized eigenvalue problem:
Lx = λDx.	(3)
Then either ZTx = 0 and λ = 1 or x = Zy where y is a solution to the generalized eigenvalue
problem:
L y = λD y.	(4)
Proof. Consider the following reformulation of the generalized eigenvalue problem (3):
Ax = Dx(1 - λ).	(5)
Since the rank of A is equal to K, there are n - K eigenvectors x associated with the eigenvalue
λ = 1, each satisfying ZTx = 0. By orthogonality, the other eigenvectors satisfy x = Zy for some
vector y ∈ RK . We get:
AZy = DZy(1 - λ),
so that
Ay = Dy(1- λ).
Thus y is a solution to the generalized eigenvalue problem (4).	□
3	Regularization of Block Models
Let A be the adjacency matrix of some undirected graph. We consider a regularized version of the
graph where an edge of weight α is added between all pairs of nodes, for some constant α > 0. The
corresponding adjacency matrix is given by:
Aα = A + αJ,
2
Published as a conference paper at ICLR 2020
where J = 1n1Tn is the all-ones matrix of same dimension as A. We denote by Dα = diag(Aα1n)
the corresponding degree matrix and by Lα = Dα - Aα the Laplacian matrix.
We first consider a simple block model where the graph consists of K disjoint cliques of respective
sizes nι > n > •…> nκ nodes, with nκ ≥ 1. In this case, We have A = ZZT, where Z is the
membership matrix.
The objective of this section is to demonstrate that, in this setting, the k-th dimension of the spectral
embedding isolates the k - 1 largest cliques from the rest of the graph, for any k ∈ {2, . . . , K}
Lemma 1. Let λ1 ≤ λ2 ≤ . . . ≤ λn be the eigenvalues associated with the generalized eigenvalue
problem:
Lαx = λDαx.	(6)
We have λ1 = 0 < λ2 ≤ . . . ≤ λK < λK+1 = . . . = λn = 1.
Proof. Since the Laplacian matrix Lα is positive semi-definite, all eigenvalues are non-negative
(Chung, 1997). We know that the eigenvalue 0 has multiplicity 1 on observing that the regularized
graph is connected. Now for any vector x,
xTAα x = xTAx + αxT Jx = ||Z T x||2 + α(1nTx)2 ≥0,
so that the matrix Aα is positive semi-definite. In view of (5), this shows that λ ≤ 1 for any
eigenvalue λ. The proof then follows from Proposition 1, on observing that the eigenvalue 1 has
multiplicity n 一 K.	□
Lemma 2. Let x be a solution to the generalized eigenvalue problem (6) with λ ∈ (0, 1). There
exists some s ∈ {+1, 一1} such that for each node i in block j,
Sign(Xi) = S ^⇒	nj ≥ ɑɪ ʌn.
λ
Proof. In view of Proposition 1, we have x = Zy where y is a solution to the generalized eigenvalue
problem of the aggregate graph, with adjacency matrix:
Aa = Z T AaZ = Z T(A + αJ )Z.
Since A = ZZT and W = ZTZ, we have Aa = W2 + αZT JZ. Using the fact that Z 1k = 1n,
we get J = 1n 1Tn = ZJKZT with JK = 1K 1TK the all-ones matrix of dimension K × K, so that:
Aa = W (IK + αJκ )W,
where IK is the identity matrix of dimension K × K. We deduce the degree matrix:
D a = W (W + αnIκ),
and the Laplacian matrix:
La = Da - Aa = αW(nIκ 一 JKW).
The generalized eigenvalue problem associated with the aggregate graph is:
L a y = λD ay.
After multiplication by W-1, we get:
α(nIK 一 JKW)y = λ(W + αnIK)y.
Observing that JKWy = 1k 1KKWy = (1KKWy)1κ H 1k, we conclude that:
(an(1 — λ) — λW)y H 1k,	(7)
and since W = diag(n1, . . . , nK),
∀j = ι,...,κ, yj H  --------1-__—.	(8)
λnj 一 α(1 一 λ)n
The result then follows from the fact that X = Zy.	□
3
Published as a conference paper at ICLR 2020
Lemma 3. The K smallest eigenvalues satisfy:
0 = λι < μι < λ2 < μ2 < …< λκ < μκ,
where for all j = 1, . . . , K,
αn
μ j —	.
αn + nj
Proof. We know from Lemma 1 that the K smallest eigenvalues are in [0, 1). Let x be a solution
to the generalized eigenvalue problem (6) with λ ∈ (0, 1). We know that x = Zy where y is
an eigenvector associated with the same eigenvalue λ for the aggregate graph. Since 1K is an
eigenvector for the eigenvalue 0, We have yτDα1κ — 0. Using the fact that Dα — W(W + anlκ),
we get:
K
nj(nj + αn)yj — 0.
j=1
We then deduce from (7) and (8) that λ ∈ {μι,..., μκ} and
K1
Enj (nj+αn) λ∕-μ--1 = 0.
j=1	j
This condition cannot be satisfied if λ < μι or λ > μκ as the terms of the sum would be either all
positive or all negative.
Now let y be another eigenvector for the aggregate graph, with yτDαyf — 0, for the eigenvalue
λ0 ∈ (0, 1). By the same argument, we get:
K
nj(nj + αn)yjyj0 — 0,
j=1
and
K
nj (nj +αn)
j=1
1
1
λ∕μj — 1 λ0∕μj — 1
with λ0 ∈ {μι,...,μκ}. This condition cannot be satisfied if λ and λ0 are in the same interval
(μj, μj+ι) for some j as the terms in the sum would be all positive. There are K - 1 eigenvalues in
□
(0, 1) for K - 1 such intervals, that is one eigenvalue per interval.
The main result of the paper is the following, showing that the k - 1 largest cliques of the original
graph can be recovered from the spectral embedding of the regularized graph in dimension k.
Theorem 1.	Let X be the spectral embedding of dimension k, as defined by (2), for some k in the
set {2, . . . , K}. Then sign(X) gives the k - 1 largest blocks of the graph.
Proof. Let x be the j-th column of the matrix X, for some j ∈ {2, . . . , k}. In view of Lemma 3,
this is the eigenvector associated with eigenvalue λ- ∈ (μ--ι,μj), so that
1 - λj
α-------n ∈ (nj-i,n-).
λj
In view of Lemma 2, all entries ofx corresponding to blocks of size n1, n2 . . . , nj-1 have the same
sign, the other having the opposite sign.	□
Theorem 1 can be extended in several ways. First, the assumption of distinct block sizes can easily be
relaxed. If there are L distinct values of block sizes, say m1, . . . , mL blocks of sizes n1 > . . . > nL,
there are L distinct values for the thresholds μ- and thus L distinct values for the eigenvalues λ- in
[0, 1), the multiplicity ofthej-th smallest eigenvalue being equal to mj. The spectral embedding in
dimension k still gives k - 1 cliques of the largest sizes.
4
Published as a conference paper at ICLR 2020
Second, the graph may have edges between blocks. Taking A = ZZT + εJ for instance, for some
parameter ε ≥ 0, the results are exactly the same, with α replaced by +α. A key observation is that
regularization really matters when ε → 0, in which case the initial graph becomes disconnected and,
in the absence of regularization, the spectral embedding may isolate small connected components
of the graph. In particular, the regularization makes the spectral embedding much less sensitive to
noise, as will be demonstrated in the experiments.
Finally, degree correction can be added by varying the node degrees within blocks. Taking A =
θZZTθ, for some arbitrary diagonal matrix θ with positive entries, similar results can be obtained
under the regularization Aα = A+αθJθ. Interestingly, the spectral embedding in dimension k then
recovers the k - 1 largest blocks in terms of normalized weight, the ratio of the total weight of the
block to the number of nodes in the block.
4 Regularization of B ipartite Graphs
Let B = Rn+×m be the biadjacency matrix of some bipartite graph with respectively n, m nodes in
each part, i.e., Bij > 0 if and only if there is an edge between node i in the first part of the graph
and node j in the second part of the graph, with weight Bij . This is an undirected graph of n + m
nodes with adjacency matrix:
A
0B
BT	0
The spectral embedding of the graph (2) can be written in terms of the biadjacency matrix as follows:
BX2 = D1X1(I - Λ)
BTX1 = D2X2(I - Λ)
(9)
where X1 , X2 are the embeddings of each part of the graph, with respective dimensions n × k and
m × k, D1 = diag(B1m) and D2 = diag(BT1n). In particular, the spectral embedding of the graph
follows from the generalized SVD of the biadjacency matrix B.
The complete regularization adds edges between all pairs of nodes, breaking the bipartite structure
of the graph. Another approach consists in applying the regularization to the biadjacency matrix,
i.e., in considering the regularized bipartite graph with biadjacency matrix:
Bα = B + αJ,
where J = 1n1Tm is here the all-ones matrix of same dimension as B. The spectral embedding of
the regularized graph is that associated with the adjacency matrix:
Aα
0	Bα
BαT	0
(10)
As in Section 3, we consider a block model so that the biadjacency matrix B is block-diagonal with
all-ones block matrices on the diagonal. Each part of the graph consists of K groups of nodes of
respective sizes n1 > . . . > nK and m1 > . . . > mK, with nodes of block j in the first part
connected only to nodes of block j in the second part, for all j = 1, . . . , K.
We consider the generalized eigenvalue problem (6) associated with the above matrix Aα . In view
of (9), this is equivalent to the generalized SVD of the regularized biadjacency matrix Bα . We have
the following results, whose proofs are deferred to the appendix:
Lemma 4. Let λ1 ≤ λ2 ≤ . . . ≤ λn be the eigenvalues associated with the generalized eigenvalue
problem (6). We have λ1 = 0 < λ2 ≤ . . . ≤ λK < λK+1 = . . . = λn-2K < . . . < λn = 2.
Lemma 5. Let x be a solution to the generalized eigenvalue problem (6) with λ ∈ (0, 1). There
exists s1, s2 ∈ {+1, -1} such that for each node i in block j of part p ∈ {1, 2},
njmj
Sign(Xi) = Sp < ⇒	7	.	≥	, ʌ ≥ 1 - λ.
(nj + αn)(mj + αm)
Lemma 6. The K smallest eigenvalues satisfy:
0 = λι < μι < λ2 < μ2 < …< λκ < μκ,
where for all j = 1, . . . , K,
μj = 1- 7-------njm--------7.
(nj + αn)(mj + αm)
5
Published as a conference paper at ICLR 2020
Theorem 2.	Let X be the spectral embedding of dimension k, as defined by (2), for some k in the
set {2, . . . , K}. Then sign(X) gives the k - 1 largest blocks of each part of the graph.
Like Theorem 1, the assumption of decreasing block sizes can easily be relaxed. Assume that block
pairs are indexed in decreasing order of μj. Then the spectral embedding of dimension k gives the
k - 1 first block pairs for that order. Itis interesting to notice that the order now depends on α: when
α → 0+, the block pairs j of highest value (n + 箸)-1 (equivalently, highest harmonic mean of
proportions of nodes in each part of the graph) are isolated first; when α → +∞, the block pairs
j of highest value njm (equivalently, the highest geometric mean of proportions of nodes in each
part of the graph) are isolated first.
The results also extend to non-block diagonal biadjacency matrices B and degree-corrected models,
as for Theorem 1.
5	Experiments
We now illustrate the impact of regularization on the quality of spectral embedding. We focus on a
clustering task, using both synthetic and real datasets where the ground-truth clusters are known. In
all experiments, we skip the first dimension of the spectral embedding as it is not informative (the
corresponding eigenvector is the all-ones vector, up to some multiplicative constant). The code to
reproduce these experiments is available online1.
5.1	Toy graph
We first illustrate the theoretical results of the paper with a toy graph consisting of 3 cliques of
respective sizes 5, 3, 2. We compute the spectral embeddings in dimension 1, using the second
smallest eigenvalue. Denoting by Z the membership matrix, we get X ≈ Z(-0.08, 0.11, 0.05)T
for α = 1, showing that the embedding isolates the largest cluster; this is not the case in the absence
of regularization, where X ≈ Z (0.1, -0.1, 0.41)T.
5.2	Datasets
This section describes the datasets used in our experiments. All graphs are considered as undirected.
Table 1 presents the main features of the graphs.
Stochastic Block-Model (SBM) We generate 100 instances of the same stochastic block model
(Holland et al., 1983). There are 100 blocks of size 20, with intra-block edge probability set to 0.5
for the first 50 blocks and 0.05 for the other blocks. The inter-block edge probability is set to 0.001
Other sets of parameters can be tested using the code available online. The ground-truth cluster of
each node corresponds to its block.
20newsgroup (NG) This dataset consists of around 18000 newsgroups posts on 20 topics. This
defines a weighted bipartite graph between documents and words. The label of each document
corresponds to the topic.
Wikipedia for Schools (WS) (Haruechaiyasak & Damrongrat, 2008). This is the graph of hyper-
links between a subset of Wikipedia pages. The label of each page is its category (e.g., countries,
mammals, physics).
Table 1: Main features of the graphs.
dataset	SBM	NG	WS
# nodes (n)	2000^^	10723	4591
# edges	≈ 5.103	≈ 2.106	≈ 2.105
# clusters in ground truth	100	20	14
1 https://github.com/nathandelara/Spectral- Embedding- of- Regularized- Block- Models/
6
Published as a conference paper at ICLR 2020
5.3	Metrics
We consider a large set of metrics from the clustering literature. All metrics are upper-bounded by
1 and the higher the score the better.
Homogeneity (H), Completeness (C) and V-measure score (V) (Rosenberg & Hirschberg,
2007). Supervised metrics. A cluster is homogeneous if all its data points are members of a single
class in the ground truth. A clustering is complete if all the members of a class in the ground truth
belong to the same cluster in the prediction. Harmonic mean of homogeneity and completeness.
Adjusted Rand Index (ARI) (Hubert & Arabie, 1985). Supervised metric. This is the corrected
for chance version of the Rand Index which is itself an accuracy on pairs of samples.
Adjusted Mutual Information (AMI) (Vinh et al., 2010) Supervised metric. Adjusted for chance
version of the mutual information.
Fowlkes-Mallows Index (FMI) (Fowlkes & Mallows, 1983). Supervised metric. Geometric mean
between precision and recall on the edge classification task, as described for the ARI.
Modularity (Q) (Newman, 2006). Unsupervised metric. Fraction of edges within clusters com-
pared to that is some null model where edges are shuffled at random.
Normalized Standard Deviation (NSD) Unsupervised metric. 1 minus normalized standard de-
viation in cluster size.
5.4	Experimental setup
All graphs are embedded in dimension 20, with different regularization parameters. To compare
the impact of this parameter across different datasets, we use a relative regularization parameter
(w∕n2)α, where W = 1TA1n is the total weight of the graph.
We use the K-Means algorithm with to cluster the nodes in the embedding space. The parameter
K is set to the ground-truth number of clusters (other experiments with different values of K are
reported in the Appendix). We use the Scikit-learn (Pedregosa et al., 2011) implementation of K-
Means and the metrics, when available. The spectral embedding and the modularity are computed
with the Scikit-network package, see the documentation for more details2.
5.5	Results
We report the results in Table 2 for relative regularization parameter α = 0, 0.1, 1, 10. We see that
the regularization generally improves performance, the optimal value of α depending on both the
dataset and the score function. As suggested by Lemma 3, the optimal value of the regularization
parameter should depend on the distribution of cluster sizes, on which we do not have any prior
knowledge.
To test the impact of noise on the spectral embedding, we add isolated nodes with self loop to the
graph and compare the clustering performance with and without regularization. The number of
isolated nodes is given as a fraction of the initial number of nodes in the graph. Scores are computed
only on the initial nodes. The results are reported in Table 3 for the Wikipedia for Schools dataset.
We observe that, in the absence of regularization, the scores drop even with only 1% noise. The
computed clustering is a trivial partition with all initial nodes in the same cluster. This means that
the 20 first dimensions of the spectral embedding focus on the isolated nodes. On the other hand,
the scores remain approximately constant in the regularized case, which suggests that regularization
makes the embedding robust to this type of noise.
2 https://scikit- network.readthedocs.io/
7
Published as a conference paper at ICLR 2020
Table 2: Impact of regularization on clustering performance. SBM								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.19	0.27	0.22	0.0	0.01	0.03	0.45	0.76
0.1	0.33	0.35	0.34	0.0	0.01	0.01	0.52	0.91
1	0.36	0.37	0.36	0.0	0.01	0.01	0.50	0.92
10	0.28	0.34	0.30	0.0	0.00	0.02	0.36	0.78
NG								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.40	0.70	0.51	0.19	0.50	0.34	0.21	0.55
0.1	0.44	0.70	0.54	0.22	0.54	0.35	0.21	0.59
1	0.46	0.67	0.54	0.20	0.54	0.33	0.20	0.60
10	0.37	0.55	0.45	0.13	0.44	0.26	0.17	0.56
WS								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.23	0.29	0.25	0.05	0.25	0.26	0.25	0.49
0.1	0.26	0.29	0.28	0.10	0.27	0.26	0.29	0.61
1	0.23	0.24	0.23	0.04	0.23	0.20	0.30	0.65
10	0.19	0.22	0.20	0.00	0.19	0.20	0.23	0.53
Table 3: Impact of noise on clustering performance (WS dataset).
α = 0
noise	H	C	V	ARI	AMI	FMI	Q	std
0%	0.23	0.29	0.25	0.05	0.25	0.26	0.25	0.49
1%	0.00	0.49	0.00	0.00	0.00	0.39	0.00	0
5%	0.00	0.49	0.00	0.00	0.00	0.39	0.00	0
10%	0.00	0.49	0.00	0.00	0.00	0.39	0.00	0
α=1								
noise	H	C	V	ARI	AMI	FMI	Q	std
0%	0.23	0.24	0.23	0.04	0.23	0.2	0.3	0.65
1%	0.24	0.24	0.24	0.04	0.23	0.2	0.3	0.66
5%	0.23	0.23	0.23	0.05	0.22	0.2	0.3	0.67
10%	0.24	0.23	0.23	0.05	0.23	0.2	0.3	0.67
6	Conclusion and Perspectives
In this paper, we have provided a simple explanation for the well-known benefits of regularization
on spectral embedding. Specifically, regularization forces the embedding to focus on the largest
clusters, making the embedding more robust to noise. This result was obtained through the explicit
characterization of the embedding for a simple block model, and extended to bipartite graphs.
An interesting perspective of our work is the extension to stochastic block models, using for instance
the concentration results proved in (Lei et al., 2015; Le et al., 2017). Another problem of interest
is the impact of regularization on other downstream tasks, like link prediction. Finally, we would
like to further explore the impact of the regularization parameter, exploiting the theoretical results
presented in this paper.
8
Published as a conference paper at ICLR 2020
References
Arash A Amini, Aiyou Chen, Peter J Bickel, Elizaveta Levina, et al. Pseudo-likelihood methods for
community detection in large sparse networks. The Annals ofStatistics, 41(4):2097-2122, 2013.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In Advances in neural information processing systems, pp. 585-591, 2002.
Thomas Bonald, Alexandre Hollocou, and Marc Lelarge. Weighted spectral embedding of graphs.
In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton),
pp. 494-501. IEEE, 2018.
Kamalika Chaudhuri, Fan Chung, and Alexander Tsiatas. Spectral clustering of graphs with general
degrees in the extended planted partition model. In Conference on Learning Theory, pp. 35-1,
2012.
Fan RK Chung. Spectral graph theory. American Mathematical Soc., 1997.
Edward B Fowlkes and Colin L Mallows. A method for comparing two hierarchical clusterings.
Journal of the American statistical association, 78(383):553-569, 1983.
Choochart Haruechaiyasak and Chaianun Damrongrat. Article recommendation based on a topic
model for wikipedia selection for schools. In International Conference on Asian Digital Libraries,
pp. 339-342. Springer, 2008.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social networks, 5(2):109-137, 1983.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2(1):193-218,
1985.
Antony Joseph, Bin Yu, et al. Impact of regularization on spectral clustering. The Annals of Statis-
tics, 44(4):1765-1791, 2016.
Nathan De Lara. The sparse + low rank trick for matrix factorization-based graph algorithms. In
Proceedings of the 15th International Workshop on Mining and Learning with Graphs (MLG),
2019.
Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration and regularization of random
graphs. Random Structures & Algorithms, 51(3):538-561, 2017.
Jing Lei, Alessandro Rinaldo, et al. Consistency of spectral clustering in stochastic block models.
The Annals of Statistics, 43(1):215-237, 2015.
Ulrike Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416, De-
cember 2007. ISSN 0960-3174. doi: 10.1007/s11222-007-9033-z. URL http://dx.doi.
org/10.1007/s11222-007-9033-z.
Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national
academy of sciences, 103(23):8577-8582, 2006.
Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in neural information processing systems, pp. 849-856, 2002.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic block-
model. In Advances in Neural Information Processing Systems, pp. 3120-3128, 2013.
Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 joint conference on empirical methods in natural
language processing and computational natural language learning (EMNLP-CoNLL), pp. 410-
420, 2007.
9
Published as a conference paper at ICLR 2020
P Snell and Peter Doyle. Random walks and electric networks. Free Software Foundation, 2000.
Daniel A Spielman. Spectral graph theory and its applications. In Foundations of Computer Science,
2007. fOcS,07. 48th Annual IEEE Symposium on, pp. 29-38. IEEE, 2007.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings
comparison: Variants, properties, normalization and correction for chance. Journal of Machine
Learning Research, 11(Oct):2837-2854, 2010.
Yilin Zhang and Karl Rohe. Understanding regularized spectral clustering via graph conductance.
In Advances in Neural Information Processing Systems, pp. 10631-10640, 2018.
Appendix
We provide of proof of Theorem 2 as well as a complete set of experimental results.
A	Regularization of B ipartite Graphs
The proof of Theorem 2 follows the same workflow as that of Theorem 1. Let Z1 ∈ Rn×K and
Z2 ∈ Rm×K be the left and right membership matrices for the block matrix B ∈ Rn×m . The
aggregated matrix is B = ZTbZ? ∈ RK × K. The diagonal matrices of block sizes are W1 = ZTZ1
and W2 = Z2TZ2. We have the equivalent of Proposition 1:
Proposition 2. Let x1 , x2 be a solution to the generalized singular value problem:
Bx2 = σD1x1
B Tx1 = σD2 x2
Then either Z1T x1 = Z2T x2 = 0 and σ = 0 or x1 = Z1y1 and x2 = Z2y2 where y1, y2 is a solution
to the generalized singular value problem:
B BBy = σD 1y1,
I BTyι = σD2y2.
Proof. Since the rank of B is equal to K, there are n-K pairs of singular vectors (x1, x2) associated
with the singular values 0, each satisfying Z1T x1 = 0 and Z2T x2 = 0. By orthogonality, the other
pairs of singular vectors satisfy x1 = Z1y1 and x2 = Z2y2 for some vectors y1, y2 ∈ RK. By
replacing these in the original generalized singular value problem, we get that (y1, y2) is a solution
to the generalized singular value problem for the aggregate graph.	□
In the following, we focus on the block model described in Section 4, where B = Z1Z2T.
Proof of Lemma 4. The generalized eigenvalue problem (6) associated with the regularized matrix
Aα is equivalent to the generalized SVD of the regularized biadjacency matrix Bα:
Bαx2 = σDα,1x1
BαT x1 = σDα,2x2 ,
with σ = 1 - λ.
In view of Proposition 2, the singular value σ = 0 has multiplicity n - K, meaning that the eigen-
value λ = 1 has multiplicity n - K. Since the graph is connected, the eigenvalue 0 has multiplicity
1. The proof then follows from the observation that if (x1, x2) is a pair of singular vectors for the
singular value σ, then the vectors x = (x1, ±x2)T are eigenvectors for the eigenvalues 1 - σ, 1 + σ.
Proof of Lemma 5. By Proposition 2, we can focus on the generalized singular value problem for
the aggregate graph:
f BaB2 = σDα,1y1
B Bayι = σDα,2y2,
Since
Ba = W1(IK + αJK )W2,
10
Published as a conference paper at ICLR 2020
and
we have:
D Dα,ι = W1(W2 + αnI),
[Dα,2 = W2(W1 + αmI),
W1(IK+αJK)W2y2 = W1(W2 +αnI)y1σ,
W2(IK + αJK)W1y1 = W2(W1 + αmI)y2σ.
Observing that JKW1y1 Y 1k and JKW2y2 Y 1k, We get:
(W2 + αmlκ)yισ 一 W2y2 Y 1k,
(W1 + αnIK)y2σ - W1y1 Y 1K.
As tWo diagonal matrices commute, We obtain:
((Wι + anIK)(W2 + ɑmlκ)yισ ― W1W2y1 = (η1(W1 + anIK) + η2W2)lκ,
∖ (Wι + anIK)(W2 + amIK)y2σ 一 W1W2y2 = (η1W1 + η2(W2 + amIK))1k,
for some constants η1 , η2, and
{_	ηι(nj + an) + η2mj
y1,j	(nj + an)(mj + αm)σ 一 nmj)
η1 nj + η2 (mj + am)
y2 j = 7------U-----------ʌ--------.
,	(nj + an)(mj + am)σ 一 njmj
Letting si = ―sign(ηι(nj + an) + η2mj) and s2 = ―sign(ηιnj- + η2(mj + am)), We get:
sign(y1,j) = s1 O sign(y2j ) = Ss O (nj + anjmj + am) ≥ σ = 1 一 人
and the result folloWs from the fact that x1 = Z1y1 and x2 = Z2y2 .
Proof of Lemma 6. The proof is the same as that of Lemma 3, Where the threshold values folloW
from Lemma 5:
njmj
μj = 1 - 7	； w ；	?♦
(nj + an)(mj + am)
Proof of Theorem 2. Let x be the j-th column of the matrix X, for some j ∈ {2, . . . , k}. In vieW of
Lemma 6, this is the eigenvector associated with eigenvalue λj ∈ (μj-ι ,μj). In view of Lemma 4,
all entries ofx corresponding to blocks of size n1, n2 . . . , nj-1 have the same sign, the other having
the opposite sign.
B	Experimental Results
In this section, we present more extensive experimental results.
Tables 4 and 5 present results for the same experiment as in Table 2 but for different values of K,
namely K = 2 (bisection of the graph) and K = Ktruth/2 (half of the ground-truth value). As for
K = Ktrue , regularization generally improves clustering performance. However, the optimal value
of a remains both dataset dependent and metric dependent. Note that, for the NG and WS datasets,
the clustering remains trivial in the case K = 2, one cluster containing all the nodes, until a certain
amount of regularization.
Table 6 presents the different scores for both types of regularization on the NG dataset. As we can
see, preserving the bipartite structure of the graph leads to slightly better performance.
Finally, Table 7 shows the impact of regularization in the presence of noise for the NG dataset. The
conclusions are similar as for the WS dataset: regularization makes the spectral embedding much
more robust to noise.
11
Published as a conference paper at ICLR 2020
Table 4: Impact of regularization on clustering performance. K = 2. SBM								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.00	0.43	0.00	0.0	0.0	0.10	0.00	0.01
0.1	0.00	0.47	0.00	0.0	0.0	0.10	0.00	0.00
1	0.01	0.04	0.01	0.0	0.0	0.07	0.34	0.83
10	0.01	0.09	0.01	0.0	0.0	0.09	0.13	0.22
NG								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.00	0.36	0.00	0.00	0.00	0.23	0.00	0.00
0.1	0.00	0.36	0.00	0.00	0.00	0.23	0.00	0.00
1	0.15	0.72	0.25	0.06	0.25	0.28	0.16	0.63
10	0.12	0.61	0.20	0.04	0.20	0.26	0.13	0.51
WS								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.00	0.49	0.00	0.00	0.00	0.39	0.00	0.00
0.1	0.07	0.42	0.13	0.00	0.12	0.34	0.09	0.26
1	0.03	0.27	0.05	-0.01	0.05	0.35	0.09	0.13
10	0.02	0.16	0.04	-0.02	0.03	0.34	0.10	0.16
Table 5:		Impact of regularization on clustering performance. K SBM					= Ktrue/2.	
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.08	0.21	0.11	0.0	0.00	0.05	0.41	0.47
0.1	0.20	0.27	0.23	0.0	0.01	0.02	0.55	0.84
1	0.24	0.29	0.26	0.0	0.00	0.02	0.54	0.90
10	0.19	0.28	0.23	0.0	0.00	0.03	0.40	0.70
NG								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.27	0.76	0.40	0.11	0.39	0.31	0.20	0.41
0.1	0.28	0.73	0.41	0.11	0.40	0.30	0.18	0.43
1	0.38	0.72	0.50	0.18	0.50	0.34	0.21	0.57
10	0.31	0.62	0.42	0.11	0.42	0.27	0.17	0.51
WS								
α	H	C	V	ARI	AMI	FMI	Q	NSD
0	0.23	0.29	0.25	0.05	0.25	0.26	0.25	0.49
0.1	0.26	0.29	0.28	0.10	0.27	0.26	0.29	0.61
1	0.23	0.24	0.23	0.04	0.23	0.20	0.30	0.65
10	0.19	0.22	0.20	-0.00	0.19	0.20	0.23	0.53
12
Published as a conference paper at ICLR 2020
Table 6: Regularization of the adjacency vs. biadjacency matrix on the NG dataset (α = 1).
K = Ktrue/2
	H	C	V	ARI	AMI	FMI	Q	std
	 Adj.	0.38	0.72	0.50	0.18	0.50	0.34	0.21	0.57
Biadj.	0.41	0.72	0.52	0.19	0.52	0.35	0.21	0.61
K = Ktrue								
	H	C	V	ARI	AMI	FMI	Q	std
	 Adj.	0.46	0.67	0.54	0.20	0.54	0.33	0.2	0.60
Biadj.	0.47	0.68	0.56	0.21	0.55	0.34	0.2	0.61
Table 7: Impact of noise on clustering performance (NG dataset).
α = 0
noise	H	C	V	ARI	AMI	FMI	Q	std
0%	0.40	0.70	0.51	0.19	0.50	0.34	0.21	0.55
1%	0.00	1.00	0.00	0.00	0.00	0.23	0.00	0
5%	0.14	0.65	0.23	0.06	0.23	0.27	0.13	0.30
10%	0.00	0.36	0.01	0.00	0.00	0.23	0.00	0.00
α=1								
noise	H	C	V	ARI	AMI	FMI	Q	std
0%	0.46	0.67	0.54	0.20	0.54	0.33	0.2	0.60
1%	0.48	0.66	0.56	0.21	0.56	0.33	0.2	0.64
5%	0.49	0.66	0.56	0.23	0.56	0.34	0.2	0.66
10%	0.45	0.66	0.54	0.20	0.54	0.33	0.2	0.59
13