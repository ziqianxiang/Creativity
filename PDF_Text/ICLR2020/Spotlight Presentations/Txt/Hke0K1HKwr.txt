Published as a conference paper at ICLR 2020
Sequential Latent Knowledge Selection for
Knowledge-Grounded Dialogue
Byeongchang Kim Jaewoo Ahn Gunhee Kim
Department of Computer Science and Engineering
Seoul National University, Seoul, Korea
{byeongchang.kim,jaewoo.ahn}@vision.snu.ac.kr gunhee@snu.ac.kr
http://vision.snu.ac.kr/projects/skt
Ab stract
Knowledge-grounded dialogue is a task of generating an informative response
based on both discourse context and external knowledge. As we focus on bet-
ter modeling the knowledge selection in the multi-turn knowledge-grounded di-
alogue, we propose a sequential latent variable model as the first approach to
this matter. The model named sequential knowledge transformer (SKT) can keep
track of the prior and posterior distribution over knowledge; as a result, it can
not only reduce the ambiguity caused from the diversity in knowledge selec-
tion of conversation but also better leverage the response information for proper
choice of knowledge. Our experimental results show that the proposed model
improves the knowledge selection accuracy and subsequently the performance of
utterance generation. We achieve the new state-of-the-art performance on Wizard
of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging
benchmarks. We further validate the effectiveness of our model over existing con-
versation methods in another knowledge-based dialogue Holl-E dataset (Moghe
et al., 2018).
1 Introduction
Knowledge-grounded dialogue is a task of generating an informative response based on both dis-
course context and selected external knowledge (Ghazvininejad et al., 2018). For example, itis more
descriptive and engaging to respond “I’ve always been more of a fan of the American football team
from Pittsburgh, the Steelers!” than “Nice, I like football too.” (Dinan & Weston, 2019). As it has
been one of the key milestone tasks in conversational research (Zhang et al., 2018), a majority of
previous works have studied how to effectively combine given knowledge and dialogue context to
generate an utterance (Zhang et al., 2018; Li et al., 2019b; Parthasarathi & Pineau, 2018; Madotto
et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) proposed to tackle the
knowledge-grounded dialogue by decomposing it into two sub-problems: first selecting knowledge
from a large pool of candidates and generating a response based on the selected knowledge and
context.
In this work, we investigate the issue of knowledge selection in the multi-turn knowledge-grounded
dialogue, since practically the selection of pertinent topics is critical to better engage humans in
conversation, and technically the utterance generation becomes easier with a more powerful and
consistent knowledge selector in the system. Especially, we focus on developing a sequential latent
variable model for knowledge selection, which has not been discussed in previous research. We
believe it brings several advantages for more engaging and accurate knowledge-based chit-chat.
First, it can correctly deal with the diversity in knowledge selection of conversation. Since one can
choose any knowledge to carry on the conversation, there can be one-to-many relations between
dialogue context and knowledge selection. Such multimodality by nature makes the training of a
dialogue system much more difficult in a data-driven way. However, if we can sequentially model
the history of knowledge selection in previous turns, we can reduce the scope of probable knowledge
candidates at current turn. Second, the sequential latent model can better leverage the response
information, which makes knowledge selection even more accurate. It is naturally easy to select the
knowledge in the pool once the response is known, because the response is generated based on the
1
Published as a conference paper at ICLR 2020
Well, I help make sure people do not drown or
get injured while in or near the water!
Apprenti C)	、
(1) A IifegUard is a rescuer who supervises the safety ...
(2)kifeguards are strong swimmers and trained in ...
^8^In some areas, lifeguards are part of the emergency.
♦…
(L — 2) Despite the considerable amount of activity …
(L — 1) The season officially started on May in the ...
(L) These dates conventionally delimit the period of ...
Task 1 : Knowledge Selection
I,ve heard that in some places, lifeguards also
help with other sorts of emergencies!
Task 2 : Response Generation
Figure 1: An example of wizard,s tasks in
knowledge-grounded conversation of Wizard of
Wikipedia (Dinan et al., 2019).
Wizard
Table 1: Accuracy of knowledge selection with
and without knowing the response. We test with
GRU (Cho et al., 2014), Transformer (Vaswani
et al., 2017) and BERT (Devlin et al., 2019) as
the sentence encoder. For human evaluation, we
randomly sample 20 dialogues and ask human
annotators to select the most likely knowledge
sentence from the pool.
Methods	w/o response	w/ response
GRU	20.0	660
Transformer	22.5	70.4
	BERT		23.4	78.2
Transformer + GT history	25.4	70.4
BERT + GT hiStory	27.3	79.2
Random	27	27
Human	17.1	83.7
selected knowledge. Our sequential model can keep track of prior and posterior distribution over
knowledge, which are sequentially updated considering the responses in previous turns, and thus
we can better predict the knowledge by sampling from the posterior. Third, the latent model works
even when the knowledge selection labels for previous dialogue are not available, which is common
in practice. For example, if multiple people have discussion about given documents, knowledge
selection of previous turns is done by others. The latent model can infer which knowledge others
are likely to select and use.
Finally, the contributions of this work are as follows.
1.	We propose a novel model named sequential knowledge transformer (SKT). To the best of
our knowledge, our model is the first attempt to leverage a sequential latent variable model
for knowledge selection, which subsequently improves knowledge-grounded chit-chat.
2.	Our experimental results show that the proposed model improves not only the knowledge
selection accuracy but also the performance of utterance generation. As a result, we achieve
the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) and a
knowledge-annotated version of Holl-E (Moghe et al., 2018) dataset.
2	Problem Statement and Motivation
As a main testbed of our research, we choose the Wizard of Wikipedia (WoW) benchmark (Di-
nan et al., 2019), since it is one of the most large-scale and challenging datasets for open-domain
multi-turn knowledge-based dialogue. Moreover, the dataset can evaluate the algorithm’s ability for
solving the two subproblems of knowledge selection and response generation. That is, it provides
ground-truth labels of knowledge selection and clear grounding between the pairs of selected knowl-
edge and response. In our experiments, we also evaluate on Holl-E (Moghe et al., 2018) as another
dataset for knowledge-grounded dialogue, after collecting clearer labels of knowledge sentences.
The Flow of Conversation. The WoW (Dinan et al., 2019) deals with a chit-chat dialogue task
where two speakers discuss in depth about a given topic. One speaker (coined as Wizard) is to be
both engaging and knowledgeable on the topic with access to an information retrieval (IR) system
over Wikipedia to supplement its knowledge. The other speaker (Apprentice) is curious and eager
to learn about the topic. With an example in Figure 1, the conversation flow takes place as follows.
1.	One topic is chosen among 1,431 topics and shared between the two speakers.
2.	Given an apprentice’s utterance and a wizard’s previous utterance, the IR system retrieves
relevant knowledge, which includes the first paragraph of top 7 articles each for wizard
and apprentice and the first 10 sentences of the original Wikipedia page of the topic (e.g.
the lifeguard wikipage). The knowledge pool contains 67.57 sentences on average. Then.
2
Published as a conference paper at ICLR 2020
the wizard must choose a single relevant sentence from them (knowledge selection) and
construct an utterance (response generation).
3.	The conversation repeats until a minimum number of turns (5 each) reaches.
The Motivation of Sequential Latent Models. The goal of the task is to model the wizard that
solves the two subproblems of knowledge selection and response generation (Dinan et al., 2019). In
the knowledge selection step, a single relevant knowledge sentence is chosen from a pool of candi-
dates, and in the response generation step, a final utterance is generated with the chosen knowledge
and dialogue context. This pipeline is originally proposed to tackle open-domain TextQA (Chen
et al., 2017); for example, Min et al. (2018) show its effectiveness for single-document TextQA, to
which the key is to locate the sentences that contain the information about the answer to a question.
For knowledge-grounded dialogue, however, there can be one-to-many relations between the dia-
logue context and the knowledge to be selected unlike TextQA. Except a direct question about con-
text, one can choose any diverse knowledge to carry on the conversation. Therefore, the knowledge
selection in dialogue is diverse (i.e. multimodal) by nature, which should be correctly considered
in the model. It is our main motivation to propose a sequential latent variable model for knowledge
selection, which has not been studied yet. The latent variable not only models such diversity of
knowledge but also sequentially track the topic flow of knowledge in the multi-turn dialogue.
Another practical advantage of the sequential latent model lies in that it is easy to find which knowl-
edge is chosen once the response is known, since the response is written based on the selected
knowledge. Table 1 clearly validates this relation between knowledge and response. In the WoW
dataset, knowing a response boosts the accuracy of knowledge sentence selection for both human
and different models. These results hint that knowledge selection may need to be jointly modeled
with response generation in a sequence of multi-turn chit-chats, which can be done by the sequential
latent models.
3	Approach
We propose a novel model for knowledge-grounded conversation named sequential knowledge
transformer (SKT), whose graphical model is illustrated in Figure 2. It is a sequential latent model
that sequentially conditions on previously selected knowledge to generate a response.
We will use 1 ≤ t ≤ T to iterate over dialogue turns, 1 ≤ m ≤ M and 1 ≤ n ≤ N to respectively
iterate over words in the utterance of apprentice and wizard, and 1 ≤ l ≤ L to denote knowledge
sentences in the pool. Thus, T is the dialogue length, M and N are the length of each utterance of
apprentice and wizard, and L is the size of the knowledge pool.
The input to our model at turn t is previous turns of conversation, which consists of utterances
from apprentice x1, ..., xt, utterances from wizard y1, ..., yt-1 and the knowledge pool k1 , ..., kt,
where kt = {kt,l} = kt,1, ..., kt,L. The output of the model is selected knowledge kts and the
wizard’s response yt . Below, we discuss sentence embedding, knowledge selection and utterance
decoding in our approach. Note that our technical novelty lies in the knowledge selection model,
while exploiting existing techniques for text encoding and utterance decoding.
Sentence Encoding. We represent an apprentice utterance xt to an embedding htx using BERT
(Devlin et al., 2019) and average pooling over time steps (Cer et al., 2018):
Htx =BERTbase([xt1;...;xtM]) ∈RM×768,htx = avgpool(Htx) ∈ R768.	(1)
Likewise, the utterance of Wizard yt-1 is embedded as hty-1 and knowledge sentences are as
{htk,l} = htk,1 , ..., htk,L. Each apprentice-wizard utterance pair htxy = [htx; hty] at dialog turn t is
jointly represented through a GRU (Cho et al., 2014) layer: dtxy = GRUdialog(dtx-y1, htxy) ∈ R768.
Sequential Knowledge Selection. Compared to previous works, we make two significant mod-
ifications. First, we regard the knowledge selection as a sequential decision process instead of a
single-step decision process. Second, due to the diversity of knowledge selection in dialogue, we
model it as latent variables. As a result, we can carry out the joint inference of multi-turns of knowl-
edge selection and response generation rather than separate inference turn by turn.
3
Published as a conference paper at ICLR 2020
Tr Train/Test
♦ Test only
T Train only
Figure 2: A graphical representation of the proposed sequential knowledge transformer (SKT)
model. At the third turn, the goal is to generate wizard,s response (y3) given dialogue context
(x≤3, y<3). Our model sequentially infer which knowledge is likely to be used (k≤3), from which
the utterance y3 is generated.
There have been much research on sequential latent variable models (Chung et al., 2015; Fraccaro
etal., 2016; Goyal et al., 2017; Anejaetal., 2019; Shankar & Sarawagi, 2019). For example, Shankar
& Sarawagi (2019) propose a posterior attention model that represents the attention of seq2seq
models as sequential latent variables. Inspired by them, we factorize the response generation with
latent knowledge selection and derive the variational lower bound as follows:
log p(y∣x) = log YX pθ (yt ∣x≤t, y<t, k≤t)∏θ(kt∣x≤t, y<t, k<t)	⑵
t kt
≥ XEqφ(kt-1) ∣Eqφ(kt)[logPθ(yt∣x≤t,y<t,kt)] - Dkl®K) k ∏θ(kt))],	(3)
t
where qφ(kt) is shorthand for qφ(kt∣x≤t, y≤t, k<t) and ∏θ(kt) for ∏θ(kt∣x≤t, y<t, k<t) for
brevity. Note that pθ(yt∣∙) is a decoder network, ∏(kt) is a categorical conditional distribution
of knowledge given dialogue context and previously selected knowledge, and qφ(kt) is an inference
network to approximate posterior distributionpθ(kt∣x≤t, y≤t, k<t).
The conditional probability of generating wizard’s response yt given dialogue context x≤t and y<t,
can be re-written from Eq. (2) as follows:
t-1
P(yt∣x≤t,y<t) ≈ YXqφ(ki) (X Pθ (yt∣x≤t, y<t,kt)∏θ (kt)).	(4)
i=1 ki	kt
The detailed derivation can be found in Appendix. Eq.(4) means that we first infer from the knowl-
edge posterior which knowledge would be used up to previous turn t - 1, estimate the knowledge
for current turn t from prior knowledge distribution and generate an utterance from the inferred
knowledge. Figure 2 shows an example of this generation process at t = 3. We parameterize the
decoder network pθ, the prior distribution of knowledge πθ, and the approximate posterior qφ with
deep neural networks as will be discussed.
From the posterior distribution qφ(kt-1) we draw a sample kts-1, and then update πθ and qφ with
the sentence embedding of sampled knowledge (htk-1,s) and the embeddings of previous and current
utterances (dtx-y1, dtxy, htx). We use an attention mechanism over current knowledge pool {htk,l} to
compute knowledge distribution given the dialogue context. This process is modeled as
∏θ(kt∣x≤t, y<t,k≤t-1) = SOftmax(qprior[hk,1,…,hk,L]>) ∈ RL	(5)
qφ(kt∣x≤t, y≤t,k≤t-1) = SOftmax(qpost[hk,1,…,hk,L]>) ∈ RL,	(6)
where
qtprior =Wprior([dtx-y1;htx;GRUhist(dtk-2,htk-1,s)]),	(7)
qtpost=Wpost([dtxy;GRUhist(dtk-2,htk-1,s)]),	(8)
dtk is the hidden state of GRUhist and we initialize d0xy = d0k = 0 ∈ R768, and Wprior, Wpost ∈
R768×(768*2) are the parameters. We here use the GRU (Li et al., 2017; Aneja et al., 2019) to
sequentially condition previously selected knowledge to πθ and qφ .
4
Published as a conference paper at ICLR 2020
Finally, we sample knowledge kts over attention distribution in Eq. (6) and pass it to the decoder. At
test time, we select the knowledge with the highest probability over distribution in Eq. (5).
Decoding with Copy Mechanism. We generate the wizard’s response at turn t, given cur-
rent context xt and selected knowledge sentence kts . We feed their concatenated embedding
Htxk = [Htx ; Htk ] to the decoder pθ . To maximize the effect of selected knowledge for response
generation, we choose the Copy mechanism (Xia et al., 2017; Li et al., 2019b) with Transformer
decoder (Vaswani et al., 2017). We obtain the output word probability (Zhao et al., 2019a):
htn =Decoder(Htxks,yt<n), qtn, Kt, Vt =htnWq>,HtxksWk>,HtxksWv>,	(9)
ptg,enn (w) = softmax(Wouthtn), ptc,onpy(w) = softmax(qtnKt),	(10)
pt,n(w) = (1-atony) *Pgen(W) + αC,OPy *Pcopy(w),	(11)
where atopy = σ(W>jpy PPCny(w) ∙ Vt) and σ is a sigmoid. Finally, We select the word with the
highest probability ynt +1 = arg maxw∈V Pt,n (w) where V is the dictionary. Unless the word ynt +1
is an EOS token, we repeat generating the next word by feeding ynt +1 to the decoder.
3.1 Training
Obviously, there is a large gap in knowledge selection accuracy between training with or without
true labels (e.g. 23.2 of E2E Transformer MemNet with labels vs 4.8 of PostKS without labels in
Table 2). As one way to take advantage of true labels for training of latent models, prior research has
employed auxiliary losses over latent variables (Wen et al., 2017; Zhao et al., 2017). Similarly, we
use the knowledge loss from Dinan et al. (2019) (i.e. the cross-entropy loss between predicted and
true knowledge sentences) as an auxiliary loss for the latent variable. Thus, the training objective is
a combination of the variational lower-bound from Eq. (3) and the auxiliary knowledge loss as
1T
L = - T X Eqφ(kt-1) [Eqφ (kt) [log Pθ (ytlx≤t, y<t, kS)]
t=1
- DKLSφ(kt) k πθ (kt)) + λ log qφ(ka) i,
(12)
{^^^
Knowledge loss
where kS is a sampled knowledge from qφ(kt∣x≤t, y≤t, k<t), ka is a true knowledge, and λ is a
hyperparameter. Note that knowledge is sequentially sampled from attention distribution as in Eq.
(6). We train our model by mini-batch gradient descent. We approximate the expectation by drawing
one sample from the posterior with Gumbel-Softmax function (Jang et al., 2017; Maddison et al.,
2017b). Further details of optimization can be found in Appendix.
4	Experiments
We evaluate our model mainly on the Wizard of Wikipedia (Dinan et al., 2019) and additionally
Holl-E (Moghe et al., 2018) as another knowledge-grounded chit-chat dataset. We quantitatively
and qualitatively compare our approach with other state-of-the-art models.
4.1	Datasets
Wizard of Wikipedia. It contains 18,430 dialogues for training, 1,948 dialogues for validation and
1,933 dialogues for test. The test set is split into two subsets, Test Seen and Test Unseen. Test Seen
contains 965 dialogues on the topics overlapped with the training set, while Test Unseen contains
968 dialogues on the topics never seen before in training and validation set.
Holl-E. It contains 7,228 dialogues for training, 930 dialogues for validation and 913 dialogues for
test. A single document is given per dialogue; the documents include about 58 and 63 sentences on
average for training/validation and test set, respectively. The dataset provides spans in the document
as additional information to provide which parts of the document is used to generate a response.
However, the span labels are rather inconsistent; for example, they are often shorter than a single
sentence or contain multiple consecutive sentences. Thus, we collect a new set of ground-truth (GT)
5
Published as a conference paper at ICLR 2020
Table 2: Quantitative results on the Wizard of Wikipedia dataset (Dinan et al., 2019). The method
with [*] does not use the knowledge loss. The scores of E2E Transformer MemNett and Transformer
(no knowledge)t are from the original paper. The variant (BERT vocab∕ is re-runned using the
authors’ code, since the vocabulary is different from original paper due to the use of BERT.
Method	Test Seen				Test Unseen			
	PPL	R-1	R-2	ACC	PPL	R-1	R-2	ACC
Random knowledge selection	-	^"8.T^	1.4	~7Γ	-	-80-	1.2	^3-
Repeat last utterance	-	14.5	3.1	-	-	14.1	2.9	-
Transformer (no knowledge)* (Dinan et al., 2019)	41.8	17.8	-	-	87.0	14.0	-	-
E2E Transformer MemNett (Dinan et al., 2019)	63.5	16.9	-	22.5	97.3	14.4	-	12.2
E2E Transformer MemNet (BERT vocab)t	53.2	17.7	4.8	23.2	137.8	13.6	1.9	10.5
POStKS* (Lian et al., 2019)	79.1	13.0	1.0	4.8	193.8	13.1	1.0	4.2
E2E BERT	ɪr	^8^	4.5	^7^	105.7	T3Γ^	2.2	T36^
PostKS + Knowledge Loss	54.5	18.1	5.3	23.4	144.8	13.5	2.0	9.4
E2E BERT + PostKS	54.6	17.8	5.3	25.5	113.2	13.4	2.3	14.1
E2E BERT + PostKS + Copy	52.2	19.0	6.5	25.5	83.4	15.6	3.9	14.4
Ours	3Σ0~	393~	6.8	168~	81.4		4.2	183~
Table 3: Quantitative results on the Holl-E dataset (Moghe et al., 2018) with single reference and
multiple references test set.
Method	Single Reference				Multiple References			
	PPL	R-1	R-2	Acc	PPL	R-1	R-2	Acc
Random knowledge selection	-	~4A~	1.8	T.9-	-	T03^^	3.6	^.^
Repeat last utterance	-	11.4	1.5	-	-	13.6	2.0	-
E2E Transformer MemNet (Dinan et al., 2019)	140.6	20.1	10.3	22.7	83.6	24.3	12.8	32.3
POStKS* (Lian et al., 2019)	196.6	15.2	6.0	1.5	114.1	19.2	7.9	3.2
E2E BERT	112.6	-25.9-	18.3	ɪɪ	66.9	ɪf	22.7	ɪF
PostKS + Knowledge Loss	135.1	19.9	10.7	22.5	81.9	23.8	12.9	32.2
E2E BERT + PostKS	119.9	27.8	20.1	27.6	66.7	33.7	25.8	37.3
E2E BERT + POStKS + Copy		47.4	29.2	22.3	27.8	27.9	35.9	29.0	37.8
Ours	48.9	198~	23.1	^2	28.5	363~	29.7	392^
knowledge per document so that it is similar to that of WoW where all of the GT knowledge are in
the form of sentences. Basically, we select the sentence that includes the span as the GT knowledge
sentence. If the span is given over multiple sentences, we select the minimum number of consecutive
sentences containing the span as GT. If no span is given, we use the no passages used tag as GT,
which amounts to 5% of all GT labels. It indicates that the gold utterance is generated with no
knowledge grounding and the model should predict the label of no passages used for this sample to
be correct. We make our new set of GT annotations available in the project page.
4.2	Experimental Setting
Evaluation Metrics. We follow the evaluation protocol of WoW (Dinan et al., 2019). We measure
unigram F1 (R-1), bigram F1 (R-2) and perplexity (PPL) for response generation, and the accuracy
for knowledge selection. For n-gram metrics, we remove all the punctuations and (a, an, the) before
computing the score. We remind that lower perplexity and higher n-gram (R-1, R-2) scores indicate
better performance.
The test set for Holl-E is split into two subsets, single reference and multiple references. The dataset
basically provides a single response per context (denoted as single reference). However, for some
conversations, more responses (e.g. 2-13) are collected from multiple annotators per context (mul-
tiple references). For evaluation of multiple references, we take the best score over multiple GTs by
following Moghe et al. (2018). For knowledge accuracy, we regard the model’s prediction is correct
if it matches at least one of the correct answers.
Baselines. We closely compare with two state-of-the-art knowledge-grounded dialogue models.
The first one is E2E Transformer MemNet (Dinan et al., 2019), which uses a Transformer memory
network for knowledge selection and a Transformer decoder for utterance prediction. The second
6
Published as a conference paper at ICLR 2020
one is PostKS (Lian et al., 2019), which uses the posterior knowledge distribution as a pseudo-label
for knowledge selection. For fair comparison, we replace all GRU layers in PostKS with Transform-
ers. We also compare with four variants of these models as an ablation study: (i) E2E BERT, where
we replace the Transformer memory network with pre-trained BERT, (ii) PostKS + Knowledge loss,
where we additionally use the knowledge loss, (iii) E2E BERT + PostKS, which combines all the
components of baselines, and (iv) E2E BERT + PostKS + Copy, where we additionally use the copy
mechanism with the Transformer decoder.
We use official BERT tokenizer to tokenize the words and use pre-defined BERT vocabulary (V =
30522) to convert token to index1 . All the baselines use the exactly same inputs with our model
except PostKS, which does not make use of knowledge labels as proposed in the original paper.
4.3	Quantitative Results
Table 2 compares the performance of different methods on the Wizard of Wikipedia dataset. Our
model outperforms the state-of-the-art knowledge-grounded dialogue models in all metrics for
knowledge selection (accuracy) and utterance generation (unigram F1, bigram F1). The PostKS that
is trained with no knowledge label shows low accuracy on knowledge selection, which is slightly
better than random guess. However, it attains better performance than E2E Transformer MemNet
with the knowledge loss in the WoW Test Seen, which shows that leveraging prior and posterior
knowledge distribution is effective for knowledge-grounded dialogue, although using sequential la-
tent variable improves further. BERT improves knowledge selection accuracy, but not much as in
TextQA because of diversity in knowledge selection of conversation. The E2E BERT + PostKS +
Copy performs the best among baselines, but not as good as ours, which validates that sequential
latent modeling is critical for improving the accuracy of knowledge selection and subsequently ut-
terance generation. Additionally, the performance gaps between ours and baselines are larger in
Test Unseen. It can be understood that the sequential latent variable can generalize better. Adding
the copy mechanism to the baseline substantially improves the accuracy of utterance generation,
but barely improves the knowledge selection, which also justifies the effectiveness of the sequential
latent variable. Transformer (no knowledge) shows the lowest perplexity in the WoW Test Seen, and
it is mainly due to that it may generate only general and simple utterances since no knowledge is
grounded. This behavior can be advantageous for the perplexity, while the other knowledge-based
models take a risk of predicting wrong knowledge, which is unfavorable for perplexity.
Table 3 compares the performance of our model on Holl-E dataset. Similarly, our model outperforms
all the baselines in all metrics. One notable trend is that BERT considerably reduces the perplexity
in all models, which may be due to that the dataset size of Holl-E is much smaller than WoW and
BERT prevents overfitting (Hao et al., 2019).
4.4	Qualitative Results
Single-Turn Human Evaluation. We perform a user study to complement the limitation of auto-
matic language metrics. We evaluate several aspects of utterance generation using the similar setting
in Guu et al. (2018). We randomly sample 100 test examples, and each sample is evaluated by three
unique human annotators on Amazon Mechanical Turk (AMT). At test, we show dialogue context
and generated utterance by our method or baselines. We ask turkers to rate the quality of each ut-
terance in two aspects, which are referred to Li et al. (2019a): (i) Engagingness: how much do you
like the response? and (ii) Knowledgeability: how much is the response informative? Each item is
scored from 1 to 4 to avoid catch-all category in the answer (Dalal et al., 2014), where 1 means not
at all, 2 is a little, 3 is somewhat, and 4 is a lot. To mitigate annotator bias and inter-annotator vari-
ability, we adjust human scoring with Bayesian calibration (Kulikov et al., 2019). Note that human
evaluation on knowledge selection is not possible, since any knowledge could be fine for a given
context, which is key motivation for our sequential latent model - diversity of knowledge selection.
Table 4 summarizes the results of the single-turn human evaluation, which validates that annotators
prefer our results to those of baselines. Again, the performance gaps between ours and baselines are
larger in Test Unseen, thank to better generality of our sequential latent model.
1https://github.com/tensorflow/models/tree/master/official/nlp/bert.
7
Published as a conference paper at ICLR 2020
Table 4: Single-turn human evaluation results on the Wizard of Wikipedia. We report the mean
ratings and their standard errors of different methods for engagingness and knowledgeability scores.
TMN stands for E2E Transformer MemNet (Dinan et al., 2019).
Method	Test Seen				Test Unseen			
	Raw		Calibrated		Raw		Calibrated	
	Engage	Knowledge	Engage	Knowledge	Engage	Knowledge	Engage	Knowledge
PostKS	1.65 (0.05)	1.72 (0.06)	1.51(0.02)	1.72 (0.01)	1.66 (0.06)	1.74 (0.06)	1.38 (0.02)	1.60 (0.02)
TMN	2.57 (0.05)	2.47 (0.06)	2.41 (0.02)	2.49 (0.01)	2.39 (0.06)	2.21 (0.06)	2.12(0.02)	2.05 (0.02)
Ours	2.59 (0.05)	2.53 (0.06)	2.45 (0.02)	2.55 (0.01)	2.52 (0.06)	2.35 (0.06)	2.26 (0.02)	2.21 (0.02)
Human	3.14(0.05)	3.09 (0.05)	3.00 (0.02)	3.12(0.01)	3.11(0.05)	2.99 (0.05)	2.83 (0.01)	2.85 (0.02)
Table 5: Multi-turn human evaluation results on the Wizard of Wikipedia. We report the averages
and standard deviations (in parentheses).
Method	Test Seen	Test Unseen
E2E Transformer MemNet (Dinan et al., 2019)	2.36 (1.38)	2.10 (0.96)
Ours	2.39 (0.99)	2.38 (1.01)
Human (Dinan et al., 2019)	4.13 (1.08)	4.34 (0.98)
Multi-turn Human Evaluation. We add another human evaluation results in a multi-turn setting
using the evaluation toolkit from Wizard of Wikipedia (Dinan et al., 2019). Humans are paired with
one of the models and Chat about a specific topic (given a choice of 2-3 topics) for 3-5 dialogue
turns. After conversation, they score their dialogue partners on a scale of 1-5, with the rating
indicating how much they liked the conversation. We collect the votes for 110 randomly sampled
conversations from 11 different turkers.
Table 5 compares the results of different methods for the multi-turn evaluation. Human annotators
prefer our results to those of baselines with a larger gap in Test Unseen.
Dialogue Examples. Figure 3 shows selected examples of utterance prediction. In each set, we
show dialogue context, human response, and utterances generated by our method and baselines.
Thanks to the use of latent variables, our model can better capture the changes in dialogue topics
and thus generate more appropriate responses.
5	Related Work
Knowledge-based conversations have been studied much including collecting new datasets (Qin
et al., 2019; Zhang et al., 2018; Ghazvininejad et al., 2018; Zhou et al., 2018; Dinan et al., 2019;
Moghe et al., 2018) or developing new models (Lian et al., 2019; Li et al., 2019b; Yavuz et al.,
2019; Zhao et al., 2019b; Dinan et al., 2019; Liu et al., 2019). Most works on the models have less
investigated the knowledge selection issue but instead focused on how to effectively combine given
knowledge and dialogue context to improve response informativeness. For example, Ghazvininejad
et al. (2018) aid a Seq2Seq model with an external knowledge memory network, and Li et al. (2019b)
propose an Incremental Transformer to encode multi-turn utterances along with knowledge in related
documents. Recently, Dinan et al. (2019) propose both a dataset of Wizard of Wikipedia and a model
to leverage the two-step procedure of selecting knowledge from the pool and generating a response
based on chosen knowledge and given context.
One of the most related models to ours may be Lian et al. (2019), who also focus on the knowledge
selection issue in the two-stage knowledge-grounded dialogue. However, our work is novel in that
we model it as a sequential decision process with latent variables and introduce the knowledge
loss. Thanks to these updates, our model achieves significantly better performance as shown in the
experiments.
Sequential Latent Variable Models. There have been many studies about sequential latent vari-
able models. Chung et al. (2015) propose one of the earliest latent models for sequential data, named
VRNN. Later, this architecture is extended to SRNN (Fraccaro et al., 2016) and Z-Forcing (Goyal
et al., 2017). There have been some notable applications of sequential latent models, including doc-
8
Published as a conference paper at ICLR 2020
Seen Test (Topic: Italian Cuisine)	Unseen Test (Topic: Hunting)
A: I love chicken Parmigiana as well, but I think my ultimate favorite is beef IaSagna... Extra cheese please! W: Chicken with sauce and mozzarella….Be still my heart! A: Truthfully, anything with cheese is the best	W: That is true but We always have to watch out for excessive hunting. It has caused some species to be endangered. A: Yes I agree. I don,t believe in the useless hunting that poachers do. Its so cruel.
(OUrS) i love pizza too ! it ' S a traditional italian dish consisting of yeasted flatbread typically topped with tomato sauce and cheese (TMN) i love cheese ! (E2E BERT+KL) i like mine topped with vegetables , meats , and condiments . (Human) especially cheddar cheese ! it ' S the second most popular cheese in the use !	(Ours) i agree , poaching has been defined as the illegal hunting or capturing of wild animals . (TMN) i thinks so , i ' m not sure if you , re talking about poaching , but i know that poodles are the second most intelligent breed behind the poodle . (E2E BERT+KL) i agree . i think it ' s a great way to catch fish . (Human) agreed , i remember reading one time that unless you plan to kill the animals its not considered hunting .
Figure 3: Examples of generated responses by our model and baselines on Wizard of Wikipedia.
TMN stands for E2E Transformer MemNet, and A and W for apprentice and wizard. Examples
with selected knowledge sentences can be found at Appendix E.
ument summarization (Li et al., 2017), image captioning (Aneja et al., 2019) and text generation
(Shao et al., 2019). Another related class of sequential latent models may be latent attention mod-
els (Deng et al., 2018; Wang et al., 2018; Yang et al., 2017), which exploit the latent variables to
model the attention mapping between input and output sequences. Although our method is partly
influenced by such recent models, it is novel to propose a sequential latent model for the knowledge-
grounded chit-chat problem.
6 Conclusion
This work investigated the issue of knowledge selection in multi-turn knowledge-grounded dia-
logue, and proposed a sequential latent variable model, for the first time, named sequential knowl-
edge transformer (SKT). Our method achieved the new state-of-the-art performance on the Wiz-
ard of Wikipedia benchmark (Dinan et al., 2019) and a knowledge-annotated version of Holl-E
dataset (Moghe et al., 2018). There are several promising future directions beyond this work. First,
we can explore other inference models such as sequential Monte Carlo methods using filtering vari-
ational objectives (Maddison et al., 2017a). Second, we can study the interpretability of knowledge
selection such as measuring the uncertainty of attention (Heo et al., 2018).
Acknowledgments
We thank Hyunwoo Kim, Chris Dongjoo Kim, Soochan Lee, Junsoo Ha and the anonymous review-
ers for their helpful comments. This work was supported by SK T-Brain corporation and Institute
of Information & communications Technology Planning & Evaluation (IITP) grant funded by the
Korea government (MSIT) (No.2019-0-01082, SW StarLab). Gunhee Kim is the corresponding
author.
References
Jyoti Aneja, Harsh Agrawai, Dhruv Batra, and Alexander Schwing. Sequential Latent Spaces for
Modeling the Intention During Diverse Image Captioning. In ICCV, 2019.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching Word Vectors
with Subword Information. In TACL, 2016.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal Sentence Encoder.
arXiv:1803.11175, 2018.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-
Domain Questions. In ACL, 2017.
9
Published as a conference paper at ICLR 2020
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations Using RNN Encoder-
Decoder for Statistical Machine Translation. In EMNLP, 2014.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A Recurrent Latent Variable Model for Sequential Data. In NIPS, 2015.
Dev K Dalal, Nathan T Carter, and Christopher J Lake. Middle Response Scale Options are Inap-
propriate for Ideal Point Scales. J. Bus. Psychol., 29(3):463-478, 2014.
Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent Alignment and
Variational Attention. In NIPS, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-Training of Deep
Bidirectional Transformers for Language Understanding. In NAACL-HLT, 2019.
Emily Dinan and Jason Weston. Advances in Conversational AI. https://ai.facebook.
com/blog/advances-in-conversational-ai/, 2019.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard
of Wikipedia: Knowledge-Powered Conversational Agents. In ICLR, 2019.
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical
Structured Prediction Losses for Sequence to Sequence Learning. In NAACL-HLT, 2017.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential Neural Models
with Stochastic Layers. In NIPS, 2016.
Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih,
and Michel Galley. A Knowledge-Grounded Neural Conversation Model. In AAAI, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the Difficulty of Training Deep Feedforward
Neural Networks. In AISTATS, 2010.
Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu
Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur, and Amazon Alexa AL Topical-Chat: Towards
Knowledge-Grounded Open-Domain Conversations. In Interspeech, 2019.
Anirudh Goyal Alias Parth Goyal, Alessandro Sordoni, Marc-Alexandre Cote, Nan Rosemary Ke,
and Yoshua Bengio. Z-Forcing: Training Stochastic Recurrent Networks. In NIPS, 2017.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating Sentences by
Editing Prototypes. TACL, 6:437-450, 2018.
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and Understanding the Effectiveness of
BERT. In EMNLP, 2019.
Jay Heo, Hae Beom Lee, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, and Sung Ju
Hwang. Uncertainty-Aware Attention for Reliable Interpretation and Prediction. In NIPS, 2018.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with Gumbel-Softmax. In
ICLR, 2017.
Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.
Ilya Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. Importance of a Search
Strategy in Neural Dialogue Modelling. In INLG, 2019.
Margaret Li, Jason Weston, and Stephen Roller. ACUTE-EVAL: Improved Dialogue Evaluation
with Optimized Questions and Multi-turn Comparisons. arXiv:1909.03087, 2019a.
Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. Deep Recurrent Generative Decoder for Abstrac-
tive Text Summarization. In EMNLP, 2017.
10
Published as a conference paper at ICLR 2020
Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li, and Jie Zhou. Incremental Transformer
with Deliberation Decoder for Document Grounded Conversations. In ACL, 2019b.
Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, and Hua Wu. Learning to Select Knowledge
for Response Generation in Dialog Systems. In IJCAI, 2019.
Zhibin Liu, Zheng-Yu Niu, Hua Wu, and Haifeng Wang. Knowledge aware conversation generation
with reasoning on augmented graph. In EMNLP, 2019.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih,
Arnaud Doucet, and Yee Teh. Filtering Variational Objectives. In NIPS, 2017a.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous
Relaxation of Discrete Random Variables. 2017b.
Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2Seq: Effectively Incorporating Knowl-
edge Bases into End-to-End Task-Oriented Dialog Systems. In EMNLP, 2018.
Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. Efficient and Robust Question
Answering from Minimal Context over Documents. In ACL, 2018.
Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M Khapra. Towards Exploiting
Background Knowledge for Building Conversation Systems. In EMNLP, 2018.
Prasanna Parthasarathi and Joelle Pineau. Extending Neural Generative Conversational Model using
External Knowledge Sources. In EMNLP, 2018.
Gabriel Pereyra, George Tucker, Jan ChoroWski, Eukasz Kaiser, and Geoffrey Hinton. Regularizing
Neural Networks by Penalizing Confident Output Distributions. In ICLR, 2017.
Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin Choi,
and Jianfeng Gao. Conversing by Reading: Contentful Neural Conversation With On-Demand
Machine Reading. In ACL, 2019.
Shiv Shankar and Sunita SaraWagi. Posterior Attention Models for Sequence to Sequence Learning.
In ICLR, 2019.
Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and Diverse Text
Generation With Planning-based Hierarchical Variational Model. In EMNLP, 2019.
Ashish VasWani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In NIPS, 2017.
Weiyue Wang, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. Neural Hidden Markov
Model for Machine Translation. In ACL, 2018.
Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and Steve Young. Latent intention dialogue models.
In ICML, 2017.
Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Deliberation
NetWorks: Sequence Generation Beyond One-Pass Decoding. In NIPS, 2017.
Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, and Alex Smola. Neural Machine Translation
With Recurrent Attention Modeling. In EACL, 2017.
Semih Yavuz, Abhinav Rastogi, Guan-Lin Chao, and Hakkani-Tur Dilek. DeepCopy: Grounded
Response Generation With Hierarchical Pointer NetWorks. In SIGDIAL, 2019.
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, DouWe Kiela, and Jason Weston. Per-
sonalizing Dialogue Agents: I Have a Dog, Do You Have Pets Too? In ACL, 2018.
Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning Discourse-level Diversity for Neural
Dialog Models using Conditional Variational Autoencoders. In ACL, 2017.
11
Published as a conference paper at ICLR 2020
Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and Jingming Liu. Improving Grammatical Error
Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data. In NAACL-
HLT, 2019a.
Xueliang Zhao, Chongyang Tao, Wei Wu, Can Xu, Dongyan Zhao, and Rui Yan. A Document-
Grounded Matching Network for Response Selection in Retrieval-based Chatbots. In IJCAI,
2019b.
Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. A Dataset for Document Grounded Con-
versations. In EMNLP, 2018.
A Derivation of Conditional Probability
In Section 3, we re-write the conditional probability of wizard’s response yt given dialogue context
x≤t and y<t from Eq. (2) to Eq. (4). We can simply derive it as follows:
p(y|x)	(13)
=YXPθ(yt∣x≤t,y<t,k≤t)∏θ(kt) (byEq. (2))	(14)
t kt
t-1
=YXPθ(yi∣x≤i,y<i))pθ(ki)(XPΘ(yt∣x≤t,y<t,k≤t)∏θ(kt)) (by Bayes, rule) (15)
i=1 ki	kt
t-1
≈ YXPθ(yi∣χ≤i,y<i))qφ(ki)(XPθ(yt∣χ≤t,y<t,k≤t)∏θ(kt))	(16)
i=1 ki	kt
t-1
=YX qφ(ki)( X Pθ (yt∣χ≤t, y<t, kt)πθ(kt)	(x≤t and y<t are given)	(17)
i=1 ki	kt
≈ p(yt∣χ≤t, y<t),	(18)
where qφ(ki) is an approximated posterior distribution andpθ(ki) is a true posterior distribution.
B Training Details
All the parameters except pretrained parts are initialized with Xavier method (Glorot & Bengio,
2010). We use Adam optimizer (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.999, = 1e - 07.
For the models without BERT, we set the learning rate to 0.001 and initialize the embedding matrix
with fastText (Bojanowski et al., 2016) trained on the Common Crawl corpus. For the models
with BERT, we set the learning rate to 0.00002 and initialize encoder weights with BERT-Base,
Uncased pretrained weights. We apply label smoothing (Pereyra et al., 2017; Edunov et al., 2017;
Vaswani et al., 2017) for both knowledge selection and response generation, and set 0.1 and 0.05
for each. We set the temperature of Gumbel-Softmax to τ = 0.1 and the hyperparameter for the
knowledge loss to λ = 1.0. For efficiency, we batch the dialogues rather than individual turns. We
train our model up to 5 epochs on two NVIDIA TITAN Xp GPU.
C Knowledge Selection Accuracy over Turns
Table 6 compares the knowledge selection accuracy of different methods for each turn on the Wiz-
ard of Wikipedia. Thanks to the sequential latent variable, our model consistently outperforms other
methods for all turns in knowledge selection accuracy. Notably, in all models, the accuracy signif-
icantly drops after the first turn, which is often easily predictable as a topic definition sentence. It
shows the diversity nature in knowledge selection, as discussed in Section 2.
D Quantitative Results on Semi-Supervised Setting
Table 7 shows the results of our model with partial knowledge labels on the Wizard of Wikipedia. We
attain better performance with more labeled knowledge data for training as expected. Furthermore,
12
Published as a conference paper at ICLR 2020
Table 6: Knowledge selection accuracy for each turn on the Wizard of Wikipedia (Dinan et al.,
2019). The method with [*] uses no knowledge loss. TMN stands for E2E Transformer MemNet.
Method	Test Seen					Test Unseen				
	1st	2nd	3rd	4th	5th	1st	2nd	3rd	4th	5th
POStKS* (Lianetal.,2019)	3.6	3.6	^^41 ^^	7.0	9.5	3.4	3.0	4.7	4.1	9.9
PostKS + Knowledge Loss	55.4	19.3	10.7	8.7	7.0	26.0	3.8	4.0	3.9	3.8
TMN (Dinan et al., 2019)	55.8	19.5	10.4	7.6	6.2	25.9	7.0	4.1	4.2	6.1
E2E BERT + PostKS	56.5	20.6	13.7	10.4	9.2	36.0	8.1	6.1	6.8	5.7
Ours	59.1	20.6	15.8	12.8	9.1	52.9	8.8	8.4	6.4	10.7
Table 7: Performance of our model with partial knowledge labels on Wizard of Wikipedia (Dinan
et al., 2019).
Method	Test Seen				Test Unseen			
	PPL	R-1	R-2	ACC	PPL	R-1	R-2	ACC
E2E Transformer MemNett (Dinan et al., 2019)	63.5	16.9	-	22.5	97.3	14.4	-	12.2
E2E Transformer MemNet (BERT VoCaby	53.2	17.7	4.8	23.2	137.8	13.6	1.9	10.5
Ours	52.0	19.3	6.8	26.8	81.4	16.1	4.2	18.3
1/2 knowledge labeled	49.0	19.2	6.6	25.1	77.8	16.1	4.1	16.7
1/4 knowledge labeled	45.7	18.7	6.1	22.4	78.0	15.8	3.6	13.8
1/8 knowledge labeled	45.3	18.6	6.0	21.0	79.9	15.7	3.6	12.3
no knowledge loss	54.7	17.1	4.6	0.3	88.2	15.5	3.4	0.1
our model achieves competitive performance with less label. For instance, our model using only 1/4
labeled training data is comparable to E2E Transformer MemNet and even better in Test Unseen.
As a result, our sequential latent knowledge selection model can be utilized in a semi-supervised
method without severe drop in the performance.
E	Examples with S elected Knowledge
Figure 4 and 5 show selected examples of knowledge selection and response generation. In each set,
given dialogue context, we compare selected knowledge and generated utterances by our method
and baselines with human ground truths.
13
Published as a conference paper at ICLR 2020
Seen Test (Topic: Italian Cuisine)
A: I love chicken PaImigiana as well, but I think my ultimate favorite is beef lasagna…
Extra cheese please!
W: Chicken with sauce and mozzarella….Be still my heart!
A: Truthfully, anything with cheese is the best
	Task 1: Knowledge Selection	Task 2: Response Generation
Ours	Pizza is a traditional Italian dish consisting of a yeasted flatbread typically topped with tomato sauce and cheese and baked in an oven. (from “Pizza” Wikipedia page)	i love pizza too ! it ' S a traditional italian dish consisting of yeasted flatbread typically topped with tomato sauce and cheese
TMN	no_PaSSageS_USed	i love cheese !
E2E BERT +KL	It can also be topped with additional vegetables, meats, and condiments, and can be made without cheese. (from “Pizza” Wikipedia page)	i like mine topped with vegetables , meats , and condiments .
Human	It is the second-most popular cheese in the US (behind mozzarella), with an average annual consumption of 10 lb (4.5 kg) per capita. (from “Cheddar cheese” Wikipedia page)	especially cheddar cheese ! it ' S the second most popular cheese in the use !
Figure 4:	Examples of selected knowledge and generated responses by our model and baselines on
the WoW Seen Test set.
Unseen Test (Topic: Hunting)
A: Yes, that is the best way to do it. Apparently in some areas the government will actually pay
money to hunt for …
W: That is true but We always have to watch out for excessive hunting. It has cause some species to
be endangered.
A: Yes I agree. I don,t believe in the useless hunting that poachers do. It,s so cruel.
	Task 1: Knowledge Selection	Task 2: Response Generation
Ours	Poaching has traditionally been defined as the illegal hunting or capturing of wild animals, usually associated with land use rights. (from “Poaching” Wikipedia page)	i agree , poaching has been defined as the illegal hunting or capturing of wild animals .
TMN	no_PaSSageS_USed	i thinks so , i' m not sure if you ' re talking about poaching , but i know that poodles are the second most intelligent breed behind the poodle .
E2E BERT +KL	Hunting can also be a means of pest control. (from “Poaching” Wikipedia page)	i agree . i think it ' S a great way to catch fish .
Human	It is also not considered hunting to pursue animals without intent to kill them, as in wildlife photography, birdwatching, or scientific research (from “Hunting” Wikipedia page)	agreed , i remember reading one time that unless you plan to kill the animals its not considered hunting .
Figure 5:	Examples of selected knowledge and generated responses by our model and baselines on
the WoW Unseen Test set.
14