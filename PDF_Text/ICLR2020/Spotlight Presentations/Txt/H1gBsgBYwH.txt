Published as a conference paper at ICLR 2020
Generalization of Two-layer Neural Net-
works: An Asymptotic Viewpoint
Jimmy Ba1,2, Murat A. Erdogdu1,2, Taiji Suzuki3,4, Denny Wu1,2,4, Tianzong Zhang2,5
University of Toronto1, Vector Institute2, University of Tokyo3, RIKEN AIP4, Tsinghua University5
{jba,erdogdu,dennywu}@cs.toronto.edu,
taiji@mist.i.u-tokyo.ac.jp, ztz16@mails.tsinghua.edu.cn
Ab stract
This paper investigates the generalization properties of two-layer neural networks
in high-dimensions, i.e. when the number of samples n, features d, and neurons
h tend to infinity at the same rate. Specifically, we derive the exact population
risk of the unregularized least squares regression problem with two-layer neural
networks when either the first or the second layer is trained using a gradient flow
under different initialization setups. When only the second layer coefficients are
optimized, we recover the double descent phenomenon: a cusp in the population
risk appears at h ≈ n and further overparameterization decreases the risk. In
contrast, when the first layer weights are optimized, we highlight how different
scales of initialization lead to different inductive bias, and show that the resulting
risk is independent of overparameterization. Our theoretical and experimental
results suggest that previously studied model setups that provably give rise to
double descent might not translate to optimizing two-layer neural networks.
1	Introduction
In modern neural networks, the number of parameters can easily exceed the number of training
samples, yet in many circumstances, there is little sign of overfitting even in the absence of explicit
regularization (Zhang et al., 2016). This phenomenon is usually explained by the interplay between
the model architecture and the optimization method. Existing works have analyzed the implicit
regularization of gradient descent on simple models (Gunasekar et al., 2018; Ji and Telgarsky, 2018),
and provided generalization guarantees (Arora et al., 2018; Bartlett et al., 2017; Dziugaite and Roy,
2017) that align with the empirical observations.
Recently, a series of works highlighted the implicit regularization of interpolators in the overpa-
rameterized regime (Belkin et al., 2018; Spigler et al., 2018; Geiger et al., 2018; Advani and Saxe,
2017). Specifically, a second decrease in the population risk is observed when the model is further
overparameterized beyond the interpolation limit, i.e. when the model achieves zero training error.
This phenomenon is known as double descent, and can be precisely quantified for certain linear
models (Hastie et al., 2019; Mei and Montanari, 2019; Belkin et al., 2019; Bartlett et al., 2019;
Xu and Hsu, 2019). Among the recent works, Hastie et al. (2019) and Mei and Montanari (2019)
explicitly derived the population risk of linear regression and random features regression models in
high dimensions using tools from random matrix theory.
However, there is still a gap between the practical benefit of overparameterization and the recently
proved double descent phenomenon, which is typically established under models that exhibits the
following structure: the trained model solves a linear inverse problem, and the “cusp” in the risk
arises from the instability of the inverse at the interpolation threshold. Moreover, given a dataset
or fixed n, d, the number of parameters in the linear regression model is also fixed, i.e. the level of
overparameterization cannot be altered. It is therefore unclear if the trend persists in the optimization
of more complex models, for instance in two-layer neural networks where overparameterization can
be controlled simply by adding more neurons.
In this work, we analyze the generalization properties of two-layer neural networks in the unreg-
ularized least squares regression setting and examine the presence/absence of the double descent
phenomenon. We consider the proportional asymptotic limit where the number of samples n, input
1
Published as a conference paper at ICLR 2020
features d, and neurons h tend to infinity at the same rate, under which overparameterization cor-
responds to increasing the limit of h/n (network “width”). This regime is particularly interesting
because even though n → ∞, the empirical risk is not equivalent to the population risk. In addition,
the joint scaling of n, d, h is parallel to the practical choice of model architectures, where it is common
to train a larger network when the number of samples and input features are larger. Following Hastie
et al. (2019), we assume unit Gaussian input and noisy linear observations, and analytically derive
the population risk of the solution of gradient flow on either the first or the second layer parameters
when the flow is initialized close to zero.
Our findings can be summarized as follows (see Figure 1):
•	When only the second layer is optimized, we derive the
risk in its bias-variance decomposition and demonstrate
the presence of the double descent phenomenon.
•	When the first layer is optimized, we compare two so-
lutions of gradient flow from different scales of initial-
ization, which we term as vanishing and non-vanishing
initialization, and show in both cases the population
risk is independent to overparameterization.
•	For the vanishing initialization, we show that the risk
of the gradient flow solution is asymptotically close to
that of a rank-1 model. For non-vanishing initializa-
tion, we show that the gradient flow solution is well-
approximated by a kernel model and derive the risk.
Figure 1: Illustration of the double
descent risk curve in two-layer linear
networks (SNR= 16). Brighter color
indicates larger γ1 = d/n. Double
descent is observed when the second
layer coefficients are optimized (main
figure), but not when the first layer
weights are optimized (subfigure).
1.1 Related Works
Global Convergence of Two-layer Networks. A plethora of recent works have explored the global
convergence of shallow neural networks. Mei et al. (2018; 2019); Chizat and Bach (2018a); Rotskoff
and Vanden-Eijnden (2018); Sirignano and Spiliopoulos (2018); Nitanda and Suzuki (2017) studied
the mean-field limit where the number of neurons h → ∞ and the second layer scaled by 1/h, and
established correspondence between the main-particle limit of gradient descent and Wasserstein
gradient flow to demonsrate global convergence. On the other hand, Jacot et al. (2018); Du et al.
(2018); Oymak and Soltanolkotabi (2019); Allen-Zhu et al. (2018b); Song and Yang (2019) considered
a different scaling and showed that gradient descent on overparameterized models converges to global
minimizer at a linear rate; key to these results is an observation that optimization via gradient descent
is asymptotically equivalent to kernel regression with respect to the neural tangent kernel.
Active vs. Lazy Training. Following Chizat and Bach (2018b), we refer to the two aforementioned
scalings as the active and lazy (kernel) regime. It has been observed that different regimes lead to
contrasting inductive biases. Williams et al. (2019); Woodworth et al. (2019); Li et al. (2017) showed
that for certain two-layer network or overparameterized linear model, the scale of initialization
controls the implicit regularization of gradient descent (from sparse to smooth solution). In the
student-teacher setup (Tian, 2017; Zhong et al., 2017), Ghorbani et al. (2019b;a) showed that kernel
models in high dimensions perform no better than low-degree polynomials on the input or fully-
trained two-layer network. Additionally, Suzuki (2018); Allen-Zhu and Li (2019); Yehudai and
Shamir (2019); Wei et al. (2018) demonstrated that neural network outperforms linear estimators
(including kernel method) in learning various target functions. The difference between fixed bases
and adaptive bases mirrors the difference in optimizing the first or second layer in our setup.
Generalization of Overparameterized Models. It is often observed that overparameterization
does not result in overfitting (Neyshabur et al., 2014). In the lazy regime, generalization guarantees
can be derived from the distance traveled by the parameters (Neyshabur et al., 2018; Nagarajan and
Kolter, 2019), which becomes small if the model is sufficiently overparameterized (Arora et al., 2019b;
Li and Liang, 2018; Allen-Zhu et al., 2018a; Cao and Gu, 2019). Compared to these guarantees that
usually require significant overparameterization, our result relies on stronger data assumptions, but
consequently we obtain the exact population risk instead of a vacuous upper-bound. Beyond the
kernel regime, Advani and Saxe (2017); Goldt et al. (2019) analyzed the generalization dynamics of
overparameterized models in the student-teacher setup.
2
Published as a conference paper at ICLR 2020
Double Descent. The term double descent refers to the phenomenon that the population risk of an
empirical risk minimizer manifests a "cusp" at the interpolation threshold, and further overparame-
terization decreases the risk. First observed in Krogh and Hertz (1992), the phenomenon has been
recently connected to the benefit of overparameterization (Belkin et al., 2018; Geiger et al., 2018;
Spigler et al., 2018; Advani and Saxe, 2017), and can be precisely characterized for certain simple
models (Hastie et al., 2019; Belkin et al., 2019; Bartlett et al., 2019; Xu and Hsu, 2019). Our work is
inspired by Hastie et al. (2019) which uses random matrix theory to derive the asymptotic risk for
linear and random feature models. Concurrent to our work, Mei and Montanari (2019) analyzed the
random features model and derived its population risk for which double descent occurs both in bias
and variance. This aligns with our results on optimizing the second layer in Section 4 although we
do not derive the bias component explicitly. Compared to Hastie et al. (2019); Mei and Montanari
(2019), the focus of this work is to highlight the different generalization property of models obtained
from optimizing different layers of the network and from different initialization.
Random Matrix Theory. High-dimensional models, including kernel models and neural networks,
can be analyzed by studying the properties of random matrices. El Karoui et al. (2010); Cheng
and Singer (2013); Fan and Montanari (2019) studied the spectral properties of kernel matrix via
decomposing the nonlinearity with Taylor series or Hermite polynomials, which in turn explains
the generalization of high-dimensional kernel ridgeless interpolators (Liang and Rakhlin, 2018).
In addition, similar tools have been used to study two-layer neural networks (Louart et al., 2018;
Pennington and Worah, 2017) and related quantities such as the Fisher information matrix (Karakida
et al., 2018; Pennington and Worah, 2018).
2	Preliminaries: Two-layer Neural Network
Consider the following bias-free two-layer neural network f : Rd → R with h hidden units
h
f(x) = X aiφ(hx, wii),	(1)
i=1
where x ∈ Rd is the input, wi ∈ Rd is the weights corresponding to neuron i, ai ∈ R is the i-th
coefficient of the second layer, and φ : R → R is a Lipschitz continuous activation function with
bounded Gaussian moments, i.e. E[φ(G)k] < ∞, ∀k ∈ Z+ for G 〜N(0,1). For concise notation,
we write W = [w1, ...wh] ∈ Rd×h for the weight matrix, a = [a1, ...ah] ∈ Rh for the coefficient
vector, X = [x1, ...xn] ∈ Rd×n for the data matrix, y ∈ Rn for the corresponding vector of labels,
and Φ = φ(W>X) ∈ Rh×n for the feature matrix at the first layer. We omit arguments of f when
they are clear from the context.
We consider a student-teacher setup, in which data is generated by a teacher model F : Rd → R with
additive noise, and the student model aims to minimize the squared loss:
iid	1 n
(xi,εi)〜Px	X Pε,	yi	= F(xi)	+ εi,	L(X; f )	= ʒ-£ (yi -	f(xi))	,	(2)
2n
i=1
where E[xi] = 0, Cov(xi) = Σ, E[εi] = 0, Var(εi) = σ2. We are interested in the population risk
R(f) = EPx [(F (x) - f (x))2]. Our analysis will be made under the proportional asymptotics:
n, d, h → ∞; d/n → γ1, h/n → γ2;	γ1, γ2 ∈ (0, ∞),
in which overparameterization corresponds to increasing γ2 . Thus the characteristics of double
descent considered in this work are: 1) large population risk as γ2 → 1; 2) decrease in the risk for
γ2 > 1. While the empirical risk can be minimized in various ways, we analyze the solution of
gradient flow, in which we update either the first layer W or the second layer a:
dW(t) = -VwL(X; f) dt or	da(t) = S(X; f) dt,	(3)
from small initialization. The rest of the paper is organized as follows. In Section 3, we start with a
simple example of two-layer linear network as warm-up. In Section 4, we consider optimizing the
second layer coefficients (flow over a) of a non-linear two-layer neural network under fixed Gaussian
first layer, which is a random feature model. Section 5 considers optimizing the first layer weights
(flow over W) of such network under fixed Rademacher second layer. We defer all proofs and details
on experiments to appendix.
3
Published as a conference paper at ICLR 2020
3 Warm-up: Linear Network
We begin with a simple linear model with φ(x) = x, i.e. Φ = W>X. We remark that although the
model is linear, the solution obtained by gradient flow on the two-layer model can be different than
that from directly solving the linear regression problem on input features.
Training the Second Layer. Following Hastie et al. (2019), we fix the first layer parameters to be
randomly drawn from a unit Gaussian and optimize the coefficients a by minimizing a>Φ - y22.
The following lemma characterizes the solution of the gradient flow.
Lemma 1 (Least squares solution). Given data matrix X, response vector y and model f (x) =
hφ(x>W), ai with fixed first layer coefficients W, gradient flow on the coefficients a starting from
zero initialization converges to a = Φ*y, where f stands for the Moore-Penrose inverse.
We make two assumptions on the data and the teacher model to simplify the computation.
(A1) Gaussian Features: Xi 〜N(0,Id); (A2) Linear Teacher: F(X) = hx, βi, kβk = r.
Denote the linear student network as f (x) =(x, βi, where β = W a and a is the least-square
solution defined by Lemma 1. We write the population risk in its bias-variance decomposition.
R = Ex 〜Pχ[kβ - βk∑∣x,w ]
kE[β∣X,W] - βk2 +tr (Cov(β∣X,
S--------{z--------}、' 一一
B =bias	{ {z.
V =variance
(4)
where kXk2Σ = X>ΣX. We compute the bias and the variance separately to obtain the risk.
Theorem 2. Given (A1)(A2) and let Wi i'i.d' N(0, d-1Id), at n, d, h → ∞ we have
f =2Tr + γ2σ2,	Y2 <Y1,
I	Y1g2	g2
R(γ1<1) →
I γi σ2,	Y2 > Y1,
g1
f Ym r + γr σ2,	γ2 < ι,
I γ1 g2	g2
R(γ1>1) →
I Y^Tr + g1±%σ, Y > 1.
γ1 g2	g1 g2
(5)
where d/n → γι, h/n → γr, gι = ∣γι — 1|, and gr = ∣γr — 1|.
We observe that when d > n (i.e. γ1 > 1), we obtain the double descent risk curve, i.e., the population
risk achieves its maximum at γr → 1 and further overparameterization (γr > 1) reduces both the
bias and the variance. Conversely when n > d and h > d (i.e. γ1 < min(1, γr)), the population
risk becomes constant and equals to that of the minimum-norm solution βm® = X % on the input
features.
Training the First Layer. When the first layer of a linear network is optimized via gradient flow
and the second layer is fixed, the following holds for zero-initialization of W.
Proposition 3. Given W(0) = 0 and fixed ainit 6= 0, at any time t > 0 of the gradient flow on
>
W, W(t) is rank-1. Further, β = Wa converges to the least squares solution of y = X>β, the
population risk of which is given in (Hastie et al., 2019, Thm. 1 & 3)) as
R(Y1<1) → 1—1γiσ2; R(γι>1) → Y1Y-1 r2 + Y1⅛ɪσ2.	⑹
In this case, overparameterization by increasing γr does not influence the population risk. In addition,
since the obtained two-layer linear model is equivalent to the minimum-norm solution on the input
features βmin, optimizing the first layer always results in smaller or equal population risk compared
to optimizing the second layer.
In this simple scenario for two-layer linear networks, double descent is observed only when the second
layer is optimized, which reduces the objective to least squares regression on the intermediate features.
On the other hand, training the first layer from zero-initialization always yields the same solution
that is independent to overparameterization. One natural question to ask is: does this phenomenon
generalize to nonlinear two-layer neural networks? The following sections answer this question in
the affirmative under certain conditions.
4
Published as a conference paper at ICLR 2020
(a)risk(γι < 1).
(b) variance (ReLU).
(c) bias (ReLU).
Figure 2: Population risk of two-layer neural networks with optimized second layer under (A1)(A2). Brighter
color indicates larger γ1. (a) risk of linear network with r2 /σ2 = 16 and γ1 < 1. (γ1 > 1 is shown in Figure 1)
(b) variance of network with ReLU activation. Black line corresponds to γ1 → ∞ predicted by Corollary 5.
(c) bias of network with ReLU activation. Black line corresponds to γ1 → ∞ for linear network, which is
empirically observed as an upper-bound. Note that as γ2 → 1 both bias and variance becomes unbounded.
4 Nonlinear Model: Optimizing the Second Layer
In this section, we analyze the case when the second layer a is learned under fixed W and a nonlinear
activation function φ (a random feature model). We first observe that by Lemma 1, the gradient flow
finds the solution a = Φ%. We again consider the following bias-variance decomposition.
R = Eχ~Pχ [∣∣Φ(χ>w)a - F(χ)∣∣2∣x,w]
(7)
Ex [∣∣E[φ(x>W)a∣X,W] - F(x)∣∣2]
X-----------------------------
^™^{^^^™
B=bias
+ Ex
/
[∣∣φ(χ>w)a - E[φ(χ>w)a]∣∣2∣X, w].
_____________________ - /
^^^≡^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^r
V =variance
We highlight that the variance term does not depend on the target function. The following result
characterizes the variance of the random feature model.
Theorem 4. Given (A1) and WiL吧.N(0, d-1Id), when n, d, h → ∞, we have
(σ21⅛，
V=
I σ2 lim - Y2丁m1(ξ,c1x,c2x)∣	+ 丁m2(ξ,c1x,c2x)∣	+ γ2-——
ξ→0	∂x	x=0 ∂x	x=0	ξ2
γ2 < 1,
γ2 > 1.
in which mι(ξ, ρ, T), m2(ξ, ρ, T) is the unique solution in {∣mι∣, ∣m2∣ < 1∕=ξ} of
-1	-1	2	τ2γ1-1γ2m21(c2m2 -τ)-2τc2m1m2+c22m1m22
m- = -ξ-ρ-Yi Y2τ mi-c1m2 +----------------7------------------------,	(8)
m1(c2m2 -T) -γ1γ2-1
m-i = -ξ-rγ2m1 + SmCc m2-T-1,	(9)
mi(c2m2 -T)-γiγ2-i
where variables ξ, ρ, T satisfies =ξ > 0 or ξ < 0, ρ > T > 0, and constants ci, c2 defined as,
ci = E[φ(G)2] - E[φ(G)]2,	c2 = E[Gφ(G)]2,	(10)
for G 〜N(0,1) and =ξ denoting the imaginary part of ξ.
Remark. ci ≥ c2 and the equality holds iff φ is linear.
Corollary 5. If we let γi → ∞, the variance is equal to the lowest value of the variance of the linear
model V(γι→∞) = σ2min{γ2,1}∕∣1 - γ2∣.
The proof of Theorem 4 largely follows from Hastie et al. (2019) with techniques similar to Cheng
and Singer (2013), but with modifications in otder to handle unnormalized and uncentered activation
functions. The above theorem holds irrespective of the underlying teacher model, and is consistent
with the double descent risk curve as it suggests that for all γi , variance of the random feature model
5
Published as a conference paper at ICLR 2020
peaks at h = n then drops as γ2 further increases. Note that as γ1 → ∞, a linear and nonlinear
network would have the same asymptotic variance.
Since double descent is observed in the variance term, we do not derive the bias for all γ1 , γ2. Instead,
we show that for linear teacher, the bias also becomes unbounded as γ2 → 1.
Proposition 6. Given (A1)(A2) and Wi i'i.d' N (0, Id), then B → ∞ as γ2 → 1. Furthermore, B is
finite when γ2 > 1.
Thus we have shown that a “cusp” in the population risk appears at h = n, which aligns with the
double descent phenomenon. Empirically as γ1 → ∞ the nonlinear model also shares the same
asymptotic bias with the linear model, as shown in Figure 2. We note that (Mei and Montanari, 2019,
Thm. 1 & 3) analytically solved the risk of random feature model for a larger class of target functions
than ours and confirmed that double descent appears in both the bias and the variance.
5 Nonlinear Model: Optimizing the First Layer
Having observed the double descent phenomenon in optimizing the second layer, in the sequel we
consider a two-layer neural network with fixed second layer coefficients initialized from a Rademacher
distribution ai 〜Unif{-1∕√h, 1 √h}, and the first layer W is optimized with the following update
哈=-dLdWW) = n X [yi - a>Φ(W>Xi)] Xi[φ0(x>W) ◦ a],	(11)
n i=1
which potentially has different stationary solutions with no explicit form, depending on the initializa-
tion. We denote the solution of this flow at time t started from designated initialization by Winit (t),
its stationary solution by W, and the corresponding network by f .
Remark. Although we letn → ∞, this dynamics does not corresponds to the population gradient flow
considered in Tian (2017). For instance when X 〜N(0, I/d), the spectrum of the data covariance is
Marcenko-Pastur, whereas the population covariance is identity.
As we cannot characterize the gradient flow solution from all possible initializations, we consider
two specific scales of initialization:
Vanishing: WVan(0)〜N(0,Id∕dh1+e);	Non-vanishing: WNV(0)〜N(0,Id∕d-e).
Note that neither of the two initializations correspond to the “mean-field” regime (e.g. analyzed in
Mei et al. (2018)) due to the 1∕√h scaling of the second layer. In other words, as h increases, we
expect the distance traveled by each parameter to decrease under both initializations. The difference,
however, is the “relative” amount the parameters traveled compared to their initialized magnitude,
which leads to solutions with contrasting properties. As we will see, under (A1)(A2) and vanishing
initialization we
havekW(0)-WckF/kW(0)kF	1, i.e
. the contribution of initialization vanishes
at the end of training, whereas for non-vanishing initialization the inequality is in the opposite
direction, i.e. Wc “barely moves” and resembles the initialization W (0).
5.1	Vanishing Initialization
As d, h → ∞, the vanishing initialization becomes arbitrarily close to zero-initialization. We thus
expect the gradient flow under vanishing initialization to "resemble" that of starting from exactly
zero if the flow converges sufficiently fast and the gradient being Lipschitz. The Lipschitz condition
(Lemma 18) can be established under the following assumption on the activation.
(A3): φ is smooth, Lipschitz and monotone with φ0(0) = 0; ∣φ0(±x) - φ0(±∞)∣ = O(e-x).
The above assumption requires that the derivative of the nonlinearity φ saturates beyond a O(1)
region, which holds true for the commonly-used smooth activations such as sigmoid and SoftPlus.
In addition, the choice of scaling ensures that the gradient flow converges sufficiently fast. We thus
have the following characterization of the population risk:
6
Published as a conference paper at ICLR 2020
一	._ .	. O , .	.	,	_______ _ _
Theorem 7. Given (A1-3). Let T = O (log log h) and f(∙) = f (∙, W (T)) ,then as n,d,h → ∞,
the gradient flow reaches a o(1) first-order stationary point at time Tie ∣∣∂W (T )∕∂t∣∣ F ∈ o(1), at
which point the population risk is given as
R(f → max (0, γ1一1 ∖ r2 + min—1, 1} σ2.	(12)
γ1	|1 -γ1 |
The expression above is the same as the risk of the least squares solution on input β = X%; therefore
the risk is independent to overparameterization (increasing γ2). The intuition is that when the weights
are initialized sufficiently small and travel infinitesimally, then the activation can be linearized around
0 and thus the model is equivalent to a two-layer linear network. Note that this result does not apply
to the non-smooth ReLU activation. Instead, in Appendix E we heuristically show that under the
additional assumption that the data is symmetric, the risk of ReLU network is also independent to γ2 .
5.2	Non-vanishing Initialization
When initialization is sufficiently large, the amount each parameter travels to minimize the empirical
risk becomes asymptotically negligible compared to the magnitude of initialization. In this case
we establish under (A1-3) that (11) is asymptotically equivalent to the kernel gradient flow on the
tangent kernel: k(x, y) = hVw讪f (x), Vw讪f (y)). Theconvergedparameters under this linearized
dynamics has the following closed-form:
Vec(W*) ≈ Vec(Winit) + △; ∆ = Jt(y - finit(X)); J^ = Vvec(W叫,finit(xi), (13)
where J ∈ Rn×(d×h) is the Jacobian matrix w.r.t. to the model parameters. We remark that in contrast
to most NTK-type global convergence results that require the width of the model to grow faster than
the number of data points (e.g. Du et al. (2018)), our result is not built upon the overparameterization
on width, but instead an anti-concentration that relies on the scale of initialization. Consequently the
above initialization is larger than the scale that is commonly used in practice.
One may naturally expect the double descent phenomenon to appear in this kernel solution, as △
exhibits the form of a least squares solution which contains a pseudo-inverse. However, we show that
this is not the case under the same assumptions in Section 4; in fact, the risk is also independent to γ2 .
An obstacle in computing the risk of the kernel model is the potentially non-zero finit(X). We thus
adopt the "doubling-trick" from Chizat and Bach (2018b) to ensure f init(∙) = 0, i.e. We assume the
following symmetric property on the initialized weights:
(A4) Symmetric Initialization: ∀i ∈ [1, h], ∃!j ∈ [1, h] s.t. ai wiinit = —aj wijnit.
Theorem 8. Given (A1-4) and let n, d, h → ∞, the stationary solution f has the following risk
WfT γ2-1+
γ1 (γ1 + γ1 m + m — 2) + 1
2γι ,γι(γι + m(γι(m + 2) + 2) — 2) + 1
2
+ /	γ1 + γ1m + 1	_____ - 1 ʌ σ2
4 ,γι(γι + m(γι(m + 2) + 2) — 2) + 1	4y
where m = b2∕b0, b2 = E[φ0(G)]2, and b2 = E[φ0(G)2] — b2, G 〜N(0,1).
Note that the population risk is again independent to γ2, and thus double descent does not appear for
this initialization. Roughly speaking, the reason that the risk does not become unbounded at some
point is that in the asymptotic limit the pseudo-inverse (JJ＞户 is stable due to the nonlinearity and
dh n. We make tWo additional observations:
• the stability of the inverse at γ1 → 1 depends on the loWest eigenvalue of the tangent kernel
matrix (smaller λmin(K) entails larger variance), Which is determined by the nonlinearity;
• While our result only holds for netWork With zero initial output, for non-symmetric (i.i.d.)
initialization We also observe that the risk is independent to γ2, but the bias is higher than
that in symmetric initialization as shoWn in Figure 8. We comment that the non-zero finit(X)
in (13) behaves as zero-mean Gaussian due to central limit theorem; therefore, in the kernel
regime the function output at initialization is equivalent to additive noise to the labels y, and
We thus expect the magnitude of finit (X) to negatively influence the model generalization.
7
Published as a conference paper at ICLR 2020
Figure 3: Bias and variance of two-layer sigmoid network with optimized first layer under (A1)(A2). Individual
dotted lines correspond to different γ2 (from 0.2 to 2) which is independent to the risk. The bias and variance for
both initializations is well-aligned with Theorem 7 and Theorem 8.
5.3 Comparing the Initializations
Figure 3 shows the agreement between theoretical prediction and experimental results. Although in
both cases the risk is independent to overparameterization (γ2), the two initializations lead to models
with contrasting properties, as demonstrated by the following comparison on the risk.
Corollary 9. For any γι ∈ (0, ∞) and nonlinearity φ, B(fVan) ≤ B(fNV) ≤ 1. On the other hand,
for all m > 0, V(fNV) = O⑴,whereas V(fVan) can be arbitrarily large as γι → 1.
Remark. m ≥ 0 for all smooth activations φ, and the equality holds if φ is linear.
Intuitively, small initialization enables the model "evolve" more during optimization and better align
with the data and target function. This potentially results in a lower bias, at the expense of overfitting
more to the noise (high variance). In contrast, with sufficiently large initialization the final model
becomes close to the initialized model, and thus we may expect it to be less “aligned” to the target
(high bias) but is more stable (lower variance).
In illustrate the different inductive bias of the two initializations, we plot the trajectory of neurons in
Appendix A Figure 4. Observe that for vanishing initialization the neurons stay close to one another
throughout the trajectory, which results in a low-rank weight matrix, as predicted by Theorem 7. In
contrast, for non-vanishing initialization the neurons stay close to initialization (therefore full-rank),
which validates the kernel approximation. Last but not least, although the derived risk is only for
learning a linear target function, we empirically observe that when the teacher is also a two-layer
network, the population risk follows the same trend, i.e. double descent occurs when only the second
layer is optimized, as shown in Figure 7.
6 Discussion and Future Works
We derived the exact population risk of high-dimensional two-layer neural networks in learning a
linear target function over Gaussian data with additive label noise, and showed that optimizing the
first or the second layer via gradient flow results in solutions with contrasting properties. Specifically,
double descent is present when the second layer coefficients are optimized, but not when the first
layer weights are optimized under certain initializations. Moreover, we highlight that the scale of
initialization leads to different inductive bias in optimizing the first layer.
It should be noted that our analysis only applies to the unregularized objective: it has been shown
that explicit regularization (such as `2 penalty) stabilizes the singularity at γ2 → 1 (Mei and
Montanari, 2019), and algorithmic regularization (Li et al., 2019a; Dong et al., 2019) also provides
robustness against noisy observations. We further remark that our findings do not directly contradict
the experimental double descent phenomenon, nor the practical benefit of overparameterization. In
particular, the interpolation limit could occur at γ2 → 0 which is beyond the regime we consider (such
as Figure 4 in (Belkin et al., 2018)). Thus what we conclude is that under the studied proportional
8
Published as a conference paper at ICLR 2020
asymptotics, the mechanism that provably gives rise to double descent from previous works on least
squares regression might not translate to neural networks trained with gradient descent.
To simplify the computation, we rely on a set of strong assumptions similar to those in Hastie et al.
(2019), some of which we believe can be relaxed in future works, such as isotropic Gaussian input
and linear target. Importantly, the two specific scales of initialization studied in Section 5 are by no
means exhaustive, and thus one would expect that under a different initialization of the first layer, or
the mean-field 1/h scaling of the second layer, the risk of the trained model can be very different.
Changing the loss function may also alter the generalization behavior of the network. Another
challenging problem is to extend the current analysis to beyond two layers. Last but not least, our
result characterizes gradient flow which resembles gradient descent with small stepsize, and thus
it would be interesting to study the effect of learning rate schedule, which has a known impact on
generalization (Smith and Le, 2017; Li et al., 2019b).
Acknowledgement
We thank Xiuyuan Cheng, Xuechen Li, Yiping Lu, Atsushi Nitanda, Shengyang Sun and anonymous
reviewers for helpful comments and feedback. JB and DW were partially funded by LG Electronics
and NSERC. JB and MAE were supported by the CIFAR AI Chairs program. TS was partially
supported by JSPS Kakenhi (26280009, 15H05707 and 18H03201), Japan Digital Design and
JST-CREST.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017.
Peter L Bartlett, Philip M Long, Gdbor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. arXiv preprint arXiv:1906.11300, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019.
9
Published as a conference paper at ICLR 2020
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random
Matrices: Theory and Applications, 2(04):1350010, 2013.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in neural information processing systems, pages
3036-3046, 2018a.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018b.
Bin Dong, Jikai Hou, Yiping Lu, and Zhihua Zhang. Distillation ≈ early stopping? harvesting dark
knowledge utilizing anisotropic information retrieval for overparameterized neural network. arXiv
preprint arXiv:1910.01255, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Noureddine El Karoui et al. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):
1-50, 2010.
Zhou Fan and Andrea Montanari. The spectral norm of random inner-product kernel matrices.
Probability Theory and Related Fields, 173(1-2):27-85, 2019.
Mario Geiger, Stefano Spigler, StePhane d'Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. The jamming transition as a paradigm to understand the loss landscape of
deep neural networks. arXiv preprint arXiv:1809.09349, 2018.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019a.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019b.
Sebastian Goldt, Madhu S Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova
Generalisation dynamics of online learning in over-parameterised neural networks. arXiv preprint
arXiv:1901.09085, 2019.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pages
9461-9471, 2018.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571-8580, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in
deep neural networks: Mean field approach. arXiv preprint arXiv:1806.01316, 2018.
Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. pages 950-957,
1992.
10
Published as a conference paper at ICLR 2020
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is prov-
ably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680,
2019a.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pages 8157-
8166, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203,
2017.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019b.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel" ridgeless" regression can generalize.
arXiv preprint arXiv:1808.00387, 2018.
Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional
data. arXiv preprint arXiv:1805.11916, 2018.
Cosme Louart, Zhenyu Liao, Romain Couillet, et al. A random matrix approach to neural networks.
The Annals of Applied Probability, 28(2):1190-1248, 2018.
VA Marcenko and Leonid Pastur. Distribution of eigenvalues for some sets of random matrices.
Math USSR Sb, 1:457-483, 01 1967.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. arXiv preprint
arXiv:1805.12076, 2018.
Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv
preprint arXiv:1712.05438, 2017.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,
2019.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances
in Neural Information Processing Systems, pages 2637-2646, 2017.
Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-
hidden-layer neural network. In Advances in Neural Information Processing Systems, pages
5410-5419, 2018.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error. arXiv
preprint arXiv:1805.00915, 2018.
11
Published as a conference paper at ICLR 2020
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. arXiv preprint arXiv:1808.09372, 2018.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019.
Stefano Spigler, Mario Geiger, St6phane d'Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart.
A jamming transition from under-to over-parametrization affects loss landscape and generalization.
arXiv preprint arXiv:1810.09665, 2018.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces:
optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 3404-3413. JMLR. org, 2017.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.
Francis Williams, Matthew Trager, Claudio Silva, Daniele Panozzo, Denis Zorin, and Joan Bruna.
Gradient dynamics of shallow univariate relu networks. arXiv preprint arXiv:1906.07842, 2019.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Ji Xu and Daniel Hsu. On the number of variables to use in principal component regression. 2019.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. arXiv preprint arXiv:1904.00687, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 4140-4149. JMLR. org, 2017.
12
Published as a conference paper at ICLR 2020
A Additional Figures and Plots
(a) vanishing initialization.
0.15
o.ιo
0.05
0.00
-0.05
-0.10
-0.15
-0.10	-0.05	0.00	0.05	0.10
(b) non-vanishing initialization.
Figure 5: Population risk of two-layer linear network with fixed random 1st layer with SNR=25/16
under Gaussian input and linear teacher. Brighter color indicates larger γ1 .
Figure 4: trajectory of neurons from initialization (dark blue) to optimum (orange) on the first two dimensions
(two-layer SoftPlus student and linear teacher; SNR= 1/4). For vanishing initialization the neurons stay close
to one another throughout the trajectory, whereas for non-vanishing initialization the neurons stay close to
initialization.
S ummary of the Presence / Absence of Double Descent
Singularity in	2nd Layer Trained (RF)	Vanishing Init.	Non-vanishing Init.
Bias	Yi： No; γ2: Yes	Yi： No; γ2: No	Yi： No; γ2: No
Variance	Yi： No; Y2： Yes	Yi： Yes; Y2： No	Yi： No; γ2: No
13
Published as a conference paper at ICLR 2020
Figure 6: Bias and variance of two-layer SoftPlus network with optimized first layer under (A1)(A2). Individual
dotted lines correspond to different γ2 (from 0.2 to 2) which is independent to the risk. The bias and variance for
both initializations is well-aligned with Theorem 7 and Theorem 8, respectively.
(a) trained 2nd layer.	(b) trained 1st layer.
Figure 7: Population risk (scaled by 1/d) of two-layer ReLU network trained to fit a two-layer ReLU teacher
model with h = d neurons. Brighter color corresponds to larger γ1. Similar to the linear teacher case, double
descent is observed when the second layer is optimized (a) but not when the first layer is optimized (b).
Figure 8: Bias of (a) SoftPlus and (b) sigmoid two-layer network with optimized first layer under (A1)(A2).
Note that bias under i.i.d. initialization is also independent to overparameterization (γ2), but is higher than the
bias under symmetric initialization (“doubling trick”) and not always upper-bounded by the null risk r2 .
B	Background
B.1 Rotational Invariance
The rotational invariance of Gaussian distribution is crucial in our analysis throughout this paper.
A basic observation is that for a random Gaussian matrix X and any fixed unitary matrix U, the
distribution of X and UX are the same.
14
Published as a conference paper at ICLR 2020
Lemma 10 (Rotational Invariance). Denote A(X) ∈ Rd×d a matrix function ofX ∈ Rd×n. If A(X)
satisfies that A(UX) = UA(X)UT for all unitary U, then
EX [βT A(X )β] = 1 βτ βEχ [tr(A(X))].
d
(15)
for any fixed nonzero β ∈ Rd and random matrix X with each entry i.i.d. Xij 〜N (0, σ2).
Proof. Choose a set of Unitary matrices {Ui}id=1 such that Ui>β = kβk ei, where ei is the i-th
canonical vector in Rd. Since UiX 〜X, We have E[A(X)] = E[A(UiX)] = UiE[A(X)]U> and
hence
1 d	1 d	βTβ
E[βTA(X)β] = d ∑E[βTUiA(X)U>β] = d ^>TE[A(X)]ei = βdβE[tr(A(X))]. (16)
d	dd
i=1	i=1
Note that the property also holds for matrix function A(X) that satisfies A(XU) = UA(X)UT, and
can be extended to matrix function A that takes multiple matrices as input.
□
For brevity we will refer to rotational invariance instead of equation (15).
B.2	MARCENKO-PASTUR LAW
For a real symmetric random matrix A ∈ Rp×p , define its empirical spectral density as
μa(dλ)
1p
^∑Sδλi(A)(λ)dλ,
p i=1
where δa(x) = δ(x - a) is the Dirac delta function. Assume A = Wp 〜 Wp (I, n) is a Wishart
matrix, i.e. Wp = X>X/n and X ∈ Rn×p is random Gaussian matrix With each column i.i.d.
Xi 〜N(0,I). Marcenko and Pastur (1967) showed that as n,p → ∞ andp/n = Y ∈ (0, ∞), the
empirical spectral density μwp (dλ) converges weakly to a limiting density μMP(γ) (λ):
μMP(γ)(dλ) = [1 - Y 1] + δθ(λ)dλ + 2∏ λ ʌ/((I + √γ)2 - λ)(λ -(I- √γy2)dλ∙	(17)
We say μMP(γ) is the density of the Marcenko-PaStur distribution with support S = [(1 - √γ)2, (1 +
√γ)2] (for 0 < γ < 1) or S = {0} ∪ [(1 - √γ)2, (1 + √γ)2] (for Y ≥ 1). Note that this implies that
the smallest non-zero eigenvalue of A is bounded away from 0 a.s. for Y 6= 1.
The explicit form of Marcenko-Pastur distribution allows us to investigate the asymptotic properties
of random matrices. Generally speaking, by Pormanteau theorem one can translate any bounded
continuous function on the empirical spectral density to the one on MarCenkO-PaStUr distribution, i.e.
for any bounded f(λ) ∈ C(S), as n,p → ∞ with p/n = Y, almost surely
f f (λ) ∙ μw (dλ) → f fs(λ) ∙ μMP (dλ).
(18)
One implication is the following trace concentration on the inverse Wishart matrix for Y < 1,
1	1p 1	1	1	1
tr(X X)	=tr (PWp	= = P X λi(Wp)	= JS λμWp(dλ)	→ JS λμMP(Y)(dλ)	= L.
(19)
We remark that instability of the trace of the invert Wishart matrix as Y → 1 plays an important role
in the double descent phenomenon.
B.3	Orthogonal Polynomials
Orthogonal polynomials are useful in the analysis of nonlinear random matrices. Suppose φ is a
function in L2(R, μg), where G 〜N(0,1), μg(dx) = (√2π)-1e-x /2dx is the Gaussian measure.
For n ≥ 0, define the Hermite polynomials
Hn(X) = (-1)ne-x2/2 黑 e-x2/2,	(20)
15
Published as a conference paper at ICLR 2020
Note that orthogonality can be easily verified:
/
R
E[Hj(G)Hk(G)]
Hj(X)Hk(x)μG(dx) = j! ∙ δjk.
(21)
Since {Hi(χ)}∞=0 forms a set of orthogonal basis in L2 (R, μg), the function φ(x) can be expanded
under the Hermite basis as
∞∞
φ(x) = X CiHi(X) = X (k! J φ(a)Hi(a)μG(da)) Hi(X).	(22)
Following Cheng and Singer (2013), we mainly focus on the first few terms of the Hermite expansion.
One can check that H0(X) = 1, H1(X) = X, and φ(X) can be expanded as
φ(X) = c0 + c1X + φ⊥(X),	(23)
where c0 = E[φ(G)], c1 = E[Gφ(G)], and terms in the RHS are orthogonal to one another in
L2 (R, μG). Taking square and expectation over both sides yields
E[φ(G)2] = E[φ(G)]2 + E[Gφ(G)]2 + E[φ⊥(G)2],	(24)
which indicates E[φ(G)2] - E[φ(G)]2 - E[Gφ(G)]2 = E[φ⊥(G)2] ≥ 0. Note that the equality holds
if and only if E[φ⊥ (G)2] = 0, i.e. φ(X) = c0 +c1X is linear. One application of this decomposition is
to “linearize” a non-linear matrix in high dimensions, which will be useful for the following sections.
C Proof of Main Results
C.1 Proof of Lemma 1
Given features X ∈ Rd×n, labels y ∈ Rd and model parameters θ, the gradient flow of θ on the
squared loss y - X>θ 22 can be written as
dθ(t) = 1 X(y - X>θ(t)).	(25)
∂t n
Thus with initialization θ0, the solution of this ODE at time t can be written in explicit form
θ(t) = e-nxx>θo + (XX>)t(I - e-nxx>) Xy.	(26)
Since θ0 = 0, taking t → ∞ yields the desired result.
C.2 Proof of Theorem 2
We compute the bias and variance for different cases of γ1, γ2. We first discuss the case where the
random feature ΦX is not full rank (Case I). Otherwise when ΦX is full rank, we discuss whether it
is full column rank (Case II) or full row rank (Case III).
Case I:	W>X is not full rank, i.e. γ1 < 1, γ2 > γ1. In this case rank (ΦX) = d < min(n, h),
and thus by taking the Moore-Penrose inverse we obtain
β = W (X >W )ty = (XX >)-1X y.	(27)
It is clear that the mean and variance is identical to the underparameterized regime in Hastie et al.
(2019), i.e. when n, d, h → ∞,
B → 0; V →	σ2.	(28)
16
Published as a conference paper at ICLR 2020
Case II:	WTX has full column rank, i.e. γ < 1,γι > γ2. By Lemma 1, the solution of the
second layer coefficients is
B = W (WT XX tW )-1 WT X y.	(29)
Denote W = UΣ VT the singular value decomposition. We perform the block decomposition:
Σ
∑0
0
X0
X1
(30)
X
where ∑0 ∈ Rh×h, X0 ∈ Rh×n, Xi ∈ R(d-h)×n, and notice that X0, Xi are independent. By a
concentration of measure argument (e.g. Tao (2012); Hastie et al. (2019)) one can show that the
quantity below tightly concentrates at its expectation. For the variance we have
V = tr (W (WtXXtW)-1 WtXτσ2XW ((WtXXtW)-1 WT)
=σ2tr (WτW (WτXXτW)-1) = σ2tr (∑τ∑ (∑τUτXXτU∑)-1)
〜σ2tr (∑τ∑ (ΣTXXτ∑)-1) = σ2tr((X0X0r)-1) → σ2-^2-,	(31)
where the last equality follows from Appendix B.2. Similarly for the bias term we have
B = ∣∣W(WτXXτW)-1WτXXτβ - β∣∣2
=BT (W(WτXXτW)-1WτXXT - Id) (W(WτXXτW)-1WτXXT - Id)β
=Jtr ((W(WτXXτW)-1WτXXT - Id)T(W(WτXXτW)-1WτXXT - Id))
=r2 tr ((U ∑V τ(V ∑τU τXX τU ∑Vτ )-1V ∑τU T XX T - Id)T(∙∙∙))
=r2 tr ((U ∑(∑τXX τ∑)-1∑τXX τUT - UU T)T(∙∙∙))
=r2tr ((∑(∑TXXT∑)-1∑τXXT - Id)T (∙∙∙)) ,	(32)
where symmetric arguments are omitted as (∙ ∙ ∙ ), and (i) follows from the rotational invariance
argument introduced in Lemma 10 and that βτ β = r2. By the block decomposition (30),
∑ (∑τXXτ∑)-1 ∑τXXT - Id =	0 (X0x>I-1x0XT	.	(33)
Therefore the bias term simplifies to
B = r2tr ((∑(∑TXXτ∑)-1∑τXXT - Id)τ(∑(∑τXXτ∑)-1∑τXXT -
r2
=至(tr ((X0X>)-1X0X>X1X>(X0XT)-1) + (d - h))
Jr ( (d - h)h , d h∖、Y1 - Y2 2
→ ?(nɪɪ + d - h) → γ√τ-^2)r.
(34)
Thus we have obtained that as n, d, h → ∞
B→
γ1-γ2 户
YI(I - 72)
(35)
Case III:	WτX has full row rank, i.e. 71 > 1,72 > 1. Similarly, the least squares solution is
β = WW τX (X τWW τX )-1y,
(36)
17
Published as a conference paper at ICLR 2020
Simplifying the variance:
V = tr (WWTX (XTWWTX)-1 σ2 (XτWWTX)-1 XTWWT)
=σ2tr (WWTUΣVT (VΣtUtWWTUΣVT)-2 VΣtUtWWT)
〜σ2tr(WWτΣ (ΣTWWτΣ)-2 ΣτWWτ) .	(37)
where We applied the SVD of X = U ΣVT and the rotational invariance argument. Using a similar
block decomposition on W :
Σ
Σ0
0
W0
W1
(38)
W
where Σ0 ∈ Rn×n, W° ∈ Rn×h,Wι ∈ R(d-n)×h, and W°, Wi independent. We thus simplify V as
V = σ2tr (WWτΣ(ΣτWWτΣ)-2ΣτWWτ)
=2t (∖ W0WTΣ0(ΣTW0WTΣ0)-2Σ0W0WT	…
=σ tr	∙∙∙	WiWTΣ0(Σ0W0WTΣ0)-2Σ0W0WT
=σ2 (tr (Σ-τΣ-i) +tr (WiWT (W°W>)-1 Σ-τΣ-i (W°W>)-1 WοWj))
=σ2tr ((X T X)-1) + σ2tr (WTWiWT (W0W))-1 (Σ0Σ°)-1 (W0W>)-1 W0) . (39)
Hence we obtain the following expression on the variance
V → σ2^ɪɪ + σ2(d - n)Ew,χVtr ((W0WT)-1 (Σ0Σ°)-1) → σ2 (^-I + ^ɪ).
(40)
We omit the derivation of bias, which follows a similar derivation:
B → γ2(Yi- I)『2
γ1(γ2 -1).
(41)
Combining Case I, II, III yields theorem 2.
□
C.3 Proof of Proposition 3
Given the squared loss, one can derive the dynamics of W with fixed second layer a w.r.t the loss:
dW^ = -1X(y - XτW(t)α)ατ.	(42)
∂t n
Note that the update of W can be written as a linear combination of α. Since W(0) = 0, we can
write W(t) = W(t)α T for some &. The corresponding flow on W is
^ = -nX(y - XτW(t)∣∣a∣∣2),	(43)
which gives the following solution
W* = ɪ7XIy ⇒ β = W*a = Xty.	(44)
ka∣∣2
Thus gradient flow on the first layer leads to the minimum-norm solution on the input features. □
18
Published as a conference paper at ICLR 2020
C.4 Proof of Theorem 4
Following the bias-variance decomposition (7), the variance term can be written as (σ2 omitted)
V = Ex,ε hka>φ(W>x) - Eεa>φ(W>x)k22i
=Eχh∣∣[Φ(W >X )]tφ(W >x)∣∣2i
=tr ([φ(X>W)]t[φ(W>X)]tEχ[φ(W>x)φ(W>x)>])
=tr[φ(X>W)t[φ(W>X)tKW ,	(45)
where we define the expected non-linear Gram matrix KW ∈ Rh×h as
KW = Ex hφ(W>x)φ(W>x)>i.	(46)
and for each entry we have (KW)[i,j] = Ex φ(wi>x)φ(wj>x) .
Random matrix in the form of covariance matrix of nonlinear features has been studied in many
works Hastie et al. (2019); Mei and Montanari (2019); Liao and Couillet (2018); Louart et al. (2018);
Pennington and Worah (2017). We note that our setup for the variance term is very similar to that for
nonlinear features in Hastie et al. (2019) with modifications mentioned below.
In contrast to the linear network in Section C.2, the Gram matrix of a nonlinear activation is almost
surely full-rank, as specified in the following lemma from Pennington and Worah (2017):
Lemma 11. Suppose φ is not linear. Then the smallest singular value of Φ = φ(W>X) ∈ Rh×n
is of order O(√n). To be precise, consider the empirical SPectraI density μφφ>/n(dλ), when
n,d,h → ∞ with d/n → γι and h/n → γ2, μφφ>/n(dλ) converges weakly to
μφφ>∕n(dλ) → [1 — Y2 1]+δo(λ)dλ + μ+(dλ),
where μ+(dλ) has non-negative SUPPort [ρ, ∞) with ρ > 0.
We therefore consider two scenarios: Φ is full column rank (Case I) or full row rank (Case II).
Case 1. h < n. In this case (45) simplifies into
V = tr ((φ(W>X)φ(X>W)) TKW) =Jimtr ((ΦΦ> - ξI)-1 KW) =Jim Vξ,
where the continuity and boundness ofVξ at ξ = 0- is guaranteed by Lemma 11 when γ2 = h/n 6= 1.
A ridge ξ is added make use of (Louart et al., 2018, Theorem 1), which derived the asymptotic
equivalent of the resolvent ΦΦ> - ξI -1. It follows that as n, d, h → ∞,
tr h-1(ΦΦ> - ξI)-1
__________Kw
1 + h-1tr (h(ΦΦ>
≤1 ∣∣(ΦΦ>-ξi)-1 - (n----------/W----------.—丁]
h ∣∣	∖h 1 + h-1tr (h(ΦΦ> -ξI) Kw)J
• ∣∣ n_________Kw_____________
∣∣ h 1 + h-1tr (h(ΦΦ> -ξI)-1 KW) 2
≤ 1 O(nT∕2+e)O(n1∕2) → 0.
F
(47)
where we have used the inequality tr (AB) ≤ kAkF kBk2, and equivalently by taking ξ → 0 we get
n tr ((ΦΦ> -ξI)-1Kw)
lim lim ---------------------------——= 1.	(48)
ξ-0 n,d,h→∞ h 1+tr ((ΦΦ> - ξI)	Kw)
19
Published as a conference paper at ICLR 2020
Therefore limξ→o limn,d,h→∞ n/h ∙ tr ((ΦΦ> - ξI)-1 KW) = γ2∕(l - γ2). Note that ∂Vξ/∂ξ =
tr ξ(ΦΦ> - ξI)-2KW is bounded around the neighbourhood of ξ = 0, hence following the same
argument as (Hastie et al., 2019, Theorem 4), we exchange the limit of ξ → 0 and n, d, h → 0:
V → 产厂.	(49)
Case 2. h > n. Techniques used in the current proof are largely borrowed from Hastie et al.
(2019); Cheng and Singer (2013), and we include the full proof for completeness. It should be noted
that compared to Hastie et al. (2019) we handle the non-zero expectation of the nonlinearity under
Gaussian distribution, i.e. the off-diagonal entries of the kernel matrix is no longer zero-centered. For
simplicity we mainly adhere to the notations in Hastie et al. (2019).
We briefly summarizes the procedure for deriving V. Instead of calculating the variance directly,
we analyze a modified quantity Vξ and then take ξ → 0, which can be connected to the trace of the
resolvent of matrix A defined in (56); this translates the calculation of Vξ into the calculation of the
..... . .~..........................
Stieltjes transform of A (57), (61), (62).
C.5 DERIVING THE VARIANCE V FOR h > n
Step 1. An equivalent expression. For notational simplicity we omit the magnitude σ:
V = tr φ(W>X)(φ(X>W)φ(W>X))-2φ(X>W)KW .
(50)
and due to the same continuity argument as in Case 1 we have
V = Iim 1 [tr (S(S>S - ξIn)-2S>KW) ] = Iim Vξ.
where S = φ(W>X)∕√n = Φ∕√n, ξ ∈ C and =ξ > 0 or ξ < 0.
We decompose the normalized feature matrix S = φ(W>X)∕√n as S = UΣV>, where Σ =
diagh×n(φι, •一，φn) ∈ Rh×n is atall diagonal matrix, and U = [uι, ∙∙∙ , uh] ∈ Rh×h is the set of
orthogonal eigenvectors of SS> = φ(W>X)φ(X>W)∕n, and V ∈ Rn×n is the set of orthogonal
eigenvectors of S>S = φ(X>W)φ(W>X)∕n. Now the variance can be written as
Vξ = Itr(U∑(∑>∑ + ξIn)-2∑U>Kw).
(51)
By the same argument as in Lemma 13 of Hastie et al. (2019) one can show that when n, d, h → ∞
Vξ = n htr (U∑(∑>∑ + ξIn)-2∑U>Kw) i → n [tr (U∑(∑>∑ + ξIn)-2∑U>KW) ], (52)
in which KW is the approxmation of KW defined in Lemma 16. Writing the trace explicitly (denote
eigenvalues λi = φi2, and φn+1
φh = 0), we have
1	1h λ
Vξ → nntr (U∑(∑>∑ + ξIn)-2∑U>KW) = Y2- X (λ + ξ)2 U>KWu	(53)
Since the positive support of spectrum λ is lower bounded and the density at 0 is 1 - γ2-1, we have
1	h	λi	>	λ	λ +
腔→γ2hh,dln→∞∑ (λ+eFUiKWui = γ2J WeFμ∞(dλ) =γ2Jλ>ρ≡>μ∞(dλ),
(54)
where we define μn (x) = h Ph=1 δλi (χ)u>KWUi and its positive part μ+ (x). Hence we have
V=ξi→o 腔=ξi→o Mλ>ρ /铲 μ∞(dλ)=U>ρ 1 μ∞(dλ).
(55)
20
Published as a conference paper at ICLR 2020
We define the following matrix An(ρ,ς,τ) ∈ RN×N where N = n + h:
An(ρ, ς,τ) = [ ρIh + ς 1S1> + TQ , ] .	(56)
S	0n
And denote the Stieltjes transform of An as
mn(ξ, ρ, ς,τ) = Itr ((An(ρ, ς,τ) - ξIN)-1) .	(57)
Then following the definition of KW one can show that
1	宜 w X — ξIh
mn(ξ,rx,sx,tx) = Tr	§>
S
-In
-1
(58)
and taking matrix derivative gives
d~+ ∖ I 1 ÷τ-	- ∖ -ξIh	S	-I	I"	Kw
-∂Xm n(ξ,rx,sx,tx)lχ=0	= n trU S>	—In]	[ 0
=1tr (∖ ξ2Ih + SST 0
=n	0	In + S>S
1tr (U(ΣΣ> + ξ2Ih)-1U>Kw
n
0 ]「—ξIh	S
0	S>	—In
∏KWO ]!
1h 1
=-X V—72 u> KWui.
n i=1 λ + ξ2
(59)
Denote the limit m(ξ, ρ, ς, τ) = limn,h,d→∞ mn(ξ, ρ, ς, τ), and the derivative is given as
^m(ξ,rx,sx,tx)I	= lim 1 X 占u>KWUi
∂x	x=0	h,d,n→∞ n i=1 λ + ξ2 i
1	γ2 — 1	1	+
=Y2	2 , .2 μ∞(dλ) = e2	+ Y2	2μ∞(dμ∞(dλ). (60)
λ≥0 λ + ξ	ξ	λ>ρ λ + ξ
For simplicity we define the following function on ξ:
—
q(ξ)
gm(ξ, rx, sx, tx) I
∂x	x=0
(61)
—
also denote q+(ξ) = q(ξ) — γξ-1 = γ2 fλ> ɪ^μ∞(dλ). Comparing with (55) yields:
V = ξli→m0q+(ξ).	(62)
Step 2. Calculating q(ξ) and mn(ξ, ρ, ς, T) This subsection aims to calculate m(ξ, ρ, ς, T) and
q(ξ) = —mX(ξ, rx, sx, tx) ∣χ=o, from which the variance can be computed from (57)(61)(62).
We define An by subtracting the off-diagonal entries of the upper-left block ofAn:
An(ρ,T) =	ρIhS+>TQ 0S ] ,	(63)
S	0n
where S = S 一 aoIp×n, i.e. Sik = φ(w>xk) — a0 =夕(w>xk), ao = E[φ(x)]. The Stieltjes
transform of An given by mn(ξ, ρ, T) = ntr ((An(ρ, T) — ξIN)-1). The following Lemma shows
that mn and mn, have the same limit:
Lemma 12. when n → ∞ andfor =ξ > 0 or ξ < 0, we have mn(ξ, ρ, T) → mn(ξ, ρ, ς, T).
Proof. By definition of √!n and An,
>
An(ρ,ς,T) — An(ρ,T)
1h	0h	ξ	a0	1h	0h
0n	1n	a0	0	0n	1n
(64)
21
Published as a conference paper at ICLR 2020
which is a rank-2 matrix. By theorem A.43 from Bai and Silverstein (2010), which characterizes the
effect of finite-rank perturbation on the e.s.d. of random matrices:
sup |F An (x) — F An (x)| ≤ O (n-1),	(65)
x
where FM is the empirical spectral distribution of M ∈ Rn×n . The claim follows from the Stieltjes
continuity theorem (e.g. Section 2.4 in Tao (2012)).	□
To calculate the Stieltjes transform mn, we take advantage of the block structure of An:
m1,n(ξ, P,τ) = Ptr ((An(P, τ) - ξIN)-1.p,1..p]) ,	(66)
m2,n(ξ, P,τ) = 1tr ((An(P,τ) - ξIN)[p+l..p+n,p+1..p+n]) ∙	(67)
One can observe that mn (ξ, P, τ) = γ2m1,n (ξ, P, τ) + m2,n(ξ, P, τ).
In the following equations we omit the subscript n, as well as dependency on P, ς , τ . Following
Aa
Hastie et al. (2019), We rewrite the matrix An = A =	。丁 0	, where A* Isa (N — 1) X (N — 1)
matrix with last column and row of A removed and s the activation vector:
A* =	PIhS+>τQ	S*	; a> = [φ(W>xn)> 0n>-1] = [s> 0n>-1].	(68)
S*	0n-1
Hence by the block matrix inverse formula
*	*
(A TlN)	= [ * [—ξ-a>(A*-ξlN-I)-%]-1].	(69)
Plugging this back in the Stieltjes transform we have
m2,n(ξ, ρ, τ)	=ntr	((An(P,	ς,	τ)	— ξIN)[h+1,N])	=	Ea	[(An(P,τ)	—	ξIN)	ɪ] NN
=Ea h( — ξ — aT(A* — ξlN-i)Ta)T].	(70)
To obtain an asymptotic description of m2,n , we perform the orthonormal decomposition on
the nonlinearity 夕 introduced in Cheng and Singer (2013): 夕(x) = a、x + 夕⊥(x), where
aι = Ex〜N(o,i)[x夕(x)]. For each Wi(1 ≤ i ≤ h), we perform the following orthonormal de-
composition (along the direction of Xn and the direction of Wi perpendicular to Xn):
T	Xn	Xn
Wi = WiXn	+ W i = nin
J{z∙} kXnk	kXn k
ηi
+ W i.
(71)
We can thus simplify the activation vector a as
aT
kXnkη1)
|
a1kXnkη1
+
√n夕 ⊥(IlXnlI?。
1T= HkXnknh)
n
√naι∣∣Xnknh	0
^^^^^{^^^^^~
a1>=[s1>,0>]
0	…	0
|---{----}
n-1
...0
-- J
^{z
n-1
}
-√n=φ⊥(kχnknh)	0	：	0
n-1
|
}
(72)
22
Published as a conference paper at ICLR 2020
and for 1 ≤ i = j ≤ h, 1 ≤ k ≤ n — 1,
QLSi嵩+wi)	(%高+Wj)= ηiηj+w⅛.
A
Qij
(73)
Similarly We decompose the activation function in S,
Cf 1	xnxk l -τ
Sik = √ndηi TxTT+w i xk
1 Xnxk l 1
Tnaln，向T + √n
1	xnxk l 〜T
αιwi xk + -j=φ⊥ η,-∏-	+ Wi xk
Vn	Ilxn Il
1	-T -	1	xnxk l 1
Tno(W i xk)+√nαιηi 向τ+√n
|----------
Sik
, |---------V-------' Y
ai ηiUk
Yrh xxxr+w T xk) -”(W T xk)[.(74)
^^z
Eik
,
We thus have an equivalent expression of matrix A*
A*
TIK ri
ρIh + τQ S*
ST 0n-1
TIK ri
ρIh + τQ S*
ST 0n-1
+
+
、
0h
u
∙∖z
U
tηη T	aιηu T	+	Eq	Ei
aiuηT	0n_1	J [ ET	0n-1
A* + UCU T + E.
+ e0 e1
E T 0n-1
|------V-------'
E
(75)
By argument similar to (Hastie et al., 2019, B.1.2), E diminishes to 0 as n → ∞ with respect to the
Frobenius norm, therefore by the Woodbury,s identity and the expression of m2,n in (70)
m2,n(ξ, ρ, τ )= Ea [( — ξ — α > (A* — ξIN-1) 1a)]
→ Ea [( — ξ — a > (A* — ξIN-1 + UCU T ) -1 a)
=Ea [( — ξ — aT (A* — ξIN-1)-1a +
a t
1^
u
(A* - ξIN-1)-1 U (C-1 + UT (A* - ξIN-1)-1U)	U(A* - ξIN-1)
^z
v>
/ ∖
■\z
S
,
∙∖z
V
3 )-i
(76)
We bound each term u, v, S to compute m2,n. For U
EaU = Ea [aτ(A* — ξlN-1)-1a] = tr(EJssT](A* — £In-1)-1.h,1..h]) = bγ2m1,n(ξ,p,τ),
(77)
where b = Ex〜N(o,1)[夕(x)2] = Ex〜N(o,1)[(Φ(x) — Eφ(x))2] = r. From a standard concentration
of measure argument we have that as n, h, d → ∞
U → EaU = rγ2m1,n(ξ, P,T).
And for v (note that U is dependent on a)
EaVT = Ea[aT(A*— ξlN-1)-1U] = EakST,0n_1](A*— ξlN-1)-1	0ɪɪ 0h 1
=[Es[ST(A*-ξIN-1)-1.h,1..h]η] 0] = [tr((A* —ξIN-1)-1.h,1..h]ES[ηsτ])
|-----------------------------------------------------V--------------'
v
where V = tr ((A* — £In-1)-1.h,1..h]Es[ηsτ]) = a1√7∣7τ1m1,n(ξ, p, τ). We thus have
v → Eav = a1 √72∕71m1,n(ξ,P,τ) 0 .
(78)
0].
(79)
(80)
23
Published as a conference paper at ICLR 2020
as n,d,p → ∞. And finally for S,
EaS-1 = Ea [C-1 + UTd - ξlN-1)-1U]
T
aι
ai
0
-1 + Ea 口 O1-'
0	1/a1
1/a1	-T/a
0	1/a1
1/a1	-T/a：
0	1/a1
1/a1	-丁/a：
+ Ea
+
+
1/ai
,2
1
2
1
2
1
γ-1γ2m1,n(ξ, P,τ)
-I—	~
ητ(A*
tr
T
.≈	-1
(A* - ξlN-1)
-ξIN-1)-1.h]η
0	uτ
-ξiN-1 )-1h]Ea[ηητ]
0
72/71m1,n(€,P,T )
0
1/。1
m2,n(ξ,P,τ) - Ta
η
0n-1
0h
u
/ 7	.	、
(A* - ξIN-I)
[41..…]U
0
tr ((A* -ξIN-1)[h；1..h+n-1]Ea[uu>])
0
m2,n(ξ, P,τ)
(81)
0
And hence as n,d,h → ∞,
S-1 → EaST
Y-1Y2 m1,n 仁,P,τ)
1/a1
1/。1
m2,n (ξ,P,τ) - τA2
(82)
Therefore by combining (78), (80), (82), We arrive at the following expression on m2,n,
m2,n(ξ, P,τ) → Ea [( - ξ - u + VTSv) ] → ( - ξ - u + VTSV)
→( - ξ - rγ2m1,n + (a
m1,n)
Y-1γ2m1,n (ξ,P,τ )
-ξ - rγ2m1,n +
72a?m1,n(a2m2,n - T)
1/ai
-1
1/a1
m2,n(ξ, P,τ) - Ta
-1	-1
」[1,1]
m1,n(a2m2,n - T) - γ1γ-1
(83)
Similarly we can calculate m1,n(ξ, P, T) as
m1,n(ξ,P,τ) →
(	-1	2	T27-172m2 n(α2m2-T)-2τα2m1,nm2,n + a4m1,nm2 n∖ 1
-ξ-P-Y1 Y2τ m1,n -rm2,n +-------------,----口----------；--------------------
∖	m1,n(α2m2,n - τ) - γ1γ2	)
(84)
Uniqueness in (84)(83) follows from (Hastie et al., 2019, Sec B.1) and is omitted.	□
C.6 Proof OF Corollary 5
In this section we take the limit γ1 → ∞. In this case (8), (9) simplify to
m2 = (-ξ - rγ2m1)-1
(85)
m1 = (-ξ — P — γ2T 2m1 — rm2)	.	(86)
Recall that m(ξ, p, T) = γ2m1(ξ, p, T) + m2(ξ, p, T). By taking the derivative we have
,八 ∂	“	、i
-q(S) = ∂Xm(ξ,rx,tx)[τ
∂	、I	∂
r∂Pm(ξ,ρ, 0) 1 ρ=0 + t∂Tm(ξ, 0，T)L=0
∂ 一 、|	∂	―	、|	∂	―	、|	∂	―	、|
rγ2 ∂Pm1(ξ,ρ, 0ko + r∂Pm2(ξ,ρ, 0ko + tγ2 ∂Tm1(ξ, 0,τ)L=0+ t∂Tm2(ξ, 0,τ)L=0
(87)
24
Published as a conference paper at ICLR 2020
Observe that (85), (86) constitutes a set of implicit functions. Thus differentiating the two functions
with respect to τ, ρ and then substitute by ρ = τ = 0 gives
q(ξ)
(r(Y2 - 1) - ξ2) (J(r(γ2 - 1)+ ξ2)2 - 4rγ2ξ2 + r(γ2 - 1) + ξ2
2ξ2J(r(Y2 - 1) + ξ2)2 - 4rγ2ξ2
(88)
Hence by (62) we obtain the asymptotic variance:
V(γι→∞) = ξ→0 q+(ξ) = ξ→0 (q(ξ)- γ2ξ-^) = γ21-1.	(89)
Combining the case where γ2 < 1 in Theorem 4 completes the proof.
Remark. For φ(x) = ReLU(X), ci = 1/2 — 1∕(2π), c? = 1/4. For φ(x) = SoftPlus(X)
log(1 + ex), numerical integration yields c1 ≈ 0.2715, c2 = 1/4.
C.7 Proof of Corollary 6
C.7.1 UNBOUNDED BIAS WHEN h = n
2
From the bias-variance decomposition (7), the bias B is written as B = %tr (Qi + Q2 + Id), where
Qi = X [φ(W >X )]tKw [φ(X > W )]tX >;	Q2 = X [φ(W >X )]t W >.	(90)
When h = n, due to the nonlinearity of φ, we have φ(W>X) is full rank a.s., and hence
[φ(X> W)「= [φ(X> W)]-i. We have the following bound for Qi
dtr(Qi) = dtr(X[φ(W>X)]tKW[φ(X>W)]tX>) = gtr (Kw[φ(X>W)]-iX>X[φ(W>X)]-i)
dd	d
≥ d.(KWW W(X>W)]-2邛。(W>XD = λminnKW1 tr ((SSY1 ∙ dX>X
Since W and X∕√d are Rd×n = Rd×p follows the same distribution where each entry i.i.d.
N(0, 1/d),
八 匚 Jr(Qi) ≥ 1tr ((SS>)-i ∙ 1X>X)〜1tr ((S>S)-1 ∙ W>W)
dλmin (KW)	n	d	n
=Itr ((S>S) 1 ∙ (I + Q)) = Iim - gmn(ξ, x, 0, x) → ∞,	(91)
where in Section C.5 we have showed (91) is unbounded when n → ∞. Moreover, by (154) and
Weyl’s theorem we have λmin(KW) = O(1), and thus d-itr (Qi) is unbounded. For d-itr (Q2),
dtr(Q2) = dtr (w>X[Φ(W>X)]-i) ≤ dλmaχ(φ(W>X)-i)tr (W>X) = O(1).	(92)
dd	d
To sum up, forn → ∞ and γ2 → 1 we have B → ∞.
C.7.2 B OUNDED BIAS WHEN γ2 > 1
Since h > n, the two terms in the expression of the bias can be written as
Qi = Xφ(X>W)φ(W>X))-iφ(X>W)KWφ(W>X)φ(X>W)φ(W>X))-iX>,	(93)
Q2 = X φ(X>W)φ(W>X))-iφ(X>W)W>.	(94)
Therefore we have
2tr(Qi) = 2tr ΓX>X (φ(X>W)φ(W>X))-1φ(X>W)Kwφ(W>X)(φ(X>W)φ(W>X))-
25
Published as a conference paper at ICLR 2020
≤ 2λmaχ (X>X]r (Q(X>W)φ(W>X))-1φ(X>W)Kwφ(W>X)(φ(X>W)φ(W>
=O⑴∙ tr (φ(WTX) (φ(X>W)φ(WτX)) 2φ(X>W)Kw)
≤ O(1) ∙ λmaχ (φ(WτX) (φ(XτW)φ(WτX))-2φ(XτW)) ∙ tr(Kw)
=O(1) ∙ σ-2n(Φ(XτW))tr(Kw)=。⑴∙ O(n-1) ∙ O(n) = O⑴，	(95)
in which we used Lemma 11 and tr (AB) ≤ λmax(A)tr (B) for positive semi-definite A, B. Simi-
larly
2
dtr(Q2)
2tr ( Q(XτW)φ(WτX)) 1φ(XτW) • 1WτX
≤ tr ( ( (φ(XτW)φ(WτX)) 1φ(XτW))(…)T) + tr (d-2XτWWτX)
≤ n ∙σmin (φ(XτW))-2 + 1 λmaχ (JXXT) tr (WWτ) = O(1),
dd
(96)
where we applied tr (AB) ≤ (tr AτA + tr BτB )/2. We therefore conclude that B is bounded
when h > n and h, n → ∞.
Remark. Concurrent to this work, Mei and Montanari (2019) provides a complete characterization
of the bias term and confirms our observations above.
C.8 Proof of Theorem 7
For simplicity we assume n, d, h to be even and let d0 = d/2, n0 = n/2 and h0 = h/2. Since the
second layer is fixed ai 〜 Unif{-1∕√h, 1∕√h}, We let ai = 1∕√h and ai+h0 = -1∕√h for all
1 ≤ i ≤ h0. We therefore write aτ = h-1/2 [1h0, -1h0]τ and W = [W+, W-]:
f(x; W-, W+) = aτφ(Wτx) = √1= 1τφ(W[x)
√h 1τφ(Wlx).
(97)
—
The empirical risk can thus be Written as
L(X； W+,W-) =	J X L(x;	W+,W-)	= J X	[y	- √= 1τφ(Wτχ) + √ 1τφ(W∑x)i2.
n x∈X	n x∈X	h	h
(98)
C.8.1 Defining Gradient Flows
In this section We define three gradient floWs and shoW that the three floWs are similar in some sense.
GF-Original is the original gradient floW (11), i.e.
dW0 = 2n0 X h√h	√h 1τφ(WOTxi) + √h 1τφ(WOTxi))xiφ0(XTWO)i(99)
=2⅛ X(y - yo(t))√hιτ ◦ Φ0(XWO) ,	(100)
starting from the vanishing initialization wo(0)〜N(0,I∕dh1+e). Note that the gradient for the
negative part W-O can be similarly defined.
We noW define the floW under the same objective but from exact zero initialization wiD (0) = 0
termed GF-Double. Due to zero initialization, a basic observation is that the solution [W+D , W-D] is
26
Published as a conference paper at ICLR 2020
at most rank-2, and more precisely, the parameters in the flow takes the form of W±D (t) = w±D(t)1>
where w±D (t) admits the following dynamics:
∂WD	1 2n0 r 1 /	、	1
--∂t+ = g+ (WD)=丽£ [√- (yi - √h^φ(^wD>Xi) + √hφ(wD>Xi)) φ0(wD>Xi)xJ .
i=1	(101)
Lastly, we define the GF-Single with solution denoted as W± = WS± (t) :
∂wS	ι 2n0	ι /	、	I
-∂t+ = g+(w+) = 2n^∑S [√h("i — √hφ0(o)w+>χi + √hφo(0)w-rχi) Φ0(o)χiJ
(102)
from zero initialization W±D (0) = 0. This can be seen as replacing the nonlinearity φ with its
first-order Taylor expansion at the origin.
C.8.2 From GF-double to GF-single
Step 1. Solution of GF-single. Among the three flows defined above, only GF-single an explicit
form at any time t. Specifically, the solution can be written as:
w+(t) = -w-(t) = -⅛ I— — e-*xx>t) (XX>)-1xy,
2h
when d < n, or otherwise
w+(t) = -w-(t) = ɪ X (l-e-φ0n0)2X>x) (X>X)-1y.
2h
(103)
(104)
when d > n. For simplicity we elaborate the proof only for d < n. We provide a condition under
which the difference between the trajectories can be controlled, and we first that this condition holds
for the linearized flow GF-single for all t in Lemma 17.
Condition A:
""O仁)；UX>w(tM∞ = O(Po√0gdJ
Step 2. Bounding the Difference in Gradient Flow Trajectory. Due to low rank property of
GF-single and GF-double, in this subsection we slightly abuse the notation and define
f (x; w±) = f (x; w+1>, W-1>) = √hφ(w>x) — √hφ(w>x).
(105)
We now show that the difference between the two trajectories defined above is asymptotically
vanishing for W+ (W- follows the same argument). Compare the two trajectories up to time T
T
g+D (W+D(t)) — gS+(WS+(t))dt
2
UW+D(T) — WS+(T)U2
gS+(WS+(t)) — gS+(W+D (t)) dt∣∣ +
2
T
g+D(W+D(t)) — gS+(W+D(t)) dt
0
2
— φ0(0)(W+D(t) — WS+(t))>xi + φ0(0)(W-D (t) — WS- (t))>xi φ0(0)xi
T
=O(I) / (∣∣wD(t) — w+(t)∣∣2 + ∣∣wD(t) - w-(t)∣∣2) dt + E+,
0
where we have defined the error term as
T
dtUU + E+
U2
(106)
(107)
g+D(W+D (s)) — gS+(W+D(s)) dt∣∣ .
∣2
27
Published as a conference paper at ICLR 2020
To bound the error term, we note that at t = 0 Condition A holds for GF-double. Assume that for
some 0 ≤ t ≤ T , Condition A also holds for GF-double, we have
E+ = Z	g+D(w+D(t)) - gS+(w+D (t)) dt
2
dt
2
(≤)n√h Z0 O(E)O(IogC d)O⑴ dt+√1n Z0 O(√d)O (logτ) dt = O(logτ)T.
(108)
where (i) follows from the error of Taylor expansion on φ and (ii) from Condition A. Therefore, by
Equation (106) and Gronwall’s inequality,
IlwD(T) - w+(T儿 ≤C1 •号QT =O(p0y^) → 0.	(109)
for T ∈ O(log log h). This shows that up to time T, the difference between the trajectories of
GF-single and GF-double vanishes under the the assumption that Condition A holds for wD up to
T. Importantly, note that wS(0) = wD (0) = 0, and that Condition A holds for wS for all T > 0.
Therefore, by a standard contradiction argument (e.g. (Du et al., 2018, Lemma 3.4)), one can show
that this closeness between the two trajectories implies that Condition A also holds for wD up to
time T. With Equation (108) we bound the difference in population risk between the two flows.
Step 3. Bounding the Difference in Risk. Now we consider the difference of the population risk
RS and RD of two models with parameters wS± and w±D .
|2(RS - Rd)| JEx (x>β - f(x; w±))2 - Ex (x>β - f(x; wD))2∣
≤qEx[f (x;w±) - f(x;wD)]2Ex[β>x - f (x; w±) + β›x - f (x; wD)]2
≤ J hEx [∣φ(w+>x) - φ(wD>x)∣ + ∣φ(w->x) - φ(wD>x)∣]2
•	qEx[∣β>χ-f (x; w±)∣ + ∣β>χ-f (x; wD )|]2
≤ 2qhEX[i^(w+>x)-^(w+D>x)12+_1^(wS>x)-^(wD>x)j2j
•	qEx[∣β>x-f (x; w±)∣2 + Iβ>x- f (x; wD )|2]
(iii)	!---------------------------------------- ----------
≤ 2y hLip(φ)Ex[∣w+>x — wD>x|2 + ∣w->x — wD>x∣2] •，RS + RD
≤o(√h)q∣∣w+ -wD∣∣2 + ∣∣w- - wD∣∣2 ≤ O(poly√0gh) ,	Qi。)
where (i) is due to Cauchy-Schwarz inequality on norm, (ii) from Jenson’s inequality on squares and
Young’s inequality, and (iii) from the Lipschitz assumption on the activation, and the observation
28
Published as a conference paper at ICLR 2020
that both RS and RD are finite for γ1 6= 1 due to the justified Condition A above. Therefore the
difference between RS and RD vanishes for T = O(log log h).
Step 4. Bounding the Difference from Stationarity We compute the difference in risk between
the model at some finite time t and the stationary point i.e. t = ∞,
IRS(t) — RS(∞)∣ ≤ C√h ∙ ∣∣w+(t) — w+(∞)Il = C√h
=C e-φ0n(0)2XXL(XX>)-1XX>β	≤ C exp (-φ0(0)2
∣	∣2
1	- φ0(o)2 XX>
———e	no
2 √h
t(XX>)-1Xy
t kβk2 =C3e-C4t.
2
(111)
1XX >
n
2
for constants C3, C4 > 0. Combining (110) and (111) yields for T = log log h,
|RD(T) - RS(∞)∣ ≤ IRS(T) - RD(T)∣ + IRS(T) - RS(∞)∣
=O (Po半gh) + O ( J) → 0.
h	polylog h
(112)
The result in (112) for case d > n follows a similar proof and is omitted.
C.8.3 From GF-original to GF-double
In this section we compare GF-original with GF-double. Note that the two flows differ only at
initialization: vanishing initialization WO (0)〜N(0,I∕dh1^e) v.s. zero initialization wD(0) = 0.
Note that at initialization ∣∣y - f (X)∣b = O(√n), and gradient flow decreases the empirical risk;
therefore the condition for Lemma 18 is satisfied, and by the Lipschitz condition on the empirical
gradient we have
∣∣WO(T)-WD(T)∣∣2F
=∣∣w O (O) - w D (o)∣∣F + / T d∣∣w O(t)况W D(t)∣∣F dt
=∣∣w o(0)∣∣F+/T ∣∣w o (t) - w D (t))∣∣F ∣ SLX WO(t)) - dL(Xd WD(t)) [ dt
≤ ∣∣wo(0)∣∣F+/T ∣∣wo(t) - WD(t)∣∣F + ∣ dL(XdWO(t)) - dL(X∂WD)) ∣∣2 dt
≤∣∣WO(0)∣∣2F+(1+Lf)ZT∣∣WO(t)-WD(t)∣∣2Fdt,	(113)
And hence by Gronwall’s lemma one obtains:
∣∣Wo(T) - WD(T)∣∣f ≤ ∣∣Wo(0)∣∣f e(1+Lf)T/2 = O(d-0+e"2)eCT	(114)
Therefore the difference in the function output can be bounded as
∣∣f(x,WD(T))-f(x,Wo(T))∣∣2= ∣∣φ(x>WD(T))a-φ(x>Wo(T))a∣∣2
≤ ∣∣Φ(x>WD(T)) - φ(x>Wo(T))∣∣2 ka∣2 ≤ Lφ |阿卜 ∣∣WD(T) - Wo(T升尸
=O(√d) ∙ ∣∣WD(T) - Wo(T)∣∣f = O(d-e∕2)eCT	(115)
Taking T = loglog h, together with the same argument in Step 1 yields
IRo(T) - rd(T)I = O (吟/署h) → 0.	(116)
C.8.4 Putting things together
By (112) and (116), we know that for T = log log h,
IRo(T) -RS(∞)I ≤ IRo(T) -RD(T)I+ IRD(T) -RS(∞)I →0.	(117)
29
Published as a conference paper at ICLR 2020
Finally the proof is completed by observing that RS (∞) is the risk of the minimum-norm solution on
the input discussed in Section 3. In addition, note that at T = log log h, from (109)(114) one obtains
that Il W O (t) - W S (t)∣∣F → 0, and therefore by Lemma 18we have ∣∣∂L(X; W O (t))∕∂W∣∣F → 0,
i.e. the flow on the original objective reaches a o(1) first-order stationary point.
C.9 Proof of Theorem 8
Denote ω = vec(W) = vec([W+, W-]), and ω0 = vec(Winit). Define
d f (X; ω(t)) > df (X; ω(t))
K (t) = ^^ω^------------∂ω(tτ~，	()
which is the kernel matrix of the neural tangent kernel Jacot et al. (2018); Du et al. (2018). In the
following sections we show that under the non-vanishing initialization, the trained two-layer network
is well-approximated by the regression model on the NTK, for which we derive the population risk.
C.9. 1 The Kernel Linearization
Write yNN (t) = fNN (X, t) ∈ Rn and its evolution:
dyNN(t) = 1 K(t)(y - Vnn(t)) dt,	(119)
n
and the corresponding linearized flow:
QyNTK ⑴=nK (O)(V - yNTK(t)) dt,	(120)
Previous works (e.g. Du et al. (2018); Oymak and Soltanolkotabi (2019)) have proved (non-
asymptotically) that the two trajectories (119) and (120) are close if the model is overparameterized,
i.e. h = poly(n), under no assumptions on the teacher model. In our asymptotic setup (together with
assumptions (A1-3)), we argue that similar conclusion holds without significant overparameterization.
We first show the global convergence of the training of two-layer neural network fNN. We employ
an argument similar to (Du et al., 2018, Theo. 3.2) by first identifying the condition under which
training converges at linear rate:
Condition B: λmin(K(t)) = O(d).
From Corollary 15 we know that at initialization the lowest eigenvalue of the NTK matrix satisfies
λmin(K(0)) = O(d), and thus the kernel regression on the NTK enjoys linear convergence. By
Lemma 19, we know that for kW (t) - W (0)k2 = O(d1-), the order of λmin(K(t)) = O(d) re-
mains unchanged. Therefore, ifwe assume that up to time T the weights satisfy kwi(t) - wi(0)k2 =
O(d-1/2), then Condition B is satisfied for yNN and from (Chizat and Bach, 2018b, Lemma B1) we
have the following linear convergence
kyNN(t)-yk2≤C1kyNN(0)-yk2e-C2t.	(121)
Since IlyNN(°) - V∣∣2 = Ο(√n) at initialization, setting T = Ο(logd) ensures that the training
loss n1 IlyNN(T) - yk2 → ° as n → ∞. Consequently it is easy to check that ∣∣ dL∂WW(T)) ∣∣ → °
and thus at time T the gradient flow reaches an o(1) first order stationary point.
We now verify that each weight vector wi travels at most O(d-1/2) from initialization. The norm of
gradient for wi can be bounded as:
"LX WT))	= 1X ∖ √1f(y - Vnn (t)) ◦ Φ0(X >Wi(t))]∣∣
∣	∂wi(t)	∣2	∣ n h NN	∣2
(i)	1
≤ θ ⑴ d15 kX k2 ky - yNN (t)k2 ≤ O(l)d IIIy - yNN (0)k2 eM (122)
where (i) follows from the boundedness of φ0. Integrating the gradient yields Iwi(t) - wi(0)I2 =
O(d-1/2), i.e. IW (t) - W (0)I2 = O(1). Thus the distance traveled by W indeed satisfies the norm
30
Published as a conference paper at ICLR 2020
assumption above, and following the same argument as (Du et al., 2018, Lemma 3.4) we conclude
that Condition B holds true for the gradient flow of fNN for t > 0.
Next We show that for any input X with ∣∣x∣∣2 = O(√d), the prediction of the neural network is
uniformly close to the prediction of the prediction of the kernel model, a result similar to (Arora et al.,
2019a, Lemma F.1). Following the notation of Arora et al. (2019a), we write the time derivative of
the prediction as
IfNN (X ,t) = 1UNN (X ,t)>(y — yNN (t));，NTK (X ,t = 1 UNTK (X ,t)>(y — yNTK (t)),
where UNN(x, t)="噂苗⑶)f 瘠之，" ∈ Rn and UNTK(x, t) similarly defined on the initial-
ized weights ω(0). We bound the difference between the predictions on X UP to terminal time T
as
IfNN (X,t) - fNTK (X, t)|
IZT
0
"dtfNN (X, t) - XtfNTK (X, t)
dtI
=nI∕0 1"NN(X,t)>(y — yNN(t)) — UNTK(X,t)>(y — yNTK(t))] dt|
≤-| Z UNTK (X, t)>(yNN (t) — yNTK (t)) dt|
n0
1T
+ -|	(unN (X ,t) — UNTK (X ,t))>(y — yNN (t)) dt|
n0
≤1 kUNTK (X ,t)∣∣2 [ kyNN 9 —yNTK (t)k2 dt
n0
1T
+ -max ∣∣unn(x,t) - UNTK(X,t)∣∣2	Ily — Jnn(t)k2 dt.
n t<T	0
(123)
For the first term have
∣yNN(T) —
yNTK(T)∣2 ≤ 1 [T ∣k(t)(y - yNN(t)) — K(0)(y —
yNTK(t))∣2 dt
n0
≤ 1nmax ∣K(t) - K(0)∣2 /Tky — Unn(t)k2 dt +1 ∣K(0)∣2 ∣'T ∣Unn(t) - Untk(t)k2 dt
n 0<t<T	0	n	0
≤ 10(d1∕2-e')O(√) + O(I) L
(ii)
∣yNN(t) —
yNTK(t)∣2 dt ≤ O(d-0 ),
(124)
0
where (i) is due to Corollary 15, Lemma 19 for some 0 > 0 and the linear convergence of yNN, and
(ii) is due to Gronwall’s inequality. Note that the log factor in T is omitted. Similarly, for the second
term we have
1	t	(i) 1	0	0
max ∣∣unn(X,t) - UNTK(X,t)k2	Ily — Unn(t)k2 dt ≤ O(d1/2-e )O(√d) = O(d-e ),
n t<T	0	n
(125)
where we used Lemma 19 and the linear convergence of yNN in (i). Combining the two cases yields
IfNN (X ,t) - fNTK (X ,t)∣ ≤ 1 11 Untk (X,t)k2 O(d-e0) + O(L) = O(d-e0),	(126)
where we utilized Corollary 14 in (i). Thus we know that the difference between the population risk
of fNN and fNTK is also asymptotically vanishing (note that the derivation above is independent of
the target function as long as ∣∣y∣2 = O(√n)). Therefore, in the following subsection we compute
the risk of the linearized (kernel) model fNTK .
C.9.2 Computing the Kernel Risk
Given input X ∈ Rd×n and label y = β>X + ε, gradient flow on the tangent kernel solves the
following equation of the parameters ω :
y = f (X； ω)= fX ω0) >(ω — ωo),	(127)
∂ω
31
Published as a conference paper at ICLR 2020
where ∂f (X; ω0)∕∂ω is a dh X n matrix with column ∂f (xi； ω0)∕∂ω. Note that for n → ∞ and
γ1, γ2 ∈ (0, ∞), dh > n trivially holds, and by Corollary 15 we know that solution is given by
ωι = ωo + fX包
∂ω
∂f (X; ωo) > ∂f (X; ωo)
∂ ω	∂ ω
-1
(X>β+ε).
(128)
And the population risk can be written as (note that there is a factor of 2 due to the "doubling trick" at
initialization to ensure fιnιt(∙) = 0):
2R = Eχ,ε [(x>β — f(x; ωι))2]
Ex,ε x>β -
∂ω
2
(ω1 - ω0)
∂f(x; ω0) > ∂f(X; ω0)	∂f(X; ω0) > ∂f(X; ω0)
∂ω
∂ω
∂ω
∂ω
(X >β + εj
Ex ](x>β - U>KKX1X>β)2
I-------------------：
{z"
2B
+ Ex [U 宜X1KKx
^{z
2V
σ2,
.}
(129)
—
」 |
where a bias-variance decomposition is made here, and for simplicity we define
U
∂f (X； ωo) > ∂f(x; ωo)
∂ω	∂ω
∂f(X； ωo) > ∂f (X; ωo)
∂ω	∂ω
(130)
C.9.3 Approximating the Kernel Matrix
In this section we drop the negligible in the initialization. Following Cheng and Singer (2013)
We utilize the orthonormal decomposition of φ0(x) in L2(R, μg). Denote bo = E[φ0(G)], and
b12 = E[φ0(G)2] - b20. We have the orthogonal decomposition of φ0
φ0(x) = b0 + φ0⊥(x),	(131)
where E[φ0⊥(G)] = 0. We develop the following lemmas to approximate the kernel matrix.
Lemma 13 (Approximation of (KX)ij). There exist constants c, c0 > 0 such that for i 6= j with
probability 1 - e-cnε2 we have
1 df (Xi ω0) > df(xj ； ω0)	1 . Trr . 2
d^ω	∂ω	dboxiXj <ε
(132)
and with probability 1 -
c0 nε
1 ∂f (Xi； ωo) > ∂f (Xi； ωo)
d	∂ω	∂ω
- (b20 + b12 )
< ε.
(133)
e
Proof. When i 6= j (i.e. Equation (132)), we have
1
d [KX]ij
1 ∂f(xi; ωo) > ∂f(xj; ωo)
d	∂ω	∂ω
1h
d x
k=1
∂f(xi; ωo) > ∂f(xj; ωo)
∂wk	∂wk
→dx>XjEw hφ0(w>Xi)φ0(w>Xj)i
1h
dh £ x>xj^(wkXi)^(wkXj)
k=1
=dH (Xi, Xj ).
(134)
32
Published as a conference paper at ICLR 2020
The matrix H(xi, xj) = xi>xjEw φ0(w>xi)φ0(w>xj) can be seen as the expected tangent kernel
of nonlinear activation function studied in Du et al. (2018); Arora et al. (2019b). Moreover, due to
the assumed boundedness of φ0(x) (A3), by Hoeffding’s inequality we have
1
Pr
d	∂ω
∂ω
一 dH(xi，Xj) <)
Xi>Xjε > 1 — e-c1hε2.	(135)
d
In addition, by the concentration of xi>xj and kxik22, i.e. Pr xi>xj/d > ε < 1 - e-c2dε2 and
Pr |xi> xi /d - 1| < ε > 1 - e-c3 dε2, the orthonormal decomposition φ0(x) = b0x + φ0⊥(x) leads
to the following linear approximation of the matrix H
d H (Xi, Xj) = b2 dx>xj+O((X>χj/d)2).
and by taking ε = xi>xj/d under the joint event we can show that
(136)
1 ∂f(xi; 30)> ∂f(xj∙; ωo)
d	∂ω
∂ω
- d b2x> Xj
< ε2
(137)
with probability 1 - e-cdε2. The same argument follows for the case where i = j.
Corollary 14 (Approximation of U). For large enough l > 0, with probability 1 — de-c logl d
□
d ku - u k2
1 ∂f(x; ω0) > ∂f(X; ω0)
d	∂ω
∂ω
logl d
< d ,
2
(138)
1
- dU
where U = b2x>X.
Proof. Taking ε = logl d/d together with Lemma 13 yields the desired result.
Corollary 15 (Approximation of 宜X). With probability 1 一 de-c logl d
□
1 ∂f(X; ω0) > ∂f(X; ω0)
d	∂ω
∂ω
< logl d,
F
(139)
1
-清X
where 宜X = b0X>X + b1dI.
Proof. Also by directly applying Lemma 13.
□
Remark. For initialization larger or equal to wi(0)〜N(0, Id/d), the above approximation does not
depend on the scale of initialization.
Remark. For φ(x) = SoftPlus(x), b20 = 1/4, b12 = 0.043379. For φ(x) = sigmoid(x) = (1 +
e-x)-1, b02 = 0.042692, b21 = 0.002144. Note that b1 ≥ 0 for all smooth activations φ, and the
equality holds (i.e. b1 = 0) if and only if φ is linear. We comment that smaller b1 entails larger
variance as γ1 → 1, and vice versa, as shown in the following section.
C.9.4 The Bias Term
With these approximation above we proceed to calculating (129)
2B = Ex (x>β — U >Kχ1X >β)2
We first bound the error in substituting U with U:
M>KX1X>β — U>KX1X>β∣2 ≤ kU — Uk2 MX1 ILkXk2 kβk2
=O flogl d ∙ d-1 ∙ √d ∙ 1)= O
(140)
(141)
33
Published as a conference paper at ICLR 2020
where (i) is due to the fact that
we have as n,d,h → ∞
Mχ1∣2
λm1n(KX) = O(1∕d) and IIX∣∣2 = O(√d). Therefore
2B = EX ](x>β - uTKXIX>β)[ → EX ](x>β - uTKXIX>β^2
=EX (XTe - b0xτX木XIXTe) 2 .	(142)
By taking expectation over x and the rotational invariance argument similar to Hastie et al. (2019),
EX ](xτβ - b2xτXKXIXTβ)2 =EX IeT(I -琉XKx1XT)2 β
=βdβtr ((l -珠XKx1XT)(I- b0XKXIXT)).
(143)
ɪ 1 T , ∙	1	1 ,1	∙	1	, ∙ ,	τC> 1	T√- FC 1 ∙ /Y cc∖
In addition, we bound the error in substituting KX by KX defined in (139):
dtr (XKx1XT - XKx1Xt) J = dtr(XTXKx1(KX - KX)Kx1) |
<d∣∣χTX ∣∣2∣∣Kx1∣12 11KX- KXIUKXX1∣∣2
=O (d-1 ∙ d ∙ d-1 ∙ d logl d ∙ d-1) = O o "g "
(144)
and similarly,
ɪtr (XKx1XTXKx1XT - XKx1XTXKx1Xt) I
=dtr (xTX(KX1 - &x1)XtX(Kχ1 + &χ1)) ∣
<d HX τχ ∣∣21∣Kχ1∣∣21∣Kx- Kx IUIKχ1∣∣2 ∣∣x T X ∣∣21∣Kx1+Kχ1∣2
=O (d-1 ∙ d ∙ d-1 ∙ d logl d ∙ d-1 ∙ d ∙ d-1) = O
Combining these two formulas in (143) yields
2B → EX ](xτβ -琉XTXKx1XTe)
→ β>βtr ((l -bXK-1XT)(I- b0xKXIXT))
=e>βtr ((I -珠X(b0XτX + b2dI)-1Xτ)2).
Utilizing the Marcenko-Pastur law from Section B.2 we obtain
Yi - 1 +Y1(γ1 + Yim + m - 2) + 1
2γι	2γιPYI(YI + m(γι(m + 2)+2) - 2) + 1
where m = b-2b2.
(145)
(146)
(147)
C.9.5 THE VARIANCE TERM
Similarly, for the variance we utilize the approximation
2V = EX IUKXIKχiuTi σ2
(148)
34
Published as a conference paper at ICLR 2020
Specifically, we bound the approximation error
UK-1K-1u> - UK-1K-1U>∣ ≤ ku - uk9
xx	xx	2
=O
d • 1
d
12
• d ∙ ɪ
d
(149)
and similarly
∣Ex ∣uK-1K-1u> - uK-1K-1u>
∣x x x	x x
KX1) (KX1 + KX1) Ex uu >
=tr (Kχ1 (KX - KKx) KKχ1 (Kχ1 + KKχ1) XTX)
≤ 归x1Mkx - Kx IUKχ1ILMχ1+Kχ1MX TX 口2
—
=O(d-1 ∙ dlogl d ∙ d-1 ∙ d-1 ∙ d)
(150)
By combining the two approximations above we know that as n, d, p → ∞
Therefore the variance is given as
2V - Ex IUKXIKXIU>] σ2
→ 0.
(151)
2V → σ2Ex [uKXIKXIU>
=σ2Ex [b0xTX (b0X>X + b-dI)-2 XTXi
=σ21tr (1XTX • (1XTX +
dd	d
σ2 [ - 1+	,	γ- + γ-m + 1	____ ʌ
〈 2	2√γ-(γ- + m(γ-(m + 2)+2) - 2) + 1 1 '
(152)
where m is defined in the derivation of the bias term.
C.9.6 Putting Things Together
Recall the population risk is the sum of the bias and variance
RT r2 P!--1 +
γ1 (γ1 + γ1 m + m - 2) + 1
2γι vzγι(γι + m(γι(m + 2) +2) - 2) + 1
+ σ2 -1+	,	γ- + γ-m +1
∖ 4	4vzγ-(γ- + m(γ-(m + 2) + 2) - 2) + 1
(153)
Observe that the population risk is independent of γ2, i.e. double descent does not occur when
the network is overparameterized via changing the width. In addition, the bias is monotonically
increasing and upper-bounded by the null risk r2 and lower-bounded by the bias of the least squares
SolUtion on the input features β = X*y, whereas the variance remains bounded for all γι ∈ (0, ∞)
as long as m > 0, i.e. φ is nonlinear.

35
Published as a conference paper at ICLR 2020
D Useful Lemmas
Lemma 16. Given KW from (46), define
KW = rIh + s1h1> + tQ,	(154)
where Q ∈ Rh×h with Qi6=j = wi>wj and Qi,i = 0, and r = E[φ(G)2] - E[φ(G)]2, s =
E[Φ(G)]2, t = E[Gφ(G)]2. Then as n,d,h → ∞, ∣∣Kw 一 Kw∣∣ ≤ logc d a.s..
Proof. Consider the event where A = | kwik2 - 1| < , |wi>wj| <	. Under event A, for the
diagonal term of the kernel matrix we have
∖[Kw]ii - [KKw]J = ∣Eχ[φ(w>x)φ(w>x)] 一 E[φ(G)2]∣
= E[φ(kwik2 G)2] 一 E[φ(G)2] = O().
And for off-diagonal term, by the decomposition introduced in Section B.3 we have
[KW ]ij
Ex φ(wi>x)φ(wj>x)
kwik2 kwjk2 + Ex[φ⊥(wi>x)φ⊥(wj>x)],
(155)
(156)
hence |[Kw ]j 一 KW ]j < (w>wj )2. Notice that Ae holds a.s. for e = logc d∕√d and large
enough c > 0; We therefore have ∣∣Kw - 宜w∣∣ ≤ logc d.	□
Lemma 17. Let β(t) be the solution to the gradient flow at time t defined in (103)(104). Then as
n, d → ∞ and γ1 6= 1 the following holds.:
..^ . ...	,	-Γ ^ ....	.	.
kβ(t)k2 = OP (1);	l∣X >β(t)k∞ = OP (poly log d).
Proof. We consider d < n for simplicity, and result for the other case folloWs in similar fashion.
In this case β(t) =(I ― exp(-[XX>)) (XX>)-1Xy. From (Hastie et al., 2019, Corollary 1) we
know that kβ(∞)∣2 = ∣∣(XX>)-1Xy∣∣2 = O(1) for γι < 1. Note that ∣∣I 一 exp(-[XX>)∣∣2 =
O(1) for t ≥ 0; it follows that kβ(t)∣2 = O(1).
For the second part, we utilize the SVD X = UΣV>, where Σ = [Σ; θ], Σ ∈ Rd×d and Σ近=λi.
We have (I 一 exp(-[XX>)) = UΣU> where Σ” = 1 一 exp(-tλ2∕n) if i ≤ d and 0 otherwise.
kX>β(t)k∞ = X>(I 一 exp(_-XX>力(XX>)-1 Xy
∣	n	∣∞
≤∣∣vΣ>U>uΣU>UΣ-2U>UΣV>∣∣ kyk∞
≤∣∣v ∑>∑∑-2∑V >L(kX >βk∞ + kεk∞)
≤ kVk∞ ∣∣∑>∑∑-2∑L kV>k∞ (kX>βk∞ + kεk∞) ≤) OP(polylogd), (157)
where (i) follows from the concentration of the Gaussian maxima, and the fact that the law of V is the
Haar measure on SO(n), and thus for any unit vector z independent to V, Vz is uniform on sphere
and k V z∣∞ = O (log d∕√d). We wherefore have
kVk∞ =Sup白=Ol等)肆=O(logd),	(158)
z kzk∞	d kzk∞
Note that this result also implies that ky 一 X>β(t)k∞ = OP (poly log d).
□
36
Published as a conference paper at ICLR 2020
Lemma 18. For weight matrices W,W0 satisfying ∣∣y 一 f(X)|卜=O(√n), where f(X)=
φ(XW)a Withfixed a% 〜Unif{ —1∕√h, 1∕√h}, given (A1)-(A3), the gradientof the empirical risk
defined in (11) is Lipschitz w.r.t. W in the Frobenius norm, i.e.
IdL∂WW2 - dL∂W吧 L ≤ L ∣W - W0∣F .	(159)
F
Proof. Denote y1 = φ(X>W1)a and y2 = φ(X>W2)a for W1,W2 satisfying the assumption
above (which can be seen as a condition on the magnitude of training loss), we have
II∂L(W1)	∂L(W2) II
∂Wl	∂W2	F
=nX [(y 一 yι)a> ◦ φ0(X>Wι)] 一 ɪX [(y - y?)a> ◦ φ0(X>W2)]
≤》∣X∣2 ∣(y - yι)a> ◦ Φ0(X>Wι) - (y - y?)a> ◦ Φ0(X>W2)∣F
≤O (√d) I(y2 - yι)a> ◦ Φ0(X>Wι)∣∣F
+ O (√d) II(y - y2)a> ◦ (φ0(X>W1) - φ0(X>W2))∣∣F .	(16O)
We upper bound the two terms separately:
(i)
II。- yι)a> ◦ φ0(X>Wι)∣∣2 ≤ maχ{φ0(X>Wι)j}版-yι∣2 I∣ak2
(ii)
≤ O(1) ∣φ0(X>W1)a-φ0(X>W2)a∣F
(≤)O(1) ∣X∣2 ∣Wι - W2∣f = O(√d) ∣Wι - W2∣f ,	(161)
where we applied the inequality ∣A ◦ B ∣F ≤ max{lAij'|} kBkF in (i), boundedness of φ0 in (ii) and
Lipschitzity of φ in (iii). Similarly, for the second term
∣∣(y - y2)a> ◦ (φ0(X>W1) - φ0(X>W2))∣∣F
≤max{∣ai∣} ky - y2k2 ∣∣φ0(X>Wι) - φ0(X>W2)∣∣f
(i)
≤O(1) ∣X∣2 kWι - W2∣f = O(√d) kWι - W2∣f ,	(162)
where we used the assumption on the training loss and the Lipscthizity of φ0 in (i). Combining the
two terms yields the desired result.
□
Lemma 19. Under assumptions (A1-3) and the non-vanishing initialization, given that ∣wi (t) -
wi(0)∣2 = O(d-1/2) for all i, then we have ∣K (t) - K (0)∣2 = O(d1/2-0 ) for some positive
0 ∈ Θ(1).
Proof. Recall the definition of the NTK:
Kij ⑴="(；：”)"(Xjiω" = x>xj 1 X φ0(Wk (t)τ xi)φ (Wk (t)>xj ),	(163)
∂ω(t)	∂ω(t)	h
or equivalently the matrix form
K(t) = XτX ◦ 1[φ0(XτW(t))φ0(W(t)τX)].	(164)
h
At initialization, Xi 〜 N(0, Id) and Wk(0) 〜 N(0, deId). ThUs for fixed Xi, by Gaussian
anti-concentration We have Pr ∣x>Wk| < log d ≤ O(1/d1/2+e1) for some e1 > 0. In addi-
tion, note that ∣Wk (t) - Wk (0)∣2 = O(d-1/2) for all k, and therefore for i, j, k such that
37
Published as a conference paper at ICLR 2020
∣x>wk (0)| > O (log d) and ∣x>wk (0)| > O (log d), we know that ∣φ0(x>wk (t))φ∖x> Wk (t))-
φ0(x>Wk (0))φ0 (x>Wk (0))| = O(d-2).
Given fixed xi, define yk = 1{|xi>wk | < log d} as the indicator variable that the k-th neuron does
not saturate. We know that E[yk] = O(1/d1/2+1 ), and Var[yk] = E[yk2] - E[yk]2 = O(1/d1/2+1 ).
By Bernstein’s inequality
1 h	hε2
Pr h X yk - E[yk] >ε ≤ 2exp (- 2σ2 + 2ε∕3 1	(165)
Setting ε = Jh⅞⅝h, we know that with probability at least 1 - h-c,
h X yk ≤ ε + E[yk] = O (phwg3h) .	(166)
k=1
Therefore, given xi and xj, for large enough c1 with probability at least 1 - h-3 we have
hh
X φ0(wk(t)>xi)φ0(wk(t)>xj) - X φ0(wk(0)>xi)φ0(wk(0)>xj) = O(h1/2-4),	(167)
k=1	k=1
in which we utilized the boundedness of φ0 . Taking union bound over d2 elements in the random
feature matrix yields
kK(t) - K(0)k2
=X>X ◦ 1 [φ0(X>W(t))φ0(W(t)>X) - φ0(X>W(0))φ0(W(0)>X)]
≤h ∣∣XτX∣∣2 max {∣Φ0(X>W(t))Φ0(W(t)>X) - Φ0(X>W(0))Φ0(W(0)>X)∣ij}
≤ 1 O(d)O(d1∕2-ej = O(d1∕2-ej.	(168)
h
Using the exact same argument, one can derive that ∣∣unn(X) - UNTK(X)k2 = O(d1/2-e0), the
proof of which we omit.
□
E Additional Results
E.1 RISK OF ReLU NETWORK UNDER SYMMETRIC DATA
If the dataset is symmetric, that is
(A5) Symmetric Data: ∀i ∈ [1, n], ∃!j ∈ [1, n] s.t. xi + xj = 0,
then population risk of the gradient flow solution can be given explicitly for certain nonlinearities:
Proposition 20. Given (A1-3)(A5), if the nonlinearity satisfies φ0(x) + φ0(-x) = C for constant C,
then as n, d, h → ∞
R(γι<0.5)(Z) → 1 -Y1γισ2;	R(γι≥0.5)(Z) = (1 -4)r2 + 271 - 1 σ2.	(169)
Note that the requirement on the nonlinearity holds for ReLU and SoftPlus. This expression is
again independent to γ2 and aligns with the experimental results in Figure 9 (we only plot the bias
component for verification). In addition, the bias is upper-bounded by the null risk for all γ1 . We
remark that the symmetry assumption does not hold for i.i.d. samples from symmetric distributions,
and Figure 9 demonstrates that the additional condition alters the risk.
38
Published as a conference paper at ICLR 2020
Figure 9: Bias of two-layer ReLU networks with optimized first layer under Gaussian data and linear teacher.
Individual dotted lines correspond to different γ2 (from 0.2 to 2) which is independent to the risk. (a) Vanishing
initialization. The bias under symmetric data is predicted by Proposition 20. (b) Non-vanishing initialization.
The red and blue lines represent models optimized from i.i.d. and symmetric initialization, respectively. The
bias for symmetric initialization is predicted by Theorem 8.
Proof. Without loss of generality assume X = [X0, -X0]. Then by (100) we have
∂w	1 2n0
+= = ʒ— X [(yi - hoφ(w>Xi) + hoφ(w>Xi))φ0(w>Xi)xJ ,	(170)
∂t	2n0 i=1
and the flow for w- follows from symmetry. In this case one can show that from exact zero
initialization, for nonlinearity satisfying φ(x) - φ(-x) = x, such as ReLU and SoftPlus,
"w+) + "w-) = ɪ- ^iX h(yi — hoφ(w>Xi) + hoφ(w>Xi)[ (φ0(w>Xi) 一 φ0(w>Xi))Xii = 0.
∂t	∂t 2no 乙 Lvi 0W' +	5 - i	+ =八—i)) i
i=1
(171)
And therefore the gradient flow of w+ is
i - h0φ(w+> Xi) + h0φ(-w+>Xi) φ0 (w>+Xi)Xi
n0
2— X [(yi - hφ(w>Xi) + hoφ(-w>Xi)) (φ0(w>Xi) + φ0(-w>Xi))xJ
0 i=1
-h0w>xi)xii = 2⅛X0y0 - 2⅛h0X0Xo w+.	(172)
The flow of w- follows from symmetry. Solving for the stationary points (i.e. gradient becomes
zero), it the clear that
f[(XX>)-1Xy, Y1 < 0.5,
I ho
w(+t=∞) = -w(-t=∞) =	(173)
I ɪX(X>X)-1y, Yi > 0.5.
h0
And hence the asymptotic risk is
R(Y1<°⑸ → 1 -Y1γ1 σ2; R(γι≥0⑸=1--圭)r2 + 2γ11— 1 σ2.	(174)
The same conclusion holds for vanishing initialization if we assume that the trajectory stays close to
that of exact zero initialization. Note that although the prediction aligns well with the experimental
results, the argument in Theorem 7 does not directly apply due to the undefined derivative of ReLU
at the origin, and thus this result is not rigorously justified.	□
39
Published as a conference paper at ICLR 2020
F Experiment Setup
Optimizing the Second Layer. We compute the minimum-norm solution by directly solving
the pseudo-inverse. We set n = 1000 and vary γι, γ2 from 0.1 to 3. The linear teacher model
F(x) = x>β is fixed as β = -1d∕√d. For each (γ1,γ2) We average across 50 random draws of
data.
Optimizing the First Layer. For both initializations, we use gradient descent with small step size
(η = 0.1) and train the model for minimally 25000 steps and till kVwf(X, W)kF < 10-6. We fix
n = 320 and vary γι, γ2 from 0.1to3 with the same linear teacher model β = -1d∕√d. The risk is
averaged across 20 models trained from different initializations.
40