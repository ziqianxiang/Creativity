Published as a conference paper at ICLR 2020
Training individually fair ML models with
Sensitive Subspace Robustness
Mikhail Yurochkin	Amanda Bower*, Yuekai Sun^
IBM Research	Department of Mathematics *
MIT-IBM Watson AI Lab	Department of Statistics^
mikhail.yurochkin@ibm.com	University of Michigan
{amandarg,yuekai}@umich.edu
Ab stract
We consider training machine learning models that are fair in the sense that their
performance is invariant under certain sensitive perturbations to the inputs. For
example, the performance of a resume screening system should be invariant un-
der changes to the gender and/or ethnicity of the applicant. We formalize this
notion of algorithmic fairness as a variant of individual fairness and develop a dis-
tributionally robust optimization approach to enforce it during training. We also
demonstrate the effectiveness of the approach on two ML tasks that are susceptible
to gender and racial biases.
1	Introduction
Machine learning (ML) models are gradually replacing humans in high-stakes decision making
roles. For example, in Philadelphia, an ML model classifies probationers as high or low-risk (Metz
& Satariano, 2020). In North Carolina, “analytics” is used to report suspicious activity and fraud by
Medicaid patients and providers (Metz & Satariano, 2020). Although ML models appear to elim-
inate the biases of a human decision maker, they may perpetuate or even exacerbate biases in the
training data (Barocas & Selbst, 2016). Such biases are especially objectionable when it adversely
affects underprivileged groups of users (Barocas & Selbst, 2016).
In response, the scientific community has proposed many mathematical definitions of algorithmic
fairness and approaches to ensure ML models satisfy the definitions. Unfortunately, this abundance
of definitions, many of which are incompatible (Kleinberg et al., 2016; Chouldechova, 2017), has
hindered the adoption of this work by practitioners. There are two types of formal definitions of
algorithmic fairness: group fairness and individual fairness. Most recent work on algorithmic fair-
ness considers group fairness because it is more amenable to statistical analysis (Ritov et al., 2017).
Despite their prevalence, group notions of algorithmic fairness suffer from certain shortcomings.
One of the most troubling is there are many scenarios in which an algorithm satisfies group fairness,
but its output is blatantly unfair from the point of view of individual users (Dwork et al., 2011).
In this paper, we consider individual fairness instead of group fairness. Intuitively, an individually
fair ML model treats similar users similarly. Formally, an ML model is a map h : X → Y, where X
and Y are the input and output spaces. The leading notion of individual fairness is metric fairness
(Dwork et al., 2011); it requires
dy (h(x1), h(x2)) ≤ Ldx(x1, x2) for all x1, x2 ∈ X ,	(1.1)
where dx and dy are metrics on the input and output spaces and L ≥ 0 is a Lipschitz constant.
The fair metric dx encodes our intuition of which samples should be treated similarly by the ML
model. We emphasize that dx (x1, x2) being small does not imply x1 and x2 are similar in all
respects. Even if dx (x1 , x2) is small, x1 and x2 may differ in certain problematic ways, e.g. in
their protected/sensitive attributes. This is why we refer to pairs of samples x1 and x2 such that
dx(x1, x2) is small as comparable instead of similar.
Despite its benefits, individual fairness was dismissed as impractical because there is no widely
accepted fair metric for many ML tasks. Fortunately, there is a line of recent work on learning the
1
Published as a conference paper at ICLR 2020
fair metric from data (Ilvento, 2019; Wang et al., 2019). In this paper, we consider two data-driven
choices of the fair metric: one for problems in which the sensitive attribute is reliably observed, and
another for problems in which the sensitive attribute is unobserved (see Appendix B).
The rest of this paper is organized as follows. In Section 2, we cast individual fairness as a form of
robustness: robustness to certain sensitive perturbations to the inputs ofan ML model. This allows us
to leverage recent advances in adversarial ML to train individually fair ML models. More concretely,
we develop an approach to audit ML models for violations of individual fairness that is similar to
adversarial attacks (Goodfellow et al., 2014) and an approach to train ML models that passes such
audits (akin to adversarial training (Madry et al., 2017)). We justify the approach theoretically (see
Section 3) and empirically (see Section 4).
2	Fairnes s through (distributional) robustness
To motivate our approach, imagine an auditor investigating an ML model for unfairness. The auditor
collects a set of audit data and compares the output of the ML model on comparable samples in
the audit data. For example, to investigate whether a resume screening system is fair, the auditor
may collect a stack of resumes and change the names on the resumes of Caucasian applicants to
names more common among the African-American population. If the system performs worse on
the edited resumes, then the auditor may conclude the model treats African-American applicants
unfairly. Such investigations are known as correspondence studies, and a prominent example is
Bertrand & Mullainathan’s celebrated investigation of racial discrimination in the labor market. In a
correspondence study, the investigator looks for inputs that are comparable to the training examples
(the edited resumes in the resume screening example) on which the ML model performs poorly. In
the rest of this section, we formulate an optimization problem to find such inputs.
2.1	Fair Wasserstein distances
Recall X and Y are the spaces of inputs and outputs. To keep things simple, we assume that the ML
task at hand is a classification task, so Y is discrete. We also assume that we have a fair metric dx
of the form
dχ(xi,X2)2，hxi - X2, ∑(xi — X2)i 2,
where Σ ∈ Sd+×d. For example, suppose we are given a set of K “sensitive” directions that we wish
the metric to ignore; i.e. d(x1, x2) 1 for any x1 and x2 such that x1 - x2 falls in the span of the
sensitive directions. These directions may be provided by a domain expert or learned from data (see
Section 4 and Appendix B). In this case, we may choose Σ as the orthogonal complement projector
of the span of the sensitive directions. We equip X with the fair metric and Z , X × Y with
dz((χι,yι), (χ2,y2))，dχ(χ1,χ2) + ∞ ∙ i{yι = y2}.
We consider dz2 as a transport cost function on Z. This cost function encodes our intuition of which
samples are comparable for the ML task at hand. We equip the space of probability distributions on
Z with the fair Wasserstein distance
W(P,Q) = infΠ∈C(P,Q) RZ×Zc(z1,z2)dΠ(z1,z2),
where C (P, Q) is the set of couplings between P and Q. The fair Wasserstein distance inherits
our intuition of which samples are comparable through the cost function; i.e. the fair Wasserstein
distance between two probability distributions is small if they are supported on comparable areas of
the sample space.
2.2	Auditing ML models for algorithmic bias
To investigate whether an ML model performs disparately on comparable samples, the auditor col-
lects a set of audit data {(xi, yi)}in=1 and solves the optimization problem
maxP:W (P,Pn)≤ Z `(z, h)dP (z),	(2.1)
where ` : Z × H → R+ is a loss function, h is the ML model, Pn is the empirical distribution of the
audit data, and > 0 is a small tolerance parameter. We interpret as a moving budget that the au-
ditor may expend to discover discrepancies in the performance of the ML model. This budget forces
2
Published as a conference paper at ICLR 2020
the auditor to avoid moving samples to incomparable areas of the sample space. We emphasize
that equation 2.1 detects aggregate violations of individual fairness. In other words, although the
violations that the auditor’s problem detects are individual in nature, the auditor’s problem is only
able to detect aggregate violations. We summarize the implicit notion of fairness in equation 2.1 in
a definition.
Definition 2.1 (distributionally robustly fair (DRF)). An ML model h : X → Y is (, δ)-
distributionally robustly fair (DRF) WRT the fair metric dx iff
maxP:W (P,Pn)≤ Z `(z, h)dP (z) ≤ δ.
(2.2)
Although equation 2.1 is an infinite-dimensional optimization problem, it is possible to solve it
exactly by appealing to duality. Blanchet & Murthy showed that the dual of equation 2.1 is
SUPp:w(P,Pn)≤ EP ['(Z, h) = infλ≥0{λe + Ep“ ['λ(Z, ⅛)] },
'λ((xi, yi), h) , supx∈X '((x, yi), θ) - λdx(X, Xi).
(2.3)
This is a univariate optimization problem, and it is amenable to stochastic optimization. We describe
a stochastic approximation algorithm for equation 2.3 in Algorithm 1. Inspecting the algorithm, we
see that it is similar to the PGD algorithm for adversarial attack.
Algorithm 1 stochastic gradient method for equation 2.3
Require: starting point λ1, step sizes αt > 0
1: repeat
2：	draw mini-batch (χtι,yQ,..., " ,ytB)〜Pn
3：	Xjb — argmaXχ∈χ'((x,ytb), h) - λdχ(xtb,x), b ∈ [B]
4:	λt+1 J max{0, "t - αt(E - B1 Pb=I dx(Xtb, xJb))}
5： until converged
It is known that the optimal point of equation 2.1 is the discrete measure ɪ Pn=I 6(7入(方稔),yi), where
Tλ : X → X is the unfair map
Tλ(xi) J argmaXχ∈χ'((x,yi), h) - λdx(x, Xi)
(2.4)
We call Tλ an unfair map because it reveals unfairness in the ML model by mapping samples in the
audit data to comparable areas of the sample space that the system performs poorly on. We note
that Tλ may map samples in the audit data to areas of the sample space that are not represented in
the audit data, thereby revealing disparate treatment in the ML model not visible in the audit data
alone. We emphasize that Tλ more than reveals disparate treatment in the ML model; it localizes the
unfairness to certain areas of the sample space.
We present a simple example to illustrating fairness through robustness (a similar example appeared
in Hashimoto et al. (2018)). Consider the binary classification dataset shown in Figure 1. There are
two subgroups of observations in this dataset, and (sub)group membership is the protected attribute
(e.g. the smaller group contains observations from a minority subgroup). In Figure 1a we see the
decision heatmap of a vanilla logistic regression, which performs poorly on the blue minority sub-
group. The two subgroups are separated in the horizontal direction, so the horizontal direction is the
sensitive direction. Figure 1b shows that such classifier is unfair with respect to the corresponding
fair metric, i.e. the unfair map equation 2.4 leads to significant loss increase by transporting mass
along the horizontal direction with very minor change of the vertical coordinate.
Comparison with metric fairness Before moving on to training individually fair ML models,
we compare DRF with metric fairness equation 1.1. Although we concentrate on the differences
between the two definitions here, they are more similar than different： both formalize the intuition
that the outputs of a fair ML model should perform similarly on comparable inputs. That said, there
are two main differences between the two definitions. First, instead of requiring the output of the
ML model to be similar on all inputs comparable to a training example, we require the output to
be similar to the training label. Thus DRF not only enforces similarity of the output on comparable
inputs, but also accuracy of the ML model on the training data. Second, DRF considers differences
3
Published as a conference paper at ICLR 2020
(a) unfair classifier
(C) classifier from SenSR
Figure 1: Figure (a) depicts a binary classification dataset in which the minority group shown on the
right of the plot is underrepresented. This tilts the logistic regression decision boundary in favor of
the majority group on the left. Figure (b) shows the unfair map of the logistic regression decision
boundary. It maps samples in the minority group towards the majority group. Figure (c) shows an
algorithmically fair classifier that treats the majority and minority groups identically.
between datasets instead of samples by replacing the fair metric on inputs with the fair Wasserstein
distance induced by the fair metric. The main benefits of this modifications are (i) it is possible
to optimize equation 2.1 efficiently, (ii) we can show this modified notion of individual fairness
generalizes.
2.3 Fair training with Sensitive Subspace Robustness
We cast the fair training problem as training supervised learning systems that are robust to sensitive
perturbations. We propose solving the minimax problem
inf sup	EP ['(Z, h)]
h∈H P:W (P,Pn)≤
hinHλ≥0 λe + EPn['λ(Z, h)],
(2.5)
where 'λ is defined in equation 2.3. This is an instance of a distribUtiOnally robust optimization
(DRO) problem, and it inherits some of the statistical properties of DRO. To see why equation 2.5
encourages individual fairness, recall the loss function is a measure of the performance of the ML
model. By assessing the performance of an ML model by its worse-case performance on hypothet-
ical populations of users with perturbed sensitive attributes, minimizing equation 2.5 ensures the
system performs well on all such populations. In our toy example, minimizing equation 2.5 implies
learning a classifier that is insensitive to perturbations along the horizontal (i.e. sensitive) direction.
In Figure 1c this is achieved by the algorithm we describe next.
To keep things simple, we assume the hypothesis class is parametrized by θ ∈ Θ ⊂ Rd and replace
the minimization with respect to H by minimization with respect to θ. In light of the similarities
between the DRO objective function and adversarial training, we borrow algorithms for adversarial
training (Madry et al., 2017) to solve equation 2.5 (see Algorithm 2).
Algorithm 2 Sensitive Subspace Robustness (SenSR)
Require: starting point θ∖, step sizes ɑt,βt > 0
1:	repeat
2:	sample mini-batch (xι ,yι),..., (XB ,yB)〜Pn
3:	XJb — argmaxχ∈χ'((x,ytb),θ) - λtdχ",x), b ∈ [B]
4:	λt+ι J maχ{0, λ - αt(e - B1 PB=I dx(Xtb, χJb))}
5:	θt+ι J Gt- βB PB=I dθ'((Xtb, ytb), θt)
6:	until converged
Related work Our approach to fair training is an instance of distributionally robust optimization
(DRO). In DRO, the usual sample-average approximation of the expected cost function is replaced
by LDRO(θ)，SuPP∈u EP ['(Z, θ)J, where U is a (data dependent) uncertainty set of probability
distributions. The uncertainty set may be defined by moment or support constraints (Chen et al.,
2007; Delage & Ye, 2010; Goh & Sim, 2010), f -divergences (Ben-Tal et al., 2012; Lam & Zhou,
4
Published as a conference paper at ICLR 2020
2015; Miyato et al., 2015; Namkoong & Duchi, 2016), and Wasserstein distances (Shafieezadeh-
Abadeh et al., 2015; Blanchet et al., 2016; Esfahani & Kuhn, 2015; Lee & Raginsky, 2017; Sinha
et al., 2017). Most similar to our work is Hashimoto et al. (2018): they show that DRO with a
χ2-neighborhood of the training data prevents representation disparity, i.e. minority groups tend
to suffer higher losses because the training algorithm ignores them. One advantage of picking a
Wasserstein uncertainty set is the set depends on the geometry of the sample space. This allows
us to encode the correct notion of individual fairness for the ML task at hand in the Wasserstein
distance.
Our approach to fair training is also similar to adversarial training (Madry et al., 2017), which
hardens ML models against adversarial attacks by minimizing adversarial losses of the form
supu∈U `(z + u, θ), where U is a set of allowable perturbations (Szegedy et al., 2013; Goodfel-
low et al., 2014; Papernot et al., 2015; Carlini & Wagner, 2016; Kurakin et al., 2016). Typically, U
is a scaled `p-norm ball: U = {u : kukp ≤ }. Most similar to our work is Sinha et al. (2017): they
consider an uncertainty set that is a Wasserstein neighborhood of the training data.
There are a few papers that consider adversarial approaches to algorithmic fairness. Zhang et al.
(2018) propose an adversarial learning method that enforces equalized odds in which the adversary
learns to predict the protected attribute from the output of the classifier. Edwards & Storkey (2015)
propose an adversarial method for learning classifiers that satisfy demographic parity. Madras et al.
(2018) generalize their method to learn classifiers that satisfy other (group) notions of algorithmic
fairness. Garg et al. (2019) propose to use adversarial logit pairing (Kannan et al., 2018) to achieve
fairness in text classification using a pre-specified list of counterfactual tokens.
3	SenSR trains individually fair ML models
One of the main benefits of our approach is it provably trains individually fair ML models. Further,
it is possible for the learner to certify that an ML model is individually fair a posteriori. As we shall
see, both are consequences of uniform convergence results for the DR loss class. More concretely,
we study how quickly the uniform convergence error
δn , supθ∈Θ {卜UPPW*(P,Pn)≤e EP ['(Z, θV - supP:W(P,PQ≤e EP ['(Z, θ)] | 1 ,	3I)
where W* is the Wasserstein distance on ∆(Z) with a transportation cost function c* that is possibly
different from c, vanishes. We permit some discrepancy in the (transportation) cost function to
study the effect of a data-driven choice of c. In the rest of this section, we regard c* as the exact
cost function and c as a cost function learned from human supervision. We start by stating our
assumptions on the ML task:
(A1) the feature space X is bounded: D , max{diam(X), diam* (X)} < ∞;
(A2) the functions in the loss class L = {'(∙,θ) : θ ∈ Θ} are non-negative and bounded:
0 ≤ `(z, θ) ≤ M for all z ∈ Z and θ ∈ Θ, and L-Lipschitz with respect to dx:
suPθ∈Θ{suP(xι,y),(x2,y)∈Z |'((x1, 夕),θ) - '((x2 , y), θ) |} ≤ Ldx(X1,x2)；
(A3) the discrepancy in the (transportation) cost function is uniformly bounded:
sup(x1,y),(x2,y)∈Z |c((x1, y), (x2, y)) - c*((x1, y), (x2, y))| ≤ δcD2.
Assumptions A1 and A2 are standard (see (Lee & Raginsky, 2017, Assumption 1, 2, 3)) in the DRO
literature. We emphasize that the constant L in Assumption A2 is not the constant L in the definition
of metric fairness; it may be much larger. Thus most models that satisfy the conditions of the loss
class are not individually fair in a meaningful sense.
Assumption A3 deserves further comment. Under A1, A3 is mild. For example, if the exact fair
metric is
dx(xi,X2) = (xi - X2)TΣ*(xi - X2)2 ,
then the error in the transportation cost function is at most
|c((x1, y), (x2, y)) - c*((x1, y), (x2,y))|
= |(x1 - x2)TΣ(x1 - x2) - (x1 - x2)TΣ* (x1 - x2)|
≤ d2 Uς-ς* k2
一	λmin(Σ* ),
5
Published as a conference paper at ICLR 2020
We see that the error in the transportation cost function vanishes in the large-sample limit as long as
Σ is a consistent estimator of Σ*.
We state the uniform convergence result in terms of the entropy integral of the loss class: C(L) =
f∞，log N∞(F, r)dr, where N∞(L, r) as the r-covering number of the loss class in the uniform
metric. The entropy integral is a measure of the complexity of the loss class.
Proposition 3.1 (uniform convergence).
Under Assumptions A1-A3, equation 3.1 satisfies
δn ≤
48C(L)
√n
48LD2
√ne
LδcD2	71 “Jog 2、1
+F+M"中
(3.2)
with probability at least 1 - t.
We note that Proposition 3.1 is similar to the generalization error bounds by Lee & Raginsky (2017).
The main novelty in Proposition 3.1 is allowing error in the transportation cost function. We see
that the discrepancy in the transportation cost function may affect the rate at which the uniform
convergence error vanishes: it affects the rate if δc is ωp(√n).
A consequence of uniform convergence is SenSR trains individually fair classifiers (if there are such
classifiers in the hypothesis class). By individually fair ML model, we mean an ML model that has
a small gap
SupPW*(p,p*)≤e Ep['(Z,θ)] - Ep*['(Z,θ)],	(3.3)
The gap is the difference between the optimal value of the auditor’s optimization problem equa-
tion 2.1 and the (non-robust) risk. A small gap implies the auditor cannot significantly increase the
loss by moving samples from P* to comparable samples.
Proposition 3.2. Under the assumptions A1-A3, as long as there is θ ∈ Θ such that
suppw*(p,p*)≤e Ep ['(Z, θ)] ≤ δ*	(3.4)
for some δ* > 0, θ ∈ argmi□θ∈θ SupPW(p,pn)≤e EP ['(Z, h)] satisfies
suppw*(p,p*)≤eEp ['(Z,θ)] - Ep*['(Z,θ)] ≤ δ* + 2δn,
where δn is the uniform convergence error equation 3.1.
Proposition 3.2 guarantees Algorithm 2 trains an individually fair ML model. More precisely, if
there are models in H that are (i) individually fair and (ii) achieve small test error, then Algorithm
2 trains such a model. It is possible to replace equation 3.4 with other conditions, but a condition to
its effect cannot be dispensed with entirely. If there are no individually fair models in H, then it is
not possible for equation 2.5 to learn an individually fair model. If there are individually fair models
in H, but they all perform poorly, then the goal of learning an individually fair model is futile.
Another consequence of uniform convergence is equation 3.3 is close to its empirical counterpart
SupP：W(P,Pn)≤ EP ['(Z, θ)] - EPn ['(Z,θ)].	(3.5)
In other words, the gap generalizes. This implies equation 3.5 is a certificate of individual fairness;
i.e. it is possible for practitioners to check whether an ML model is individually fair by evaluating
equation 3.5.
Proposition 3.3. Under the assumptions A1-A3, for any e > 0,
supθ∈Θ {supPW(P,Pn)≤e EP ['(Z, θ)] - EPn ['(Z, θV -(SupPW(P,P*)≤e EP ['(Z, θ)] - EP* ['(Z,如)}
≤ 2δn w.p. at least 1 - t.
4	Computational results
In this section, we present results from using SenSR to train individually fair ML models for two
tasks: sentiment analysis and income prediction. We pick these two tasks to demonstrate the efficacy
of SenSR on problems with structured (income prediction) and unstructured (sentiment analysis) in-
puts and in which the sensitive attribute (income prediction) is observed and unobserved (sentiment
analysis). We refer to Appendix C and D for the implementation details.
6
Published as a conference paper at ICLR 2020
Table 1: Sentiment prediction experiments over 10 restarts
	Acc.,%	Race gap	Gend. gap ∣ Cuis. gap	
SenSR	94±1	0.30±.05	0.19±.03	0.23±.05
SenSR-E	93±1	0.11±.04	0.04±.03	1.11±.15
Baseline	95±1	7.01±.44	5.59±.37	4.10±.44
Project	94±1	1.00±.56	1.99±.58	1.70±.41
Sinha+	94±1	3.88±.26	1.42±.29	1.33±.18
Bolukb.+	94±1	6.85±.53	4.33±.46	3.44±.29
Figure 2: Box-plots of sentiment scores
4.1	Fair sentiment prediction with word embeddings
Problem formulation We study the problem of classifying the sentiment of words using positive
(e.g. ‘smart’) and negative (e.g. ‘anxiety’) words compiled by Hu & Liu (2004). We embed words
using 300-dimensional GloVe (Pennington et al., 2014) and train a one layer neural network with
1000 hidden units. Such classifier achieves 95% test accuracy, however it entails major individual
fairness violation. Consider an application of this sentiment classifier to summarizing customer re-
views, tweets or news articles. Human names are typical in such texts and should not affect the
sentiment score, hence we consider fair metric between any pair of names to be 0. Then sentiment
score for all names should be the same to satisfy the individual fairness. To make a connection to
group fairness, following the study of Caliskan et al. (2017) that reveals the biases in word embed-
dings, we evaluate the fairness of our sentiment classifier using male and female names typical for
Caucasian and African-American ethnic groups. We emphasize that to satisfy individual fairness,
the sentiment of any name should be the same.
Comparison metrics To evaluate the gap between two groups of names, N0 for Caucasian
(or female) and N1 for African-American (or male), We report ∣n^j Pn∈N (h(n)ι - h(n)o)-
PNJ Pn∈N1 (h(n)ι - h(n)o), where h(n)k is logits for class k of name n (k = 1 is the positive
class). We use list of names provided in Caliskan et al. (2017), Which consists of 49 Caucasian
and 45 African-American names, among those 48 are female and 46 are male. The gap between
African-American and Caucasian names is reported as Race gap, while the gap between male and
female names is reported as Gend. gap in Table 1. As in Speer (2017), we also compare sentiment
difference of two sentences: “Let’s go get Italian food” and “Let’s go get Mexican food”, i.e. cuisine
gap (abbreviated Cuis. gap in Table 1), as a test of generalization beyond names. To embed these
sentences we average their word embeddings.
Sensitive subspace We consider embeddings of 94 names that we use for evaluation as sensitive
directions, which may be regarded as utilizing the expert knowledge, i.e. these names form a list
of words that an expert believes should be treated equally. Fair metric is then defined using an
orthogonal complement projector of the span of sensitive directions as we discussed in Section 2.1.
When expert knowledge is not available, or we wish to achieve general fairness for names, we utilize
a side dataset of popular baby names in New York City.1 The dataset has 11k names, however only
32 overlap with the list of names used for evaluation. Embeddings of these names define a group
of comparable samples that we use to learn sensitive directions with SVD (see Appendix B.2 and
Algorithm 3 for details). We take top 50 singular vectors to form the sensitive subspace. It is worth
noting that, unlike many existing approaches in the fairness literature, we do not use any protected
attribute information. Our algorithm only utilizes training words, their sentiments and a vanilla list
of names.
Results From the box-plots in Figure 2, we see that both race and gender gaps are significant
when using the baseline neural network classifier. It tends to predict Caucasian names as “positive”,
while the median for African-American names is negative; the median sentiment for female names
is higher than that for male names. We considered three other approaches to this problem: the algo-
rithm of Bolukbasi et al. (2016) for pre-processing word embeddings; pre-processing via projecting
1titled “Popular Baby Names” and available from https://catalog.data.gov/dataset/
7
Published as a conference paper at ICLR 2020
Table 2: Summary of Adult classification experiments over 10 restarts
	B-Acc,%	S-Con.	GR-Con.	GapRGMS	GapRRMS	GapGmax	GapRmax
SenSR	78.9	.934	.984	.068	.055	.087	.067
Baseline	82.9	.848	.865	.179	.089	.216	.105
Project	82.7	.868	1.00	.145	.064	.192	.086
Adv. Debias.	81.5	.807	.841	.082	.070	.110	.078
CoCL	79.0	-	-	.163	.080	.201	.109
out the sensitive subspace that we used for training SenSR (this is analogous to Prost et al. (2019));
training a distributionally robust classifier with Euclidean distance cost (Sinha et al., 2017). All
approaches improved upon the baseline, however only SenSR can be considered individually fair.
Our algorithm practically eliminates gender and racial gaps and achieves the notion of individual
fairness as can be seen from almost equal predicted sentiment score for all names. We remark that
using expert knowledge (i.e. evaluation names) allowed SenSR-E (E for expert) to further improve
both group and individual fairness. However we warn practitioners that if the expert knowledge is
too specific, generalization outside of the expert knowledge may not be very good. In Table 1 we
report results averaged across 10 repetitions with 90%/10% train/test splits, where we also verify
that accuracy trade-off with the baseline is minor. In the right column we present the generalization
check, i.e. comparing a pair of sentences unrelated to names. Utilizing expert knowledge led to
a fairness over-fitting effect, however we still see improvement over other methods. When utiliz-
ing SVD of a larger dataset of names we observe better generalization. Our generalization check
suggests that fairness over-fitting is possible, therefore datasets and procedure for verifying fairness
generalization are needed.
4.2	Adult
Problem formulation Demonstrating the broad applicability of SenSR outside of natural language
processing tasks, we apply SenSR to a classification task on the Adult (Dua & Graff, 2017) data set
to predict whether an individual makes at least $50k based on features like gender and occupation
for approximately 45,000 individuals. Models that predict income without fairness considerations
can contribute to the problem of differences in pay between genders or races for the same work.
Throughout this section, gender (male or female) and race (Caucasian or non-Caucasian) are binary.
Comparison metrics Arguably a classifier is individually unfair if the classifications for two data
points that are the same on all features except demographic features are different. Therefore, to
assess individual fairness, we report spouse consistency (S-Con.) and gender and race consistency
(GR-Con.), which are measures of how often classifications change only because of differences in
demographic features. For S-Con (resp. GR-con), we make 2 (resp. 4) copies of every data point
where the only difference is that one is a husband and the other is a wife (resp. difference is in gender
and race). S-Con (resp. GR-Con) is the fraction of corresponding pairs (resp. quadruples) that have
the same classification. We also report various group fairness measures proposed by De-Arteaga
et al. (2019) with respect to race or gender based on true positive rates, i.e. the ability of a classifier
to correctly identify a given class. See Appendix D.5 for the definitions. We report GapRRMS, GapRGMS,
GapmRax, and GapmGax where R refers to race, and G refers to gender. We use balanced accuracy (B-
acc) instead of accuracy2 to measure predictive ability since only 25% of individuals make at least
$50k.
Sensitive subspace Let {(xi, xgi)}im=1 be the set of features xi ∈ RD of the data except the
coordinate for gender is zeroed and where xgi indicates the gender of individual i. For γ > 0,
let Wg = argminw∈RDm1 Pm=I -Xgi(WTXi) + log(1 + ewTxi) + Ykwk2, i.e. Wg is the learned
hyperplane that classifies gender given by regularized logistic regression. Let eg ∈ RD (resp. er)
be the vector that is 1 in the gender (resp. race) coordinate and 0 elsewhere. Then the sensitive
subspace is the span of [Wg, eg, er]. See Appendix B.1 for details.
2Accuracy is reported in Table 4 in Appendix D.
8
Published as a conference paper at ICLR 2020
Results See Table 2 for the average3 of each metric on the test sets over ten 80%/20% train/test
splits for Baseline, Project (projecting features onto the orthogonal complement of the sensitive
subspace before training), CoCL (De-Arteaga et al., 2019), Adversarial Debiasing (Zhang et al.,
2018), and SenSR. With the exception of CoCL (De-Arteaga et al., 2019), each classifier is a 100
unit single hidden layer neural network. The Baseline clearly exhibits individual and group fairness
violations. While SenSR has the lowest B-acc, SenSR is the best by a large margin for S-Con.
and has the best group fairness measures. We expect SenSR to do well on GR-consistency since
the sensitive subspace includes the race and gender directions. However, SenSR’s individually fair
performance generalizes: the sensitive directions do not directly use the husband and wife directions,
yet SenSR performs well on S-Con. Furthermore, SenSR outperforms Project on S-Con and group
fairness measures illustrating that SenSR does much more than just ignoring the sensitive subspace.
CoCL only barely improves group fairness compared to the baseline with a significant drop in B-
acc and while Adversarial Debiasing also improves group fairness, it is worse than the baseline on
individual fairness measures illustrating that group fairness does not imply individual fairness.
5	Summary
We consider the task of training ML systems that are fair in the sense that their performance is
invariant under certain perturbations in a sensitive subspace. This notion of fairness is a variant
of individual fairness (Dwork et al., 2011). One of the main barriers to the adoption of individual
fairness is the lack of consensus on a fair metric for many ML tasks. To circumvent this issue, we
consider two approaches to learning a fair metric from data: one for problems in which the sensitive
attribute is observed, and another for problems in which the sensitive attribute is unobserved. Given
a data-driven choice of fair metric, we provide an algorithm that provably trains individually fair
ML models.
Acknowledgments
This work was supported by the National Science Foundation under grants DMS-1830247 and
DMS-1916271.
References
Solon Barocas and Andrew D. Selbst. Big Data’s Disparate Impact. SSRN Electronic Journal, 2016.
ISSN 1556-5068. doi: 10.2139/ssrn.2477899.
Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya
Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar,
Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Monin-
der Singh, Kush R. Varshney, and Yunfeng Zhang. AI Fairness 360: An extensible toolkit
for detecting, understanding, and mitigating unwanted algorithmic bias, October 2018. URL
https://arxiv.org/abs/1810.01943.
Aharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust Solutions of Optimization Problems Affected by Uncertain Probabilities. Management
Science, 59(2):341-357, November 2012. ISSN 0025-1909. doi:10.1287/mnsc.1120.1641.
Marianne Bertrand and Sendhil Mullainathan. Are Emily and Greg More Employable Than Lakisha
and Jamal? A Field Experiment on Labor Market Discrimination. American Economic Review,
94(4):991-1013, September 2004. ISSN 0002-8282. doi: 10.1257/0002828042002561.
Jose Blanchet and Karthyek R. A. Murthy. Quantifying Distributional Model Risk via Optimal
Transport. arXiv:1604.01446 [math, stat], April 2016.
Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust Wasserstein Profile Inference and Appli-
cations to Machine Learning. arXiv:1610.05627 [math, stat], October 2016.
3The standard error is reported in the supplement. Each standard error is within 10-2.
9
Published as a conference paper at ICLR 2020
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing Systems, pp. 4349-4357, 2016.
Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science, 356(6334):183-186, April 2017. ISSN
0036-8075, 1095-9203. doi: 10.1126/science.aal4230.
Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks.
arXiv:1608.04644 [cs], August 2016.
Xin Chen, Melvyn Sim, and Peng Sun. A Robust Optimization Perspective on Stochastic Program-
ming. Operations Research, 55:1058-1071, 2007. doi: 10.1287/opre.1070.0441.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. arXiv:1703.00056 [cs, stat], February 2017.
Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexan-
dra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in
bios: A case study of semantic representation bias in a high-stakes setting. arXiv preprint
arXiv:1901.09451, 2019.
Erick Delage and Yinyu Ye. Distributionally Robust Optimization Under Moment Uncertainty with
Application to Data-Driven Problems. Operations Research, 58:595-612, 2010. doi: 10.1287/
opre.1090.0741.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. Fairness Through
Awareness. arXiv:1104.3913 [cs], April 2011.
Harrison Edwards and Amos Storkey. Censoring Representations with an Adversary.
arXiv:1511.05897 [cs, stat], November 2015.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven Distributionally Robust Optimization
Using the Wasserstein Metric: Performance Guarantees and Tractable Reformulations. May 2015.
Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H Chi, and Alex Beutel. Counter-
factual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM
Conference on AI, Ethics, and Society, pp. 219-226. ACM, 2019.
Joel Goh and Melvyn Sim. Distributionally Robust Optimization and Its Tractable Approximations.
Operations Research, 58(4-part-1):902-917, August 2010. ISSN 0030-364X, 1526-5463. doi:
10.1287/opre.1090.0795.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. December 2014.
Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness With-
out Demographics in Repeated Loss Minimization. arXiv:1806.08010 [cs, stat], June 2018.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 168-177.
ACM, 2004.
Christina Ilvento. Metric Learning for Individual Fairness. arXiv:1906.00250 [cs, stat], June 2019.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
10
Published as a conference paper at ICLR 2020
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent Trade-Offs in the Fair Deter-
mination of Risk Scores. arXiv:1609.05807 [cs, stat], September 2016.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Machine Learning at Scale.
November 2016.
H. Lam and Enlu Zhou. Quantifying uncertainty in sample average approximation. In 2015 Win-
ter Simulation Conference (WSC), pp. 3846-3857, December 2015. doi: 10.1109/WSC.2015.
7408541.
Jaeho Lee and Maxim Raginsky. Minimax Statistical Learning with Wasserstein Distances.
arXiv:1705.07815 [cs], May 2017.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning Adversarially Fair and
Transferable Representations. arXiv:1802.06309 [cs, stat], February 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv:1706.06083 [cs, stat],
June 2017.
Cade Metz and Adam Satariano. An Algorithm That Grants Freedom, or Takes It Away. The New
York Times, February 2020. ISSN 0362-4331.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
Smoothing with Virtual Adversarial Training. arXiv:1507.00677 [cs, stat], July 2015.
Hongseok Namkoong and John C. Duchi. Stochastic Gradient Methods for Distributionally Robust
Optimization with F-divergences. In Proceedings of the 30th International Conference on Neu-
ral Information Processing Systems, NIPS’16, pp. 2216-2224, Barcelona, Spain, 2016. Curran
Associates Inc. ISBN 978-1-5108-3881-9.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The Limitations of Deep Learning in Adversarial Settings. November 2015.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Flavien Prost, Nithum Thain, and Tolga Bolukbasi. Debiasing embeddings for reduced gender bias
in text classification. arXiv preprint arXiv:1908.02810, 2019.
Ya’acov Ritov, Yuekai Sun, and Ruofei Zhao. On conditional parity as a notion of non-
discrimination in machine learning. arXiv:1706.08519 [cs, stat], June 2017.
Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally
Robust Logistic Regression. September 2015.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying Some Distributional Robustness
with Principled Adversarial Training. arXiv:1710.10571 [cs, stat], October 2017.
Robyn Speer. How to make a racist ai without really trying, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. December 2013.
Hanchen Wang, Nina Grgic-Hlaca, Preethi Lahoti, Krishna P. Gummadi, and Adrian Weller. An
Empirical Study on Learning Fairness Metrics for COMPAS Data with Human Supervision.
arXiv:1910.10255 [cs], October 2019.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating Unwanted Biases with Adver-
sarial Learning. arXiv:1801.07593 [cs], January 2018.
11
Published as a conference paper at ICLR 2020
A	Proofs
A.1 Proof of Proposition 3.1
By the duality result of Blanchet & Murthy (2016), for any e > 0,
sup	Ep ['(Z, θ)] — sup	Ep ['(Z, θ)]
PW*(P,P*)≤e	P :W (P,Pn)≤e
=inf {λe + Ep* [彼(Z, θ)]} — λne + Ep” [% (Z, θ)]
λ ≥ U
≤ Ep*['* (Z,θ)] — Epn['l (Z,θ)],
where λn ∈ arg minλ>uλe + EPn ['λ(Z, θ)]. By assumption A3,
阳；(Z ⑼-'λn (z,θ)∣
= sup '((X2,y),θ) — λnC*((x,y), (X2,y)) — SUP '((X2,y),θ) — λnc((x,y), (X2,y))
X2∈X	X2∈X
≤ suP λn∣c*((x,y), (x2,y)) — C((X,y), (x2,y))∣
X2∈X
≤ λnδc ∙ D2 .
This implies
sup	Ep ['(Z, θ)] — sup	Ep ['(Z, θ)]
P=W*(P,P*)≤e	P :W (P,Pn)≤e
≤ Ep* ['λn(z,θ)] — Epn 依：(z,θ)] + λnδcD2.
This bound is crude; it is possible to obtain sharper bounds under additional assumptions on the loss
and transportation cost functions. We avoid this here to keep the result as general as possible.
similarly,
sup	Ep ['(Z,θ)] — sup	Ep ['(Z,θ)]
P: W (P,Pn)≤e	P=W*(P,P*)≤e
≤ Epn['λ*(Z,θ)] — Ep*[,λ**(Z,θ)]
≤ EPn ['λ**(Z,θ)] — Ep* [,λ**(Z, θ)] + λ*δ°D2,
where λ* ∈ argminλ>u{λe + Ep* [，；* (Z, θ)]}.
Lemma A.1 (Lee & Raginsky (2017)). Let λ ∈ argminλ>uλe + EP ['λ(Z, θ)]. As long as the
function in the loss class are L-Lipschitz with respect to dx (see Assumption A2), λ ≤ √.
Proof. By the OPtImahty of λ,
λe ≤ 麓 + Ep [ sup '((X2, Y),θ) — λdχ(X, x2)2 — '((X, Y),θ)]
X2∈X
=麓 + Ep [Q(Z,θ) — '(Z,θ)]
≤ λ + Ep ['λ(Z,θ) — 4(Z,θ)]
=λ + Ep [ sup '((X2, Y),θ) — '((X, Y),θ) — λdχ(X,⑹2]
X2∈X
for any λ ≥ 0. By Assumption A2, the right side is at most
晨 ≤ λe + Ep [ sup Ldχ(X, x2) — λdχ(X, x2)2]
X2∈X
≤ λe + sup Lt — λt2
t≥U
We minimize the right side WRT t (set t =长)and λ (set λ = 2√e) to obtain λe ≤ L√e.	□
12
Published as a conference paper at ICLR 2020
By Lemma A.1, we have
sup	EP ['(Z, θ)]	- sup	EP ['(Z, θ)]	≤ EP*	['λ*	(Z, θ)] - EPn	['λ*	(Z,θ)]	+ LδcD2
P=W*(P,P*)≤e	P:W (P,Pn )≤e	n	n	√e
sup	EP ['(Z,	θ)]	- sup	EP ['(Z,	θ)]	≤	EPn ['λζ	(Z, θ)]	- EP*	['λ] (Z,	θ)]	+-
P:W (P,Pn)≤e	P = W*(P,P*)≤e	√e
We combine the preceding bounds to obtain
sup	EP ['(Z, θ)] - sup	EP ['(Z, θ)]
P: W (P,Pn)≤e	P=W*(P,P*)≤e
≤ /sup I Rz f(z)d(pn - Pj(Z)I +---√-,
where Lc* = {'λ* (∙,θ) ： λ ∈ [0, √], θ ∈ Θ} is the DR loss class. In the rest of the proof, We
bound supf ∈Lc* ∣ JZ f (z)d(P* - Pn)(Z)I with standard techniques from statistical learning theory.
Assumption A2 implies the functions in F are bounded:
0 ≤ '((xι,yι),θ) - λdχ(⅛xτ7xιT ≤ 'λ(zι,θ) ≤ sup '((x2,y1),θ) ≤ M∙
1	X2∈X
This implies has bounded differences, so δn concentrates sharply around its expectation. By the
bounded-differences inequality and a symmetrization argument,
sup IRZ f (z)d(Pn - P*)(z)∣ ≤ 2Rn(Lc*) + M(logɪ)2
f ∈Lc*	2n
WP at least 1 - t, where Rn(F) is the Rademacher complexity ofF:
1n
Rn(F) = E sup - fσif(Zi).
f∈F n i=1
Lemma A.2. The Rademacher complexity of the DR loss class is at most
Rn(LC) ≤ 24√≡ + 2√D2.
n	n-
Proof. To study the Rademacher complexity of Lc, we first show that the Lc-indexed Rademacher
process Xf，n P3 σif(Zi) is sub-Gaussian WRT to a pseudometric. Let fι = 'λ1 (∙, θι) and
f2 = 'λ2 (∙,θ2).Define	ɪ
dLc (f1, f2) , ∣∣'(∙, θ1) - '(∙, θ2) k∞ + D2∖λ1 - "∙
We check that Xf is sub-Gaussian WRT dLc :
E[exp(t(Xf1 - Xf2 ))]
tn
=E[exP(n ∑>i" (Zi,θι)-眩。 (Zi, θ2)))]
n i=1
=E[exp( ^∣σ('λι (Z,θι) - % (Z,θ2)))]n
=E[exp(-tσ( sup inf '((xι, Y),θι) - λ1dx(x1,X)2 - '((x2, Y), θ2) + λ2dχ(X, x2)2)))]n
n	x1 ∈X x2 ∈X
=E[exp(-tσ( sup '((xι, Y),θι) - '((xι,Y),θ2) + (λ2 - λ1)dx(x1,X)2)))]n
n	x1 ∈X
≤ exp(1 t2dLc(f1,f2)).
Let N(Lc, dLc, -) be the --covering number of (Lc, dLc). We observe
N(LC, dLc, -) ≤ N(L, k∙ k∞, f) ∙ N([0, -Le], ∖ ∙ ∖, 2DD2)	(A.1)
13
Published as a conference paper at ICLR 2020
By Dudley,s entropy integral,
Rn(LC) ≤
12 / ∞
√n Jo
log N(Lc, dfc, E)2de
≤
≤
≤
12	∞................................. 、、1
√n J	(log N(Lj ∙k∞, 2 )+ N([θ, √], H, 2D2 )) de
√n(/ logN(L,k∙k∞,i)2de+/ N([θ,用2⅛)2de
24C(L)	24LD2 ∕*21 .1. j
U + F J0 log( 1")de
where we recalled equation A.1 in the second step. We evalaute the integral on the right side to
1
arrive at the stated bound: ʃɔ2 log( ɪ )de < 1.	□
By Lemma A.2,
sup
f ∈Lc*
lʃz f (z)d(Pn - P*)(z)∣ ≤
48C(L)	48LD2
√n	√ne
+ M(空)2,
2n
which implies
sup	EP ['(Z,θ)] - sup	EP ['(Z,θ)]
P: W (P,Pn)≤e	P=W*(P,P*)≤e
≤
48C(L)	48LD2
√n	√ne
LδcD2
+	+ M (
log 2
2n
)2
WP at least 1 - t.
A.2 Proofs of Propositions 3.2 and 3.3
Proofof Proposition 3.2. It is enough to show
supP：W*(P,P* )≤e EP ['(Z, θ)] ≤ δ* + 2δn
because the loss function is non-negative. We have
sup EP ['(Z,θ)] ≤	sup	EP ['(Z,θ)] + δn
P=W*(P,P*)≤e	P :W (P,Pn)≤e
≤	sup	EP ['(Z, 8)] + δn
PW (P,Pn)≤e
≤ sup	EP ['(Z, 8)] + 2δn
P = W*(P,P*)≤e
≤ δ* +2δn.
□
Proofof Proposition 3.3.
sup	(EP ['(Z, θ)]	- EPn	['(Z, θ)]) - sup	(EP ['(Z,	θ)]	- Ep*	[2(Z,	θ)])
P = W*(P,Pn)≤e	P :W (P,P*)≤e
= sup	Ep ['(Z, θ)] - sup	Ep ['(Z, θ)] + Ep* ['(Z, θ)] - Epn ['(Z, θ)]
P = W*(P,P*)≤e	P :W (P,Pn)≤e
≤ δn + Ep*['(Z,θ)] - Epn['(Z,θ)]
The loss function is bounded, so it is possible to bound Ep* ['(Z, θ)] - EPn ['(Z, θ)] by standard
uniform convergence results on bounded loss classes.	□
14
Published as a conference paper at ICLR 2020
B Data-driven fair metrics
B.1	Learning the fair metric from observations of the sensitive attribute
Here we assume the sensitive attribute is discrete and is observed for a small subset of the training
data. Formally, we assume this subset of the training data has the form {(Xi, Ki, Yi)}, where Ki is
the sensitive attribute of the i-th subject. To learn the sensitive subspace, we fit a softmax regression
model to the data
P(Ki = l | Xi)=	:xp(aTXi + bl_, 1 = 1,..., k,
( i Ii)	Pk=ι exp(*Xi + bl),	,	, ,
and take the span of A = [a1 . . . ak] as the sensitive subspace to define the fair metric as
dx(x1, x2)2 = (x1 - x2)T(I - Pran(A))(x1 - x2).	(B.1)
This approach readily generalizes to sensitive attributes that are not discrete-valued: replace the
softmax model by an appropriate generalized linear model.
In many applications, the sensitive attribute is part of a user’s demographic information, so it may not
be available due to privacy restrictions. This does not preclude the proposed approach because the
sensitive attribute is only needed to learn the fair metric and is neither needed to train the classifier
nor at test time.
B.2	Learning the fair metric from comparable samples
In this section, we consider the task of learning a fair metric from supervision in a form of compa-
rable samples. This type of supervision has been considered in the literature on debiasing learned
representations. For example, method of Bolukbasi et al. (2016) for removing gender bias in word
embeddings relies on sets of words whose embeddings mainly vary in a gender subspace (e.g. (king,
queen)).
To keep things simple, we focus on learning a generalized Mahalanobis distance
dχ(x1,x2)=(2(xι)-夕(x2))τΣ(2(xι)-夕(x2))1,	(B.2)
where 夕(x) : X → Rd is a known feature map and Σ ∈ s+×d is a covariance matrix. Our approach
is based on a factor model
ψi = A*ui + B*vi + J,
where 夕i	∈	Rd	is the learned representation of	Xi,	Ui ∈	RK	(resp.	Vi	∈	RL)	is the Sensi-
tive/irrelevant (resp. relevant) attributes ofxi to the task at hand, and i is an error term. For example,
in Bolukbasi et al. (2016), the learned representations are the embeddings of words in the vocabu-
lary, and the sensitive attribute is the gender bias of the words. The sensitive and relevant attributes
are generally unobserved.
Recall our goal is to obtain Σ so that equation B.2 is small whenever v1 ≈ v2. One possible choice of
Σ is the projection matrix onto the orthogonal complement of ran(A), which we denote by Pran(A).
Indeed,
dx(x1, x2)2 = (PI-22)T(I - Pran(A)Xn-22)	(B3
≈ (v1 - v2)TBT(I - Pran(A))B*(VI- v2),	(B.4)
which is small whenever v1 ≈ v2. Although ran(A) is unknown, it is possible to estimate it from
the learned representations and groups of comparable samples by factor analysis.
The factor model attributes variation in the learned representations to variation in the sensitive and
relevant attributes. We consider two samples comparable if their relevant attributes are similar. In
other words, ifI ⊂ [n] is (the indices of) a group of comparable samples, then
HΦi = HUIAT + JHVlHr+≈HEi ≈ HUIAT + HEI,	(B.5)
15
Published as a conference paper at ICLR 2020
where H = I∣ι∣ -= 1∣ι∣ 1% is the centering or de-meaning matrix and the rows of Φi (resp. UI,
Vi) are n (resp. %, v/ If this group of samples have identical relevant attributes, i.e. VI = 1∣IIvT
for some v, then HVI vanishes exactly. As long as ui and i are uncorrelated (e.g. E ui iT = 0),
equation B.5 implies
EΦITHΦI ≈ AEUIT H UI AT +EEITHEI,
This suggests estimating ran(A) from the learned representations and groups of comparable samples
by factor analysis. We summarize our approach in Algorithm 3.
Algorithm 3 estimating Σ for the fair metric
1:	Input: {夕 i}n=ι, comparable groups Iι,..., IG
2:	AT ∈ argminwg,a{2 PG=1 ∣∣HsΦIg - WgATkF}	> factor analysis
3:	Q J qr(A)	. get orthonormal basis of ran (A)
4:	∑ — Id — QQT
C	SenSR implementation details
This section is to accompany the implementation of the SenSR algorithm and is best understood
by reading it along with the code implemented using TensorFlow.4 We discuss choices of learn-
ing rates and few specifics of the code. Words in italics correspond to variables in the code and
following notation in parentheses defines corresponding name in Table 3, where we summarize all
hyperparameter choices.
Handling class imbalance Datasets we study have imbalanced classes. To handle it, on every
epoch(E) (i.e. number of epochs) we subsample a batchsize(B) training samples enforcing equal
number of observations per class. This procedure can be understood as data augmentation.
Perturbations specifics Our implementation of SenSR algorithm has two inner optimization prob-
lems — subspace perturbation and full perturbation (when > 0). Subspace perturbation can be
viewed as an initialization procedure for the attack. We implement both using Adam optimizer
(Kingma & Ba, 2014) inside the computation graph for better efficiency, i.e. defining correspond-
ing perturbation parameters as Variables and re-setting them to zeros after every epoch. This is in
contrast with a more common strategy in the adversarial robustness implementations, where pertur-
bations (i.e. attacks) are implemented using tf.gradients with respect to the input data defined as a
Placeholder.
Learning rates As mentioned above, in addition to regular Adam optimizer for learning the pa-
rameters we invoke two more for the inner optimization problems of SenSR. We use same learning
rate of 0.001 for the parameters optimizer, however different learning rates across datasets for sub-
SPaceSteP(s) and fuktep(f). Two other related parameters are number of steps of the inner opti-
mizations: subspace-epoch(se) and fulLePoch(fe). We observed that setting subspace perturbation
learning rate too small may prevent our algorithm from reducing unfairness, however setting it big
does not seem to hurt. On the other hand, learning rate for full perturbation should not be set too big
as it may prevent algorithm from solving the original task. Note that full perturbation learning rate
should be smaller than perturbation budget eps() — we always use /10. In general, malfunction-
ing behaviors are immediately noticeable during training and can be easily corrected, therefore we
did not need to use any hyperparameter optimization tools.
4https://github.com/IBM/sensitive-subspace-robustness
16
Published as a conference paper at ICLR 2020
Table 3: SenSR hyperparameter choices in the experiments
	E	B	s	se		f	fe
Sentiment	4K	1K	0.1	10	0.1	0.01	10
Adult	12K	1K	10	50	10-3	10-4	40
Table 4: Summary of Adult classification experiments over 10 restarts
	Accuracy	B-TPR	GaPGMS	GaPRRMS	GaPGmax	GaPRmax
SenSR	.787±.003	.789±.003	.068±.004	.055±.003	.087±.005	.067±.004
Baseline	.813±.001	.829±.001	.179±.004	.089±.003	.216±.003	.105±.003
Project	.813±.001	.827±.001	.145±.004	.064±.003	.192±.004	.086±.004
Adv. Debias.	.812±.001	.815±.002	.082±.005	.070±.006	.110±.006	.078±.005
CoCL	-	.790	.163	.080	.201	.109
D Additional Adult experiment details
D.1 Preprocessing
The continuous features in Adult are the following: age, fnlwgt, capital-gain,
capital-loss, hours-per-week, and education-num. The categorical features are the
following: workclass, education, marital-stataus, occupation, relationship,
race, sex, native-country. See Dua & Graff (2017) for a description of each feature. We
remove fnlwgt and education but keep education-num, which is a integer representation
of education. We do not use native-country, but use race and sex as predictive features.
We treat race as binary: individuals are either White or non-White. For every categorical feature,
we use one hot encoding. For every continuous feature, we standardize, i.e., subtract the mean and
divide by the standard deviation. We remove anyone with missing data leaving 45,222 individuals.
This data is imbalanced: 25% make at least $50k per year. Furthermore, there is demographic imbal-
ance with respect to race and gender as well as class imbalance on the outcome when conditioning
on race or gender: 86% of individuals are white of which 26% make at least $50k a year; 67% of
individuals are male of which 31% make at least $50k a year; 11% of females make at least $50k a
year; and 15% of non-whites make at least $50k a year.
D.2 Full experimental results
See Tables 4 and 5 for the full experiment results. The tables report the average and the standard
error for each metric on the test set for 10 train and test splits.
D.3 Sensitive subspace
To learn the hyperplane that classifies females and males, we use our implementation of regularized
logistic regression with a batch size of 5k, 5k epochs, and .1 `2 regularization.
Table 5: Summary of individual fairness metrics in Adult classification experiments over 10 restarts
	Spouse Consistency	Gender and Race Consistency
SenSR	.934±.012	.984±.000
Baseline	.848±.008	.865±.004
Project	.868±.005	1±0
Adv. Debias.	.807±.002	.841±.012
17
Published as a conference paper at ICLR 2020
D.4 Hyperparameters and training
For each model, we use the same 10 train/test splits where use 80% of the data for training. Because
of the class imbalance, each minibatch is sampled so that there are an equal number of training
points from both the “income at least $50k class” and the “income below $50k class.”
D.4.1 Baseline, Project, and SenSR
See Table 3 for the hyperparameters we used when training Baseline, Project, and SenSR (Baseline
and Project use a subset). Hyperparameters are defined in Appendix C.
D.4.2 Advesarial debiasing
We used Zhang et al. (2018)’s adversarial debiasing implementation in IBM’s AIF360 package (Bel-
lamy et al., 2018) where the source code was modified so that each mini-batch is balanced with
respect to the binary labels just as we did with our experiments and dropout was not used. Hyperpa-
rameters are the following: adversary loss weight = .001, num epochs = 500, batch size = 1000,
and privileged groups are defined by binary gender and binary race.
D.5 Group fair metrics
Let C be a set of classes, A be a binary protected attribute and Y,Y ∈ C be the true class label and
the predicted class label. Then for a ∈ {0,1} and C ∈ C define TPRa,c = P(Y = c|A = a, Y = c);
GaPA,c = TPRo,c-TPRi,c； GapAMS =，吉 Pc∈c GapA,c; Gapmax = argmax°∈c∣GaPA,0∣;
Balanced Acc = ∣C∣ Pc∈c P(Y = c|Y = c).
For Adult, we report GapRRMS, GapRGMS, GapmRax, and GapmGax where C is composed of the two classes
that correspond to whether someone made at least $50k, R refers to race, and G refers to gender.
18