Published as a conference paper at ICLR 2020
Online and Stochastic Optimization beyond
Lipschitz Continuity: A Riemannian Approach
Kimon Antonakopoulos
Inria, Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
38000 Grenoble, France
kimon.antonakopoulos@inria.fr
E. Veronica Belmega
ETIS UMR8051, CY University, ENSEA, CNRS, F-95000, Cergy, France
belmega@ensea.fr
Panayotis Mertikopoulos
Inria, Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
38000 Grenoble, France
panayotis.mertikopoulos@imag.fr
Abstract
Motivated by applications to machine learning and imaging science, we study a
class of online and stochastic optimization problems with loss functions that are not
Lipschitz continuous; in particular, the loss functions encountered by the optimizer
could exhibit gradient singularities or be singular themselves. Drawing on tools
and techniques from Riemannian geometry, We examine a Riemann-LiPschitz (RL)
continuity condition which is tailored to the singularity landscape of the problem’s
loss functions. In this Way, We are able to tackle cases beyond the Lipschitz
frameWork provided by a global norm, and We derive optimal regret bounds and
last iterate convergence results through the use of regularized learning methods
(such as online mirror descent). These results are subsequently validated in a class
of stochastic Poisson inverse problems that arise in imaging science.
1	Introduction
The surge of recent breakthroughs in machine learning and artificial intelligence has reaffirmed the
prominence of first-order methods in solving large-scale optimization problems. One of the main
reasons for this is that the computation of higher-order derivatives of functions with thousands - if not
millions - of variables quickly becomes prohibitive; another is that gradient calculations are typically
easier to distribute and parallelize, especially in large-scale problems. In view of this, first-order
methods have met with prolific success in many diverse fields, from machine learning and signal
processing to wireless communications, nuclear medicine, and many others [10, 34, 37].
This success is especially pronounced in the field of online oPtimization, i.e., when the optimizer faces
a sequence of time-varying loss functions f, t = 1,2,…,one at a time - for instance, when drawing
different sample points from a large training set [11, 35]. In this general framework, first-order
methods have proven extremely flexible and robust, and the attained performance guarantees are well
known to be optimal [1, 11, 35]. Specifically, if the optimizer faces a sequence of G-Lipschitz convex
losses, the incurred min-max regret after T rounds is Ω(GT1/2), and this bound can be achieved by
inexpensive first-order methods - such as online mirror descent and its variants [11, 35, 36, 41].
Nevertheless, in many machine learning problems (support vector machines, Poisson inverse problems,
quantum tomography, etc.), the loss landscape is not Lipschitz continuous, so the results mentioned
above do not apply. Thus, a natural question that emerges is the following: Is it Possible to aPPly
online oPtimization tools and techniques beyond the standard LiPschitz framework? And, if so, how?
1
Published as a conference paper at ICLR 2020
Our approach and contributions. Our point of departure is the observation that Lipschitz conti-
nuity is a property of metric spaces - not normed spaces. Indeed, in convex optimization, LiPschitz
continuity is typically stated in terms of a global norm (e.g., the Euclidean norm), but such a norm
is de facto independent of the point in space at which it is calculated. Because of this, the standard
Lipschitz framework is oblivious to the finer aspects of the problem’s loss landscape - and, in
particular, any singularities that may arise at the boundary of the problem’s feasible region. On the
other hand, in general metric spaces, this is no longer the case: the distance between two points
is no longer given by a global norm, so it is much more sensitive to the geometry of the feasible
region. For this reason, if the (Riemannian) distance dist(x, x0) between two points x and x0 becomes
larger and larger as the points approach the boundary of the feasible region, a condition of the form
| f (x) - f (x0)| = O(dist(x0 , x)) may still hold even if f becomes singular at the boundary.
We leverage this observation by introducing the notion of Riemann-Lipschitz (RL) continuity, an
extension of “vanilla” Lipschitz continuity to general spaces endowed with a Riemannian metric. We
show that this metric can be chosen in a principled manner based solely on the singularity landscape
of the problem’s loss functions - i.e., their growth rate at infinity and/or the boundary of the feasible
region. Subsequently, using a similar mechanism to choose a Riemannian regularizer, we provide an
optimal O(T 1/2) regret guarantee through the use of regularized learning methods - namely, “follow
the regularized leader” (FTRL) and online mirror descent (OMD).
Our second contribution concerns an extension of this framework to stochastic programming. First,
in the context of stochastic convex optimization, we show that an online-to-batch conversion yields
an O(T -1/2) value convergence rate. Second, motivated by applications to nonconvex stochastic
programming (where averaging is not a priori beneficial), we also establish the convergence of the
method’s last iterate in a class of nonconvex problems satisfying a weak secant inequality. Finally,
we supplement our theoretical analysis with numerical experiments in Poisson inverse problems.
Related work. To the best of our knowledge, the first treatment ofa similar question was undertaken
by Bauschke et al. [3] who focused on deterministic, offline convex programs (ft = f for all t) without
a Lipschitz smoothness assumption (i.e., Lipschitz continuity of the gradient, as opposed to Lipschitz
continuity of the objective). To tackle this issue, Bauschke et al. [3] introduced a second-order
“Lipschitz-like” condition of the form V2 f 4 jβV2h for some suitable Bregman function h, and they
showed that Bregman proximal methods achieve an O(1/T) value convergence rate in offline convex
problems with perfect gradient feedback.
Always in the context of deterministic optimization, Bolte et al. [8] extended the results of Bauschke
et al. [3] to unconstrained non-convex problems and established trajectory convergence to critical
points for functions satisfying the KUrdyka-EOjaSieWiCZ (KL) inequality. In a slightly different
vein, Lu et al. [25] considered functions that are also strongly convex relative to the Bregman
function defining the Lipschitz-like condition for the gradients, and they shoWed that mirror descent
achieves a geometric convergence rate in this context. Finally, in a very recent preprint, Hanzely et al.
[17] examined the rate of convergence of an accelerated variant of mirror descent under the same
Lipschitz-like smoothness assumption.
Importantly, all these Works concern offline, deterministic optimization problems With perfect gradient
feedback and regularity assumptions that cannot be exploited in an online optimization setting (such
as the KL inequality). Beyond offline, deterministic optimization problems, Lu [24] established
the ergodic convergence of mirror descent in stochastic non-adversarial convex problems under a
“relative continuity” condition of the form ∣∣Vf (X)k ≤ G infX，√2D(X0, X)/kXX - Xk (with D denoting
the divergence of an underlying “reference” Bregman function h). More recently, Hanzely and
Richtarik [16] examined the performance of stochastic mirror descent under a combination of relative
strong convexity and relative smoothness / Lipschitz-like conditions, and established a series of
convergence rate guarantees that mirror the corresponding rates for ordinary (Euclidean) stochastic
gradient descent. Except for trivial cases, these conditions are not related to Riemann-Lipschitz
continuity, so there is no overlap in our results our methodology.
Finally, in a very recent paper, BeCigneUI and Ganea [5] established the convergence of a class of
adaptive Riemannian methods in geodesically convex problems (extending in this way classical
results for AdaGrad to a manifold setting). Importantly, the Riemannian methodology of [5] involves
the exponential mapping of the underlying metric and focuses on geodesic convexity, so it concerns
an orthogonal class of problems. The only overlap would be in the case of flat Riemannian manifolds:
2
Published as a conference paper at ICLR 2020
however, even though the manifolds we consider here are topologically simple, they are not flat.1 In
view of this, there is no overlap with the analysis and results of [5].
2	Problem setup
We begin by presenting the core online optimization framework that we will consider throughout the
rest of our paper. This can be described by the following sequence of events:
1.	At each round t = 1,2,... ,the optimizer chooses an action Xt from a convex - but not necessarily
closed or compact - subset X of an ambient normed space V 兰 ’d.
2.	The optimizer incurs a loss ft(Xt) based on some (unknown) convex loss function ft : X → ’.
3.	The optimizer updates their action and the process repeats.
Remark 1. For posterity, we note that if X is not closed, ft (or its derivatives) could become singular
at a residual point x ∈ bd(X) \ X ; in particular, we do not assume here that ft admits a smooth
extension to the closure cl(X) of X (or even that it is bounded over bounded subsets of X).
In this broad framework, the most widely used figure of merit is the minimization of the agent’s
regret. Formally, the regret of a policy Xt ∈ X , t = 1, 2, . . . , is defined as
T
Regx(T)=	[ft(Xt)-ft(x)],	(1)
t=1
for all x ∈ X. We then say that the policy Xt leads to no regret if Regx(T) = o(T) for all x ∈ X.
In addition to convexity, the standard assumption in the literature for the problem’s loss functions is
Lipschitz continuity, i.e.,
| ft(x0) - ft(x)| ≤Gtkx0-xk	(LC)
for some Gt ≥ 0, t = 1, 2, . . . , and for all x, x0 ∈ X . Under (LC), if the agent observes at each
stage t an element vt of ∂ ft(Xt), straightforward online policies based on gradient descent enjoy a
bound of the form RegX(T) = O(GTT1/2), With GT = T-1 P3 G2 [11, 35, 41]. In particular, if
G ≡ lim supt→∞ GT < ∞ (e.g., if each f is G-Lipschitz continuous over X), we have the bound
Regx(T) = O(GT1/2)	(2)
which is well known to be min-max optimal in this setting [1].
A note on notation. Throughout our paper, we make a clear distinction between V and its dual, and
we use Dirac,s notation〈矶 X i for the duality pairing between V ∈ V * and X ∈ V (not to be confused
with the notation〈•, ∙〉for a scalar product on V). Also, unless mentioned otherwise, all notions of
boundary and interior should be interpreted in the relative (as opposed to topological) sense. We
also make the blanket assumption that the subdifferential ∂ft of ft admits a continuous selection
Vft(x) ∈ ∂ft(x) for all X ∈ dom ∂f ≡ {X ∈ X : ∂f (x) , 0}.
3 Riemann-Lipschitz continuity
Despite its generality, (LC) may fail to hold in a wide range of problems and applications, ranging
from support vector machines to Poisson inverse problems, quantum tomography, etc. [3, 8, 25]. The
loss functions of these problems exhibit singularities at the boundary of the feasible region, so the
standard regret analysis cited above no longer applies. Accordingly, our first step will be to introduce
a family of local norms ∣∣∙∣∣X, X ∈ X, such that a variant of (LC) holds even if the derivatives ∂f /∂Xi
of f blow up near the boundary of X .
To achieve this, we will employ the notion of a Riemannian metric. This is simply a position-
dependent scalar product on V, i.e., a continuous assignment of bilinear pairings〈•，•〉X, X ∈ X,
satisfying the following conditions for all z, z0 ∈ V and all X ∈ X :
1For example, the open unit simplex endowed with the Shahshahani metric is isometric to the positive orthant
of a sphere with the round metric (cf. Section 3). This space has constant positive curvature and the geodesics
are portions of great circles, so the two analyses are very different in that case.
3
Published as a conference paper at ICLR 2020
1.	Symmetry: hz, z0ix = hz0, zix.
2.	Positive-definiteness: hz, zix ≥ 0 with equality if and only if z = 0.
More concretely, in the standard basis {ei}：=、of ’d, We define the metric tensor of O as the matrix
g(X) ∈ ’d×d with components
gij(x) = hei, ejix	i, j = 1, . . .,d.	(3)
The norm of z ∈ V at x ∈ X is then defined as
d
kzk2x ≡ hz, zi2x =	gij(x)zizj = z>g(x)z.	(4)
i,	j=1
In this way, a Riemannian metric allows us to measure lengths and angles between displacement
vectors at each x ∈ X ; for illustration, we provide some notable examples below:
Example 1	(Euclidean geometry). The ordinary Euclidean metric on X = ’d is g(x) = I. This yields
the standard expressions kzk2x = Pid=1 zi2 and hz, z0ix = Pid=1 zizi0, both independent of x.
Example 2	(Hyperbolic geometry). The Poincare metric on the positive orthant X = ’++ is
g(x) = diag(1/x12,. . ., 1/xd2),	(5)
leading to the local norm kz∣∣2 = P = Z22/x2. Under (5), ’ ++ can be seen as a variant of PoincarCs
half-space model for hyperbolic geometry [22]; this will become important later.
Given a Riemannian metric on X, the length of a curve γ: [0, 1] → X is defined as Lg [γ] =
J0 kγ(S)kγ(S)ds, and the Riemannian distance between X1, X2 ∈ X is given by
distg(x1,x2) = infγ Lg[γ].	(6)
Under this definition, it is natural to introduce the following Riemannian variant of (LC):
Definition 1. We say that f: X → ' is Riemann-LipSChitz continuous relative to g if
|f(X0) - f (X)| ≤ Gdistg(X, X0) for some G ≥ 0 and all X, X0 ∈ X.	(7)
Albeit simple to state, (RLC) may be difficult to verify because it requires the computation of the
distance function distg of g - which, in turn, relies on geodesic calculations to identify the shortest
possible curve between two points. Nevertheless, if f is differentiable, Proposition 1 below provides
an alternative characterization of Riemann-Lipschitz continuity which is easier to work with:
Proposition 1. Suppose that f: X → ’ is differentiable. Then, (RLC) holds if and only if
kgradf(X)kX ≤ G forallX∈ X.	(RLC)
Remark 2. In the above, the Riemannian gradient grad f(X) off at X is defined as follows: First, let
Z = span{X0 - X : X, X0 ∈ X} denote the tangent hull of X. Then, grad f(X) ∈ Z is defined by the
characteristic property
f0 (X; z) = hgrad f(X), ziX forallz∈ Z.	(8)
Existence and uniqueness of grad f(X) is due to the fact that g(X) is positive-definite - and, hence,
invertible [22]. In particular, if X is full-dimensional (so Z = V), we have:
[grad f(X)]i = Xdj=1 g(X)i-j1 ∂jf(X).	(9)
The proof of Proposition 1 requires the introduction of further tools from Riemannian geometry;
seeing as these notions are not used anywhere else in our paper, we relegate it to the appendix.
Instead, we close this section with a simple example of a singular function which is nonetheless
Riemann-Lipschitz continuous:
4
Published as a conference paper at ICLR 2020
Example 3. Let X = [0, 1]d\{0} (so X is convex but neither open nor closed) and let f(x) =
-log( a > x) for some positive vector a ∈ ’++. If We take gj x) = δ ij/(X1 + •一+ Xd )2, We get
,Pdτ a2 • (Pdτ Xi)2	Pdτ a2
kgrad fχ)k2 =	(a Tx)2	≤ (rnin⅛.
(10)
Thus, although f is not Lipschitz continuous in the standard sense, it is Riemann-Lipschitz continuous
relative to g; We Will revisit this example in our treatment of Poisson inverse problems in Section 6.
More generally, Example 3 suggests the folloWing rule of thumb: if f exhibits a gradient singularity of
the form ∣∂if (x)| = O(φ(x)) at some residual point X ∈ cl(X) \ X of X, taking gij(x) = φ(x)2δij gives
kgradf(X)k* 2X = φ(X)-2 Pid=1[∂if(X)]2 = O(1). On that account, f is Riemann-Lipschitz continuous,
even though its derivative is singular; We find this heuristic particularly appealing because it provides
a principled choice of Riemannian metric under Which f satisfies (RLC).
4 Algorithms
In this section, We present the algorithms that We Will study in the sequel: “folloW the regularized
leader” (FTRL) and online mirror descent (OMD). Both methods have been Widely studied in the
literature in the context of “vanilla” Lipschitz continuity; hoWever, beyond this basic setting, treating
FTRL/OMD in the Riemannian frameWork of the previous section is an intricate affair that requires
several conceptual modifications. For this reason, We take an in-depth look into both methods beloW.2
4.1	Regularization
We begin With the idea of regularization through a suitable penalty function. In our Riemannian
setting, We adapt this notion as folloWs:
Definition 2. Let g be a Riemannian metric on X and let h : V → ’ be a proper loWer semi-
continuous (l.s.c.) convex function With domh = X.3 We say that h is a Riemannian regularizer on
X if:
1.	The subdifferential of h admits a continuous selection, i.e., a continuous function Vh such
that V h (x) ∈ ∂h (x) for all X ∈ X ◦三 dom ∂h.
2.	h is strongly conveX relative to g, i.e.,
h(x') ≥ h(x) +〈Vh(x)|x'- x〉+ ɪKkX'- x∣∣X	(11)
for some K > 0 and all X ∈ X0, X0 ∈ X.
The Bregman divergence induced by h is then defined for all P ∈ X, X ∈ Xo as
D(p,x) = h(p) - h(x) - 〈Vh( x)| p - x〉.	(12)
There are tWo points Worth noting in the above definition. First, the domain of h is all of X, but this
need not be the case for the subdifferential ∂h of h: by convex analysis arguments [33, Chap. 26],
we have ri X ⊆ Xo ≡ dom ∂h ⊆ X. To connect the two, we will say that h is a Riemann-Legendre
regularizer when Xo = ri X and D(P, xn) → 0 whenever xn → P.
Second, strong convexity in (11) is defined relative to the underlying Riemannian metric. If the norm
in (11) does not depend on x, we recover the standard definition; however, the dependence of the
second-order term in (11) on g can change the landscape significantly. Lemma 1 and Example 5
below provide an illustration of this interplay between g and h:
Lemma 1. A Riemannian regularizer h is K-strongly convex relative to g if and only if
 D(P, x) ≥ 2 KkP -XkX.	(13)
2For convenience, we tacitly assume in what follows that g(X) < μI for some μ > 0 and all X ∈ X. Since
g < 0, this can always be achieved without loss of generality by replacing g by g + μI.
3Following standard convex analysis terminology, “proper” means here that h , +∞ while lower semi-
continuous refers to the property that lim inf x→x0 h(x) ≥ h(x0) for all x0 ∈ V.
5
Published as a conference paper at ICLR 2020
The proof of Lemma 1 follows from a rearrangement of (11) so we omit it. Instead, we present below
some examples of Riemannian regularizers:
Example 4.	Let X = [0, 1]d, and consider the so-called Burg entropy h(x) = - Pid=1 logxi. It is easy
to see that h(ɪ) is strongly convex relative to the standard Euclidean norm ∣∣∙∣∣2. Moreover, We have
D(P,X) = X∣P - log P - 1	(14)
and, by Taylor’s theorem With Lagrange remainder, We readily get
D(P, X) ≥ 1 X (P_xi)- = kP - XkX	(15)
2 i=1	xi
Where kzkX = Pid=1 zi2/Xi denotes the so-called Shahshahani norm on X (i.e., h is also strongly convex
relative to ∣∣∙∣∣χ). This regularizer has been used extensively in the setting of Poisson inverse problems
and plays a central role in the analysis of Bauschke et al. [3], Hanzely and RiChtdrik [16], He et al.
[18], and Lu et al. [25]. For completeness, We revisit it in Section 6.
Example 5.	Let X and g be as in Example 3, and let h(X) = (1 + r2)/ Pid=1 Xi With r2 = Pid=1 Xi2. Then,
a tedious (but otherWise straightforWard) algebraic calculation gives
D(P, X) ≥ X P P∖2 = kP - xkX	(16)
i=1 ( j=1 Xj)
i.e., h is strongly convex relative to g. By contrast, due to the singularity of g at 0, it is easy to check
that the Euclidean regularizer h(X) = (1/2) Pid=1 Xi2 is not strongly convex relative to g.
4.2 Algorithms and feedback structure
With these preliminaries in hand, We begin With the FTRL algorithm, Which We state here as folloWs:
Xt+1 = arg minXts=1 fs (X) + γ-1h(X).	(FTRL)
In the above, γ > 0 is a step-size parameter Whose role is discussed beloW; as for the existence of the
arg min, this is justified by the loWer semicontinuity and strong convexity of h together With the fact
that domh = X (so the minimum cannot be attained in the residual set cl(X) \ X of X).
In terms of feedback, FTRL assumes that the optimizer has access to all the loss functions encountered
up to a given round (except, of course, for the current one). In many cases of practical interest, this
assumption is too restrictive and, instead, the optimizer only has access to a first-order oracle for each
ft . To model this feedback structure, We assume that once Xt has been chosen, the optimizer receives
an estimate Vt of Vf (Xt) satisfying the following hypotheses:
a)	Unbiasedness:	…[v11 Ft] = Vf (Xtt).	(17a)
b)	Finite mean square:…[|也k21 Ft] ≤ M2.	(17b)
In the above, k∙k* denotes the dual norm of k∙kχt, i.e., the Riemannian norm at the point Xt where
the oracle was called (we suppress here the index Xt and write k∙k* instead of k∙kχt,* to lighten the
notation). In particular, the oracle feedback vt may fail to be bounded in L2 relative to any global
norm on V*; as such, (17) is considerably weaker than the standard L2 -boundedness assumPtionfor
global norms. Finally, in terms of measurability, the expectation in (17) is conditioned on the history
Ft of Xt up to stage t; since vt is generated randomly from Xt, it is not Ft-measurable.
To proceed, the main idea of mirror descent is to replace fs (X) in (FTRL) with the first-order surrogate
fs(x) J fs(XS) +〈Vfs(Xs)|x - XSi. In this way, substituting Vs for the estimate of Vfs(XS) received at
stage s, we obtain the linearized FTRL scheme
Xt +1 = argminX ∈x {γ Σ S=1〈v SIX i + h (X)}.	(18)
To rewrite this process in recursive form, introduce the auxiliary (dual) variable
Yt+1 = Yt - γVt	(19)
6
Published as a conference paper at ICLR 2020
so Yt+1 = -γ Pts=1 vs, and hence
Xt+1 = arg min{h(x) - hYt+1|xi} = arg max{hYt+1|xi - h(x)}.
x∈X
x∈X
(20)
Therefore, letting
Q(y) = arg maxx∈X {hy|xi - h(x)}
(21)
denote the so-called “mirror map” of the method, we obtain the following incarnation of the online
mirror descent (OMD) algorithm:
Yt+1 = Yt - γvt
Xt+1 = Q(Yt+1).
(OMD)
This version of OMD is also known as “dual averaging” [26, 29, 30, 38] or “lazy mirror descent”
[35]; for a “greedy” variant, see [6, 27, 28] and references therein.
5 Analysis and results
5.1	Regret analysis
We begin by stating our main results for the regret minimization properties of FTRL and OMD.
Throughout this section, we make the following blanket assumptions:
1.	Both algorithms are initialized at the “prox-center” xc = arg min h of X and are run with
(constant) step-size a/T1/2 for some a > 0 chosen by the optimizer.
2.	The t-th stage loss function ft : X → ’ is convex and satisfies (RLC) with constant Gt.
3.	The optimizer's aggregate loss PT=I f attains its minimum value at some X* ∈ X.
The purpose of the last assumption is to avoid cases where the infimum of a loss function is not
attained within the problem’s feasible region (such as e-X over ’+). We then have:
Theorem 1.	Let Reg(T) ≡ RegX* (T), GT = TT P = Gj, and M2 = TT P = M2. Then:
a)	The FTRL algorithm enjoys the regret bound
Reg(T) ≤
D (X *, XC )
α
(22a)
b)	The OMD algorithm with noisy feedbaCk of the form (17) enjoys the mean regret bound
E[Reg(T)] ≤ D(： XC) + αK j √T.	(22b)
In particular, if sup ±Gt < ∞, SUPtMt < ∞, both algorithms guarantee O( VT) regret.
Remark 3. We emphasize here that the O( VT) regret bound above is achieved even if X is unbounded
or if the “Bregman depth” H ≡ supX∈X D(X, XC) = suph - infh of X is infinite.4 Of course, if H < ∞
and G (or M) is known to the optimizer, (22) can be optimized further by tuning α. When these
constants are unknown, achieving an optimized constant by means of an adaptive step-size policy is
an important question, but one which lies beyond the scope of this paper.
The main idea behind the proof of Theorem 1 is to relate the Riemannian structure of X to the
Bregman regularization framework underlying (FTRL) and (OMD). A first such link is provided by
the Bregman divergence (12); however, because of the primal-dual interplay between Xt ∈ X and
Yt ∈ V*, the Bregman divergence is not sufficiently adapted. To overcome this difficulty, we employ
the FenChel Coupling between a target point p ∈ X and y ∈ V*, defined here as
Φ(p, y) = h(p) + h*(y) - hy|pi forallp∈X,y∈V*,	(23)
4To see this, simply note that D(x, XC) = h(X) - h(XC) -〈Vh(XC)|X - XCCi < ∞ for all X ∈ X = domh (recall
also that, since XC = arg min h, we have 0 ∈ ∂h(XC) so XC ∈ dom ∂h).
7
Published as a conference paper at ICLR 2020
with h*(y) = maxX∈χ{〈y|X〉一 h(x)} denoting the convex conjugate of h. As We show in the appendix,
the Fenchel coupling (which is non-negative by virtue of Young’s inequality) enjoys the key property
γ2	2
Φ(p, y - γv) ≤ Φ(p, y) -hγv∣Q(y) -P〉+ 宗IWkQ⑼,*.	(24)
It is precisely this primal-dual inequality which allows us to go beyond the standard Lipschitz
framework: compared to (primal-primal) inequalities of a similar form for global norms [2, 21, 27,
30, 40], the distinguishing feature of (24) is the advent of the Riemannian norm kvkX,*. Thanks to the
intricate connection between g and h, the second-order term in (24) can be controlled even when the
received gradient is unbounded relative to any global norm, i.e., even if the objective is singular.
The main obstacle to achieve this is that the underlying Riemannian metric g, the Fenchel coupling
Φ and the Bregman divergence D (all state-dependent notions of distance) need not be compatible
with one another. That this is indeed the case is owed to Lemma 1: tethering the Riemannian norm
in (13) to the second argument of the Bregman divergence instead of the first (or any other point
in-between) plays a crucial role in deriving (24). Any other relation between g and h along these
lines is not amenable to analyzing (FTRL) or (OMD) in this framework.
5.2 Applications to stochastic optimization
The second part of our analysis concerns stochastic optimization problems of the form
minimize f (X)=…[F (X; ω)]
subject to X ∈ X
(Opt)
with the expectation taken over some model sample space Ω. Our first result here is as follows:
Theorem 2.	Assume that f is convex and Riemann-LiPschitz continuous in mean square, i.e.,
sup x …[∣∣V F (x; ω)k2 ,*] ≤ M2 for some M > 0 .If (OMD) is run for T iterations with a constant
step-size oftheform a/ TT and stochastic gradients Vt = VF(Xt; ωt) generated by an i.i.d. sequence
ωt ∈ Ω, we have
…[f (Xt )] ≤ min f + — +	----L	(25)
α	2K	T
where XT = (1/T) P3 Xt is the “ergodic average” OfXt and DC = infX*∈argmin f D(x*, XC) < ∞
denotes the Bregman distance of the ProX-center Xc of X to arg min f.
The key novelty in Theorem 2 is that the optimal O(T -1/2) convergence rate of OMD is maintained
even if the stochastic gradients ofF become singular at residual Points X ∈ cl(X) \ X. As with
the regret guarantee of Theorem 1, this is achieved by the intricate three-way relation between the
landscape of f, the underlying Riemannian metric g (which is tailored to the singularity profile
of the latter), and the Riemannian regularizer h. The proof of Theorem 2 likewise relies on an
online-to-batch conversion of the regret guarantees of (OMD) for the sequence of stochastic gradients
VF(∙; ωt) of f; the details can be found in the appendix.
To go beyond the ergodic guarantees of Theorem 2, we also analyze below the convergence of the
“last iterate” of OMD, i.e., the actual sequence of generated Points Xt. This is of particular interest
for non-convex problems where ergodic convergence results are of limited value (because Jensen’s
inequality no longer applies). To obtain global convergence results in this setting, we focus on a class
of functions which satisfy a weak secant inequality of the form
inf{〈Vf(X)|X - X*〉 : X* ∈ arg min f, X ∈ K} > 0	(SI)
for every closed subset K of X that is separated by neighborhoods from arg min f. Variants of
this condition have been widely studied in the literature and include non-convex functions with
complicated ridge structures [9, 13, 19, 20, 23, 31, 39, 40]. In this very general setting, we have:
Theorem 3.	Assume f satisfies (SI) and is Riemann-LiPschitz continuous in L2. SuPPose further
that arg min f is bounded and (OMD) is run with a sequence of stochastic gradients vt = VF(Xt; ωt),
a Riemann-Legendre regularizer h, and a variable steP-size γt such that Pt∞=1 γt = ∞, Pt∞=1 γt2 < ∞.
Then, with Probability 1, Xt converges to some (Possibly random) X* ∈ arg min f.
8
Published as a conference paper at ICLR 2020
Poisson Loss Minimization
10
0.100
0.001
10-5
10-7
10-9
	-fs^j*<5≡π≡⅛`*⅛		土一τ r ,	
		丁		∙*∙→
	-LR (e	rg. average	■工 J、	■**♦
	•…LR (1 -CMP	ast iterate) (erg. avera	% ,⅛ ge)	K	'o ∏ b
	--CMP	(last iterate	)	K ■⅛
	-•— RMD →∙- RMD	(erg. avera (last iterat	ge) )	K O
1	10	100	1000
Figure 1: Reconstruction of the Lena test image from a sample contaminated with Poisson noise. Left to right:
(a) the contaminated sample; (b) CMP reconstruction; (c) RMD reconstruction; and (d) Poisson likelihood
loss at each iteration. The RMD process provides a sharper definition of image features relative to the CMP
algorithm (which is the second-best).
The proof for Theorem 3 hinges on combining (quasi-)supermartingale convergence results with
the basic inequality (24); we detail the proof in the paper,s appendix. Seeing as (SI) holds trivially
for (pseudo-)convex functions, we only note here that Theorem 3 complements Theorem 2 in an
important way: the convergence of Xt implies that of Xt, so the convergence of E[f (Xt)] to min f
follows immediately from Theorem 3; however, the rate of convergence (25) doesn,t. In practice,
the ergodic average converges to interior minimizers faster than the last iterate but lags behind when
tracking boundary points and/or in non-convex landscapes; we explore this issue in Section 6 below.
6 Numerical experiments in Poisson inverse problems
For the purposes of validation, we proceed with an application of our algorithmic results to a broad
class of Poisson inverse problems that arise in tomography problems. Referring the reader to the
appendix for the details, the objective of interest here is the Poisson likelihood loss (generalized
Kullback-Leibler divergence):
fχ)=X j=1 [ UjIOg H)J+(Hχ) j - uj i	(26)
where U ∈ R： is a vector of Poisson data observations (e.g., pixel intensities) and H ∈ Rm×d is an
ill-conditioned matrix representing the data-gathering protocol. Since the generalized KL objective
of (26) exhibits an O(1/X) singularity at the boundary of the orthant, we consider the Poincare
metric g(x) = diag(1/xι,…，1/Xd) under which the KL divergence is Riemann-Lipschitz continuous
(cf. Example 2). Going back to Example 3, a suitable Riemannian regularizer for this metric is
h(x) = Pm=11 /χ2, which is 1-strongly convex relative to g. We then run the induced mirror descent
algorithm with an online-to-batch conversion mechanism as described in Section 5.2. For reference
purposes, we call the resulting process Riemannian mirror descent (RMD).
Subsequently, we ran RMD on a Poisson denoising problem for a384 × 384 test image contaminated
with Poisson noise (so d ≈ 105 in this case). For benchmarking, we also ran a fast variant of the
widely used Lucy-Richardson (LR) algorithm [7], and the recent composite mirror prox (CMP)
method of [18]; all methods were run with stochastic gradients and the same minibatch size. Because
of the “dark area” gradient singularities when [Hx] j → 0, Euclidean stochastic gradient methods
oscillate without converging, so they are not reported. As we see in Fig. 1, the RMD process provides
the sharpest reconstruction of the original. In particular, after an initial warm-up phase, the last iterate
of Riemannian mirror descent consistently outperforms the LR algorithm by 7 orders of magnitude,
and CMP by 3. We also note that the Poisson likelihood loss decreases faster under the last iterate of
RMD relative to the different algorithmic variants that we tested, exactly because of the hysteresis
effect that is inherent to ergodic averaging.
Overall, we note that the introduction of an additional degree of freedom (the choice of Bregman
function and that of the local Riemannian norm), makes RMD a particularly flexible and powerful
paradigm for loss models with singularities. We find these results particularly encouraging for further
investigations on the interplay between Riemannian geometry and Bregman-proximal methods.
9
Published as a conference paper at ICLR 2020
7 Concluding remarks
Owing to its connections with machine learning (support vector machines, Poisson inverse problems,
quantum tomography, etc.), venturing beyond Lipschitz continuity is a fruitful research direction that
has recently generated considerable interest in the literature. Depending on the type of continuity
or smoothness encountered (Lipschitz continuity of the objective or Lipschitz continuity of the
objective’s gradients), the results can be significantly different, and it is not a priori clear which
surrogate smoothness/continuity condition would be the most appropriate for any given problem.
The present paper provides a complementary, Riemannian-geometric viewpoint which we feel can
be fairly promising for the design of efficient optimization algorithms in this context. The precise
characterization of the interplay between the different continuity conditions considered in the literature
is an important open issue which we leave for future work.
Acknowledgments
The authors gratefully acknowledge financial support from the French National Research Agency
(ANR) under grants ORACLESS (ANR-16-CE33-0004-01)and ELIOT (ANR-18-CE40-0030), as
well as the FAPESP 2018/12579-7 project. This work also benefited from financial support by MIAI
Grenoble Alpes (Multidisciplinary Institute in Artificial Intelligence).
References
[1]	Abernethy, Jacob, Peter L. Bartlett, Alexander Rakhlin, Ambuj Tewari. 2008. Optimal strategies and
minimax lower bounds for online convex games. COLT ’08: Proceedings of the 21st Annual Conference
on Learning Theory.
[2]	Balandat, Maximilian, Walid Krichene, Claire Tomlin, Alexandre Bayen. 2016. Minimizing regret on
reflexive Banach spaces and Nash equilibria in continuous zero-sum games. NIPS ’16: Proceedings of the
30th International Conference on Neural Information Processing Systems.
[3]	Bauschke, Heinz H., Jerome Bolte, Marc Teboulle. 2017. A descent lemma beyond LiPschitz gradient
continuity: First-order methods revisited and applications. Mathematics of Operations Research 42(2)
330-348.
[4]	Bauschke, Heinz H., Patrick L. Combettes. 2017. Convex Analysis and Monotone Operator Theory in
Hilbert Spaces. 2nd ed. SPringer, New York, NY, USA.
[5]	BCcigneul, Gary, Octavian-Eugen Ganea. 2019. Riemannian adaptive optimization methods. ICLR '19:
Proceedings of the 2019 International Conference on Learning Representations.
[6]	Beck, Amir, Marc Teboulle. 2003. Mirror descent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters 31(3) 167-175.
[7]	Bertero, Mario, Patrizia Boccacci, Gabriele Desider》, Giuseppe Vicidomini. 2009. Image deblurring with
Poisson data: from cells to galaxies. Inverse Problems 25(12) 123006.
[8]	Bolte, JCrome, Shoham Sabach, Marc Teboulle, Yakov Vaisbourd. 2018. First order methods beyond
convexity and Lipschitz gradient continuity with applications to quadratic inverse problems. SIAM Journal
on Optimization 28(3) 2131-2151.
[9]	Bottou, LCon. 1998. Online learning and stochastic approximations. On-line learning in neural networks
17(9) 142.
[10]	Bubeck, SCbastien. 2015. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning 8(3-4) 231-358.
[11]	Bubeck, SCbastien, Nicold Cesa-Bianchi. 2012. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning 5(1) 1-122.
[12]	Chen, Gong, Marc Teboulle. 1993. Convergence analysis of a proximal-like minimization algorithm using
Bregman functions. SIAM Journal on Optimization 3(3) 538-543.
[13]	Facchinei, Francisco, Jong-Shi Pang. 2003. Finite-Dimensional Variational Inequalities and Complemen-
tarity Problems. Springer Series in Operations Research, Springer.
[14]	Ferreira, Orizon P. 2006. Proximal subgradient and a characterization of Lipschitz function on Riemannian
manifolds. Journal of Mathematical Analysis and Applications 313 587-597.
[15]	Hall, P., C. C. Heyde. 1980. Martingale Limit Theory and Its Application. Probability and Mathematical
Statistics, Academic Press, New York.
[16]	Hanzely, Filip, Peter Richtdrik. 2018. Fastest rates for stochastic mirror descent methods. https:
//arxiv.org/abs/1803.07374.
10
Published as a conference paper at ICLR 2020
[17]	Hanzely, Filip, Peter Richtdrik, Lin Xiao. 2018. Accelerated Bregman proximal gradient methods for
relatively smooth convex optimization. https://arxiv.org/abs/1808.03045.
[18]	He, Niao, Zaid Harchaoui, Yichen Wang, Le Song. 2016. Fast and simple optimization for Poisson
likelihood models. https://arxiv.org/abs/1608.01264.
[19]	Jiang, Houyuan, Huifu Xu. 2008. Stochastic approximation approaches to the stochastic variational
inequality problem. IEEE Trans. Autom. Control 53(6) 1462-1475.
[20]	Karimi, Hamed, Julie Nutini, Mark Schmidt. 2016. Linear convergence of gradient and proximal-gradient
methods under the Polyak-匕OjasieWiCZ condition. https://arxiv.org/abs/1608.04636.
[21]	Krichene, Walid. 2016. Continuous and discrete dynamics for online learning and convex optimization.
Ph.D. thesis, Department of Electrical Engineering and Computer Sciences, University of California,
Berkeley.
[22]	Lee, John M. 1997. Riemannian Manifolds: an Introduction to Curvature. No. 176 in Graduate Texts in
Mathematics, Springer.
[23]	Ljung, Lennart. 1978. Strong convergence of a stochastic approximation algorithm. Annals of Statistics
6(3) 680-696.
[24]	Lu, Haihao. 2017. "Relative-continuity" for non-Lipschitz non-smooth convex optimization using stochastic
(or deterministic) mirror descent. https://arxiv.org/abs/1710.04718.
[25]	Lu, Haihao, Robert M. Freund, Yurii Nesterov. 2018. Relatively-smooth convex optimization by first-order
methods and applications. SIAM Journal on Optimization 28(1) 333-354.
[26]	Mertikopoulos, Panayotis, Zhengyuan Zhou. 2019. Learning in games With continuous action sets and
unknoWn payoff functions. Mathematical Programming 173(1-2) 465-507.
[27]	Nemirovski, Arkadi Semen, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro. 2009. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization 19(4) 1574-1609.
[28]	Nemirovski, Arkadi Semen, David Berkovich Yudin. 1983. Problem Complexity and Method Efficiency in
Optimization. Wiley, NeW York, NY.
[29]	Nesterov, Yurii. 2007. Dual extrapolation and its applications to solving variational inequalities and related
problems. Mathematical Programming 109(2) 319-344.
[30]	Nesterov, Yurii. 2009. Primal-dual subgradient methods for convex problems. Mathematical Programming
120(1) 221-259.
[31]	Nevel’son, M. B., Rafail Z. Khasminskii. 1976. Stochastic Approximation and Recursive Estimation.
American Mathematical Society, Providence, RI.
[32]	Robbins, Herbert, David Sigmund. 1971. A convergence theorem for nonnegative almost supermartingales
and some applications. J. S. Rustagi, ed., Optimizing Methods in Statistics. Academic Press, NeW York,
NY, 233-257.
[33]	Rockafellar, Ralph Tyrrell. 1970. Convex Analysis. Princeton University Press, Princeton, NJ.
[34]	Scutari, Gesualdo, Francisco Facchinei, Daniel P6rez Palomar, Jong-Shi Pang. 2010. Convex optimization,
game theory, and variational inequality theory in multiuser communication systems. IEEE Signal Process.
Mag. 27(3) 35-49.
[35]	Shalev-ShWartz, Shai. 2011. Online learning and online convex optimization. Foundations and Trends in
Machine Learning 4(2) 107-194.
[36]	Shalev-ShWartz, Shai, Yoram Singer. 2007. Convex repeated games and Fenchel duality. Advances in
Neural Information Processing Systems 19. MIT Press, 1265-1272.
[37]	Sra, Suvrit, Sebastian NoWozin, Stephen J. Wright. 2012. Optimization for Machine Learning. MIT Press,
Cambridge, MA, USA.
[38]	Xiao, Lin. 2010. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research 11 2543-2596.
[39]	Zhang, Hui, Wotao Yin. 2013. Gradient methods for convex minimization: Better rates under Weaker
conditions. https://arxiv.org/abs/1303.4645.
[40]	Zhou, Zhengyuan, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, Peter W. Glynn. 2017.
Stochastic mirror descent for variationally coherent optimization problems. NIPS ’17: Proceedings of the
31st International Conference on Neural Information Processing Systems.
[41]	Zinkevich, Martin. 2003. Online convex programming and generalized infinitesimal gradient ascent. ICML
’03: Proceedings of the 20th International Conference on Machine Learning. 928-936.
11
Published as a conference paper at ICLR 2020
A RiEManN-LipscHiTz continuity
In this appendix, our main goal is to prove Proposition 1, i.e., the equivalence between (7) and (RLC)
when f is differentiable.
To do so, we first need to introduce the notion of a geodesic, i.e., a length-minimizing curve that
attains the infimum infγ L[γ] over all piecewise smooth curves joining two points x1, x2 ∈ U. That
such a curve exists and is unique in our setting is a basic fact of Riemannian geometry [22]. Moreover,
given a tangent vector z ∈ Z, this leads to the definition of the exponential mapping exp: U × Z → U
so that
(x, z) 7→ expx(z) = γz(1),	(A.1)
where YZ denotes the unique geodesic emanating from X with initial velocity vector YZ = z. We then
have expx(tz) = γz (t) for all t and, moreover, for sufficiently small r > 0, the restriction of expx to a
ball of radius r in Z is a diffeomorphism onto its image in U . The largest positive number iX such
that the above holds for all r < iX is then known as the injectivity radius of U at X [22].
Our proof of the equivalence between (7) and (RLC) follows a simplified version of the approach of
Ferreira [14] who, to our knowledge, was the first to discuss the concept of proximal subgradients in
Riemannian manifolds. To that end, fix some X ∈ U , let Z = grad f (X) and consider the ray emanating
from X
Y(t) = expX(tZ/kZkX).	(A.2)
Since Y is a geodesic, we readily obtain distg(X, Y(t)) = t for all sufficiently small t. Also, by
construction, we have exp-X 1 Y(t) = tZ/kZkX. Hence, with f convex, it follows that, for some constant
α > 0 and for sufficiently small positive δ < iX, we have
f (Y(t)) - f(X) ≥ hZ, exp-X 1(Y(t))i	(A.3)
- α distg(X, Y(t))2,	(A.4)
where we used the local topological equivalence of the Riemannian topology and the standard
topology of ’d, and the fact that expZ (t) is a diffeomorphism for sufficiently small δ > 0 - and hence,
for all δ < iX . Thus, if f is also Riemann-Lipschitz continuous in the sense of (7), we will also have
Gt≥ f (Y(t)) - f(X) ≥ hZ, tZ/kZkXiX - αt2.	(A.5)
Thus, by isolating the leftmost and rightmost hand sides, dividing by t, and taking the limit t → 0, we
get
kgradf(X)kX= kZkX ≤ G,	(A.6)
as was to be shown.
To establish the converse, assume that (RLC) holds, fix X, X0 ∈ X pick K > G and a sufficiently small
δ > 0, and consider the Riemannian distance majorant w(X) = distg(X, X0) if distg(X, X0) < δ, and
W(X) = K distg(x, x0) + ε2∕(δ - ε) when δ < distg(x, X0) < 2δ, with ε = distg(x, XZ) - δ.
A simple calculation then shows that kgrad w(X)kX ≥ K > G. It is also straightforward to show that
the minimum of f + w is attained at X0 , so
f (X0) = f (X0) + w(X0)
≤ f (X) + w(X)
≤ f(X) + K distg(X, X0).	(A.7)
Then, by interchanging X and X0 above, we obtain | f (X) - f(X0)| ≤ K distg(X, X0). Since K > G has
been chosen arbitrarily, (7) follows.
B PropErtiEs of Mirror Mappings and tHE FEncHEl coupling
We begin by recalling and clarifying some of the notational conventions used in the paper. First, let
V 兰 ’d be a finite-dimensional real space; then, its dual space will be denoted by Y ≡ V*, and We
write hy|X〉for the duality pairing between y ∈ Y and X ∈ V. Also, if ∣∣∙∣∣ is a norm on V, the dual
norm on Y is defined as kyk∣* ≡ sup{〈y|X〉: kXk ≤ 1}.
12
Published as a conference paper at ICLR 2020
Given an extended-real-valued convex function f : X → ’ ∪ {∞}, we will write dom f ≡ {x ∈
V : f (x) < ∞} for its effective domain. The subdifferential of f at x ∈ dom f is then defined as
∂f(x) ≡ {y ∈ Y : f (x0) - f(x) + hy|x0 - xi ≥ 0 for all x0 ∈ V} and the domain of subdifferentiability
of f is dom∂f ≡ {X ∈ dom f : ∂f , 0}. Finally, assuming it exists, the directional derivative of f
at x along z ∈ V is defined as f0(x; z) ≡ d/dt|t=0 f (x + tz). We will then say that f is differentiable
at x ∈ dom f if there exists Vf (X) ∈ Y such that〈Vf (x)|Z = f 0(X; Z) for all vectors of the form
z= X0 - X, X0 ∈ dom f.
With these notational conventions at hand, we proceed to prove some auxiliary results and estimates
that are used throughout the analysis of Section 5. To recall the basic setup, we assume throughout
what follows that h is a Riemannian regularizer in the sense of Definition 2. The convex conjugate
h*: Y → ' of h is then defined as
h * (y) = sup{hy∣ x i-h (x)}.	(b.1)
X∈X
Since h is K-strongly convex relative to g, it is also strongly convex relative to the Euclidean norm
(recall here that g(X) < μI). As a result, the supremum in (B.1) is always attained, and h*(y) is finite
for all y ∈ Y [4]. Moreover, by standard results in convex analysis [33, Chap. 26], h* is differentiable
on Y and its gradient satisfies the identity
Vh*(y) = argmax{〈y|Xi - h(X)}.	(B.2)
X∈X
Thus, recalling the definition of the mirror map Q : Y → X (cf.. Section 4):
Q(y) = arg max{〈y|Xi - h(X)},	(B.3)
X∈X
we readily get
Q(y) = Vh*(y).	(B.4)
Together with the prox-mapping induced by h, all these notions are related as follows:
Lemma B.1. Let h be a Riemannian regularizer on X. Then, for all X ∈ dom ∂h and all y, v ∈ Y, we
have:
a)	x = Q(y)	u⇒ y ∈ ∂h(X).	(B.5a)
b)	X+ = Q(Vh(X) + v) u⇒ Vh(X) + V ∈ ∂h(X+)	(B.5b)
Finally, ifX = Q(y) and p ∈ X, we have
〈Vh(X)|X - pi ≤ 〈y|X - pi.	(B.6)
Remark. Note that (B.5b) directly implies that ∂h(X+) , 0, i.e., X+ ∈ dom ∂h for all v ∈ Y. An
immediate consequence of this is that the update rule X+ = Q(Vh(X) + v) is well-posed, i.e., it can be
iterated in perpetuity.
Proof of Lemma B.1. To prove (B.5a), note that X solves (B.2) if and only if y - ∂h(X) 3 0, i.e., if
and only if y ∈ ∂h(X). Eq. (B.5b) is then obtained in the same manner.
For the inequality (B.6), it suffices to show it holds for all P ∈ X◦三 dom ∂h (by continuity). To do
so, let
φ(t) =	h(X +	t(p	- X))	-	[h(X)	+ 〈y|X	+ t(p	-	X)i].	(B.7)
Since h is strongly convex relative to g and y ∈ ∂h(X) by (B.5a), it follows that φ(t) ≥ 0 with equality
if and only if t = 0. Moreover, note that ψ(t) = 〈Vh(X+ t(p -X)) -y|p - Xi is a continuous selection of
subgradients of φ. Given that φ and ψ are both continuous on [0, 1], it follows that φ is continuously
differentiable and φ0 = ψ on [0, 1]. Thus, with φ convex and φ(t) ≥ 0 = φ(0) for all t ∈ [0, 1], we
conclude that φ0(0) = 〈Vh(X) - y|p - Xi ≥ 0, from which our claim follows.
As we mentioned earlier, much of our analysis revolves around a ”primal-dual” divergence between
a target point p ∈ X and a dual vector y ∈ Y, called the Fenchel coupling. Following [26], this is
defined as follows for all p ∈ X, y ∈ Y:
Φ(p, y) = h(p) + h*(y) - 〈y|pi.	(B.8)
The following lemma illustrates some basic properties of the Fenchel coupling:
13
Published as a conference paper at ICLR 2020
(B.11)
(B.12)
Lemma B.2. Let h be a Riemannian regularizer on X with convexity modulus K. Then, for all p ∈ X
and all y ∈ Y, we have:
1.	Φ(p, y) = D(P, Q(y)) if Q(y) ∈ X◦ (but not necessarily otherwise).
2.	Ifx = Q(y), then Φ(P, y) ≥ K kX - Pk2
Proof. For our first claim, let x = Q(y). Then, by definition we have:
Φ(P, y) = h(P) - hy|Q(y)i - h(Q(y)) - hy| Pi = h(P) - h(x) - hy|P - xi.	(B.9)
Since y ∈ ∂h(x), We have h'(x; P - x) = hy|p - X〉whenever X ∈ X◦, thus proving our first claim. For
our second claim, working in the previous spirit we get that:
Φ(P, y) = h(P) - h(x) - hy|P - x〉	(B.10)
Thus, we obtain the result by recalling the strong convexity assumption for h with respect to the
Riemannian norm ∣∣∙kX.	■
We continue with some basic relations connecting the Fenchel coupling relative to a target point
before and after a gradient step. The basic ingredient for this is a primal-dual analogue of the so-called
“three-point identity” for Bregman functions [12]:
Lemma B.3. Let h be a regularizer on X. FiX some P ∈ X and let y, y+ ∈ Y. Then, letting X = Q(y),
we have
Φ(P, y+) = Φ(P, y) + Φ(X, y+) + hy+ - y|X - P〉.
Proof. By definition, we get:
Φ( p ,y+) = h (p) + h * (y+) -hy+l P〉
Φ(p, y) = h(p) + h* (y) - hylP〉.
Then, by subtracting the above we get:
Φ( p, y+) - Φ( p, y) = h (p) + h *(y+) - hy+l P〉- h (p) - h *(y) + hyl P〉
=h *(y+) - h *(y) - hy+ - yl P〉
= h*(y+) - hylQ(y)〉 + h(Q(y)) - hy+ - ylp〉
= h*(y+) - hylX〉 + h(X) - hy+ - ylp〉
= h*(y+) + hy+ - ylX〉 - hy+lX〉 + h(X) - hy+ - ylp〉
= Φ(X,y+) + hy+ - ylX-p〉
and our proof is complete.
With all this at hand, we have the following key estimate:
Proposition B.1. Let h be a Riemannian regularizer on X with conveXity modulus K, fiX some p ∈ X,
let X = Q(y) for some y ∈ Y. Then, for all v ∈ Y, we have:
Φ(p,y + v) ≤ Φ(p,y) + hv∣x-P〉+ ɪkv∣l2,*	(b.14)
2K
Proof. By the three-point identity (B.11), we get
Φ(p,y) =	Φ(p,y +	v)	+	Φ(Q(y	+ v), y) + hy -	(y + v)lQ(y	+ v)	-p〉	(B.15)
and hence, after rearranging:
Φ(p,y + v) = Φ(p, y) - Φ(Q(y + v), y) + hvlQ(y + v) -p〉
= Φ(p, y) - Φ(Q(y + v), y) + hvlx - p〉 + hvlQ(y + v) -x〉	(B.16)
By Young’s inequality [33], we also have
K1
hv|Q(y + V) - X〉≤ ^ykQ(y + V) - XkX + y^kvkX,*	(B.17)
2	2K
Our claim then follows by the fact that Φ(Q(y + v),y) ≥ K kQ(y + v) - Xk2 (cf. Lemmas 1 and B.2). ■
(B.13)
14
Published as a conference paper at ICLR 2020
C Analysis of FTRL
Our goal here is to prove the regret bound (22a) of Theorem 1. The starting point of our analysis is
the following basic bound:
Lemma C.1 (35, Lemma 2.3). The sequence of actions generated by (FTRL) satisfies
T
Regx(T) ≤ h(x) - h(X1) +	[ft(Xt) - ft(Xt+1)].	(C.1)
t=1
Importantly, the above bound does not require any Lipschitz continuity or strong convexity assump-
tions, so it applies to our setting “as is”. The importance of Riemann-LiPschitz continuity lies in the
following:
Lemma C.2. Iff is convex and Riemann-Lipschitz continuous with constant G, then:
f (X) - f (XZ) ≤ GIl X 0-X k X forallx, x 0 ∈ X °.	(C.2)
Proof. By the convexity of f, we have:
f (X) - f (X0) ≤〈Vf (X)|X - X0i = hgrad f (x), X -X'〉X
≤ Igradf(X)IXI X - X0IX
≤ GI X - X0I X	(C.3)
where the first line follows from the definition of the Riemannian gradient of f, the second one from
the Cauchy-Schwartz inequality, and the last from Proposition 1.
With these preliminary results at hand, we obtain the following basic bound for FTRL:
Proposition C.1. Suppose that (FTRL) is run against a sequence of loss conveX loss functions with
assumptions as in Section 5. Then, for all X ∈ X, we have:
T
Regx(T) ≤ T + 2γ X G	(C.4)
Proof. Our proof is patterned after Shalev-Shwartz [35], but with an important difference regarding
the use of Riemannian norms and Riemann-Lipschitz continuity. To begin, let
h(X)
Ft (X) = 2 fs (X) + q	(C.5)
s=1	γ
denote the “cumulative” loss faced by the optimizer up to roun t - 1, including the regularization
penalty. By the definition of the FTRL policy, Xt ∈ arg min Ft(X), so 〈VFt(Xt)|X - Xt〉 ≥ 0 and,
likewise, 〈VFt+1(Xt+1)|X -Xt+1〉 ≥ 0 for all X ∈ X. Furthermore, since h is K-strongly convex relative
to g, Ft and Ft +ι will be (K/γ)-strongly convex relative to g.
Putting all this together, we obtain:
Ft(Xt +ι) ≥ Ft(Xt) + KkXt +ι - XtkX+1	(C.6a)
2γ	t+1
Ft +1(Xt) ≥ Ft +1(Xt +1) + KkXt - Xt +ι kX,	(C.6b)
2γ	t
and hence, after summing the above inequalities:
KK
ft(Xt) - ft(Xt +1) ≥ 2γ 11Xt +1 - Xt11Xt +1 + 2γ 11Xt - Xt +1kXt
≥ K k Xt +1 - XtkX,.	(C.7)
2γ	t
On the other hand, Lemma C.2 gives
ft(Xt)-ft(Xt+1) ≤ GtkXt - Xt+1kXt	(C.8)
15
Published as a conference paper at ICLR 2020
so,	combining the last two inequalities, we get:
K
b 1因 +1- XtkXt ≤ Gt.	(C.9)
2γ	t
Therefore, plugging this back into (C.8) yields
ft(Xt) - ft(Xt +ι) ≤ 2YG2,	(C.10)
K
and our result obtains from Lemma C.1.
The proof of (22a) then follows by applying Proposition C.1 with a step-size of the prescribed form.
D Ergodic analysis of OMD
This appendix is devoted to the proof of our main regret bound for (OMD). We begin by recalling
ther recursive definition of the (lazy) OMD method:
Yt+1
Xt+1
= Yt - γvt
= Q(Yt+1)
(OMD)
with Q defined as in Appendix B and oracle feedback subject to the hypotheses (17). We may then
write the oracle feedback received by the optimizer at time t as Vt = V f (Xt) + Ut +1； hence, by the
unbiasedness assumption (17a), it follows that …[Ut +1 | Ft] = 0, i.e., Ut is a martingale difference
sequence (MDS) relative to Ft .
Now, applying Proposition B.1 to (OMD), we get:
Y2
Φ(X*, Yt +1) ≤ φ(X*, Yt) - γhvt∖Xt - X*i + 2γκkvVtkXt,*
γ2
Φ(x*, Yt) + YhVft(Xt)|x* - Xti- YhUt +1∣Xt-X*〉+ 2γκkVtkXt,*
Hence, after rearranging and telescoping, we obtain
T	TT
Reg(T) ≤ XhVft(Xt)|Xt - X*i ≤	(X ,”)+ X ξt +1 + 2K XkvtkXt,*
t=1	Y	t=1	t=1
(D.1)
(D.2)
where, in the last line, we used the definition of the Riemannian dual norm ∣∣∙∣∣* ≡ ∣∣∙∣∣X*,*, and we set
ξt+1 = hUt+1∖X* - Xt〉. Our result then follows by taking expectations on both sides.
E Last-iterate analysis of OMD
In this last section, we will present the convergence analysis for the last iterate of (OMD) to arg min f.
We begin by recalling two important results from probability theory. The first is a version of the law
of large numbers for martingale difference sequences that are bounded in L2 [15]:
Theorem E.1. Let Yt = Pti=1 ζi be a martingale and βt a non-decreasing positive sequence such that
limt→∞ βt = ∞. Then,
lim Yt /βt = 0 almost surely	(E.1)
t→∞
on the set P∞=1 β-2 …[42 ∖ Ft-1] < ∞.
The second is a convergence result for quasi-supermartingales due to Robbins and Sigmund [32]:
Lemma E.1. Let (Ft)ten be a non-decreasing sequence of σ- algebras. Let (αt)t∈n, (θt)t∈n non-
negative Ft一 measurable random variables, (ηt)ten is an Ft一 measurable non-negative SummabIe
random variable and the following inequality holds:
…[at +1 ∖ Ft] ≤ αt - θt + ηt almost SUrely	(E.2)
Then, (αt方∈n converges almost SUrely towards a [0, ∞)-valued random variable.
16
Published as a conference paper at ICLR 2020
An application of this lemma leads us to the following result which is of independent interest:
Proposition E.1. Let Xt be the sequence of iterates generated by (OMD) run with a step-size
sequence γt such that Pt∞=1 γt2 < ∞ and a stochastic oracle as in the statement of Theorems 2 and 3.
Then, for all x* ∈ arg min f, Φ(X*, Yt) converges with probability L
Proof. Let X* ∈ arg min f. Recalling our main estimation:
γ2
Φ(X*, Yt +1) ≤ Φ(X*, Yt) - γthvt∣Xt - X*〉x + 2kkvtk。	(E.3)
and taking conditional expectations on both sides, we get due to Ft - measurability arguments:
E[Φ(x*, Yt +1)∣Ft] ≤ Φ(x*, Yt) - Ythvt∣Xt - x*ix + γK …口也唳,*|Ft].	(E.4)
Since, (2K)-1 P∞=1 Y2 …[∣∣vtIlXt,*|Ft] ≤ M(2K)-1 P∞=1 Y2 < ∞. Thus, by applying the above We get
the result.
Having this at hand, We can establish the folloWing proposition:
Proposition E.2. Let Xt be the sequence of iterates generated by (OMD) with assumptions as in
Theorem 3. Then, for all x* ∈ arg min f, the sequence IXt - x*IXt is bounded with probability 1.
Proof. Recalling our main estimation and taking condition expectations on both sides, We get:
Y2
E[Φ(x*, Yt +1) |	Ft]	≤	Φ(x*, Yt) -	Ythv11Xt -	x*ix	+	2K	E[kvtkXt,*∣Ft]	(E.5)
Hence, by the above corollary, We have that the sequence Φ(x*, Yt) converges With probability 1 for
all x* ∈ arg min f. Thus, it is also bounded With probability 1 for all x* . We then get
2
kXt - X*kXt ≤ KΦ(X*, Yt)	(E.6)
Which concludes our proof.
We continue by shoWing that Xt possesses a subsequence that converges to arg min f:
Proposition E.3. Let Xt be the sequence of iterates generated by (OMD) with assumptions as in
Theorem 3. Then, with probability 1, there exists a (possibly random) subsequence of Xt which
converges to arg min f.
Proof. Assume to the contrary that, With positive probability, the sequence Xt generated by (OMD)
admits no limit points in arg min f. Conditioning on this event, there exists a (nonempty) closed set
C ⊂ X Which is separated by neighborhoods from arg min f and is such that Xt ∈ C for all suffiently
large t. Then, by relabeling Xt if necessary, We can assume Without loss of generality that Xt ∈ C for
all t ∈ N. Thus, by Proposition B.1, we get:
Y2
Φ(X*, Yt +1) ≤ Φ(X*, Yt) - Ythvt∖Xt - X*i + 2KkvtkXt,*
Y2
=Φ(X*, Yt) - Yt〈Vf (Xt)∖Xt - X*〉- YthUt+1∖Xt - X*〉+ 2KkvtkXt,*
Y2
≤ Φ(X*, Yt) - Ytδ(C) + Ytξt +1 + 2KkvtkXt,*	(E.7)
where in the last line we set δ(C) = inf{hVf(X)∖X - X*〉 : X* ∈ arg min f, X ∈ C} > 0 (by (SI)),
Ut+1 = vt - V f (Xt), ξt+1 = -hUt+1∖Xt - X*〉 and βt = Pit=1 Yi. Thus, by telescoping and factorizing we
get:
Φ(X*, Yt +1) ≤ Φ(X*, Y1) -βJδ(C) - PS=1 Yξξ+1 - PS=1 ；kvSkkX,* 1	(E.8)
βt	2Kβt
17
Published as a conference paper at ICLR 2020
By the unbiasedness assumption for Ut, We have …[ξt+1 | Ft]=〈…[Ut +1 | Ft]|Xt - X*〉= 0. Moreover,
for all x* ∈ arg min f, we have
∞∞	∞
X Y2 E[ξt +1 IFt ] ≤ X Y2k Xt - x *kXt E[ Ut +1 IFt ] ≤ X γ2Φ(x *, Yt) E[ Ut +1 | Ft ] < ∞	(E.9)
t=1	t=1	t=1
where the last (strict) inequality is obtained due to the finite mean square property, the boundness
of Φ( x*, Yt) and the fact that Pt∞=1 Yt2 < ∞. Thus, we can apply the law of large numbers for L2-
martingales stated above and conclude that βt-1 Pts=1 Ysξs+1 converges to 0 almost surely. On the other
hand, for the term St+1 = Pts=1 Y2s kvsk2X,*, since vs+1 is Fs-measurable for all s = 1, 2 . . . , t - 1 we
have:
. t T	]
E[ St +1 IFt ]=…2γ2kv 岐* + γ2kvtkXt,* Ft = St + Y Ehkv tkXt,* ∣ 司 ≥ St (E.10)
- i =1	J
so St is a submartingale with respect to Ft. Furthermore, by the law of total expectation, we also get:
t∞
E[St +1] = E[E[St +1	I	F,]]	≤ σ2 X Y:	≤	σ2	X YY	<	∞,	(E.11)
i=1	t=1
implying that St is bounded in L1. Thus, due to Doob’s submartingale convergence theorem [15], we
coclude that St converges to some (almost surely finite) random variable S ∞ so limt→∞ S+ = 0 with
probability 1.
Now, by letting t → ∞ in (E.8), we get Φ( x*, Yt) → -∞, a contradiction. Going back to our original
assumption, this shows that there exists a subsequence of Xt which converges to arg min f with
probability 1, as claimed.
With all this at hand, we proceed to the proof of our last-iterate convergence result:
Proof of Theorem 3. By the boundedness (and hence compactness) of arg min f, Proposition E.3
implies that, with probability 1, there exists some x* ∈ arg min f such that Xtk → x* for some (possibly
random) subsequence Xtk of Xt. By the Riemann-Legendre property of h, it follows that Φ(X*, Ytk)=
D( x*,Xtk) → 0 as k → ∞, implying in turn that limt→∞ D( x*,Xt) = 0 (by Proposition E.1). Since
D(x*, Xt) ≥ KkXt - x*kX ≥ μkXt - x*k2, we conclude that Xt → x*, and our proof is complete. ■
F Applications to Poisson inverse problems and numerical experiments
F.1 Detailed statement of the problem
The class of Poisson inverse problems that we consider stem from linear systems of the form
u = Hx+z	(F.1)
where
•	x ∈ ’d+ is the object under study (a signal, image, . . . ).
•	u ∈ ’+m+ is the observed data (usually m	d).
•	The kernel matrix H ∈ ’m×d is a representation of the data-gathering protocol and is
typically highly ill-conditioned (e.g., a Toeplitz matrix in the case of image deconvolution
problems).
•	z ∈ ’m is the noise affecting the measurements.
When data points are obtained by means of a counting process, measurements can be modeled
as Poisson random variables of the form Uj 〜Pois(Hx)j. Then, up to an additive constant, the
log-likelihood of x ∈ ’d given an observation u ∈ ’+m+ will be
L(x; u) = -
jlog
uj
(Hx)j
+ (H x) j - u j .
(F.2)
18
Published as a conference paper at ICLR 2020
Hence, obtaining a maximum likelihood estimate for x leads to the archetypal Poisson inverse problem
minimize f (x) ≡ DKL(u, Hx),
subject to x ∈ ’d+,
(PIP)
where DKL(p, q) = Pmj=1[pj log(pj/qj) + qj - pj] denotes the generalized KL divergence on ’+m. For
an extensive review of Poisson inverse problems, we refer the reader to Bertero et al. [7].
In many cases of practical interest, measurements arrive in distinct batches over time - e.g., as
sequential optical sections in microscopy and tomography. Moreover, due to the large numbers of
pixels/voxels involved (a typical range of values for m is between 106 and 107), gradients of f are
very costly to compute; as such, optimization methods that rely on accurate gradient data are difficult
to apply in this setting. Accordingly, a natural workaround to this obstacle is to exploit the online
nature of the measurement process, model (PIP) as an online optimization problem, and then to use
an online-to-batch conversion to get a candidate solution [35].
On the downside, this online optimization analysis crucially requires the loss functions faced by
the optimizer to be Lipschitz continuous. However, this assumption does not hold for (PIP): if
fj(x) = -uj log(uj/(Hx)j) denotes the singular part of the KL divergence for the j-th sample, we
readily get
∂fj = UHji
∂ Xi (Hx) j
(F.3)
This shows that the gradient offj exhibits an O(1/x) singularity at the boundary of ’d+, so f cannot
be Lipschitz under any global norm on ’d .
As suggested by Example 3, this singularity can be lifted by considering the local norm
In this case, we have
kW∣X = (x 1 + …+ Xd)2 Pd=1 Z for all V ∈ Rd.
d
H f(x)k2 = 2
i=1
2 H2jixi2	u2j Pid=1 H2jixi2
U ：------ =
(Hx)2	[P d=1 HjiXif
= O(U2j ),
(F.4)
(F.5)
so IlVfjkχ is bounded under this modified norm. This is the principal motivation for considering the
Riemannian mirror descent method defined with respect to this metric and the regularizer presented
in Example 5.
F.2 Details on the experiments
In the rest of this appendix, we discuss in more detail the algorithms tested in Section 6. The
algorithms we considered are
1.	The accelerated LUcy-RichardSon algorithm, as presented in [7] and corresponding to OMD
with the entropic regularizer h(X) = Pi Xi log Xi.
2.	The composite mirror prox (CMP) of He et al. [18], corresponding to OMD with an extra
gradient step and the log-barrier (Burg) regularizer h(X) = - Pi log Xi of Example 4.
3.	The Riemannian mirror descent (RMD) algorithm detailed in Section 6, corresponding to
the POinCare-Iike regularizer of Example 5.
All algorithms were run with stochastic gradients drawn with the same minibatch size (n = 256) and
a step-size of the form γt X 1/ √t (corresponding to the stochastic variant of each algorithm). For
comparison purposes, we harvested each algorithm’s last generated sample (“last iterate”) as well as
the corresponding ergodic average (defined here as XT = P3 YtXtl PT=I Yt). Overall, the algorithms'
last generated sample provided consistently better results than the ergodic average. The ground truth
and the evolution of the Poisson likelihood loss are all reported in Fig. 1.
Remark. We should note here that the method of He et al. [18] can be seen as an “extra-gradient”
version of the NoLips algorithm of Bauschke et al. [3] and the “relative stochastic gradient descent”
scheme of Hanzely and Richtarik [16] (the difference between the last two being the step-size policy).
In our experiments, Burg mirror descent with and without an extra-gradient step behaved similarly,
with the extra-gradient version (CMP) performing slightly better. To minimize clutter, and because we
are already comparing RMD to CMP above, we do not report this extra set of numerical experiments.
19