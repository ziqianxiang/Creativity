Published as a conference paper at ICLR 2020
A Latent Morphology Model for Open-
Vocabulary Neural Machine Translation
Duygu Ataman*
University of Zurich
ataman@cl.uzh.ch
Wilker Aziz
University of Amsterdam
w.aziz@uva.nl
Alexandra Birch
University of Edinburgh
a.birch@ed.ac.uk
Ab stract
Translation into morphologically-rich languages challenges neural machine
translation (NMT) models with extremely sparse vocabularies where atomic
treatment of surface forms is unrealistic. This problem is typically addressed by
either pre-processing words into subword units or performing translation directly
at the level of characters. The former is based on word segmentation algorithms
optimized using corpus-level statistics with no regard to the translation task. The
latter learns directly from translation data but requires rather deep architectures.
In this paper, we propose to translate words by modeling word formation through
a hierarchical latent variable model which mimics the process of morphological
inflection. Our model generates words one character at a time by composing two
latent representations: a continuous one, aimed at capturing the lexical semantics,
and a set of (approximately) discrete features, aimed at capturing the morphosyn-
tactic function, which are shared among different surface forms. Our model
achieves better accuracy in translation into three morphologically-rich languages
than conventional open-vocabulary NMT methods, while also demonstrating a
better generalization capacity under low to mid-resource settings.
1	Introduction
Neural machine translation (NMT) models are conventionally trained by maximizing the likelihood
of generating the target side of a bilingual parallel corpus of observations one word at a time condi-
tioned of their full observed context. NMT models must therefore learn distributed representations
that accurately predict word forms in very diverse contexts, a process that is highly demanding in
terms of training data as well as the network capacity. Under conditions of lexical sparsity, which
includes both the case of unknown words and the case of known words occurring in surprising con-
texts, the model is likely to struggle. Such adverse conditions are typical of translation involving
morphologically-rich languages, where any single root may lead to exponentially many different
surface realizations depending on its syntactic context. Such highly productive processes of word
formation lead to many word forms being rarely or ever observed with a particular set of mor-
phosyntactic attributes. The standard approach to overcome this limitation is to pre-process words
into subword units that are shared among words, which are, in principle, more reliable as they are
observed more frequently in varying context (Sennrich et al., 2016; Wu et al., 2016). One draw-
back related to this approach, however, is that the estimation of the subword vocabulary relies on
word segmentation methods optimized using corpus-dependent statistics, disregarding any linguistic
notion of morphology and the translation objective. This often produces subword units that are se-
mantically ambiguous as they might be used in far too many lexical and syntactic contexts (Ataman
et al., 2017). Moreover, in this approach, a word form is then generated by prediction of multiple
subword units, which makes generalizing to unseen word forms more difficult due to the possibility
that a subword unit necessary to reconstruct a given word form may be unlikely in a given context.
To alleviate the sub-optimal effects of using explicit segmentation and generalize better to new mor-
phological forms, recent studies explored the idea of extending NMT to model translation directly at
*Work done while the first author was a doctoral student at the University of Trento and a visiting post-
graduate student at the University of Edinburgh.
1
Published as a conference paper at ICLR 2020
the level of characters (Kreutzer & Sokolov, 2018; Cherry et al., 2018), which, in turn, have demon-
strated the requirement of using comparably deeper networks, as the network would then need to
learn longer distance grammatical dependencies (Sennrich, 2017).
In this paper, we explore the benefits of explicitly modeling variation in surface forms of words
using techniques from deep latent variable modeling in order to improve translation accuracy for
low-resource and morphologically-rich languages. Latent variable models allow us to inject induc-
tive biases relevant to the task, which, in our case, is word formation during translation. In order
to formulate the process of morphological inflection, design a hierarchical latent variable model
which translates words one character at a time based on word representations learned composition-
ally from sub-lexical components. In particular, for each word, our model generates two latent
representations: i) a continuous-space dense vector aimed at capturing the lexical semantics of the
word in a given context, and ii) a set of (approximately) discrete features aimed at capturing that
word’s morphosyntatic role in the sentence. We then see inflection as decoding a word form, one
character at a time, from a learned composition of these two representations. By forcing the model
to encode each word representation in terms of a more compact set of latent features, we encour-
age them to be shared across contexts and word forms, thus, facilitating generalization under sparse
settings. We evaluate our method in translating English into three morphologically-rich languages
each with a distinct morphological typology: Arabic, Czech and Turkish, and show that our model is
able to obtain better translation accuracy and generalization capacity than conventional approaches
to open-vocabulary NMT.
2	Neural Machine Translation
In this paper, we use recurrent NMT architectures based on the model developed by Bahdanau et al.
(2014). The model essentially estimates the conditional probability of translating a source sequence
x = hx1, x2, . . . xmi into a target sequence y = hy1, y2, . . . yli via an exact factorization:
l
p(y|x, θ) =	p(yj |x, y<i, θ)	(1)
i=1
where y<i stands for the sequence preceding the ith target word. At each step of the sequence,
a fixed neural network architecture maps its inputs, the source sentence and the target prefix, to
the probability of the ith target word observation in context. In order to condition on the source
sentence fully, this network employs an embedding layer and a bi-directional recurrent neural
network (bi-RNN) based encoder. Conditioning on the target prefix y<i is implemented using a
recurrent neural network (RNN) based decoder, and an attention mechanism which summarises
the source sentence into a context vector ci as a function of a given prefix (Luong et al., 2015).
Given a parallel training set D, the parameters θ of the network are estimated to attain a local
minimum of the negative log-likelihood function L(θ∣D) = - Px y〜D logp(y∣x, θ) via stochastic
gradient-based optimization (Bottou & Cun, 2004).
Atomic parameterization estimates the probability of generating each target word yi in a single
shot:
p(yilx,y<i,θ) = Pv=⅛⅛),
(2)
where E ∈ Rv×d is the target embedding matrix and the decoder output hi ∈ Rd represents x
and y<i . Clearly, the size v of the target vocabulary plays an important role in determining the
complexity of the model, which creates an important bottleneck when translating into low-resource
and morphologically-rich languages due to the sparsity in the lexical distribution.
Recent studies approached this problem by performing NMT with subword units, a popular one of
which is based on the Byte-Pair Encoding algorithm (BPE; Sennrich et al., 2016), which finds the
optimal description of a corpus vocabulary by iteratively merging the most frequent character se-
quences. Atomic parameterization could also be used to model translation at the level of characters,
which is found to be advantageous in generalizing to morphological variations (Cherry et al., 2018).
2
Published as a conference paper at ICLR 2020
Hierarchical paramaterization further factorizes the probability of a target word in context:
li
p(yi|x, y<i, θ) =	p(yi,j |x, y<i, yi,<j, θ)	(3)
j=1
where the ith word yi = hyi,1, . . . , yi,li i is seen as a sequence of li characters. Generation follows
one character at a time, each with probability computed by a fixed neural network architecture
with varying inputs, namely, the source sentence x, the target prefix y<i, and the prefix yi,<j of
characters already generated for that word. In this case there are two recurrent cells, one updated
at the boundary of each token, much like in the standard case, and another updated at the character
level. Luong & Manning (2016) propose hierarchical parameterization to compute the probability
p(yi|x, y<i, θ) for unknown words, while for known words they use the atomic parameterization. In
this paper, we use the hierarchical parameterization method for generating all target words, where
we also augment the input embedding layer with a character-level bi-RNN, which computes each
word representation yi as a composition of the embeddings of their characters (Ling et al., 2015).
3	A Latent Morphology Model (LMM) for Learning Word
Representations
The application of a hierarchical structure for learning word representations in language modeling
(Vania & Lopez, 2017) or semantic role labeling (Sahin & Steedman, 2018) have shown that such
representations encode many cues about the morphological features of words by establishing a map-
ping between phonetic units and lexical context. Although it can provide an alternative solution
to open-vocabulary NMT by potentially alleviating the need for subword segmentation, the quality
of word representations learned by an hierarchical model is still highly dependant on the amount
of observations (Sahin & Steedman, 2018; Ataman et al., 2019), since the training data is essential
in properly modeling the lexical distribution. On the other hand, the process of word formation,
particularly morphological inflection, has many properties that remain universal across languages,
where a word is typically composed of a lemma, representing its lexical semantics, and a distinct
combination of categorical inflectional features expressing the word’s syntactic role in the phrase
or sentence. In this paper, we propose to manipulate this universal structure in order to enforce an
inductive bias on the prior distribution of words and allow the hierarchical parameterization model
in properly learning lexical representations under conditions of data sparsity.
3.1	Generative model
Our generative LMM for NMT formulates word formation in terms of a stochastic process, where
each word is generated one character at a time by composing two latent representations: a continuous
vector aimed at capturing the lemma, and a set of sparse features aimed at capturing the inflectional
features. The motivation for using a stochastic model is twofold. First, deterministic models are by
definition unimodal: when presented with the same input (the same context) they always produce
the same output. When we model the word formation process, it is reasonable to expect a larger
degree of ambiguity, that is, for the same context (e.g. a noun prefix), we may continue by inflecting
the word differently depending on the (latent) mode of operation we are at (e.g. generating nomina-
tive, accusative or dative noun). Second, in stochastic models, the choice of distribution gives us a
mechanism to favour a particular type of representation. In our case, we use sparse distributions for
inflectional features to accommodate the fact that morphosyntactic features are discrete in nature.
Our latent variable model is an instance of a variational auto-encoder (VAE; Kingma & Welling,
2013) inspired by the model of Zhou & Neubig (2017) for morphological reinflection.
Generation of the ith word starts by sampling a Gaussian-distributed representation in context. This
requires predicting the Gaussian location ui and scale si vectors,1
Zi∣x,y<i 〜N(ui, diag(si Θ Si))
ui = dense(hi; θu)	(4)
si = ζ(dense(hi; θs))
1 Notation We use capital Roman letters for random variables (and lowercase letters for assignments).
Boldface Roman letters are reserved for neural network output vectors, and stands for elementwise multipli-
cation. Finally, we denote typical neural network layers as layer(inputs; parameters).
3
Published as a conference paper at ICLR 2020
Figure 1: LMM for computing word representations while translating the sentence ‘... went home’
into Turkish (‘eve-(to)home gitti(he/she/it)went’). The character-level decoder is initialized with
the attentional vector hi computed by the attention mechanism using current context ci and the word
representation ti as in Luong & Manning (2016).
where prediction of the location (in Rd) and scale (in Rd>0) from the word-level decoder hidden state
hi (which represents x and y<i) is performed by two dense layers, and the scale values are ensured
to be positive with the softplus (ζ) activation.2
Generation proceeds by then sampling a K -dimensional vector fi of sparse scalar features (see §3.2)
conditioned on the source x, the target prefix y<i, and the sampled lemma zi. We model sampling of
fi conditioned on zi in order to capture the insight that inflectional transformations typically depend
on the category of a lemma. Having sampled fi and zi , the representation of the ith target word is
computed by a transformation of zi and fi, i.e. ti = dense([zi , fi]; θcomp).
As shown in Figure 1, our model generates each word character by character auto-regressively by
conditioning on the word representation ti predicted by the LMM, the current context ci , and the
previously generated characters following the hierarchical parameterization.3 See Algorithm 1 for
details on generation.
Input: model parameters θ, latent lemma zi , latent morphological attributes fi , observed
character sequence hyi,1, . . . , yi,li i if training or placeholders if test, decoder state hi,
and context vector ci
Result: updated decoder hidden state, prediction (a word), probability of prediction (for loss)
initialization;
ti = dense([zi, fi]; θcomp) ;
initialize char-rnn with a projection of [ti , ci];
for j < li and j < max do
compute output layer from char-rnn state ;
if training then
I set prediction to observation yi,j∙;
else
I set prediction to arg max of output SoftmaX layer ;
end
assess log-probability of prediction ;
update word-level RNN decoder with prediction;
end
Algorithm 1: Word generation: in training the word is observed, thus we only update the de-
coder and assess the probability of the observation, in test, we use mean values of the distribu-
tions to represent most likely values for z and f and populate predictions with beam-search.
2In practice, we sample zi via a reparameterization in terms of a fiXed Gaussian, namely, zi = ui + i si
for Ei 〜N(0, Id). This is known as the reparameterization trick (Kingma & Welling, 2013), which allows
back-propagation through stochastic units (Rezende et al., 2014).
3Formally, because the decoder is an RNN, we are also conditioning on z<i and f<i. We omit this depen-
dence to avoid clutter.
4
Published as a conference paper at ICLR 2020
3.2	Sparse features
Since each target word yi may have multiple inflectional features, ideally, we would like fi to be
K feature indicators, which could be achieved by sampling from K independent Bernoulli distribu-
tions parameterized in context. The problem with this approach is that sampling Bernoulli outcomes
is non-differentiable, thus, their training requires gradient estimation via REINFORCE (Williams,
1992) and sophisticated variance reduction techniques. An alternative approach that has recently
become popular is to use relaxations such as the Concrete distribution or Gumbel-Softmax (Maddi-
son et al., 2017; Jang et al., 2017) in combination with the straight-through estimator (ST; Bengio
et al., 2013). This is based on the idea of relaxing the discrete variable from taking on samples in the
discrete set {0, 1} to taking on samples in the continuous set (0, 1) using a distribution for which a
reparameterization exists (e.g. Gumbel). Then, a non-differentiable activation (e.g. a threshold func-
tion) maps continuous outcomes to discrete ones. ST simply ignores the discontinuous activation
in the backward pass, i.e. it assumes the Jacobian is the identity matrix. This does lead to biased
estimates of the gradient of the loss, which is in conflict with the requirements behind stochastic
optimization (Robbins & Monro, 1951).
An alternative presented by Louizos et al. (2018) achieves a different compromise, it gets rid of bias
at the cost of mixing both sparse and dense outcomes. The idea is to obtain a continuous sample
c ∈ (0, 1) from a distribution for which a reparameterization exists and stretch it to a continuous
support (l, r) ⊃ (0, 1) using a simple linear transformation s = l + (r - l)c. A rectifier is then
employed to map the negative outcomes to 0 and the positive outcomes larger than one to 1, i.e.
f = min(1, max(0, s)). The rectifier is only non-differentiable at s = 0 and at s = 1, however,
because the stretched variable s is sampled from a continuous distribution, the chance of sampling
s = 0 and s = 1 is essentially 0. This stretched-and-rectified distribution allows: i) the sampling
procedure to become differentiable with respect to the parameters of the distribution, ii) to sample
sparse outcomes with an unbiased estimator, and iii) to calculate the probability of sampling f = 0
and f = 1 in closed form as a function of the parameters of the underlying distribution, which
corresponds to the probability of sampling s < 0 and s > 1, respectively.
In their paper, Louizos et al. (2018) used the BinaryConcrete (or Gumbel-Sigmoid) as the underlying
continuous distribution, the sparsity of which is controlled via a temperature parameter. However,
in our study, we found this parameter difficult to predict, since it is very hard to allow a neural
network to control its value without unstable gradient updates. Instead, we opt for a slight variant
by Bastings et al. (2019) based on the Kumaraswamy distribution (Kumaraswamy, 1980), a two-
parameters distribution that closely resembles a Beta distribution and is sparse whenever its (strictly
positive) parameters are between 0 and 1. In the context of text classification, Bastings et al. (2019)
shows this stretch-and-rectify technique to work better than methods based on REINFORCE.
For each token yi , we sample K independent Kumaraswamy variables in context,
Ci,k∣x,y<i,zi 〜Kuma(ai,k ,bi,k) k =1,...,K
[ai, bi] = ζ(dense([zi, hi]; θab))
(5)
which makes a continuous random vector ci in the support (0, 1)K.4 We then stretch-and-rectify
the samples via fi,k = min(1, max(0, l - (r - l)ci,k)) making fi a random vector in the support
[0, 1]K.5 The probability that fi,k is exactly 0 is
-l
{0} fE TZ / I T 、I
πi,k =	Kuma(c|ai,k, bi,k)dc
and the probability that fi,k is exactly 1 is
1-l
{1} T	∕*r-τ ，，	/ I	T 、I
πi,k = 1 -	Kuma(c|ai,k,bi,k)dc
and therefore the complement
π(0,1) = 1 - π{0} - π{1}
πi,k = 1 - πi,k - πi,k
(6a)
(6b)
(6c)
4In practice we sample ci,k via a reparameterization of a fixed uniform variable, namely, ci,k = (1 -
(1 — εi,k)1∕bi,k)1∕ai,k where εi,k 〜U(0,1), which much like the Gaussian reparameterization enables back-
propagation through samples (Nalisnick & Smyth, 2016).
5We use l = —0.1 and r = 1.1. Figure 2 in the appendix illustrates different instances of this distribution.
5
Published as a conference paper at ICLR 2020
is the probability that fi,k be any continuous value in the open set (0, 1). In §3.4, we will derive
regularizers based on πi(,0k,1) to promote sparse outcomes to be sampled with large probability.
3.3	Parameter estimation
Parameter estimation of neural network models is typically done via maximum-likelihood estimation
(MLE), where we approach a local minimum of the negative log-likelihood function via stochastic
gradient descent with gradient computation automated by the back-propagation algorithm. Using
the following shorthand notation:
α(zi) , p(zi |x, y<i, z<i, f<i, θ)
K
β(fi) ,	p(fi,k |x, y<i, z<i, f<i, zi, θ)
k=1
γ(yi) ,	p(yi,j |x, y<i, z≤i, f≤i,yi,<j,θ) .
j=1
The log-likelihood for a single data point can be formulated as:
log p(y|x, θ) = log
Z Yl
i=1
α(zi)β(fi)γ(yi)dzdf
(7a)
(7b)
(7c)
(8)
the computation of which is intractable. Instead, we resort to variational inference (VI; Jordan et al.,
1999), where we optimize a lower-bound on the log-likelihood
l
E	Xlc，： α(zi)β(fi)γ(yi)	(9)
Eq(Zf lx,y,l) [X log q(z,f ∣χ,λ) J	(9)
expressed with respect to an independently parameterized posterior approximation q(z, f|x, y, λ).
For as long as sampling from the posterior is tractable and can be performed via a reparameterization,
we can rely on stochastic gradient-based optimization. In order to have a compact parameterization,
we choose
l
q(z, f|x, y, λ) := Yα(zi)β(fi) .	(10)
i=1
This simplifies the lowerbound, which then takes the form of l nested expectations, the ith of which
is Eα(zi)β(fi) [log γ(yi)]. This is similar to the stochastic decoder of Schulz et al. (2018), though
our approximate posterior is in fact, also our parameterized prior.6 Although this objective does
not particularly promote sparsity, we employ sparsity-inducing regularization techniques that will
be discussed in the next section.
Concretely, for a given source sentence x, target prefix y<i, and a latent sample z≤i, f≤i, we obtain
a single-sample estimate of the loss by computing Li(θ) = - log γ(yi).
3.4	Regularization
In order to promote sparse distributions for the inflectional features, we apply a regularizer inspired
by expected L0 regularization (Louizos et al., 2018). Whereas L0 is a penalty based on the number
of non-zero outcomes, we design a penalty based on the expected number of continuous outcomes,
which corresponds to πi(,0k,1) as shown in Equation (6). For a given source sentence x, target prefix
y<i, and a latent sample z<i, f<i, we aggregate this penalty for each feature
K
Ri(θ) = X πi(,0k,1)	(11)
k=1
6This means we refrain from conditioning on the observation yi itself when sampling zi and fi . Whereas
this gives our posterior approximation access to less features than the true posterior would have, we do not
employ a fixed uninformative prior, but rather an autoregressive network trained on the likelihood of generated
word forms. This is a common approximation for latent variable models that employ autoregressive priors
(Goyal et al., 2017).
6
Published as a conference paper at ICLR 2020
and add it to the cost function with a positive weight ρ. The final loss of the NMT model is
|y|
L(θ∣D) = X XLi(θ) + ρRi(θ) .	(12)
x,y〜D i=1
3.5	Predictions
In our model, obtaining the conditional likelihood for predicting the most likely hypothesis requires
marginalisation of the latent variables, which is intractable. An alternative approach is to heuristi-
cally search through the joint distribution,
arg max p(y, z, f|x) ,	(13)
y,z,f
rather than the marginal, an approximation that has been referred to as Viterbi decoding (Smith,
2011). During beam search, we populate the beam with alternative target words, and for each
prefix y<i in the beam, we resort to deterministically choosing the latent variables based on a single
sample which we deem representative of their distributions, which is a common heuristic in VAEs
for translation (Zhang et al., 2016; Schulz et al., 2018). For unimodal distributions, such as the
Gaussian p(zi|x, y<i, z<i, f<i), we use the analytic mean, whereas for multimodal distributions,
such as the Hard Kumaraswamy p(fi|x, y<i, z≤i, f<i), we use the argmax.7
4	Evaluation
4.1	Models
We evaluate our model by comparing it in machine translation against three baselines which con-
stitute the conventional open-vocabulary NMT methods, including architectures using atomic pa-
rameterization either with subword units segmented with BPE (Sennrich et al., 2016) or characters,
and the hierarchical parameterization method employed for generating all words in the output. We
implement all architectures using Pytorch (Paszke et al., 2017) within the OpenNMT-py framework
(Klein et al., 2017)8.
4.2	Data and Languages
In order to evaluate our model we design two sets of experiments. The experiments in §4.4.1 aim to
evaluate different methods under low-resource settings, for languages with different morphological
typology. We model the machine translation task from English into three languages with distinct
morphological characteristics: Arabic (templatic), Czech (fusional), and Turkish (agglutinative).
We use the TED Talks corpora (Cettolo, 2012) for training the NMT models for these experiments.
In §4.4.3, we conduct more experiments in Turkish to demonstrate the case of increased data sparsity
using multi-domain training corpora, where we extend the training set using corpora from EU Book-
shop (SkadinS et al., 2014), Global Voices, Gnome, Tatoeba, Ubuntu (Tiedemann, 2012), KDE4
(Tiedemann, 2009), Open Subtitles (Lison & Tiedemann, 2016) and SETIMES (Tyers & Alperen,
2010)9. The statistical characteristics of the training sets are given in Tables 4 and 5. We use the
official evaluation sets of the IWSLT10 for validating and testing the accuracy of the models. In
order to increase the number of unknown and rare words in the evaluation sets we measure accuracy
on large test sets combining evaluation sets from many years (Table 6 presents the evaluation sets
used for development and testing). The accuracy of each model output is measured using BLEU
(PaPineni et al., 2002) and chrF3 (Popovic, 2015) metrics, whereas the significance of the improve-
ments are computed using bootstrap hypothesis testing (Clark et al., 2011). In order to measure the
accuracy in predicting the correct syntactic description of the references, we also compute BLEU
7We maximize across the three configurations of each feature, namely, max{πi{,0k}, πi{,1k} , πi(,0k,1)}. If πi(,0k,1)
is highest, we return the mean of the underlying Kumaraswamy variable.
8Our software is available at: https://github.com/d-ataman/lmm
9The size of the resulting combined corpora is further reduced to filter out noise and reduce the computa-
tional cost of the experiments using data selection methods (Cuong & Simaan, 2014).
10The International Workshop on Spoken Language Translation
7
Published as a conference paper at ICLR 2020
scores over the output sentences segmented using a morphological analyzer. We use the AlKhalil
Morphosys (BoUdchiche et al., 2017) for segmenting Arabic, Morphidata (Strakova et al., 2014) for
segmenting Czech and the morphological lexicon model of Oflazer (Oflazer & Kuruoz, 1994) and
disambigUation tool of Sak (Sak et al., 2007) for segmenting TUrkish sentences into seqUences of
lemmas and morphological features.
4.3	Training Settings
All models are implemented using gated recurrent units (GRU) (Cho et al., 2014), and have a single-
layer bi-RNN encoder. The source sides of the data used for training all NMT models, and the target
sides of the data used in training the subword-level NMT models are segmented using BPE with
16,000 merge rules. We implement all decoders using a comparable number of GRU parameters,
including 3-layer stacked-GRU subword and character-level decoders, where the attention is com-
puted after the 1st layer (Barone et al., 2017) and a 3-layer hierarchical decoder which implements
the attention mechanism after the 2nd layer. All models use an embedding dimension and GRU size
of 512. LMM uses the same hierarchical GRU architecture, where the middle layer is augmented
using 4 multi-layer perceptrons with 256 hidden units. We use a lemma vector dimension of 150, 10
inflectional features (See §A.3 for experiments conducted to tune the feature dimensions) and set the
regularization constant to ρ = 0.4. All models are trained using the Adam optimizer (Kinga & Ba,
2014) with a batch size of 100, dropout rate of 0.2, learning rate of 0.0004 and learning rate decay
of 0.8, applied when the perplexity does not decrease at a given epoch.11 Translations are generated
with beam search with a beam size of 5, where the hierarchical models implement the hierarchical
beam search algorithm (Ataman et al., 2019).
4.4	Results
4.4	. 1 The Effect of Morphological Typology
The experiment results given in Table 1 show the performance of each model in translating En-
glish into Arabic, Czech and Turkish. In Turkish, the most sparse target language in our benchmark
with rich agglutinative morphology, using character-based decoding shows to be more advantageous
compared to the subword-level and hierarchical models, suggesting that increased granularity in the
vocabulary units might aid in better learning accurate representations under conditions of high data
sparsity. In Arabic, on the other hand, using a hierarchical decoding model shows to be advanta-
geous compared to the subword and character-level models, as it might be useful in better learning
syntactic dependencies. LMM obtains improvements of 0.51 and 0.30 BLEU points in Arabic and
Turkish over the best performing baselines, respectively. The fact that our model can efficiently
work in both Arabic and Turkish confirms that it can handle the generation of both concatenative
and non-concatenative morphological transformations. The results in the English-to-Czech trans-
lation direction do not indicate a specific advantage of using either method for generating fusional
morphology, where morphemes are already optimized at the surface level, although our model is
still able to achieve translation accuracy comparable to the character and subword-level models.
4.4.2	Predicting Unseen Words
In addition to the general machine translation evaluation using automatic metrics, we perform a more
focused statistical analysis to illustrate the performance of different methods in predicting unseen
words by computing the average perplexity per character on the input sentences which contain out-
of-vocabulary (OOV) words as suggested by Cotterell et al. (2018). We also analyze the outputs
generated by each decoder in terms of the frequency of unknown words in each model output and
the Kullback-Leibler (KL) divergence between the character trigram distributions of the references
and outputs, which represents the coherence between the statistical distribution learned by each
model and the reference translations.
Our analysis results generally confirm the advantage of increased granularity during the generation
of unseen words, where the character-level decoder can generate a higher rate of unseen word forms
and higher KL-divergence with the reference, suggesting superior ability in generalizing to new
11Perplexity is the exponentiated average negative log-likelihood per segment (BPE, or character) that a
model assigns to a dataset. It corresponds to the model’s average surprisal per time step.
8
Published as a conference paper at ICLR 2020
Model	BLEU	AR t-BLEU	chrF3	(only in-domain) CS			BLEU	TR t-BLEU	chrF3
				BLEU	t-BLEU	chrF3			
SubWords	14.27	51.24	0.3927	16.60	5422^^	0.4123	8.52^^	38.03	0.3763
Char.s	12.72	47.56	0.3804	16.94	5280~-	0.4103	10.63	40.63	0.3810
Hierarch.	15.55	54.01	0.4154	16.79	48.27~~	0.4068	9.74^^	35.91	0.3771
-LMM-	16.06	55.97	0.4251	16.97	50:35	0.4095	10.93	45.47	0.3889
				(multi-domain)					
			Model		TR				
				BLEU	t-BLEU	chrF3			
			SubWords	10.42	42:65	0.3722			
			Char.s	8.94	37.12~~	0.3274			
			Hierarch.	10.35	40.54~~	0.3870			
			LMM	11.48	4823~~	0.3939			
Table 1: Above: Machine translation accuracy in Arabic (AR), Czech (CS) and Turkish (TR) in
terms of BLEU and ChrF3 metrics as well as BLEU scores computed on the output sentences tagged
with the morphological analyzer (t-BLEU) using in-domain training data. Below: The performance
of models trained with multi-domain data. Best scores are in bold. All improvements over the
baselines are statistically significant (p-value < 0.05).
output and not necessarily copying previous observations as the subword-level model, however, this
advantage is more visible in Turkish and less in Czech or Arabic. The hierarchical decoder which
performs the search at the level of words, on the other hand, behaves with less uncertainty in terms
of the perplexity values although it cannot demonstrate the ability to generalize to new forms and
neither can closely capture the actual distribution in the target language.
Due to its stochastic nature, our model yields higher perplexity values compared to the hierarchical
model, whereas the values range between subword and character-based models, possibly finding an
optimal level of granularity between the two solutions. The KL-divergence and OOV rates con-
firm that our model has the potential in better generalize to new word forms as well as different
morphological typology.
Model	OOV%	AR Ppl	KL-Div	OOV%	CS Ppl	KL-Div	OOV%	TR Ppl	KL-Div
SubWords	1.75	2.84	12,871	-239^^	2.62	^^8,954	3.54	2.78	17,342
Char.s	3.08	2.46	29,607	-190^^	2.61	17,092	4.28	2.38	38,043
Hierarch.	1.96	2.59	15,064	-087^^	2.65	29,022	1.53	2.46	68,743
-LMM-	3.78	2.68	9,892	-24~~	2.71	14,296	4.89	2.59	38,930
Table 2: Percentage of out-of-vocabulary (OOV) words in the output, normalized perplexity mea-
sures (PPl) per characters and the KL divergence between the reference and outputs of systems
trained with in-domain data on different language directions.
4.4.3	The Effect of Data Size
Repeating the experiments in the English-to-Turkish translation direction by increasing the amount
of training data with multi-domain corpora demonstrates a more challenging case, where there is
a greater possibility of observing new words in varying context, either in the form of morpholog-
ical inflections due to previously unobserved syntactic conditions, or a larger vocabulary extended
with terminology from different domains. In this experiment, the character-level model experi-
ences a drop in performance and its accuracy is much lower than the subword-level one, suggesting
that its capacity cannot cope with the increased amount of training data. Empirical results suggest
that with increased capacity, character-level models carry the potential to reach comparable perfor-
mance to subword-level models (Cherry et al., 2018). On the other hand, our model reaches a much
larger improvement of 0.82 BLEU points over the subword-level and 2.54 BLEU points over the
character-level decoders, suggesting that it could make use of the increased amount of observations
for improving the translation performance, which possibly aid the morphology model in becoming
more accurate.
9
Published as a conference paper at ICLR 2020
4.4.4	The Impact of Inflectional Features
In order to understand whether the latent inflectional features in fact capture information about
variations related to morphological transformations, we first try generating different surface forms
of the same lemma by sampling a lemma vector with LMM for the input word ’go’ and generating
outputs using the fixed lemma vector and assigning different values to the inflectional features. In
the second experiment, we assess the impact of the inflectional features by setting all features f
to 0 and translating a set of English sentences with varying inflected forms in Turkish. Table 3
presents different sets of feature values and the corresponding outputs generated by the decoder and
the outputs generated with or without the inflectional component.
Features	Output	English Translation
[1,1,1,1,1,1,1,1,1,1]	git	go (informal)
[0,1,1,1,1,1,1,1,1,1]	’a git	to go
[0,1,0,1,1,1,1,1,1,1]	’da git	at go
[0,0,0,1,1,0,0,1,1,0]	gidin	go (formal)
[1,1,0,0,0,0,1,0,1,1]	gitmek	to go (infinitive)
[0,0,1,0,0,0,0,0,0,1]	gidiyor	(he/she/it is) going
[0,0,0,0,0,0,0,0,1,0]	gidip	by going (gerund)
[0,0,1,1,0,0,1,0,1,0]	gidiyoruz	(we are) going
Input	Output with f	Output without f
he went home. he came from home. it is good to be home. his home has red walls.	eve gitti. evden geldi. evde olmak iyi. evinde kirmizi duvarlar var.	eve gitti. eve geldi. evde olmak iyi. evde kirmizi duvar var.
Table 3: Above: Outputs of LMM based on the lemma ‘git’ (‘go’) and different sets of inflectional
features. Below: Examples of predicting inflections in context with or without using features.
The model generates different surface forms for different sets of features, confirming that the latent
variables represent morphological features related to the infinitive form of the verb, as well as its
formality conditions, prepositions, person, number and tense. Decoding the set of sentences given
in the second experiment LMM always generates the correct inflectional form, although when the
feature values are set to 0 the model omits some inflectional features in the output, suggesting
that despite partially relying on the source-side context, it still encodes important information for
generating correct surface forms in the inflectional features.
5	Conclusion
In this paper we presented a novel decoding architecture for NMT employing a hierarchical latent
variable model to promote sparsity in lexical representations, which demonstrated promising appli-
cation for morphologically-rich and low-resource languages. Our model generates words one char-
acter at a time by composing two latent features representing their lemmas and inflectional features.
We evaluate our model against conventional open-vocabulary NMT solutions such as subword and
character-level decoding methods in translationg English into three morphologically-rich languages
with different morphological typologies under low to mid-resource settings. Our results show that
our model can significantly outperform subword-level NMT models, whereas demonstrates better
capacity than character-level models in coping with increased amounts of data sparsity. We also con-
duct ablation studies on the impact of feature variations to the predictions, which prove that despite
being completely unsupervised, our model can in fact manage to learn morphosyntactic information
and make use of it to generalize to different surface forms of words.
6	Acknowledgments
The authors would like to thank Marcello Federico, Orhan Firat, Adam Lopez, Graham Neubig,
Akash Srivastava and Clara Vania for their feedback and suggestions. This project received funding
from the European Union’s Horizon 2020 research and innovation programme under grant agree-
ments 825299 (GoURMET) and 688139 (SUMMA).
10
Published as a conference paper at ICLR 2020
References
Duygu Ataman, Matteo Negri, Marco Turchi, and Marcello Federico. Linguistically-motivated
vocabulary reduction for neural machine translation from Turkish to English. The Prague Bulletin
OfMathematical Linguistics,108(1):331-342, 2017.
Duygu Ataman, Orhan Firat, Mattia A Di Gangi, Marcello Federico, and Alexandra Birch. On
the importance of word boundaries in character-level neural machine translation. arXiv preprint
arXiv:1910.06753, 2019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Antonio Valerio Miceli Barone, Jindrich HelcL Rico Sennrich, Barry Haddow, and Alexandra Birch.
Deep architectures for neural machine translation. In Proceedings of the Second Conference on
Machine Translation, pp. 99-107, 2017.
Joost Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with differentiable
binary variables. Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 2963-2973, 2019.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Leon Bottou and Yann L. Cun. Large scale online learning. In S. Thrun, L. K. Saul, andB. Scholkopf
(eds.), Advances in Neural Information Processing Systems 16, pp. 217-224. MIT Press, 2004.
Mohamed Boudchiche, Azzeddine Mazroui, Mohamed Ould Abdallahi Ould Bebah, Abdelhak
Lakhouaja, and Abderrahim Boudlal. Alkhalil morpho sys 2: A robust arabic morpho-syntactic
analyzer. Journal of King Saud University-Computer and Information Sciences, 29(2):141-146,
2017.
Mauro Cettolo. Wit3: Web inventory of transcribed and translated talks. In Conference of European
Association for Machine Translation, pp. 261-268, 2012.
Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. Revisiting
character-based neural machine translation with capacity and compression. In Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4295-4305,
2018.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. In Proceedings of 8th Workshop on
Syntax, Semantics and Structure in Statistical Translation (SSST), pp. 103-111, 2014.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 176-181.
Association for Computational Linguistics, 2011.
Ryan Cotterell, Sebastian J. Mielke, Jason Eisner, and Brian Roark. Are all languages equally hard
to language-model? In Proceedings of the 2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short
Papers), pp. 536-541, 2018.
Hoang Cuong and Khalil Simaan. Latent domain translation models in mix-of-domains haystack. In
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics,
pp. 1928-1939, 2014.
Anirudh Goyal Alias Parth Goyal, Alessandro Sordoni, Marc-Alexandre Cote, Nan Rosemary Ke,
and Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In Advances in neural
information processing systems, pp. 6713-6723, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax.
International Conference on Learning Representations, 2017.
11
Published as a conference paper at ICLR 2020
MichaelI. Jordan, Zoubin Ghahramani, TommiS. Jaakkola, and Lawrence K. Saul. An introduction
to variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.
D Kinga and J Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: Open-
source toolkit for neural machine translation. Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, System Demonstrations, pp. 67-72, 2017.
Julia Kreutzer and Artem Sokolov. Learning to segment inputs for nmt favors character-level pro-
cessing. In Proceedings of the 15th International Workshop on Spoken Language Translation, pp.
166-172, 2018.
Ponnambalam Kumaraswamy. A generalized probability density function for double-bounded ran-
dom processes. Journal of Hydrology, 46(1-2):79-88, 1980.
Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W. Black. Character-based neural machine
translation. arXiv preprint arXiv:1511.04586, 2015.
Pierre Lison and Jorg Tiedemann. OPensUbtitles2016: Extracting large parallel corpora from movie
and TV subtitles. 2016.
Christos LoUizos, Max Welling, and Diederik P Kingma. Learning sparse neUral networks throUgh
L0 regUlarization. arXiv preprint arXiv:1712.01312, 2018.
Minh-Thang LUong and Christopher D. Manning. Achieving open vocabUlary neUral machine trans-
lation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 1054-1063, 2016.
Minh-Thang LUong, HieU Pham, and Christopher D Manning. Effective approaches to attention-
based neUral machine translation. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pp. 1412-1421, 2015.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribUtion: A continoUs re-
laxation of discrete random variables. International Conference on Learning Representations,
2017.
Eric Nalisnick and Padhraic Smyth. Stick-breaking variational aUtoencoders. arXiv preprint
arXiv:1605.06197, 2016.
Kemal Oflazer and Ilker Kuruoz. Tagging and morphological disambiguation of turkish text. In
Proceedings of the fourth conference on Applied natural language processing, pp. 144-149. As-
sociation for Computational Linguistics, 1994.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pp. 311-318, 2002.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
Pytorch. NeurIPS Autodiff Workshop, 2017.
Maja Popovic. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation, pp. 392-395, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International Con-
ference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp.
1278-1286, 2014.
12
Published as a conference paper at ICLR 2020
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407,1951.
Gozde Gul Sahin and Mark Steedman. Character-level models versus morphology in semantic
role labeling. In Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 386-396, 2018.
Hasim Sak, TUnga Gungor, and MUrat Saraclar. Morphological disambiguation of turkish text with
perceptron algorithm. In International Conference on Intelligent Text Processing and Computa-
tional Linguistics, pp. 107-118. Springer, 2007.
Philip Schulz, Wilker Aziz, and Trevor Cohn. A stochastic decoder for neural machine translation.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 1243-1252, 2018.
Rico Sennrich. How grammatical is character-level neural machine translation? Assessing MT
quality with contrastive translation pairs. In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics (Volume 2, Short Papers), pp. 376-382,
2017.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715-1725, 2016.
Raivis Skadins, Jorg Tiedemann, Roberts Rozis, and Daiga Deksne. Billions of parallel words for
free: Building and using the eu bookshop corpus. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LREC), pp. 1850-1855, 2014.
Noah A Smith. Linguistic structure prediction. Synthesis lectures on human language technologies,
4(2):1-274, 2011.
Jana Strakova, Milan Straka, and Jan Hajic. Open-Source Tools for Morphology, Lemmatization,
POS Tagging and Named Entity Recognition. In Proceedings of 52nd Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pp. 13-18. Association for
Computational Linguistics, 2014.
Jorg Tiedemann. News from opus-a collection of multilingual parallel corpora with tools and inter-
faces. In Recent advances in natural language processing, volume 5, pp. 237-248, 2009.
Jorg Tiedemann. Parallel data, tools and interfaces in opus. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evaluation (LREC), pp. 2214-2218, 2012.
Francis M Tyers and Murat Serdar Alperen. South-east european times: A parallel corpus of balkan
languages. In Proceedings of the LREC Workshop on Exploitation of Multilingual Resources and
Tools for Central and (South-) Eastern European Languages, pp. 49-53, 2010.
Clara Vania and Adam Lopez. From characters to words to in between: Do we capture morphology?
In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 2016-2027, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation
system: Bridging the gap between human and machine translation. 2016.
Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and Min Zhang. Variational neural machine
translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 521-530, 2016.
Chunting Zhou and Graham Neubig. Multi-space variational encoder-decoders for semi-supervised
labeled sequence transduction. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 310-320, 2017.
13
Published as a conference paper at ICLR 2020
A Appendix
A. 1 The statistical characteristics of experimental data
Language Pair	# sentences	# tokens		# types	
		Source	Target	Source	Target
English-Arabic	238K	-5M^^	4M	120K	220K
English-Czech	H8K	-2M^^	2M	50K	118K
English-Turkish	136K	-2M^^	3M	53K	171K
Table 4: Training sets based on the TED Talks corpora (M: Million, K: Thousand).
Language Pair	# sentences	# tokens Source Target		# types Source Target
English-Turkish	434K	8M	6M	135K	373K
Table 5: The multi-domain training set (M: Million, K : Thousand).
Language	Data sets		# sentences
English-Arabic	Development Testing	dev2010, test2010 test2011, test2012 test2013, test2014	6K 4K
English-Czech	Development Testing	dev2010, test2010, test2011 test2012, test2013	3K 3K
English-Turkish	Development Testing	dev2010, test2010 test2011, test2012	3K 3K
Table 6: Development and testing sets (K : Thousand).
A.2 The Kumaraswamy distribution
D.5
0.0
Figure 2: The top row shows the density function of the continuous base distribution over (0, 1).
The middle row shows the result of stretching it to include 0 and 1 in its support. The bottom row
shows the result of rectification: probability mass under (l, 0) collapses to 0 and probability mass
under (1, r) collapses to 1, which cause sparse outcomes to have non-zero mass. Varying the shape
parameters (a, b) of the underlying continuous distribution changes how much mass concentrates
outside the support (0, 1) in the stretched density, and hence the probability of sampling sparse
outcomes.
14
Published as a conference paper at ICLR 2020
A.3 The Effect of Feature Dimensions
We investigate the optimal lemma and inflectional feature sizes by measuring the accuracy in
English-to-Turkish translation using different feature vector dimensions. The results given in Figure
3 show that gradually compressing the word representations computed by recurrent hidden states,
with an original dimension of 512, from 500 to 100, leads to increased output accuracy, suggesting
that encoding more compact representations might provide the model with a better generalization
capability. Our results also show that using a feature dimension of 10 is sufficient in reaching the
best accuracy.
Figure 3: The effect of feature dimensions on translation accuracy in Turkish.
15