Published as a conference paper at ICLR 2020
Neural Arithmetic Units
Andreas Madsen
Computationally Demanding
amwebdk@gmail.com
Alexander Rosenberg Johansen
Technical University of Denmark
aler@dtu.dk
Ab stract
Neural networks can approximate complex functions, but they struggle to perform
exact arithmetic operations over real numbers. The lack of inductive bias for
arithmetic operations leaves neural networks without the underlying logic necessary
to extrapolate on tasks such as addition, subtraction, and multiplication. We present
two new neural network components: the Neural Addition Unit (NAU), which can
learn exact addition and subtraction; and the Neural Multiplication Unit (NMU)
that can multiply subsets of a vector. The NMU is, to our knowledge, the first
arithmetic neural network component that can learn to multiply elements from a
vector, when the hidden size is large. The two new components draw inspiration
from a theoretical analysis of recently proposed arithmetic components. We find
that careful initialization, restricting parameter space, and regularizing for sparsity
is important when optimizing the NAU and NMU. Our proposed units NAU and
NMU, compared with previous neural units, converge more consistently, have
fewer parameters, learn faster, can converge for larger hidden sizes, obtain sparse
and meaningful weights, and can extrapolate to negative and small values.1
1 Introduction
When studying intelligence, insects, reptiles, and humans have been found to possess neurons with the
capacity to hold integers, real numbers, and perform arithmetic operations (Nieder, 2016; Rugani et al.,
2009; Gallistel, 2018). In our quest to mimic intelligence, we have put much faith in neural networks,
which in turn has provided unparalleled and often superhuman performance in tasks requiring high
cognitive abilities (Silver et al., 2016; Devlin et al., 2018; OpenAI et al., 2018). However, when using
neural networks to solve simple arithmetic problems, such as counting, multiplication, or comparison,
they systematically fail to extrapolate onto unseen ranges (Lake & Baroni, 2018; Suzgun et al., 2019;
Trask et al., 2018). The absence of inductive bias makes it difficult for neural networks to extrapolate
well on arithmetic tasks as they lack the underlying logic to represent the required operations.
A neural component that can solve arithmetic problems should be able to: take an arbitrary hidden
input, learn to select the appropriate elements, and apply the desired arithmetic operation. A recent
attempt to achieve this goal is the Neural Arithmetic Logic Unit (NALU) by Trask et al. (2018).
The NALU models the inductive bias explicitly via two sub-units: the NAC+ for addition/subtraction
and the NAC∙ for multiPlication/division. The sub-units are softly gated between, using a sigmoid
function, to exclusively select one of the sub-units. However, we find that the soft gating-mechanism
and the NAC. are fragile and hard to learn.
In this paper, we analyze and improve upon the NAC+ and NAC. with respect to addition, subtraction,
and multiPlication. Our ProPosed imProvements, namely the Neural Addition Unit (NAU) and Neural
Multiplication Unit (NMU), are more theoretically founded and improve performance regarding
stability, speed of convergence, and interpretability of weights. Most importantly, the NMU supports
both negative and small numbers and a large hidden input-size, which is paramount as neural networks
are overparameterized and hidden values are often unbounded.
The improvements, which are based on a theoretical analysis of the NALU and its components, are
achieved by a simplification of the parameter matrix for a better gradient signal, a sparsity regularizer,
and a new multiplication unit that can be optimally initialized. The NMU does not support division.
1Implementation is available on GitHub: https://github.com/AndreasMadsen/stable-nalu.
1
Published as a conference paper at ICLR 2020
Zl
Θ-*"Θ Θ^*"Θ	©■*©
H) H) H
WlJel W1,2 τ2 ...跖›引
Figure 1: Visualization of the NMU, where the weights (Wi,j) controls gating between 1 (identity) or
xi , each intermediate result is then multiplied explicitly to form zj .
However, We find that the NAC∙ in practice also only supports multiplication and cannot learn
division (theoretical analysis on division discussed in section 2.3).
To analyze the impact of each improvement, we introduce several variants of the NAC∙. We find
that allowing division makes optimization for multiplication harder, linear and regularized weights
improve convergence, and the NMU way of multiplying is critical when increasing the hidden size.
Furthermore, we improve upon existing benchmarks in Trask et al. (2018) by expanding the “simple
function task”, expanding “MNIST Counting and Arithmetic Tasks” with a multiplicative task, and
using an improved success-criterion Madsen & Johansen (2019). This success-criterion is important
because the arithmetic units are solving a logical problem. We propose the MNIST multiplication
variant as we want to test the NMU,s and NAC. ,s ability to learn from real data and extrapolate.
1.1	Learning a 10 parameter function
Consider the static function t = (xι + x2) ∙ (xι + x2 + x3 + x4) for X ∈ R4. To illustrate the ability
of NAC., NALU, and our proposed NMU, we conduct 100 experiments for each model to learn this
function. Table 1 shows that the NMU has a higher success rate and converges faster.
Table 1: Comparison of the success-rate, when the model converged, and the sparsity error for all
weight matrices, with 95% confidence interval on the t = (xi + X2) ∙ (xi + X2 + X3 + X4) task. Each
value is a summary of 100 different seeds.
Op	Model	Success	Solved at iteration step		Sparsity error
		Rate	Median	Mean	Mean
	NAC.	13% -+85%%	5.5 ∙ 104	5.9 ∙ 104 上嚅 —6.6∙103	7.5 ∙10-6 +2.0∙10-6
×	NALU	26% -+98%%	7.0 ∙ 104	4 +62103 7.8 ∙ 10 —8.6∙103	9 2 ∙ 10-6 +1.7∙10 6 9.2 10	—1.7∙10-6
	NMU	94% -+36%%	1.4 ∙ 104	1.4 ∙ 104 —2.1∙102	—8 +6∙4∙10-9 2.6 ∙ 10	—6410-9
2	Introducing differentiable b inary arithmetic operations
We define our problem as learning a set of static arithmetic operations between selected elements of a
vector. E.g. for a vector X learn the function (x5 + xi) ∙ x7. The approach taking in this paper is to
develop a unit for addition/subtraction and a unit for multiplication, and then let each unit decide
which inputs to include using backpropagation.
We develop these units by taking inspiration from a theoretical analysis of Neural Arithmetic Logic
Unit (NALU) by Trask et al. (2018).
2.1	Introducing NALU
The Neural Arithmetic Logic Unit (NALU) consists of two sub-units; the NAC+ and NAC. . The
sub-units represent either the {+, -} or the {×, ÷} operations. The NALU then assumes that either
NAC+ or NAC. will be selected exclusively, using a sigmoid gating-mechanism.
2
Published as a conference paper at ICLR 2020
The NAC+ and NAC∙ are defined accordingly,
TTT-	」	1 /TTT-	∖	/	∖	∕Y∖
Wh',h'-ι = tanh(Wh',h`-1)σ(Mh` ,h`-1)	(I)
H'-ι
NAC+ : zh` = X Wh',hjzh`-i	⑵
h'-ι = 1
(H-	∖
NAC∙ : zh' = exp	Wh wh`,h`-i log(∣Zhj | + E)	(3)
∖h'-ι = 1	)
where W, M ∈ RH'×H'-1 are weight matrices and zh`-i is the input. The matrices are combined
using a tanh-sigmoid transformation to bias the parameters towards a {-1, 0, 1} solution. Having
{-1, 0, 1} allows NAC+ to compute exact {+, -} operations between elements of a vector. The
NAC∙ uses an exponential-log transformation for the {×, ÷} operations, which works within E
precision and for positive inputs only.
The NALU combines these units with a gating mechanism Z = g Θ NAC+ + (1 — g) Θ NAC∙ given
g = σ(Gx). Thus allowing NALU to decide between all of {+, -, ×, ÷} using backpropagation.
2.2	Weight matrix construction and the Neural Addition Unit
Glorot & Bengio (2010) show that E [zh`] = 0 at initialization is a desired property, as it pre-
vents an explosion of both the output and the gradients. To satisfy this property with Whl-^hlt =
tanh(Wh'-ι,h')σ(Mh'-ι,h'), an initialization must satisfy E[tanh(wh`7,h`)] = 0. In NALU, this
initialization is unbiased as it samples evenly between + and -, or × and ÷. Unfortunately, this
initialization also causes the expectation of the gradient to become zero, as shown in (4).
E ∂M^d∑h; = E ]∂WU E htanh(Wh'-ι,h' )i E hσ'(Mh'-ι,h' )i = 0	⑷
Besides the issue of initialization, our empirical analysis (table 2) shows that this weight construction
(1) do not create the desired bias for {-1, 0, 1}. This bias is desired as it restricts the solution space
to exact addition, and in section 2.5 also exact multiplication, which is an intrinsic property of an
underlying arithmetic function. However, this bias does not necessarily restrict the output space as a
plain linear transformation will always be able to scale values accordingly.
To solve these issues, We add a sparsιfyιng regularιzer to the loss function (L = L + λsparseR',sparse)
and use a simple linear construction, where whʃ`-i,h` is clamped to [-1,1] in each iteration.
Wh'-ι,h' = min(max(Wh'-ι,h', -1), 1),
1	h` H'-ι
R',sparse = Hz H--∑ ∑ min (∣Wh'-ι,h'l,1-∣Wh'-ι,h'
`	`-1 h` =1 h`-1 =1
H'-1
NAU : Zh'= E Wh',hj zh`-i
h'-ι = 1
(5)
(6)
(7)
2.3	Challenges of division
The NAC∙, as formulated in equation 3, has the capability to compute exact multiplication and
division, or more precisely multiplication of the inverse of elements from a vector, when a weight in
Wh'-ι,h' is -1.
3
Published as a conference paper at ICLR 2020
However, this flexibility creates critical optimization challenges. By expanding the exp-log-
transformation, NAC∙ can be expressed as
H'-ι
NAC∙: Zh'= Y (IZh" + e)Wh',h'-1 .	(8)
h'-ι = 1
In equation (8), if ∣Zh'-ι ∣ is near zero (E [zh`-j = 0 is a desired property when initializing (Glorot
& Bengio, 2010)), Wh'-1,h' is negative, and e is small, then the output will explode. This issue is
present even for a reasonably large e value (such as e = 0.1), and just a slightly negative whʃ`-i,h`,
as visualized in figure 2. Also note that the curvature can cause convergence to an unstable area.
This singularity issue in the optimization space also makes multiplication challenging, which further
suggests that supporting division is undesirable. These observations are also found empirically in
Trask et al. (2018, table 1) and Appendix C.7.
Figure 2: RMS loss curvature for a NAC+ unit followed by a NAC.. The weight matrices are
constrained to W1 = ww11 ww11 w01 w01 , W2 = [ w2 w2 ]. The problem is (xι + x2) ∙ (xι + x2 + x3 + x4)
for x = (1, 1.2, 1.8, 2). The solution is w1 = w2 = 1 in (a), with many unstable alternatives.
2.4	INITIALIZATION OF NAC.
Initialization is important for fast and consistent convergence. A desired property is that weights are
initialized such that E[zhʃ`] = 0 (Glorot & Bengio, 2010). Using second order Taylor approximation
and assuming all zh`-i are uncorrelated; the expectation of NAC. can be estimated as
E[zh'] ≈ (1 + 2Var[Wh',h'-ι]log(|E[zh`-i]| + e)2)	⇒ E[zh'] > L	⑼
As shown in equation 9, satisfying E[zh∕ = 0 for NAC. is likely impossible. The variance cannot
be input-independently initialized and is expected to explode (proofs in Appendix B.3).
2.5	The Neural multiplication unit
To solve the the gradient and initialization challenges for NAC. we propose a new unit for multipli-
cation: the Neural Multiplication Unit (NMU)
Wh'-ι,h' = min(max(Wh'-ι,h', 0), 1),
(10)
1	H'	H'-1
R',sparse = Hz H--E E min (Wh'-ι,h', 1 - Wh'-ι,h')	(11)
-1 h'=1 h'-1 =1
H'-1
NMU : Zh' = Y (Wh'-ι,h'Zh'-1 + 1 - Wh'-ι,h')	(12)
h'-1 =1
The NMU is regularized similar to the NAU and has a multiplicative identity when Wh'-1 ,h' = 0.
The NMU does not support division by design. As opposed to the NAC., the NMU can represent
input of both negative and positive values and is not e bounded, which allows the NMU to extrapolate
to Zh'-1 that are negative or smaller than e. Its gradients are derived in Appendix A.3.
4
Published as a conference paper at ICLR 2020
2.6	Moments and initialization
The NAU is a linear layer and can be initialized using Glorot & Bengio (2010). The NAC+ unit can
also achieve an ideal initialization, although it is less trivial (details in Appendix B.2).
The NMU is initialized with E[Wh',h'-J = 1∕2. Assuming all zh`-i are uncorrelated, and
E[zh'-ι ] = 0, which is the case for most neural units (Glorot & Bengio, 2010), the expectation can
be approximated to
E[zh'] ≈ (1 J，	(13)
which approaches zero for H'-ι → ∞ (see Appendix B.4). The NMU can, assuming Var [zh`-i ] = 1
and H'-ι is large, be optimally initialized with Var[Wh'-ι,h/ = 1 (proof in Appendix B.4.3).
2.7	Regularizer scaling
We use the regularizer scaling as defined in (14). We motivate this by observing optimization consists
of two parts: a warmup period, where wh`-,h` should get close to the solution, unhindered by the
sparsity regularizer, followed by a period where the solution is made sparse.
λsparse
t - λstart
λsparse max mm -----------------------
λend - λstart
(14)
2.8	Challenges of gating between addition and multiplication
The purpose of the gating-mechanism is to select either NAC+ or NAC. exclusively. This assumes
that the correct sub-unit is selected by the NALU, since selecting the wrong sub-unit leaves no
gradient signal for the correct sub-unit.
Empirically we find this assumption to be problematic. We observe that both sub-units converge at
the beginning of training whereafter the gating-mechanism, seemingly random, converge towards
either the addition or multiplication unit. Our study shows that gating behaves close to random for
both NALU and a gated NMU/NAU variant. However, when the gate correctly selects multiplication
our NMU converges much more consistently. We provide empirical analysis in Appendix C.5 for
both NALU and a gated version of NAU/NMU.
As the output-size grows, randomly choosing the correct gating value becomes an exponential
increasing problem. Because of these challenges we leave solving the issue of sparse gating for future
work and focus on improving the sub-units NAC+ and NAC..
3	Related work
Pure neural models using convolutions, gating, differentiable memory, and/or attention architectures
have attempted to learn arithmetic tasks through backpropagation (Kaiser & Sutskever, 2016; Kalch-
brenner et al., 2016; Graves et al., 2014; Freivalds & Liepins, 2017). Some of these results have
close to perfect extrapolation. However, the models are constrained to only work with well-defined
arithmetic setups having no input redundancy, a single operation, and one-hot representations of
numbers for input and output. Our proposed models does not have these restrictions.
The Neural Arithmetic Expression Calculator (Chen et al., 2018) can learn real number arithmetic by
having neural network sub-components and repeatedly combine them through a memory-encoder-
decoder architecture learned with hierarchical reinforcement learning. While this model has the
ability to dynamically handle a larger variety of expressions compared to our solution they require an
explicit definition of the operations, which we do not.
In our experiments, the NAU is used to do a subset-selection, which is then followed by either
a summation or a multiplication. An alternative, fully differentiable version, is to use a gumbel-
softmax that can perform exact subset-selection (Xie & Ermon, 2019). However, this is restricted to a
predefined subset size, which is a strong assumption that our units are not limited by.
5
Published as a conference paper at ICLR 2020
4	Experimental results
4.1	Arithmetic datasets
The arithmetic dataset is a replica of the “simple function task” by Trask et al. (2018). The goal is to
sum two random contiguous subsets of a vector and apply an arithmetic operation as defined in (15)
s1,end	s2,end
t = E Xi ◦ E Xi where X ∈ Rn,xi 〜UniformEower, rupper], ◦ ∈ {+, -, ×} (15)
i=s1,start	i=s2,start
where n (default 100), U [rlower, rupper] (interpolation default is U[1, 2] and extrapolation default is
U [2, 6]), and other dataset parameters are used to assess learning capability (see details in Appendix
C.1 and the effect of varying the parameters in Appendix C.4).
4.1.1	Model evaluation
We define the success-criterion as a solution that is acceptably close to a perfect solution. To evaluate
if a model instance solves the task consistently, we compare the MSE to a nearly-perfect solution
on the extrapolation range over many seeds. If W1, W2 defines the weights of the fitted model,
Wf is nearly-perfect, and W2 is perfect (example in equation 16), then the criteria for successful
convergence is Lw∖W? < LWj,w*, measured on the extrapolation error, for E = 10-5. We report
a 95% confidence interval using a binomial distribution (Wilson, 1927).
W1f
1-E
1-E
1 — E 0 + E 0 + E ʌʌ^*
1-E 1-E 1-E ,	2
1]
(16)
To measure the speed of convergence, we report the first iteration for which Lwι,w2 < Lwj,w*
is satisfied, with a 95% confidence interval calculated using a gamma distribution with maximum
likelihood profiling. Only instances that solved the task are included.
We assume an approximate discrete solution with parameters close to {-1, 0, 1} is important for
inferring exact arithmetic operations. To measure the sparsity, we introduce a sparsity error (defined
in equation 17). Similar to the convergence metric, we only include model instances that did solve
the task and report the 95% confidence interval, which is calculated using a beta distribution with
maximum likelihood profiling.
Esparsity = Imax min(IWh'-l,h'1, |1 TWh'-l,hJD	(17)
h`-i,h`
4.1.2	Arithmetic operation comparison
We compare models on different arithmetic operations ◦ ∈ {+, -, ×}. The multiplication models,
NMU and NAC., have an addition unit first, either NAU or NAC+, followed by a multiplication
unit. The addition/substraction models are two layers of the same unit. The NALU model consists of
two NALU layers. See explicit definitions and regularization values in Appendix C.2.
Each experiment is trained for 5 ∙ 106 iterations with early stopping by using the validation dataset,
which is based on the interpolation range (details in Appendix C.2). The results are presented in table
2. For multiplication, the NmU succeeds more often and converges faster than the NAC. and NALU.
For addition and substraction, the NAU and NAC+ has similar success-rate (100%), but the NAU is
significantly faster at solving both of the the task. Moreover, the NAU reaches a significantly sparser
solution than the NAC+. Interestingly, a linear model has a hard time solving subtraction. A more
extensive comparison is included in Appendix C.7 and an ablation study is included in Appendix C.3.
4.1.3	Evaluating theoretical claims
To validate our theoretical claim, that the NMU model works better than NAC. for a larger hidden
input-size, we increase the hidden size of the network thereby adding redundant units. Redundant
units are very common in neural networks, which are often overparameterized.
Additionally, the NMU model is, unlike the NAC. model, capable of supporting inputs that are both
negative and positive. To validate this empirically, the training and validation datasets are sampled
for U[-2, 2], and then tested on U[-6, -2] ∪ U[2, 6]. The other ranges are defined in Appendix C.4.
6
Published as a conference paper at ICLR 2020
Table 2: Comparison of: success-rate, first iteration reaching success, and sparsity error, all with 95%
confidence interval on the “arithmetic datasets” task. Each value is a summary of 100 different seeds.
Op	Model	Success	Solved at iteration step		Sparsity error
		Rate	Median	Mean	Mean
	NAC.	31% -+810%%	2.8 ∙ 106	3.0 ∙106 +2.4∙105	5.8 ∙10-4 —4.6∙10-4
×	NALU	0% +-40%%	—	—	—
	NMU	98% +-15%%	1.4 ∙ 106	1∙5 ∙106 +6.6∙104	42 ∙10-7 +2.9∙i0-8
	NAC+	100% +-04%%	2.5 ∙ 105	4 9 ∙ 105 +52104 2 ɪʊ -4.5∙104	2.3 ∙10-1-6.5∙10-3
	Linear	100% +-04%%	6.1 ∙ 104	6.3 ∙104 +3 需	2.5 /0-1-3.610-4
+	NALU	14% -+58%%	1.5 ∙ 106	1.6 ∙106 +3.3∙105	1.7 ∙10-1 -2.7∙10-2
	NAU	100% +-04%%	1.8 ∙ 104	3.9 ∙ 105 +4.5,104 —3. 7 ∙ 10	-- In-5 +ι.3∙ 10 5 3.2 ∙ 10 -1.3∙10-5
	NAC+	100% +-04%%	9.0 ∙ 102 3	3.7 ∙105 +3.8∙104	2.3 ∙10-1 -5.4∙10-3
	Linear	7% +-74%%	3.3 ∙ 106	1.4 ∙106 +6.1∙105	1.8 ∙10-1 -7.8∙10-2
-	NALU	14% -+58%%	1.9 ∙ 106	1.9 ∙ 106 +4.4∙105 —4.5∙ 105	2.1 ∙10-1 -2.2∙10-2
	NAU	100% +-04%%	5.0 ∙ 103	Irl ∏5 +1.7∙104 1.6 ∙ 10 —1.6∙104	6.6 ∙10-f 10-2
Finally, for a fair comparison We introduce two new units: A variant of NAC., denoted NAC.,σ, that
only supports multiplication by constraining the weights with W = σ(W). And a variant, named
NAC.,nmu, that uses clamped linear weights and sparsity regularization identically to the NMU.
Figure 3 shows that the NMU can handle a much larger hidden-size and negative inputs. Furthermore,
results for NAC.,σ and NAC.,NMU validate that removing division and adding bias improves the
success-rate, but are not enough when the hidden-size is large, as there is no ideal initialization.
Interestingly, no models can learn U[1.1, 1.2], suggesting certain input ranges might be troublesome.
5e+06
4e+06
3e+06
2e+06
1e+06
0e+00
2	4	6	8	10	2	4	6	8	10
Hidden size
0.0025-.
0.0020-
0.0015-
0.0010-
• ∙
0.0005-
0.0000- 4 ▲■区 d
Illll
Interpolation range
model -∙ NAC.,NMU -∙ NAC.,0 -∙ NAC. -∙ NALU -∙ NMU
Figure 3:	Multiplication task results when varying the hidden input-size and when varying the
input-range. Extrapolation ranges are defined in Appendix C.4.
7
Published as a conference paper at ICLR 2020
4.2 Product of sequential MNIST
To investigate if a deep neural network can be optimized when backpropagating through an arithmetic
unit, the arithmetic units are used as a recurrent-unit over a sequence of MNIST digits, where the
target is to fit the cumulative product. This task is similar to “MNIST Counting and Arithmetic Tasks”
in Trask et al. (2018)2, but uses multiplication rather than addition (addition is in Appendix D.2).
Each model is trained on sequences of length 2 and tested on sequences of up to 20 MNIST digits.
We define the success-criterion by comparing the MSE of each model with a baseline model that
has a correct solution for the arithmetic unit. If the MSE of each model is less than the upper 1%
MSE-confidence-interval of the baseline model, then the model is considered successfully converged.
Sparsity and “solved at iteration step” is determined as described in experiment 4.1. The validation
set is the last 5000 MNIST digits from the training set, which is used for early stopping.
In this experiment, we found that having an unconstrained “input-network” can cause the
multiplication-units to learn an undesired solution, e.g. (0.1 ∙ 81 + 1 - 0.1) = 9. Such network do
solve the problem but not in the intended way. To prevent this solution, we regularize the CNN output
With Rz = H,h` PH PH-II (I - Wh'-ι,h') ∙ (1 - zh`-i )2. This regularizer is applied to the
NMU and NAC. NMU models. See Appendix D.4 for the results where this regularizer is not used.
Figure 4 shows that the NMU does not hinder learning a more complex neural network. Moreover,
the NMU can extrapolate to much longer sequences than what it is trained on.
Solved at iteration step
Iiiiiiiiiii
1 2 4 6 8 10 12 14 16 18 20
Extrapolation length
Sparsity error
Iiiiiiiiiii
1 2 4 6 8 10 12 14 16 18 20
model -∙- NAC.,nmu -∙- NAC.,σ -∙- NAC.	LSTM NALU -∙- NMU
Figure 4:	MNIST sequential multiplication task. Each model is trained on sequences of two digits,
results are for extrapolating to longer sequences. Error-bars represent the 95% confidence interval.
5 Conclusion
By including theoretical considerations, such as initialization, gradients, and sparsity, we have
developed the Neural Multiplication Unit (NMU) and the Neural Addition Unit (NAU), which
outperforms state-of-the-art models on established extrapolation and sequential tasks. Our models
converge more consistently, faster, to an more interpretable solution, and supports all input ranges.
A natural next step would be to extend the NMU to support division and add gating between the NMU
and NAU, to be comparable in theoretical features with NALU. However we find, both experimentally
and theoretically, that learning division is impractical, because of the singularity when dividing by
zero, and that a sigmoid-gate choosing between two functions with vastly different convergences
properties, such as a multiplication unit and an addition unit, cannot be consistently learned.
Finally, when considering more than just two inputs to the multiplication unit, our model performs
significantly better than previously proposed methods and their variations. The ability for a neural unit
to consider more than two inputs is critical in neural networks which are often overparameterized.
2The same CNN is used, https://github.com/pytorch/examples/tree/master/mnist.
8
Published as a conference paper at ICLR 2020
Acknowledgments
We would like to thank Andrew Trask and the other authors of the NALU paper, for highlighting the
importance and challenges of extrapolation in Neural Networks.
We would also like to thank the students Raja Shan Zaker Kreen and William Frisch M0ller from
The Technical University of Denmark, who initially showed us that the NALU do not converge
consistently.
Alexander R. Johansen and the computing resources from the Technical University of Denmark,
where funded by the Innovation Foundation Denmark through the DABAI project.
References
Kaiyu Chen, Yihan Dong, Xipeng Qiu, and Zitian Chen. Neural arithmetic expression calculator.
CoRR, abs/1809.08590, 2018. URL http://arxiv.org/abs/1809.08590.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.
Karlis Freivalds and Renars Liepins. Improving the neural GPU architecture for algorithm learning.
CoRR, abs/1702.08727, 2017. URL http://arxiv.org/abs/1702.08727.
Charles R. Gallistel. Finding numbers in the brain. Philosophical Transactions of the Royal Society
B: Biological Sciences, 373(1740):20170119, 2018. doi: 10.1098/rstb.2017.0119. URL https:
//royalsocietypublishing.org/doi/abs/10.1098/rstb.2017.0119.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics (AISTATS 2010), volume 9,pp. 249-256, May 2010.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.
URL http://arxiv.org/abs/1410.5401.
Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In 4th International Conference on
Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, 2016. URL http://arxiv.org/abs/1511.08228.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. In 4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1507.01526.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In The 3rd
International Conference for Learning Representations, San Diego, 2015, pp. arXiv:1412.6980,
Dec 2014.
Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, StockhoImSmdSsan, Stockholm, Sweden, July 10-15,
2018, pp. 2879-2888, 2018. URL http://proceedings.mlr.press/v80/lake18a.
html.
Andreas Madsen and Alexander R. Johansen. Measuring arithmetic extrapolation performance.
In Science meets Engineering of Deep Learning at 33rd Conference on Neural Information
Processing Systems (NeurIPS 2019), volume abs/1910.01888, Vancouver, Canada, October 2019.
URL http://arxiv.org/abs/1910.01888.
Andreas Nieder. The neuronal code for number. Nature Reviews Neuroscience, 17:366 EP -, 05
2016. URL https://doi.org/10.1038/nrn.2016.40.
9
Published as a conference paper at ICLR 2020
OPenAL Marcin Andrychowicz, BoWen Baker, Maciek Chociej, Rafal Jdzefowicz, Bob McGrew,
Jakub W. Pachocki, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech
Zaremba. Learning dexterous in-hand maniPulation. CoRR, abs/1808.00177, 2018. URL http:
//arxiv.org/abs/1808.00177.
Rosa Rugani, Laura Fontanari, Eleonora Simoni, Lucia Regolin, and Giorgio Vallortigara. Arithmetic
in newborn chicks. Proceedings ofthe Royal SocietyB: Biological Sciences, 276(1666):2451-2460,
2009. doi: 10.1098/rsPb.2009.0044. URL https://royalsocietypublishing.org/
doi/abs/10.1098/rspb.2009.0044.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. LillicraP,
Madeleine Leach, Koray Kavukcuoglu, Thore GraePel, and Demis Hassabis. Mastering the
game of go with deeP neural networks and tree search. Nature, 529(7587):484-489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961.
Mirac Suzgun, Yonatan Belinkov, and Stuart M. Shieber. On evaluating the generalization of LSTM
models in formal languages. In Proceedings of the Society for Computation in Linguistics (SCiL),
PP. 277-286, January 2019.
Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blun-
som. Neural arithmetic logic units. In Advances in Neural Information Process-
ing Systems 31, PP. 8035-8044. 2018. URL http://papers.nips.cc/paper/
8027- neural- arithmetic- logic- units.pdf.
Edwin B. Wilson. Probable inference, the law of succession, and statistical inference. Journal
of the American Statistical Association, 22(158):209-212, 1927. doi: 10.1080/01621459.1927.
10502953. URL https://www.tandfonline.com/doi/abs/10.1080/01621459.
1927.10502953.
Sang Michael Xie and Stefano Ermon. Differentiable subset samPling. CoRR, abs/1901.10517, 2019.
URL http://arxiv.org/abs/1901.10517.
10
Published as a conference paper at ICLR 2020
A Gradient derivatives
A.1 WEIGHT MATRIX CONSTRUCTION
For clarity the weight matrix construction is defined using scalar notation
TTT-	， 1 ∕TT7-	、 / nʌʃ	、	，A C'
Whg,hg-1 = tanh(Wh',h'-ι )σ(Mhg,hg-ι )	(18)
The of the loss with respect to Wh`,h`-i
and Mh',h'-1
is then derived using backpropagation.
∂L ∂Whe ,he-1 "	_	∂L	∂Whe,he-1 dWhe,he-1 ∂Whe,he-1 ∂L 二		(1 - tanh2(Whe,heτ))σ(Mhe,heTI) ^2^-1	(19)
∂L ∂Mhe,he——	_	∂L	∂Whe,heT1	(	) ∂Whe,heτ1 ∂Mhe,heTI ∂L 二		tanh(Whe,hgT1)σ(Mhe,heTI)(I — σ(Mhe,heTI)) ^he"]
As seen from this result, one only needs to consider ^^：：——
with respect to Whg,hg_1 and Mhg,hg-ι is a multiplication on
for NAC+ and NAC∙, as the gradient
∂L
dWh',h'-ι .
A.2 Gradient of NAC.
The NAC. is defined using scalar notation.
H h`-i
zh` = exp I E Wh',h'_1 IOg(Izh J I + e)
∖h'-ι = 1
(20)
The gradient of the loss with respect to Whg,hg_1 can the be derived using backpropagation.
∂Zh'
∂Wht,ht-1
H HeT	∖
eχp I E Whe,h'TIIOg(IzheTII + e) ) bg(IzheT11 + e)
∖h'-1 = l	)
(21)
zh` log(IzheT J + e)
We now wish to derive the backpropagation term δhe = ∂∂L, because zhe affects {zhe+1} H+11=1 this
becomes:
δ ∂ L	X十十	∂ L dzhe+1
M	dzhe	he+1 = 1 dzhe+1 dzhe
X+1 ʌ dzhe 十1
he"，E
(22)
To make it easier to derive z；；+1 We re-express the zhe as zhe+1.
zhe+1
exp
Whe+1,helog(IzheI + e)
(23)
11
Published as a conference paper at ICLR 2020
The gradient of ；；；+1 is then:
dzh'+1
∂Zh2
exp
Whe+ι,hjog(∣zh/ + e)
Wh'+1 ,he
∂ log(∣Zh'∣ + e)
∂zhe
exp
Whe+ι,he log(∣zhJ + E) Whe+ι,h`
abs0(zhe)
∣zhe∣ + E
(24)
zhe+ι whe+ι,he
abs0(zhe)
∣zhe∣ + E
abs0(Zhg) is the gradient of the absolute function. In the paper We denote this as Sign(Zhg) for brevity.
However, depending on the exact definition used there may be a difference for Zhg = 0, as abs0(0) is
undefined. In practicality this doesn,t matter much though, although theoretically it does mean that
the expectation of this is theoretically undefined when E[zhg] = 0.
A.3 Gradient of NMU
In scalar notation the NMU is defined as:
Hg-1
Zhg= Π	(Whg-1,hg Zhg-I + 1 - Whg-1,hg)
hg-1 = 1
(25)
The gradient of the loss with respect to Whg-I ,hg is fairly trivial. Note that every term but the one
for h'-ι, is just a constant with respect to Whg-I ,hg. The product, except the term for h`- can be
expressed as ----------Zhl-----------. Using this fact, the gradient can be expressed as:
Whg-1,hg zhg-1 + I-Whg-1,hg
∂L	∂ L	∂Zhg
dwhg,hg-1	dZhg dwhg,hg-1
∂L
Zhg
dZhg Whg-1,hg Zhg-1 +1 - Whg-1,hg
(Zhg-1 - 1)	(26)
Similarly, the gradient ∂ZL- which is essential in backpropagation can equally easily be derived as:
∂L
dZhg-1
Hg
X
hg = 1
∂L ∂Zhg
∂Zhg ∂Zhg-1
Hg	Z
X1 Whg-1,hgZhg-I + 1- Whg-1,hg Whihg
(27)
12
Published as a conference paper at ICLR 2020
B Moments
B.1	Overview
B.1.1	Moments and initialization for addition
The desired properties for initialization are according to Glorot et al. (Glorot & Bengio, 2010):
E[zh' ] = 0
V ar[zh`] = Var zh`-1
E
V ar
∂L
dzh'-1
∂L
dzh'-1
0
(28)
B.1.2	Initialization for addition
Glorot initialization can not be used for NAC+ as wh`-i,h` is not sampled directly. Assuming that
TTT-	T T ∙ C	Γ	1	1 7l^Γ	T T ∙ C	Γ	1.1	.1	♦	Il ♦	1 /
wh`,h`-i 〜Uniform[-r, r] and Mh'"、〜Uniform[-r, r], then the variance can be derived (see
proof in Appendix B.2) to be:
Var[Wh'-ι,h'] = 2r
1-
tanh(r)
r
tanh
(2))
(29)
One can then solve for r, given the desired variance (Var[Wh'-ι,h'] = Hg ；+4) (Glorot & Bengio,
2010).	-
B.1.3	Moments and initialization for multiplication
Using second order multivariate Taylor approximation and some assumptions of uncorrelated stochas-
tic variables, the expectation and variance of the NAC. layer can be estimated to:
1	1	、c2 H'-1
f (c1,c2) = (1 + c1 2Var[Wh',h'-i]log(IE[zh'-i]| + e)2)
E[zh'] ≈ f(1,1)
Var[zh'] ≈ f(4,1)- f(1, 2)
E
Var
∂L
dzh'-1
∂L
dzh'-1
h` f (4,1) Var[Wh',h'-ι]
≈ Var
2+
3
(|E[zh'-i]| + e)4
Var[zh'-1]
(30)
0
This is problematic because E [zhj ≥ 1, and the variance explodes for E [zh`-j = 0. E"-/ = 0
is normally a desired property (Glorot & Bengio, 2010). The variance explodes for E [zh`-j = 0,
and can thus not be initialized to anything meaningful.
13
Published as a conference paper at ICLR 2020
For our proposed NMU, the expectation and variance can be derived (see proof in Appendix B.4)
using the same assumptions as before, although no Taylor approximation is required:
V ar
E[zh' ] ≈
∂L
dzh'-1
≈0
Var[zh'] ≈ fVar[Wh'-ι,h'] + 1)	(Var[zh`-i] + 1)H'-1
n Hj
4；
(31)
∂l -
dzh'-1 一
≈ Var -dL H'
[dzh`j '
E
—
Var[Wh'-ι,h'] + 4)	(Var[zh`-i] + 1)H'τ-1
These expectations are better behaved. It is unlikely that the expectation of a multiplication unit can
become zero, since the identity for multiplication is 1. However, for a large H'-ι it will be near zero.
The variance is also better behaved, but do not provide a input-independent initialization strategy.
We propose initializing with Var[Wh'-ι,h/ = 4, as this is the solution to Var[zh/ = Var[zh'-J
assuming Var[zh'-J = 1 and a large H'-ι (see proof in Appendix B.4.3). However, more exact
solutions are possible if the input variance is known.
B.2	Expectation and variance for weight matrix construction in NAC layers
The weight matrix construction in NAC, is defined in scalar notation as:
TTT-	J 1 /TTT-	∖	/ 71〉	∖
Wh',h'-ι = tanh(Wh',h'-ι )σ(Mh',h'-ι )
(32)
Simplifying the notation of this, and re-expressing it using stochastic variables with uniform distribu-
tions this can be written as:
,ʌ . , ʌ .
W 〜tanh(W)σ(M)
W 〜U[-r,r]	(33)
M 〜U[-r,r]
Since tanh(W) is an odd-function and E[W] = 0, deriving the expectation E[W] is trivial.
- - - ,ʌ . , - , ʌ . , - , ʌ ,,
E[W] = E[tanh(W)]E[σ(M)] = 0 ∙ E[σ(M)] = 0
(34)
EI	♦	♦	T .	1	1	TTT-	1 7l^Γ	•	1	1	.	F	♦	1 ∙ r∙ 1 .
The variance is more complicated, however as W and M are independent, it can be simplified to:
Var[W] = E[tanh(W)2]E[σ(M)2] - E[tanh(W)]2E[σ(M)]2 = E[tanh(W)2]E[σ(M)2] (35)
These second moments can be analyzed independently. First for E[tanh(W)2]:
Z∞
tanh(x)2fU[-r,r] (x) dx
∞
=[tanh(x)2 dx
2r -r
=ɪ ∙ 2 ∙ (r - tanh(r))
2r
tanh(r)
=1----------
r
(36)
14
Published as a conference paper at ICLR 2020
Then for E[tanh(MM)2]:
Z ∞
σ(x)2fu[-r,r](x) dx
-∞
=ɪ j σ(x)2 dx
2r J_r
=2r (r-tanh (2))
(37)
Which results in the variance:
B.3 Expectation and variance of NAC.
B.3.1 FORWARD PASS
γtjλ1 1 A	tanh(r)
VariW] = 2- ( 1 ——詈
(38)
Expectation Assuming that each zh`-i are uncorrelated, the expectation can be simplified to:
E[zh'] = E exp
H2-1
X Wh',h'_1 log(∣zh`-11 + e)
h2-1 = 1
H'-1
∏ eχp(wh',h'-ι IOg(Izh'-11 + e))
h'-ι = 1
Hg-1
≈	∏ E [exP(Whg,hg-1 IOg(Izhg-11 + e))]
hg-1 = 1
=EieXP(Whg,hg-1 IOg(Izhg-11 + e))产T
=E [(∣zhg-11 + e)Whg,hg-1]Hg-1
=E [f (zhg-1 ,Whg,hg-1 )]Hg-1
(39)
Here We define g as a non-linear transformation function of two independent stochastic variables:
f(zh'-1, Wh',h'-1) = (∣zh'-1∣ + e)Wh',h'-1
(40)
We then apply second order Taylor approximation of f, around (E[zh'_1], E[卬初&―]).
E[f(zh'-1 ,Wh',h'-1)] ≈ E
f (E[zh'-1 ],E [Wh',h'-1 ])
zh'-1
Wh',h'-1
E[Wh',h'-1]
-a∕(Zhg-1,Whg,hg-1)] i
dzhg-1	I
df (Zhg-I ,Whg,hg-1 '	IJzh ] = E [zh 1]
8Wh〃 h。 、	R hg-1 L hg-1j
L	g,g-1	」(Whg,hg-1 = E[Whg,hg-1 ]
zh'-1
+ — ττr
2 Wh',h'
'-1
-E[zhg-1]
-EiWhg,hg-1]
(41)
E
+
1
-E[zh`-i]
—
T
T
d2f(zh'1,Wh',h'-1 )
d2zhg-1
d f (zh'-1 Wh',h'-1 )
dzhg-1 dWhg,hg-1
g2f (Zhg-1 ,Whg,h'-1 ) J I
dzh'-1 dWh',h'-1	I I
a2f⅛W1 'W"'"-1' I 1 Jzhg-I = Eizhg-」
hg,hg-1	」(Whg ,hg-1= E[Whg,hg-1 ]
zh`-i
."'-I
-Eizh'-」
-EiWhg,hg-1[
15
Published as a conference paper at ICLR 2020
Because E[" -E[zh-]] = 0, ©卬M.- -©卬跖％-]] = 0, and。℃",卬…一]=0.
This simplifies to:
E[g(zh'-ι, Whg,hg-ι)] ≈ g(E[zh'-i],E[Wh',h'-i])
r	-IT d g(zh`-i ,wh`,h`-i)] I
+ -yar	zh'-ι	2	d2zh'-ι	I I	(42)
2	[Wh',h'-J	d g⅛-1‘Wh'"1)I lʃzh`-i = E[zh'-1]
L	"'hj	」(Wh',h'-i = E[Wh',h'-i]
Inserting the derivatives and computing the inner products yields:
E[f(zh'-i ,Wht,ht-i)] ≈ (∣E[zh'-i ]| + e)EW"h'-i]
+ 2Var[zh'-i](|E[zh`-i]| + e)E[W",h'τ]-2E[Wh',h'-i](E[Wh',h'-i] - 1)
+ 2Var[Wh',h'-i](|E[zh'-i]| + e)E[W","τ] IOg(IE[zh'-i]| + e)2
=1 + - Var [Wh',h'-i] IOg(IE[zh'-i]| + e)2
(43)
This gives the final expectation:
E[zh/ = E [g(zh`-i, Wh',h'-i)]H'τ
-1	2∖ H'-i	(44)
≈ (1 + 2Var[Wh',h'-i]Iog(IE[zh'-i]| + e)2)
We evaluate the error of the approximation, where Whg,hg-i 〜 U[-rw, rw] and zh`-i 〜 U[0, rz].
These distributions are what is used in the arithmetic dataset. The error is plotted in figure 5.
50
40
30
20
10
0.0
0.1
0.2
0.3
0.4
0.5
Figure 5: Error between theoretical approximation and the numerical approximation estimated by
random sampling of 100000 observations at each combination of rz and rw.
error
0.00
-0.02
-0.04
-0.06
0
Variance The variance can be derived using the same assumptions as used in “expectation”, that
all zh`-i are uncorrelated.
Var[zh'] = E[⅛J - E[zh']2
H'-i
E ∏	(|zhj| + e)2,W"-i
h'-i = 1
H'-i
-E ∏ (|zh'-i| + e)Wh',h'-i
h'-i = 1
E [f(zh`-i, 2 ∙ Wh',h'-i)]H'τ - E [f(zh'-i, Wh',h'-i)『H'T
(45)
16
Published as a conference paper at ICLR 2020
We already have from the expectation result in (43) that:
E[f(z‰-1 ,Wht,ht-1)] ≈ 1 + 1 Var[Wh',h'.ι]IOg(IE[zh`-i]I + e)2	(46)
By substitution of variable We have that:
e [/(zht-ι, 2 ∙ Whe,he-1)] ≈ 1 + 1V ar [2 ∙ W%',%'.」IOg(IE[z⅛-J + e)2
≈ 1 + 2 ∙ Var [Wh',h'_1] IOg(IE[zh'.iU + e)2
This gives the variance:
Var[zh' ] = E [g(zh'_1,2 ∙ Wh',hj)] Hj- E [/(Zh『,Wh',hj)] 2H'-1
≈ (1 + 2 ∙ Var[Wh',h'_1]log(IE[zh'.1]I + e)2)Hj
—
1	∖2∙H'.1
1 + 2 ∙ Var[Wh',h'_1]IOg(IE[zh'_1U + e)2)
(47)
(48)
B.3.2 BACKWARD PASS
Expectation The expectation of the back-propagation term assuming that 必十1 and z；2+1 are
h`
mutually uncorrelated:
H'+1	K	r ɑ	-
E[δh∕= E X δh'+1 -⅛ ≈ %+1E[δh'+1]E -⅛1	(49)
7 ,l	Ozh`	Ozh`
h'+1 = 1	'	' j
Assuming that zh'+1, Wh'+1,h', and zh` are uncorrelated:
,0zh'+1'
,∂zh'
E
≈ E[zh'+1 ]E[Wh'+1,h']E [：bS0(?)1 = E[zh`+1 ] ∙ 0 ∙ E [：bS0(?)
L i zh` i + e	i zh` i + e
0
(50)
Variance Deriving the variance is more complicated:
Var
,0zh'+1
,∂zh'
Var
zh'+1 Wh'+1,h'
abs0(zh')
1zhj + e
(51)
Assuming again that zh'+1, Wh'+1 ,h`, and zh` are uncorrelated, and likewise for their second moment:
Var
一0zh'+1
∂zh'
≈ E[z2'+1] E[W2'+1,h'] E
abs0 (zh` )ʌ 2
E[zh'+1 ]2 E[Wh'+1,h']2E
1zh/ + J
abs0(zh' )"
—
U + e
2
e[z2'+1 ]Var[Wh'+1,h']E [f⅞s0⅛^Y
∖〔zhg i + e /
(52)
-E[zh'+1]2 ∙ 0 ∙ E
-abs'(zh`) ]2
JzhJ + e」
e[z2'+1 ]Var[Wh'+1,h']E [(⅞s0⅛τ)2
∖ *h' i + e /
Using Taylor approximation around E[zh∕ We have:
E
abs0(zh' )ʌ 2
IzI + e
≈ (IE[zh']I+ e)2
1
,1	6 tλ γ 1
H----------------V Varlzhj
2(IE[zh']I + e)4	[h']
(IE[zh']I+ e)2 + (IE[zhJ + e)
4 Var[zh' ]
1
3
(53)
17
Published as a conference paper at ICLR 2020
Finally, by reusing the result for E[沆]from earlier the variance can be expressed as:
Var
∂L
dzh'-1
≈ Var ]∂Z-] H' (1÷ 2 ∙ Var[Whg,h'_i]IOg(IE [zh'-ι ]| + E)2) ' 1
- Var[Wh',h'-i]
1
-----------2 ÷
(∣E["]∣ ÷ e)2
3
(IE[zh'-i]| ÷ e)4
Var[zhg-ι]
(54)
B.4 Expectation and variance of NMU
B.4.1	FORWARD PASS
Expectation Assuming that all zh`-i are independent:
-H'-i	-
E[zh'] = E	Π (Whg-i,h` zh`-i ÷ 1 - Whg-i,h`)
h`-i = 1
≈ E [Whg-i,h'zht-i ÷1 - Wh'-i,h']”τ
≈ (E[Wh'-i,hg]E[zh'-i]÷1 - E[Whg-i,hg])HgT
(55)
Assuming that E[zhg-i] = 0 which is a desired property and initializing E[Whg-i,hg] = 1∕2, the
expectation is:
E[zhg] ≈ (E[Whg-i,hg]E[zhg-i]÷1 - E[Whg-i,hg])HgT
(1	1∖ Hj
≈(2 ∙0÷1 - 2)
(56)
Variance Reusing the result for the expectation, assuming again that all zhg-i are uncorrelated,
and using the fact that Whg-i,h` is initially independent from zhg-i:
Var[zhg] = E显]-E[zhg]2
≈E[%] -(2
2-Hg-i
Hg-i
ɪɪ (Whg-i,hgzhg-i ÷1 - Whg-i,hg)
hg-i=1
≈ E[(Whg-nhgzhg-i ÷1 - Whg-nhg)2]Hg-i
1 ∖ 2∙Hg-I
2)
1∖ 2-Hg-i
2)
(57)
E
—
—
=(E[Whg-i,hg]E[z2g-i] - 2E[Whg-i,hg]E[zhg-i] ÷ E[Whg-^]
Hg-i
÷ 2E[Whg-i,hg ]E [zhg-i ] - 2E[Whg-i ,hg ]÷1
1∖ 2-Hg-i
2)
—
Assuming that E[zhg-i] = 0, which is a desired property and initializing E[Whg-i,hg] = 1∕2, the
variance becomes:
Var[zhg] ≈
2-Hg-i
/ 1 ∖ 2∙Hg-i
≈ ((Var[Whg-i,hg] ÷ E[Whg-i,hg]2) (Var[zhg-i] ÷ 1)) g-i - (2)
Var[Whg-i,hg] ÷ 4 )	(Var[zhg-i] ÷1)HgT
1 ∖ 2∙Hg-I
2)
—
(58)
18
Published as a conference paper at ICLR 2020
B.4.2	Backward pass
Expectation For the backward pass the expectation can, assuming that ∂τL and d Zhe are uncor-
°zh	°zhe-∖
related, be derived to:
∂ L
dzhe-1
E
HEJjL dzht
d	IdZhC dz‰-ι.
HeE
[∂Zh;]
E
∂zhe
dzhg-i
_____________Zhg_____________W
Whg-i,he Zhg-I +1 — Whg-i,hg	h"i"
(59)
HeE [工 E
[dzh"
_____________Zhg
Whg-i,hg zhg-i + 1 - Whg-i,hg
E[Whg-ι,hg]
Initializing	E[Whg-i,hg ]	=	1∕2,	and inserting the result for the expectation
E h___________Zh_____________].
L Whg-i,hg zhg-i +I-Whg-i,hg J
E
-∂ L
一 dzhg-i
≈ HeE ∂zhg
(60)
留
Assuming that E
0, which is a desired property (Glorot & Bengio, 2010).
E
∂L
azhg-i
≈ 0 ∙ h` ∙
(61)
0
Variance For the variance of the backpropagation term, we assume that ∂ZL- is uncorrelated with
∂Zhg
dzhg-i .
Var
∂L
∂zhg-1
Assuming again that E
Var
HeVar
∂L ∂zhg
∂zhg ∂zhg-i
∂zhg
^hg,
+ Var
∂zhg
azhg-i
[∂⅛]
∂L -
azhg-i.
+E
∂zhg
azhg-i
0, and reusing the result E [衾曜]=(2产-：
≈ Var IWL] He
[∂zhgj '
2-Hg-i
+ Var
∂zhg
azhg-i
(62)
(63)
2
Focusing now on Var [言 J], We have:
2
Var
∂zhg 一
azhg-i _
E ( —........Zh———.......
∖ Whg-i,hg zhg-i +1 - Whg-i,hg
_____________Zhg_____________
Whg-i,hgzhg-i + 1 - Whg-i,hg_
E[W>i,hg]
2
E [Whg-i,hg ]2
(64)
-E
19
Published as a conference paper at ICLR 2020
Inserting the result for the expectation E W-----------------Zhi〔 W-------- and Initializing again
vvhg-1,hg zhg-1 +1 Whg-1,hg
E[Whg-1,hg] = 1∕2.
Var
∂Zhg
dzhg-ι.
J z_____________Zhg
[I Whg-1,hg zhg-1 +1 - Whg- 1,hg
1、2-(Hg-1-1) ,1、2
2)	⑴
2,
E[WL1,he ]
(65)
≈ E
—
2
—
E ____________迎___________
[∖Whg-1,hg zhg-1 +1 - Whg-1,hg
æ2Hg-I
E[%皿]
Using the identity
E[Whg-I,h/ = 1∕2∙
that EWhLι,hj
Var[Whg-1,hg] + E[Whg-1,hg]2, and again
using
Var
∂zhg -
dzhg-i.
Zhg
[∖ Whg-i,hg zh`-i +1 - Whg-i,hg
]∖ 2∙Hg-ι
2)
To derive E (whg-1 人 Zh
Zh
,g-1 +1 -Wh g-1,hg
Var[Whg-1,hg] + 4
(66)
the result for Vαr[zhg] can be used, but for 瓦-ι
≈ E
—
2
Hg7 - 1, because there is one less term. Inserting E (而---------Zhg ■, W-----
' 1	,	S	[∖Whg-1,hg zhg-1 +l-Whg-1,hg
(Var[Whg-1 ,hg] + 1)Hg-IT (Var[zhg-1 ] + 1)HgTT,wehave:
2
Var
∂Zhg
dzhg-1.
≈ (Var[Whg-1,h∕ + 4
(Var[zhg-1] + 1)HgTT
Hg-1-1
• (Var [Whg-ι,h/ + 4
H'-1
2∙H'-ι
(67)
Var[Whg-1,hg] + 4
(Var[zhg-1] + 1)Hg-IT
]∖ 2∙Hg-ι
2)
—
Inserting the result for V ar 屋：" ]into the result for V ar [dz∂L ]:
Var
∂L -
dzhg-1 .
H2-1
Var[Whg-1,hg] + 4
(Var[zhg-1] + 1)HgTT
-1
1∖ 2∙H'-1
2)
(68)
(Var[zhg-1] + 1)Hg-T
—
20
Published as a conference paper at ICLR 2020
B.4.3	Initialization
The Whi should be initialized with E[Wh'-ι,h/ = 1, in order to not bias towards inclusion or
exclusion of zh`-1 . Using the derived variance approximations (68), the variance should be according
to the forward pass:
Var[Wh'-ι,h'] = ((1 + Var[zh'])-H'τ Var[zh'] + (4 + 4Var[zh'])-H'-1) H-T - |	(69)
And according to the backward pass it should be:
Var [Wh'_1,h/
(Var[zh∕ + 1)1-H'τ ! H-T - 1
(70)
Both criteria are dependent on the input variance. If the input variance is know then optimal initializa-
tion is possible. However, as this is often not the case one can perhaps assume that Var[zh'-J = 1.
This is not an unreasonable assumption in many cases, as there may either be a normalization layer
somewhere or the input is normalized. If unit variance is assumed, the variance for the forward pass
becomes:
Var[Wh'-ι,h'] = (2-h'-1 + 8-H'-1)H'-τ - 4 = 8 ((4H'-1 + 1)H'-1 - 2)	(71)
And from the backward pass:
1
VarWh…']=(+ 广 - I
(72)
The variance requirement for both the forward and backward pass can be satisfied with
Var[Wh'-ι ,h`] = 4 for a large H'-i.
21
Published as a conference paper at ICLR 2020
C Arithmetic task
The aim of the “Arithmetic task” is to directly test arithmetic models ability to extrapolate beyond the
training range. Additionally, our generalized version provides a high degree of flexibility in how the
input is shaped, sampled, and the problem complexity.
Our “arithmetic task” is identical to the “simple function task” in the NALU paper (Trask et al., 2018).
However, as they do not describe their setup in details, we use the setup from Madsen & Johansen
(2019), which provide Algorithm 3, an evaluation-criterion to if and when the model has converged,
the sparsity error, as well as methods for computing confidence intervals for success-rate and the
sparsity error.
I--------subset---------1
©----------
r I I I I I I n
I-overlap-ι
Θ----------
I------subset-----1
Figure 6: Shows how the dataset is parameterized.
C.1 Dataset generation
The goal is to sum two random subsets of a vector x (a and b), and perform an arithmetic operation
on these (a ◦ b).
s1,end	s2,end
a =	xi, b =	xi,	t = a ◦ b	(73)
i=s1,start	i=s2,start
Algorithm 1 defines the exact procedure to generate the data, where an interpolation range will be
used for training and validation and an extrapolation range will be used for testing. Default values are
defined in table 3.
Table 3: Default dataset parameters for “Arithmetic task”
Parameter name	Default value	Parameter name	Default value
Input size	100	Interpolation range	U[1,2]
Subset ratio	0.25	Extrapolation range	U[2,6]
Overlap ratio	0.5		
Algorithm 1 Dataset generation algorithm for “Arithmetic task”		
1	function DATASET(OP(∙, ∙) : Operation, i : InputSize, S	SubsetRatio, o : OverlapRatio,
	R : Range)	
2	X《-UNIFORM(RloWer, Rupper, i)	. Sample i elements uniformly
3	k — UNIFORM(0, 1 - 2s - o)	. Sample offset
4	a — SUM(x[ik : i(k + s)])	. Create sum a from subset
5	b J SUM(x[i(k + s — o) : i(k + 2s — 0)])	. Create sum b from subset
6	t J OP(a, b)	. Perform operation on a and b
7	:	return x, t	
C.2 Model defintions and setup
Models are defined in table 4 and are all optimized with Adam optimization (Kingma & Ba, 2014)
using default parameters, and trained over 5 ∙ 106 iterations. Training takes about 8 hours on a single
CPU core(8-Core Intel Xeon E5-2665 2.4GHz). We run 19150 experiments on a HPC cluster.
22
Published as a conference paper at ICLR 2020
The training dataset is continuously sampled from the interpolation range where a different seed is
used for each experiment, all experiments use a mini-batch size of 128 observations, a fixed validation
dataset with 1 ∙ 104 observations sampled from the interpolation range, and a fixed test dataset with
1 ∙ 104 observations sampled from the extrapolation range.
Table 4: Model definitions
Model	Layer 1	Layer 2	^ λsparse	λstart	λend
NMU	NAU	NMU	10	106	2 • 106
NAU	NAU	NAU	0.01	5 • 103	5 • 104
NAC.	NAC+	NAC.	—	—	—
NAC∙,σ	NAC+	NAC.,σ	—	—	—
NAC.,NMU	NAC+	NAC.,NMU	10	106	2 • 106
NAC+	NAC+	NAC+	—	—	—
NALU	NALU	NALU	—	—	—
Linear	Linear	Linear	—	—	—
ReLU	ReLU	ReLU	—	—	—
ReLU6	ReLU6	ReLU6	—	—	—
C.3 Ablation study
To validate our model, we perform an ablation on the multiplication problem. Some noteworthy
observations:
1.	None of the W constraints, such as Rsparse and clamping W to be in [0, 1], are necessary
when the hidden size is just 2.
2.	Removing the Rsparse causes the NMU to immediately fail for larger hidden sizes.
3.	Removing the clamping of W does not cause much difference. This is because Rsparse also
constrains W outside of [0, 1]. The regularizer used here is Rsparse = min(|W |, |1 - W|),
which is identical to the one used in other experiments in [0, 1], but is also valid outside [0, 1].
Doing this gives only a slightly slower convergence. Although, this can not be guaranteed in
general, as the regularizer is omitted during the initial optimization.
4.	Removing both constraints, gives a somewhat satisfying solution, but with a lower success-
rate, slower convergence, and higher sparsity error.
In conclusion both constraints are valuable, as they provide faster convergence and a sparser solution,
but they are not critical to the success-rate of the NMU.
4e+06
3e+06
2e+06
1e+06
Solved at iteration step
0e+00
0.00
0.25
0.20
0.15
0.10
0.05
1.00
0.75
0.50
0.25
0.00
2468	10	2468	10	2468	10
Hidden size
model →- NMU →- nmu, no RsParse ♦ NMU, no RSParSe, no W-clamp →- NMU, no W-clamp
Figure 7:	Ablation study where Rsparse is removed and the clamping of W is removed. There are 50
experiments with different seeds, for each configuration.
23
Published as a conference paper at ICLR 2020
C.4 Effect of dataset parameter
To stress test the models on the multiplication task, we vary the dataset parameters one at a time while
keeping the others at their default value (default values in table 3). Each runs for 50 experiments with
different seeds. The results, are visualized in figure 8.
In figure 3, the interpolation-range is changed, therefore the extrapolation-range needs to be
changed such it doesn’t overlap. For each interpolation-range the following extrapolation-
range is used: U[-2, -1] uses U[-6, -2], U[-2, 2] uses U[-6, -2] ∪ U[2, 6], U[0, 1] uses U[1, 5],
U[0.1, 0.2] uses U[0.2, 2], U[1.1, 1.2] uses U[1.2, 6], U[1, 2] uses U[2, 6], U[10, 20] uses U[20, 40].
size 0fthe input vector
0e+00
0.00	0.25	0.50	0.75	1.00
The ratio of which subsets ove rlap with each other
Relative size of subsets compared to input size
0.1	0.2	0.3	0.4	0.5
model →- NAC.,nmu + NAC.,σ →- NAC. + Gated NAU/NMU →- NALU →- NMU
Figure 8:	Shows the effect of the dataset parameters.
C.5 Gating convergence experiment
In the interest of adding some understand of what goes wrong in the NALU gate, and the shared
weight choice that NALU employs to remedy this, we introduce the following experiment.
We train two models to fit the arithmetic task. Both uses the NAC+ in the first layer and NALU in the
second layer. The only difference is that one model shares the weight between NAC+ and NAC. in
the NALU, and the other treat them as two separate units with separate weights. In both cases NALU
should gate between NAC+ and NAC. and choose the appropriate operation. Note that this NALU
24
Published as a conference paper at ICLR 2020
model is different from the one presented elsewhere in this paper, including the original NALU paper
(Trask et al., 2018). The typical NALU model is just two NALU layers with shared weights.
Furthermore, we also introduce a new gated unit that simply gates between our proposed NMU and
NAU, using the same sigmoid gating-mechanism as in the NALU. This combination is done with
seperate weights, as NMU and NAU use different weight constrains and can therefore not be shared.
The models are trained and evaluated over 100 different seeds on the multiplication and addition task.
A histogram of the gate-value for all seeds is presented in figure 9 and table 5 contains a summary.
Some noteworthy observations:
1.	When the NALU weights are separated far more trials converge to select NAC+ for both
the addition and multiplication task. Sharing the weights between NAC+ and NAC. makes
the gating less likely to converge for addition.
2.	The performance of the addition task is dependent on NALU selecting the right operation. In
the multiplication task, when the right gate is selected, NAC. do not converge consistently,
unlike our NMU that converges more consistently.
3.	Which operation the gate converges to appears to be mostly random and independent of the
task. These issues are caused by the sigmoid gating-mechanism and thus exists independent
of the used sub-units.
These observations validates that the NALU gating-mechanism does not converge as intended. This
becomes a critical issues when more gates are present, as is normally the case. E.g. when stacking
multiple NALU layers together.
Ooooooo
6 4 2 6 4 2
IUn0。
Gated NAU/NMU	NALU (separate)
NALU (shared)
Multiplication
0
mul
Addition
solved
FALS FALSE
TRU TRUE
0.25 0.50 0.75 add mul 0.25 0.50 0.75 add mul 0.25 0.50 0.75 add
Gate
Figure 9:	Shows the gating-value in the NALU layer and a variant that uses NAU/NMU instead of
NAC+/NAC. . Separate/shared refers to the weights in NAC+/NAC. used in NALU.
Table 5: Comparison of the success-rate, when the model converged, and the sparsity error, with 95%
confidence interval on the “arithmetic datasets” task. Each value is a summary of 100 different seeds.
Op	Model	Success	Solved at iteration step		Sparsity error
		Rate	Median	Mean	Mean
	Gated NAU/NMU	62% +9% 62% -10%	1.5 ∙ 106	1.5 ∙106+3:8:104	-n In-5 +2.3∙10-5 5.0 ∙ 10 -1.8∙10-5
×	NALU (separate)	22% +-97%%	2.8 ∙ 106	q O In6 +3.9∙105 3.3 T0 -3.6∙105	5.8 ∙10-2 -4.3∙10-2
	NALU (shared)	24% +-97%%	2.9 ∙ 106	3∙3 W +3.6∙105	1.0 ∙10-3 -4:5:10—4
	Gated NAU/NMU	37% +10% 37% -9%	1.9 ∙ 104	4∙2 ∙105 +6:7:104	1.7 ∙10-1 +4:0110-2
+	NALU (separate)	51% -+1100%%	1.4 ∙ 105	2.9 ∙ ι05 +3.3:104	ιoι ∩-1 +1.4∙10 2 1.8 ∙ 10	-1410-2
	NALU (shared)	34% +-190%%	1.8 ∙ 105	3.ι ∙ 105 +4:4:104	1.8 ∙10-1-2.1∙10-2
25
Published as a conference paper at ICLR 2020
C.6 Regularization
The λstart and λend are simply selected based on how much time it takes for the model to converge.
The sparsity regularizer should not be used during early optimization as this part of the optimization
is exploratory and concerns finding the right solution by getting each weight on the right side of
±0.5.
In figure 10, 11 and 12 the scaling factor λsparse is optimized.
λsparse = λsparse max(min(Tt~"tart , 1), 0)	(74)
λend - λstart
τ-1∙	<C CI	CC , C C	♦ TL T * T T	.1	.,1	. ∙	1	.	, C .1	.	,♦
Figure 10:	Shows effect of λsparse m NAU on the arithmetic dataset for the + operation.
1.00
0.75
0.50
0.25
0.00
0	0,01 0.1	1	10 100
Success rate
Solved at iteration step
Sparse regualizer
τ-,∙	<< CI	CC .cC	♦、TATT	.1	.,1	1,	.C .1
Figure 11:	Shows effect of λsparse in NAU on the arithmetic dataset for the — operation.
Success rate
Sparse regualizer
τ-,∙	TCCl	CC .cC	.∖τ∙w<∙ττ	• 1	.,1	1, .c .1	..	.♦
Figure 12:	Shows effect of λsparse in NMU on the arithmetic dataset for the X operation.
C.7 Comparing all models
Table 6 compares all models on all operations used in NALU (Trask et al., 2018). All variations
of models and operations are trained for 100 different seeds to build confidence intervals. Some
noteworthy observations are:
26
Published as a conference paper at ICLR 2020
1.	Division does not work for any model, including the NAC. and NALU models. This may
seem surprising but is actually in line with the results from the NALU paper (Trask et al.
(2018), table 1) where there is a large error given the interpolation range. The extrapolation
range has a smaller error, but this is an artifact of their evaluation method where they
normalize with a random baseline. Since a random baseline will have a higher error for the
extrapolation range, errors just appear to be smaller. A correct solution to division should
have both a small interpolation and extrapolation error.
2.	NAC. and NALU are barely able to learn √Z, with just 2% success-rate for NALU and 7%
success-rate for NAC..
3.	NMU is fully capable of learning z2 . It learn this by learning the same subset twice in the
NAU layer, this is also how NAC. learn z2 .
4.	The Gated NAU/NMU (discussed in section C.5) works very poorly, because the NMU
initialization assumes that E[zh`-i] = 0. This is usually true, as discussed in section 2.6,
but not in this case for the first layer. In the recommended NMU model, the NMU layer
appears after NAU, which causes that assumption to be satisfied.
Table 6: Comparison of the success-rate, when the model converged, and the sparsity error, with 95%
confidence interval on the “arithmetic datasets” task. Each value is a summary of 100 different seeds.
OP	Model	Success	Solved at iteration steP		SParsity error
		Rate	Median	Mean	Mean
	NAC.,NMU	93% +-47%%	1.8 ∙ 106	2∙0 ∙106 -9.7∙104	9 5 10-7 +4.2∙10-7 9.5 T0	-4.2∙10-7
	NAC.,σ	100% -+04%%	2.5 ∙ 106	2∙6 ∙ ι06 -7.8∙104	4 6 10-5 +5.0∙10-6 4.6 ∙ 10	-5.6∙10-6
	NAC.	31% +-180%%	2.8 ∙ 106	3.0 ∙106 -2.9∙105	5 8 10-4 +4.8∙10-4 5.8 ∙ 10	-2.6∙10-4
	NAC+	0% +-40%%	—	—	—
	Gated NAU ate NMU	0% +-40%%	—	—	—
	Linear	0% +-40%%	—	—	—
×	NALU	0% +-40%%	—	—	—
	NAU	0% +-40%%	—	—	—
	NMU	98% +-15%%	1.4 ∙ 106	1.5 ∙106-6.6∙104	4∙2 ∙10-7 -2.9∙10-8
	ReLU	0% +-40%%	—	—	—
	ReLU6	0% +-40%%	—	—	—
	NAC.,NMU	0% +4% 0% -0%	—	—	—
	NAC.,σ	0% +-40%%	—	—	—
	NAC.	0% +-40%%	—	—	—
	NAC+	0% +-40%%	—	—	—
	NAU Gated NMU	0% +-40%%	—	—	—
	Linear	0% +-40%%	—	—	—
/	NALU	0% +-40%%	—	—	—
	NAU	0% +4% 0% -0%	—	—	—
	NMU	0% +-40%%	—	—	—
	ReLU	0% +-40%%	—	—	—
	ReLU6	0% +-40%%	—	—	—
27
Published as a conference paper at ICLR 2020
Table 6: Comparison of the success-rate, when the model converged, and the sparsity error, with 95% confidence interval on the “arithmetic datasets” task. Each value is a summary of 100 different seeds. (continued)					
OP	Model	Success		Solved at	SParsity error
		Rate	Median	Mean	Mean
	NAC.,nmu	0% +-40%%	—	—	—
	NAC∙,σ	0% +-40%%	—	—	—
	NAC∙	0% +-40%%	—	—	—
	NAC+	100% -+04%%	2.5 ∙ 105	4 9 ∙ 105 +52104 .∙v ɪʊ -45104	2.3 ∙ 10-1+6.5∙10-3 -6.5∙10 3
	NAU Gated NMU	0% +-40%%	—	—	—
	Linear	100% -+04%%	6.1 ∙ 104	6.3 ∙104 -3.3:103	- c 1 ∩-1 +3.6∙10-4 2.5 ^ 10	-3.6∙10-4
+	NALU	14% +-85%%	1.5 ∙ 106	1.6 "-3.8:105	1 7 10-1 +2.7∙10-2 1.7 ^ 10	-2.5∙10-2
	NAU	100% -+04%%	1.8 ∙ 104	3.9 ∙ 105 -4.5:104	3.2 ∙10-5 -1.3110-5
	NMU	0% +-40%%	—	—	—
	ReLU	62% +-910%%	6.2 ∙ 104	7.6 ∙104 -7.3∙103	2 5 10-1 +2.4∙10-3 2.5 ∙ 10	-2.4∙10-3
	ReLU6	0% +-40%%	—	—	—
	NAC∙,NMU	0% +4% 0% -0%	—	—	—
	NAC∙,σ	0% +-40%%	—	—	—
	NAC∙	0% +-40%%	—	—	—
	NAC+	100% -+04%%	9.0 ∙ 103	3.7 ∙105 璨∙∙104	- - - ∩-1 +5.4∙10-3 2.3 ^ 10	-5.4∙10-3
	NAU Gated NMU	0% +-40%%	—	—	—
	Linear	7% +-74%%	3.3 ∙ 106	1.4 ∙106 -6.1∙105	1 8 10-1 +7.2∙10-2 1.8 ∙ 10	-5.8∙10-2
-	NALU	14% +-85%%	1.9 ∙ 106	1 9 ∙ 106 +4∙4∙105 l.y ɪʊ -4.5∙105	- -	- ∩-1 +2.2∙10 2 2.1 ∙ 10	-2.2∙10-2
	NAU	100% -+04%%	5.0 ∙ 103	Irl ∏5 +17104 1.6 ∙ 10 -1.6∙104	6.6 ∙10-2 -1.5∙10-2
	NMU	56% +-910%%	1.0 ∙ 106	1.0 ∙ 106 -5.8∙102	3.4 ∙10-4 -3.6∙10-5
	ReLU	0% +-40%%	—	—	—
	ReLU6	0% +-40%%	—	—	—
	NAC∙,nmu	3% +-52%%	1.0 ∙ 106	Inln6 +NaN ∙10-Inf 1.0 ∙ 10 -NaN∙10-Inf	1 - In-I +8.3∙10-3 1.7 ∙ 10	-8.1∙10-3
	NAC∙,σ	0% +-40%%	—	—	—
	NAC∙	7% +-74%%	4.0 ∙ 105	1.5 ∙106 -5.6∙105	24.10-1+1710-2 2.4 10	-1.7∙10-2
	NAC+	0% +-40%%	—	—	—
	Gtd NAU Gated NMU	0% +-40%%	—	—	—
	Linear	0% +-40%%	—	—	—
√z	NALU	2% +-51%%	2.6 ∙ 106	3.3 ∙106 -2.8∙106	- ∩ In-I +2.5∙10-6 5.0 ∙ 10	-8.0∙10-6
	NAU	0% +-40%%	—	—	—
	NMU	0% +-40%%	—	—	—
	ReLU	0% +-40%%	—	—	—
	ReLU6	0% +-40%%	—	—	—
28
Published as a conference paper at ICLR 2020
Table 6: Comparison of the success-rate, when the model converged, and the sparsity error, with 95% confidence interval on the “arithmetic datasets” task. Each value is a summary of 100 different seeds. (continued)					
OP	Model	Success		Solved at	SParsity error
		Rate	Median	Mean	Mean
	NAC.,nmu	100% -+04%%	1.4 ∙ 106	1.5 ∙106-7.4∙104	2∙9 ∙10-7 -1.4∙10-8
	NAC∙,σ	100% -+04%%	1.9 ∙ 106	1 9 ∙ 106 +53104 l.y ɪʊ -6.2∙104	1 8 10-2 +4.3∙10-4 1.8 ∙ 10	-4.3∙10-4
	NAC∙	77% +-79%%	3.3 ∙ 106	3.2 ∙ 106 -2.0∙105	1 8 10-2 +5.8∙10-4 1.8 ∙ 10	-5.7∙10-4
	NAC+	0% +-40%%	—	—	—
	Gated NAU ated NMU	0% +-40%%	—	—	—
	Linear	0% +-40%%	—	—	—
z2	NALU	0% +-40%%	—	—	—
	NAU	0% +-40%%	—	—	—
	NMU	100% -+04%%	1.2 ∙ 106	1.3 ∙ 106 -3.6:104	3 7 10-5 +5.4∙10-5 3.7 ∙ 10	-3.7∙10-5
	ReLU	0% +-40%%	—	—	—
	ReLU6	0% +-40%%	—	—	—
29
Published as a conference paper at ICLR 2020
D	Sequential MNIST
D. 1 Task and evaluation criteria
The simple function task is a purely synthetic task, that does not require a deep network. As such it
does not test if an arithmetic layer inhibits the networks ability to be optimized using gradient decent.
The sequential MNIST task takes the numerical value of a sequence of MNIST digits and applies a
binary operation recursively. Such that ti = Op(ti-1, zt), where zt is the MNIST digit’s numerical
value. This is identical to the “MNIST Counting and Arithmetic Tasks” in Trask et al. (2018, section
4.2). We present the addition variant to validate the NAU’s ability to backpropagate, and we add an
additional multiplication variant to validate the NMU’s ability to backpropagate.
The performance of this task depends on the quality of the image-to-scalar network and the arithmetic
layer’s ability to model the scalar. We use mean-square-error (MSE) to evaluate joint image-to-scalar
and arithmetic layer model performance. To determine an MSE threshold from the correct prediction
we use an empirical baseline. This is done by letting the arithmetic layer be solved, such that only
the image-to-scalar is learned. By learning this over multiple seeds an upper bound for an MSE
threshold can be set. In our experiment we use the 1% one-sided upper confidence-interval, assuming
a student-t distribution.
Similar to the simple function task we use a success-criteria as reporting the MSE is not interpretable
and models that do not converge will obscure the mean. Furthermore, because the operation is applied
recursively, natural error from the dataset will accumulate over time, thus exponentially increasing the
MSE. Using a baseline model and reporting the successfulness solves this interpretation challenge.
D.2 Addition of sequential MNIST
Figure 13 shows results for sequential addition of MNIST digits. This experiment is identical to
the MNIST Digit Addition Test from Trask et al. (2018, section 4.2). The models are trained on a
sequence of 10 digits and evaluated on sequences between 1 and 1000 MNIST digits.
Note that the NAU model includes the Rz regularizer, similarly to the “Multiplication of sequential
MNIST” experiment in section 4.2. However, because the weights are in [-1, 1], and not [0, 1], and
the idendity of addition is 0, and not 1, Rz is
Rz
1
H'-ιH'
H' h'-i
XX (i-∣Wh'-ι,h'l) ∙ Z2'-1
h` h`-1
(75)
To provide a fair comparison, a variant of NAC+ that also uses this regularizer is included, this
variant is called NAC+,Rz . Section D.3 provides an ablation study of the Rz regularizer.
Success rate	Solved at iteration step
Sparsity error
0.12 I
0.09
0.06
0.03
0.00 ▲▲▲▲▲▲上▲▲▲▲上
1 10 200 400 600 800 1000
1 10 200 400 600 800 1000	1 10 200 400 600 800 1000
Extrapolation length
model →- NAC+,rz →- NAC+ →- LSTM →- NALU →- NAU
Figure 13:	Shows the ability of each model to learn the arithmetic operation of addition and back-
propagate through the arithmetic layer in order to learn an image-to-scalar value for MNIST digits.
The model is tested by extrapolating to larger sequence lengths than what it has been trained on. The
NAU and NAC+,Rz models use the Rz regularizer from section 4.2.
30
Published as a conference paper at ICLR 2020
D.3 Sequential addtion without THE Rz REGULARIZER
As an ablation study of the Rz regularizer, figure 14 shows the NAU model without the Rz regularizer.
Removing the regularizer causes a reduction in the success-rate. The reduction is likely larger, as
compared to sequential multiplication, because the sequence length used for training is longer. The
loss function is most sensitive to the 10th output in the sequence, as this has the largest scale. This
causes some of the model instances to just learn the mean, which becomes passable for very long
sequences, which is why the success-rate increases for longer sequences. However, this is not a valid
solution. A well-behavior model should be successful independent of the sequence length.
Sparsity error
0.12 —
0.09
0.06
0.03
CCC	.	A ▲▲▲▲▲▲▲▲
0.00 . I ....................
1 10 200 400 600 800 1000
Extrapolation length
model →- NAC+,rz →- NAC+ →- LSTM →- NALU →- NAU
Figure 14:	Same as figure 13, but where the NAU model do not use the Rz regularizer.
D.4 SEQUENTIAL MULTIPLICATION WITHOUT THE Rz REGULARIZER
As an ablation study of the Rz regularizer figure 15 shows the NMU and NAC.,nmu models without
the Rz regularizer. The success-rate is somewhat similar to figure 4. However, as seen in the “sparsity
error” plot, the solution is quite different.
Success rate
Solved at iteration step
1.00
0.75
0.50
0.25
0.00
3e+05
2e+05
1e+05
0e+00
0.3
0.2
0.1
0.0
Sparsity error
Iiiiiiiiiii
1 2 4 6 8 10 12 14 16 18 20
10 12 14 16 18 20	1 2 4 6 8 10 12 14 16 18 20
I I
1 2
I I
4 6
I
8
Extrapolation length
model -∙- NAC∙,nmu ~9~ NAC∙,σ ~9~ NAC.	LSTM NALU NMU
Figure 15:	Shows the ability of each model to learn the arithmetic operation of addition and back-
propagate through the arithmetic layer in order to learn an image-to-scalar value for MNIST digits.
The model is tested by extrapolating to larger sequence lengths than what it has been trained on. The
NMU and NAC. NMU models do not use the Rz regularizer.
31