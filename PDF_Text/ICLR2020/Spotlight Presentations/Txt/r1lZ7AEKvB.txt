Published as a conference paper at ICLR 2020
The Logical Expressiveness of
Graph Neural Networks
Pablo Barcelo
IMC, PUC & IMFD Chile
Jorge Perez
DCC, UChile & IMFD Chile
Egor V. Kostylev
University of Oxford
Juan Reutter
DCC, PUC & IMFD Chile
Mikael Monet
IMFD Chile
Juan-Pablo Silva
DCC, UChile
Ab stract
The ability of graph neural networks (GNNs) for distinguishing nodes in graphs
has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for
checking graph isomorphism. This characterization, however, does not settle the
issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs
as true or false) can be expressed by GNNs. We tackle this problem by focusing
on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied
fragment of first order logic. FOC2 is tightly related to the WL test, and hence to
GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs,
in which the features of each node in the graph are updated, in successive layers,
only in terms of the features of its neighbors. We show that this class of GNNs is
too weak to capture all FOC2 classifiers, and provide a syntactic characterization
of the largest subclass of FOC2 classifiers that can be captured by AC-GNNs.
This subclass coincides with a logic heavily used by the knowledge representation
community. We then look at what needs to be added to AC-GNNs for capturing
all FOC2 classifiers. We show that it suffices to add readout functions, which
allow to update the features of a node not only in terms of its neighbors, but also
in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We
experimentally validate our findings showing that, on synthetic data conforming
to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs
can generalize even to graphs of sizes not seen during training.
1	Introduction
Graph neural networks (GNNs) (Merkwirth & Lengauer, 2005; Scarselli et al., 2009) are a class
of neural network architectures that has recently become popular for a wide range of applications
dealing with structured data, e.g., molecule classification, knowledge graph completion, and Web
page ranking (Battaglia et al., 2018; Gilmer et al., 2017; Kipf & Welling, 2017; Schlichtkrull et al.,
2018). The main idea behind GNNs is that the connections between neurons are not arbitrary but
reflect the structure of the input data. This approach is motivated by convolutional and recurrent
neural networks and generalize both of them (Battaglia et al., 2018). Despite the fact that GNNs
have recently been proven very efficient in many applications, their theoretical properties are not
yet well-understood. In this paper we make a step towards understanding their expressive power
by establishing connections between GNNs and well-known logical formalisms. We believe these
connections to be conceptually important, as they permit us to understand the inherently procedural
behavior of some fragments of GNNs in terms of the more declarative flavor of logical languages.
Two recent papers (Morris et al., 2019; Xu et al., 2019) have started exploring the theoretical prop-
erties of GNNs by establishing a close connection between GNNs and the Weisfeiler-Lehman (WL)
test for checking graph isomorphism. The WL test works by constructing a labeling of the nodes of
the graph, in an incremental fashion, and then decides whether two graphs are isomorphic by com-
paring the labeling of each graph. To state the connection between GNNs and this test, consider the
simple GNN architecture that updates the feature vector of each graph node by combining it with the
aggregation of the feature vectors of its neighbors. We call such GNNs aggregate-combine GNNs,
1
Published as a conference paper at ICLR 2020
or AC-GNNs. The authors of these papers independently observe that the node labeling produced
by the WL test always refines the labeling produced by any GNN. More precisely, if two nodes are
labeled the same by the algorithm underlying the WL test, then the feature vectors of these nodes
produced by any AC-GNN will always be the same. Moreover, there are AC-GNNs that can repro-
duce the WL labeling, and hence AC-GNNs can be as powerful as the WL test for distinguishing
nodes. This does not imply, however, that AC-GNNs can capture every node classifier—that is,
a function assigning true or false to every node—that is refined by the WL test. In fact, it is not
difficult to see that there are many such classifiers that cannot be captured by AC-GNNs; one simple
example is a classifier assigning true to every node if and only if the graph has an isolated node.
Our work aims to answer the question of what are the node classifiers that can be captured by GNN
architectures such as AC-GNNs.
To start answering this question, we propose to focus on logical classifiers—that is, on unary formu-
las expressible in first order predicate logic (FO): such a formula classifies each node v according
to whether the formula holds for v or not. This focus gives us an opportunity to link GNNs with
declarative and well understood formalisms, and to establish conclusions about GNNs drawing upon
the vast amount of work on logic. For example, if one proves that two GNN architectures are cap-
tured with two logics, then one can immediately transfer all the knowledge about the relationships
between those logics, such as equivalence or incomparability of expressiveness, to the GNN setting.
For AC-GNNs, a meaningful starting point to measure their expressive power is the logic FOC2, the
two variable fragment of first order predicate logic extended with counting quantifiers of the form
∃≥N夕，which state that there are at least N nodes satisfying formula 夕(Cai et al., 1992). Indeed,
this choice of FOC2 is justified by a classical result due to Cai et al. (1992) establishing a tight
connection between FOC2 and WL: two nodes in a graph are classified the same by the WL test if
and only if they satisfy exactly the same unary FOC2 formulas. Moreover, the counting capabilities
of FOC2 can be mimicked in FO (albeit with more than just two variables), hence FOC2 classifiers
are in fact logical classifiers according to our definition.
Given the connection between AC-GNNs and WL on the one hand, and that between WL and FOC2
on the other hand, one may be tempted to think that the expressivity of AC-GNNs coincides with
that of FOC2 . However, the reality is not as simple, and there are many FOC2 node classifiers (e.g.,
the trivial one above) that cannot be expressed by AC-GNNs. This leaves us with the following
natural questions. First, what is the largest fragment of FOC2 classifiers that can be captured by
AC-GNNs? Second, is there an extension of AC-GNNs that allows to express all FOC2 classifiers?
In this paper we provide answers to these two questions. The following are our main contributions.
•	We characterize exactly the fragment of FOC2 formulas that can be expressed as AC-
GNNs. This fragment corresponds to graded modal logic (de Rijke, 2000), or, equivalently,
to the description logic ALCQ, which has received considerable attention in the knowledge
representation community (Baader et al., 2003; Baader & Lutz, 2007).
•	Next we extend the AC-GNN architecture in a very simple way by allowing global read-
outs, where in each layer we also compute a feature vector for the whole graph and combine
it with local aggregations; we call these aggregate-combine-readout GNNs (ACR-GNNs).
These networks are a special case of the ones proposed by Battaglia et al. (2018) for re-
lational reasoning over graph representations. In this setting, we prove that each FOC2
formula can be captured by an ACR-GNN.
We experimentally validate our findings showing that the theoretical expressiveness of ACR-GNNs,
as well as the differences between AC-GNNs and ACR-GNNs, can be observed when we learn from
examples. In particular, we show that on synthetic graph data conforming to FOC2 formulas, AC-
GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not
seen during training.
2	Graph Neural Networks
In this section we describe the architecture of AC-GNNs and introduce other related notions. We
concentrate on the problem of Boolean node classification: given a (simple, undirected) graph
G = (V, E) in which each vertex v ∈ V has an associated feature vector xv, we wish to clas-
sify each graph node as true or false; in this paper, we assume that these feature vectors are one-hot
2
Published as a conference paper at ICLR 2020
encodings of node colors in the graph, from a finite set of colors. The neighborhood NG (v) of a
node v ∈ V is the set {u | {v, u} ∈ E}.
The basic architecture for GNNs, and the one studied in recent studies on GNN expressibility (Mor-
ris et al., 2019; Xu et al., 2019), consists of a sequence of layers that combine the feature vectors
of every node with the multiset of feature vectors of its neighbors. Formally, let {AGG(i) }iL=1 and
{COM(i)}iL=1 be two sets of aggregation and combination functions. An aggregate-combine GNN
(i)
(AC-GNN) computes vectors xv for every node v of the graph G, via the recursive formula
Xvi) = COM⑶ XviT), AGG ⑴(IxuiT) ∣ U ∈ NG(V)») , for i = 1,...,L	(1)
where each x(v0) is the initial feature vector xv ofv. Finally, each node v ofG is classified according
to a Boolean classification function CLS applied to X(vL). Thus, an AC-GNN with L layers is defined
as a tuple A = ({AGG(i)}L=ι, {COM(i)}L=ι, CLS), and we denote by A(G, V) the class (i.e., true
or false) assigned by A to each node v in G.1
There are many possible aggregation, combination, and classification functions, which produce dif-
ferent classes of GNNs (Hamilton et al., 2017; Kipf & Welling, 2017; Morris et al., 2019; Xu et al.,
2019). A simple, yet common choice is to consider the sum of the feature vectors as the aggregation
function, and a combination function as
COM⑴(xι, x2) = f (xιC ⑴ + x2A⑴ + b⑴)，
(2)
where C(i) and A(i) are matrices of parameters, b(i) is a bias vector, and f is a non-linearity func-
tion, such as relu or sigmoid. We call simple an AC-GNN using these functions. Furthermore, we
say that an AC-GNN is homogeneous if all AGG(i) are the same and all COM(i) are the same (share
the same parameters across layers). In most of our positive results we construct simple and homoge-
neous GNNs, while our negative results hold in general (i.e., for GNNs with arbitrary aggregation,
combining, and classification functions).
The Weisfeiler-Lehman (WL) test is a powerful heuristic used to solve the graph isomorphism prob-
lem (Weisfeiler & Leman, 1968), or, for our purposes, to determine whether the neighborhoods of
two nodes in a graph are structurally close or not. Due to space limitations, we refer to (Cai et al.,
1992) for a formal definition of the underlying algorithm, giving only its informal description: start-
ing from a colored graph, the algorithm iteratively assigns, for a certain number of rounds, a new
color to every node in the graph; this is done in such a way that the color of a node in each round
has a one to one correspondence with its own color and the multiset of colors of its neighbors in the
previous round. An important observation is that the rounds of the WL algorithm can be seen as the
layers of an AC-GNN whose aggregation and combination functions are all injective (Morris et al.,
2019; Xu et al., 2019). Furthermore, as the following proposition states, an AC-GNN classification
can never contradict the WL test.
Proposition 2.1 (Morris et al., 2019; Xu et al., 2019). If the WL test assigns the same color to two
nodes in a graph, then every AC-GNN classifies either both nodes as true or both nodes as false.
3	Connection between GNNs and logic
3.1	Logical node classifiers
Our study relates the power of GNNs to that of classifiers expressed in first order (FO) predicate logic
over (undirected) graphs where each vertex has a unique color (recall that we call these classifiers
logical classifiers). To illustrate the idea of logical node classifiers, consider the formula
α(x) := Red(X) ∧ ∃y(E(x, y) ∧ Blue(y)) ∧ ∃z(E(x,z) ∧ Green(z)).
(3)
1For graph classification, which we do not consider in this paper, the classification function CLS inputs the
multiset {x(vL) | v ∈ V } and outputs a class for the whole graph. Such a function is often called readout in
previous work (Morris et al., 2019; Xu et al., 2019). In this paper, however, we use the term readout to refer to
intermediate global operations performed while computing features for nodes (see Section 5).
3
Published as a conference paper at ICLR 2020
This formula has one free variable, x, which is not bounded by any quantifier of the form ∃ or ∀,
and two quantified variables y and z. In general, formulas with one free variable are evaluated over
nodes of a given graph. For example, the above formula evaluates to true exactly in those nodes v
whose color is Red and that have both a Blue and a Green neighbor. In this case, we say that node v
of G satisfies α, and denote this by (G, v) |= α.
Formally, a logical (node) classifier is given by a formula 夕(x) in FO logic with exactly one free
variable. This formula classifies as true those nodes V in G such that (G, V)=夕，while all other
nodes (i.e., those with (G, V)=夕)are classified as false. We say that a GNN classifier captures a
logical classifier when both classifiers coincide over every node in every possible input graph.
Definition 3.1. A GNN classifier A captures a logical classifier 夕(x) if for every graph G and
node v in G, it holds that A(G, V) = true ifand only if (G, V)= A
3.2	LOGIC FOC2
Logical classifiers are useful as a declarative formalism, but as we will see, they are too powerful
to compare them to AC-GNNs. Instead, for reasons we explain later we focus on classifiers given
by formulas in FOC2, the fragment ofFO logic that only allows formulas with two variables, but in
turn permits to use counting quantifiers.
Let us briefly introduce FOC2 and explain why it is a restriction of FO logic. The first remark is
that reducing the number of variables used in formulas drastically reduces their expressive power.
Consider for example the following FO formula expressing that x is a red node, and there is another
node, y, that is not connected to x and that has at least two blue neighbors, z1 and z2 :
β(x) := Red(x) ∧ ∃y(-E(x, y)∧∃z1∃z2 [ E(y, zι)∧E(y, z2)∧zι = z2∧Blue(zι)∧Blue(z2)]).
The formula β (x) uses four variables, but it is possible to find an equivalent one with just three: the
trick is to reuse variable x and replace every occurrence of z2 in β(x) by x. However, this is as far
as we can go with this trick: β(x) does not have an equivalent formula with less than three variables.
In the same way, the formula α(x) given in Equation (3) can be expressed using only two variables,
x and y, simply by reusing y in place of z .
That being said, it is possible to extend the logic so that some node properties, such as the one
defined by β(x), can be expressed with even less variables. To this end, consider the counting
quantifier ∃≥N for every positive integer N . Analogously to how the quantifier ∃ expresses the
existence of a node satisfying a property, the quantifier ∃≥N expresses the existence of at least N
different nodes satisfying a property. For example, with ∃≥2 we can express β(x) by using only two
variables by means of the classifier
γ(x) := Red(x) ∧ ∃y (-E(x, y) ∧ ∃≥2x [ E(y, x) ∧ Blue(x) ]).	(4)
Based on this idea, the logic FOC2 allows for formulas using all FO constructs and counting quanti-
fiers, but restricted to only two variables. Note that, in terms of their logical expressiveness, we have
that FOC2 is strictly less expressive than FO (as counting quantifiers can always be mimicked in FO
by using more variables and disequalities), but is strictly more expressive than FO2, the fragment of
FO that allows formulas to use only two variables (as β(x) belongs to FOC2 but not to FO2).
The following result establishes a classical connection between FOC2 and the WL test. Together
with Proposition 2.1, this provides a justification for our choice of logic FOC2 for measuring the
expressiveness of AC-GNNs.
Proposition 3.2 (Cai et al., 1992). For any graph G and nodes u, V in G, the WL test colors V and
u the same after any number of rounds iff u and V are classified the same by all FOC2 classifiers.
3.3	FOC2 and AC-GNN classifiers
Having Propositions 2.1 and 3.2, one may be tempted to combine them and claim that every FOC2
classifier can be captured by an AC-GNN. Yet, this is not the case as shown in Proposition 3.3 below.
In fact, while it is true that two nodes are declared indistinguishable by the WL test if and only if
they are indistinguishable by all FOC2 classifiers (Proposition 3.2), and if the former holds then such
nodes cannot be distinguished by AC-GNNs (Proposition 2.1), this by no means tells us that every
FOC2 classifier can be expressed as an AC-GNN.
4
Published as a conference paper at ICLR 2020
Proposition 3.3. There is an FOC2 classifier that is not captured by any AC-GNN.
One such FOC2 classifier is γ(x) in Equation (4), but there are infinitely many and even simpler
FOC2 formulas that cannot be captured by AC-GNNs. Intuitively, the main problem is that an AC-
GNN has only a fixed number L of layers and hence the information of local aggregations cannot
travel further than at distance L of every node along edges in the graph. For instance, the red node
in γ(x) may be farther away than the node with the blue neighbours, which means that AC-GNNs
would never be able to connect this information. Actually, both nodes may even be in different
connected components of a graph, in which case no number of layers would suffice.
The negative result of Proposition 3.3 opens up the following important questions.
1.	What kind of FOC2 classifiers can be captured by AC-GNNs?
2.	Can we capture FOC2 classifiers with GNNs using a simple extension of AC-GNNs?
We provide answers to these questions in the next two sections.
4	The expressive power of AC-GNNs
Towards answering our first question, we recall that the problem with AC-GNN classifiers is that
they are local, in the sense that they cannot see across a distance greater than their number of layers.
Thus, ifwe want to understand which logical classifiers this architecture is capable of expressing, we
must consider logics built with similar limitations in mind. And indeed, in this section we show that
AC-GNNs capture any FOC2 classifier as long as we further restrict the formulas so that they satisfy
such a locality property. This happens to be a well-known restriction of FOC2 , and corresponds
to graded modal logic (de Rijke, 2000) or, equivalently, to description logic ALCQ (Baader et al.,
2003), which is fundamental for knowledge representation: for instance, the OWL 2 Web Ontology
Language (Motik et al., 2012; W3C OWL Working Group, 2012) relies on ALCQ.
The idea of graded modal logic is to force all subformulas to be guarded by the edge predicate E .
This means that one cannot express in graded modal logic arbitrary formulas of the form ∃yφ(y),
i.e., whether there is some node that satisfies property 夕.Instead, one is allowed to check whether
some neighbor y of the node X where the formula is being evaluated satisfies 夕.That is, We are
allowed to express the formula ∃y (E(χ, y) ∧ 夕(y)) in the logic as in this case 夕(y) is guarded by
E(x, y). We can define this fragment ofFO logic using FO syntax as follows. A graded modal logic
formula is either Col(χ), for Col a node color, or one of the following, where 夕 and ψ are graded
modal logic formulas and N is a positive integer:
-φ(χ), ψ(χ) ∧ ψ(χ),	∃≥Ny (E(χ,y) ∧ o(y))∙
Notice then that the formula δ(x) := Red(x) ∧ ∃y E(x, y) ∧ Blue(y) is in graded modal logic,
but the logical classifier γ(χ) in Equation (4) is not, because the use of -E(x, y) as a guard is
disallowed. As required, we can now show that AC-GNNs can indeed capture all graded modal
logic classifiers.
Proposition 4.1. Each graded modal logic classifier is captured by a simple homogeneous AC-GNN.
The key idea of the construction is that the vectors’ dimensions used by the AC-GNN to label nodes,
represent the sub-formulas of the captured classifier. Thus, if a feature in a node is 1 then the node
satisfies the corresponding sub-formula, and the opposite holds after evaluating L layers, where L
is the “quantifier depth” of the classifier (which does not depend on the graph). The construction
uses simple, homogeneous AC-GNNs with the truncated relu non-linearity max(0, min(x, 1)). The
formal proof of Proposition 4.1, as well as other formal statements, can be found in the Appendix.
An interesting question that we leave as future work is to investigate whether the same kind of
construction can be done with AC-GNNs using different aggregate and combine operators than the
ones we consider here; for instance, using max instead of sum to aggregate the feature vectors of
the neighbors, or using other non-linearity such as sigmoid, etc.
The relationship between AC-GNNs and graded modal logic goes further: we can show that graded
modal logic is the “largest” class of logical classifiers captured by AC-GNNs. This means that the
only FO formulas that AC-GNNs are able to learn accurately are those in graded modal logic.
5
Published as a conference paper at ICLR 2020
Theorem 4.2. A logical classifier is captured by AC-GNNs if and only if it can be expressed in
graded modal logic.
The backward direction of this theorem is Proposition 4.1, while the proof of the forward direction
is based on a recently communicated extension of deep results in finite model theory (Otto, 2019).
We point out that the forward direction holds no matter which aggregate and combine operators are
considered, i.e., this is a limitation of the architecture for AC-GNNs, not of the specific functions
that one chooses to update the features.
5	GNNs for capturing FOC2
5.1	GNNs with global readouts
In this section we tackle our second question: which kind of GNN architecture we need to capture
all FOC2 classifiers? Recall that the main shortcoming of AC-GNNs for expressing such classifiers
is their local behavior. A natural way to break such a behavior is to allow for a global feature com-
putation on each layer of the GNN. This is called a global attribute computation in the framework
of Battaglia et al. (2018). Following the recent GNN literature (Gilmer et al., 2017; Morris et al.,
2019; Xu et al., 2019), we refer to this global operation as a readout.
Formally, an aggregate-combine-readout GNN (ACR-GNN) extends AC-GNNs by specifying read-
out functions {READ(i)}iL=1, which aggregate the current feature vectors of all the nodes in a graph.
Then, the vector x(vi) of each node v in G on each layer i, is computed by the following formula,
generalizing Equation (1):
Xvi) = COM⑺(XviT), AGG⑴([xUiT)I U ∈ NG(V)»), READ⑴([xfT)I U ∈ G»)). (5)
Intuitively, every layer in an ACR-GNN first computes (i.e., “reads out”) the aggregation over all
the nodes in G; then, for every node v, it computes the aggregation over the neighbors of v; and
finally it combines the features of v with the two aggregation vectors. All the notions about AC-
GNNs extend to ACR-GNNs in a straightforward way; for example, a simple ACR-GNN uses the
sum as the function READ(i) in each layer, and the combination function COM(i)(X1, X2, X3) =
f (xιC⑶ + X2 A(i) + X3R(i) + b(i)) With a matrix R(i), generalizing Equation (2).
5.2	ACR-GNNS AND FOC2
To see hoW a readout function could help in capturing non-local properties, consider again the logical
classifier γ(x) in Equation (4), that assigns true to every red node v as long as there is another node
not connected With v having tWo blue neighbors. We have seen that AC-GNNs cannot capture this
classifier. HoWever, using a single readout plus local aggregations one can implement this classifier
as folloWs. First, define by B the property “having at least 2 blue neighbors”. Then an ACR-GNN
that implements γ(x) can (1) use one aggregation to store in the local feature of every node if the
node satisfies B, then (2) use a readout function to count hoW many nodes satisfying B exist in
the Whole graph, and (3) use another local aggregation to count hoW many neighbors of every node
satisfiy B. Then γ is obtained by classifying as true every red node having less neighbors satisfying
B than the total number of nodes satisfying B in the Whole graph. It turns out that the usage of
readout functions is enough to capture all non-local properties of FOC2 classifiers.
Theorem 5.1. Each FOC2 classifier can be captured by a simple homogeneous ACR-GNN.
The construction is similar to that of Proposition 4.1 and uses simple, homogeneous ACR-GNNs—
that is, the readout function is just the sum of all the local node feature vectors. Moreover, the
readout functions are only used to deal With subformulas asserting the existence of a node that is
not connected to the current node in the graph, just as We have done for classifier γ(x). As an
intermediate step in the proof, We use a characterization of FOC2 using an extended version of
graded modal logic, Which Was obtained by Lutz et al. (2001). We leave as a challenging open
problem Whether FOC2 classifiers are exactly the logical classifiers captured by ACR-GNNs.
6
Published as a conference paper at ICLR 2020
5.3	Comparing the number of readout layers
The proof of Theorem 5.1 constructs GNNs whose number of layers depends on the formula being
captured—that is, readout functions are used unboundedly many times in ACR-GNNs for captur-
ing different FOC2 classifiers. Given that a global computation can be costly, one might wonder
whether this is really needed, or if it is possible to cope with all the complexity of such classifiers by
performing only few readouts. We next show that actually just one readout is enough. However, this
reduction in the number of readouts comes at the cost of severely complicating the resulting GNN.
Formally, an aggregate-combine GNN with final readout (AC-FR-GNN) results out of using any
number of layers as in the AC-GNN definition, together with a final layer that uses a readout func-
tion, according to Equation (5).
Theorem 5.2. Each FOC2 classifier is captured by an AC-FR-GNN.
The AC-FR-GNN in the proof of this theorem is not based on the idea of evaluating the formula
incrementally along layers, as in the proofs of Proposition 4.1 and Theorem 5.1, and it is not simple
(note that AC-FR-GNNs are never homogeneous). Instead, it is based on a refinement of the GIN
architecture proposed by Xu et al. (2019) to obtain as much information as possible about the local
neighborhood in graphs, followed by a readout and combine functions that use this information
to deal with non-local constructs in formulas. The first component we build is an AC-GNN that
computes an invertible function mapping each node to a number representing its neighborhood (how
big is this neighborhood depends on the classifier to be captured). This information is aggregated so
that we know for each different type of a neighborhood how many times it appears in the graph. We
then use the combine function to evaluate FOC2 formulas by decoding back the neighborhoods.
6 Experimental results
We perform experiments with synthetic data to empirically validate our results. The motivation of
this section is to show that the theoretical expressiveness of ACR-GNNs, as well as the differences
between AC- and ACR-GNNs, can actually be observed when we learn from examples. We perform
two sets of experiments: experiments to show that ACR-GNNs can learn a very simple FOC2 node
classifier that AC-GNNs cannot learn, and experiments involving complex FOC2 classifiers that
need more intermediate readouts to be learned. We implemented our experiments in the PyTorch
Geometric library (Fey & Lenssen, 2019). Besides testing simple AC-GNNs, we also tested the GIN
network proposed by Xu et al. (2019) (we consider the implementation by Fey & Lenssen (2019)
and adapted it to classify nodes). Our experiments use synthetic graphs, with five initial colors
encoded as one-hot features, divided in three sets: train set with 5k graphs of size up to 50-100
nodes, test set with 500 graphs of size similar to the train set, and another test set with 500 graphs of
size bigger than the train set. We tried several configurations for the aggregation, combination and
readout functions, and report the accuracy on the best configuration. Accuracy in our experiments
is computed as the total number of nodes correctly classified among all nodes in all the graphs in
the dataset. In every case we run up to 20 epochs with the Adam optimizer. More details on the
experimental setting, data, and code can be found in the Appendix. We finally report results on a
real benchmark (PPI) where we did not observe an improvement of ACR-GNNs over AC-GNNs.
Separating AC-GNNs and ACR-GNNs We consider a very simple FOC2 formula defined by
α(x) := Red(x) ∧ ∃y Blue(y), which is satisfied by every red node in a graph provided that the
graph contains at least one blue node. We tested with line-shaped graphs and Erdos-Renyi (E-R)
random graphs with different connectivities. In every set (train and test) we consider 50% of graphs
not containing any blue node, and 50% containing at least one blue node (around 20% of nodes are
in the true class in every set). For both types of graphs, already single-layer ACR-GNNs showed
perfect performance (ACR-1 in Table 1). This was what we expected given the simplicity of the
property being checked. In contrast, AC-GNNs and GINs (shown in Table 1 as AC-L and GIN-
L, representing AC-GNNs and GINs with L layers) struggle to fit the data. For the case of the
line-shaped graph, they were not able to fit the train data even by allowing 7 layers. For the case
of random graphs, the performance with 7 layers was considerably better. In a closer look at the
performance for different connectivities of E-R graphs, we found an improvement for AC-GNNs
when we train them with more dense graphs (details in the Appendix). This is consistent with
the fact that AC-GNNs are able to move information of local aggregations to distances up to their
7
Published as a conference paper at ICLR 2020
	Line Train	Line Test		E-R Train	E-R Test	
		same-size	bigger		same-size	bigger
AC-5	0.887	0.886	0.892	0.951	0.949	0.929
AC-7	0.892	0.892	0.897	0.967	0.965	0.958
GIN-5	0.861	0.861	0.867	0.830	0.831	0.817
GIN-7	0.863	0.864	0.870	0.818	0.819	0.813
ACR-1	1.000	1.000	1.000	1.000	1.000	1.000
Table 1: Results on synthetic data for nodes labeled by classifier α(x) := Red(x) ∧ ∃y Blue(y)
	α1 Train	α1 Test		α2 Train	α2 Test		α3 Train	α3 Test	
		same-size	bigger		same-size	bigger		same-size	bigger
AC	0.839	0.826	0.671	0.694	0.695	0.667	0.657	0.636	0.632
GIN	0.567	0.566	0.536	0.689	0.693	0.672	0.656	0.643	0.580
AC-FR-2	1.000	1.000	1.000	0.863	0.860	0.694	0.788	0.775	0.770
AC-FR-3	1.000	1.000	0.825	0.840	0.823	0.604	0.787	0.767	0.771
ACR-1	1.000	1.000	1.000	0.827	0.834	0.726	0.760	0.762	0.773
ACR-2	1.000	1.000	1.000	0.895	0.897	0.770	0.800	0.799	0.771
ACR-3	1.000	1.000	1.000	0.903	0.902	0.836	0.817	0.802	0.748
Table 2: Results on E-R synthetic data for nodes labeled by classifiers αi(x) in Equation (6)
number of layers. This combined with the fact that random graphs that are more dense make the
maximum distances between nodes shorter, may explain the boost in performance for AC-GNNs.
Complex FOC2 properties In the second experiment we consider classifiers αi(x) constructed as
αo(x) := BlUe(x),	αi+ι(x)= ∃[N,M]y(αi(y) ∧-E(x,y)),	(6)
where ∃[N,M] stands for “there exist between N and M nodes” satisfying a given property. Observe
that each α%(x) is in FOC2, as ∃[N,M] can be expressed by combining ∃≥N and -∃≥M +1. We
created datasets with E-R dense graphs and labeled them according to α1(x), α2 (x), and α3(x),
ensUring in each case that approximately half of all nodes in oUr dataset satisfy every property. OUr
experiments show that when increasing the depth of the formUla (existential qUantifiers with nega-
tions inside other existential qUantifiers) more layers are needed to increase train and test accUracy
(see Table 2). We report ACR-GNNs performance Up to 3 layers (ACR-L in Table 2) as beyond that
we did not see any significant improvement. We also note that for the bigger test set, AC-GNNs and
GINs are Unable to sUbstantially depart from a trivial baseline of 50%. We tested these networks
with Up to 10 layers bUt only report the best resUlts on the bigger test set. We also test AC-FR-GNNs
with two and three layers (AC-FR-L in Table 2). As we expected, althoUgh theoretically Using a
single readoUt gives the same expressive power as Using several of them (Theorem 5.2), in practice
more than a single readoUt can actUally help the learning process of complex properties.
PPI We also tested AC- and ACR-GNNs on the Protein-Protein Interaction (PPI) benchmark (Zit-
nik & Leskovec, 2017). We chose PPI since it is a node classification benchmark with different
graphs in the train set (as opposed to other popUlar benchmarks for node classification sUch as Core
or Citeseer that have a single graph). AlthoUgh the best resUlts for both classes of GNNs on PPI
were qUite high (AC: 97.5 F1, ACR: 95.4 F1 in the test set), we did not observe an improvement
when Using ACR-GNNs. Chen et al. (2019) recently observed that commonly Used benchmarks are
inadeqUate for testing advanced GNN variants, and ACR-GNNs might be sUffering from this fact.
7 Final remarks
OUr resUlts show the theoretical advantages of mixing local and global information when classifying
nodes in a graph. Recent works have also observed these advantages in practice, e.g., Deng et al.
8
Published as a conference paper at ICLR 2020
(2018) use global-context aware local descriptors to classify objects in 3D point clouds, You et al.
(2019) construct node features by computing shortest-path distances to a set of distant anchor nodes,
and Haonan et al. (2019) introduced the idea of a “star node” that stores global information of the
graph. As mentioned before, our work is close in spirit to that of Xu et al. (2019) and Morris et al.
(2019) establishing the correspondence between the WL test and GNNs. In contrast to our work,
they focus on graph classification and do not consider the relationship with logical classifiers.
Regarding our results on the links between AC-GNNs and graded modal logic (Theorem 4.2), we
point out that very recent work of Sato et al. (2019) establishes close relationships between GNNs
and certain classes of distributed local algorithms. These in turn have been shown to have strong
correspondences with modal logics (Hella et al., 2015). Hence, variants of our Proposition 4.1 could
be obtained by combining these two lines of work (but it is not clear if this combination would yield
AC-GNNs that are simple). However, these works do not investigate the impact of having non-local
computations (such as the readouts that we consider), hence our results on the relationships between
FO an ACR-GNNs (Theorem 5.1 and 5.2) do not follow from these.
Morris et al. (2019) also studied k-GNNs, which are inspired by the k-dimensional WL test. In
k-GNNs, graphs are considered as structures connecting k-tuples of nodes instead of just pairs of
them. We plan to study how our results on logical classifiers relate to k-GNNs, in particular, with
respect to the logic FOCk that extends FOC2 by allowing formulas with k variables, for each fixed
k > 1. Recent work has also explored the extraction of finite state representations from recurrent
neural networks as a way of explaining them (Weiss et al., 2018; Koul et al., 2019; Oliva & Lago-
Fernandez, 2019). We would like to study how our results can be applied for extracting logical
formulas from GNNs as possible explanations for their computations.
Acknowledgments
This work was partly funded by the Millennium Institute for Foundational Research on Data2.
References
Franz Baader and Carsten Lutz. Description logic. In Handbook of modal logic, pp. 757-819.
North-Holland, 2007.
Franz Baader, Diego Calvanese, Deborah L. McGuinness, Daniele Nardi, and Peter F. Patel-
Schneider (eds.). The description logic handbook: theory, implementation, and applications.
Cambridge University Press, 2003.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinlcius Flores
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
CagIar GUICehre, H. Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish
Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan
Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu.
Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261, 2018.
URL http://arxiv.org/abs/1806.01261.
Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identification. Combinatorica, 12(4):389-410, 1992.
Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? A dissection
on graph classification. CoRR, abs/1905.04579, 2019. URL https://arxiv.org/abs/
1905.04579.
Maarten de Rijke. A Note on graded modal logic. Studia Logica, 64(2):271-283, 2000.
Haowen Deng, Tolga Birdal, and Slobodan Ilic. PPFnet: Global context aware local features for
robust 3d point matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, SaltLake City, UT USA, June 18-22, 2018, pp. 195-205, 2018.
2https://imfd.cl/en/
9
Published as a conference paper at ICLR 2020
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with PyTorch Geometric.
CoRR, abs/1903.02428, 2019. URL https://arxiv.org/abs/1903.02428.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August, 2017, pp. 1263-1272,
2017.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems, NIPS 2017, Long Beach, CA, USA, December 4-9, 2017, pp.
1024-1034, 2017.
Lu Haonan, Seth H Huang, Tian Ye, and Guo Xiuyan. Graph star net for generalized multi-task
learning. arXiv preprint arXiv:1906.12330, 2019.
Lauri Hella, Matti Jarvisalo, Antti KUUsisto, Juhana Laurinharju, TUomo Lempiainen, Kerkko LU-
osto, Jukka Suomela, and Jonni Virtema. Weak models of distributed computing, with connec-
tions to modal logic. Distributed Computing, 28(1):31-53, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, 2017.
Anurag Koul, Sam Greydanus, and Alan Fern. Learning finite state representations of recurrent pol-
icy networks. In Proceedings of the 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
Carsten Lutz, Ulrike Sattler, and Frank Wolter. Modal logic and the two-variable fragment. In
Proceedings of the International Workshop on Computer Science Logic, CSL 2001, Paris, France,
September 10-13, 2001, pp. 247-261. Springer, 2001.
Christian Merkwirth and Thomas Lengauer. Automatic generation of complementary descriptors
with molecular graph networks. J. of Chemical Information and Modeling, 45(5):1159-1168,
2005.
Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and Leman go neural: higher-order graph neural networks.
In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, Honolulu,
Hawaii, USA, January 27 - February 1, 2019, pp. 4602-4609, 2019.
Boris Motik, Bernardo Cuenca Grau, Ian Horrocks, Zhe Wu, Achille Fokoue, and Carsten Lutz.
OWL 2 Web ontology language profiles (second edition). W3C recommendation, W3C, 2012.
URL http://www.w3.org/TR/owl2- profiles/.
Christian Oliva and Luis F. Lago-Fernandez. On the interpretation of recurrent neural networks
as finite state machines. In Part I of the Proceedings of the 28th International Conference on
Artificial Neural Networks, ICANN 2019, Munich, Germany, September 17-19, 2019, pp. 312-
323. Springer, 2019.
Martin Otto. Graded modal logic and counting bisimulation. https://www2.mathematik.
tu-darmstadt.de/~otto/Papers/cml19.pdf, 2019.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation Ratios of Graph Neural Net-
works for Combinatorial Problems. arXiv preprint arXiv:1905.10261, 2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Trans. Neural Networks, 20(1):61-80, 2009.
Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convolutional networks. In Proceedings of
The Semantic Web - 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June
3-7, 2018, pp. 593-607, 2018.
10
Published as a conference paper at ICLR 2020
W3C OWL Working Group. OWL 2 Web ontology language document overview (second edition).
W3C recommendation, W3C, 2012. URL https://www.w3.org/TR/owl2-overview/.
Boris Yu. Weisfeiler and Andrei A. Leman. A Reduction of a graph to a canonical form and an
algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12-16, 1968.
Translated from Russian.
Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks
using queries and counterexamples. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, pp.
5244-5253, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graph neural
networks? In Proceedings of the 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, Long Beach, California,
USA, June 9-15, 2019, pp. 7134-7143, 2019.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. CoRR, abs/1707.04638, 2017. URL http://arxiv.org/abs/1707.04638.
11
Published as a conference paper at ICLR 2020
APPENDIX
A Proof of Proposition 3.3
We first recall the proposition.
Proposition 3.3. There is an FOC2 classifier that is not captured by any AC-GNN.
Proof. Consider the following FOC2 node property α(v) := Red(v) ∧ ∃x Green(x). We will show
by contradiction that there is no AC-GNN that captures α, no matter which aggregation, combining,
and final classification functions are allowed. Indeed, assume that A is an AC-GNN capturing α,
and let L be its number of layers. Consider the graph G that is a chain of L + 2 nodes colored Red,
and consider the first node v0 in that chain. Since A captures α, and since (G, v0) 6|= α, we have
that A labels v0 with false, i.e., A(G, v0) = false. Now, consider the graph G0 obtained from G by
coloring the last node in the chain with Green (instead of Red). Then one can easily show that A
again labels v0 by false in G0. But we have (G0, v0) |= α, a contradiction.
The above proof relies on the following weakness of AC-GNNs: if the number of layers is fixed
(i.e., does not depend on the input graph), then the information of the color of a node v cannot travel
further than at distance L from v. Nevertheless, we can show that the same holds even when we
consider AC-GNNs that dispose of an arbitrary number of layers (for instance, one may want to run
a homogeneous AC-GNN for f(|E|) layers for each graph G = (V, E), for a fixed function f).
Assume again by way of contradiction that A is such an extended AC-GNN capturing α. Consider
the graph G consisting of two disconnected nodes v, u, with v colored Red and y colored Green.
Then, since (G, v) |= α, we have A(G, v) = true. Now consider the graph G0 obtained from G by
changing the color of u from Green to Red. Observe that, since the two nodes are not connected, we
will again have A(G0, v) = true, contradicting the fact that (G0, v) 6|= α and that A is supposed to
capture α.
By contrast, it is easy to see that this formula can be done with only one intermediate readout, using
the technique in the proof of Theorem 5.1.	□
B Proof of Proposition 4.1
We first recall the proposition.
Proposition 4.1. Each graded modal logic classifier is captured by a simple homogeneous AC-GNN.
We first define formally the semantics of the graded modal logic (de Rijke, 2000) over simple undi-
rected node-colored graphs (de Rijke, 2000), assuming the FO syntax introduced in the paper.
Definition B.1. We define when a node V in a graph G satisfies a graded modal logic formula 夕(x),
written as V =夕 in G (where “in G ” may be omitted when clear), recursively asfollows:
•	if 夕(X)= Col(X), then V=夕 if and only if Col is the color of V in G,
•	if φ(x)二夕0(x) ∧ 夕00(x), then V=夕 ifand only if V =夕0 and V =夕00, and similarly with
—φ0(x), and
•	if 夕(x) = ∃≥N(E(x,y) ∧ 夕 0(y)), then V 二夕 ifand only ifthe set ofnodes {u | u ∈ NG(V)
and V =夕0} has CardinaIity at least N.
We can now proceed to the proof of the proposition.
Proofof Proposition 4.1. Let 夕(x) be a graded modal logic formula. We will construct an AC-
GNN AW that is further simple and homogeneous. Let sub(夕)=(夕ι,夕2,...,夕L) be an enumer-
ation of the sub-formulas of 夕 such that if 夕k is a subformula of 夕' then k ≤ '. The idea of the
construction of AW is to have feature vectors in RL such that every component of those vectors
represents a different formula in sub(夕).Then AW will update the feature vector Xvi) of node V
ensuring that component' of xv') gets a value 1 if and only if the formula 夕' is satisfied in node v.
12
Published as a conference paper at ICLR 2020
We note that φ = 夕L and thus, the last component of each feature vector after evaluating L layers
in every node gets a value 1 if and only if the node satisfies 夕.We will then be able to use a final
classification function CLS that simply extracts that particular component.
Formally, the simple homogeneous AC-GNN AW has L layers and uses the aggregation and combine
functions
AGG(X) = X x,
x∈X
COM(x, y) = σ(xC + yA + b),
where A, C ∈ RL×L, and b ∈ RL are defined next, and σ is the truncated ReLU activation defined
by σ(x) = min(max(0, x), 1). The entries of the `-th columns of A, C, and b depend on the
sub-formulas of 夕 as follows:
Case 0.	if 夕'(χ)	=	Col(χ) with Col one of the (base) colors, then C⅛	=	1,
Case 1.	if 夕'(x)=夕j(x) ∧ 夕k(x) then g` = Ck' = 1 and b` = —1,
Case 2.	if 夕'(x)=	-夕k(x) then Ck' = -1 and b` = 1,
Case 3.	if 夕'(x)	=	∃≥N(E(x, y) ∧ 夕k(y)) then Ak' = 1 and b` = -N + 1,
and all other values in the `-th columns of A, C, and b are 0.
We now prove that AW indeed captures 夕.Let G = (V, E) be a colored graph. For every node V
in G We consider the initial feature vector XvO) = (xi,...,xl) such that X' = 1 if sub-formula 夕'
is the initial color assigned to v, and x' = 0 otherwise. By definition, AC-GNN AW will iterate the
aggregation and combine functions defined above for L rounds (L layers) to produce feature vectors
(i)
xv for every node v ∈ G and ` = 1, . . . , L as follows:
x(vi) = COM(x(vi-1), AGG( {x(ui-1) | u ∈N(v)}))
= σx(vi-1)C+ X x(ui-1)A + b.	(7)
u∈N (v)
We next prove that for every 夕' ∈ sub(夕)，every i ∈ {',..., L}, and every node V in G it holds that
(Xvi))` = 1 if Vl=夕',and (Xvi))' = 0 otherwise,	(8)
where (x(vi))' is the `-th component ofx(vi)—that is, the `-th component of x(vi) has a 1 if and only if
v satisfies 夕' in G. In the rest of the proof We will be continuously using the value of (Xvi))` whose
general expression is
(X(vi))'=σXL (X(vi-1))kCk'+ X XL (X(ui-1))kAk'+b'.	(9)
k=1	u∈N (v) k=1
We proceed to prove (8) by induction on the number of sub-formulas of every 夕`.If 夕' has one
sub-formula, then 夕'(x) = Col(χ) with Col a base color. We next prove that (XvI))` = 1 if and
only if V has Col as its initial color. Since 夕'(x) = Col(χ) we know that C'' = 1 and Ck' = 0 for
every k 6= ` (see Case 0 above). Moreover, we know that b' = 0 and Ak' = 0 for every k. Then,
from Equation (9) we obtain that
(Xv1))' = σ(X (Xv0) )k Ck' + X XX (XU0))k Ak' + b') = σ((Xv0))').
k=1	{v,u}∈E k=1
Then, given that (X(v0))' = 1 if the initial color of V is Col and (X(v0))' = 0 otherwise, we have
that (Xv1))' = 1 if (G,v) = 夕' and (Xv1))' = 0 otherwise. From this it is easy to prove that for
every i ≥ 1 the vector (Xvi))' satisfies the same property. Now assume that 夕' has more than one
13
Published as a conference paper at ICLR 2020
sub-formula, and assume that for every 夕k with k < ' the property (8) holds. Let i ≥ '. We are left
to consider the following cases, corresponding to the cases for the shape of the formula above.
Case 1.	Assume that 夕'(x) = Wj (x) ∧ 夕k (x). Then g` = Ck' = 1 and b` = -1. Moreover,
We have Cm' = 0 for every m = j, k and An' = 0 for every n (see Case 2 above). Then, from
Equation (9) we obtain that
(x(vi))' = σ(x(vi-1))j + (x(vi-1))k - 1.
Since the number of each proper sub-formula of W' is strictly less than both ` and i, by in-
duction hypothesis we know that (x(vi-1))j = 1 if and only if v |= Wj and (x(vi-1))j = 0
otherwise. Similarly, (x(vi-1))k = 1 if and only if v |= Wk and (x(vi-1))k = 0 otherwise.
Now, since (x(vi) )' = σ((x(vi-1))j + (x(vi-1) )k - 1) we have that (x(vi) )' = 1 if and only if
(x(vi-1))j +(x(vi-1))k - 1 ≥ 1 that can only happen if (x(vi-1))j = (x(vi-1))k = 1. Then (x(vi))' = 1
if and only if v |= Wj and v |= Wk—that is, if and only if v |= W' (since W'(x) = Wj (x) ∧ Wk(x)),
and (x(vi))' = 0 otherwise. This is exactly what we wanted to prove.
Case 2.	Assume that 夕'(x) = -Wk (x). Then Ck' = -1 and b` = 1. Moreover, we have Cm' = 0
for every m 6= k and An' = 0 for every n (see Case 2 above). Then, from Equation (9) we obtain
that
(x(vi))' = σ - (x(vi-1))k + 1 .
By induction hypothesis we know that (x(vi-1))k = 1 if and only if v |= Wk and (x(vi-1))k = 0 oth-
erwise. Since (x(vi))' = σ(-(x(vi-1))k+1) we have that (x(vi))' = 1 if and only if 1 - (x(vi-1))k ≥ 1
that can only happen if (x(vi-1))k = 0. Then (x(vi))' = 1 if and only if v 6|= Wk—that is, if and only
ifv |= Wk, which holds if and only if v |= W', and (x(vi))' = 0 otherwise. This is exactly what we
wanted to prove.
Case 3.	Assume that W'(x) = ∃≥N(E(x, y) ∧ Wk(y)). Then Ak' = 1 and b' = -N + 1. Moreover
for every m we have that Cm' = 0 (see Case 3 above). Then, from Equation (9) we obtain that
(x(vi))' = σ-N+1+ X (x(ui-1))k.
{u,v}∈E
By induction hypothesis we know that (x(ui-1))k = 1 if and only if v |= Wk and (x(ui-1))k = 0
otherwise. Then we can write (x(vi) )' = σ(-N + 1 + m) where
m = |{u | u ∈ N(v) and u |= Wk}|.
Thus, we have that (x(vi) )' = 1 if and only if m ≥ N, that is if and only if there exists at least
N nodes connected with v that satisfy Wk, and (x(vi) )' = 0 otherwise. From that we obtain that
(x(vi))' = 1 if and only ifv |= W' since W'(x) = ∃≥N (E(x, y) ∧ Wk(y)), which is what we wanted
to prove.
To complete the proof we only need to adda final classification after the L iterations of the aggregate
and combine layers that simply classifies a node v as true if the component of x(vL) corresponding
to w holds 1.	□
C Proof of Theorem 4.2
We first recall the theorem.
Theorem 4.2. A logical classifier is captured by AC-GNNs if and only if it can be expressed in
graded modal logic.
Note that one direction follows immediately from Proposition 4.1, so we only need to show the
following proposition.
14
Published as a conference paper at ICLR 2020
Proposition C.1. If a logical classifier α is not equivalent to any graded modal logic formula, then
there is no AC-GNN that captures α.
To prove this proposition, we will need the following definition, which is standard in modal logics
theory.
Definition C.2. Let G be a graph (simple, undirected and node-colored), v be a node in G, and L ∈
N. The unravelling of v in G at depth L, denoted by UnrLG (v), is the (simple undirected node-
colored) graph that is the tree having
一 a node (v,uι,..., Ui) for each path (v,uι,...,ui) in G with i ≤ L,
-an edge between (v,uι,..., Ui-ι) and (v,uι,...,Ui) when {%-ι,Ui} is an edge in G
(assuming that u0 is v), and
- each node (v, u1, . . . , ui) colored the same as ui in G.
We then observe the following.
Observation C.3. Let G and G0 be two graphs, andv and v0 be two nodes in G and G0, respectively.
Then for every L ∈ N, the WL test assigns the same color to v and v0 at round L if and only if there
is an isomorphism between UnrLG (v) and UnrLG0 (v0) sending v to v0.
We will write UnrLG(v) ' UnrLG0 (v0) to denote the existence of the isomorphism as in this observa-
tion. To prove Proposition C.1, we first rephrase Proposition 2.1 in terms of unravellings.
Proposition C.4. Let G and G0 be two graphs with nodes v in G and v0 in G0 such that
UnrLG(v) ' UnrLG0 (v0) for every L ∈ N. Then for any AC-GNN A, we have A(G, u) = A(G0, u0).
Proof. Follows directly from Proposition 2.1 and Observation C.3.	□
The crucial part of the proof of Proposition C.1 is the following non-trivial result, intuitively es-
tablishing that the fragment of unary FO formulas that only depend on the unravelling of a node is
exactly the graded modal logic.
Theorem C.5 (Otto, 2019). Let α be a unary FO formula. If α is not equivalent to a graded
modal logic formula then there exist two graphs G, G0 and two nodes v in G and u0 in G0 such
that UnrLG (v) ' UnrLG0 (v0) for every L ∈ N and such that u |= α in G but u0 6|= α in G0.
Proof. This directly follows from the van Benthem & Rosen characterization obtained in (Otto,
2019, Theorem 2.2) for finite structures (graphs), by noticing that for the notion of graded bisimula-
tion 〜# introduced in this note, We have that G, u 〜# G0, u0 if and only if we have that UnrLG (v) '
UnrLG0 (v0) for every L ∈ N. We point out here that the fact that the edge relation in G is undirected
in our setting (as opposed to E being directed in (Otto, 2019)), and the fact that every node can
only have one color in our setting (as opposed to being able to satisfy multiple “unary predicates”
in (Otto, 2019)) are inessential, and that the proof of (Otto, 2019, Theorem 2.2) carries over to this
setting.	□
We can now gather all of these to prove Proposition C.1.
Proof of Proposition C.1. Let α be a logical classifier (i.e., a unary FO formula) that is not equiva-
lent to any graded modal logic formula. Assume for a contradiction that there exists an AC-GNN Aα
that captures α. Since α is not equivalent to any graded modal logic formula, by Theorem C.5 there
exist two graphs G, G0 and two nodes v in G and u0 in G0 such that UnrLG(v) ' UnrLG0 (v0) for ev-
ery L ∈ N and such that (?) u |= α in Gbutu0 6|= α in G0. Since we have that UnrLG(v) ' UnrLG0 (v0)
for every L ∈ N, by Proposition C.4 we should have that Aα (G, u) = Aα (G0, u0). But this contra-
dicts (?) and the fact that Aa is supposed to capture ɑ.	□
15
Published as a conference paper at ICLR 2020
D	Proof of Theorem 5.1
We first recall the theorem.
Theorem 5.1. Each FOC2 classifier can be captured by a simple homogeneous ACR-GNN.
To prove the theorem, we will use a characterization of the unary FOC2 formulas provided by (Lutz
et al., 2001) that uses a specific modal logic. That logic is defined via what are called modal param-
eters. We adapt the definitions of (Lutz et al., 2001) to deal with simple undirected node-colored
graphs.
Definition D.1. A modal parameter is an expression built from the following grammar:
S ::= id | e | S ∪ S | S ∩ S | S.
Given an undirected colored graph G = (V, E) and a node v of G, the interpretation of S on v is
the set εS (v) ⊆ V defined inductively as follows:
一 if S = id then εs (v) ：= {v};
-	if S = e then εs (V) := {u | {u, v} ∈ E};
-	if S =	Si	∪	S2 then εs (V)	:= εsι (V)	∪ εs? (V);
-	ifS =	S1	∩	S2 then εS(v)	:= εS1 (v)	∩ εS2 (v);
-	ifS = S0 then εS(V) := V \ εS(V).
The modal logic EMLC consists of all the unary formulas that are built with the following grammar:
中：：=CI 中 ∧ φ l-ψ | hSi≥N φ,
where C ranges over node colors, S over modal parameters, and N over N. The semantics of the
first four constructs is defined as expected, and for an undirected colored graph G = (V, E) and
node v ∈ V, we have (G, V) I= hS)'≥N 夕 if and only if there exist at least N nodes U in εs (V) such
that (G, U) I= A
Example D.2. On an undirected graph G = (V,E), the EMLC formula h-e) ≥2 (hei ≥3Green)
holds on a node V ∈ V if V has at least two nonadjacent nodes U (and since our graphs have no
self-loops, V could be U) such that U has at least three green neighbors.
The following theorem is essentially a reformulation of (Lutz et al., 2001, Theorem 1) to our context
(Lutz et al. (2001) show this for FO2 without counting quantifiers and for EMLC without counting,
but an inspection of the proofs reveals that the result extends to counting quantifiers).
Theorem D.3 (Lutz et al., 2001, Theorem 1). For every EMLC formula, there exists an equiv-
alent FOC2 unary formula. Conversely, for every unary FOC2 formula, there exists an equivalent
EMLC formula.
In order to simplify the proof, we will use the following lemma.
Lemma D.4. Let 夕 be an EMLC formula. Then there exists an EMLC formula 夕0 equivalent to 夕
such that each modal parameter appearing in 夕0 is one of the following:
a)	id, thus representing the current node;
b)	e, thus representing the neighbours of the current node;
c)	-e ∩ -id, thus representing the nodes distinct from the current node and that are not neighbours
of the current node;
d)	id ∪ e, thus representing the current node and its neighbors;
e)	-id, thus representing all the nodes distinct from the current node:
f)	-e, thus representing the nodes that are not neighbours of the current node (note that this in-
cludes the current node);
16
Published as a conference paper at ICLR 2020
g)	e ∪ e, thus representing all the nodes;
h)	e ∩ e, thus representing the emptyset.
Proof. Let v be a node in a graph G, and consider the following three disjoint sets of nodes:
1.	the singleton set consisting of v itself,
2.	the set of neighbors of v ,
3.	the set of nodes that are not neighbors of v and that are not v.
These sets can be expressed by modal parameters: the first is obtained by taking S = id; the second
is obtained by taking S = e; and the third is obtained by taking S = e ∩ id. It is straightforward
to verify by induction on S that, for any modal parameter S, if εS (v) contains an element of one of
the three sets, then it must contain all the elements of that set. But then, this implies that a modal
parameter can only represent a (possibly empty) disjoint union of these three sets. Conversely, it is
clear that any disjoint union over these three sets can be represented by a modal parameter. It is then
routine to check that the 8 cases (a)-(h) are obtained as all the 23 possible unions of these three sets
(including the empty union, i.e., the emptyset). For instance, case (f) is the union of sets 1 and 3. □
Proofof Theorem 5.1. The proof is similar to that of Proposition 4.1. Let φ be an EMLC formula
equivalent to the targeted FOC2 unary formula that is of the form given by Lemma D.4, and let
sub(夕)=(夕 ι, ψ2,...,ψL) be an enumeration of the sub-formulas of 夕 such that if 夕 k is a sub-
formula of 夕' then k ≤ '. We will build a simple homogeneous ACR-GNN AW computing feature
vectors x(vi) in RL such that every component of those vectors represents a different formula in
sub(夕).In addition, We will also make use of global feature vectors XG) in RL. The GNN AW will
update the feature vector x(vi) of each node v in a graph ensuring that component ` of x(vi) gets a
(i)
value 1 if and only if the formula 夕' is satisfied in node V (and 0 otherwise). Similarly, xG will
be updated to make sure that every component represents the number of nodes in G that satisfy the
corresponding subformula. The readout and aggregate functions simply sum the input feature vec-
tors. When 夕' is of the form described by Cases 0-3 in the proof of Proposition 4.1, we define the
`-th columns of the matrices A, C and bias b as in that proof, and the `-th column of R (the matrix
that multiplies the global readout feature vector) as the zero vector. We now explain how we define
their '-th columns when 夕' is of the form hS)≥N夕k, according to the 8 cases given by Lemma D.4:
Case a. if 夕' =hid)≥N 夕k, then Ck' = 1 if N = 1 and 0 otherwise;
Case b.	if 夕'	=he>≥N夕k, then Ak' = 1 and b` = -N + 1;
Case c.	if 夕'	=h-e ∩ -id)≥N夕k, then Rk' = 1 and Ck' = Ak'	= -1 and	b` = -N + 1;
Case d.	if 夕'	=(id ∪ e)≥N夕k, then Ck' = 1 and Ak' = 1 and b'	= -N +	1;
Case e.	if 夕'	=(-idi≥N夕k, then Rk' = 1 and Ck' = -1 and b'	= -N +	1;
CaSef if 夕' =(-ei≥NWk, then Rk' = 1 and Ak' = -1 and b' = -N + 1;
Case g. if 夕' =(e ∪ -e)≥N夕k, then Rk' = 1 and b' = -N + 1;
Case h. if 夕' =(e ∩ -e)≥N夕k, then all relevant values are 0;
and all other values in the `-th columns ofA, C, R, and b are 0. The proof then goes along the same
lines as the proof of Proposition 4.1.	□
E Proof of Theorem 5.2
We first recall the theorem.
Theorem 5.2. Each FOC2 classifier is captured by an AC-FR-GNN.
17
Published as a conference paper at ICLR 2020
In the following proof we will use the machinery introduced in Appendices C and D. We will also
make use of a particular AC-GNN with L layers, which we call ApLrimes, that maps every node v
in a graph G to a natural number representing the complete unravelling of v of depth L in G (note
that we do not claim that this AC-GNN can be realized in practice, this construction is mostly for
theoretical purposes). Let primes : N → N be the function such that primes(i) is the i-th prime
number indexed from 0. For instance, we have that primes(0) = 2, primes(1) = 3, etc. Now
consider the function f(∙, ∙) that has as input a pair (c, X) where C ∈ N and X is a multiset of
numbers in N, and produces a number in N as output, defined as follows
k
f(c, {x1, x2, . . . ,xk }) = 2c ×	primes(xi + 1).
i=1
It is not difficult to prove that, as defined above, f(∙, ∙) is an injective function. Thus using the results
by Xu et al. (2019) (see the proof of their Theorem 3) we know that f can be used to implement the
combine and aggregate operators of an AC-GNN such that for every graph G, after L layers, the
color (natural number) assigned to every node in G has a one to one correspondence with the color
assigned to that node in the L-th iteration of the WL test over G. We call this AC-GNN ApLrimes.
Observation E.1. We note that Xu et al. (2019) also constructed an injective function that has
(c, X ) as inputs where c ∈ N and X is a multiset of elements in N (see their Lemma 5 and Corollary
6). Nevertheless we cannot directly use that construction as it assumes the existence of a fixed N
such that the size of all multisets are bounded by N. This would put also a bound of N on the
maximum number of neighbors in the input graphs. Thus we developed a new function (using an
encoding based on prime numbers) to be able to deal with general graphs of unbounded degree.
Proofof Theorem 5.2. Let α be an FOC2 unary formula, and let 夕 be an equivalent EMLC formula
that uses only modal parameters of the form given by Lemma D.4. We construct an ACR-FR-GNN
AW capturing 夕 and hence α.
Let L be the quantifier depth of 夕(i.e., the deepest nesting of hS)≥N quantifiers). For a subfor-
mula 夕0 of 夕，We also define the nesting depth ndφ (夕0) of 夕0 in 夕 to be the number of modal param-
eters under which 夕0 is in 夕.The first L -1 layers of AW are the same as those of AL-m11es, which do
not use readouts. With Observation C.3 at hand and using the fact that the inverses of the aggregation
and combination functions of ApLr-im1 es are computable, this ensures that, after L - 1 layers, for any
graph G and node v in G, we can compute from ApLr-im1 es(G, v) the unravelling UnrLG-1 (v). Thus, we
can assume without loss of generality (by modifying the last combination function for instance), that
after L - 1 layers AW computes UnrLG-1 (v) in every node v of G. We then use a readout whose out-
put is a natural number representing the multiset {UnrLG-1 (v) | v node in G }; for instance, we can
encode this multiset using the same technique that we use for Aprimes . Again, since this technique
uses functions with computable inverses, we can assume without loss of generality that the output of
this readout is actually the multiset {UnrLG-1 (v) | v node in G }. Finally, we use a final combination
function COM(L), that uses only the feature of the current node and the output of the readout—that
is, the final feature ofa node v is COM(L)(UnrLG-1 (v), {UnrLG-1 (u) | u node in G }).
We now explain how we define COM(L). By induction on the structure of 夕,for every subformula 夕0
of 夕,we do the following: for every node V in G and every node U in UnrGT(V) that is at depth (i.e.,
the distance from V) at most ndφ(φ0) in the tree UnrGT (v), we will label U by either 夕0 or by -，.
We do so to ensure that (?) for every node V in G and every node u = (V, u1 , . . . , ui) in UnrLG-1 (V),
we label U by 夕0 if and only if (G, Ui)=夕0. We explain our labeling process by induction on the
structure of 夕，and one can easily check in each case that (?) will hold by induction. Let V bea node
in G and U be a node in UnrGT(V) that is at depth at most ndφ (夕0) in the unravelling.
Case 1.	If 夕0 is a color Col, we label U by 夕0 if U is of that color, and by -夕0 otherwise.
Case 2.	If 夕0 is 夕 1 ∧ 夕2, then observe that we have ndφ(φ0) = ndφ(^ι) = ndw(夕2), so that U is
at depth at most both ndφ(^ι) and ndw(夕2) in the unravelling UnrL-1(v). Thus, we know that we
have already labeled U by either 夕 1 or -夕1, and also by either 夕2 or -夕2. We then label U by 夕0
if U is already labeled by 夕 1 and 夕2, and we label it by -夕0 otherwise.
18
Published as a conference paper at ICLR 2020
Case 3.	The case when 夕0 is a negation is similar.
Case 4.	If 夕0 is hS)≥N夕00, then We only explain the case When the modal parameter S is -e ∧ -id,
as the other cases work similarly. First, observe that for every node v0 in G, we have labeled the
root of UnrGT(V0) by either 夕00 or by -夕00: this is because the root of UnrGT(v0) is always at
depth 0 ≤ ndφ(^00) in UnrGT (v0). Let m be the number of nodes u0 ∈ G such that we have
labeled the root of UnrGT (v0) by 夕00. Next, note that for every children u0 of U in UnrLG-1 (v),
we have that u0 is at depth at most ndφ(^00) in UnrGT (v), so that we have already labeled u0 by
either 夕 00 or -夕 00. Let n be the number of children of U (in UnrGT(V)) that we have labeled by 夕 00.
Then we label U by 夕0 if m 一 n ≥ N, and by -夕0 otherwise.
We then simply define COM(L)(UnrLG-1(V), {UnrLG-1(u) | u node in G }) to be 1 if the root
of UnrGT(V) is labeled with 夕,and 0 otherwise, which concludes the proof.	□
F Details on the experimental setting and results
All our code and data can be accessed online at https://github.com/juanpablos/
GNN-logic
In all our experiments we tested different aggregate, combine and readout functions. For aggregate
and readout we only consider the sum, average, and max functions. For the combine function we
consider the following variants:
•	COM1(x, y, z) = f(xA + yB + zC + b),
•	COM2(x, y, z) = f (MLP1 (x) + MLP2(y) + MLP3(z) + b),
•	COM3(x, y, z) = MLP(x + y + z + b),
•	COM4(x, y, z) = MLP(xA + yB + zC + b).
The above definitions are for ACR-GNNs. For AC-GNNs we consider similar variants but without
the z input. We also used batch normalization in between every GNN and MLP layer. We did not
use any regularization. When processing synthetic data we use a hidden size of 64 and trained with
a batch-size of 128, and the Adam optimizer with PyTorch default parameters for 50 epochs. We
did not do any hyperparameter search besides changing the aggregation, combination, and readout
functions. For the activation functions we always used relu. We observed a consistent pattern in
which sum aggregator and readout produced better results compared with the others. This is in line
with our constructions in Proposition 4.1 and Theorem 5.1. The choice of the combination function
did not produce a significant difference in the performance.
DATA FOR THE EXPERIMENT WITH CLASSIFIER α(x) := RED(x) ∧ ∃y BLUE(y)
For training and testing we constructed three sets of graphs: (a) Train set containing 5k graphs with
nodes between 50 and 100, (b) Test set, same size, containing 500 graphs with the same number
of nodes as in the train set (between 50 and 100 nodes), and (c) Test set, bigger size, containing
500 graphs with nodes between 100 and 200. All graphs contain up to 5 different colors. To force
the models to try to learn the formula, in every set (train and test) we consider 50% of graphs not
containing any blue node, and 50% containing at least one blue node. The number of blue nodes in
every graph is fixed to a small number (typically less than 5 nodes). Moreover, to ensure that there
is a significant number of nodes satisfying the formula, we force graphs to contain at least 1/4 of its
nodes colored with red. The colors of all the other nodes are distributed randomly. With all these
restrictions, every dataset that we created had at least a 18% of nodes satisfying the property. We
consider two classes of graphs: line graphs and Erdos-Renyi graphs.
Line graphs these are connected graphs in which every node in the graph has degree 2 except
for two nodes (the extreme nodes) that have degree 1. To mimic the impossibility proof in Propo-
sition 3.3 we put the blue nodes in one of the “sides” of the line, and the red nodes in the other
“side”. More specifically, consider the line graph with N nodes V1, . . . , VN such that Vi is connected
with Vi+ι. Then, we ensure that every blue node appears in one of vι,..., V N and every red node
appears in one of V N+「...，VN.
19
Published as a conference paper at ICLR 2020
	# Graphs	Avg. # Nodes	Avg. # Edges	Avg. # positive
Line train	5,000	75	74	18
Line test	500	75	74	18
Line test bigger	500	148	147	36
Erdos-Renyi train	5,000	75	115	18
Erdos-Renyi test	500	75	115	18
Erdos-Renyi test bigger	500	148	226	36
Table 3: Synthetic data for the experiment with classifier α(x) := Red(x) ∧ ∃y Blue(y)
	Erdos-Renyi + 20%			Erdos-Renyi + 50%			Erdos-Renyi + 100%		
	Train Acc.	Test Acc.		Train Acc.	Test Acc.		Train Acc.	Test Acc.	
		same-size	bigger		same-size	bigger		same-size	bigger
AC-2	0.810	0.807	0.778	0.829	0.835	0.791	0.861	0.864	0.817
AC-5	0.940	0.937	0.901	0.975	0.971	0.958	0.994	0.994	0.993
AC-7	0.963	0.961	0.946	0.983	0.978	0.981	0.995	0.995	0.995
GIN-2	0.797	0.795	0.771	0.813	0.818	0.784	0.838	0.840	0.803
GIN-5	0.838	0.836	0.819	0.846	0.847	0.833	0.841	0.844	0.838
GIN-7	0.838	0.840	0.803	0.841	0.844	0.838	0.784	0.788	0.773
ACR-1	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000
Table 4: Detailed results for Erdos-Renyi synthetic graphs with different connectivities
Erdos-Renyi graphs These are random graphs in which one specifies the number N of nodes and
the number M of edges. For this experiment we consider as extreme cases the case in which graphs
contain the same number of nodes and edges and graphs in which the number of edges is twice the
number of nodes.
Some statistics of the datasets are shown in Table 3.
Experiments for dense ERDOS-RENYI graphs
We also took a closer look at the performance for different connectivities of random graphs (Table 4).
We define the set “Erdos-Renyi + k%" as a set of graphs in which the number of edges is k%
larger than the number of nodes. For example, “Erdos-Renyi + 100%" contains random graphs in
which the number of egdes doubles the number of nodes. We see a consistent improvement in the
performance of AC-GNNs and GINs when we train and test them with more dense graphs and more
layers (Table 4).
DATA FOR THE EXpERIMENT WITH CLAssIFIER αi (x) IN EQUATION (6)
For this case We only consider dense Erdos-Renyi synthetic graphs. For the train set We consider
graphs with nodes varying from 40 to 50 nodes and edges from 280 to 350 and similarly for the first
test set. For the bigger test set, we consider graphs with nodes from 51 to 60 with edges ranging from
360 and 480. For labeling we consider the following formulas (starting from α0 (x) := Blue(x)):
αι(x) := ∃[8,10]y(αo(y) ∧ -E(x,y)),
α2(x) := ∃[10,20]y(αι(y) ∧ -E(x,y)),
α3(x) := ∃[10,30]y(α2(y) ∧ -E(x,y)).
The choices of the intervals for every classifier were for the pourpose of having approximately half
of the nodes in the random graphs marked as true. statistics of the datasets are shown in Table 5.
20
Published as a conference paper at ICLR 2020
	# Graphs	Avg. # Nodes	Avg. # Edges	Pos. α1	Pos. α2	Pos. α3
Train	5,000	45	315	47%	63%	57%
Test	500	45	315	47%	64%	56%
Test bigger	500	56	420	49%	40%	23%
Table 5: Synthetic data for the experiment with classifier αi (x) in Equation (6)
	F1 Test
AC-2 AC-3 AC-4	97.2 ± 0.3 97.5 ± 0.3 97.5 ± 0.2
ACR-2 ACR-3 ACR-4	93.5 ± 0.3 94.2 ± 1.2 95.4 ± 0.9
Table 6: Performance of AC-GNN and ACR-GNN in the PPI benchmark
PPI Experiments
We consider the standard train/validation/test split for this benchmarck (Fey & Lenssen, 2019). We
use a hidden size of 256 and the Adam optimizer for 500 epochs with early stopping when the
validation set did not improve for 20 epochs. We did not do any hyperparameter search besides
changing the aggregation, combination, and readout functions. As opposed to the synthetic case,
in this case we observed a better performance when the average or the max functions are used for
aggregation. Table 6 shows the best results for different layers (average of 10 runs). As we can see,
ACR-GNNs do not imply an improvement over AC-GNNs for this benchmark.
21