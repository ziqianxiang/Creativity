Published as a conference paper at ICLR 2020
Doubly Robust Bias Reduction in
Infinite Horizon Off-Policy Estimation
Ziyang Tang *
The University of Texas at Austin
ztang@cs.utexas.edu
Yihao Feng *
The University of Texas at Austin
yihao@cs.utexas.edu
Lihong Li	Dengyong Zhou
Google Research	Google Research
lihong@google.com dennyzhou@google.com
Qiang Liu
The University of Texas at Austin
lqiang@cs.utexas.edu
Ab stract
Infinite horizon off-policy policy evaluation is a highly challenging task due to the
excessively large variance of typical importance sampling (IS) estimators. Recently,
Liu et al. (2018a) proposed an approach that substantially reduces the variance of
infinite horizon off-policy evaluation by estimating the stationary density ratio, but
at the cost of introducing biases due to errors in density ratio estimation. In this
paper, we develop a bias-reduced augmentation of their method, which can take
advantage of a learned value function to improve accuracy. Our method is doubly
robust in that the bias vanishes when either the density ratio or value function
estimation is perfect. In general, when either of them is accurate, the bias can also
be reduced. Both theoretical and empirical results show that our method yields
significant advantages over previous methods.
1	Introduction
A key problem in reinforcement learning (RL) (Sutton & Barto, 1998) is off-policy policy evaluation:
given a fixed target policy of interest, estimating the average reward garnered by an agent that follows
the policy, by only using data collected from different behavior policies. This problem is widely
encountered in many real-life applications (e.g., Murphy et al., 2001; Li et al., 2011; Bottou et al.,
2013; Thomas et al., 2017), where online experiments are expensive and high-quality simulators are
difficult to build. It also serves as a key algorithmic component of off-policy policy optimization (e.g.,
Dudlk et al., 2011; Jiang & Li, 2016; Thomas & Brunskill, 2016; LiU et al., 20l9b).
There are two major families of approaches to policy evaluation. The first approach is to build a
simulator that mimics the reward and next-state transitions of the real environment (e.g., Fonteneau
et al., 2013). While straightforward, this approach strongly relies on the model assumptions in
building the simulator, which may invalidate evaluation results. The second approach is to use
importance sampling to correct the sampling bias in off-policy data, so that an (almost) unbiased
estimator can be obtained (Liu, 2001; Strehl et al., 2010; Bottou et al., 2013). A major limitation,
however, is that importance sampling can become inaccurate due to high variance. In particular, most
existing IS-based estimators compute the weight as the product of the importance ratios of many
steps in the trajectory, causing excessively high variance for problems with long or infinite horizon,
yielding a curse of horizon (Liu et al., 2018a).
Recently, Liu et al. (2018a) proposes a new estimator for infinite-horizon off-policy evaluation, which
presents significant advantages to standard importance sampling methods. Their method directly
estimates the density ratio between the state stationary distributions of the target and behavior policies,
instead of the trajectories, thus avoiding exponential blowup of variance in the horizon. While Liu
et al.’s method shows much promise by significantly reducing the variance, in practice, it may suffer
from high bias due to the error or model misspecficiation when estimating the density ratio function.
*The first two authors contributed equally to this work.
1
Published as a conference paper at ICLR 2020
In this paper, we develop a doubly robust estimator for infinite horizon off-policy estimation, by
integrating Liu et al.’s method with information from an additional value function estimation. This
significantly reduces the bias of Liu et al.’s method once either the density ratio, or the value function
estimation is accurate (hence doubly robust). Since Liu et al.’s method already promises low variance,
our additional bias reduction allows us to achieve significantly better accuracy for practical problems.
Technically, our new bias reduction method provides a new angle of double robustness for off-policy
evaluation, orthogonal to the existing literature of doubly robust policy evaluation that solely devotes
to variance reduction (Jiang & Li, 2016; Thomas & Brunskill, 2016; Farajtabar et al., 2018), mostly
based on the idea of control variates (e.g. Asmussen & Glynn, 2007). Our double robustness for bias
reduction is significantly different, and instead yields an intriguing connection with the fundamental
primal-dual relations between the density (ratio) functions and value functions (e.g., Bertsekas, 2000;
Puterman, 2014). This new perspective can inspire more efficient algorithms for policy evaluation,
and lead to unified frameworks for these types of double robustness in future work.
2	Background
Infinite Horizon Off-Policy Estimation Let M =(S, A, r, T, μ0i be a Markov decision process
(MDP) with state space S, action space A, reward function r, transition probability function T,
and initial-state distribution μ0. A policy π maps states to distributions over A, with π(a∣s) being
the probability of selecting a given s. The average discounted reward for π, with a given discount
γ ∈ (0, 1) 1, is defined as
P= Ytrt
P= Yt
Rπ :
lim ET〜∏
T→∞
where τ = {st, at, rt}0≤t≤T is a trajectory with states, actions, and rewards collected by fol-
lowing policy π in the^DP, starting from so 〜μo. Given a set of n trajectories, D =
{s(i),ati),r(i)}ι≤i≤n,o≤t≤τ, collected under a behavior policy ∏o(a|s), the off-policy evaluation
problem aims to estimate the average discounted reward Rn for another target policy ∏(a∣s).
Estimation via Value Function The value function for policy π is defined as the expected accu-
mulated discounted future rewards started from a certain state: Vπ(S) = ET〜∏[P∞=o γtrt∣so = s].
We use rπ (S) = Ea〜∏(.∣s)[r(s, a)] to denote the average reward for state S given policy ∏. Under the
definition, the value function can be seen as a fixed point of the Bellman equation:
Vπ (S)= rπ(s) + Y Pπ V π(s), Pπ V π(s):= Ea 〜π(∙∣s),sθ 〜T (∙∣s,α)[Vπ (s0)],	∀s ∈ S,	⑴
where PπV(S) is the average of the next value function given the current state S and policy π; see
Appendix A.1 for details.
The value function and the expected reward Rπ is related in the following straightforward way
Rn = (1- Y)Es〜μo[Vπ(s)],	⑵
where the expectation is with respect to the distribution μo(s) of the initial states so at time t.
Therefore, given an approximation V of Vπ, and samples Do := {s0i)}ι≤i≤no drawn from μo(s),
we can estimate Rπ by
RRVAL [V]=上也 XX V(SOi)).
no	i=1
Note that this estimator is off-policy in nature, since it requires no samples from the target policy π.
Estimation via State Density Function Denote d∏,t(∙) as average visitation of St in time step t.
The state density function, or the discounted average visitation, is defined as:
d∏(S)= 7lim Pt=0Y dπ,t(S) = (I — Y) X Ytd∏,t(S),
T→∞	t=oYt	t=o
1For average case when γ = 1, the definition of Rπ is the same. However, the definition of value function is
different. We will assume γ < 1 throughout our main paper for simplicity; for average case check appendix B
for more details.
2
Published as a conference paper at ICLR 2020
where (1 - γ) can be viewed as the normalization factor introduced by Pt∞=0 γt.
Similar to Bellman equation for value function, the state density function can also be viewed as a
fixed point to the following recursive equation (Liu et al., 2018a, Lemma 3):
d∏(s0) = (1 - Y)μo(s0) + γTπd∏(s0), where Tnd∏(s0) ：= XT(s0∣s, a)π(a∣s)d∏(s). (3)
s,a
The operator Tπ is an adjoint operator of Pπ used in (1); see Appendix A.1 for a discussion.
If the density function dπ is known, it provides an alternative way for estimating the expected reward
Rπ , by noting that
Rn = Es~d∏ ,a~∏(∙∣s)[r(S,a)].	(4)
We can see that both density function dπ and value function V π can be used to estimate the expected
reward Rπ. We clarify the connection in detail in Appendix A.1.
Off-Policy State Visitation Importance Sampling Equation (4) can not be directly used for off-
policy estimation, since it involves expectation under the behavior policy π . Liu et al. (2018a)
addressed this problem by introducing a change of measures via importance sampling:
R = Es~d∏o ,a~∏o(∙∣s) wπ∕∏o(S) ∏~(a∣ S) r(s, a) , with	wπ∕∏o(S)=行(S) ,	(5)
where w∏∕∏0 (s) is the density ratio function of d∏ and d∏0.
Given an approximation W of w∏∕∏0, and samples D = {S(i), a(i),rt"}1≤i≤n,0≤t≤τ collected from
policy π0 , we can estimate Rπ as:
RnIS同=Z X X YtW(S(i))4⅛⅛%	Z = X X YtW(S(i))^⅛T⅛，(6)
Z i=1 t=0	no(ai|Si)	i=1 t=0	π0 (at |St )
where Z is the normalized constant of the importance weights.
3 Doubly Robust Estimator
Doubly robust estimator is first proposed into reinforcement learning community to solve contextual
bandit problem by Dudlk et al. (2011) as an estimator combining inverse propensity score (IPS)
estimator and direct method (DM) estimator.
Jiang & Li (2016) introduce the idea of doubly robust estimator into off-policy evaluation in rein-
forcement learning. It incorporates an approximate value function as a control variate to reduce the
variance of importance sampling estimator. Inspired by previous works, we propose a new doubly
robust estimator based on our infinite horizon off-policy estimator RSπIS.
3.1 Doubly Robust Estimator for Infinite Horizon MDP
El	1 F	t	.	T⅛TT Γ-Γ>T	1	1	. ■	Λ	t J	.	T⅛TT Γ ^T	t . Λ
The value-based estimator RVπAL[V] and density-ratio-based estimator RSπIS[wb] are expected to be
accurate when Vb and wb are accurate, respectively. Our goal is to combine their advantages, obtaining
a doubly robust estimator that is accurate once either V or wb or is accurate.
To simplify the problem, it is useful to examine the limit of infinite samples, with which RπVAL[V]
and RSπIS[wb] converge to the following limit of expectations:
RSπIS[wb] := n,Tli→m∞ RbSπIS[wb] =	rπ(S)dπ0(S)wb(S),
s
_ -∙^∙-
RVAL [Vb ] :
lim
n0→∞
^- -∙^∙-
RbVAL [Vb]
(I-Y) EV (s)μ0(s).
s
(7)
(8)
3
Published as a conference paper at ICLR 2020
Here and throughout this work, we assume V and wb are fixed pre-defined approximations, and only
consider the randomness from the data D.
AC.、	...	. .	.	"C ~Cl C ”	..	「、	1 ʌ
A first observation is that we expect to have rπ ≈ V - γPπV by Bellman equation (1), when V
approximates the true value V π. Plugging this into RSπIS[wb] in Equation (7), we obtain the following
“bridge estimator”, which incorporates information from both wb and V :
Rπ
Rbridge
s
dπ0 (s)wb(s),
(9)
where operator Pπ is defined in Bellman equation (1). The corresponding empirical estimator is
defined by
Rnridge W = X TIaYWSV(号-Z12Π≡ V(H))，W
Where Z1 = Pn=I PT=01 Ytw(Sf)) and Z2 = Pn=I PT=01 Yt+1wb(Sf))β∏∕∏omf)ls" are Self-
normalized constant of important weights each empirical estimation.
HoWever, directly estimating Rπ using the bridge estimator Rbπridge [V, wb] yields a poor estimation,
because it includes the errors from both wb and V and can be “doubly worse”. HoWever, We can
construct our “doubly robust” estimator by canceling Rbπridge [V, wb] out from RSπIS [wb] and RVπAL [V]:
RDR[V,wb] = Xrπ(S)d∏o(S)Wb(S) + (I-Y) X V(S)μo(S) - X (V(S)-YPπV(S)) d∏o(S)W(S).
And its corresponding empirical estimator can be Written as:
{^^^"∖lf^^^^
Rbπridge[Vb,wb]
(11)
^- -∙^∙ --	^- - -- ^- -∙^∙- ^- -∙^∙ --
RbπDR[Vb, wb] = RbSπIS[wb] + RbπVAL[Vb] - Rbbπridge[Vb, wb].
Doubly Robust Bias Reduction The double robustness of RπDR[V, wb] is reflected in the folloWing
key theorem, Which shoWs that it is accurate once either Vb or wb is accurate.
Theorem 3.1 (Doubly Robustness). Let RDπR[V, wb] := limn0,n,T →∞ RπDR [V, wb] be the limit of RDπR
when it has infinite samples. Following the definition above, we have
RDrR[V, w] - Rn = Es〜d∏0 [εw(S)εv(s)] ,	(12)
where εVb and εwb are errors of V and wb, respective, defined as follows
εw = dTTx — W(s)，	εv(s) = V(s) - rn(S)-YPnV(s).
dn0 (S)	V
The error εwb of wb is measured by the difference with the true density ratio dn (S)/dn0 (S), and the
error εVb of V is measured using the Bellman residual.
From the theorem We can see that, if V is exact (V ≡ Vn ), We have εVb ≡ 0; if wb is exact (wb ≡
dn /dn0), We have εwb ≡ 0. Therefore, our estimator is consistent (i.e., limn,n0→∞ RDnR[V, wb] = Rn)
if either V or wb are exact. The estimator is thus doubly robust in this sense. In contrast, RSnIS [wb] and
RnVAL [V] can be more sensitive to the error of wb and V, respectively:
RSIS[w] - R=	Es〜d∏0	[εw(S)r	(S)] ,	RVAL[V] - R=	Es〜d∏0	[wn∕no(s)εV(S)]	.
Variance Analysis Different from the bias reduction, the doubly robust estimator does not guar-
antee to the reduce the variance over RSnIS[wb] or RVnAL [V] in general. HoWever, as We shoW in the
folloWing result, We can break the variance of RnDR [V, wb] into tWo parts. The first is the variance
of RnVAL [V] Which is often relatively small. The second is generally no greater than the variance of
RSnIS [wb], and can be much smaller When V ≈ Vn. Moreover, RnVAL [V] and RSnIS [wb] avoid the curse
of horizon by design, so their variances tend to be much smaller than the corresponding estimators
that apply IPS correction on the trajectories.
4
Published as a conference paper at ICLR 2020
Algorithm 1 Infinite Horizon Doubly Robust Estimator
Input: Transition data Dπ0 = {s(ti), at(i), rt(i)}1≤i≤n,0≤t≤T from policy π0;
Do = {s0j)}ι≤j≤no be samples from initial distribution μo
target policy π, value function estimate V ; density ratio estimate wb.
Estimation: Use RDπR in (11) to estimate Rπ using sample from D and D0.
ɪʌ _ .	. . _ *J * .	-t ZT T_ .∙ _ .	_ Λ . _ 1	.∙ . ∖ A	T-17Γ Γτ≥ ^1	■	, ■	,	11	1	1	∙T>	1
Proposition 3.1 (Variance Analysis). Assume RDR [V, w] is estimated based on sample Do 〜μo and
D∏o 〜 d∏o, which we assume to be independent Ofeach other We have
「:r>-	-I	「：r>_ _CrI	「:r>_	_ .^l
VarD0,Dπ0 RbπDR[Vb, wb] = VarD0 RbVπAL[Vb] + VarDπ0 Rbrπes[Vb, wb] ,	(13)
with R∏es[V, w] := R∏s H — Rb∏,idge [V ,υj]= E d∏0 [εV (S)W(S)] and εV (S) the TD error of V,
εbVb (S) = rcπ (S) - Vb (S) + γPcπVb (S),
with rπ(S)	=	r(s,a)π(a∣s)∕∏o(a∣s), PπV(S)	=	π(a∣s)∕∏o(a∣s)V(s0).	Therefore,
T T	/ τ-iιτ Γτ≥ ^1 ∖	1	11	1	-ʊ- - I	.	. 1	1 τ τ^rπ ^	/ ∖	_	r>
VarDπ	(Rrπes[V, wb])	can be small when V is close	to the	true value Vπ,	or εbVb (S)	≈	0.
^
From the proposition we can see that when V is close to the true value Vπ , the variance of the
• 1	1 ^	F	1 ∙	∙1 1	11	1 .	.1	♦	C τ⅛ττ Γ ^T *	∕'	. <	♦	C
residual εbVb may be negligibly small compared to the variance of RSπIS[wb]. A further comparison of
,1	1	,	^-7Γ Γ√> ʌl	A"
the variances between Rrπes [V, wb] and RSπIS
error ɛv is negligible, We have VarD0,dπ°
[wb] is provided in Appendix A.3. In the case when the TD
-^	^ 一ι	「公 ^ ι
RbπDR[Vb, wb] ≈ VarD0 RbπVAL [Vb]
. Typically, the variance
C τ⅛ττ 「介 1 ♦	11	.1	τ⅛ττ Γ ^T	.1	♦	C ♦	.	ι∙	JIir
of RπVAL [V] is smaller than RSπIS [wb] since the variance of importance sampling methods heavily
depends on the effective sample size, Which is less controllable compared to RπVAL [V]. Therefore, the
variance of our doubly robust estimator may even be smaller than that of RSπIS [wb] in practice.
The variance in (13) is a sum of two terms because of the assumption that samples from μo and d∏0
are independent. In practice, they have dependency but it is possible to couple the samples from μo
and dπ0 in a certain way to further decrease the variance, which we leave as future work.
Proposed Algorithm for Off-Policy Evaluation Suppose we have already obtained V and wb, as
estimations of Vπ and w∏∕∏0, respectively, we can directly use equation (11) to estimate Rn. A detail
procedure is described in Algorithm 1.
4 Double Robustness and Lagrangian Duality
In this section, we reveal a surprising connection between our double robustness and Lagrangian
duality. We show that our doubly robust estimator is equivalent to the Lagrangian function of a
primal-dual formulation of policy evaluation. This connection is of interest in itself, and may provide
a foundation for deriving more effective algorithms.
We start with the following classic, optimization formulation of policy evaluation (Puterman, 2014):
Rπ
min 1(1 - Y) Xμo(c)V(S)	s.t. V(S) ≥ rπ(s) + YPπV(s),	∀s} ,	(14)
where we find V to maximize its average value, subject to an inequality constraint on the Bellman
equation. It can be shown that the solution of (14) is achieved by the true value function Vπ, hence
yielding a true expected reward Rπ .
Introducing a Lagrangian multiplier ρ ≥ 0, we can derive the Lagrangian function L(V, ρ) of (14),
L(V, ρ) = (I-Y) X μo (S)V (S)- X P(S)(V (s) -rπ (s) + YP πV(S)).	(15)
ss
x -	♦	7- ∕τ r ∖	∙.ι	J . τ⅛ττ rʊ ^T ∙	/1 ι ∖	.ι . .ι	1
Comparing L(V, ρ) with our estimator RDπR[V, wb] in (11), we can see that they are in fact equivalent
in expectation.
5
Published as a conference paper at ICLR 2020
Theorem 4.1 (Primal Dual). I) Define Wρ∕∏o (s)=壮"；S). We have
L(V, ρ)	=	Rdr[V, Wρ∕∏o],	and hence	L(V,	d∏)	=	L(Vπ,ρ)	=	Rn,	∀ V, ρ,
which suggests that L(V, ρ) is “doubly robust” in that it equals Rπ if either V = Vπ or ρ = dπ.
II) The primal problem (14) forms a strong duality with the following dual problem,
Rπ = max
ρ≥0
s)rπ (S)	s.t.	ρ(s') = (1 - γ)μo(s') + YTnρ(s'),	∀s'},	(16)
where Tπ is defined in (3).
This shows that the dual problem is equivalent to constraint ρ using the fixed point equation (3) and
maximize the average reward given distribution ρ. Since the unique fixed point of (3) is dπ (S), the
solution of (16) naturally yields the true reward Rπ, hence forming a zero duality gap with (14).
It is natural to intuit the double robustness of the Lagrangian function. From (15), L(V, ρ) can be
viewed as estimating the reward Rπ using value function with a correction of Bellman residual
(V - rπ - γPπV). If V = Vπ , the estimation equals the true reward and the correction equals
zero. From the dual problem (16), L(V, ρ) can be viewed as estimating Rπ using density function ρ,
corrected by the residual (ρ - (1 - γ)μo - YTπρ). We again get the true reward if P = d∏.
It turns out that we can use the primal-dual formula when γ = 1 to obtain the double robust estimator
for the average reward case. We clarify it in appendix B.
Remark The fact that the density function dπ forms a dual variable of the value function Vπ is
widely known in the optimal control and reinforcement learning literature (e.g., Bertsekas, 2000;
Puterman, 2014; de Farias & Van Roy, 2003), and has been leveraged in various works for policy
optimization. However, it does not seem to be well exploited in the literature of off-policy policy
evaluation.
5	Related Work
Off-Policy Value Evaluation The problem of off-policy value evaluation has been studied exten-
sively in contextual bandits and MDPs (Fonteneau et al., 2013; Li et al., 2015; Jiang & Li, 2016;
Thomas & Brunskill, 2016; Liu et al., 2018b; Farajtabar et al., 2018; Hanna et al., 2019; Xie et al.,
2019; Rowland et al., 2019). However, most of the existing works are based on importance sampling
(IS) to correct the mismatch between the distribution of the whole trajectories induced by the be-
havior and target policies, which faces the “curse of horizon” (Liu et al., 2018a) when extended to
long-horizon (or infinite-horizon) problems.
Several other works (Guo et al., 2017; Hallak & Mannor, 2017; Liu et al., 2018a; Gelada & Bellemare,
2019; Nachum et al., 2019; Liu et al., 2019a) have been proposed to address the high variance issue
in the long-horizon problems. Liu et al. (2018a) apply importance sampling on the average visitation
distribution of state-action pairs, instead of the distribution of the whole trajectories, providing an
effective approach to break “the curse of horizon”. However, they require to learn a density ratio
function over the whole state-action pairs, which may induce large bias. Our work incorporates
the density ratio and value function estimation, which significantly reduces the induced bias of two
estimators, resulting a doubly robust estimator.
Our work is also closely related to DR techniques used in finite horizon problems (Murphy et al.,
2001; Dudk et al., 2011; Jiang & Li, 2016; Thomas & Brunskill, 2016; Farajtabar et al., 2018),
which incorporate an approximate value function as control variates to IS estimators. Different from
existing DR approaches, our work is related to the well known duality between the density and the
value function, which reveals the relationship between density (ratio) learning (Liu et al., 2018a) and
value function learning. Based on this interesting observation, we further obtain the doubly robust
estimator for estimating average reward in infinite-horizon problems.
A recent work (Xie et al., 2019) has also explored the idea of tracking marginal state distribution
shifts at every single time step in an episode, instead of the stationary state distribution across all time
steps (Liu et al., 2018a). It provides additional benefits for understanding and analyzing problems
6
Published as a conference paper at ICLR 2020
—h- On Policy (Monte Carlo)
—Value Estimate
IOT5
10-4.5
10—30
10-2.5
10-3.5
---*
10—3.5
IO-40
500	1000	1500
—≡- Naive Average
—h— Weighted DR
—e— Density Ratio
—0— Our Method
ιo-2
ιo-6
IO-8
IO-4
10-4
於一 X
ιo^6
IO-30
10-3.5
H-
10-4∙°
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
一_g_7 冬一e
ιo2
(b) Bias Square
(c) Variance
*
一P
(d)	MSE with H changes
(e)	Bias square with α changes
(f)	Bias square with β changes

Figure 1: Off Policy Evaluation Results on Taxi. Default parameter, discounted factor γ = 0.99,
mixed ratio α = β = 1, horizon length H = 600. For (a)-(c) the x-axis is the number of trajectories
and y-axis corresponds to MSE, Bias Square and Variance, respectively. For (d) we fix the total
number of samples (number of trajectories times horizon length) and change the horizon length as
x-axis and observe the MSE. (e) and (f) show the change the mixed ratio of α, β with the change of
bias. We repeat each experiment for 1000 runs.
such as short horizon and time-variant MDPs. They call this approach marginalized importance
sampling (MIS). Later on, Kallus & Uehara (2019a) incorporate the DR technique to improve the
MIS estimator. Independent of this work, Kallus & Uehara (2019b); Uehara et al. (2019) extend
previous works to propose estimators can be viewed as a variant of ours.
Primal-Dual Value Learning Primal-dual optimization techniques have been widely used for
off-policy value function learning and policy optimization (Liu et al., 2015; Chen & Wang, 2016; Dai
et al., 2017; 2018; Feng et al., 2019). Nevertheless, the duality between density and value function
has not been well explored in the literature of off policy value estimation. Our work proposes a new
doubly robustness technique for off-policy value estimation, which can be naturally viewed as the
Lagrangian function of the primal-dual formulation of policy evaluation, providing an alternative
unified view for off policy value evaluation.
6	Experiment
In this section, we conduct simulation experiments on different environmental settings to compare
our new doubly robust estimator with existing methods. We mainly compare with infinite horizon
based estimator including state importance sampling estimator (Liu et al. (2018a)) and value function
estimator. We do not report results on the vanilla trajectory-based importance sampling estimators
because of their significant higher variance, but we do compare with the doubly robust version induced
by Thomas & Brunskill (2016) (self-normalized variant of Jiang & Li (2016)). In all experiments we
compare with Monte Carlo and naive average as Liu et al. (2018a) suggested. The ground truth for
each environment is calculated by averaging Monte Carlo estimation with a very large sample size.
7
Published as a conference paper at ICLR 2020
-→<- On Policy (Monte Carlo)
—Value Estimate
—B— Naive Average
→<- Weighted DR
—θ- Density Ratio
-0- Our Method
(a) MSE	(b) Bias Square	(c) Variance	(d) MSE with H changes
Figure 2: Off Policy Evaluation Results on Puck-Mountain. We set discounted factor γ = 0.995 as
default. For (a)-(c) we set the horizon H = 1000 and the x-axis is the number of trajectories for used
for evaluation. For (d) we fix the total number of samples and change the horizon length.
(d) MSE with H changes
(a) MSE	(b) Bias Square	(c) Variance
Figure 3: Off Policy Evaluation Results on InvertedPendulum-v2. We set discounted factor γ = 0.995
as default. For (a)-(c) we set the horizon H = 1000 and the x-axis is the number of trajectories for
used for evaluation. For (d) we fix the total number of samples and change the horizon length.
Taxi Environment We follow Liu et al. (2018a)’s tabular environment Taxi, which has 2000 states
and 6 actions in total. For more experimental details, please check appendix C.1.
We pre-train two different V and V trained with a small and fairly large size of samples, respectively,
where V is very close to true value function V π but V is relatively further from it. Similarly we
pre-train b and P ≈ d∏. For estimation We use a mixed ratio α, β to control the bias of the input V, ρ,
where V = αV +(1 - α)V and P = βρ +(1 - β)ρ.
Figure 1(a)-(c) shoW results of comparison for different methods as We changing the number of
trajectories. We can see that the MSE performance of value function(RπVAL) and state visitation
importance sampling(RSπIS) estimators are mainly impeded by their large biases, while our method
has much less bias thus it can keep decreasing as sample size increase and achieves same performance
as on policy estimator. Figure 1(d) shows results if we change the horizon length. Notice that here
we keep the number of samples to be the same, so if we increase our horizon length we will decrease
the number of trajectories in the same time. We can see that our method alongside with all infinite
horizon methods will get better result as horizon length increase. Figure 1(e)-(f) indicate the “double
robustness” of our method, where our method benefits from either a better V or a better ρ.
Puck-Mountain Puck-Mountain is an environment similar to Mountain-Car, except that the goal
of Puck-Mountain is to push the puck as high as possible in a local valley, which has a continuous
state space of R2 and a discrete action space similar to Mountain-Car. We use the softmax functions
of an optimal Q-function as both target policy and behavior policy, where the temperature of the
behavior policy is higher (encouraging exploration). For more details of constructing policies and
training algorithms for density ratio and value functions, please check appendix C.2.
8
Published as a conference paper at ICLR 2020
Figure 2(a)-(c) show results of comparison for different methods as we changing the number of
trajectories. Similar to taxi, we find our method has much lower bias than density ratio and value
function estimation, which yields a better MSE. In Figure 2(d) the performance for all infinite horizon
estimator will not degenerate as horizon increases, while finite horizon method such as finite weighted
horizon doubly robust will suffer from larger variance as horizon increases.
InvertedPendulum InvertedPendulum is a pendulum that has its center of mass above its pivot
point. We use the implementation of InvertedPendulum from OpenAI gym (Brockman et al., 2016),
which is a continuous control task with state space in R4 and we discrete the action space to be
{-1, -0.3, -0.2, 0, 0.2, 0.3, 1}. More experiment details can be found in appendix C.2.
In Figure 3(a)-(c) our method again significantly reduces the bias, which yields a better MSE
comparing with value and density estimation. Figure 3(d) also shows that our method consistently
outperforms all other methods as the horizon increases with a fixed total timesteps.
7	Conclusion
In this paper, we develop a new doubly robust estimator based on the infinite horizon density ratio
and off policy value estimation. Our new proposed doubly robust estimator can be accurate as long
as one of the estimators are accurate, which yields a significant advantage comparing to previous
estimators. Future directions include deriving more novel optimization algorithms to learn value
function and density(ratio) function by using the primal dual framework.
Acknowledgement
This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to
acknowledge Google Cloud for their support.
References
S0ren AsmUssen and Peter W Glynn. Stochastic SimuIation: algorithms and analysis, volume 57.
Springer Science & Business Media, 2007.
Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition,
2000. ISBN 1886529094.
Leon Bottou, Jonas Peters, JOaqUin Quinonero-Candela, Denis Xavier Charles, D. Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. Journal of Machine Learning
Research,14:3207-3260, 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of rein-
forcement learning. arXiv preprint arXiv:1612.02516, 2016.
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual kernel embeddings. In Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics (AISTATS), pp. 1458-1467, 2017. CoRR abs/1607.04579.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. SBEED:
Convergent reinforcement learning with nonlinear function approximation. In Proceedings of the
Thirty-Fifth International Conference on Machine Learning (ICML), pp. 1133-1142, 2018.
Daniela Pucci de Farias and Benjamin Van Roy. The linear programming approach to approximate
dynamic programming. Operations Research, 51(6):850-865, 2003.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In
Proceedings of the 28th International Conference on Machine Learning (ICML), pp. 1097-1104,
2011.
9
Published as a conference paper at ICLR 2020
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In Proceedings of the 35th International Conference on Machine Learning
(ICML),pp.1446-1455, 2018.
Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the bellman equation. Neural
Information Processing Systems (NeurIPS), 2019.
Raphael Fonteneau, Susan A. Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforcement
learning based on the synthesis of artificial trajectories. Annals of Operations Research, 208(1):
383-416, 2013.
Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the
covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp.
3647-3655, 2019.
Zhaohan Guo, Philip S. Thomas, and Emma Brunskill. Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems 30
(NIPS), pp. 2489-2498, 2017.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In Proceedings of the 34th
International Conference on Machine Learning (ICML), pp. 1372-1383, 2017.
Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an
estimated behavior policy. In Proceedings of the 36th International Conference on Machine
Learning, volume 97, pp. 2605-2613, 2019.
Nan Jiang and Lihong Li. Doubly robust off-policy evaluation for reinforcement learning. In
Proceedings of the 23rd International Conference on Machine Learning (ICML), pp. 652-661,
2016.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evalua-
tion in markov decision processes. arXiv preprint arXiv:1908.08526, 2019a.
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon: Double reinforcement
learning in infinite-horizon processes. arXiv preprint arXiv:1909.05850, 2019b.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-
bandit-based news article recommendation algorithms. In Proceedings of the 4th International
Conference on Web Search and Data Mining (WSDM), pp. 297-306, 2011.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In
Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 608-616, 2015.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In UAI, pp. 504-513. Citeseer, 2015.
Jun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-
Verlag, 2001. ISBN 0387763694.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5361-
5371, 2018a.
Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A Faisal, Finale Doshi-
Velez, and Emma Brunskill. Representation balancing mdps for off-policy policy evaluation. In
Advances in Neural Information Processing Systems, pp. 2644-2653, 2018b.
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy
evaluation via conditional importance sampling. arXiv preprint arXiv:1910.06508, 2019a.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with
state distribution correction. arXiv preprint arXiv:1904.08473, 2019b.
10
Published as a conference paper at ICLR 2020
Susan A. Murphy, Mark van der Laan, and James M. Robins. Marginal mean models for dynamic
regimes. Journal of the American Statistical Association, 96(456):1410-1423, 2001.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. Neural Information Processing Systems (NeurIPS),
2019.
Martin L Puterman. Markov Decision Processes.: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
Mark Rowland, Anna Harutyunyan, Hado van Hasselt, Diana Borsa, Tom SchaUL Remi Munos,
and Will Dabney. Conditional importance sampling for off-policy learning. arXiv preprint
arXiv:1910.07479, 2019.
Alexander L. Strehl, John Langford, Lihong Li, and Sham M. Kakade. Learning from logged
implicit exploration data. In Advances in Neural Information Processing Systems 23 (NIPS-10),
pp. 2217-2225, 2010.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, March 1998. ISBN 0-262-19398-1.
Philip S. Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pp.
2139-2148, 2016.
Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh, Ishan Durugkar, and Emma
Brunskill. Predictive off-policy policy evaluation for nonstationary decision problems, with
applications to digital marketing. In Proceedings of the 31st AAAI Conference on Artificial
Intelligence (AAAI), pp. 4740-4745, 2017.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for
off-policy evaluation, 2019. arXiv:1910.12809.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Optimal off-policy evaluation for reinforcement learn-
ing with marginalized importance sampling. Neural Information Processing Systems (NeurIPS),
2019.
11
Published as a conference paper at ICLR 2020
A Proof
A. 1 Transition Operator for Bellman Equation
For simplicity, we define the following two operators thorough our proofs to simplify our notations.
Definition A.1. Given a policy π and the unknown environment transition T, we define Tπ and Pπ
over any function f : S → R as
(Tnf )(s0) = X T (s0∣s,a)∏(a∣s)f(s)
s,a
(Pπf )(s)= X T(s0∣s,a)∏(a∣s)f(so)
s0,a
Using these operator notations, we can rewrite the above two recursive equations as:
Vπ =rπ + γPπV π,
d∏ = (1 - Y)μo + γTπd∏,
where rπ(s) = Ea〜∏(∙∣s)[r(s,a)].
These transition operators have the following nice adjoint property.
Lemma A.2. For two function f and g, if the following summation is finite, we will have
X(Pπf) (s)g(s) =Xf(s) (Tπg) (s).	(17)
Proof.
E(Pπf)(s)g(s) =E I ET(s0∣s,a)∏(a∣s)f(s0) I g(s)
s	s s0,a
=X f(SO) (XT(SOBa)n(a|S)g(S))
s0	s,a
f(SO) (Tπg) (SO)
s0
□
Using this property we can actually using Bellman Equations to re-derive the two different way to get
Rπ.
[ P= Y trt ]
[ɪF ]
Rn= τlim∞ ET (i)~π
EVπ(S)(I-γ)μo(s)
s
XVπ(S)(I-γTπ)dπ(S)
s
X(I - γPπ) Vπ(S)dπ(S)
s
Xrπ(S)dπ(S).
s
12
Published as a conference paper at ICLR 2020
A.2 Proof of Theorem 3.1
C Y /ɪʌ IlCl .	∖ r 7->TT rʊ ^T	1 ∙	τ⅛ττ 「介 ^T 1 . 1 I- ∙. C τ⅛ττ
Theorem 3.1 (Doubly Robustness). Let RDπR[V , wb] := limn0,n,T→∞ RπDR[V , wb] be the limit of RDπR
when it has infinite samples. Following the definition above, we have
RDR[V,wb] — Rn = Es〜d∏0 [εw(s)εv(s)] ,	(12)
where εVb and εwb are errors of V and wb, respective, defined as follows
εw = ⅛¾- W(S),
εVb (S) = Vb (S) - rπ(S) - γPπVb(S).
The error εw of W is measured by the difference with the true density ratio d∏ (s)∕d∏0 (S), and the
error εVb of V is measured using the Bellman residual.
Proof. Using the property of the operator, We can rewrite (1 - Y)μo(s) using Bellman equation as
dπ - γTπ dπ , thus we have
RVAL[V ] =(I-Y) X V (S)μo(S)
s
=XVb(S) (dπ -YTπdπ) (S)
s
=X(I-YPπ)Vb(S)dπ(S).
s
and similarly if we break rπ as (I - YPπ)V π, for RSπIS [Wb] we have:
RSπIS[Wb] =X(I-YPπ)Vπ(S)dwb(S),
s
where dwb = dπ0 Wb for short.
Compare with Rπ = Ps (I - YPπ) Vπ(S)dπ(S), we can see the main difference between RSπIS and
RπVAL with Rπ are they replace dπ and dwb and V π with V respectively. If we add them together and
minus the connection estimator, we have we will have:
RDπR[Vb,Wb]-Rπ=RSπIS[Wb]+RπVAL[Vb]-Rbπridge[Vb,Wb]-Rπ
=X ((I - YPπ) Vπ(s)dw(s) + (I - YPπ) V(s)d∏(s)
s
-(I-YPπ)Vb(S)dwb(S) - (I - YPπ) Vπ(S)dπ(S)
=X(I-YPπ)(Vπ-Vb)(S)(dwb-dπ)(S)
s
=	dπ0 (S)εVb (S)εwb(S)
s
=Es~d∏o [εV(S)εW(S)],
where the third equation is because (I - YPπ) (Vπ - Vb) = (I - YPπ) Vπ - (I - YPπ) Vb =
rπ - (I - YPπ) V. Similarly, the bias for RπDR and RSπIS can be calculated as:
RSπIS[Wb] -Rπ =X(I-YPπ)Vπ(S)(dwb(S)-dπ(S))
s
= r (S)dπ0 (S)εwb (S)
s
=Es〜d∏0 归w(S)rπ(S)],
and
RVπAL[Vb]-Rπ=X(I-YPπ)(Vb(S)-Vπ(S))dπ(S)
s
=£ εV (S)Wn/no(S)dno (S)
s
=Es~d∏0 [wn∕∏o (S)εV (S)].
13
Published as a conference paper at ICLR 2020
□
A.3 More discussions on the Variance in Proposition 3.1
Recall the proposition 3.1.
Proposition 3.1 (Variance Analysis). Assume RDR [V, w] is estimated based on sample Do 〜μo and
D∏o 〜 d∏o, which we assume to be independent Ofeach other We have
「:r>-	-I	「：r>_ _CrI	「:r>_	_ .^l
VarD0,Dπ0 RbπDR[Vb, wb] = VarD0 RbVπAL[Vb] + VarDπ0 Rbrπes[Vb, wb] ,	(13)
With R黑[V,w]:= R∏is [w] - Rb∏ridge [V ,^b]= E Dπο 卜立(s)w(s)] and εv(s) the TD error of V,
εbVb (s) = rcπ (s) - Vb (s) + γPcπVb (s),
With rπ(S)	=	r(s,a)π(a∣s)∕∏o(a∣s), PπV(S)	=	π(a∣s)∕∏o(a∣s)V(s0).	Therefore,
T T	/ τ-iιτ Γτ≥ ^1 ∖	1	11	1	-ʊ- - I .	. 1	1 τ τ^rπ	^ / ∖	_	r>
VarDπ (Rrπes[V, wb]) can be small when V is close to the true value Vπ,	or εbVb (S)	≈	0.
Proof. Since RπDR [V, wb] = RπVAL [V] + Rrπes [V, wb] and we have the assumption that the samples from
Do and D∏° are independent, the proposition comes immediately.	□
ɪ .	∙ . ∙ t .t	♦	C τ⅛ττ rʊ ^T t	.t	τ⅛ττ Γ ^T -V-VT	.	.	. ∙ .	. ∙	1	t
Intuitively, the variance of Rrπes [V, wb] is less than RSπIS [wb]. We want to quantitatively study every
pieces of orthogonal randomness of Rrπes [V, wb] and RSπIS [wb] in details, and we state the analysis as
the following theorem.
Theorem A.3. Let VarDπ Rrπes [V, wb] be defined in Proposition 3.1, and suppose the normalization
is constant (and hence an ordinary importance sampling which is unbiased). Then we can further
break it into two terms
VarD∏0 hRπ,ss[V,wb]i = n (Var [wb(s)εv(S)]+E hwb(s)2 (SI(S,a)+γδ2Ga, s0))2D,	(18)
where εVb (S) = V(S) - rπ (S) - γPπV(S0) is the Bellman residual(not the empirical one), δ1(S, a) =
∏(als)) r(s, a) — rπ(s) is the randomnessfor action and δ2(s, a, s0) = ∏∏(als)) V(s0) — Pπ V(s) is the
randomness for transition operator over function V. Both δ1 and δ2 is zero mean if we condition over
S.
Compared with Var RbSπIS [wb] we have:
Var R∏s[W]] = ɪ(Var [τb(s)rπ(s)] + E [通(s)2δι(s, a)2])	(19)
>Λ	C T⅛TT rʊ ^T	Λ	♦一
Proof. Rrπes[V, wb] can be written as
ReS[V,W] = 1 X Wb(S) (β∏∕∏0⑷S)(r + YV(SO))- V(S)),
where β∏∕∏0 (a∣S) is short for £((？；)). We can break β∏∕∏0 (a∣S)(r + YV(s0)) - V(s) into
βπ∕π0 (a|S)(r(S, a) + YVb(S0)) - Vb (S)
=(-Vb (S) +rπ(S) +YPπVb(S)) + (βπ∕π0 (a|S)r(S, a) - rπ(S)) +Y (βπ∕π0 (a|S)Vb (S0) - PπVb(S))
`-------------{z-----------}	`-----------------------------}
δ1	δ2
= - εVb (S) + δ1(S, a) + Yδ2(S, a, S0).
14
Published as a conference paper at ICLR 2020
where εVb = V - rπ - PπV is the Bellman residual and the if we condition over s we have the
expectations for δ1 and δ2 are 0. Also notice that ifwe condition over s then εVb (s) become a constant
thus it is independent to δ1 and δ2 . Thus we have:
Var [通(S) (β∏∕∏0 ⑷S)Cr + YV(SO))- V(S))i
=Var [W(s) (-εp(S) + δι(s, a) + γδ2(s, a, s0))]
=Var wb(S)εVb (S) + E hwb(S)2 (δ1(S, a) + γδ2(S, a, S0))2i
Therefore we have:
Var [RDr[V,Q]] = (I n Y) Var[V (so)]+1(Var [须 S)εp (s)]+E [W(s)2 (δι(S,a) + γδ2(S,a,S0 ))2]).
For Var RbSπIS [wb] we have:
Var hRπiS[b]i =1 Var [tb(S)en/no (a|S)r(S,a)]
=LVar [ιb(S)rπ(s) + W(s)6i(s, a)]
n
= L(Var [须S)rπ (s)] + E [W)2(s)6i(s, a)2]).
□
From the theorem we can see that the variance of residual comes from two parts, the majority part
relies on the variance of ∣εp(s)| is usually much smaller than rπ as the majority variance of state
visitation importance sampling. When Vb ≈ Vπ, εVb (S) ≈ 0, the variance of residual only comes
from δ1 (S, a) and δ2 (S, a, S0).
In practice when we get trajectories data from π0, to get samples uniformly from dπ0, we can either
draw sample St depends on its discounted factor Yt or add an importance weight Yt as we did in our
empirical estimator in equation (6) and equation (10).
A.4 Proof of Theorem 4.1
Theorem 4.1 (Primal Dual). I) Define Wρ∕∏o (s)=壮"；S). We have
L(V,ρ) = RDR[V,wρ∕∏0], andhence L(V,d∏) = L(Vπ,ρ) = Rn, ∀ V,ρ,
which suggests that L(V, P) is “doubly robust” in that it equals Rπ if either V = Vπ or P = dπ.
II) The primal problem (14) forms a strong duality with the following dual problem,
Rπ = max
ρ≥0
s)rπ(s)	s.t.	P(SO) = (I-Y)μo(s0) + YTnP(S0),	∀s0} ,	(16)
where Tπ is defined in (3).
Proof. The Lagrangian can be written as:
L(V,P) =(1 - Y) Xμo(s)V(s)-XP(S)(V(S)-rπ(S)-YPπV(S))
s
s
£(1 - γ)μo(S)V(S) -EP(S)(I-YPπ) V(s) + £ρ(S)rπ(S)
s
'--------{—
=RVπAL[V]
Rnidge [V,wρ∕∏0 ]
s
}	<.
s
} '--------7-------'
= RSIS[wp∕∏0 ]
£(1 - γ)μo(S)V(S) - E(I-YTn) P(S)V(S) + EP(S)rπ(S)
s
s
s
E ((1 - γ)μo(S) - (I - γTn)p(s)) V(s) + EP(S)rπ(s).
s
s
15
Published as a conference paper at ICLR 2020
We can see that the Lagrangian L(V, ρ) is actually our doubly robust estimator RDr[V, Wρ∕∏o].
From the last equation we can derive our dual as:
max
ρ≥0
s.t.
ρ(s)rπ (s)
s
P(S) = (I - Y)μo(S) + γTπP(S), ∀s.
□
B Doubly Robust Estimator for Average Case
B.1 Primal Dual Framework
We start from primal dual framework to get our doubly robust estimator similar to section 4. To
estimate the average reward of a given policy π, we can consider solve the following linear program-
ming:	m≥a0x	P(S)rπ (S) ρs s.t. X P(S) = 1, P(S) = Tπ P(S), ∀S,	(20) s
where ρ(S) is the stationary distribution of states under Pπ , and the objective is the average reward
given π.
Consider the Lagrangian of above linear programming:
L(V,P, V) = X P(s)rπ(S)- X V(S)(P(S)- rP(S))- V(X P(S)- 1)
ss	s
=v - Xp(s)(V(s) - rπ(s) - PπV(s) + v).	(21)
s
From Equation (21) we can get the dual formula as:
min V
V,v
s.t. V + V(s) -PπV(s) - rπ(s) ≥ 0, ∀s,	(22)
where V(s) is the value function and V is the average reward We want to optimize.
Notice that in average case, Vπ (S) can be viewed as the fixed-point solution to the following Bellman
equation:
V π (S)- Es0,a∣s 〜d∏ [V π (s')] = EaIs 〜π [r(s, a) - V] ∙
Note that this explains the constraint and only if we pick V = Rπ, we can find a V to guarantee the
constraint V + V(s) - Pπ V(s) - rπ (s) ≥ 0 holds true.
B.2 Doubly Robust Estimator
We want to build the doubly robust estimator via the lagrangian. However, the Lagrangian consist
of three term ρ, V and V. It is counter-intuitive if we already given an estimator of V ≈ Rπ into our
estimator.
A better way to solve this problem is to remove the constraint P P(S) = 1, but we divide it as an
self-normalization. Then our Lagrangian becomes
一Ps P(S)rπ(S) - Ps V(s)(p(s)-Lp(s))
L(V,P) =	PP(S)	.
which can be utilized to define the doubly robust estimator for average reward.
16
Published as a conference paper at ICLR 2020
Definition B.1. Given a learned value function V(S) for policy π and an estimated density ratio
W(S) for w∏∕∏o (S), we define
^- -∙^∙ - -
RbπDR[Vb, wb] :
Ps,a,r,s0∈D 须S) (β∏∕∏o (。冈(厂 + V3)) - V(S))
s∈D wb(S)
where β∏∕∏0 (a|S) = ⅛⅛⅜
Similarly to Theorem 3.1 we will have our double robustness for our average doubly robust estimator:
Theorem B.2. Suppose we have infinite samples and we can get
RπDR[Vb, wb]
Es〜d∏o [W(s) (r∏(s) - V(s)+ PπV(s))]
Es〜d∏o [W(S)]
Then we have
RDR[V,w] - R = Es〜d∏0 [εw (S)εV (S)],
where εVb and εwb are errors ofV and Wb, respective, defined as follows
(23)
Wb(S)	dπ (S)
--------~:~~ - ------1_~ .
Es〜d∏0 [w(s)]	d∏0 (S)
εVb (S) = rπ - Vb+PπVb -Rπ.
Proof. A key observation is that
Es〜d∏0 [w∏∕∏o (S)εv(s)] =Es〜d∏[rπ(s) - V(s) + PπW(s) - Rπ]
=(Es〜d∏ [rπ(s)] - Rn) + Es〜d∏ [-V(S) + PπW(s)]
=0
Thus we have
	_ 		 RDr[V,w]- Rn =Egdn0	EsfW(S)] (Rn + εV (S)I	- Rπ
=Es ~d∏o	_ Es 〜W(SW(s)] εV ⑶	
=Es ~d∏o	Wb(S) Es〜dπ° [W(s)]εV(S)] - Es~dπ0 [wπ∕π0(s)εV(S)I	
=Es~d∏o [εW(S)εV(s)].
□
Similar to discounted case we have RπDR[V, Wb] = Rπ if either Wbor V is accurate.
17
Published as a conference paper at ICLR 2020
C Experimental Details
C.1 Tabular Case: Taxi
Behavior and Target Policies Choosing We use an on-policy Q-learning to get a sequence of
policy π0, π1, ..., π19 as data size increases. We pick the last policy π19 (almost optimum) as our
target policy and π18 as our behavior policy to guarantee that those policies are not far away. We set
our discounted factor γ = 0.99.
Train Vb and ρb Separate from testing, we use a set of independent sample to first train a value
function V and a density function ρb. Both V and ρb have bias due to finite sample approximation.
For training V and ρb, we choose to use Monte Carlo method to estimate V and ρb. We first use the
finite samples to get an estimated model T(s0|s, a) and rewards function rb(s, a) and d0. Then we
solve the following linear equation (by iteration like power method, which is actually Monte Carlo):
V(S) = X π⑷S)Qn Ga),
a
Qbπ(S,a) = rb(S, a) + γ X Tb(S0|S, a)Vb (S0),
s0
νb(s, a) = b(s)π(a∣s),
ρb(S) = (1 - γ)db0(S) + γ X Tb(S0|S, a)νb(S, a).
s,a
Estimate Rπ Using V and ρb We put V and ρb into the Lagrangian as equation (15) as our doubly
robust estimator. For those states we haven’t visited, we set V(S) and ρb(S) as 0 and we self-normalized
the ρbto get a fair estimation.
C.2 Continuous States Off-Policy Evaluation
Evaluation Environments We evaluate our method on two infinite horizon environments: Puck-
Mountain and InvertedPendulum.
Puck-Mountain is an environment similar to Mountain-car, except that the goal of the task is to push
the puck as high as possible in a local valley whose initial position is at the bottom of the valley. If
the ball reaches the top sides of the valley, it will hit a roof and changes the speed to its opposite
direction with half of its original speeds. The rewards was determined by the current velocity and
height of the puck.
InvertedPendulum is a pendulum that has its center of mass above its pivot point. It is unstable and
without additional help will fall over. We train a near optimal policy that can make the pendulum
balance for a long horizon. For both behavior and target policies, we assume they are good enough to
keep the pendulum balance and will never fall down until they reach the maximum timesteps. We use
the implementation from OpenAI Gym (Brockman et al., 2016) and changing the dynamic by adding
some additional zero mean Gaussian noise to the transition dynamic.
Behavior and Target Policies Learning We use the open source implementation2 of deep Q-
learning to train a 32 × 32 MLP parameterized Q-function to converge. We then use the softmax
policy of learned the Q-function as the target policy π, which has a default temperature τ = 1. For
the behavior policy π0, we set a relative large temperature which encourages exploration. We set the
temperature of the behavior policy τ0 = 1.88 for Puck-Mountain and τ0 = 1.50 for InvertedPendulum
respectively.
Training of density ratio W(S) and value function V(S) We use a SePerate training dataset with
200 trajectories whose horizon length is 1000 to learn the density ratio W(S) and the value function
2httPs://github.com/oPenai/baselines
18
Published as a conference paper at ICLR 2020
Algorithm 2 Optimization of density ratio wb
Input: Transition data D = {si , ai , s0i , ri}in=1 from the behavior policy π0 ; a target policy π
for which we want to estimate the expected reward. Discount factor γ ∈ (0, 1), starting state
D0 = {s(j0)}jm=1 from initial distribution.
Initial the density ratio w(s) = wθ(s) to be a neural network parameterized by θ, f(s) = fβ(s)
to be a neural network parameterized by β. We need to ensure that the final layer of θ is a softmax
layer.
for iteration = 1,2,...,T do
Randomly choose a batch M ⊆ {1, . . . , n} uniformly from the transition data D and a batch
M0 ⊆ {1, . . . , m} uniformly from start states D0 .
for iteration = 1,2,..., K do
Update the parameter β by β J β + EeReL(wθ, φβ), where
L(wθ,fβ) = |jM| X ((W(Si)-Yw(Si)∏π⅛7⅛ - 1 f(Si))f (Si))-(I-Y)击 X f(Sj)
i∈M	j∈M0
end for
Update the parameter θ by θ J θ - EθRθL(wθ, fe).
end for
Output: the density ratio wb = wθ .
Algorithm 3 Optimization of value function V
Input: Transition data D = {Si,ai, S0i, ri}in=1 from the behavior policy π0; a target policy π for
which we want to estimate the expected reward. Discount factor Y ∈ (0, 1).
Initial the value function V (S) = Vφ(S) to be a neural network parameterized by φ, f(S) = fe(S)
to be a neural network parameterized by β .
for iteration = 1,2,...,T do
Randomly choose a batch M ⊆ {1, . . . , n} uniformly from the transition data D.
for iteration = 1,2,..., K do
Update the parameter β by β J β + EeReL(Vφ, Φβ), where
L(Vφ,fe) = pM∣ X V (Vφ(si) - ∏Wri + γVφ(si))) fe(Si)- 1 fβ(Si)2
end for
Update the parameter φ by φ J φ - EφRφL(Vφ, fe).
end for
. ... .. √>
Output: the density ratio V = Vφ .
V(s). For the training of density ratio, we adapt the algorithm 2 in LiU et al. (2018a) to train a
neural network parameterized wθ(S). Instead of taking the test function f(S) into an RKHS HK, we
parameterize the test fUnction f(S) = fe(S) to be a neUral network with parameter β, and perform
minimax optimization over parametr θ and β. A detail description can be foUnd in Algorithm 2.
Similarly, for the training of valUe fUnction, we Use primal-dUal based optimization methods (Dai
et al., 2018; Feng et al., 2019) to minimize the bellman residUal:
min maF∣⅛ X ((%(Si)- ∏(⅛⅛(ri+γVφ (Si)))fe (Si)- 2fe (Si)2),
where Vφ(S) is the parameterized valUe fUnction and fe(S) is the test fUnction. We also perform
minimax over parameter φ and β. A detail description can be foUnd in Algorithm 3.
For the network strUctUres, we Use 32×32 feed forward neUral networks to parameterize valUe fUnction
Vφ and density ratio wθ(S), and we Use one hidden neUral network with 10 Units to parameterize the
test fUnction fe(S). We Use Adam Optimizer for all oUr experiments.
19
Published as a conference paper at ICLR 2020
—H— On Policy (Monte Carlo)
—Value Estimate
-B- Naive Average
→- Weighted DR
Figure 4: Additional results on Taxi, with an extra curve for model based method.
—θ- Density Ratio -Model-based
—0— Our Method
Estimate Rπ using V and wb Given data samples from the policy π0 , We can directly use RπDR in
equation (11) to estimate Rπ .
D Additional Experimental Results
We add a standard model-based baseline in Figure 4 following the same procedure as Liu et al.
(2018a).3 Figure 4 shows that the performance of the model-based approach largely depends of the
size of the data, which is consistent with that in Liu et al. (2018a).
3https://github.com/zt95/infinite-horizon-off-policy-estimation.
20