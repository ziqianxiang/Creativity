title,year,conference
 Atheoretical analysis of contrastive unsupervised representation learning,2019, In Proc
 Learning representations by maximizingmutual information across views,2019, arXiv preprint 1906
 Mine: Mutual information neural estimation,2018, In Proc
 Semi-supervised sequence learning,2015, In Proc
 In Proc,2019, of ACL
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, In Proc
 Asymptotic evaluation of certain markov process expectationsfor large time,1983, iv
 Learning deep representations by mutual information estimationand maximization,2019, In Proc
 Universal language model fine-tuning for text classification,2018, InProc
 Span-BERT: Improving pre-training by representing and predicting spans,2019, arXiv preprint 1907
 Adam: a method for stochastic optimization,2015, In Proc
 Cross-lingual language model pretraining,2019, arXiv preprint1901
 Self-organization in a perceptual network,1988, Computer
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint 1907
 An efficient framework for learning sentence representa-tions,2018, In Proc
 Distributed representa-tions of words and phrases and their compositionality,2013, In Proc
 Learning word embeddings efficiently with noise-contrastiveestimation,2013, In Proc
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, In Proc
 Greedy infomax for biologically plausibleself-supervised representation learning,2019, In Proc
 Estimation of entropy and mutual information,2003, Neural computation
 Glove: Global vectors for wordrepresentation,2014, In Proc
 Deep contextualized word representations,2018, In Proc
 Improving language under-standing by generative pre-training,2018, Technical report
 Languagemodels are unsupervised multitask learners,2019, Technical report
 MASS: Masked sequence to sequencepre-training for language generation,2019, In Proc
 On mutual informationmaximization for representation learning,2019, arXiv preprint 1907
 Representation learning with contrastive predictivecoding,2019, arXiv preprint 1807
 XLNet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprint1906
 Learningand evaluating general linguistic intelligence,2019, arXiv preprint 1901
