title,year,conference
 Unitary evolution recurrent neural networks,2016, InInternational Conference on Machine Learning
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Almost linear VC dimension bounds for piecewisepolynomial networks,1999, In Advances in Neural Information Processing Systems
 Learning phrase representations using RNN encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Unifying orthogonalMonte Carlo methods,2019, In International Conference on Machine Learning
 Wav2Letter: an end-to-end ConvNet-based speech recognition system,2016, arXiv preprint arXiv:1609
 Learning fast algorithmsfor linear transforms using butterfly factorizations,2019, In The International Conference on MachineLearning (ICML)
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Rigging the lottery: Makingall tickets winners,2019, arXiv preprint arXiv:1911
 Benchmarking sparsematrix-vector multiply in five minutes,2007, In SPEC Benchmark Workshop
 TIMIT acoustic-phonetic continuous speech corpus LDC93S1,1993, WebDownload
 Acoustic modellingfrom the signal domain using CNNs,2016, In Interspeech
 Recent advances inconvolutional neural networks,2018, Pattern Recognition
 Nearly-tight VC-dimension bounds forpiecewise linear neural networks,2017, In Satyen Kale and Ohad Shamir (eds
 Deep residUal learning for imagerecognition,2016, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Unitary triangUlarization of a nonsymmetric matrix,0004, J
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in Neural Information Processing Systems
 Fastfood-computing hilbert space expansions inloglinear time,2013, In International Conference on Machine Learning
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Multidimensional butterfly factorization,2018, Applied andComputational Harmonic Analysis
 Efficient cepstral normal-ization for robust speech recognition,1993, In ARPA Workshop on Human Language Technology
 Autoshufflenet: Learning permutationmatrices via an exact lipschitz continuous penalty in deep convolutional neural networks,2019, arXivpreprint arXiv:1901
 A fast cosine transform in one and two dimensions,0096, IEEE Transactions on Acoustics
 Fast approximation of rotations and Hessians matrices,2014, arXivpreprint arXiv:1404
 Learning latent permutationswith Gumbel-Sinkhorn networks,2018, In International Conference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature Communications
 ACDC: a structuredefficient linear layer,2016, In International Conference on Learning Representations
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In The International Conference on Machine Learning(ICML)
 Estimating phoneme class conditionalprobabilities from raw speech signal using convolutional neural networks,2013, In Interspeech
 On the use of filter-bank energies as features for robust speech recognition,1999, InInternational Symposium on Signal Processing and its Applications (ISSPA)
 The kaldi speech recognition toolkit,2011, In IEEE 2011 Workshop on Automatic SpeechRecognition and Understanding
 Speaker recognition from raw waveform with sincnet,2018, In IEEEWorkshop on Spoken Language Technology
 Light gated recur-rent units for speech recognition,2018, In IEEE Transactions on Emerging Topics in ComputationalIntelligence
 The PyTorch-Kaldi speech recognitiontoolkit,2019, In IEEE International Conference on Acoustics
 Nonlinear total variation based noise removalalgorithms,1992, Physica D: nonlinear phenomena
 Low-rank matrix factorization for deep neural network training with high-dimensional output targets,2013, InProceedings of the IEEE International Conference on Acoustics
 Learning thespeech front-end with raw waveform CLDNNs,2015, In Interspeech
 Green AI,2019, arXiv preprintarXiv:1907
 Structured transforms for small-footprint deeplearning,2015, In Advances in Neural Information Processing Systems
 Learning compressedtransforms with low displacement rank,2018, In Advances in Neural Information Processing Systems(NeurIPS)
 Learning longer-term depen-dencies in RNNs with auxiliary losses,2018, arXiv preprint arXiv:1803
 Recurrence relations and fast algorithms,2010, Applied and Computational HarmonicAnalysis
 WaveNet: A generative model forraw audio,2016, arXiv preprint arXiv:1609
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Compact nonlinear maps andcirculant extensions,2015, CoRR
 Orthogonal random features,2016, In D
 On compressing deep models by loWrank and sparse decomposition,2017, In IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Learning filterbanks from raW speech for phone recognition,2018, In IEEE InternationalConference on Acoustics
 Shufflenet: An extremely efficientconvolutional neural netWork for mobile devices,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Building efficientdeep neural netWorks With unitary group convolutions,2019, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Le et al,2018, (2015) propose the Permuted MNIST task
2	Model and evaluationOur baseline Bi-LSTM architecture is taken from the PyTorch-Kaldi repository,1993,6 This is a strongbaseline model that
