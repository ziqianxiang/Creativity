title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, In International Conference on Machine Learning
 Infinite attention: Nngp and ntk for deep attention networks,2020, In International Conferenceon Machine Learning (ICML)
 Onexact computation with an infinitely wide neural net,2019, In Advances In Neural Information ProcessingSystems
 A mean field theory of quantized deep networks:The quantization-depth trade-off,2019, arXiv preprint arXiv:1906
 Kernel methods for deep learning,2009, In Advances In NeuralInformation Processing Systems
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Deep convolutionalnetworks as shallow gaussian processes,2019, In International Conference on Learning Representations
 On the selection of initialization and activationfunction for deep neural networks,2018, arXiv preprint arXiv:1805
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
 Universal statistics of fisher information indeep neural networks: mean field approach,2018, 2018
 Learning multiple layers of features from tiny images,2009, Technical report
 Deep neural networks as gaussian processes,2018, In International Conference onLearning Representations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Bayesian deep convolutional networks with manychannels are gaussian processes,2019, In International Conference on Learning Representations
 Exponentialexpressivity in deep neural networks through transient chaos,2016, In Advances In Neural InformationProcessing Systems
 Critical initialisation for deep sig-nal propagation in noisy rectifier neural networks,2018, In S
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Disentangling trainability and general-ization in deep learning,2019, arXiv preprint arXiv:1912
 Mean field residual networks: On the edge of chaos,2017, In AdvancesIn Neural Information Processing Systems
 Wide residual networks,2016, In Proceedings of the BritishMachine Vision Conference (BMVC)
 Gradient descent optimizes over-parameterized deep relu networks,1573, Machine Learning
