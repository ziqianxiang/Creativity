title,year,conference
 A Tight Convergence Analysis for StochasticGradient Descent with Delayed Updates,2018, jun 2018
 Stochastic Gradient Pushfor Distributed Deep Learning,2019, 2019
 Revisiting distributedsynchronous sgd,2016, arXiv preprint arXiv:1604
 Large scale distributed deep networks,2012, Advances in NeuralInformation Processing Systems
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Slow andStale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD,2018, mar 2018
 Taming Momentum in a DistributedAsynchronous Environment,2019, 2019
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Learning multiple layers of features from tiny images,2009, 2009
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2017, arXiv preprint arXiv:1710
 Step Size Matters in Deep Learning,2018, In NIPS
 Discrete-time signal processing,1999, Pearson Education India
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Measuring the effects of data parallelism on neural network training,2018, arXiv preprintarXiv:1811
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 A unified analysis of stochastic momentummethods for deep learning,2018, arXiv preprint arXiv:1808
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Staleness-aware async-sgd for distributed deeplearning,2015, arXiv preprint arXiv:1511
