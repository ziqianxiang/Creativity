Figure 1: Performing convolution on this real world image using a correlative filter, such as a Gaussiankernel, adds correlations to the resulting image, which makes object recognition more difficult. Theprocess of removing this blur is called deconvolution. What if, however, what we saw as the realworld image was itself the result of some unknown correlative filter, which has made recognitionmore difficult? Our proposed network deconvolution operation can decorrelate underlying imagefeatures which allows neural networks to perform better.
Figure 2: Visualizing the 15 × 15 deconvolution kernels from 1024 random images from the ImageNetdataset. The kernels in the R,G,B channels consistently show center-surround structures.
Figure 3: (Left) Given a single channel image, and a 3 × 3 kernel, the kernel is first flattened into a 9dimensional vector w . The 9 image patches, corresponding to the image regions each kernel entrysees when overlaying the kernel over the image and then shifting the kernel one pixel each step, areflattened into a tall matrix X . It is important to note that because the patches are shifted by just onepixel, the columns of X are highly correlated. The output y is calculated with matrix multiplicationXw, which is then reshaped back into a 2D image. (Top Right) In a convolution layer the matrix Xand Cov is calculated from Algorithm 1. (Bottom Right) The pixel-wise and channel-wise correlationis removed by multiplying this X matrix with with Cov-2, before the weight training.
Figure 4: (a) The input image. (b) The absolute value of the zero-meaned input image. (c) Thedeconvolved input image (min-max normalized, gray areas stands for 0). (d) Taking the absolutevalue of the deconvolved image.
Figure 5: (a-b): Regression losses on Fashion-MNIST dataset showing the effectiveness of deconvo-lution versus batch normalization on a non-convolutional type layer. (a) One layer, linear regressionmodel with L2 loss. (b) One layer, linear regression model with logistic regression loss. (c-d): Resultsof a 3-hidden-layer Multi Layer Perceptron (MLP) network on the MNIST dataset.
Figure 6: The mean IoU, pixel-wise accuracy and training loss on the Cityscapes dataset usingDeepLabV3 with a ResNet-50 backbone. In our setting, we modified all the convolution layers toremove batch normalizations and insert deconvolutions.
Figure 7: Comparison of coupled/uncoupled Newton-Schulz iterations on a 27 × 27 covariance matrixconstructed from the Lenna Image.
Figure 8: (a) The effects of weight decay on stochastic gradient descent (SGD) and batch normal-ization (BN) versus SGD and deconvolution (Deconv), training on the CIFAR-100 dataset on theVGG-13 network. Here we notice that increased weight decay leads to worse results for standardtraining. However, in our case with deconvolution, the final accuracy actually improves with increasedweight decay (.0005 to .005). Each experiment is repeated 5 times. We show the confidence interval(+/- 1 std). (b)The training loss of the VGG-11 network on the ImageNet dataset. Only the first1000 iterations are shown. Comparison is made among SGD, SGD with batch normalization anddeconvolution.
Figure 9: An on-center cell and off-center cell found in animal vision system. The on-center cell(left) responds maximally when a stimuli is given at the center and a lack of stimuli is given in acircle surrounding it. The off-center cell (right) responds in the opposite way.
Figure 10: (a) Histograms of input signals (pixel values) before/after deconvolution. (b) Log densityof the input signals before/after deconvolution.) The x-axis represents the normalized pixel value.
Figure 11: (a)Input features to the 5-th convolution layer in VGG-11. (b) Taking the absolute value ofthe deconvolved features. The features have been min-max normalized for visualization.(Best viewon a display.)The CPU timing on a batch size of 128 can be found in Table 3. Here we fix the Newton-Schulziteration times to be 5.
Figure 12: The accuracy vs walltime curves of VGG-11 networksbatch normalization and network deconvolution.
