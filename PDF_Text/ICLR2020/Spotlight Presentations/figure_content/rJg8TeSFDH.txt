Figure 1: Taking PreReSNet32 with standard hyperparameters and replacing WD during first phase (Fixed LR)by exponential LR according to Theorem 2.9 to the schedule ηt = 0.1 X 1.481t, momentum 0.9. Plot onright shows weight norm W of the first convolutional layer in the second residual block grows exponentially,satisfying HwtH = constant. Reason being that according to the proof it is essentially the norm square ofthe weights when trained with Fixed LR + WD + Momentum, and published hyperparameters kept this normroughly constant during training.
Figure 2: PreReSNet32 trained with standard Step Decay and its corresponding TaPered-EXPonential LR sched-ule. AS predicted by Theorem 2.12, they have similar trajectories and performances.
Figure 3: Instant LR decay is crucial when LR growth ηt/ηt-ι -1 is very small. The original LR of Step Decayis decayed by 10 at epoch 80,120 respectively. In the third phase, LR growth ηt/ηt-ι - 1 is approximately 100times smaller than that in the third phase, it would take TEXP-- hundreds of epochs to reach its equilibrium. Asa result, TEXP achieves better test accuracy than TEXP--. As a comparison, in the second phase, ηt/ηt-ι — 1is only 10 times smaller than that in the first phase and it only takes 70 epochs to return to equilibrium.
Figure 4: Instant LR decay has only temporary effect when LR growth ηt/ηt-ι — 1 is large. The blue line usesan exponential LR schedule with constant exponent. The orange line multiplies its LR by the same constanteach iteration, but also divide LR by 10 at the start of epoch 80 and 120. The instant LR decay only allows theparameter to stay at good local minimum for 1 epoch and then diverges, behaving similarly to the trajectorieswithout no instant LR decay.
Figure 5: Both Cosine and Step Decay schedule behaves almost the same as their exponential counterpart, aspredicted by our equivalence theorem. The (exponential) Cosine LR schedule achieves better test accuracy,with a entirely different trajectory.
Figure 6: The orange line corresponds to PreReSNet32 trained with constant LR and WD divided by 10 atepoch 80 and 120. The blue line is TEXP-- corresponding to Step Decay schedule which divides LR by 10 atepoch 80 and 120. They have similar trajectories and performances by a similar argument to Theorem 2.12.(SeeTheorem B.2 and its proof in Appendix B)which covers the result without momentum in (Arora et al., 2019) as a special case:tRt+ι = Ro + 工 Di.
Figure 7:	Degree of homogeneity of the output of basic modules given degree of homogeneity of the input.
Figure 8:	Degree of homogeneity for all modules in vanilla CNNs/FC networks.
Figure 9:	An example of the full network structure of ResNet/PreResNet represented by composite modules de-fined in Figure 10,11,13,14, where 'S' denotes the starting part of the network, 'Block' denotes a normal blockwith residual link, 'D-Block' denotes the block with downsampling, and 'N' denotes the normalization layerdefined previously. Integer X ∈ {0,1, 2} depends on the type of network. See details in Figure 10,11,13,14.
Figure 10: Degree of homogeneity for all modules in ResNet without affine transformation in normalizationlayer. The last normalization layer is omitted.
Figure 11: Degree of homogeneity for all modules in ResNet without affine transformation in normalizationlayer. The last normalization layer is omitted.
Figure 12: Degree of homogeneity for all modules in vanilla CNNs/FC networks.
Figure 13: Degree of homogeneity for all modules in ResNet with trainable affine transformation. The lastnormalization layer is omitted.
Figure 14: Degree of homogeneity for all modules in PreResNet with trainable affine transformation. The lastnormalization layer is omitted.
Figure 15: The network can be not scale variant if the GN or IN is used and the bias of linear layer is trainable.
