Figure 1: Emergent Skill Progression From Multi-Agent Autocurricula. Through the reward signalof hide-and-seek (shown on the y-axis), agents go through 6 distinct stages of emergence. (a) Seekers(red) learn to chase hiders, and hiders learn to crudely run away. (b) Hiders (blue) learn basic tooluse, using boxes and sometimes existing walls to construct forts. (c) Seekers learn to use ramps tojump into the hiders’ shelter. (d) Hiders quickly learn to move ramps to the edge of the play area, farfrom where they will build their fort, and lock them in place. (e) Seekers learn that they can jumpfrom locked ramps to unlocked boxes and then surf the box to the hiders’ shelter, which is possiblebecause the environment allows agents to move together with the box regardless of whether they areon the ground or not. (f) Hiders learn to lock all the unused boxes before constructing their fort. Weplot the mean over 3 independent training runs with each individual seed shown with a dotted line.
Figure 2: Agent Policy Architecture. All entities are embedded with fully connected layers withshared weights across entity types, e.g. all box entities are encoded with the same function. Thepolicy is ego-centric so there is only one embedding of “self” and (#agents - 1) embeddings ofother agents. Embeddings are then concatenated and processed with masked residual self-attentionand pooled into a fixed sized vector (all of which admits a variable number of entities). x and vstand for state (position and orientation) and velocity.
Figure 3: Environment specific statistics used to track stages of emergence in hide-and-seek. Weplot the mean across 3 seeds with each individual seed shown in a dotted line, and we overlay the 6emergent phases of strategy: (1) Running and Chasing, (2) Fort Building, (3) Ramp Use, (4) RampDefense, (5) Box Surfing, (6) Surf Defense. We track the maximum movement of any box or rampduring the game as well as during the preparation phase (denoted with “Prep”). We similarly trackhow many objects of each type were locked at the end of the episode and preparation phase. Asagents train, their interaction with the tools in their environment changes. For instance, as the agentslearn to build forts they move boxes and lock boxes much more during the preparation phase.
Figure 4: Effect of Scale on Emergent Autocur-ricula. Number of episodes (blue) and wall clocktime (orange) required to achieve stage 4 (rampdefense) of the emergent skill progression pre-sented in Figure 1. Batch size denotes number ofchunks, each of which consists of 10 contiguoustransitions (the truncation length for backpropaga-tion through time).
Figure 5: Behavioral Statistics from Count-Based Exploration Variants and Random Network Dis-tillation (RND) Across 3 Seeds. We compare net box movement and maximum agent movementbetween state representations for count-based exploration: Single agent, 2-D box location (blue);Single agent, box location, rotation and velocity (green); 1-3 agents, full observation space (red).
Figure 6: Fine-tuning Results. We plot the mean normalized performance and 90% confidence inter-val across 3 seeds smoothed with an exponential moving average, except for Blueprint Constructionwhere we plot over 6 seeds due to higher training variance.
