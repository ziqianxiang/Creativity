Figure 1: A schematic of MetaGenRL. On the left a population of agents (i ∈ 1,..., N), whereeach member consist of a critic Q；" and a policy ∏(i) that interact with a particular environmente(i) and store collected data in a corresponding replay buffer B⑺. On the right a meta-learnedneural objective function La that is shared across the population. Learning (dotted arrows) proceedsas follows: Each policy is updated by differentiating La, while the critic is updated using the usualTD-error (not shown). La is meta-learned by computing second-order gradients that can be obtainedby differentiating through the critic.
Figure 2: An overview of Lam x(Φ), V).
Figure 3: Comparing the test-time training behavior of the meta-learned objective functions byMetaGenRL to other (meta) reinforcement learning algorithms. We train randomly initialized agentson (a) environments that were encountered during training, and (b) on significantly different environ-ments that were unseen. Training environments are denoted by f in the legend. All runs are shownwith mean and standard deviation computed over multiple random seeds (MetaGenRL: 6 meta-train× 2 meta-test seeds, RL2 : 6 meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds,and 6 seeds for all others).
Figure 4: Meta-training with 20 agents on Cheetah and Lunar. We test the objective function at fivestages of meta-training by using it to train three randomly initialized agents on Hopper.
Figure 5: We meta-train MetaGenRL using several alternative parametrizations of Lα on a) Lunarand Cheetah, and b) present results of testing on Cheetah. During meta-training a representative ex-ample of a single agent population is shown with shaded regions denoting standard deviation acrossthe population. Meta-test results are reported as per usual across 6 meta-train × 2 meta-test seeds.
Figure 6: We meta-train MetaGenRL on the LunarLander and HalfCheetah environments usingone, three, and five inner gradient steps on φ. Meta-test results are reported across 3 meta-train × 2meta-test seeds.
Figure 7: Comparing the test-time training behavior of the meta-learned objective functions byMetaGenRL to other (meta) reinforcement learning algorithms on Hopper. We consider withindistribution testing (a), and out of distribution testing (b) by varying the meta-training environments(denoted by f) for the meta-RL approaches. All runs are shown with mean and standard deviationcomputed over multiple random seeds (MetaGenRL: 6 meta-train × 2 meta-test seeds, RL2 : 6 meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds, and 6 seeds for all others).
Figure 8: Comparing the test-time training behavior of the meta-learned objective functions byMetaGenRL to other (meta) reinforcement learning algorithms on Cheetah. We consider withindistribution testing (a), and out of distribution testing (b) by varying the meta-training environments(denoted by f) for the meta-RL approaches. All runs are shown with mean and standard deviationcomputed over multiple random seeds (MetaGenRL: 6 meta-train × 2 meta-test seeds, RL2 : 6 meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds, and 6 seeds for all others).
Figure 9:	Comparing the test-time training behavior of the meta-learned objective functions byMetaGenRL to other (meta) reinforcement learning algorithms on Lunar. We consider within dis-tribution testing (a), and out of distribution testing (b) by varying the meta-training environments(denoted by f) for the meta-RL approaches. All runs are shown with mean and standard devia-tion computed over multiple random seeds (MetaGenRL: 6 meta-train × 2 meta-test seeds, RL2 : 6meta-train × 2 meta-test seeds, EPG: 3 meta-train × 2 meta-test seeds, and 6 seeds for all others).
Figure 10:	Meta-training with 20 agents on LunarLander. We meta-test the objective function atdifferent stages in training on the same environment.
Figure 11: The left plot shows all 12 random seeds on the meta-test environment Lunar while theright has the 4 worst random seeds removed. The variance is now reduced significantly.
Figure 12: Stable meta-training requires a largepopulation size of at least 20 agents. Meta-training performance is shown for a single runwith the mean and standard deviation across theagent population.
Figure 13: Meta-training on Cheetah, Lunar,Walker, and Ant with 20 or 40 agents; meta-testing on the out-of-distribution Hopper envi-ronment. We compare to previous MetaGenRLconfigurations.
