Figure 1: The Hamiltonian manifold hypothesis:natural images lie on a low-dimensional manifoldin pixel space, and natural image sequences (suchas one produced by watching a two-body system,as shown in red) correspond to movement on themanifold according to Hamiltonian dynamics.
Figure 2: Hamiltonian Generative Network schematic. The encoder takes a stacked sequence of imagesand infers the posterior over the initial state. The state is rolled out using the learnt Hamiltonian. Notethat we depict Euler updates of the state for schematic simplicity, while in practice this is done using aleapfrog integrator. For each unroll step we reconstruct the image from the position q state variablesonly and calculate the reconstruction error.
Figure 3: A: standard normalising flow, where the invertible function fi is implemented by a neuralnetwork. B: Hamiltonian flows, where the initial density is transformed using the learned Hamiltoniandynamics. Note that we depict Euler updates of the state for schematic simplicity, while in practice thisis done using a leapfrog integrator.
Figure 4: A schematic representation of NHF which can perform expressive density modelling byusing the learned Hamiltonians as normalising flows. Note that we depict Euler updates of the state forschematic simplicity, while in practice this is done using a leapfrog integrator.
Figure 5: Ground truth HamiltonianS and samples from generated datasets for the ideal pendulum,mass-spring, and two- and three-body systems used to train HGN.
Figure 6: Average pixel MSE for each step of a single train and test unroll on four physical systems.
Figure 7: Example of a train and a test sequence from the dataset of a three-body system, its inferredforward, backward, double speed and half speed rollouts in time from HGN, and a forward rollout fromHNN. HNN did not learn the dynamics of the system and instead learned to reconstruct an averageimage.
Figure 8: Examples of sample rollouts for all four datasets from a trained HGN.
Figure 9: MUltimodal density learning using Hamiltonian flows. From left to right: KDE estimatorsof the target and learned densities; learned kinetic energy K(P) and potential energy V(q); singleleapfrog step and an integrated flow. The potential energy learned multiple attractors, also clearlyvisible in the integrated flow plot. The basins of attraction are centred at the modes of the data.
Figure 10: Comparison between RNVP (Dinh et al., 2017) and NHF on a Gaussian Mixture Dataset.
Figure 11: Average MSE for pixel reconstructions during training of HGN (leapfrog). The x axisindicates the training iteration number 1e+2.
Figure 12: Average MSE for pixel reconstructions during training of HNN (conv). The x axis indicatesthe training iteration number 1e+2.
Figure 13: A: example of using a SymPleCtiC (leapfrog) and a non-symplectic (EUler) integrators onthe Hamiltonian of a harmonic oscillator. The blue quadrilaterals depict a volume in phase space overthe course of integration. While the symplectic integrator conserves the volume of this region, but thenon-symplectic integrator causes it to increase in volume with each integration step. The symplecticintegrator clearly introduces less divergence in the phase space than the non-symplectic alternativeover the same integration window. B: an illustration of the leapfrog updates in the phase space, where qis position and P is momentum.
