Figure 1: Training dynamics for an ensemble of finite-width networks compared with an infi-nite network. Left: Mean and variance of the train and test MSE loss evolution throughout training.
Figure 2: Convergence of the Monte Carlo (MC) estimates of the WideResNet WRN-28-k (wherek is the widening factor) NNGP and NTK kernels (computed with monte_carlo_kernel_fn )to their analytic values (WRN-28-∞, computed with kerneLfn ), as the network gets wider byincreasing the widening factor (vertical axis) and as more random networks are averaged over(horizontal axis). Experimental detail. The kernel is computed in 32-bit precision on a 100 × 50batch of 8 × 8-downsampled CIFAR10 (Krizhevsky, 2009) images. For sampling efficiency, forNNGP the output of the penultimate layer was used, and for NTK the output layer was assumed tobe of dimension 1 (all logits are i.i.d. conditioned on a given input). The displayed distance is therelative Frobenius norm squared, i.e. kK - Kk,nkF2 / kKkF2 , where k is the widening factor and n isthe number of samples.
Figure 3: CIFAR-10 classification with varying neural network architectures. NEURAL TAN-gents simplify experimentation with architectures. Here we use infinite time NTK inference andfull Bayesian NNGP inference for CIFAR-10 for Fully Connected (FC, Listing 3), Convolutionalnetwork without pooling (CONV, Listing 2), and Wide Residual Network w/ pooling (WRES-NET, Listing 1). As is common in prior work (Lee et al., 2018; Novak et al., 2019), the classificationtask is treated as MSE regression on zero-mean targets like (-0.1, . . . , -0.1, 0.9, -0.1, . . . , -0.1) .
Figure 4: An example of the translation of a convolutional neural network into a sequence ofkernel operations. We demonstrate how the compositional nature of a typical NN computation on itsinputs induces a Corresponding compositional computation on the NNGP and NT kernels. Presentedis a 2-hidden-layer 1D CNN with nonlinearity φ, performing regression on the 10-dimensionaloutputs Z2 for each of the 4 (1, 2, 3, 4) inputs X from the dataset X. To declutter notation, unit weightand zero bias variances are assumed in all layers. Top: recursive output (z2) computation in the CNN(toP) induces a respective recursive nngp kernel (KC2 ③ Iio) computation (NTK computation beingsimilar, not shown). Bottom: explicit listing of tensor and corresponding kernel ops in each layer.
Figure 5: Performance scaling with batch size (left) and number of GPUs (right). Shows timeper entry needed to compute the analytic NNGP and NTK covariance matrices (using kerneLfn )in a 21-layer ReLU network with global average pooling. Left: Increasing the batch size whencomputing the covariance matrix in blocks allows for a significant performance increase until acertain threshold when all cores in a single GPU are saturated. Simpler models are expected to havebetter scaling with batch size. Right: Time-per-sample scales linearly with the number of GPUs,demonstrating near-perfect hardware utilization.
Figure 6: Training a neural network and its various approximations using nt. taylor_expand .
Figure 7: Predictive negative log-likelihoods and condition numbers. Top. Test negative log-likelihoods for NNGP posterior and Gaussian predictive distribution for NTK at infinite trainingtime for CIFAR-10 (test set of 2000 points). Fully Connected (FC, Listing 3) and Convolutionalnetwork without pooling (CONV, Listing 2) models are selected based on train marginal negativelog-likelihoods in Figure 3. Bottom. Condition numbers for covariance matrices corresponding toNTK/NNGP as well as respective predictive covaraince on the test set. Ill-conditioning of WideResidual Network kernels due to pooling layers (Xiao et al., 2019) could be the cause of numericalissues when evaluating predictive NLL for this kernels.
