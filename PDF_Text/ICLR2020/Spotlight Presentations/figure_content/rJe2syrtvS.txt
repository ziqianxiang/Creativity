Figure 1: Illustration of our proposed instrumentation-free system requiring minimal human engineering.
Figure 2: We draw a comparison between current real world learning systems which rely on instrumentationversus a system that learns in an environment more representative of the real world, free of instrumentation.
Figure 3: Our object reposition-ing task. The goal is to move theobject from any starting config-uration to a particular goal posi-tion and orientation.
Figure 4: We report the approximate number ofsamples needed for a policy learned with a prior off-policy RL algorithm (SAC) to achieve average train-ing performance of less than 0.15 in pose distance(defined in Appendix C.1.3) across 3 seeds on the re-positioning task. We compare training performanceafter varying three axes: ground truth rewards vs.
Figure 5: We observe that when training reset free toreach a single goal, while the pose distance at trainingtime is quite low, the pose errors obtained at test-timewith the learned policy are very high. This indicatesthat while the object is getting close to the goal attraining time, the policies being learned are still noteffective.
Figure 6: Visualizations of the goal configurations of the simulated and real world tasks being considered.
Figure 7: Quantitative evaluation of performance in simulation for bead manipulation, valve rotation and freeobject repositioning (left to right) run with 10 random seeds. The error bars show 95% bootstrap confidenceintervals for average performance. While other variants are sufficient to get good evaluation performance oneasier tasks, harder tasks like free object repositioning require random perturbations and unsupervised repre-sentation learning to learn skills reset-free. See Appendix C.1 for details of evaluation procedures.
Figure 8: Quantitative evaluation of performance in real-world for valve rotation and bead manipu-lation. Policies trained with perturbation controllers have effectively learned behaviors after 17 and5 hours of training, respectively. For more fine-grained reporting of results see Figs 13-16.
Figure 9: Evaluation rollouts of R3L on the real world tasks for policies trained without instrumentation.
Figure 10: These are the 8 initial positions used for evaluating the performance of the bead manipulationpolicy. The goal configuration (which is also an initial evaluation position) is highlighted in yellow.
Figure 11: These are the 8 initial positions used for evaluating the performance of the valve rotation policy.
Figure 12: These are the 15 initial positions used for evaluating the performance of the free object reposi-tioning policy. The goal configuration (x, y, Î¸)goal which is also an initial evaluation position is highlighted inyellow.
Figure 13: These are the results of the evaluation rollouts on the valve rotation task in the real world using ourmethod (without the VAE). Trained policies were saved at regular intervals and evaluated post-training. Eachrow is a different policy, and each column an evaluation rollout from a different initial configuration. The goalis highlighted in yellow. Our method is able to achieve high success rates after 5 hours of training.
Figure 14: These are the results of evaluation rollouts on the valve rotation task in the real world using theVICE single goal baseline. The policies fail to evaluate well, especially from initial positions far from the goalposition.
Figure 15: These are the results of the evaluation rollouts on the valve rotation task in the real world using ourmethod (without the VAE). Trained policies were saved at regular intervals and evaluated post-training. Eachrow is a different policy, and each column an evaluation rollout from a different initial configuration. The goalis highlighted in yellow. Our method is able to achieve high success rates after 17 hours of training.
Figure 16: These are the results of evaluation rollouts on the valve rotation task in the real world using theVICE single goal baseline. The policies fail to evaluate consistently, except when the initial configurationmatches the goal configuration.
