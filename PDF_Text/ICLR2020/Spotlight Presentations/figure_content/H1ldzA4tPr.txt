Figure 1: Overview of our model. A graph neural network φ takes in the current state of the physicalsystem xt , and generates object-centric representations in the Koopman space gt . We then use theblock-wise Koopman matrix K and control matrix L identified from equation 6 or equation 8 topredict the Koopman embeddings in the next time step gt+1. Note that in K and L, object pairs ofthe same relation share the same sub-matrix. Another graph neural network ψ maps gt+1 back to theoriginal state space, i.e., xt+1. The mapping between gt and gt+1 is linear and is shared across alltime steps, where we can iteratively apply K and L to the Koopman embeddings and roll multiplesteps into the future. The formulation enables efficient system identification and control synthesis.
Figure 2: Qualitative results. Top: our model prediction matches the ground truth over a long period.
Figure 3: Quantitative results on simulation. The x axis shows time steps. The solid lines indicatemedians and the transparent regions are the interquartile ranges of simulation errors. Our methodsignificantly outperforms the baselines in all testing environments.
Figure 4: Quantitative results on control and ablation studies on model hyperparameters. Left:box-plots show the distributions of control errors. The yellow line in the box indicates the median.
Figure 5:	Ablation study on the metric loss in the Rope environment. (a) shows the distributionsof the logarithm distance ratio, i.e, log ］夕［［；』2). The model trained with metric loss has a distanceratio much more concentrated to 1, which indicates it preserves the distance much better than thecounterpart. (b) illustrates the simulation error of two models, where their performance is on par. (c)shows that the model trained using the metric loss performs better control.
Figure 6:	Modeling rope with known physical parameters. We show the comparison between ourmodel and IN/PN in scenarios where we have access to the ground truth physical parameters. In thiscase, IN and PN slightly outperform our method due to the internal linear structure in our model.
