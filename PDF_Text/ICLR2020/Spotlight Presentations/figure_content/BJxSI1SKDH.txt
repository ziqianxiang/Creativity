Figure 1: LMM for computing word representations while translating the sentence ‘... went home’into Turkish (‘eve-(to)home gitti(he/she/it)went’). The character-level decoder is initialized withthe attentional vector hi computed by the attention mechanism using current context ci and the wordrepresentation ti as in Luong & Manning (2016).
Figure 2: The top row shows the density function of the continuous base distribution over (0, 1).
Figure 3: The effect of feature dimensions on translation accuracy in Turkish.
