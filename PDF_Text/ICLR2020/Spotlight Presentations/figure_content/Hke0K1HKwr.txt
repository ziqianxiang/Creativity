Figure 1: An example of wizard,s tasks inknowledge-grounded conversation of Wizard ofWikipedia (Dinan et al., 2019).
Figure 2: A graphical representation of the proposed sequential knowledge transformer (SKT)model. At the third turn, the goal is to generate wizard,s response (y3) given dialogue context(x≤3, y<3). Our model sequentially infer which knowledge is likely to be used (k≤3), from whichthe utterance y3 is generated.
Figure 3: Examples of generated responses by our model and baselines on Wizard of Wikipedia.
Figure 4:	Examples of selected knowledge and generated responses by our model and baselines onthe WoW Seen Test set.
Figure 5:	Examples of selected knowledge and generated responses by our model and baselines onthe WoW Unseen Test set.
