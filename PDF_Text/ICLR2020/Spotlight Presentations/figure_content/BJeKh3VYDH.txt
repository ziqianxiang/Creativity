Figure 1: Our algorithm upsamples an input point cloud (a) ina temporally coherent manner. Three exemplary outputs areshown in yellow in (b).
Figure 2: a) Schematic overview of fs (X). Black ar-rows represent scalar data. Point data is depicted ascolored arrows with the color indicating data cardinality(brown=k, red = kmax, green = nmax , blue = n, andpurple = n). b) Siamese network setup for temporal losscalculation.
Figure 3: An illustration of the relationship between input and output size. (a,b,d) show histograms of point setsizes for: (a,b) the input set; (c) the ground truth target sets; and (d) the network output, i.e. r times larger thanthe input. The latter deviates from the ground truth in (c), but follows its overall structure. This is confirmed in(b), which shows a heat map visualization of input vs. ground truth output size. The diagonal structure of thepeak confirms the approximately linear relationship.
Figure 4: The effect of our variable output handling for exemplary patches. In red the ground truth target, in bluethe inferred solution. Left (a) with fixed output size, and on the right (b) with the proposed support for variableoutput sizes. The latter approximates the shape of the red ground truth points significantly better. (a) leads torather uniform shapes that, e.g., cover empty space above the ground truth in both examples.
Figure 5: Ablation study for our temporal loss formulation. Black points indicate targets, while green pointsare generated (both shown as time average). a) Result from previous work; b) With L2V loss; c) the proposedvelocity loss LEV; d) our full loss formulation with LEV + LEA. While (a) has difficulties approximating thetarget shape and the flickering output is visible as blurred positions, the additional loss terms (esp. in (c) and (d))provide stable results that closely approximate the targets. Note that (b) leads to an undesirably static motionnear the bottom of the patch. As the input points here are moving the output should mimic this motion, like (c,d).
Figure 6: Left, a result without the mingling loss from Eq. 6,right with (a single point group highlighted in orange). Theformer has many repeated copies of a single pattern, whichthe mingling loss manages to distribute as can be seen in theright picture.
Figure 7:	Illustrations of the latent spaces learned by our networks. (a) shows averaged latent space values for100 random patch sequences of our 2D data set. The green curve shows our method with temporal coherenceloss, while the pink curve was generated without it. The same data is shown in frequency space in (b), where thered curve represents the frequency of the data with temporal loss, and the blue curve the frequency of the datawithout. This graph highlights the reduced amount of high frequency changes in the latent space with temporalloss, esp. in frequency space, where the red curve almost entirely lies below the blue one. (c) contains frequencyinformation for the latent space content of the same 100 patch sequences, but in a random order. In this case,the blue and red curve both contain significant amounts of high-frequencies. I.e., our method reliably identifiesstrongly changing inputs.
Figure 8:	Evaluation of the temporal stability for generated point clouds, in red with our temporal loss formulation,in blue without. Graph (a) shows the temporal change of the point density (1st derivative), while (b) shows the2nd derivative. In (c) and (d) the error of the 1st and 2nd derivatives of the positions w.r.t. ground-truth referencepoints is shown.
Figure 9: Our method applied to an animation of a moving spider. (a) Input point cloud, (b) three frames of ourmethod, (c) a detail from previous work (top) and our method (bottom). Note that our method at the bottompreserves the shape with fewer outliers, and leads to a more even distribution of points, despite generating fewerpoints in total (see Table 3).
Figure 10: Examples from our synthetic data generation process. In both sections (a) and (b) a high resolutionreference frame is shown in purple, and in green the down-sampled low resolution frames generated from it. Thetraining data is generated by sampled patches from these volumes.
Figure 11: An overview of our network architecture. The first row shows the hierarchical point convolutions,while the bottom rows illustrate the processing of extracted features until the final output point coordinates aregenerated.
Figure 12: Convergence plots for the training runs of our different 2D and 3D versions. The combined loss onlyillustrates convergence behavior for each method separately, as weights and terms differ across the four variants.
