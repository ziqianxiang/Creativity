Figure 1: (a) The schematic of the method: f is the black-box function producing the posterior probability. δ isthe required change in black-box’s output f (x). If (x, δ) is an explainer function for f, which shifts the valueof f (x) by δ. The E(∙) is an encoder that maps the data manifold Mx to the embedding manifold Mz. Xδ isan abbreviation for If (x, δ). (b) The architecture of our model: E is the encoder, Gδf denotes the conditionalgenerator G(∙, Cf (x, δ)), f is the black-box and D is the discriminator. The circles denote loss functions.
Figure 2: Visual explanations generated for three prediction tasks: smiling/not-smiling face (first two rows),young/old face (middle two rows) and Cardiomegaly/healthy chest x-ray (bottom two rows). The first columnshows the query image, followed by the corresponding generated explanations. The values above each imageare the output of the classifier f . For Cardiomegaly, we show the segmentation of the heart (yellow edge) andreport normalized heart size (values in parenthesis), which is indicative of the disease.
Figure 3: Plot of the expected outcome from the classifier, f(x)+δ, against the actual response of the classifieron generated explanations, f (xδ). The monotonically increasing trend shows a positive correlation betweenf(x) + δ and f(xδ), and thus the generated explanations are consistent with the expected condition.
Figure 4: Cardiomegaly disease is associated with large heart size. In (a) we show the positive correlationbetween the heart size and the response of the classifier f (x). (b) Comparison of the distribution of the heartsize in the four groups. (c) Plot to show the drop in accuracy of the classifier as we perturb the most relevantpixels (relevance calculated from saliency map) in the image.
Figure 5: Our comparison with popular gradient-based saliency map producing methods on the prediction taskof identifying smiling faces in CelebA dataset.
Figure 6: The visual explanations for two classifiers, both trained to classify “Smiling” attribute on CelebAdataset. For each example, the top row shows results from “Biased” classifier whose data distribution is con-founded with “Gender”. The bottom row shows explanations from “No-Biased” classifier with uniform datadistribution w.r.t gender. The top label indicates output of the classifier and the bottom label is the output ofan oracle classifier for the con-founding attribute gender. The visual explanations for the “Biased” classifierchanges the gender as it adds smile on the face.
Figure 7: Visual explanations generated for three prediction tasks on CelebA dataset. The first column showsthe query image, followed by the corresponding generated explanations.
Figure 8: Visual explanations generated for six prediction tasks on CelebA dataset. The first column showsthe query image, followed by the corresponding generated explanations. The values above each image are theoutput of the classifier f .
Figure 9: Plot of the expected outcome from the classifier, f (x) + δ, against the actual response ofthe classifier on generated explanations, f(xδ). The monotonically increasing trend shows a positivecorrelation between f(x) + δ and f(xδ), and thus the generated explanations are consistent with theexpected condition.
Figure 10: The interface for the human evaluation done using Amazon Mechanical Turk (AMT).
Figure 11: Each cell is the fraction of the generated explanations, that have flipped in source attributeas compared to the query image. The x-axis is source attribute and y-axis is the target attribute forwhich explanation is generated. Note: This is not a confusion matrix.
Figure 12: Ablation study to show the effect of KL loss term. Plot of the expected outcome from theclassifier, f(x) + δ, against the actual response of the classifier on generated explanations, f(xδ).
