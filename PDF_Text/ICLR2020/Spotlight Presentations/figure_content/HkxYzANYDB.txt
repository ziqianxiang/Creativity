Figure 1: Sample video, questions, and answers from our CoLlision Events for Video REpresentation and Rea-soning (CLEVRER) dataset. CLEVRER is designed to evaluate whether computational models can understandwhat is in the video (I, descriptive questions), explain the cause of events (II, explanatory), predict what willhappen in the future (III, predictive), and imagine counterfactual scenarios (IV, counterfactual). In the fourimages (a-d), only for visualization purposes, We apply stroboscopic imaging to reveal object motion. Thecaptions (e.g., ‘First collision’) are for the readers to better understand the frames, not part of the dataset.
Figure 2: Sample questions and programs from CLEVRER. Left: Descriptive question. Middle and right:multiple-choice question and choice. Each choice can pair with the question to form a joint logic trace.
Figure 3: Distribution of CLEVRER question types.
Figure 4: Our model includes four components: a video frame parser that generates an object-based representationof the video frames; a question parser that turns a question into a functional program; a dynamics predictor thatextracts and predicts the dynamic scene of the video; and a symbolic program executor that runs the program onthe dynamic scene to obtain an answer.
Figure 5: Performance of our model under different numberof programs used for training the question parser.
Figure 6:	Descriptive question sub-type and answer space statistics.
Figure 7:	Sample results of NS-DR on CLEVRER. ‘Pred.’ and ‘CF.’ indicate predictive and counterfactualevents extracted by the model’s dynamics predictor. The counterfactual condition shown in this example is toremove the cyan cylinder.
Figure 8:	Sample videos and questions from CLEVRER. Stroboscopic imaging is applied for motion visualiza-tion.
Figure 9:	Example of descriptive question programs.
Figure 10:	Example of explanatory question and choice programs.
Figure 11:	Example of predictive / counterfactual question and choice programs.
