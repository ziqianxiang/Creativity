Figure 1: The inference process of the standard LISTA and evolved versions with our gates3.1 Sparse Coding with Gain GatesRecent works have shown linear convergence of LISTA (Chen et al., 2018; Liu et al., 2019). In orderto guarantee the convergence, it is also demonstrated that the value of bias terms should be largeenough to eliminate all “false positive” in the support of the generated codes. However, this may leadto an issue that the magnitude of the generated code components in LISTA must be smaller than or atmost equal to those of the ground-truth. Our result in Proposition 1 makes this formal. For clarity ofthe result, we would like to introduce the following definition first.
Figure 2: The derivative function (illustrated in blue) of f(x, y) + λr(x), in which r(x) = kxk1, ismonotonic owing to the convexity of f (x, y) and r(x), and its output should be consistently smallerthan the derivative (illustrated in orange) of the upper bound in absolute value. Let x* be the optimalsolution to the problem, then we know from the figure that the estimation with a standard ISTA update(i.e., η = 1) normally “lags behind”.
Figure 3: Average results confirming our Proposition 1 and Theorem 2, in which intermediate outputsof a single network over five runs are reported in (a) and (b) while networks with varying depth areevaluated in (c), over five runs as well. It is plotted from layer indices of 1 in (a) and (b), since thetwo metrics (i.e., false positive rate and ratio of generated non-zero code components that requiregains) do not make much sense with an initial code estimation (i.e., 0).
Figure 4: Average results over the whole test set confirm our Theorem 1, in which intermediateoutputs of a single network is reported. It can be seen that: (a) the gate output converges to 1, and (b)LISTA with our gain gates converges as expected. It is plotted from layer index 1 in (a) since no gainis imposed on the initial code estimation (i.e., 0.)inwhichΘ={W(t),U(t),b(t)}t=0,...,d-1∪ Λ(0) ∪ . . . Λ(d-1) is the set of all learnable parametersin the sparse coding network that generates x(d) given y. Note that in comparison with the parameterset in a standard LISTA, it also contains the parameters in gate functions. In practice, we are given aset of training samples and opt to minimize an empirical loss instead of the one in Eq. (19).
Figure 5: Comparison of different (a) overshoot gate functions, (b) gain gate functions, and (c) theircombination over five runs. The experiment is performed with SNR=40dB.
Figure 6: Comparison of sparse coding methods in different settings over five runs. Our GLISTAconsistently outperforms the competitors in almost all test cases with different numbers of layers.
Figure 7: Experimental results validating our Proposition 2. It can be observed that the update ofISTA “lags behind”.
Figure 8: Comparison of overshoot and gain gate with similar methods over five runs.
Figure 9: Comparison of sparse coding methods under different settings over five runs. Our GLISTAconsistently outperforms the competitors in almost all test cases with different numbers of layers.
