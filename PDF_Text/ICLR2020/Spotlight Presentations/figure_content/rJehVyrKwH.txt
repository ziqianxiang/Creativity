Figure 1: Illustration of our method. We approximate a binary classifier φ that labels images asdogs or cats by quantizing its weights. Standard method: quantizing 夕 with the standard objectivefunction (1) promotes a classifier ^standard that tries to approximate 夕 over the entire input spaceand can thus perform badly for in-domain inputs. Our method: quantizing 夕 with our objectivefunction (2) promotes a classifier Oactivations that performs well for in-domain inputs. Images lying inthe hatched area of the input space are correctly classified by OaCtiVatiOnS but incorrectly by OStandard.
Figure 2: We quantize Cout filters of size Gn X K X K using a subvector size of d = K X K. Inother words, We spatially quantize the convolutional filters to take advantage of the redundancy ofinformation in the network. Similar colors denote subvectors assigned to the same codewords.
Figure 3: Compression results for ResNet-18 and ResNet-50 architectures. We explore two com-pression regimes as defined in Section 4.1: small block sizes (block sizes of d = 4 and 9) and largeblock sizes (block sizes d = 8 and 18). The results of our method for k = 256 centroids are ofpractical interest as they correspond to a byte-compatible compression scheme.
