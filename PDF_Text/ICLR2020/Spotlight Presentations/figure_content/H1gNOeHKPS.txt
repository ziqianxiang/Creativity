Figure 1: Visualization of the NMU, where the weights (Wi,j) controls gating between 1 (identity) orxi , each intermediate result is then multiplied explicitly to form zj .
Figure 2: RMS loss curvature for a NAC+ unit followed by a NAC.. The weight matrices areconstrained to W1 = ww11 ww11 w01 w01 , W2 = [ w2 w2 ]. The problem is (xι + x2) ∙ (xι + x2 + x3 + x4)for x = (1, 1.2, 1.8, 2). The solution is w1 = w2 = 1 in (a), with many unstable alternatives.
Figure 3:	Multiplication task results when varying the hidden input-size and when varying theinput-range. Extrapolation ranges are defined in Appendix C.4.
Figure 4:	MNIST sequential multiplication task. Each model is trained on sequences of two digits,results are for extrapolating to longer sequences. Error-bars represent the 95% confidence interval.
Figure 5: Error between theoretical approximation and the numerical approximation estimated byrandom sampling of 100000 observations at each combination of rz and rw.
Figure 6: Shows how the dataset is parameterized.
Figure 7:	Ablation study where Rsparse is removed and the clamping of W is removed. There are 50experiments with different seeds, for each configuration.
Figure 8:	Shows the effect of the dataset parameters.
Figure 9:	Shows the gating-value in the NALU layer and a variant that uses NAU/NMU instead ofNAC+/NAC. . Separate/shared refers to the weights in NAC+/NAC. used in NALU.
Figure 10:	Shows effect of λsparse m NAU on the arithmetic dataset for the + operation.
Figure 11:	Shows effect of λsparse in NAU on the arithmetic dataset for the — operation.
Figure 12:	Shows effect of λsparse in NMU on the arithmetic dataset for the X operation.
Figure 13:	Shows the ability of each model to learn the arithmetic operation of addition and back-propagate through the arithmetic layer in order to learn an image-to-scalar value for MNIST digits.
Figure 14:	Same as figure 13, but where the NAU model do not use the Rz regularizer.
Figure 15:	Shows the ability of each model to learn the arithmetic operation of addition and back-propagate through the arithmetic layer in order to learn an image-to-scalar value for MNIST digits.
