Figure 1: Challenges of neural audio synthesis. Full description provided in Section 1.1.
Figure 2: Autoencoder architecture. Red components are part of the neural network architecture,green components are the latent representation, and yellow components are deterministic synthesiz-ers and effects. Components with dashed borders are not used in all of our experiments. Namely,z is not used in the model trained on solo violin, and reverb is not used in the models trained onNSynth. See the appendix for more detailed diagrams of the neural network components.
Figure 3: Separate interpolations over loudness, pitch, and timbre. The conditioning features (solidlines) are extracted from two notes and linearly mixed (dark to light coloring). The features of theresynthsized audio (dashed lines) closely follow the conditioning. On the right, the latent vectors,z(t), are interpolated, and the spectral centroid of resulting audio (thin solid lines) smoothly variesbetween the original samples (dark solid lines).
Figure 4: Timbre transfer from singing voice to violin. F0 and loudness features are extracted fromthe voice and resynthesized with a DDSP autoencoder trained on solo violin.
Figure 5: Decomposition of a clip of solo violin. Audio is visualized with log magnitude spec-trograms. Loudness and fundamental frequency signals are extracted from the original audio. Theloudness curve does not exhibit clear note segmentations because of the effects of the room acous-tics. The DDSP autoencoder takes those conditioning signals and predicts amplitudes, harmonicdistributions, and noise magnitudes. Note that the amplitudes are clearly segmented along noteboundaries without supervision and that the harmonic and noise distributions are complex and dy-namic despite the simple conditioning signals. Finally, the extracted impulse response is applied tothe combined audio from the synthesizers to give the full resynthesis audio.
Figure 6: Diagram of the Additive Synthesizer component. The synthesizer generates audio as a sumof sinusoids at harmonic (integer) multiples of the fundamental frequency. The neural network isthen tasked with emitting time-varying synthesizer parameters (fundamental frequency, amplitude,harmonic distribution). In this example linear-frequency log-magnitude spectrograms show howthe harmonics initially follow the frequency contours of the fundamental. We then factorize theharmonic amplitudes into an overall amplitude envelope that controls the loudness, and a normalizeddistribution among the different harmonics that determines spectral variations.
Figure 7: Interpretable generative models enables disentanglement and extrapolation. Spectrogramsof audio examples include dereverberation of a clip of solo violin playing (left), transfer of theextracted room response to new audio (center), and transposition below the range of training data(right).
Figure 8: Diagram of the z-encoder.
Figure 9: Diagram of the decoder for the harmonic synthesizer and the filtered noise synthesizer.
Figure 10: MLP in the decoder.
