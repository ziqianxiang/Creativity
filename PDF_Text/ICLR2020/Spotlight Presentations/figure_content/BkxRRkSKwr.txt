Figure 1: Different score attribution algorithms. (a) Input occlusion assigns a negative score forthe word “interesting”, as the sentiment of the phrase becomes less negative after removing “in-teresting” from the original sentence. (b) Additive attributions assign importance scores for words“not” and “interesting” by linearly distributing contribution score of “not interesting“, exemplifiedwith Shapley Values (Shapley, 1953). Intuitively, only (c) Hierarchical explanations highlight thenegative compositional effect between the words “not” and “interesting”.
Figure 2: (Left) Illustration of the CD and SCD at calculating the decomposition for h = σ(β + γ),following Eq. 2. Red lines indicate computation that make CD explanations dependent on the wordsoutside the give phrase. (Right) Illustration of the sampling step Xδ 〜p(Xδ ∣x-δ) for calculating theimportance of the word very in SOC and SCD, with size of context N = 1. The padding operationis for SOC.
Figure 3: Hierarchical Explanation of a prediction by the BERT Transformer model on SST-2. Wegenerate explanations for all the phrases on the truncated constituency parsing tree, with positivesentiments shown in red and negative sentiments shown in blue. We see our method identify positivesegments in the overall negative sentence, such as “a breath of fresh air”also compare with a naive however neglected baseline in prior literature, which directly feed thegiven phrase to the model and take the prediction score as the importance of the phrase, noted as Di-rect Feed. In BERT models, Direct Feed is implemented by turning off the attention mask of wordsexcept the [CLS] token and the phrase to be explained. For our algorithms, we list the performanceof corpus statistic based approach (Statistic) for approximating context independent importance inEq. 3, Sampling and Contextual Decomposition (SCD), and Sampling and Occlusion (SOC) algo-rithm. In section 4.4, we also consider padding instead of sampling the context words in SCD andSOC.
Figure 4: Extracting phrase-level classification patterns from LSTM relation extraction model withSCD. Red indicate evidence for predicting the class, and blue indicate distractor for predicting theclass. By applying the agglomerative clustering algorithm and defining a threshold score, we effe-cively extract “a sister, O-Person” as a classification rule for the relation per:siblings. However, wesee CD fails in this example.
Figure 5: Results for human evaluation on the Transformer model trained on SST-2 sentiment anal-ysis dataset (between SOC, SCD, ACD, GradSHAP) and the LSTM model trained on TACREDrelation extraction dataset (between SOC, SCD, CD, DirectFeed).
Figure 6: Word ρ and phrase ρ curves as the size of the context region N and the number of samplesK change on the BERT model trained on the SST-2 dataset. Dash line notes for the performance ofpadding the context words instead of sampling.
Figure 7: Word ρ and phrase ρ curve as the size of the context region N and the number of samplesK change on LSTM trained on the SST-2 dataset. Dash line notes for the performance of paddingthe context words instead of sampling.
Figure 8: More examples about rule extraction from LSTM models trained on TACRED relationextraction dataset with SOC. Red indicate evidence for predicting the class, and blue indicate dis-tractor for predicting the class. By applying the agglomerative clustering algorithm and defining athreshold score, we effectively extract classification rules from LSTM models. The ground truthlabel noted on the topDataset	Label	PatternSST-2	Positive Negative	frighteningly evocative; insight and honesty neither funny nor provocative; kill the suspenseTACRED	person:age organization:top-member person:origin	[PERSON], [NUMBER], was; a [NUMBER] man chief engineer of the [ORGANIZATION] [Nationality] citizen; tribal member from [COUNTRY]Table 3: Phrase-level classification patterns extracted from models. We show the results of SCD andSOC respectively for the SST-2 and the TACRED dataset.
Figure 9: Explanation heatmaps generated by SOC, SCD, CD, and GradSHAP on a negativelypredicted sentence by BERT Transformer model in SST-2 dataset. Only SOC and SCD capturesadversarial conjunction connected by “but”.
