Figure 1: Proposed graphical model for style transfer via bitext completion. Shaded circles denotethe observed variables and unshaded circles denote the latents. The generator is parameterized as anencoder-decoder architecture and the prior on the latent variable is a pretrained language model.
Figure 2: Depiction of amortized variational approximation. Distributions q(y∣x) and q (x|y) representinference networks that approximate the model’s true posterior. Critically, parameters are sharedbetween the generative model and inference networks to tie the learning problems for both domains.
Figure 3: Improvement overUNMT vs. classification accuracy.
Figure 4: Word F1 score by POS tag.
Figure 5: ELBO on the validation set v.s. the number training steps.
