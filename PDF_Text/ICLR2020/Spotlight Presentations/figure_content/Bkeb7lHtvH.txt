Figure 1: Impact of learning rate and delay on generalization error. Validation error with delay (τ = 32)and different learning rates η. We observe that unless We decrease the learning rate (proportionally to 1∕τ) thereis a generalization gap from the equivalent large batch training: (left) training curve. (right) validation error asa function of learning rate after reaching steady state. Horizontal dashed line is the validation error acquiredby the equivalent large batch training (S-SGD, τ = 0). Learning rates larger then 0.05 did not converge With(τ = 32) delay. CNN trained With MNIST. More details in section E.
Figure 2: Stability threshold is maintained when η cκ 1∕τ: In the left figure We show the number of epochsit takes to diverge from a minimum as a function of the learning rate η. The black circles are the stabilitythresholds — below which, we do not escape the minimum. In the right figure for each value of delay τ weshow 1 /η where η is the maximal learning rate in which we did not diverge. Due to sampling resolution, theremight be up to 8% deviation from the maximal learning rate found. This deviation is represented in the errorbars. VGG-11 trained with CIFAR10.
Figure 3: Better generalization obtained near the stability threshold. Validation error Vs. learning rate ηof VGG-11 trained with CIFAR10 for different delay values. Solid lines represent mean validation error. Themargins represent one standard deviation. The vertical dashed line is the learning rate according to eq. 8. Thehorizontal dashed line is the accuracy obtained with τ=0 (S-SGD).
Figure 4: Stability threshold is improved when in increasing the momentum parameter, when usingshifted momentum: (left) For different values of momentum m, We examine the relation between 0n and Tobtained by numerically evaluating the value of 焉 for which the maximal roots of eq. 11 ison the unit circle.
Figure 5: ResNet50 ImageNet asynchronous training. The baseline (in blue, S-SGD) is a large batchsynchronous training, following Goyal et al. (2017), where η (X batch size. For A-SGD, based on our analysis,We additionally change η X 1∕τ (in contrast, η X 1 /√τ does not converge at all). Thus, in this case, We get thesame learning rate as in a small batch regime. We compare three A-SGD options (in orange): (1) with standardmomentum (2) with momentum value m = 0; and (3) with shifted momentum. S-SGD validation final erroris 23.8%. A-SGD achieves 28.13% when training with standard momentum (left). The error drops to 25.49%when turning off momentum (middle). With shifted momentum, the error is similar, 25.4% (right). This matchesthe analysis in section 2.2.2. Solid lines are validation error and dashed lines are train error.
Figure 6: It is necessary to keep the learning rate inversely proportional to the delay to maintainstability. In blue We see the relation between : and T obtained by numerically evaluating the valueof On for which the maximal root of eq. 5 is on the unit circle. In orange we see the analyticapproximation 卷=∏ (2τ + 1). We can see that in order to maintain stability for a given minimumpoint, as the delay τ increases we need to decrease the learning rate η to keep η inversely proportionalto τ. In appendix (Fig. 10) we show that with momentum, we also get similar linear relation, onlywith a higher slope.
Figure 7: T 〜Unif{a, b} where a = 1 and b is an integer in the range [1, 64]. In this case, ET = a+b.
Figure 8: ∀μ, ∀k = 1,…,2μ : Pr(T = k) = Pr(k 一 0.5 ≤ Z ≤ k + 0.5) where Z 〜N(μ, 1) and μ isan integer in the range [1,30]. In this case, ET = μ. We see the relation between 焉 and ET obtainedby numerically evaluating the value of 焉 for which the maximal root of eq. 19 is on the unit circle.
Figure 9: Gaussian distribution of delay. Comparison of ResNet50 trained asynchronously onImageNet with two types of delay distributions: constant i.e., round robin (orange) and discreteGaussian distribution (blue). Constant distribution reaches 25.4% validation error while Gaussiandistriution reaches a similar error of 25.3%.
Figure 10: Using (standard) momentum, it is still necessary to keep the learning rate inverselyproportional to the delay to maintain stability. For different values of momentum m, we examinethe relation between 焉 and T obtained by numerically evaluating the value of 卷 for which themaximal root of eq. 25 is on the unit circle. Similar to the results with m = 0, we can see thatmaintaining stability requires to keep the learning rate η inversely proportional to the delay τ.
Figure 11: Shifted momentum improves stability. ResNet44 trained with CIFAR10 with samehyperparameters and three training algorithms: A-SGD with momentum (red), A-SGD withoutmomentum (Orange) and A-SGD with shifted momentum (Blue). We observed "spikes" in thetraining error that appear when training with large batch size or with delay for large number of epochs,without decreasing the learning rate. While momentum negates these "spikes" in large batch training,Shifted momentum improves the convergence stability of the model and negates these "spikes" whentraining with delay.
Figure 12: Learning rates larger than 0.8∕τ diverge from the minimum. We show the validationerror Vs. epochs for different delay and learning rate values. The baseline learning rate used is 0.8according to large batch training Hoffer et al. (2017). The minimum learning rate for each τ is thestability threshold.
Figure 13: Impact of learning rate and delay on generalization error during training. Validationerror with delay (τ = 32) as a function of learning rate at different epochs. The optimal learningrate of η = 0.01 chosen found in the scan (which matches our proposed learning rate scaling),stays the same throughout the training. This might suggest why it’s beneficial to use the optimalhyperparameters for the minimum, even when the algorithm hasn’t converged to the steady state yet.
