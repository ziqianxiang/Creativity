Figure 1: Sample-based approaches regularize the learning by the expected change, as measuredby a dissimilarity measure, of the response of the network for individual samples from Task A,after learning Task A and during learning Task B, Ex〜PA [d(φ(x; θ), φ(x; θA))], where d(∙, ∙) isa dissimilarity measure between two K-dimensional vectors. Therefore, the regularization is anempirical expected change of the response for samples. EWC and MAS fall under the sample-basedcategory. The proposed distribution-based approach, on the other hand, regularizes the change inthe overall distribution of the network’s output for input samples from Task A, after learning TaskA and during learning Task B, dp (PA(∙∣θ),ρA(∙∣ΘA)), where PA(∙∣θ) is defined in equation 8 anddp(∙, ∙) is a distance measure between two probability distributions defined on Z ⊆ RK.
Figure 2: The energy landscape of various distances as a function of the translation parameter. It canbe seen that both Wasserstein and Cramer distances respect the underlying geometry of the problemwhile the Jensen-Shannon distance fails to do so.
Figure 3: Comparison among online-EWC, MAS, and SCP on learning ten permuted MNIST tasks(a), and the histogram of Log(λ × ∙) of the synaptic importances (i.e., the diagonal values of F, Ω,and Γ, for EWC, MAS, and SCP, respectively) for each method (b).
Figure 4: Qualitative and quantitative comparison between MAS and SCP on sequential learning ofauto-encoders. Columns in Panel (a) show the reconstruction of data after learning consequent tasksin a random permutation of MNIST sequence. Panel (b) shows the average '1 -reconstruction lossfor each method over all tasks and over 10 runs.
Figure 5: A qualitative comparison of the EWC, MAS, and SCP algorithms on semantic segmenta-tion of the SYNTHIA Dataset (Ros et al., 2016). Task 1 (T1) is semantic segmentation in summer,and Task 2 (T2) is semantic segmentation in winter. The first row shows performance after learningT1 on input from T1. Second row, shows performance after learning T2 on input from T2. The thirdrow shows performance on input from T1 after learning T2. The last row, magnifies the last threeimages in the third row for the ease of comparison.
Figure 6: Testing Dice score (Zou et al., 2004) ofonline-EWC, MAS, and SCP on sequential learn-ing for semantic segmentation of the summer im-ages (Task 1) and winter images (Task 2) fromSYNTHIA dataset (Ros et al., 2016). The blueand red shadings on the plots indicate the dura-tions in which the models were trained on summerand winter data, respectivelyLastly, we go beyond the benchmark yet less practical MNIST dataset and address catastrophic for-getting in a more interesting/critical application of autonomous vehicles. We specifically considerthe problem of learning semantic segmentation of road scenes in a sequential manner, where the in-put distribution of the data changes over time. Semantic segmentation is the task of assigning a classlabel to every pixel of an input image. To that end, we use two sequences of the SYNTHIA dataset(Ros et al., 2016), namely ‘SYNTHIA-SEQS-01-SUMMER’ and ‘SYNTHIA-SEQS-01-WINTER’as Task 1 and Task 2, respectively. There are 13 classes in the dataset namely: Miscellaneous, Sky,Building, Road, Sidewalk, Fence, Vegetation, Pole, Car, Sign, Pedestrian, Cyclist, and Lane Mark-ing. We keep the last 100 frames of each sequence as the testing-set and train a deep convolutionalU-Net architecture (Ronneberger et al., 2015) on the tasks mentioned above. For the loss function,we used (1 - Dice) (Zou et al., 2004), and each task was learned over 100 epochs. For the opti-mizer, we used the ADAM optimizer (Kingma & Ba, 2014) with learning rate, lr = 1e - 4. For
Figure 7:	Sample images from MNIST and SVHN dataset (a), and the testing performance of theOnline-EWC, MAS, and SCP on sequential learning starting from MNIST (Task 1) and then SVHN(Task 2) (b). The blue and red shadings on the plots indicate the durations in which the models weretrained on the MNIST and SVHN datasets, respectively.
Figure 8:	The model we used in the MNIST-to-SVHN experiment.
Figure 9: Sample super-classes from the CIFAR100 dataset and visualization of the data augmenta-tion procedure we used in this experiment (a), and the average testing accuracy of the Online-EWC,MAS, and SCP on sequential learning of the tasks (b). We note that we were, unfortunately, onlyable to run the experiment once, and will update the Figure with the results from multiple runs assoon as they are available.
