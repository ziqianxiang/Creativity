Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semanticallydifferent nodes) can be well separated, however, node embeddings are not composable, and thusresulting graph embeddings (denoted by their classes, + and -) that are created by pooling node-levelembeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are wellseparated, however the embeddings of individual nodes do not necessarily capture their domain-specific semantics. (a.iii) High-quality node embeddings are such that nodes of different types arewell separated, while at the same time, the embedding space is also composable. This allows foraccurate and robust representations of entire graphs and enables robust transfer of pre-trained modelsto a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially,our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training(Supervised Attribute Prediction) enable both node-level and graph-level pre-training.
Figure 2: Illustration of our node-level methods, Context Prediction and Attribute Masking for pre-training GNNs. (a) In Context Prediction, the subgraph is a K-hop neighborhood around a selectedcenter node, where K is the number of GNN layers and is set to 2 in the figure. The context is definedas the surrounding graph structure that is between r1- and r2-hop from the center node, where we user1 = 1 and r2 = 4 in the figure. (b) In Attribute Masking, the input node/edge attributes (e.g., atomtype in the molecular graph) are randomly masked, and the GNN is asked to predict them.
Figure 3: Test ROC-AUC of protein function prediction using different pre-training strategieswith GIN. (Left) Test ROC-AUC scores (%) obtained by different pre-training strategies, wherethe scores are averaged over the 40 fine-grained prediction tasks. (Middle and right): Scatter plotcomparisons of ROC-AUC scores for a pair of pre-training strategies on the 40 individual downstreamtasks. Each point represents a particular individual downstream task. (Middle): There are manyindividual downstream tasks where graph-level multi-task supervised pre-trained model performsworse than non-pre-trained model, indicating negative transfer. (Right): When the graph-level multi-task supervised pre-training and Attribute Masking are combined, negative transfer is avoided acrossdownstream tasks. The performance also improves over pure graph-level supervised pre-training.
Figure 4: Training and validation curves of different pre-training strategies on GINs. Solid anddashed lines indicate training and validation curves, respectively.
Figure 5: Training and validation curves of different pre-training strategies. The solid anddashed lines indicate the training and validation curves, respectively.
Figure 6: Scatter plot comparisons of ROC-AUC scores of our Context Prediction + graph-levelsupervised pre-training strategy versus the two baseline strategies (non-pre-trained and graph-levelsupervised pre-trained) on the 40 individual downstream tasks of predicting different fine-grainedprotein function labels.
