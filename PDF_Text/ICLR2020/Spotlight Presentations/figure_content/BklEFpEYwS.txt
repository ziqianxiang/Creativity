Figure 1: Left: An example of non-mutually-exclusive pose prediction tasks, which may lead to the memoriza-tion problem. The training tasks are non-mutually-exclusive because the test data label (right) can be inferredaccurately without using task training data (left) in the training tasks, by memorizing the canonical orientationof the meta-training objects. For a new object and canonical orientation (bottom), the task cannot be solvedwithout using task training data (bottom left) to infer the canonical orientation. Right: Graphical model for√^>*meta-learning. Observed variables are shaded. Without either one of the dashed arrows, Y is conditionallyindependent of D given θ and X*, which we refer to as complete memorization (Definition 1).
Figure 2: The performance of MAML and CNP with meta-regularization on the weights, as a function of theregularization strength β . We observe β provides us a knob with which we can control the degree to whichthe algorithm adapts versus memorizes. When β is small, we observe memorization, leading to large testerror; when β is too large, the network does not store enough information in the weights to perform the task.
Figure 3: An example of mutually-exclusive task distributions. In each task of mutually-exclusivefew-shot classification, different classes are randomly assigned to the N -way classification labels.
Figure 4: Graphical model of the regularization on activations. Observed variables are shaded andZ is bottleneck variable. The complete memorization corresponds to the graph without the dashedarrows.
Figure 5: Test MSE on the mutually-non-exclusive sinusoid problem as function of the number of gradientsteps used in the inner loop of MAML and MR-MAML. For each trial, we calculate the mean MSE over 100randomly generated meta-testing tasks. We report the mean and standard deviation over 5 random trials.
Figure 6: Visualization of the optimized weight matrix W that is connected to the inputs in the sinusoidregression example. The input X = (u, A) where U 〜Unif(-5, 5), A is 20 dimensional one-hot vector andthe intermediate layer is 100 dimensional, hence x ∈ R21 and W ∈ R21×100. For both CNP and MAML, themeta-regularization restricts the part of weights that is connected to A close to 0. Therefore it avoids storing theamplitude information in weights and forces the amplitude to be inferred from the task training data D, hencepreventing the memorization problem.
Figure 7: Meta-test results on the non-mutually-exclusive sinusoid regression problem with CNP. For eachrow, the amplitudes of the true curves (orange) are randomly sampled uniformly from [0.1, 4]. For illustrativepurposes, we fix the one-hot vector component of the input. (a): The vanilla CNP cannot adapt to new tasktraining data at test-time and the shape of prediction curve (blue) is determined by the one-hot amplitude notthe task training data. (b) (c): Adding meta-regularization on both activation and weights enables the CNP touse the task training data at meta-training and causes the model to generalize well at test-time.
Figure 8: Meta-test results on the non-mutually-exclusive sinusoid regression problem with MAML. Foreach row, the true amplitudes of the true curves (orange) are randomly sampled uniformly from [0.1, 4]. Forillustrative purposes, we fix the one-hot vector component of the input. (a): Due to memorization, MAMLadapts slowly and has large generalization error at test-time. (b) (c): Adding meta-regularization on bothactivation and weights recovers efficient adaptation.
Figure 9: Sensitivity of activation regularization and weight regularization with respect to the learning rate onthe pose prediction problem. For activation regularization, lower training loss corresponds to higher test MSEwhich indicates that the memorization solution is not solved. For weights regularization, lower training losscorresponds to lower test MSE which indicates proper training can converge to the adaptation solution.
Figure 10: The test accuracy of MAML with meta-regularization on the weights as a function of the regular-ization strength β on the mutually-exclusive 20-way 1-shot Omniglot problem. The plot shows the mean andstandard deviation across 5 meta-training runs. When β is small, MR-MAML slightly outperforms MAML,indicating that meta-regularization does not degrade performance on mutually-exclusive tasks. The accuracynumbers are not directly comparable to previous work (e.g., (Finn et al., 2017)) because we do not use dataaugmentation.
