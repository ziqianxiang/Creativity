Figure 1: Illustration of the doubledescent risk curve in two-layer linearnetworks (SNR= 16). Brighter colorindicates larger γ1 = d/n. Doubledescent is observed when the secondlayer coefficients are optimized (mainfigure), but not when the first layerweights are optimized (subfigure).
Figure 2: Population risk of two-layer neural networks with optimized second layer under (A1)(A2). Brightercolor indicates larger γ1. (a) risk of linear network with r2 /σ2 = 16 and γ1 < 1. (γ1 > 1 is shown in Figure 1)(b) variance of network with ReLU activation. Black line corresponds to γ1 → ∞ predicted by Corollary 5.
Figure 3: Bias and variance of two-layer sigmoid network with optimized first layer under (A1)(A2). Individualdotted lines correspond to different γ2 (from 0.2 to 2) which is independent to the risk. The bias and variance forboth initializations is well-aligned with Theorem 7 and Theorem 8.
Figure 5: Population risk of two-layer linear network with fixed random 1st layer with SNR=25/16under Gaussian input and linear teacher. Brighter color indicates larger γ1 .
Figure 4: trajectory of neurons from initialization (dark blue) to optimum (orange) on the first two dimensions(two-layer SoftPlus student and linear teacher; SNR= 1/4). For vanishing initialization the neurons stay closeto one another throughout the trajectory, whereas for non-vanishing initialization the neurons stay close toinitialization.
Figure 6: Bias and variance of two-layer SoftPlus network with optimized first layer under (A1)(A2). Individualdotted lines correspond to different γ2 (from 0.2 to 2) which is independent to the risk. The bias and variance forboth initializations is well-aligned with Theorem 7 and Theorem 8, respectively.
Figure 7: Population risk (scaled by 1/d) of two-layer ReLU network trained to fit a two-layer ReLU teachermodel with h = d neurons. Brighter color corresponds to larger γ1. Similar to the linear teacher case, doubledescent is observed when the second layer is optimized (a) but not when the first layer is optimized (b).
Figure 8: Bias of (a) SoftPlus and (b) sigmoid two-layer network with optimized first layer under (A1)(A2).
Figure 9: Bias of two-layer ReLU networks with optimized first layer under Gaussian data and linear teacher.
