Figure 1: Figure (a) depicts a binary classification dataset in which the minority group shown on theright of the plot is underrepresented. This tilts the logistic regression decision boundary in favor ofthe majority group on the left. Figure (b) shows the unfair map of the logistic regression decisionboundary. It maps samples in the minority group towards the majority group. Figure (c) shows analgorithmically fair classifier that treats the majority and minority groups identically.
Figure 2: Box-plots of sentiment scores4.1	Fair sentiment prediction with word embeddingsProblem formulation We study the problem of classifying the sentiment of words using positive(e.g. ‘smart’) and negative (e.g. ‘anxiety’) words compiled by Hu & Liu (2004). We embed wordsusing 300-dimensional GloVe (Pennington et al., 2014) and train a one layer neural network with1000 hidden units. Such classifier achieves 95% test accuracy, however it entails major individualfairness violation. Consider an application of this sentiment classifier to summarizing customer re-views, tweets or news articles. Human names are typical in such texts and should not affect thesentiment score, hence we consider fair metric between any pair of names to be 0. Then sentimentscore for all names should be the same to satisfy the individual fairness. To make a connection togroup fairness, following the study of Caliskan et al. (2017) that reveals the biases in word embed-dings, we evaluate the fairness of our sentiment classifier using male and female names typical forCaucasian and African-American ethnic groups. We emphasize that to satisfy individual fairness,the sentiment of any name should be the same.
