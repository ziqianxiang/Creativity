Figure 1: Relative error of KWNG averaged over 100 runs for varying dimension form d = 1 (yellow)to d = 10 (dark red) for the hyper-sphere model. (a): box-plot of the relative error as d increases whileN = 5000 and M = [d√N]. (b) Relative error as the sample size N increases and M = [d√N]. (c):Relative error as M increases and N = 5000. A gaussian kernel is used with a fixed bandwidth σ= 1.
Figure 2: Left (a): Training error per iteration for KWNG, WNG, and EG. Right (b): projection of thesequence of updates obtained using KWNG, WNG and EG along the first two PCA directions of theWNG trajectory. The dimension of the sample space is fixed to d = 10. Exact valued for the gradientare used for EG and WNG. For KWNG, N = 128 samples and M = 100 basis points are used. Theregularization parameters are set to: λ= 0 and = 10-10. An optimal step-size γt is used: γt = 0.1 forboth KWNG and WNG while γt =0.0001 for EG.
Figure 3: Test accuracy and Training accuracy for classification on Cifar10 (top) and Cifar100(bottom) in both the ill-conditioned case (left side) and well-conditioned case (right side) for differentoptimization methods. on Cifar10 Results are averaged over 5 independent runs except for KFACand eKFAC.
Figure 4: Evolution of the relative error of KWNG averaged over 100 runs for varying dimension formd= 1 (yellow) to d= 10 (dark red). For each run, a random value for the parameter θ and for the Euclideangradient VL(θ) is sampled from a centered Gaussian with variance 0.1. Inallcases, λ = 0 and e = 10-5.
Figure 5:	Relative error of the KWNG for varying bandwidth of the kernel. Results are averaged over100 runs for varying dimension form d= 1 (yellow) to d= 10 (dark red). For each run, a random value forthe parameter θ and for the Euclidean gradient VL(θ) is sampled from a centered Gaussian with variance0.1. In all cases, λ= e = 10-10. The sample size is fixed to N = 5000 and the number of basis pointsM = [d√N]is set to. Left: uniform distributions on a hyper-sphere, middle: multivariate normal, andright: multivariate log-normal.
Figure 6:	Training accuracy (left) and test accuracy (right) as a function of time for classificationon Cifar10 in both the ill-conditioned case (top) and well-conditioned case (bottom) for differentoptimization methods.
Figure 7:	KWNG vs Diagonal conditioning in the ill-conditioned case on Cifar10. In red and blue, theeuclidean gradient is preconditioned using a diagonal matrix D either given by Di = kT.,ik or Di = kT.,ik,where T and T are defined in Propositions 5 and 6. The rest of the traces are obtained using the stableversion of KWNG in Proposition 6 with different choices for the damping term D = I , D = kT.,ik andkT.,ik. All use a gaussian kernel except the yellow traces which uses a rational quadratic kernel.
