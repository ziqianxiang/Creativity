Figure 1: Progressive learning of hierarchical representations. White blocks and solid lines are VAEmodels at the current progression. α is a fade-in coefficient for blending in the new network com-ponent. Gray circles and dash line represents (optional) constraining of the future latent variables.
Figure 2: Quantitative comparison of disentanglement metrics. Each point is annotated by theβ value and averaged over top three best random seeds for the given β on the give model. Leftto right: reconstruction errors vs. disentanglement metrics of factor, MIG, and MIG-sup, a highervalue indicating a better disentanglement in each metric.
Figure 3: MIG vs. MIG-sup following a similar presentation in Fig. 2. A better disentanglementshould have higher MIG and higher MIG-sup, locating at the top-right quadrant of the plot.
Figure 4: Traversing each latent dimension in pro-VLAE (β = 8), VLAE (β = 10), and teacher-student model. The hierarchy of the latent variables is noted by brackets on the side.
Figure 5: Progressive learning of hierarchical representations. At each progression and for eachzl , the row of images are generated by randomly sampling from its prior distributions while fixingthe other latent variables (this is NOT traversing). The green bar at each row tracks the mutualinformation I(x; zl), while the total mutual information I(x; z) is labeled on top.
Figure 6:	Visualization of hierarchical features learnt for MNIST data. Each sub-figure is generatedby randomly sampling from the prior distribution of zl at one abstraction level while fixing theothers. The original latent code is inferred from a image with digit “0”. From left to right: z3encodes the highest abstraction: digit identity; z2 encodes stroke width; and z1 encodes other digitstyles.
Figure 7:	Visualization of hierarchical features learnt for CelebA data. Each subfigure is generatedby traversing along a selected latent dimension in each row within each hierarchy of zl ’s. From leftto right: latent variables z4 to z1 progressively learn major (e.g., gender in z4 and smile in z3) tominor representations (e.g. wavy-hair in z2 and eye-shadow in z1) in a disentangled manner.
Figure 8: An example of one factor being encoded in multiple dimensions. Each row is a traverse forone dimension (dimension order adjusted for better visualization). Notice that both dim1 and dim2are encoding floor-color, both dim3 and dim4 are encoding wall-color, and both dim5 and dim6are encoding object color. Therefore, the MIG is very low since it penalizes splitting one factor tomultiple dimensions. On the other hand, the MIG-sup and factor-metric is not too bad since onedimension mainly encodes one factor, even though there are some entanglement of color-vs-shapeand color-vs-scale.
Figure 9: An example of one dimension containing multiple factors. Each row is a traverse for onedimension (dimension order adjusted for better visualization). Notice that both models achieve highand similar MIG because all 6 factors are encoded and no splitting to multiple dimensions. However,the right-hand side model has much lower MIG-sup and factor-metric than the left-hand side model.
Figure 10: MNIST traversing results following the same generation strategy and network hierarchyas those presented in Figure 5 of (Zhao et al. (2017)). The network has 3 layers and 2 dimensionallatent code at each layer. Each image is generated by traversing each of the two-dimensional latentcode in one layer, while randomly sampling from the other layers. From left to right: The top layerz3 encodes the digit identity and tilt; z2 encodes digit width (digits around top-left are thicker thandigits around bottom-right); and the bottom layer z1 encodes stroke width. Compared to VLAE,the representation learnt in the presented method suggests smoother traversing on digits and similarresults for digit width and stroke width.
