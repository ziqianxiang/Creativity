Figure 1: Schematic diagram of the sample-wise parameter gradient distribution corresponding togreater (Left) and smaller (Right) GSNR. Pink arrows denote the gradient vectors for each samplewhile the blue arrow indicates their mean.
Figure 2: Schematic diagram of the training behavior satisfies OSGR(t) = 0 (Left), 0 <OSGR(t) < 1 (Middle) and OSGR(t) ≈ 1 (Right). Note that the Middle scenario most com-monly happens in regular tasks.
Figure 3: Left hand (LHS or OSGR by definition) and right side (RHS or OSGR as a function ofGSNR) of eq. (19). Points are drawn under different experiment settings. Left: LHS vs RHS atepoch 20, 100, 500, 2500. Each point is drawn by LHS and RHS computed at the given epoch underdifferent model structure (number of channels) or training data size; red dotted line is the line ofbest fit computed by least squares; blue dotted line is the line of reference representing LHS = RHS;the value of c in each title represents the Pearson correlation coefficient between LHS and RHScomputed by points in figure. Right: The legend. Different symbols and colors stand for differentnumber of channels and training data size. Different random noise levels are not distinguished.
Figure 4: (a): GSNR curves generated by a simple netWork based on real and random data. Anobvious upWard process in the early training stage Was observed for real data only. (b): Same plotfor ResNet18. (c): Average of psame_sign for the same model as in (a).
Figure 5: Average GSNR (a) and loss (b) curves for the frozen and non-frozen case. (c): GSNRcurves of individual parameters for the non-frozen case.
Figure 6: MNIST experiments. Left: Correlation between ∆gD(l),s,cand Ws(l, )c . Right : Ratio of weights that have opposite signs withtheir gradient mean.
Figure 7: Left hand (LHS) and right side (RHS) of eq. (19). Points are drawn under differentexperiment settings. Left figure: LHS vs RHS relation at epoch 20, 100, 500, 1000.
Figure 8: Similar with Fig. 3, but for a toy regression model discussed in in Appendix A.3.
