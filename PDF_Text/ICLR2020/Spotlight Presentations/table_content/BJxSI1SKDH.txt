Table 1: Above: Machine translation accuracy in Arabic (AR), Czech (CS) and Turkish (TR) interms of BLEU and ChrF3 metrics as well as BLEU scores computed on the output sentences taggedwith the morphological analyzer (t-BLEU) using in-domain training data. Below: The performanceof models trained with multi-domain data. Best scores are in bold. All improvements over thebaselines are statistically significant (p-value < 0.05).
Table 2: Percentage of out-of-vocabulary (OOV) words in the output, normalized perplexity mea-sures (PPl) per characters and the KL divergence between the reference and outputs of systemstrained with in-domain data on different language directions.
Table 3: Above: Outputs of LMM based on the lemma ‘git’ (‘go’) and different sets of inflectionalfeatures. Below: Examples of predicting inflections in context with or without using features.
Table 4: Training sets based on the TED Talks corpora (M: Million, K: Thousand).
Table 5: The multi-domain training set (M: Million, K : Thousand).
Table 6: Development and testing sets (K : Thousand).
