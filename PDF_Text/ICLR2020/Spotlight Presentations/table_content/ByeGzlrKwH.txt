Table 1: Comparison of each generalization error to our bound. RF is the Frobenius norm of theweight matrix, R2 is the operator norm of the weight matrix, Rp→q is the (p, q) matrix norm, L is thedepth, m is the maximum of the width, n is the sample size. Rn and Rr represent the Rademachercomplexity and local Rademacher complexity respectively. κ is a Lipschitz constant between layers.
Table 2: Notation listnotation	definitionn Zi = (xi, yi) Dn = (Zi)n=1 ψ(y,f (χ)) M Bx k∙kn INE ^ Ψ(f) Ψ(f) (ei)n=1 Rn(F0) Rn(F0) Rr (F0) r r* r L W⑶ ^ ς (') R2 RF m = (mi,. .. ,mL) m] = (mi, m2,. .., m£) S = (s1, . . . , sL) F =NN(m, R2,Rf) ^ G ^ ^ f ∈F 一 ^ b ∈g Λ (`) % ɑ β		sample size i-th observation (x^ input, yi. output) training data loss function L∞ -norm bound of models norm bound of input empirical L2-norm (∣f ∣n := PPn=I f (zi)2∕n) population L2-norm (∣f "2 := √Ez〜Pf(Z)2]) training error (empirical risk) generalization error (expected risk) Rademacher random variable conditional Rademacher complexity Rademacher complexity local Rademacher complexity 1	1 ell*	^ll upper bound of ∣f - g∣n fixed point of the local Rademacher complexity (Eq. (2)) √2(r2 + r*) depth of networks weight matrix of the '-th layer covariance matrix of the '-th layer operator norm bound of W(') Frobenius norm bound of W (') list of widths of networks list of widths of compressed networks list of ranks of the weight matrices of compressed networks the whole set of networks with width m set of trained networks set of compressed networks trained network compressed network an upper bound of the j-th largest eigenvalue of the covariance ma- trix in the '-th layer of f decreasing rate of the eigenvalues of W(') decreasing rate of the eigenValUeS of Σ(')	EI. . .	.	∏	.1	. .1	■ ■	1 T 1∙ .	1	G J ^ ■	1	777 UG ^ll ，入 9Theorem 5. Suppose that the empirical L?-distance between f and g is bounded by ∣∣f 一 g∣∣n ≤ r2for a fixed r > 0 almost surely. Let r :=，2(r2 + r2), then, under Assumptions 1, 2, 3, there existsa universal constant C > 0 such that,^	ʌ , ^ — ,Ψ(f) ≤ Ψ(f) + 2Rn(G) +t 1 + tMφ(r)+rV n +with probability at least 1 一 3e-t for all t ≥ 1.
Table 3: The effective ranks rm` and s` in each layer for each threshold ν. “In/Out” indicates thechannel sizes of the input and output.
Table 4: The intrinsic dimensionality in each layer for each threshold ν . Here again, “In/Out”indicates the channel sizes of the input and output. “Orig” indicates the number of parameters(m'+ιm' X filter size) in each layer.
Table 5: Comparison of the intrinsic dimensionality of our analysis and that in Arora et al. (2018).
