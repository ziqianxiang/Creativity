Table 1: Comparison on CIFAR-10/100 over 10 modern CNN architectures. Models are trained for1, 20, 100 epochs using batch normalization (BN) and network deconvolution (ND). Every singlemodel shows improved accuracy using network deconvolution.
Table 2: Comparison of accuracies of deconvolution with the model zoo implementation of VGG-11,ResNet-18, DenseNet-121 on ImageNet with batch normalization using the reference implementationon PyTorch. For VGG-11, we also include the performance of the original network without batchnormalization.
Table 3: Breakdown of deconvolution layer component computation time (in sec., measured on CPU)against various layer parameters. The batch size is set to 128, H Ã— W are layer dimensions, Chin- number of input channels, Chout - number of output channels, groups - the number of channelgroups, k is the kernel size and stride is the sampling stride.
Table 4: Performance and settings under different batch sizes.
