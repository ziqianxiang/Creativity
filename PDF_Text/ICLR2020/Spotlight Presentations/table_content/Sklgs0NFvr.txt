Table 1: Percentage of inter-editor agreement for counterfactually-revised movie reviewsType	Number of tokens							Full	0-50 51-100		101-150 151-200		201-250	251-300	301-329	Replacement	35.6	25.7	20.0	17.2	15.0	14.8	11.6	19.3Insertion	27.7	20.8	14.4	12.2	11.0	11.5	07.6	14.3Combined	41.6	32.7	26.3	23.4	21.6	20.3	16.2	25.5Sentiment Analysis The original IMDb dataset consists of 50k reviews divided equally acrosstrain and test splits. To keep the task of editing from growing unwieldy, we filter out the longest 20%of reviews, leaving 20k reviews in the train split from which we randomly sample 2.5k reviews,enforcing a 50:50 class balance. Following revision by the crowd workers, we partition this datasetinto train/validation/test splits containing 1707, 245 and 488 examples, respectively. We presenteach review to two workers, instructing them to revise the review such that (a) the counterfactuallabel applies; (b) the document remains coherent; and (c) no unecessary modifications are made.
Table 2: Most prominent categories of edits performed by humans for sentiment analysis (Origi-nal/Revised, in order). Red spans were replaced by Blue spans.
Table 3: Analysis of edits performed by humans for NLI hypotheses. P denotes Premise, OH denotesOriginal Hypothesis, and NH denotes New Hypothesis.
Table 4: Analysis of edits performed by humans for NLI premises. OP denotes Original Premise,NP denotes New Premise, and H denotes Hypothesis.
Table 5: Accuracy of various models for sentiment analysis trained with various datasets. Orig.
Table 6: Accuracy of various sentiment analysis models on out-of-domain dataTraining data	SVM NB	ELMo	Bi-LSTM	BERTAccuracy on Amazon Reviews				Orig. & Rev. (3.4k)	77.1	82.6	78.4	82.7	85.1Orig. (3.4k)	74.7	66.9	79.1	65.9	80.0Accuracy on Semeval 2017 (Twitter)Orig. & Rev. (3.4k)	66.5	73.9	70.0	68.7	82.9Orig. (3.4k)	61.2	64.6	69.5	55.3	79.3Accuracy on Yelp Reviews					Orig. & Rev. (3.4k)	87.6	89.6	87.2	86.2	89.4Orig. (3.4k)	81.8	77.5	82.0	78.0	85.3Table 7: Accuracy of BERT on NLI with various train and eval sets.
Table 7: Accuracy of BERT on NLI with various train and eval sets.
Table 8: Accuracy of Bi-LSTM classifier trained on hypotheses onlyTrain/Test	Original	RP	RH	RP & RHMajority class	34.7	34.6	34.6	34.6RP & RH (6.6k)	32.4	35.1	33.4	34.2Original w/ RP & RH (8.3k)	44.0	25.8	43.2	34.5Original (8.3k)	60.2	20.5	46.6	33.6Original (500k)	69.0	15.4	53.2	34.3Table 9: Accuracy of models trained to differentiate between original and revised dataModel	IMDb	SNLI/RP	SNLI/RHMajority class	50.0	66.7	66.7SVM	67.4	46.6	51.0NB	69.2	66.7	66.6BERT	77.3	64.8	69.7numbers even further. We compare this with the performance obtained by fine-tuning it on 8.3ksentence pairs sampled from SNLI training set, and show that while the two perform roughly within4 pts of each other when evaluated on SNLI, the former outperforms latter on both RP and RH.
Table 9: Accuracy of models trained to differentiate between original and revised dataModel	IMDb	SNLI/RP	SNLI/RHMajority class	50.0	66.7	66.7SVM	67.4	46.6	51.0NB	69.2	66.7	66.6BERT	77.3	64.8	69.7numbers even further. We compare this with the performance obtained by fine-tuning it on 8.3ksentence pairs sampled from SNLI training set, and show that while the two perform roughly within4 pts of each other when evaluated on SNLI, the former outperforms latter on both RP and RH.
Table 10: Most frequent insertions/deletions by human annotators for sentiment analysis.
Table 11: Most frequent insertions/deletions by human annotators for SNLI.
