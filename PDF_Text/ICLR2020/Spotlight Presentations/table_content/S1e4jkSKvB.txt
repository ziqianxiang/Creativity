Table 1: Quantities of InterestGeneralization Error (GE)	LD (fθ)-Ls (fθ)Product of Frobenius Norms (PFN)	∏ikθFkFrProduct of Spectral Norms (PSN)	∏BE：Distance to Initialization (DtI)	Pm0 -FMNumber of Parameters (NoP)	Total number of parameters in the networkSum of Spectral Norms (SoSP)	Totalnumberofparameters X(Piilθ0 - θ∕ ∣∣2)PAC Bayes (at error threshold 0.1)	Pikθ0 -θFkFr∕σ2Network Criticality Measure (at error threshold 0.1)	Pi α2kθ0 - θFkF"σ2First, as a sanity check we use our complexity measure (lower is better) to compare between aResNet18 trained on true labels and a ResNet18 trained on data where 20% of the labels are ran-domly corrupted. As seen in Figure 5a, our measure is able to correctly capture that the networktrained with true label generalizes better than the one trained on corrupted labels (4.62% error vs.
Table 2: Measuring complexity of different architectures trained on CIFAR10.
Table 3: Measuring complexity of different architectures trained on CIFAR100.
Table 4: Measuring complexity of different architectures trained on CIFAR10. The ResNet18 ar-chitectures have different channel widths. The network ResNet18 (1x width) has 16,16,32,64,128channels in the five stages.
Table 5: Measuring complexity of different architectures trained on CIFAR100. The ResNet18architectures have different channel widths. The network ResNet18 (1x width) has 16,16,32,64,128channels in the five stages.
