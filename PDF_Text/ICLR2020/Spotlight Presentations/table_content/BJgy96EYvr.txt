Table 1: Baseline algorithms. The third column is the reward used to train the value func-tion of PPO. ui and ucen are curiosity about individual state si and global state s, T1 =log (p(s0-i|s,a)/p(s0-i|s-i,a-i)), T2 = 1 - p(s0-i|s-i, a-i)/p(s0-i|s, a), and ∆Q-i(s, a) =Q-i(s, a) - Q-i(s-i, a-i). Social influence (Jaques et al., 2018) and COMA (Foerster et al., 2018)are augmented with curiosity.
Table 2: The scaling weights for different intrinsic reward terms in various tasks. βT is the weightof term Ti (See Table 1). 3血 and βeχt are scaling factors to combine r and Ui in r. u— in r.influenceis scaled by βr while Vint and Vext in plusV are respectively scaled by βpUsV and βpXusV.
