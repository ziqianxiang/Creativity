Table 1: Mean performance of our methods and baselines on Hanabi. We take the final models of 13 independentruns, i.e. 13 models per algorithm per player setting. Each model is evaluated on 100K games. Mean and s.e.mover the mean scores of the 13 models are shown in the table. The second row of each section is the win rate.
Table 2: Comparison between the previous SOTA learning methods and ours. We take the best model of 13runs for each of our methods and baselines. Each model is evaluated on 100K games with different seeds. Meanand s.e.m over the 100K games are shown in the table. The s.e.m. is less than 0.01 for most models. Boldnumbers are the best results achieved with learning algorithms. The second row of each section is the win rate.
