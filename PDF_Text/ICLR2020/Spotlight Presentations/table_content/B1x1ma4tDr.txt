Table 1: Resynthesis accuracies. Comparison of DDSP models to SOTA WaveRNN model providedthe same conditioning information. The supervised DDSP Autoencoder and WaveRNN modelsuse the fundamental frequency from a pretrained CREPE model, while the unsupervised DDSPautoencoder learns to infer the frequency from the audio during training.
Table 2: Model architecture for the f(t) encoder using a Resnet on log mel spectrograms. Spectro-grams have a frame size of 2048 and a hop size of 512, and are upsampled at the end to have thesame time resoultion as other latents (4ms per a frame). All convolutions use “same” padding anda temporal stride of 1. Each residual block uses a bottleneck structure (He et al., 2016). The finaloutput is a normalized probablity distribution over 128 frequency values (logarithmically scaled be-tween 8.2Hz and 13.3kHz (https://www.inspiredacoustics.com/en/MIDI_note_numbers_and_center_frequencies)). The finally frequency value is the weighted sum ofeach frequency by its probability.
Table 3: Parameter counts for different models. All models trained on NSynth dataset except forthose marked (Solo Violin). Autoregressive models have the most parameters with GANs requiringless. The DDSP models examined in this paper (which have not been optimized at all for size)require 2 to 3 times less parameters than GANSynth. The unsupervised model has more parametersbecause of the CREPE (small) f(t) encoder, and the autoencoder has additional parameters for thez(t) encoder. Initial experiments with extremely small models (single GRU, 256 units), have slightlyless realistic outputs, but still relatively high quality (as can be heard in the supplemental audio).
Table 4: Loudness and F0 metrics for different interpolation tasks.
