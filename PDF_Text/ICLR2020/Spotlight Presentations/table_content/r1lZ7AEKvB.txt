Table 1: Results on synthetic data for nodes labeled by classifier α(x) := Red(x) ∧ ∃y Blue(y)	α1 Train	α1 Test		α2 Train	α2 Test		α3 Train	α3 Test			same-size	bigger		same-size	bigger		same-size	biggerAC	0.839	0.826	0.671	0.694	0.695	0.667	0.657	0.636	0.632GIN	0.567	0.566	0.536	0.689	0.693	0.672	0.656	0.643	0.580AC-FR-2	1.000	1.000	1.000	0.863	0.860	0.694	0.788	0.775	0.770AC-FR-3	1.000	1.000	0.825	0.840	0.823	0.604	0.787	0.767	0.771ACR-1	1.000	1.000	1.000	0.827	0.834	0.726	0.760	0.762	0.773ACR-2	1.000	1.000	1.000	0.895	0.897	0.770	0.800	0.799	0.771ACR-3	1.000	1.000	1.000	0.903	0.902	0.836	0.817	0.802	0.748Table 2: Results on E-R synthetic data for nodes labeled by classifiers αi(x) in Equation (6)number of layers. This combined with the fact that random graphs that are more dense make themaximum distances between nodes shorter, may explain the boost in performance for AC-GNNs.
Table 2: Results on E-R synthetic data for nodes labeled by classifiers αi(x) in Equation (6)number of layers. This combined with the fact that random graphs that are more dense make themaximum distances between nodes shorter, may explain the boost in performance for AC-GNNs.
Table 3: Synthetic data for the experiment with classifier α(x) := Red(x) ∧ ∃y Blue(y)	Erdos-Renyi + 20%			Erdos-Renyi + 50%			Erdos-Renyi + 100%			Train Acc.	Test Acc.		Train Acc.	Test Acc.		Train Acc.	Test Acc.			same-size	bigger		same-size	bigger		same-size	biggerAC-2	0.810	0.807	0.778	0.829	0.835	0.791	0.861	0.864	0.817AC-5	0.940	0.937	0.901	0.975	0.971	0.958	0.994	0.994	0.993AC-7	0.963	0.961	0.946	0.983	0.978	0.981	0.995	0.995	0.995GIN-2	0.797	0.795	0.771	0.813	0.818	0.784	0.838	0.840	0.803GIN-5	0.838	0.836	0.819	0.846	0.847	0.833	0.841	0.844	0.838GIN-7	0.838	0.840	0.803	0.841	0.844	0.838	0.784	0.788	0.773ACR-1	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000Table 4: Detailed results for Erdos-Renyi synthetic graphs with different connectivitiesErdos-Renyi graphs These are random graphs in which one specifies the number N of nodes andthe number M of edges. For this experiment we consider as extreme cases the case in which graphscontain the same number of nodes and edges and graphs in which the number of edges is twice thenumber of nodes.
Table 4: Detailed results for Erdos-Renyi synthetic graphs with different connectivitiesErdos-Renyi graphs These are random graphs in which one specifies the number N of nodes andthe number M of edges. For this experiment we consider as extreme cases the case in which graphscontain the same number of nodes and edges and graphs in which the number of edges is twice thenumber of nodes.
Table 5: Synthetic data for the experiment with classifier αi (x) in Equation (6)	F1 TestAC-2 AC-3 AC-4	97.2 ± 0.3 97.5 ± 0.3 97.5 ± 0.2ACR-2 ACR-3 ACR-4	93.5 ± 0.3 94.2 ± 1.2 95.4 ± 0.9Table 6: Performance of AC-GNN and ACR-GNN in the PPI benchmarkPPI ExperimentsWe consider the standard train/validation/test split for this benchmarck (Fey & Lenssen, 2019). Weuse a hidden size of 256 and the Adam optimizer for 500 epochs with early stopping when thevalidation set did not improve for 20 epochs. We did not do any hyperparameter search besideschanging the aggregation, combination, and readout functions. As opposed to the synthetic case,in this case we observed a better performance when the average or the max functions are used foraggregation. Table 6 shows the best results for different layers (average of 10 runs). As we can see,ACR-GNNs do not imply an improvement over AC-GNNs for this benchmark.
Table 6: Performance of AC-GNN and ACR-GNN in the PPI benchmarkPPI ExperimentsWe consider the standard train/validation/test split for this benchmarck (Fey & Lenssen, 2019). Weuse a hidden size of 256 and the Adam optimizer for 500 epochs with early stopping when thevalidation set did not improve for 20 epochs. We did not do any hyperparameter search besideschanging the aggregation, combination, and readout functions. As opposed to the synthetic case,in this case we observed a better performance when the average or the max functions are used foraggregation. Table 6 shows the best results for different layers (average of 10 runs). As we can see,ACR-GNNs do not imply an improvement over AC-GNNs for this benchmark.
