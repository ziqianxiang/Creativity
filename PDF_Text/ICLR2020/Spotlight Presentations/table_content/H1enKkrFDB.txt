Table 1: Highly non-generalizable setting. Training error for ResNet-110 on CIFAR100 withrandomized labels, low lr= 0.01, and with and without weight decay. (Higher is better.) The cleantest accuracy for this setting is shown in Appendix D.1.
Table 2: Inception and FID score on CIFAR10.
Table 3:	CIFAR100 experiments.
Table 4:	Neural DiscriminatorLoss (Higher the better).
Table 5: Values of 90 percentile of log complexity measures from Figure 4. Here ∞ refers to thesituations where the product of spectral norm blows up. This is the case in deep networks likeResNet-110 and Densenet-100 where the absence of spectral normalization (Vanilla) allows theproduct of spectral norm to grow arbitrarily large with increasing number of layers. Lower is better.
Table 6: Training Error for WideResNet-28-10 on CIFAR100 with randomized labels, low lr= 0.01,and with weight decay. (Higher is better.)With and without weight decay In Figure 7a, we show the training error of Alexnet trained withSGD with and without weight decay (= 5e - 4) with a learning rate of 0.01. Again, we see that a22Published as a conference paper at ICLR 2020JojJ 山 UoQEZ=E jəuəo(a) Generalization ErrorFigure 6: Test Error and Generalization Error of AlexNet trained with SGD with lr = 0.1 on (clean)CIFAR-100. (Lower is betterw∕o WDWD(b) Test Errormore aggressive stable rank constraint decreases fitting the random data . Similar results are seen forResNet-110 in Figure 7b.
Table 7: Clean Test Accuracy on CIFAR10. The learning configuration corresponds to the non-generelizable settings with high learning rate. The corresponding shattering experiments for thissetting are shown in Table 1.
