Table 1: Comparison of LISTA and its variants (with and without gates) under different noise levels.
Table 2: Mean (± standard derivation) error in degree with different number of observations over fiveruns.	____________________________________________q	LS	L1	LISTA	GLISTA35	5.37	1.39	0.0237±0.0026	0.00210±0.0004425	5.60	2.03	0.0429±0.0068	0.00524±0.0002415	6.09	4.25	0.371±0.046	0.0255±0.00545 ConclusionIn this paper, we study LISTA for solving sparse coding problems. We discover its potential weak-nesses and introduce gated mechanisms to address them accordingly. In particular, we theoreticallyprove that LISTA with gain gates can achieve faster convergence than the standard LISTA. We alsodiscover that LISTA (with or without gates) can obtain lower reconstruction errors under a weakerassumption of “false positive” in its code estimations. It helps us improve the convergence analyses toachieve more solid theoretical results, which have been perfectly confirmed in simulation experiments.
Table 3: Comparison of the final NMSEs under different noise levels with d = 16. The conditionnumber of the dictionary is not specifically constrained.
Table 4: Comparison of the final NMSEs under different condition numbers with d = 16. The noiselevel is Chosen as SNR=40dB for all the tested Condition numbers._____________Con. num.	LISTA	LAMP	LISTA-S	LISTA-C-S	ALISTA-S	GLISTA (ours)3	-39.03±0.54	-37.26±0.13	-43.12±0.06	-44.90±0.03	-43.88±0.26	-45.33±0.0430	-29.65±0.89	-28.44±0.31	-32.30±0.17	-38.36±0.57	-31.50±0.15	-39.61±0.64100	-21.39±0.75	-22.23±0.18	-27.08±0.57	-27.94±0.34	-27.10±0.02	-34.07±0.648	Progres sive Training and Adaptive OvershootOur training mostly follows it of Chen et al.’s (2018), and some key steps are listed here: 1) Themodel is trained progressively to include more layers during the training phase. At the very beginning,only learnable parameters in the first layer is considered, and parameters in the second layer is onlyincluded once training on the first update converges, so as the third and higher layers. 2) Training afterincluding the t-th layer is split into three stages, with an initial learning rate of 0.0005 to optimizeits own learnable parameters first, and learning rates of 0.0001 and 0.00001 to jointly optimize alllearnable parameters from the 0-th to t-th layers in the second and third stages, respectively. Wemove to the next stage once no performance gain is observed on the validation set for 4000 iterations.
