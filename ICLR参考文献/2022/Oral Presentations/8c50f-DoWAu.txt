Published as a conference paper at ICLR 2022
Diffusion-Based Voice Conversion with Fast
Maximum Likelihood Sampling Scheme
Vadim Popov, Ivan Vovk, Vladimir Gogoryan,
Huawei Noah’s Ark Lab, Moscow, Russia
{vadim.popov,vovk.ivan,gogoryan.vladimir}@huawei.com
Tasnima Sadekova, Mikhail Kudinov & Jiansheng Wei
Huawei Noah’s Ark Lab, Moscow, Russia
{sadekova.tasnima,kudinov.mikhail,weijiansheng}@huawei.com
Ab stract
Voice conversion is a common speech synthesis task which can be solved in differ-
ent ways depending on a particular real-world scenario. The most challenging one
often referred to as one-shot many-to-many voice conversion consists in copying
target voice from only one reference utterance in the most general case when
both source and target speakers do not belong to the training dataset. We present
a scalable high-quality solution based on diffusion probabilistic modeling and
demonstrate its superior quality compared to state-of-the-art one-shot voice con-
version approaches. Moreover, focusing on real-time applications, we investigate
general principles which can make diffusion models faster while keeping synthesis
quality at a high level. As a result, we develop a novel Stochastic Differential
Equations solver suitable for various diffusion model types and generative tasks as
shown through empirical studies and justify it by theoretical analysis.
1	Introduction
Voice conversion (VC) is the task of copying the target speaker’s voice while preserving the linguistic
content of the utterance pronounced by the source speaker. Practical VC applications often require a
model which is able to operate in one-shot mode (i.e. when only one reference utterance is provided
to copy the target speaker’s voice) for any source and target speakers. Such models are usually
referred to as one-shot many-to-many models (or sometimes zero-shot many-to-many models, or just
any-to-any VC models). It is challenging to build such a model since it should be able to adapt to a
new unseen voice having only one spoken utterance pronounced with it, so it was not until recently
that successful one-shot VC solutions started to appear.
Conventional one-shot VC models are designed as autoencoders whose latent space ideally contains
only the linguistic content of the encoded utterance while target voice identity information (usually
taking shape of speaker embedding) is fed to the decoder as conditioning. Whereas in the pioneering
AutoVC model (Qian et al., 2019) only speaker embedding from the pre-trained speaker verification
network was used as conditioning, several other models improved on AutoVC enriching conditioning
with phonetic features such as pitch and loudness (Qian et al., 2020; Nercessian, 2020), or training
voice conversion and speaker embedding networks jointly (Chou & Lee, 2019). Also, several papers
(Lin et al., 2021; Ishihara & Saito, 2020; Liu et al., 2021b) made use of attention mechanism to
better fuse specific features of the reference utterance into the source utterance thus improving the
decoder performance. Apart from providing the decoder with sufficiently rich information, one of the
main problems autoencoder VC models face is to disentangle source speaker identity from speech
content in the encoder. Some models (Qian et al., 2019; 2020; Nercessian, 2020) solve this problem
by introducing an information bottleneck. Among other popular solutions of the disentanglement
problem one can mention applying vector quantization technique to the content information (Wu
et al., 2020; Wang et al., 2021), utilizing features of Variational AutoEncoders (Luong & Tran, 2021;
Saito et al., 2018; Chou & Lee, 2019), introducing instance normalization layers (Chou & Lee, 2019;
Chen et al., 2021b), and using Phonetic Posteriorgrams (PPGs) (Nercessian, 2020; Liu et al., 2021b).
1
Published as a conference paper at ICLR 2022
The model we propose in this paper solves the disentanglement problem by employing the encoder
predicting “average voice”: it is trained to transform mel features corresponding to each phoneme
into mel features corresponding to this phoneme averaged across a large multi-speaker dataset. As for
decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this
class of generative models has shown very good results in speech-related tasks like raw waveform
generation (Chen et al., 2021a; Kong et al., 2021) and mel feature generation (Popov et al., 2021;
Jeong et al., 2021). However, this decoder choice poses a problem of slow inference because DPM
forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for
hundreds of iterations (Ho et al., 2020; Nichol & Dhariwal, 2021). Addressing this issue, we develop
a novel inference scheme that significantly reduces the number of iterations sufficient to produce
samples of decent quality and does not require model re-training. Although several attempts have
been recently made to reduce the number of DPM inference steps (Song et al., 2021a; San-Roman
et al., 2021; Watson et al., 2021; Kong & Ping, 2021; Chen et al., 2021a), most of them apply to some
particular types of DPMs. In contrast, our approach generalizes to all popular kinds of DPMs and has
a strong connection with likelihood maximization.
This paper has the following structure: in Section 2 we present a one-shot many-to-many VC model
and describe DPM it relies on; Section 3 introduces a novel DPM sampling scheme and establishes
its connection with likelihood maximization; the experiments regarding voice conversion task as well
as those demonstrating the benefits of the proposed sampling scheme are described in Section 4; we
conclude in Section 5.
2	Voice conversion diffusion model
As with many other VC models, the one we propose belongs to the family of autoencoders. In fact,
any conditional DPM with data-dependent prior (i.e. terminal distribution of forward diffusion)
can be seen as such: forward diffusion gradually adding Gaussian noise to data can be regarded as
encoder while reverse diffusion trying to remove this noise acts as a decoder. DPMs are trained to
minimize the distance (expressed in different terms for different model types) between the trajectories
of forward and reverse diffusion processes thus, speaking from the perspective of autoencoders,
minimizing reconstruction error. Data-dependent priors have been proposed by Popov et al. (2021)
and Lee et al. (2021), and we follow the former paper due to the flexibility of the continuous DPM
framework used there. Our approach is summarized in Figure 1.
Decoder = ML SDE Solver
dXt= Q(X - Xt) - MΛ,X,Sf(n.θ) βf,dt + √¾dl¾
Starting from £ Z j√(χ. 1)
Computed at each solver iteration
一瓦个(丫)研---------------
[U-“et J
» concat 卜
Figure 1: VC model training and inference. Y stands for the training mel-spectrogram at training
and the target mel-spectrogram at inference. Speaker conditioning in the decoder is enabled by the
speaker conditioning network gt(Y ) where Y = {Yt}t∈[0,1] is the whole forward diffusion trajectory
starting at Y0 . Dotted arrows denote operations performed only at training.
X - stop gradient
∙>j^MSE Loss^
Vlogptlυ(χt∣χ0)∣
个
Forward SDE Solution
J dXt — WBt(又—Xt)dt + -∖∕βtd^t
∖ Starting from X0, t Z U(O, i)
2
Published as a conference paper at ICLR 2022
2.1	Encoder
We choose average phoneme-level mel features as speaker-independent speech representation. To
train the encoder to convert input mel-spectrograms into those of “average voice”, we take three steps:
(i) first, we apply Montreal Forced Aligner (McAuliffe et al., 2017) to a large-scale multi-speaker
LibriTTS dataset (Zen et al., 2019) to align speech frames with phonemes; (ii) next, we obtain
average mel features for each particular phoneme by aggregating its mel features across the whole
LibriTTS dataset; (iii) the encoder is then trained to minimize mean square error between output
mel-spectrograms and ground truth “average voice” mel-spectrograms (i.e. input mel-spectrograms
where each phoneme mel feature is replaced with the average one calculated on the previous step).
The encoder has exactly the same Transformer-based architecture used in Grad-TTS (Popov et al.,
2021) except that its inputs are mel features rather than character or phoneme embeddings. Note that
unlike Grad-TTS the encoder is trained separately from the decoder described in the next section.
2.2	Decoder
Whereas the encoder parameterizes the terminal distribution of the forward diffusion (i.e. the prior),
the reverse diffusion is parameterized with the decoder. Following Song et al. (2021c) We use Ito
calculus and define diffusions in terms of stochastic processes rather than discrete-time Markov
chains.
The general DPM framework we utilize consists of forward and reverse diffusions given by the
following Stochastic Differential Equations (SDEs):
dXt = 2βt(X — Xt)dt + PetdWt ,
(1)
dXt = (1(X - Xt)-Sθ(Xt,X,t)) βtdt + √βtdt- ,	(2)
where t ∈ [0, 1],W andW are two independent Wiener processes in Rn, βt is non-negative function
referred to as noise schedule, sθ is the score function with parameters θ and X is n-dimensional
vector. It can be shown (Popov et al., 2021) that the forward SDE (1) allows for explicit solution:
LaW(Xt∣Xo) = N (e-1 Rt β"Xo + (1 - e- 1 Rt ^ds) X, (1 - e- Rt 仇加卜),	(3)
where I is n × n identity matrix. Thus, if noise follows linear schedule βt = β0 + t(β1 - β0) for β0
and βι such that e- ʃθ βsds is close to zero, then Law (Xi) is close to N(X, I) which is the prior in
this DPM. The reverse diffusion (2) is trained by minimizing weighted L2 loss:
θ*
arg min L(θ)
θ
arg min
θ
Z1
0
λtEXo,Xt ksθ(Xt,X,t) - VlogPt∣0(XtIXO)k2dt,
(4)
where pt|0(Xt|X0) is the probability density function (pdf) of the conditional distribution (3) and
λt = 1 - e- Rot βsds. The distribution (3) is Gaussian, so we have
V logpt|0(Xt|X0)
Xt — Xoe-2 RO βsds — X(1 — e-2 RO 仇曲)
1 - e- RO βsds
(5)
At training, time variable t is sampled uniformly from [0, 1], noisy samples Xt are generated according
to the formula (3) and the formula (5) is used to calculate loss function L on these samples. Note
that Xt can be sampled without the necessity to calculate intermediate values {Xs}0<s<t which
makes optimization task (4) time and memory efficient. A well-trained reverse diffusion (2) has
trajectories that are close to those of the forward diffusion (1), so generating data with this DPM can
be performed by sampling Xi from the prior N(X, I) and solving SDE (2) backwards in time.
3
Published as a conference paper at ICLR 2022
The described above DPM was introduced by Popov et al. (2021) for text-to-speech task and we
adapt it for our purposes. We PUt X =2(Xo) where 夕 is the encoder, i.e. X is the “average voice”
mel-spectrogram which we want to transform into that of the target voice. We condition the decoder
sθ = sθ(Xt, X, gt(Y ), t) on some trainable function gt(Y ) to provide it with information about the
target speaker (Y stands for forward trajectories of the target mel-spectrogram at inference and the
ones of the training mel-spectrogram at training). This function is a neural network trained jointly
with the decoder. We experimented with three input types for this network:
•	d-only - the input is the speaker embedding extracted from the target mel-sPectrogram Y0
with the pre-trained speaker verification network employed in (Jia et al., 2018);
•	wodyn - in addition, the noisy target mel-sPectrogram Yt is used as input;
•	whole - in addition, the whole dynamics of the target mel-speCtrogram under forward
diffusion {Ys|s = 0.5/15, 1.5/15, .., 14.5/15} is used as input.
The decoder architecture is based on U-Net (Ronneberger et al., 2015) and is the same as in Grad-
TTS but with four times more channels to better capture the whole range of human voices. The
speaker conditioning network gt(Y ) is composed of 2D convolutions and MLPs and described in
detail in Appendix H. Its output is 128-dimensional vector which is broadcast-concatenated to the
concatenation of Xt and X as additional 128 channels.
2.3	Related VC models
To the best of our knowledge, there exist two diffusion-based voice conversion models: VoiceGrad
(Kameoka et al., 2020) and DiffSVC (Liu et al., 2021a). The one we propose differs from them
in several important aspects. First, neither of the mentioned papers considers a one-shot many-to-
many voice conversion scenario. Next, these models take no less than 100 reverse diffusion steps at
inference while we pay special attention to reducing the number of iterations (see Section 3) achieving
good quality with as few as 6 iterations. Furthermore, VoiceGrad performs voice conversion by
running Langevin dynamics starting from the source mel-spectrogram, thus implicitly assuming that
forward diffusion trajectories starting from the mel-spectrogram we want to synthesize are likely
to pass through the neighborhood of the source mel-spectrogram on their way to Gaussian noise.
Such an assumption allowing to have only one network instead of encoder-decoder architecture is
too strong and hardly holds for real voices. Finally, DiffSVC performs singing voice conversion and
relies on PPGs as speaker-independent speech representation.
3	Maximum likelihood SDE solver
In this section, we develop a fixed-step first-order reverse SDE solver that maximizes the log-
likelihood of sample paths of the forward diffusion. This solver differs from general-purpose
Euler-Maruyama SDE solver (Kloeden & Platen, 1992) by infinitesimally small values which can
however become significant when we sample from diffusion model using a few iterations.
Consider the following forward and reverse SDEs defined in Euclidean space Rn for t ∈ [0, 1]:
dXt = — 2βtXtdt + PetdWt (F),	dXt
-2BtXt - βtsθ(Xt,") dt + PetdWt (R), (6)
where W is a forward Wiener process (i.e. its forward increments Wt -Ws are independent ofWs
for t > s) and W is a backward Wiener process (i.e. backward increments Ws - Wt are independent
of Wt for s < t). Following Song et al. (2021c) we will call DPM (6) Variance Preserving (VP).
For simplicity we will derive maximum likelihood solver for this particular type of diffusion models.
The equation (1) underlying VC diffusion model described in Section 2 can be transformed into the
equation (6-F) by a constant shift and we will call such diffusion models Mean Reverting Variance
Preserving (MR-VP). VP model analysis carried out in this section can be easily extended (see
Appendices D, E and F) to MR-VP model as well as to other common diffusion model types such as
sub-VP and VE described by Song et al. (2021c).
4
Published as a conference paper at ICLR 2022
The forward SDE (6-F) allows for explicit solution:
Law(Xt|Xs) = N (γs,tXs, (1 - γs2,t) I),
(7)
for all 0 ≤ s < t ≤ 1. This formula is derived by means of Ito calculus in Appendix A. The
reverse SDE (6-R) parameterized with a neural network sθ is trained to approximate gradient of the
log-density of noisy data Xt :
θ*
arg min	λtEXtksθ(Xt,t)
θ0
-VlogPt(Xt)k2dt,
(8)
where the expectation is taken with respect to noisy data distribution LaW(Xt) with pdf pt(∙) and λt
is some positive weighting function. Note that certain Lipschitz constraints should be satisfied by
coefficients of SDEs (6) to guarantee existence of strong solutions (Liptser & Shiryaev, 1978), and
throughout this section we assume these conditions are satisfied as well as those from (Anderson,
1982) which guarantee that paths X generated by the reverse SDE (6-R) for the optimal θ* equal
forward SDE (6-F) paths X in distribution.
The generative procedure of a VP DPM consists in solving the reverse SDE (6-R) backwards in
time starting from Xi 〜N(0, I). Common Euler-Maruyama solver introduces discretization error
(Kloeden & Platen, 1992) which may harm sample quality when the number of iterations is small. At
the same time, it is possible to design unbiased (Henry-LabOrdere et al., 2017) or even exact (Beskos
& Roberts, 2005) numerical solvers for some particular SDE types. The Theorem 1 shows that in the
case of diffusion models we can make use of the forward diffusion (6-F) and propose a reverse SDE
solver which is better than the general-purpose Euler-Maruyama one in terms of likelihood.
The solver proposed in the Theorem 1 is expressed in terms of the values defined as follows:
(9)
νt-h,t(I - γ2,t)	ι * _ μt-h,t — 11 + κt,h 1
-Y0tβth	,	ωt,h =	βth	+T-Y0[; - 2,
(σ*,h)2=σ2-h,t+n ν2-h,t EXt [Tr(Var(Xo|Xt))],
(10)
where n is data dimensionality, Var (Xo|Xt) is the covariance matrix of the conditional data distri-
bution LaW(Xo|Xt) (so, Tr(Var (Xo|Xt)) is the overall variance across all n dimensions) and the
expectation EXtH is taken with respect to the unconditional noisy data distribution Law(Xt).
Theorem 1. Consider a DPM characterized by SDEs (6) with reverse diffusion trained till optimality.
Let N ∈ N be any natural number and h = 1/N. Consider the following class of fixed step size h
reverse SDE solvers parameterized with triplets ofreal numbers {(^t,h, ω^t,h, σt,h)∣t = h, 2h,.., 1}:
Xt-h = Xt + βth
2 + ωt,h) Xt + (1 + κt,h)sθ* (Xt,tj + σt,hξt,
(11)
where θ* is given by (8), t = 1, 1 - h, .., h and ξt are i.i.d. samples from N(0, I). Then:
(i)	Log-likelihood ofsamplepaths X = {Xkh}N=° Under generative model X is maximized
**	*
for κt,h = κt,h, ωt,h = ωt,h and σt,h = σt,h.
(ii)	Assume that the SDE solver (11) startsfrom random variable Xi 〜 Law (Xi). If Xo is a
constant or a Gaussian random variable with diagonal isotropic covariance matrix (i.e. δ2 I
for δ > 0), then generative model X is exactfor Kt,h = κ* hʃ, ^^th = ω" and σt,h = σt h
5
Published as a conference paper at ICLR 2022
Table 1: Input types for speaker conditioning gt(Y ) compared in terms of speaker similarity.
	Dif-LibriTTS				Dif-VCTK			
	d-only	wodyn	whole	d-only	wodyn	whole
Most similar	27.0%	38.0%	34.1%	27.2%	46.7%	23.6%
Least similar	28.9%	29.3%	38.5%	25.3%	23.9%	48.6%
The Theorem 1 provides an improved DPM sampling scheme which comes at no additional compu-
tational cost compared to standard methods (except for data-dependent term in σ* as discussed in
Section 4.3) and requires neither model re-training nor extensive search on noise schedule space. The
proof of this theorem is given in Appendix C. Note that it establishes optimality of the reverse SDE
solver (11) with the parameters (10) in terms of likelihood of discrete paths X = {Xkh}kN=0 while
the optimality of continuous model (6-R) on continuous paths {Xt}t∈[0,1] is guaranteed for a model
with parameters θ = θ* as shown in (Song et al., 2021c).
The class of reverse SDE solvers considered in the Theorem 1 is rather broad: it is the class of all
fixed-step solvers whose increments at time t are linear combination of Xt, sθ(Xt, t) and Gaussian
noise with zero mean and diagonal isotropic covariance matrix. As a particular case it includes
Euler-Maruyama solver (^t,h ≡ 0, ω^t,h ≡ 0, σ^t,h ≡ √βth and for fixed t and h → 0 we have
Klh = o(1), ωth = 0(1) and σjh = √βth(1 + δ(1)) (the proof is given in Appendix B), so the
optimal SDE solver significantly differs from general-purpose Euler-Maruyama solver only when
N is rather small or t has the same order as h, i.e. on the final steps of DPM inference. Appendix
G contains toy examples demonstrating the difference of the proposed optimal SDE solver and
Euler-Maruyama one depending on step size.
The result (ii) from the Theorem 1 strengthens the result (i) for some particular data distributions, but
it may seem useless since in practice data distribution is far from being constant or Gaussian. However,
in case of generation with strong conditioning (e.g. mel-spectrogram inversion) the assumptions
on the data distribution may become viable: in the limiting case when our model is conditioned on
c = ψ(X0) for an injective function ψ, random variable X0|c becomes a constant ψ-1 (c).
4	Experiments
We trained two groups of models: Diff-VCTK models on VCTK (Yamagishi et al., 2019) dataset
containing 109 speakers (9 speakers were held out for testing purposes) and Diff-LibriTTS models on
LibriTTS (Zen et al., 2019) containing approximately 1100 speakers (10 speakers were held out). For
every model both encoder and decoder were trained on the same dataset. Training hyperparameters,
implementation and data processing details can be found in Appendix I. For mel-spectrogram
inversion, we used the pre-trained universal HiFi-GAN vocoder (Kong et al., 2020) operating at
22.05kHz. All subjective human evaluation was carried out on Amazon Mechanical Turk (AMT)
with Master assessors to ensure the reliability of the obtained Mean Opinion Scores (MOS). In all
AMT tests we considered unseen-to-unseen conversion with 25 unseen (for both Diff-VCTK and
Diff-LibriTTS) speakers: 9 VCTK speakers, 10 LibriTTS speakers and 6 internal speakers. For
VCTK source speakers we also ensured that source phrases were unseen during training. We place
other details of listening AMT tests in Appendix J. A small subset of speech samples used in them
is available at our demo page https://diffvc-fast-ml-solver.github.io which we
encourage to visit. The code will soon be published at https://github.com/huawei-noah/
Speech-Backbones.
As for sampling, we considered the following class of reverse SDE solvers:
Xt-h = Xt + βth ( (2 + ωt,h) (Xt- X) + (1 + κt,h)sθ(Xt,X,gt(Y),") + σt,hξt,	(12)
where t = 1,1 - h,..,h and ξt are i.i.d. samples from N(0, I). For ^t,h = κ1h ^^th = ω"
and σt,h = σt hι (where Kt h ωjh and σ" are given by (10)) it becomes maximum likelihood
reverse SDE solver for MR-VP DPM (1-2) as shown in Appendix D. In practice it is not trivial
to estimate variance of the conditional distribution Law (X0|Xt), so we skipped this term in σtt,h
6
Published as a conference paper at ICLR 2022
Table 2: Subjective evaluation (MOS) of one-shot VC models trained on VCTK. Ground truth
recordings were evaluated only for VCTK speakers.
	VCTK test (9 speakers, 54 pairs)		Whole test (25 speakers, 350 pairs)	
	Naturalness	Similarity	Naturalness	Similarity
AGAIN-VC	-1.98 ± 0.05-	1.97 ± 0.08	-1.87 ± 0.03-	-1.75 ± 0.04-
FragmentVC	-2.20 ± 0.06-	2.45 ± 0.09	-1.91 ± 0.03-	-1.93 ± 0.04-
VQMIVC	-2.89 ± 0.06-	2.60 ± 0.10	-2.48 ± 0.04-	-1.95 ± 0.04-
Df-VCTK-ML-6	3.73 ± 0.06	3.47 ± 0.09	-3.39 ± 0.04-	-2.69 ± 0.05-
Df-VCTK-ML-30	3.73 ± 0.06	3.57 ± 0.09	3.44 ± 0.04	-2.71 ± 0.05-
Ground truth	4.55 ± 0.0^5~~	4.52 ± 0.01	4.55 ± 0.05~~	4.52 ± 0.07~~
Table 3: Subjective evaluation (MOS) of one-shot VC models trained on large-scale datasets.
	VCTK test (9 speakers, 54 pairs)		Whole test (25 speakers, 350 pairs)	
	Naturalness	Similarity	Naturalness	Similarity
Dif-LibriTTS-EM-6	-1.68 ± 0.06-	1.53 ± 0.07	-1.57 ± 0.02-	-1.47 ± 0.03-
Diff-LibriTTS-PF-6	-3.11 ± 0.07-	2.58 ± 0.11	-2.99 ± 0.03-	-2.50 ± 0.04-
Df-LibriTTS-ML-6	-3.84 ± 0.08-	3.08 ± 0.11	-3.80 ± 0.03-	-3.27 ± 0.05-
Df-LibriTTS-ML-30	3.96 ± 0.08	3.23 ± 0.11	4.02 ± 0.03	-3.39 ± 0.05-
BNE-PPG-VC	3.95 ± 0.08~~	3.27 ± 0.12~	3.83 ± 0.03~~	3.03 ± 0.05~~
assuming this variance to be rather small because of strong conditioning on gt (Y ) and just used
σt,h = σt-h,t calling this sampling method ML-N (N = l/h is the number of SDE solver steps).
We also experimented with Euler-Maruyama solver EM-N (i.e. Kt,h = 0, ωt,h = 0, σt,h = √βth)
and “probability flow sampling” from (Song et al., 2021C) which We denote by PF-N (Kt,h = -0.5,
ωt,h = 0, σt,h = 0).
4.1	Speaker conditioning analysis
For each dataset we trained three models - one for each input type for the speaker conditioning
network gt(Y ) (see Section 2.2). Although these input types had much influence neither on speaker
similarity nor on speech naturalness, we did two experiments to choose the best models (one for
each training dataset) in terms of speaker similarity for further comparison with baseline systems.
We compared voice conversion results (produced by ML-30 sampling scheme) on 92 source-target
pairs. AMT workers were asked which of three models (if any) sounded most similar to the target
speaker and which of them (if any) sounded least similar. For Diff-VCTK and Diff-LibriTTS models
each conversion pair was evaluated 4 and 5 times respectively. Table 1 demonstrates that for both
Diff-VCTK and Diff-LibriTTS the best option is wodyn, i.e. to condition the decoder at time t on the
speaker embedding together with the noisy target mel-spectrogram Yt . Conditioning on Yt allows
making use of diffusion-specific information of how the noisy target sounds whereas embedding
from the pre-trained speaker verification network contains information only about the clean target.
Taking these results into consideration, we used Diff-VCTK-wodyn and Diff-LibriTTS-wodyn in the
remaining experiments.
4.2	Any-to-any voice conversion
We chose four recently proposed VC models capable of one-shot many-to-many synthesis as the
baselines:
•	AGAIN-VC (Chen et al., 2021b), an improved version of a conventional autoencoder AdaIN-
VC solving the disentanglement problem by means of instance normalization;
•	FragmentVC (Lin et al., 2021), an attention-based model relying on wav2vec 2.0 (Baevski
et al., 2020) to obtain speech content from the source utterance;
7
Published as a conference paper at ICLR 2022
Table 4: Reverse SDE solvers compared in terms of FID. N is the number of SDE solver steps.
		VP DPM		sub-VP DPM		VE DPM	
		N=10	N =100	N =10	N =100	N =10	N =100
Euler-Maruyama		229.6	19.68	312.3	19.83	462.1	24.77
Reverse Diffusion		679.8	65.95	312.2	19.74	461.1	303.2
ProbabilityFloW		88.92	5.70	64.22	4.42	495.3	214.5
Ancestral Sampling		679.8	68.35	—	—	454.7	17.83
Maximum Likelihood (T =	0.1)	260.3	4.34	317.0	6.63	461.9	23.63
Maximum Likelihood (τ =	r05T	24.45	7.82	30.90	6.43	462.0	10.07
Maximum Likelihood (τ =	7TPT	41.78	7.94	48.02	6.51	48.51	12.37
•	VQMIVC (Wang et al., 2021), state-of-the-art approach among those employing vector
quantization techniques;
•	BNE-PPG-VC (Liu et al., 2021b), an improved variant of PPG-based VC models combining
a bottleneck feature extractor obtained from a phoneme recognizer with a seq2seq-based
synthesis module.
As shown in (Kim et al., 2021), PPG-based VC models provide high voice conversion quality
competitive even with that of the state-of-the-art VC models taking text transcription corresponding
to the source utterance as input. Therefore, we can consider BNE-PPG-VC a state-of-the-art model in
our setting.
Baseline voice conversion results were produced by the pre-trained VC models provided in official
GitHub repositories. Since only BNE-PPG-VC has the model pre-trained on a large-scale dataset
(namely, LibriTTS + VCTK), we did two subjective human evaluation tests: the first one comparing
Diff-VCTK with AGAIN-VC, FragmentVC and VQMIVC trained on VCTK and the second one
comparing Diff-LibriTTS with BNE-PPG-VC. The results of these tests are given in Tables 2 and
3 respectively. Speech naturalness and speaker similarity were assessed separately. AMT workers
evaluated voice conversion quality on 350 source-target pairs on 5-point scale. In the first test, each
pair was assessed 6 times on average both in speech naturalness and speaker similarity evaluation;
as for the second one, each pair was assessed 8 and 9 times on average in speech naturalness and
speaker similarity evaluation correspondingly. No less than 41 unique assessors took part in each test.
Table 2 demonstrates that our model performs significantly better than the baselines both in terms of
naturalness and speaker similarity even when 6 reverse diffusion iterations are used. Despite working
almost equally well on VCTK speakers, the best baseline VQMIVC shows poor performance on other
speakers perhaps because of not being able to generalize to different domains with lower recording
quality. Although Diff-VCTK performance also degrades on non-VCTK speakers, it achieves good
speaker similarity of MOS 3.6 on VCTK ones when ML-30 sampling scheme is used and only slightly
worse MOS 3.5 when 5x less iterations are used at inference.
Table 3 contains human evaluation results of Diff-LibriTTS for four sampling schemes: ML-30 with
30 reverse SDE solver steps and ML-6, EM-6 and PF-6 with 6 steps of reverse diffusion. The three
schemes taking 6 steps achieved real-time factor (RTF) around 0.1 on GPU (i.e. inference was 10
times faster than real time) while the one taking 30 steps had RTF around 0.5. The proposed model
Diff-LibriTTS-ML-30 and the baseline BNE-PPG-VC show the same performance on the VCTK test
set in terms of speech naturalness the latter being slightly better in terms of speaker similarity which
can perhaps be explained by the fact that BNE-PPG-VC was trained on the union of VCTK and
LibriTTS whereas our model was trained only on LibriTTS. As for the whole test set containing
unseen LibriTTS and internal speakers also, Diff-LibriTTS-ML-30 outperforms BNE-PPG-VC model
achieving MOS 4.0 and 3.4 in terms of speech naturalness and speaker similarity respectively. Due to
employing PPG extractor trained on a large-scale ASR dataset LibriSpeech (Panayotov et al., 2015),
BNE-PPG-VC has fewer mispronunciation issues than our model but synthesized speech suffers
from more sonic artifacts. This observation makes us believe that incorporating PPG features in the
proposed diffusion VC framework is a promising direction for future research.
Table 3 also demonstrates the benefits of the proposed maximum likelihood sampling scheme over
other sampling methods for a small number of inference steps: only ML-N scheme allows us to
8
Published as a conference paper at ICLR 2022
use as few as N = 6 iterations with acceptable quality degradation of MOS 0.2 and 0.1 in terms of
naturalness and speaker similarity respectively while two other competing methods lead to much
more significant quality degradation.
4.3	Maximum likelihood sampling
Figure 2: CIFAR-10 images randomly sampled from VP DPM by running 10 reverse diffusion steps
with the following schemes (from left to right): “euler-maruyama”, “probability flow”, “maximum
likelihood (τ = 0.5)”, “maximum likelihood (τ = 1.0)”.
To show that the maximum likelihood sampling scheme proposed in Section 3 generalizes to different
tasks and DPM types, we took the models trained by Song et al. (2021c) on CIFAR-10 image
generation task and compared our method with other sampling schemes described in that paper in
terms of Frechet Inception Distance (FID).
The main difficulty in applying maximum likelihood SDE solver is estimating data-dependent term
E[Tr (Var (X0∣Xt))] in 0屋.Although in the current experiments WejUSt set this term to zero, We
can think of tWo possible Ways to estimate it: (i) approximate Var (X0|Xt) With Var (X0|Xt = Xt):
sample noisy data Xt , solve reverse SDE With sufficiently small step size starting from terminal
condition Xt = Xt several times, and calculate sample variance of the resulting solutions at initial
points Xo;(ii) use the formula (58) from Appendix C to calculate Var (Xo Xt) assuming that Xo is
distributed normally With mean and variance equal to sample mean and sample variance computed
on the training dataset. Experimenting With these techniques and exploring neW ones seems to be an
interesting future research direction.
Another important practical consideration is that the proposed scheme is proven to be optimal only
for score matching netWorks trained till optimality. Therefore, in the experiments Whose results are
reported in Table 4 We apply maximum likelihood sampling scheme only When t ≤ τ While using
standard Euler-Maruyama solver for t > τ for some hyperparameter τ ∈ [0, 1]. Such a modification
relies on the assumption that score matching netWork is closer to being optimal for smaller noise.
Table 4 shoWs that despite likelihood and FID are tWo metrics that do not perfectly correlate (Song
et al., 2021b), in most cases our maximum likelihood SDE solver performs best in terms of FID. Also,
it is Worth mentioning that although τ = 1 is alWays rather a good choice, tuning this hyperparameter
can lead to even better performance. One can find randomly chosen generated images for various
sampling methods in Figure 2.
5	Conclusion
In this paper, the novel one-shot many-to-many voice conversion model has been presented. Its
encoder design and poWerful diffusion-based decoder made it possible to achieve good results both in
terms of speaker similarity and speech naturalness even on out-of-domain unseen speakers. Subjective
human evaluation verified that the proposed model delivers scalable VC solution With competitive
performance. Furthermore, aiming at fast synthesis, We have developed and theoretically justified the
novel sampling scheme. The main idea behind it is to modify the general-purpose Euler-Maruyama
SDE solver so as to maximize the likelihood of discrete sample paths of the forWard diffusion. Due
to the proposed sampling scheme, our VC model is capable of high-quality voice conversion With as
feW as 6 reverse diffusion steps. Moreover, experiments on the image generation task shoW that all
knoWn diffusion model types can benefit from the proposed SDE solver.
9
Published as a conference paper at ICLR 2022
References
Brian D.O. Anderson. Reverse-time Diffusion Equation Models. Stochastic Processes and their
Applications,12(3):313-326,1982. ISSN 0304-4149.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A Framework
for Self-Supervised Learning of Speech Representations. In Advances in Neural Information
Processing Systems, volume 33, pp. 12449-12460. Curran Associates, Inc., 2020.
Alexandros Beskos and Gareth O. Roberts. Exact Simulation of Diffusions. The Annals of Applied
Probability, 15(4):2422-2444, 2005. ISSN 10505164.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveG-
rad: Estimating Gradients for Waveform Generation. In International Conference on Learning
Representations, 2021a.
Yen-Hao Chen, D. Wu, Tsung-Han Wu, and Hung yi Lee. Again-VC: A One-Shot Voice Conversion
Using Activation Guidance and Adaptive Instance Normalization. In ICASSP 2021 - 2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5954-5958,
2021b.
Ju-Chieh Chou and Hung-yi Lee. One-Shot Voice Conversion by Separating Speaker and Content
Representations with Instance Normalization. In Interspeech 2019, 20th Annual Conference of
the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pp.
664-668. ISCA, 2019.
Pierre Henry-Labordere, Xiaolu Tan, and Nizar Touzi. Unbiased Simulation of Stochastic Differential
Equations. The Annals of Applied Probability, 27(6):3305-3341, 2017. ISSN 10505164.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume 33. Curran Associates, Inc.,
2020.
Aapo Hyvarinen. Estimation of Non-Normalized Statistical Models by Score Matching. Journal of
Machine Learning Research, 6(24):695-709, 2005.
Tatsuma Ishihara and Daisuke Saito. Attention-Based Speaker Embeddings for One-Shot Voice Con-
version. In Interspeech 2020, 21st Annual Conference of the International Speech Communication
Association, Virtual Event, Shanghai, China, 25-29 October 2020, pp. 806-810. ISCA, 2020.
Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-TTS:
A Denoising Diffusion Model for Text-to-Speech. In Proc. Interspeech 2021, pp. 3605-3609,
2021.
Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen,
Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. Transfer Learning from Speaker
Verification to Multispeaker Text-To-Speech Synthesis. In Advances in Neural Information
Processing Systems 31, pp. 4480-4490. Curran Associates, Inc., 2018.
Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, Nobukatsu Hojo, and Shogo Seki. VoiceGrad:
Non-Parallel Any-to-Many Voice Conversion with Annealed Langevin Dynamics, 2020.
Kang-wook Kim, Seung-won Park, and Myun-chul Joe. Assem-VC: Realistic Voice Conversion by
Assembling Modern Speech Synthesis Techniques, 2021.
Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational Diffusion Models,
2021.
Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations,
volume 23 of Stochastic Modelling and Applied Probability. Springer-Verlag Berlin Heidelberg,
1992.
10
Published as a conference paper at ICLR 2022
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative Adversarial Networks
for Efficient and High Fidelity Speech Synthesis. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, virtual, 2020.
Zhifeng Kong and Wei Ping. On Fast Sampling of Diffusion Probabilistic Models. In ICML Workshop
on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile
Diffusion Model for Audio Synthesis. In International Conference on Learning Representations,
2021.
Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen,
Sungroh Yoon, and Tie-Yan Liu. PriorGrad: Improving Conditional Denoising Diffusion Models
with Data-Driven Adaptive Prior, 2021.
Yist Y. Lin, Chung-Ming Chien, Jheng-Hao Lin, Hung-yi Lee, and Lin-Shan Lee. FragmentVC: Any-
To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments
with Attention. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and
SignalProcessing (ICASSP), pp. 5939-5943, 2021.
Robert S. Liptser and Albert N. Shiryaev. Statistics of Random Processes, volume 5 of Stochastic
Modelling and Applied Probability. Springer-Verlag, 1978.
Songxiang Liu, Yuewen Cao, Dan Su, and Helen Meng. DiffSVC: A Diffusion Probabilistic Model
for Singing Voice Conversion, 2021a.
Songxiang Liu, Yuewen Cao, Disong Wang, Xixin Wu, Xunying Liu, and Helen Meng. Any-to-
Many Voice Conversion With Location-Relative Sequence-to-Sequence Modeling. IEEE/ACM
Transactions on Audio, Speech, and Language Processing, 29:1717-1728, 2021b.
Manh Luong and Viet Anh Tran. Many-to-Many Voice Conversion Based Feature Disentanglement
Using Variational Autoencoder. In Proc. Interspeech 2021, pp. 851-855, 2021.
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.
Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech
2017, pp. 498-502, 2017.
Shahan Nercessian. Improved Zero-Shot Voice Conversion Using Explicit Conditioning Signals. In
Interspeech 2020, 21st Annual Conference of the International Speech Communication Association,
Virtual Event, Shanghai, China, 25-29 October 2020, pp. 4711-4715. ISCA, 2020.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Mod-
els. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pp. 8162-8171. PMLR, 18-24 Jul 2021.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR
Corpus Based on Public Domain Audio Books. In 2015 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 5206-5210, 2015.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS:
A Diffusion Probabilistic Model for Text-to-Speech. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pp. 8599-8608. PMLR, 2021.
Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. AutoVC: Zero-
Shot Voice Style Transfer with Only Autoencoder Loss. In Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
5210-5219. PMLR, 09-15 Jun 2019.
Kaizhi Qian, Zeyu Jin, Mark Hasegawa-Johnson, and Gautham J. Mysore. F0-Consistent Many-To-
Many Non-Parallel Voice Conversion Via Conditional Autoencoder. In ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6284-6288,
2020.
11
Published as a conference paper at ICLR 2022
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical
Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention - MICCAI
2015, pp. 234-241. Springer International Publishing, 2015.
Yuki Saito, Yusuke Ijima, Kyosuke Nishida, and Shinnosuke Takamichi. Non-Parallel Voice Conver-
sion Using Variational Autoencoders Conditioned by Phonetic Posteriorgrams and D-Vectors. In
2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.
5274-5278, 2018.
Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise Estimation for Generative Diffusion
Models, 2021.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In
International Conference on Learning Representations, 2021a.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum Likelihood Training of
Score-Based Diffusion Models, 2021b.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In
International Conference on Learning Representations, 2021c.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Computation, 23(7):1661-1674, 2011.
Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen, Xunying Liu, and Helen Meng. VQMIVC:
Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disen-
tanglement for One-Shot Voice Conversion. In Proc. Interspeech 2021, pp. 1344-1348, 2021.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to Efficiently
Sample from Diffusion Probabilistic Models, 2021.
Da-Yi Wu, Yen-Hao Chen, and Hung-yi Lee. VQVC+: One-Shot Voice Conversion by Vector
Quantization and U-Net Architecture. In Helen Meng, Bo Xu, and Thomas Fang Zheng (eds.),
Interspeech 2020, 21st Annual Conference of the International Speech Communication Association,
Virtual Event, Shanghai, China, 25-29 October 2020, pp. 4691-4695. ISCA, 2020.
Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92), 2019.
Heiga Zen, Rob Clark, Ron J. Weiss, Viet Dang, Ye Jia, Yonghui Wu, Yu Zhang, and Zhifeng Chen.
LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. In Interspeech, 2019.
12
Published as a conference paper at ICLR 2022
A Forward VP SDE solution
Since function γ0-,t1Xt is linear in Xt, taking its differential does not require second order derivative
term in ItO's formula:
ZXt) = d(e2 R0 βuduXt)
e2 Rθ βudu ∙ 1 βtXtdt + e2 Rot βudu ∙ (-2βtXtdt + PetdWt
Pete1 Rot βudud-→t.
(13)
Integrating this expression from S to t results in an ItO's integral:
e1 R0 BuduXt - e1 Ro βuduXs
(14)
or
Xt = e-2 Rst β"duXs + /t Pβτe-2 RTt βududW→.
s
(15)
The integrand on the right-hand side is deterministic and belongs to L2 [0, 1] (for practical noise
schedule choices), so its ItO's integral is a normal random variable, a martingale (meaning it has zero
mean) and satisfies Ito's isometry which allows to calculate its variance:
Var(Xt|Xs) = Z tβτe-RτtβuduIdτ = 1-e-Rstβudu I.
s
(16)
Thus
LaW(XtIXs )= N (e-2 Rst βuduXs, (1 - e-Rst βudu) I)= N(Ys,tX*,Q- γ2,JI)	(17)
B The optimal coefficients asymptotics
First derive asymptotics for γ:
Yt-h,t = e-2 Rtt-h βudu = 1 - 2βth + o(h),	(18)
γ2,t-h = e- Ro-h βudu = e-R0 βudueRtt-h βudu = γ2,t(1 + βth) + o(h),	(19)
Y0,t-h = e-2 Rot-h βudu = e-2 Rot βudue2 Rtt-h βudu = γo,t(1 + 1 βth) + o(h),	(20)
YLht = e-Rtt-h βudu = 1 - βth + 0(h).	(21)
Then find asymptotics for μ, V and σ2:
— A ¼ A ɪ 1(-∖ 1 - Y2,t - Y2,tβth +。㈤—1lɔ ,	Y2,t a …心力
μt-h,t = (1 - 2βth + o(h))----------1—Y------------= 1 -2βth-1—γ2-βth+o(h), (22)
νt-h,t = (Y0,t(I + 1 eh + Kh))βt1h+Y2(h) = I -0；2 Bth + Kh),	(23)
σ2-h,t = I -IY2 (Bth + o(h))(I - γ2,t(I + βth) + o(h)) = βth + o(h).	(24)
13
Published as a conference paper at ICLR 2022
Finally We get asymptotics for κ*, ω* and σ*:
*	= νt-h,t(1 - γ2,t) _ I = γo,t-h(1 - γ2-h,t) _ I
κhh	γo,tβth	γo,tβth
(βth + Wh))((I + 1 βth)Yo,t + o(h))
(25)
γ0,tβth
-1 = o(1),
βthωt,h="t-h,t-1+寸-2βth=1 - 1βth -
X (ι "，2 βth + θ(h)) + δ(h) = βth [ -1 —
,t
t,，2	1βth - 1 -Bth+X
1 - γ02,t	2	γ0,t
3+士!+o(h)=W
(26)
(σt,h)2 = σ2-h,t + ν2-h,tEXt [Tr (Var(X0|Xt))] /n = βth + o(h)
+
,t
β2h2Eχt [Tr (Var(X0∣Xt))] /n = βth(1 + δ(1)).
(27)
C Proof of the Theorem 1
The key fact necessary to prove the Theorem 1 is established in the following
Lemma 1. Let po∣t(∙∣x) be pdfofconditional distribution LaW(Xo∣Xt = x). Thenfor any t ∈ [0,1]
and x ∈ Rn
1 - γ 2	(X - Y0,tEP0|t(TX
sθ*(x, t)
(28)
—
Proof of the Lemma 1. As mentioned in (Song et al., 2021c), an expression alternative to (8) can be
derived for θ* under mild assumptions on the data density (Hyvarinen, 2005; Vincent, 2011):
θ* = argmin 广 λtEχ.〜「。(.再“〜必Ι。(.陷)|际(Xt,t) -Vlogpt∣o(XtXo)k2dt,	(29)
θ0
where Law (Xo) is data distribution with pdf po(∙) and Law (Xt∣Xo = xo) has pdf Pt∣0(∙∣x0). By
Bayes formula we can rewrite this in terms of pdfs pt(∙) and Po∣t(∙∣xt) of distributions Law (Xt) and
Law (Xo|Xt = xt) correspondingly:
θ* = argmin Z λtE%〜pt(∙)Eχο〜p0∣t(∙∣Xt)∣∣sθ(Xt,t) - Vlogpt∣o(Xt∣Xo)k2dt.	(30)
θ0
For any n-dimensional random variable ξ with finite second moment and deterministic vector a we
have
Ekξ - ak22 =Ekξ-Eξ+Eξ-ak22 =Ekξ-Eξk22+2hE[ξ-Eξ],Eξ-ai (31)
+ EkEξ - ak22 =Ekξ-Eξk22+kEξ-ak22.	()
In our case ξ = Vlogpt∣o(Xt∣Xo) and a = sθ(Xt, t), so E∣∣ξ - Eξ∣∣2 is independent of θ. Thus
θ* = argmin/1 λtEXt~pt(∙)ksθ(Xt,t) - EX0~po∣t(∙∣Xt) Mogpt∣0(XtXo)] k2dt.	(32)
Therefore, the optimal score estimation network sθ* can be expressed as
14
Published as a conference paper at ICLR 2022
sθ* (x,t) = Epo∣t(∙∣x) [VlogPt∣o(XIXO)]
(33)
for all t ∈ [0, 1] and x ∈ supp {pt} = Rn.
As proven in Appendix A, Law (Xt|X0) is Gaussian with mean vector γ0,tX0 and covariance matrix
(1 - γ02,t ) I, so finally we obtain
sθ*(x,t) = Epo∣t(∙∣χ) - J - Y2 (X - γ0,tXO)
(34)
□
Now let us prove the Theorem 1.
Proof of the Theorem 1. The sampling scheme (11) consists in adding Gaussian noise to a linear
combination of Xt and sθ* (Xt, t). Combining (11) and the Lemma 1 we get
Xi "tth ξt+Xt+βth ((2+%) Xt+(1+a)=7
+ (1 + βth (2 + ωt,h)) Xt + βh(1 + κt,h) (- 1 - γ2	(Xt - γO,tEp0lt(∙∣
=σt,hξt+(1+βth( 1+ωt,h -1 - γ2,h!!Xt+"$：,)：,G Ep0∣t(∙∣χχt)X0,
(35)
where ξt are i.i.d. random variables from standard normal distribution N(0, I) for t = 1, 1 - h, .., h.
EI	. 1	1 ♦ , ♦ F . ∙ W I W -	1	√-1	♦
Thus, the distribution Xt-h|Xt is also Gaussian:
LaW(Xt-h|Xt) = N (μt,h(Kt,h,ω⅛,h)Xt + ^t,h(Kt,h)Ep0lt(∙∣χt)Xo,σ2,hI),	(36)
μt,h(κt,h, ωt,h) = 1 + βth (2 + ωt,h - 1 — Y2,h ) ,	(37)
Vt,h(^t,h) = YQ-h,	(38)
1 - γO2,t
which leads to the following formula for the transition densities of the reverse diffusion:
八 1 I ʌ	1	k llxt-h - μt,hxt - z>t,hEp0|t(∙|xt)XOk2\
pt-h∣t(χt-hlXt) = √2πσn^exp (--------------------------2σ2h------------------>	(39)
Moreover, comparing ∕^t,h and ^t,h With μt-h,t and νt-h,t defined in (9) We deduce that
ʌ —	Y0,tβth(1 + κt,h)	…	—*	”八、
izt,h = νt-h,t ⇔	1 γ2	= νt-h,t ⇔ κt,h = κt,h.	(40)
If we also want μt,h = μt-h,t to be satisfied, then we should have
15
Published as a conference paper at ICLR 2022
1 + Bth (2 + ωt,h - J - γ2") = μt-h,t ⇔ (Mt ；；--------------ωt,h + &t,h) Bth + 1 = μt-h,t,
,	(41)
i.e. Vt,h = νt-h,t and μt,h = μt-h,t iff ^t,h = Kih and ωt,h = ωt,h for the parameters Kih and ωt,h
defined in (10).
As for the corresponding densities of the forward process X , they are Gaussian when conditioned on
the initial data point X0 :
Law (xt-hXt, X0) = N(μt-h,txt + νt-h,tX0, σt-h,t I),	(42)
where coefficients μt-h,t, νt-h,t and σt-h,t are defined in (9). This formula for Law (Xt-h∣Xt, X0)
follows from the general fact about Gaussian distributions appearing in many recent works on
diffusion probabilistic modeling (Kingma et al., 2021): if Zt∣Zs 〜N(αt∣sZs,。2§ I) and Zt∣Z0 〜
N(αt∣0Z0, σjo I) for 0 <s< t, then
一 ,一，一 一、.Vσ2∣o	一 σ" 一 σ2∣oσ2kA
Law (Zs|Zt, ZO)	= N I -^2ɑαt∣sZt	+-- αs∣0Z0, -2--I )	.	(43)
∖σt∣0	σt∣0	σt∣0	)
This fact is a result of applying Bayes formula to normal distributions. In our case ɑt∣s = γs,t and
σt2 = 1 - γs2 t .
t|s	s,t
To get an expression for the densities pt-h|t(xt-h|xt) similar to (39), we need to integrate out the
dependency on data X0 from Gaussian distribution Law (Xt-h|Xt, X0):
pt-h|t(xt-h|xt) =	pt-h,0|t(xt-h, x0|xt)dx0 =	pt-h|t,0(xt-h|xt,x0)p0|t(x0|xt)dx0
(44)
=EXo~po∣t(∙∣xt) [pt-h∣t,O(xt-h|xt, X0)],
which implies the following formula:
pt-h|t(xt-h|xt) = √∏σ-htEp0lt(∙lxt) [exp (-kxt-h -μt-2σtLh- Vt-h，tX0k2)； (45)
Note that in contrast with the transition densities (39) of the reverse process X, the corresponding
densities (45) of the forward process X are not normal in general.
Our goal is to find parameters ^, ω and σ that maximize log-likelihood of sample paths X under
probability measure with transition densities p. Put tk = kh for k = 0,1,..,N and write down this
log-likelihood:
I p(xι,xi-h, ..,xo)	ElogPtk∣tk+ι (xtk ∣xtk+ι) + logp1(x1) dx1dxi-h..dx0
k=0	(46)
N-1
=E / P(Xtk ,xtk+ι )log ptk∣tk+ι (XtJxtk+ι)dxtk+ι dxtk + I P(x1)log PI(XI )dx1.
k=0
The last term does not depend on ^, ω and σ, so we can ignore it. Let Rk be the k-th term in the sum
above. Since we are free to have different coefficients ^t,h, ^⅛,h and σt,h for different steps, we can
maximize each Rk separately. Terms Rk can be expressed as
16
Published as a conference paper at ICLR 2022
Rk
/ P(Xtk , xtk+1 ) logptk | tk+1 (Xtk lxtk + 1 )dxtk+1 dxtk
/ P(Xtk+i>Ptk ∣tk+1 (XtkIXtk+1 ) log Ptk ∣tk+1 (Xtk lxtk+1 )dxtk+1 dxtk
(47)
k+1
/Ptk ∣tk+ι (XtkIXtk+1 )logPtk ∣tk+ι (XtkIXtk+1 )dXtk
From now on We will skip subscripts of μ, V, σ, μ, ν, σ, K, ω, κ* and ω* for brevity. Denote
/	、	1	k ∣∣Xt, - μXt-1- νXo∣∣2∖	/ l 、
Q(Xtk ,Xtk+1 ,Xo) = √2∏σn exp (-	tk	2σ+--------) log Ptk∣tk+1 (Xtk IXtk+1). (48)
Using the formula (44) for the densities of X together with the explicit expression for the Gaussian
densityPtk|tk+1,0(Xtk IXtk+1 , X0) and applying Fubini’s theorem to change the order of integration,
we rewrite Rk as
Rk = EXtk+1 /Ptk∣tk+1 (XtkIXtk+1 ) logPtk∣tk+1 (XtkIXtk+1 )dXtk
=EXtk+1 / EXo 〜P0∣tk+1 (∙∣Xtk+1) [Ptk 1tk+1,0(XtkIXtk+1,X0)log Ptk|tk+1 (XtkIXtk+1)] dXtk
=EXtk+1	/ EX0〜P0∣tk+1 (∙∣Xtk+1 )[Q(Xtk ,Xtk+1 ,X0)]dXtk
=EXtk+1 EX0〜P0∣tk+1 (∙∣Xtk+1)	/ Q(Xtk, Xtk+1 ,X0)dXtk .
(49)
The formula (48) implies that the integral of Q(Xtk, Xtk+1 , X0) with respect to Xtk can be seen as
expectation of logPtk∣tk+1 (ξIXtk+1) with respect to normal random variable ξ with mean μXtk+1 +
νX0 and covariance matrix σ2 I. Plugging in the expression (39) into (48), we can calculate this
integral:
∣∣ξ - μXtk+1 - VEXO ~po∣tk+1 (∙Xtk+1 )x0 ∣∣2
2σ^2
-log √2π - n log σ - Eξ kξ-μxtk+1-VEp0∣tk+1 (∙∣Xtk+1 )X0 k2
(50)
Thus, terms Rk we want to maximize equal
Rk = - log√2π-n log σ-EXtk+1EX0~p0|tk+1 (TXtk+1)
Eξ ∣ξ-μχtk+1
一νEp0∣tk+1 (∙∣Xtk+1 )χ012
2σ2
(51)
Maximizing Rk with respect to (K, ω, σ) is equivalent to minimizing EXtk十 1 Sk where Sk is given by
Sk = n log σ + 2∣2 EX0~po∣tk+1 (∙∣Xtk+1 )Eξ ∣∣ξ - μχtk+1 - Z)Ep0∣tk+1 (∙∣Xtk+1 )χ01∣2,	(52)
where the expectation with respect to ξ 〜N(μXtk+1 + νXo, σ21) can be calculated using the fact
that for every vector a we can express Eξ∣∣ξ 一 a∣2 as
17
Published as a conference paper at ICLR 2022
Ekξ-Eξ+Eξ — ^k2 = Ekξ — Eξk2 +2hE[ξ-Eξ],Eξ-^i +E∣∣Eξ-&k2 = nσ2 + ∣∣Eξ-^k2. (53)
So, the outer expectation with respect to Law(X0 |Xtk+1) in (52) can be simplified:
EXo~P0∣tk+1 (∙∣Xtk + ι) [nσ2 + k(μ — μ)Xtk+ι + νX0 — VEX0~Po∣tk+1 (∙∣Xtk+ι )X0112]
=nσ2 + EXOk((M — μ)Xtk+ι + VXO- νEX0y X0 k2 = nσ2 + (μ — μ)2kXtk+ι k2
+ 2h(μ — μ)Xtk+ι, (V- V)EXO XOi + EXo llνXO — OEX0 XO k2 = (M — μ)2kXtk+ι k2	(54)
+ 2h(μ — μ)Xtk+ι, (V — V)EXOXOi + V2EXo kXO k2 + /1EX。X0k2 + nσ2
—	2VVhEXO XO, EXO XO i = k(μ — μ)Xtk+ι + (v - 0EXOXO) k 2 + V 2EXo ∣∣XO∣∣2
—	V2kEXOXOk22 + nσ2,
where all the expectations in the formula above are taken with respect to the conditional data
distribution Law(XO |Xtk+1 ). So, the resulting expression for the terms Sk whose expectation with
respect to Law (Xtk+1 ) we want to minimize is
Sk = n logσ + τp^2 (nσ2 + k(μ — μ)Xtk+ι + (V — 0)E[XO ∣Xtk+ι]∣∣2
2σ \	\	(55)
+ 沙2 (e [kXOk2 lXtk+ι ] — kE[XO|Xtk+i ]k2)j .
Now it is clear that κtk+ι ,h and ωtk + ι,h are optimal because μtk+ι,h(κtk+ι,h,ωtk+ι,h) = μtk,tk+1
and ^tk+ι,h(κ^k+ɪ,h) = Vtk ,tk+ι. For this choice of parameters We have
EXtk+1Sk = n log σ +击(nσ2 + V2Ej [E [kX°k2 |羽+」—|阳*羽+」||2]) .	(56)
Note that E kXOk22|Xtk+1 — kE[XO|Xtk+1]k22 = Tr (Var (XO|Xtk+1)) is the overall variance of
LaW (XO∣Xtk+ι) along all n dimensions. Differentiating EXtk十ι Sk With respect to σ shows that the
optimal σ*fc+ι h should satisfy
στn------(σ* 1 )3 (nσ2k,tk+1 + 此,tk+1 EXtk+1 [Tr (Var(XO |Xtk+1 ))] = 0,	(57)
tk+1,h	tk+1,h
which is indeed satisfied by the parameters σt,hι defined in (10). Thus, the statement (i) is proven.
When it comes to proving that X is exact, we have to show that LaW (Xtk) = LaW (Xtk) for
every k = 0,1,.., N. By the assumption that Law (Xi) = Law (Xi) it is sufficient to prove
that Ptk∣tk+ι (xtk∣xtk+ι) ≡ Ptk∣tk+ι (xtk∣xtk+ι) since the exactness will follow from this fact by
mathematical induction. If XO is a constant random variable, Law(XO|Xt) = Law (XO) also
corresponds to the same constant, so Var (X。|Xt) = 0 meaning that σt 卜=σt-h,t, and the formulae
(39) and (45) imply the desired result.
Let us now consider the second case when XO 〜N(μ,δ21). It is a matter of simple but
lengthy computations to prove another property of Gaussian distributions similar to (43): if
Zo 〜N(μ, δ21) and Zt∣Z° 〜N(αtZq, b I), then Z°∣Zt 〜N(b2+^μ + b2+^Zt, bɪg^ɪ I)
and Zt 〜 N(μαt, (b2 + δ2α2)I). In our case a = γο,t and b2 = 1 — γ2,t, therefore
Law(XO|Xt) =N
(58)
18
Published as a conference paper at ICLR 2022
So, Var (X0∣ Xt) does not depend on Xt and
(*2
σt-h,t +
ν
T EXJiY(Var(X0∣Xt))I=σ2-h,t+ν2-h,t
(59)
Since LaW(Xt∣Xt-h), LaW(Xt-h) and LaW(Xt) are Gaussian, Bayes formula implies that
Law (Xt-h ∣Xt) is Gaussian as well with the following mean and covariance matrix:
E[Xt-h∣Xt]
(60)
Y0,t-h(I — γ2-h,t) . γt-h,t(I — γ2,t-h + δ2γ0,t-h) X
1 - γ2,t + δ2γ2,t μ +	1 - γ2,t + δ2γ2,t	Xt
Var(Xt-h∣Xt)
(61)
El	■ /	I Sr \ • 1 y 1	♦ F ,1 c	1 /cd、	.	1 1 .1	/`
The distribution Law (Xt-h∣Xt) is also Gaussian by the formula (39), so to conclude the proof
we just need to show that E[Xt-h∣Xt = x] = E[Xt-h∣Xt = x] and Var(Xt-h∣Xt = X) =
Var (Xt-h ∣Xt = x) for every X ∈ Rn for the optimal parameters (10). Recall that for KJ= h and ωj h
we have Mh(Kih,⑹$)=μt-h,t and Vt,h(κ=,h) = %-h,t∙ Utilizing the formulae (9), (39), (58) and
the fact that γo,t-h ∙ Yt-h,t = Yo,t (following from the definition of Y in (7)) we conclude that
τπ Γ -ʌ	I -ʌ	T	ʌ	/	-±-	-±-	∖	ʌ	/	-±-	∖ τπ	τ r
E[Xt-h〔Xt = x] = μt,h(κt,h, ωt,h)x + %,h(κt,h)Ep01t(Tx)X。
1	- γ0,t-h	,	1 - γ2-h,t
γt-h,t τ-^o-X+γ0,t-h τ-^07
_	1 - γt2-h,t	_ ,	(1 - Y2,t-h)(1 - γ2,t) + S2Y2,t(1 - Y2,t-h)
=γ0,t-h 1-γ2,t + δ2γ2,t 阳 + γt-h,t	(1-γ2,t)(1-γ2,t + δ2γ2,t)	X
l	S2Y2,t-h(1 - Y2-h,t)	_	1 - Y2-h,t	.
+ γt-h't (1-γ2,t)(1-γ2,t + δ2γ2,t)X = γ0,t-h 1-γ2,t + δ2γ2,tμ
,	(1 - Y2,t-h)(1 - Y2,t) + δ2Y2,t-h(1 - Y2,t)	_	1 - Y2-h,t	.
+ γt-h,t	(1-γ2,t)(1-γ2,t + δ⅝2,t)	x = γ0,t-h 1 - γ2,t +。2陆 μ
+ Yt-h,t1 -Nh + ；2；2，t-h X = E[Xt-h∣Xt = X],
(62)
Var(Xt-h∣Xt = x) = (Wh)2 I=卜-h,t + ν2-h,t	) I
(1 - γ2,t-h)(1 - γ2-h,t)
+ γO,t-h
δ2(1-γ2-h,t)2
I
(63)
+
((1 - γ2,t-h)(1 - γo,t) + δ27o,t-h(1 - γo,t)) i
(1 - Y2-h,t)(1 - Y2,t-h + δ2Y⅞t-h)
I = Var(Xt-h∣Xt = x).
□
19
Published as a conference paper at ICLR 2022
D Reverse MR-VP SDE solver
MR-VP DPM is characterized by the following forward and reverse diffusions:
dXt = 1 βt(X - Xtjdt + PβtdWt ,
dXt = (2 βt(X - Xt)- βtsθ (Xt, X ,t)) dt + Petdt-.
Using the same method as in Appendix A, we can show that for s < t
LaW(XtIXs) = N(Ys,tXs + (1 - Ys,t)X, (I- Y2,t)I),	Ys,t = e-IRst βudu
With the following notation:
(64)
(65)
(66)
(67)
we can write down the parameters of Gaussian distribution Xs |Xt, X0:
E[Xs∣Xt,Xo]= X + μs,t(Xt- X) + νs,t(Xo - X), Var(Xs∣Xt,Xo)= σS,J.	(68)
The Lemma 1 for MR-VP DPMs takes the following shape:
sθ*(X, X, t) =-
(X -(I- γ0,t)X - Y0,tEp0|t(.|x)Xo)
1
((X - X) - γ0,t (Epo∣t(∙∣χ) X0 -
(69)
The class of reverse SDE solvers we consider is
Xt-h = Xt + βth ( (2 + ωt,j (Xt- X) + (I + κt,h)sθ (Xt, X, t)) + σt,hξt,	(70)
where t = 1,1 - h, .., h and ξt are i.i.d. samples from N(0, I). Repeating the argument of the
Theorem 1 leads to the following optimal (in terms of likelihood of the forward diffusion sample
paths) parameters:
νt-h,t(I - γ2,t)	ι * _ μt-h,t - 1	1 + κt,h	1
-Y0tβh	,	ωt,h =	βth	+T-ʒ0^ - 2,
(Wh)2=σ2-h,t+n ν2-h,t EXt [Tr(Var(X。阳))],
(71)
which are actually the same as the optimal parameters (10) for VP DPM. It is of no surprise since
MR-VP DPM and VP-DPM differ only by a constant shift.
E Reverse sub-VP SDE solver
Sub-VP DPM is characterized by the following forward and reverse diffusions:
dXt = -2βtXtdt + Jet(1 - e-2 詹 βudu)dWt ,
20
(72)
Published as a conference paper at ICLR 2022
dX^t = (-1 βtX^t	-	βt(1	-	e-2 Rtβudu)	Sθ(Xt,t))	dt + √βt(1 -	e-2 Rt βudu)d记.(73)
Using the same method as in Appendix A, we can show that for s < t
LaW (Xt∣Xs) = N(γs,tXs,(1 + γ4,t - γ2,t(1 + 73))I),	N = e-1 Rs βudu.	(74)
Note that for s = 0 this expression simplifies to
LaW(Xt|X0) =N(γ0,tX0,(1 - γ02,t)2 I).	(75)
With the following notation:
11 - γ2,s \ 2	1 + Y4,t- Y2,t(I + γ4,s)
…=γs,t (F厂Α = γ0,s —LξT—
2 _ (I- Y2,S)2(I + γ4,t - Y2,t(I + Y4,S))
σs,t =	(1-γ2,t)2
(76)
we can write down the parameters of Gaussian distribution XS |Xt, X0:
E[Xs |Xt, x0] = μs,txt + νs,tx0, Var(XS |Xt, XO)= σ2,t I .	(77)
The Lemma 1 for sub-VP DPMs takes the following shape:
Sθ*(x,t)
(X - γ0,tEp0∣t(∙∣x)xθ).
(78)
The class of reverse SDE solvers we consider is
Xt-h =	Xt	+	βth ( (2 + ωt,h) Xt	+ (1 +	κt,h) (1 -	e-2	R0	βudu) sθ(Xt, t))	+	σt,hξt,	(79)
where t = 1, 1 - h, .., h and ξt are i.i.d. samples from N(0, I). Repeating the argument of the
Theorem 1 leads to the following optimal (in terms of likelihood of the forward diffusion sample
paths) parameters:
* _ νt-h,t(I- γ2,t)	1	* _ μt-h,t - 1	(I + Kih)(I + γ0,t)	1
κt,h = Y0,tβth(1 + γ2,t) - ,	ωt,h =	βth	+	1-益	2,	(80)
(σ*,h)2 = σ2-h,t + 1 νt-h,tEXt [Tr (Var(X0|Xt))].
F	Reverse VE SDE solver
VE DPM is characterized by the following forward and reverse diffusions:
dXt = V(σ27dWt ,	(81)
dXt = - (σ2)' sθ(Xt,t)dt + ^σi'茄t.	(82)
21
Published as a conference paper at ICLR 2022
Since for s < t
Xt
Xs
+∕t q(σu)0 d-→,
(83)
similar argument as in Appendix A allows showing that
Law(Xt|Xs) =N
N(Xs, (σt2 - σs2)I).
(84)
With the following notation:
—σ2- σ2	— σ2- σ2	2 _ (σ2- σS)(σS- σO)
μs,t=σ2二σ^,	νs,t=σ2=σ^,	σs,t—一谭二σ一
(85)
we can write down the parameters of Gaussian distribution Xs |Xt , X0 :
EXs∣Xt,X0]= μs,tXt + Vs,tX0, Var(Xs∣Xt,X0) = σ2,J∙	(86)
The Lemma 1 for VE DPMs takes the following shape:
1
sθ*(x,t)
σ2 - σ0
(X - EP0∣t(∙∣χ)X0).
(87)
Repeating the argument of the Theorem 1 leads to the following optimal (in terms of likelihood of
the forward diffusion sample paths) reverse SDE solver:
Xt-h = Xt + (σt - σt2-h)sθ(Xt,t + σt,hξt,
where
(88)
(σt,h)2 = E - WJ σ0) + 1 ν2-h,tEχt [Tr (Var(X。|羽))],	(89)
t = 1,1- h, .., h and ξt are i.i.d. samples from N(0, I).
G	Toy examples
In this section we consider toy examples where data distribution X0 is represented by a single
point (corresponding to the case (ii) of the Theorem 1) and by two points (corresponding to more
general case (i)). In the first case the point is unit vector i = (1, 1, .., 1) of dimensionality 100,
in the second one two points i and -2i have the same probability. We compare performance of
two solvers, Euler-Maruyama and the proposed Maximum Likelihood, depending on the number
N ∈ {1, 2, 5, 10, 100, 1000} of solver steps. The output of the perfectly trained score matching
network sθ* is computed analytically and Gaussian noise with variance ε ∈ {0.0, 0.1, 0.5} is added
to approximate the realistic case when the network sθ we use is not trained till optimality. We
considered VP diffusion model (6) with β0 = 0.05 and β1 = 20.0.
The results of the comparison are given in Table 5 and can be summarized in the following:
•	for both methods, larger N means better quality;
•	for both methods, more accurate score matching networks (smaller ε) means better quality;
•	for large number of steps, both methods perform the same;
•	it takes less number of steps for the proposed Maximum Likelihood solver to converge with
a good accuracy to data distribution than it does for Euler-Maruyama solver;
22
Published as a conference paper at ICLR 2022
Table 5: Maximum Likelihood (ML) and Euler-Maruyama (EM) solvers comparison in terms of
Mean Square Error (MSE). MSE < 0.001 is denoted by conv, MSE > 1.0 is denoted by div. N is the
number of SDE solver steps, ε is variance of Gaussian noise added to perfect scores s6*.
MSE ML/EM		Xo = {i}			χo = {i, -2i}		
		ε= 0.0	ε = 0.1	ε = 0.5	ε = 0.0	ε = 0.1	ε = 0.5
N	=1	conv / div	div / div	div / div	div / div	div / div	div / div
-N	=2-	conv / div	div / div	div / div	0.15 / div	div / div	div / div
-N	=5-	conv / div	0.017 / div	0.085 / div	conv / div	0.017 / div	0.085 / div
-ɪ	=10	conv / 0.57	0.001/0.59 ∙	0.005/0.67 ∙	conv / 0.57	0.001/0.59 一	0.006/0.67 一
	100	conv / 0.01	conv / 0.01	conv / 0.01	conv / 0.01	conv / 0.01	conv / 0.01
-N=	1000	conv / conv	ConV / ConV	conv / conv	conv / conv	conv / conv	conv / conv
•	in accordance with the statement (ii) of the Theorem 1, the optimal Maximum Likelihood
solver leads to exact data reconstruction in the case when data distribution is constant and
score matching network is trained till optimality (i.e. ε = 0.0) irrespective of the number of
steps N .
Also, in the second example where X0 ∈ {i, -2i} the Maximum Likelihood SDE solver reconstructs
the probabilities of these two points better than Euler-Maruyama which tends to output “i-samples”
(which are closer to the origin) more frequently than “-2i-samples”. E.g. for ε = 0.0 and N = 10
the frequency of “i-samples” generated by Euler-Maruyama scheme is 54% while this frequency
for Maximum Likelihood scheme is 50% (500k independent runs were used to calculate these
frequencies).
H Speaker conditioning network
The function X ∙ tanh(softplus(x)) is used as a non-linearity in the speaker conditioning network
gt(Y ). First, time embedding te is obtained by the following procedure: time t ∈ [0, 1] is en-
coded with positional encoding (Song et al., 2021c), then resulting 256-dimensional vector t0 is
passed through the first linear module with 1024 units, then a non-linearity is applied to it and
then it is passed through the second linear module with 256 units. Next, noisy mel-spectrogram
Yt for wodyn input type or Yt concatenated with {Ys|s = 0.5/15, 1.5/15, .., 14.5/15} for whole is
passed through 6 blocks consisting of 2D convolutional layers each followed by instance normal-
ization and Gated Linear Unit. The number of input and output channels of these convolutions is
(1, 64), (32, 64), (32, 128), (64, 128), (64, 256), (128, 256) for wodyn input type and the same but
with 16 input channels in the first convolution for whole input type. After the 2nd and 4th blocks
MLP1 (te) and MLP2(te) are broadcast-added where MLP1 (MLP2) are composed of a non-
linearity followed by a linear module with 32 (64) units. After the last 6th block the result is passed
through the final convolution with 128 output channels and average pooling along both time and
frequency axes is applied resulting in 128-dimensional vector. All convolutions except for the final
one have (kernel, stride, zero padding) = (3, 1, 1) while for the final one the corresponding parameters
are (1, 0, 0). Denote the result of such processing ofY by c for wodyn and whole input types.
Clean target mel-spectrogram Y0 is used to obtain 256-dimensional speaker embedding d with the
pre-trained speaker verification network (Jia et al., 2018) which is not trained. Vectors d, c and t0 are
concatenated (except for d-only input type where we concatenate only d and t0), passed through a
linear module with 512 units followed by a non-linearity and a linear module with 128 units. The
resulting 128-dimensional vector is the output of the speaker conditioning network gt(Y ).
I	Training hyperparameters and other details
Encoders and decoders were trained with batch sizes 128 and 32 and Adam optimizer with initial
learning rates 0.0005 and 0.0001 correspondingly. Encoders and decoders in VCTK models were
trained for 500 and 200 epochs respectively; as for LibriTTS models, they were trained for 300 and
23
Published as a conference paper at ICLR 2022
110 epochs. The datasets were downsampled to 22.05kHz which was the operating rate of our VC
models. VCTK recordings were preprocessed by removing silence in the beginning and in the end of
utterances. To fit GPU memory, decoders were trained on random speech segments of approximately
1.5 seconds rather than on the whole utterances. Training segments for reconstruction and the ones
used as input to the speaker conditioning network gt(Y ) were different random segments extracted
from the same training utterances. Noise schedule parameters β0 and β1 were set to 0.05 and 20.0.
Our VC models operated on mel-spectrograms with 80 mel features and sampling rate 22.05kHz.
Short-Time Fourier Transform was used to calculate spectra with 1024 frequency bins. Hann window
of length 1024 was applied with hop size 256.
For Diff-LibriTTS models we used simple spectral subtraction algorithm in mel domain with spectral
floor parameter β = 0.02 as post-processing to reduce background noise sometimes produced by
these models. Noise spectrum was estimated on speech fragments automatically detected as the ones
corresponding to silence in source mel-spectrogram.
J Details of AMT tests
For fair comparison with the baselines all the recordings were downsampled to 16kHz; we also
normalized their loudness. In speech naturalness tests workers chosen by geographic criterion were
asked to assess the overall quality of the synthesized speech, i.e to estimate how clean and natural
(human-sounding) it was. Five-point Likert scale was used: 1 - “Bad”, 2 - “Poor”, 3 - “Fair”, 4 -
“Good”, 5 - “Excellent”. Assessors were asked to wear headphones and work in a quiet environment.
As for speaker similarity tests, workers were asked to assess how similar synthesized samples sounded
to target speech samples in terms of speaker similarity. Assessors were asked not to pay attention
to the overall quality of the synthesized speech (e.g. background noise or incorrect pronunciation).
Five-point scale was used: 1 - “Different: absolutely sure”, 2 - “Different: moderately sure”, 3 -
“Cannot decide more same or more different”, 4 - “Same: moderately sure”, 5 - “Same: absolutely
sure”.
24