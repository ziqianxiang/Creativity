论文题目,会议名称
Vivit: A video vision transformer, arXiv preprint arXiv:2103
 Layer normalization, arXiv preprintarXiv:1607
 Is space-time attention all you need for videounderstanding? arXiv preprint arXiv:2102,05095
 Language models arefew-shot learners, arXiv preprint arXiv:2005
 End-to-end object detection with transformers, In European Conferenceon Computer Vision
 MMDetection: Open mmlab detection toolbox andbenchmark, arXiv preprint arXiv:1906
 Encoder-decoder with atrous separable convolution for semantic image segmentation, In Proceedings ofthe European conference on computer vision (ECCV)
 MMSegmentation: Openmmlab semantic segmentation toolboxand benchmark, https://github
 An algorithm for the machine calculation of complex fourierseries, Mathematics of computation
 On the relationship between self-attention and convolutional layers, In International Conference on Learning Representations
 Randaugment: Practical automateddata augmentation with a reduced search space, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Transformer-xl: Attentive language models beyond a fixed-length context, arXiv preprintarXiv:1901
 Imagenet: A large-scale hi-erarchical image database, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale, arXiv preprintarXiv:2010
 Mdmmt: Mul-tidomain multimodal transformer for video retrieval, In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition
 Multiscale vision transformers, arXiv preprint arXiv:2104
 Dual attentionnetwork for scene segmentation, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Adaptivecontext network for scene parsing, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Multi-modal transformer forvideo retrieval, In Computer Vision-ECCV 2020: 16th European Conference
 Understanding the difficulty of training deep feedforward neuralnetworks, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Beyond self-attention: Externalattention using two linear layers for visual tasks, arXiv preprint arXiv:2105
 Deep residual learning for image recog-nition, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Mask r-cnn, In Proceedings oftheIEEE international conference on computer vision
 Benchmarking neural network robustness to common cor-ruptions and perturbations, Proceedings of the International Conference on Learning Represen-tations
 Gaussian error linear units (gelus), arXiv preprintarXiv:1606
 Vi-sion permutator: A permutable mlp-like architecture for visual recognition, arXiv preprintarXiv:2106
 Mobilenets: Efficient convolutional neural networks formobile vision applications, arXiv preprint arXiv:1704
 Deep networks withstochastic depth, In European conference on computer vision
 Densely connectedconvolutional networks, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Ccnet:Criss-cross attention for semantic segmentation, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 Batch normalization: Accelerating deep network training byreducing internal covariate shift, In International conference on machine learning
 PanoPtic feature pyramid net-works, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Imagenet classification with deep con-volutional neural networks, Advances in neural information processing systems
 Backpropagation applied to handwritten zip code recognition,Neural computation
 Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neuralarchitecture search, arXiv preprint arXiv:2103
 As-mlp: An axial shifted mlp architecturefor vision, arXiv preprint arXiv:2107
 Network in network, arXiv preprint arXiv:1312
 Microsoft coco: Common objects in context, In Europeanconference on computer vision
 Focal loss for denseobject detection, In Proceedings of the IEEE international conference on computer vision
 Pay attention to mlps, arXiv preprintarXiv:2105
 SWin transformer: Hierarchical vision transformer using shifted WindoWs, arXiv preprintarXiv:2103
 Decoupled Weight decay regularization, arXiv preprintarXiv:1711
 Rethinkingthe design principles of robust vision transformer, arXiv preprint arXiv:2105
" Pytorch: An imperative style, high-performancedeep learning library", In H
 Learning transferable visualmodels from natural language supervision, arXiv preprint arXiv:2103
 Global filter netWorks forimage classification, arXiv preprint arXiv:2107
 Very deep convolutional netWorks for large-scale imagerecognition, arXiv preprint arXiv:1409
Bottleneck transformers for visual recognition, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Sparse r-cnn: End-to-end object detection withlearnable proposals, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition 
 Going deeper with convolutions, InProceedings of the IEEE conference on computer vision and pattern recognition
 Rethink-ing the inception architecture for computer vision, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Mlp-mixer: Anall-mlp architecture for vision, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention, arXivpreprint arXiv:2012
 Resmlp: Feedforwardnetworks for image classification with data-efficient training, arXiv preprint arXiv:2105
"Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou", Goingdeeper with image transformers
 Attention is all you need, In Advances in neural informationprocessing systems
Glue: A multi-task benchmark and analysis platform for natural language understanding, arXivpreprint arXiv:1804
 Pvtv2: Improved baselines with pyramid vision transformer, arXiv preprintarXiv:2106
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions, arXiv preprint arXiv:2102
 End-to-end video instance segmentation with transformers, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Pytorch image models, https://github
 Cvt:Introducing convolutions to vision transformers, arXiv preprint arXiv:2103
 Unified perceptual parsing forscene understanding, In Proceedings of the European Conference on Computer Vision (ECCV)
 Seg-former: Simple and efficient design for semantic segmentation with transformers, arXiv preprintarXiv:2105
 Aggregated residual trans-formations for deep neural networks, In Proceedings of the IEEE conference on computer visionandpattern recognition
Xlnet: Generalized autoregressive pretraining for language understanding, Advances in neuralinformation processing systems
 Disen-tangled non-local neural netWorks, In European Conference on Computer Vision
 Multi-scale context aggregation by dilated convolutions, In ICLR
 S2-mlp: Spatial-shift mlp architecture forvision, arXiv preprint arXiv:2106
 Tokens-to-token vit: Training vision transformers from scratch onimagenet, arXiv preprint arXiv:2101
 Object-contextual representations for semantic seg-mentation, In Computer Vision-ECCV 2020: 16th European Conference
Cutmix: Regularization strategy to train strong classifiers With localizable features, In Proceed-ings of the IEEE/CVF International Conference on Computer Vision
 mixup: Beyond empiricalrisk minimization, arXiv preprint arXiv:1710
 Rethinking semantic segmentation froma sequence-to-sequence perspective With transformers, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition
 Random erasing data augmen-tation, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)
 Sceneparsing through ade20k dataset, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Deformable detr:Deformable transformers for end-to-end object detection, arXiv preprint arXiv:2010
