Published as a conference paper at ICLR 2022
Provab le RL with Exogenous Distractors via
Multistep Inverse Dynamics
Yonathan Efroni1, Dipendra Misra1, Akshay Krishnamurthy1, Alekh Agarwal2*, John Langford1
1	Microsoft Research, New York, NY
2	Google
Ab stract
Many real-world applications of reinforcement learning (RL) require the agent to
deal with high-dimensional observations such as those generated from a megapixel
camera. Prior work has addressed such problems with representation learning,
through which the agent can provably extract endogenous, latent state information
from raw observations and subsequently plan efficiently. However, such approaches
can fail in the presence of temporally correlated noise in the observations, a
phenomenon that is common in practice. We initiate the formal study of latent
state discovery in the presence of such exogenous noise sources by proposing a
new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL.
We start by establishing several negative results, by highlighting failure cases of
prior representation learning based approaches. Then, we introduce the Predictive
Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics
and is provably sample and computationally efficient in EX-BMDPs when the
endogenous state dynamics are near deterministic. The sample complexity of PPE
depends polynomially on the size of the latent endogenous state space while not
directly depending on the size of the observation space, nor the exogenous state
space. We provide experiments on challenging exploration problems which show
that our approach works empirically.
1 Introduction
In many real-world applications such as robotics there can be large disparities in the size of agent’s
observation space (for example, the image generated by agent’s camera), and a much smaller
latent state space (for example, the agent’s location and orientation) governing the rewards and
dynamics. This size disparity offers an opportunity: how can we construct reinforcement learning
(RL) algorithms which can learn an optimal policy using samples that scale with the size of the
latent state space rather than the size of the observation space? Several families of approaches have
been proposed based on solving various ancillary prediction problems including autoencoding (Tang
et al., 2017; Hafner et al., 2019), inverse modeling (Pathak et al., 2017; Burda et al., 2018), and
contrastive learning (Laskin et al., 2020) based approaches. These works have generated some
significant empirical successes, but are there provable (and hence more reliable) foundations for their
success? More generally, what are the right principles for learning with latent state spaces?
In real-world applications, a key issue is robustness to noise in the observation space. When noise
comes from the observation process itself, such as due to measurement error, several approaches have
been recently developed to either explicitly identify (Du et al., 2019; Misra et al., 2020; Agarwal
et al., 2020a) or implicitly leverage (Jiang et al., 2017) the presence of latent state structure for
provably sample-efficient RL. However, in many real-world scenarios, the observations consist of
many elements (e.g. weather, lighting conditions, etc.) with temporally correlated dynamics (see
e.g. Figure 1 and the example below) that are entirely independent of the agent’s actions and rewards.
The temporal dynamics of these elements precludes us from treating them as uncorrelated noise,
and as such, most previous approaches resort to modeling their dynamics. However, this is clearly
wasteful as these elements have no bearing on the RL problem being solved.
t Work was done while the author was at Microsoft Research.
{yefroni, dimisra, akshaykr, jcl}@microsoft.com, alekhagarwal@google.com
1
Published as a conference paper at ICLR 2022
Generalized Inverse Dynamics
Train a model to predict the index of roll-in path
fθ (idx (V ◦ a) | x0)
V Λ∕W> X a	X0
∖^a^xX
V 〜Uniform(Ψh-ι)	a 〜Uniform(A)
Policy cover for the last time step Action space
Figure 1: Left: An agent is walking next to a pond in a park and observes the world as an image. The world
consists of a latent endogenous state, containing variable such as agent’s position, and a much larger latent
exogenous state containing variables such as motion of ducks, ripples in the water, etc. Center: Graphical model
of the EX-BMDP. Right: PPE learns a generalized form of inverse dynamics that recovers the endogenous state.
As an example, consider the setting in Figure 1. An agent is walking in a park on a lonely sidewalk
next to a pond. The agent’s observation space is the image generated by its camera, the latent
endogenous state is its position on the sidewalk, and the exogenous noise is provided by motion of
ducks, swaying of trees and changes in lighting conditions, typically unaffected by the agent’s actions.
While there is a line of recent empirical work that aims to remove causally irrelevant aspects of the
observation (Gelada et al., 2019; Zhang et al., 2020), theoretical treatment is quite limited (Dietterich
et al., 2018) and no prior works address sample-efficient learning with provable guarantees. Given
this, the key question here is:
How can we learn using an amount of data scaling with just the size of the endogenous latent state,
while ignoring the temporally correlated exogenous observation noise?
We initiate a formal treatment of RL settings where the learner’s observations are jointly generated
by a latent endogenous state and an uncontrolled exogenous state, which is unaffected by the
agent’s actions and does not affect the agent’s task. We study a subset of such problems called
Exogenous Block MDPs (EX-BMDPs), where the endogenous state is discrete and decodable from
the observations. We first highlight the challenges in solving EX-BMDPs by illustrating the failures
of many prior representation learning approaches (Pathak et al., 2017; Misra et al., 2020; Jiang et al.,
2017; Agarwal et al., 2020a; Zhang et al., 2020). These failure happen either due to creating too
many latent states, such as one for each combination of ducks and passers-by in the example above
leading to sample inefficiency in exploration, or due to lack of exhaustive exploration.
We identify one recent approach developed by Du et al. (2019) with favorable properties for EX-
BMDPs with near-deterministic latent state dynamics. In Section 4 and Section 5, we develop a
variation of their algorithm and analyze its performance. The algorithm, called Path Prediction and
Elimination (PPE), learns a form of multi-step inverse dynamics by predicting the identity of the path
that generates an observation. For near-deterministic EX-BMDPs, we prove that PPE successfully
explores the environment using O((SA)2H log(∣F∣∕δ)) samples where S is the size of the latent
endogenous state space, A is the number of actions, H is the horizon and F is a function class
employed to solve a maximum likelihood problem. Several prior works (Gregor et al., 2016; Paster
et al., 2020) have also considered a multi-step inverse dynamics approach to learn a near optimal
policy. Yet, these works do not consider the EX-BMDP model. Further, it is unknown whether
these algorithms have provable guarantees, as PPE. Theoretical analysis of the performance of these
algorithms in the presence of exogenous noise is an interesting future work direction.
Empirically, in Section 6, we demonstrate the performance of PPE and various prior baselines in a
challenging exploration problem with exogenous noise. We show that baselines fail to decode the
endogenous state as well as learning a good policy. We further, show that PPE is able to recover the
latent endogenous model in a visually complex navigation problem, in accordance with the theory.
2	Exogenous Block MDP Setting
We introduce a novel Exogenous Block Markov Decision Process (EX-BMDP) setting to model
systems with exogenous noise. We describe notations before formalizing the EX-BMDP model.
2
Published as a conference paper at ICLR 2022
Notations. For a given set U, we use ∆(U) to denote the set of all probability distributions overU.
For a given natural number N ∈ N, we use the notation [N] to denote the set {l, 2,…，N}. Lastly,
for a probability distribution p ∈ ∆(U), we define its support as supp(p) = {u | p(u) > 0, u ∈ U}.
We start with describing the Block Markov Decision Process (BMDP) Du et al. (2019). This process
consists of a finite set of observations X, a set of latent states Z with cardinality Z, a finite set
of actions A with cardinality A, a transition function T : Z × A → ∆(Z), an emission function
q : Z × A → ∆(X), a reward function R : X × A → [0, 1], a horizon H ∈ N, and a start state
distribution μ ∈ ∆(Z). The agent interacts with the environment by repeatedly generating H-step
trajectories (zι, xι,aι,rι, ∙∙∙ , ZH, XH, 0h,『h) where zι 〜μ(∙) and for every h ∈ [H] we have
Xh 〜 q(∙ | Zh), rh = R(Xh, ah), and if h < H, then zh+ι 〜 T(∙ | Zh, ah). The agent does not
observe the states (zι,…，ZH), instead receiving only the observations (xi,…，XH) and rewards
(ri, ,…,『h). We assume that the emission distributions of any two latent states are disjoint, usually
referred as the block assumption: supp(q(∙ | zi)) ∩ supp(q(∙∣z2)) = 0 when zi = z?. The agent
chooses actions using a policy π : X → ∆(A). We also define the set of non-stationary policies
∏ns = ∏h as a H-length tuple, with (∏ι, ∙∙∙ , ∏h) ∈ ∏ns denoting that the action at time step h
is taken as ah,〜∏h(∙ | Xh). The value V(∏) of a policy ∏ is the expected episodic sum of rewards
V(π) := Eπ[PhH=i R(Xh, ah)]. The optimal policy is given by π? = arg maxπ∈ΠNS V(π). We
denote by Ph(X∣∏) the probability distribution over observations X at time step h when following
a policy π . Lastly, we refer to an open loop policy as an element in all AH sequences of actions.
An open loop policy follows a pre-determined sequence of actions {ai, .., aH} for H time steps,
unaffected by state information.
Given the aforementioned definitions, we define an EX-BMDP as follows:
Definition 1 (Exogenous Block Markov Decision Processes). An EX-BMDP is a BMDP such that
the latent state can be decoupled into two parts Z = (s, ξ) where s ∈ S is the endogenous state
and ξ ∈ Ξ is the exogenous state. For Z ∈ Z the initial distribution and transition functions are
decoupled, that is: μ(z) = μ(s)μξ(ξ), and T(z0 | z,a) = T(s0 | s,a)Tξ(ξ0 | ξ).
The observation space X can be arbitrarily large to model which could be a high-dimensional real
vector denoting an image, sound, or haptic data in an EX-BMDP. The endogenous state s captures the
information that can be manipulated by the agent. Figure 1, center, visualizes the transition dynamics
factorization. We assume that the set of all endogenous states S is finite with cardinality S . The
exogenous state ξ captures all the other information that the agent cannot control and does not affect
the information it can manipulate. Again, we make no assumptions on the exogenous dynamics
nor on its cardinality ∣Ξ∣ which may be arbitrarily large. We note that the block assumption of the
EX-BMDP implies the existence of two inverse mappings: φ? : X → S to map an observation to its
endogenous state, and φξ? : X → Ξ to map it to its exogenous state.
Justification of assumptions. The block assumption has been made by prior work (e.g., Du et al.
(2019), Zhang et al. (2020)) to model many real-world settings where the observation is rich, i.e., it
contains enough information to decode the latent state. The decoupled dynamics assumption made
in the EX-BMDP setting is a natural way to characterize exogenous noise; the type of noise that is
not affected by our actions nor affects the endogenous state but may have non-trivial dynamic. This
decoupling captures the movement of ducks, captured in the visual field of the agent in Figure 1, and
many additional exogenous processes (e.g., movement of clouds in a navigation task).
Goal. Our formal objective is reward-free learning. We wish to find a set of policies, we call a
policy cover, that can be used to explore the entire state space. Given a policy cover, and for any
reward function, we can find a near optimal policy by applying dynamic programming (e.g., Bagnell
et al. (2004)), policy optimization (e.g., Kakade and Langford (2002); Agarwal et al. (2020b); Shani
et al. (2020)) or value (e.g., Antos et al. (2008)) based methods.
Definition 2 (α-policy cover). Let Ψh be a finite set of non-stationary policies. We
say Ψh is an α-policy cover for the hth time step if for all Z ∈ Z it holds that
max∏∈Ψh Ph(z∣π) ≥ max∏∈∏Ns Ph(z∣π) 一 α. If ɑ = 0 we call Ψh a policy c^ver.
For standard BMDPs the policy cover is simply the set of policies that reaches each latent state of the
BMDP (Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020a). Thus, for a BMDP, the cardinality
of the policy cover scales with |Z |. The structure of EX-BMDPs allows to reduce the size of the
3
Published as a conference paper at ICLR 2022
policy cover significantly to |S|《|Z| = |S| ∣Ξ∣ when the size of the exogenous state space is large.
Specifically, we show that the set of policies that reach each endogenous state, and do not depend on
the exogenous part of the state is also a policy cover (see Appendix B, Proposition 4).
3	Failures of Prior Approaches
We now describe the limitation of prior RL approaches in the presence of exogenous noise. We
provide an intuitive analysis over here, and defer a formal statement and proof to Appendix A.
Limitation of Noise-Contrastive learning. Noise-contrastive learning has been used in RL to learn
a state abstraction by exploiting temporal information. Specifically, the Homer algorithm (Misra
et al., 2020) trains a model to distinguish between real and imposter transitions. This is done by
collecting a dataset of quads (x, a, x0, y) where y = 1 means the transition was (x, a, x0) was observed
and y = 0 means that (x, a, x0) was not observed. HOMER then trains a model pθ (y | x, a, φθ (x0))
with parameters θ, on the dataset, by predicting whether a given pair of transition was observed
or not. This provides a state abstraction φθ : X → N for exploring the environment. HOMER
can provably solve Block MDPs. Unfortunately, in the presence of exogenous noise, Homer
distinguishes between two transitions that represent transition between the same latent endogenous
states but different exogenous states. In our walk in the park example, even if the agent moves
between same points in two transitions, the model maybe able to tell these transitions apart by looking
at the position of ducks which may have different behaviour in the two transitions. This results in the
HOMER creating O(|Z|) many abstract states. We call this the under-abstraction problem.
Limitation of Inverse Dynamics. Another common approach in empirical works is based on
modeling the inverse dynamics of the system, such as the ICM module of Pathak et al. (2017). In
such approaches, we learn a representation by using consecutive observations to predict the action
that was taken between them. Such a representation can ignore all information that is not relevant for
action prediction, which includes all exogenous/uncontrollable information. However, it can also
ignore controllable information. This may result in a failure to sufficiently explore the environment.
In this sense, inverse dynamics approaches result in an over-abstraction problem where observations
from different endogenous states can be mapped to the same abstract state. The over-abstraction
problem was described at Misra et al. (2020), when the starting state is random. In Appendix A.3 we
show inverse dynamics may over-abstract when the initial starting state is deterministic.
Limitation of Bisimulation. Zhang et al. (2020) proposed learning a bisimulation metric to learn
a representation which is invariant to exogenous noise. Unfortunately, it is known that bisimulation
metric cannot be learned in a sample-efficient manner (Modi et al. (2020), Proposition B.1). Intuitively,
when the reward is same everywhere, then bisimulation merges all states into a single abstract state.
This creates an over-abstraction problem in sparse reward settings, since the agent can falsely merge
all states into a single abstract state until it receives a non-trivial reward.
Bellman rank might depend on
∣Ξ∣.
The Bellman rank was introduced in Jiang et al. (2017) as a
complexity measure for the learnability of an RL problem with function approximations. To date,
most of the learnable RL problems have a small Bellman rank. However, we show in Appendix A that
Bellman rank for EX-BMDP can scale as O(∣Ξ∣). This shows that EX-BMDP is a highly non-trivial
setting as we don’t even have sample-efficient algorithms regardless of computationally-efficient.
In Appendix A we also describe the failures of FLAMBE (Agarwal et al., 2020a)) and autoencoding
based approaches (Tang et al., 2017).
4	Reinforcement Learning for EX-BMDPs
In this section, we present an algorithm Predictive Path Elimination (PPE) that we later show can
provably solve any EX-BMDP with nearly deterministic dynamics and start state distribution of the
endogenous state, while making no assumptions on the dynamics or start state distribution of the
exogenous state (Algorithm 1). Before describing PPE, we highlight that PPE can be thought of as
4
Published as a conference paper at ICLR 2022
Algorithm 1 PPE(δ, η): Predictive Path Elimination
1:	Set Ψ1 = {⊥}, stochasticity level η ≤ 4S1H	//⊥ denotes an empty path
2:	for h = 2, . . . , H do
3:	Set N = 16(∣Ψh-ι ◦ A|)2 log (lFlψJAH)
4:	Collect a dataset D of N i.i.d. tuples (x, U) where U 〜Unf (Ψh-1 ◦ A) and X 〜P(Xh | υ).
5:	Solve multi-class classification problem: f = arg maxf ∈f P(χυ)∈D ln f (idx(υ) | x).
6:	for 1 ≤ i < j ≤ ∣Ψh-ι ◦ A| do
1
7:	Calculate the path prediction gap: ∆(i,j) = NE(X,u)∈d ∖fh(i∖x) - fh(j∣x)l .
8:	If ∆(i,j) ≤ ψ"8°A∣, then eliminate path U with idx(υ) = j. //υ and Uj visit same state
9:	Ψh is defined as the set of all paths in Ψh-1 ◦ A that have not been eliminated in line 8.
a computationally-efficient and simpler alternative to Algorithm 4 of Du et al. (2019) who studied
rich-observation setting without exogenous noise.1
PPE performs iterations over the time steps h ∈ {2, ∙∙∙ ,H}. In the hth iteration, it learns a policy
cover Ψh for time step h containing open-loop policies. This is done by first augmenting the policy
cover for previous time step by one step. Formally, we define Υh = Ψh-1 ◦ A = {π ◦ a ∖ π ∈
Ψh-1, a ∈ A} where π ◦ a is an open-loop policy that follows π till time step h - 1 and then takes
action a. Since we assume the transition dynamics to be near-deterministic, therefore, we know that
there exists a policy cover for time step h that is a subset of Υh and whose size is equal to the number
of reachable states at time step h. Further, as the transitions are near-deterministic, we refer to an
open-loop policy as a path, as we can view the policy as tracing a path in the latent transition model.
PPE works by eliminating paths in Υh so that we are left with just a single path for each reachable
state. This is done by collecting a dataset D of tuples (x, U) where U is a uniformly sampled from
ʌ
Υh and x 〜Ph(X ∖ U) (line 4). We train a classifier fh using D by predicting the index idx(υ) of
the path U from the observation x (line 5). Index of paths in Υh are computed with respect to Υh
ʌ
and remain fixed throughout training. Intuitively, if fh(i ∖ x) is sufficiently large, then we can hope
that the path Ui visits the state φ? (x). Further, we can view this prediction problem as learning a
multistep inverse dynamics model since the open-loop policy contains information about all previous
actions and not just the last action. For every pair of paths in Υh, we first compute a path prediction
gap ∆(line 7). If the gap is too small, we show it implies that these paths reach the same endogenous
state, hence we can eliminate a single redundant path from this pair (line 8). Finally, Ψh is defined
as the set of all paths in Υh which were not eliminated. PPE reduces RL to performing H standard
classification problems. Further, the algorithm is very simple and in practice requires just a single
hyperparameter (N). We believe these properties will make it well-suited for many problems.
Recovering an endogenous state decoder. We can recover a endogenous state decoder φh for
each time step h ∈ {2,3,…，H} directly from fh as shown below:
ʌ	I ʌ	ʌ
φh(x) = min i ∖ fh(i ∖ x) ≥ max fh(j∖ x) -O(1∕∣Υ,∣),i ∈ [∖Υh∖]
Intuitively, this assigns the observation to the path with smallest index that has the highest chance of
visiting x, and therefore, φ? (x). We are implicitly using the decoder for exploring, since we rely on
ʌ
using fh for making planning decisions. We will evaluate the accuracy of this decoder in Section 6.
Recovering the latent transition dynamics. PPE can also be used to recover a latent endogenous
transition dynamics. The direct way is to use the learned decoder φh along with episodes collected
by PPE during the course of training and do count-based estimation. However, for most problems,
recovering an approximate deterministic transition dynamics suffices, which can be directly read
1Alg. 4 has time complexity of O(S4A4H) compared to O(S3A3H) for PPE. Furthermore, Alg. 4 requires
an upper bound on S, whereas PPE is adaptive to it. Lastly, Du et al. (2019) assumed deterministic setting while
we provide a generalization to near-determinism.
5
Published as a conference paper at ICLR 2022
from the path elimination data. We accomplish this by recovering a partition of paths in Ψh-1 × A
where two paths in the same partition set are said to be merged with each other. In the beginning,
each path is only merged with itself. When we eliminate a path υj on comparison with υi in line 8,
then all paths currently merged with υj get merged with υi . We then define an abstract state space
Sbh for time step h that contains an abstract state j for each path υj ∈ Ψh. Further, we recover a
latent deterministic transition dynamics for time step h - 1 as Th-1 : Sh-1 × A → Sh where we set
Th-ι(i, a) = j if the path Uj ∈ Ψh gets merged with path Ui ◦ a ∈ Ψh where Ui ∈ Ψh-1.
Learning a near optimal policy given a policy cover. PPE runs in a reward-free setting. However,
the recovered policy cover and dynamics can be directly used to optimize any given reward function
with existing methods. If the reward function depends on the exogenous state then we can use the
PSDP algorithm (Bagnell et al., 2004) to learn a near-optimal policy. PSDP is a model-free dynamic
programming method that only requires policy cover as input (see Appendix D.1 for details). If
the reward function only depends on the endogenous state, we can use a computationally cheaper
value-iteration VI that uses the recovered transition dynamics. VI is a model-based algorithm that
estimates the reward for each state and action, and performs dynamic programming on the model
(see Appendix D.2 for details). In each case, the sample complexity of learning a near-optimal policy,
given the output of PPE, scales with the size of endogenous and not the exogenous state space.
5	Theoretical Analysis and Discussion
We provide the main sample complexity guarantee for PPE as well as additional intuition for why
it works. We analyze the algorithm in near-deterministic MDPs defined as follows: Two transition
functions Ti and T2 are η-close if for all h ∈ [H],a ∈ A,s ∈ Sh it holds that ∣∣Tι(∙ | s,a) - T2(∙ |
s, a)||i ≤ η. Analogously, two starting distribution μι andμ2 are η-close if ∣∣μι(∙) - μ2(∙)∣∣1 ≤ η.
We emphasize that near-deterministic dynamics are common in real-world applications like robotics.
Assumption 1 (Near deterministic endogenous dynamics). We assume the endogenous dynamics is
η-close to a deterministic model "。用,丁。,") where η ≤ 1/(4SH).
We make a realizability assumption for the regression problem solved by PPE (line 5). We assume
that F is expressive enough to represent the Bayes optimal classifier of the regression problems
created by PPE.
Assumption 2 (Realizability). For any h ∈ [H ] ,and any set ofpaths Y ⊆ Ah with |Y| ≤ SA and
where Ah denotes the set of all paths of length h, there exists fΥ? ,h ∈ F such that: fΥ? ,h (idx(U) |
x) = PRPh(φ1(φ?)(X)))∣υ0)，for all U ∈ Y and X ∈ X with pυ,∈γ Ph(φ*(x)) | υ0) > 0.
Realizability assumptions are common in theoretical analysis (e.g., Misra et al. (2020), Agarwal
et al. (2020a)). In practice, we use expressive neural networks to solve the regression problem, so we
expect the realizability assumption to hold. Note that there are at most AS(H+1) Bayes classifiers for
different prediction problems. However, this is acceptable since our guarantees will scale as ln |F|
and, therefore, the function class F can be exponentially large to accommodate all of them.
We now state the formal sample complexity guarantees for PPE below.
Theorem 1 (Sample Complexity). Fix δ ∈ (0, 1). Then, with probability greater than 1 - δ, PPE
returns a policy cover {Ψh}hH=1 such that for any h ∈ [H], Ψh is a ηH -policy cover for time step h
and ∣Ψh | ≤ S, which g^ves the total number of episodes used by PPE as O (S 2A2H ln IFISAH).
We defer the proof to Appendix C. Our sample complexity guarantees do not depend directly on
the size of observation space or the exogenous space. Further, since our analysis only uses standard
uniform convergence arguments, it extends straightforwardly to infinitely large function classes by
replacing ln |F | with other suitable complexity measures such as Rademacher complexity.
Why does PPE work? We provide an asymptotic analysis to explain why PPE works. Consider a
deterministic setting and the hth iteration of PPE. Assume by induction that Ψh-1 is an exact policy
cover for time step h - 1. Therefore, Yh = Ψh-1 ◦ Ais also a policy cover for time step h. However,
it may contain redundancies; it may contain several paths that reach the same endogenous state. We
now show how a generalized inverse dynamics objective can eliminate such redundant paths.
6
Published as a conference paper at ICLR 2022
(a) Combination lock (H = 2).
(b) Regret plot
AU2n8e 6u-pou"α
(c) Decoding accuracy
Figure 2: Results on combination lock. Left: We show the latent transition dynamics of combination lock.
Observations are not shown for brevity. Center: Shows minimal number of episodes needed to achieve a mean
regret of at most V(n?)/2. Right: State decoding accuracy (in percent) of decoders learned by different methods.
Solid lines implies no exogenous dimension while dashed lines imply an exogenous dimension of 100.
Let Ph(ξ) denote the distribution over exogenous states at time step h which is independent of agent’s
policy. The Bayes optimal classifier (fh? := fΥh,h) of the prediction problem can be derived as:
fh? (idx(υ) | x) := Ph (υ | x)
Ph(X | U)P(U)
Pυ0∈Υh Ph(X I U0)P(υ0)
Ph(X I υ)
Pυ0∈Υh Ph(X I υ0)
(b)	Ph(φ*(χ)) I U)
=Pυ0∈γh Ph(φ*(χ)) I υ0)
where (a) holds since all paths in Υh are chosen uniformly, and (b) critically uses the fact that for
any open-loop policy υ we have a factorization property,
Ph(X | U) = q (X | φ*(x), φ?(x)) Ph(φ*(x) | u)Ph(0?(x)).
Let υ1 , υ2 ∈ Υh be two paths with indices i and j respectively. We define their exact path prediction
gap as ∆(i, j) := Exh [|fh?(i | Xh) - fh?(j | Xh)|]. Assume that υ1 visits an endogenous state s at
time step h and denote ω(s) as the number of paths in Yh that reaches s. Then f?(i | Xh) = 1∕ω(s)
if φ?(Xh) = s, and 0 otherwise. If υ also visits S at time step h, then f?(i | Xh) = f?(j | Xh) for
all Xh. This implies ∆(i,j) = 0 and PPE will filter out the path with higher index since it detected
both paths reach to the same endogenous state. Conversely, let υ2 visit a different state at time step
h. If X is an observation that maps to s, then fh?(i | X) = 1∕ω(s) and fh?(j | X) = 0. This gives
lf?(i I x) — fh(j | x)| = 1∕ω(s) ≥ 1∕∣Υh∣ and, consequently, ∆(i,j) > 0. In fact, We can show
∆(i,j) ≥ O(1∕∣Yh|). Thus, PPE will not eliminate these paths upon comparison. Our complete
analysis in the Appendix generalizes the above reasoning to finite sample setting where we can only
approximate fh? and ∆, as well as to EX-BMDPs with near-deterministic dynamics.
As evident, the analysis critically relies on the factorization property that holds for open-loop policies
but not for arbitrary ones. This is the reason why we build a policy cover with open-loop policies.
6	Experiments
We evaluate PPE on two domains: a challenging exploration problem called combination lock to test
whether PPE can learn an optimal policy and an accurate state decoder, and a visual-grid world with
complex visual representations to test whether PPE is able to recover the latent dynamics.
Combination Lock Experiments. The combination lock problem is defined for a given horizon
H by an endogenous state space S = {s1,a} ∪ {sh,a, sh,b, sh,c}hH=2, an exogenous state space
Ξ = {0, 1}H, an action space A with 10 actions, and a deterministic endogenous start state of s1,a.
For any state sh,g we call g as its type which can be a, b or c. States with type a and b are considered
good states and those with type c are considered bad states. Each instance of this problem is defined
by two good action sequences (ah)hH=2, (a0h)hH=2 with ah 6= a0h, which are chosen uniformly randomly
and kept fixed throughout. At h = 1, the agent is in s1,a and action a1 leads to s2,a, a0h leads to s2,b,
and all other actions lead to s2,c. For h > 2, taking action ah in sh,a leads to sh+1,a and taking action
a0h in sh,b leads to sh+1,b. In all other cases involving taking an action in a state sh,g, we transition to
the next bad state sh+1,c. We visualize the latent endogenous dynamics in Figure 2a. The exogenous
state evolves as follows. We set ξι ∈ {0,1}H where ξι(i)〜Unf ({0,1}) for each i ∈ [H]. At
time step h, ξh is generated from ξh-1 by uniformly flipping each bit in ξh-1 independently with
7
Published as a conference paper at ICLR 2022
probability 0.1. There is a reward of 1.0 on taking the good action aH,a in sH,a and a reward of 0.1
on taking action aH,b in sH,b. Otherwise, the agent gets a reward of 0. This gives a V (π?) = 1, and
the probability that a random open loop policy gets this optimal return is 10-H.
An observation x is generated stochastically from a latent state z = (s, ξ). We map s to a vector w
encoding the identity of the state. We concatenate (w, ξ), add Gaussian noise to each dimension,
and multiply the result by a Hadamard matrix to generate x. See Appendix F for full details. Our
construction is inspired by prior work (Du et al., 2019; Misra et al., 2020).
Baseline. We compare PPE with five baselines on the combination lock problem. These include
PPO (Schulman et al., 2017) which is an actor-critic algorithm, PPO + RND (Burda et al., 2019) which
adds an exploration bonus to PPO using prediction errors, Homer that uses contrastive learning (Misra
et al., 2020), and another algorithm ID which is similar to Homer but instead of contrastive learning
it learns an inverse dynamics model to recover the state abstraction. Lastly, we also compare with
Bisim that learns a bisimulation metric along with an actor-critic agent (Zhang et al. (2020)). We use
existing publicly available codebases for these baselines. Our implementation of PPE very closely
follows the pseudo-code in Algorithm 1. We model F using a two-layer feed-forward network
with ReLU non-linearity. We train F with Adam optimization and use a validation set to do model
selection. We refer readers to Appendix F for additional experimental details.
Results. Figure 2b shows results for values of H in {5, 10, 20, 40}. For each value of H, we plot
the minimal number of episodes n needed to achieve a mean regret of at most V(n?)/2 = 0.5. We
run each algorithm 5 times with different seeds and report the median performance. If an algorithm
is unable to achieve the desired regret in 5 × 105 episodes we set n = ∞. We observe that PPO
is unable to solve the problem at H = 5. PPO + RND is able to solve the problem at H = 5 and
H = 10, showing the exploration bonus induced by random network distillation helps. However,
it is unable to solve the problem for larger values of H. We observe that Homer and ID are also
unable to solve the problem for any value of H. Bisim also fails to solve the problem for any H ≥ 5.
This agrees with the theoretical prediction that Bisim provides no learning signal when running in
sparse-reward settings. In the absence of any reward, the bisimulation objective incentivizes mapping
all observations to the same representation which is not helpful for further exploration. Lastly, PPE is
able to solve the problem for all values of H and is significantly more sample efficient than baselines.
Since the reward function of the combination-lock problem depends only on the endogenous state, we
run PPE and then a value-iteration like algorithm (see Appendix D.2) to learn a near optimal policy.
In order to understand the failure of Homer and ID, we investigate the accuracy of the state abstraction
learned by these methods and compare that with PPE. We focus on the combination lock setting with
H = 2 and evaluate the learned decoder for the last time step. As the state abstraction models are
invariant to label permutation we use the following evaluation metric: given a learned abstraction
for the endogenous state φ : X → [N] We compute 1/m Pm=I 1{Φ(Xi,l) = Φ(Xi,2) ⇔ Φ*(Xi,l)=
φ?(xi,2)}, where {χi,1,χi,2}m=1 are drawn independently from a fixed distribution D with good
support over all states. We report the percentage accuracy in Figure 2c. When there is no exogenous
noise, Homer is able to learn a good state decoder with enough samples while ID fails to learn, in
accordance with the theory. On inspection, we found that ID suffers from the under-abstraction issue
highlighted earlier as it has difficulty separating observations from s3a and s3b . On adding exogenous
noise, the accuracy of Homer plummets significantly. The accuracy of ID also drops but this drop is
mild since unlike Homer, the ID objective is able to filter exogenous noise. Lastly, we observe that
PPE is always able to learn a good decoder and is more sample efficient than baselines.
Visual Grid World Experiments. We test the ability of PPE to recover the latent endogenous
transition dynamics in visual grid-world problem.2 The agent navigates in a N × N grid world where
each grid can contain a stationary object, the goal, or the agent. The agent’s endogenous state is given
by its position in the grid and its direction amongst four possible canonical directions. The agent
can take five different actions for navigation. The world is visible to the agent as a 8N × 8N sized
RGB image. We add exogenous noise as follows: at the beginning of each episode, we independently
sample position, size and color of 5 ellipses. The position and size of these ellipses is perturbed
after each time step independent of the action. We project these ellipses on top of the world’s
image. Figure 3 shows sampled observations from the 7 × 7 gridworld that we experiment on. The
2We use the following popular gridworld codebase: https://github.com/maximecb/gym-minigrid
8
Published as a conference paper at ICLR 2022
(c) Model Errors (d) State decoding accuracy
Figure 3: Results on visual grid world. Left two: Shows sampled observations for the first two steps from the
visual gridworld domain. The agent is depicted as a red-triangle, lava in orange, walls in grey, and the goal in
green. Center Right: Shows errors of type 1 and type 2 made by the PPE in recovering the latent endogenous
dynamics. Right: State decoding accuracy of PPE, Homer and a random uniform decoder. (see Section 6)
(a) Grid World h = 1. (b) Grid World h = 2.
exogenous state is given by the position, size and color of ellipses and is much larger than |S | ≤ 4N2 .
We model F using a two-layer convolutional neural network and train it using Adam optimization.
We defer the full details of setup to Appendix F.
Since the problem has deterministic dynamics, we evaluate the accuracy of the learned transition
model by measuring it in terms of accuracy of the elimination step (Algorithm 1, line 8), since this
step induces our algorithm’s mapping from observations to endogenous latent states. For a fixed
h ∈ {2,…，H}, let Vi and Vj be two paths in Ψh-1 ◦ A. We compute two type of errors. TyPe 1
error computes whether PPE merged these paths, i.e., predicted them as mapping to the same abstract
state, when they go to different endogenous states. Type 2 error computes whether PPE predicted
the paths as mapping to different abstract states, when they map to the same endogenous state. We
report the total number of errors of both types by summing over all values of h and all pairs of
different paths in Ψh-1 ◦ A. Type 1 errors are more harmful, since they can lead to exploration
failure. Specifically, merging paths going to different states may result in the algorithm avoiding
one of the two states when exploring at the next time step. Type 2 errors are less serious but lead to
inefficiency due to using redundant paths for exploration.
Results. We report results on learning the model in in Figure 3c. We see that PPE is able to reduce
the number of type 1 errors down to 0 using 2 × 105 episodes per time step. This is important since
even a single type 1 error can cause exploration failures. Similarly, PPE is able to reduce type 2
errors and is able to get them down to 56 with 5 × 105 episodes. This is acceptable since type 2
errors do not cause exploration failures but only cause redundancy. Therefore, at 2 × 105 samples,
the algorithm makes 0 type 1 errors and just a handful type 2 errors. This is remarkable considering
that PPE compares roughly 2 × 105 pairs of paths in the entire run. Hence, it makes only ≤ 0.03%
type 2 errors. Further, the agent is able to plan using the learned transition model and receive the
optimal return. We also evaluate the accuracy of state decoding on this problem. We compare the
state decoding accuracy of PPE and Homer at H = 2 using an identical evaluation setup to the one
we used for combination lock. Figure 3d shows the results. As expected, PPE rapidly learns a highly
accurate decoder while Homer performs only as well as a random uniform decoder.
7	Conclusion
In this work, we introduce the EX-BMDP setting, an RL setting that models exogenous noise,
ubiquitous in many real-world systems. We show that many existing RL algorithms fail in the presence
of exogenous noise. We present PPE that learns a multi-step inverse dynamics to filter exogenous
noise and successfully explores. We derive theoretical guarantees for PPE in near-deterministic setting
and provide encouraging experimental evidence in support of our arguments. To our knowledge, this
is the first such algorithm with guarantees for settings with exogenous noise. Our work also raises
interesting future questions such as how to address the general setting with stochastic transitions,
or handle more complex endogenous state representations. Another interesting line of future work
direction is the analysis of other approaches that learn multi-step inverse dynamics (Gregor et al.,
2016; Paster et al., 2020) and understanding whether these approaches can also provably solve
EX-BMDPs.
9
Published as a conference paper at ICLR 2022
Acknowlegments
We would like to thank the reviewers for their suggestions and comments. We acknowledge the help
of Microsoft’s GCR team for helping with the compute. YE is partially supported by the Viterbi
scholarship, Technion.
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert E Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, 2014.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity
and representation learning of low rank mdps. Advances in Neural Information Processing Systems,
2020a.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
2020b.
Andrgs Antos, Csaba Szepesvdri, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
2008.
J Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2004.
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-
scale study of curiosity-driven learning. In International Conference on Learning Representations,
2018.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, 2017.
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. On oracle-efficient PAC RL with rich observations. In Advances in Neural Information
Processing Systems, 2018.
Thomas G Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous
state variables and rewards for reinforcement learning. arXiv preprint arXiv:1806.01584, 2018.
Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, 2019.
Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback.
In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, 2019.
Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in
markov decision processes. Artificial Intelligence, 2003.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, 2019.
10
Published as a conference paper at ICLR 2022
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low Bellman rank are PAC-learnable. In International Conference on
Machine Learning, 2017.
Sham M Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning, 2002.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Processing Systems, 2008.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. In International Conference on Machine Learning. PMLR, 2020.
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstrac-
tion and provably efficient rich-observation reinforcement learning. In International conference on
machine learning, pages 6961-6971. PMLR, 2020.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics. PMLR, 2020.
Keiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics
models. In International Conference on Learning Representations, 2020.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017.
Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision
processes. In International Conference on Machine Learning, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, 2020.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman, Filip DeTurck, and Pieter Abbeel. #Exploration: A study of count-based exploration
for deep reinforcement learning. In Advances in Neural Information Processing Systems, 2017.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020.
Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021. 
Appendix
11