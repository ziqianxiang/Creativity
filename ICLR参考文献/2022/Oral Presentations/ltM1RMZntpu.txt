Published as a conference paper at ICLR 2022
Weighted Training for Cross-Task Learning
Shuxiao Chen
University of Pennsylvania
shuxiaoc@wharton.upenn.edu
Koby Crammer
The Technion
koby@ee.technion.ac.il
Hangfeng He
University of Pennsylvania
hangfeng@seas.upenn.edu
Dan Roth
University of Pennsylvania
danroth@seas.upenn.edu
Weijie J. Su
University of Pennsylvania
suw@wharton.upenn.edu
Ab stract
In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted
training algorithm for cross-task learning based on minimizing a representation-
based task distance between the source and target tasks. We show that TAWT is easy
to implement, is computationally efficient, requires little hyperparameter tuning,
and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of
TAWT is corroborated through extensive experiments with BERT on four sequence
tagging tasks in natural language processing (NLP), including part-of-speech (PoS)
tagging, chunking, predicate detection, and named entity recognition (NER). As a
byproduct, the proposed representation-based task distance allows one to reason in
a theoretically principled way about several critical aspects of cross-task learning,
such as the choice of the source data and the impact of fine-tuning.1
1	Introduction
The state-of-the-art (SOTA) models in real-world applications rely increasingly on the usage of weak
supervision signals (Pennington et al., 2014; Devlin et al., 2019; Liu et al., 2019). Among these,
cross-task signals are one of the most widely-used weak signals (Zamir et al., 2018; McCann et al.,
2018). Despite their popularity, the benefits of cross-task signals are not well understood from a
theoretical point of view, especially in the context of deep learning (He et al., 2021; Neyshabur et al.,
2020), hence impeding the efficient usage of those signals. Previous work has adopted representation
learning as a framework to understand the benefits of cross-task signals, where knowledge transfer is
achieved by learning a representation shared across different tasks (Baxter, 2000; Maurer et al., 2016;
Tripuraneni et al., 2020; 2021; Du et al., 2021). However, the existence of a shared representation is
often too strong an assumption in practice. Such an assumption also makes it difficult to reason about
several critical aspects of cross-task learning, such as the quantification of the value of the source
data and the impact of fine-tuning (Kalan & Fabian, 2020; Chua et al., 2021).
In this paper, we propose Target-Aware Weighted Training (TAWT), a weighted training algorithm
for efficient cross-task learning. The algorithm can be easily applied to existing cross-task learning
paradigms, such as pre-training and joint training, to boost their sample efficiency by assigning
adaptive (i.e., trainable) weights on the source tasks or source samples. The weights are determined
in a theoretically principled way by minimizing a representation-based task distance between the
source and target tasks. Such a strategy is in sharp contrast to other weighting schemes common
in machine learning, such as importance sampling in domain adaptation (Shimodaira, 2000; Cortes
et al., 2010; Jiang & Zhai, 2007).
1Our code is publicly available at http://cogcomp.org/page/publication_view/963.
1
Published as a conference paper at ICLR 2022
The effectiveness of TAWT is verified via both theoretical analyses and empirical experiments. Using
empirical process theory, we prove a non-asymptotic generalization bound for TAWT. The bound is a
superposition of two vanishing terms and a term depending on the task distance, the latter of which is
potentially negligible due to the re-weighting operation. We then conduct comprehensive experiments
on four sequence tagging tasks in NLP: part-of-speech (PoS) tagging, chunking, predicate detection,
and named entity recognition (NER). We demonstrate that TAWT further improves the performance of
BERT (Devlin et al., 2019) in both pre-training and joint training for cross-task learning with limited
target data, achieving an average absolute improvement of 3.1% on the performance.
As a byproduct, we propose a representation-based task distance that depends on the quality of
representations for each task, respectively, instead of assuming the existence of a single shared repre-
sentation among all tasks. This finer-grained notion of task distance enables a better understanding of
cross-task signals. For example, the representation-based task distance gives an interpretable measure
of the value of the source data on the target task based on the discrepancy between their optimal
representations. Such a measure is more informative than measuring the difference between tasks via
the discrepancy of their task-specific functions (e.g. linear functions) as done in previous theoretical
frameworks (Tripuraneni et al., 2020). Furthermore, the representation-based task distance clearly
conveys the necessity of fine-tuning: if this distance is non-zero, then fine-tuning the representation
becomes necessary as the representation learned from the source data does not converge to the optimal
target representation.
Finally, we compare our work with some recent attempts in similar directions. Liu et al. (2020) analyze
the benefits of transfer learning by distinguishing source-specific features and transferable features
in the source data. Based on the two types of features, they further propose a meta representation
learning algorithm to encourage learning transferable and generalizable features. Instead of focusing
on the distinction between two types of features, our algorithm and analyses are based on the
representation-based task distance and are thus different. Chua et al. (2021) present a theoretical
framework for analyzing representations derived from model agnostic meta-learning (Finn et al.,
2017), assuming all the tasks use approximately the same underlying representation. In contrast, we
do not impose any a priori assumption on the proximity among source and target representations, and
our algorithm seeks for a weighting scheme to maximize the proximity. Our work is also different
from task weighting in curriculum learning. This line of work tends to learn suitable weights in the
stochastic policy to decide which task to study next in curriculum learning (Graves et al., 2017),
while TAWT aims to learn better representations by assigning more suitable weights on source tasks.
Compared to heuristic weighting strategies in multi-task learning (Gong et al., 2019; Zhang & Yang,
2021), we aim to design a practical algorithm with theoretical guarantees for cross-task learning.
2	TAWT: Target-Aware Weighted Training
2.1	Preliminaries
Suppose we have T source tasks, represented by a collection of probability distributions {Dt }tT=1
on the sample space X × Y, where X ⊆ Rd is the feature space and Y ⊆ R is the label space. For
classification problems, we take Y to be a finite subset of R. We have a single target task, whose
probability distribution is denoted as D0. For the t-th task, where t = 0, 1, . . . , T , we observe nt
i.i.d. samples St = {(xti, yti)}in=t 1 from Dt. Typically, the number of samples from the target task,
n0 , is much smaller than the samples from the source tasks, and the goal is to use samples from
source tasks to aid the learning of the target task.
Let Φ be a collection of representations from the feature space X to some latent space Z ⊆ Rr . We
refer to Φ as the representation class. Let F be a collection of task-specific functions from the latent
space Z to the label space Y. The complexity of the representation class Φ is usually much larger
(i.e., more expressive) than that of the task-specific function class F.
Given a bounded loss function ` : Y × Y → [0, 1], the optimal pair of representation and task-specific
function of the t-th task is given by
(0?,f?) ∈ argmin Lt(φt,ft),	Lt(φt,ft):= E(x,y)〜Dt['(ft ◦ Φt(X),Y)].	(2.1)
φt∈Φ,ft∈F
Note that in general, the optimal representations of different tasks are different. For brevity, all proofs
for the theory part are deferred to the Appx. A.
2
Published as a conference paper at ICLR 2022
2.2	Derivation of TAWT
Under the assumption that the optimal representations {φ:}T=o are similar, a representation learned
using samples only from the source tasks would perform reasonably well on the target task. Conse-
quently, we can devote n0 samples from the target task to learn only the task specific function. This
is a much easier task, since the complexity of F is typically much smaller than that of Φ.
This discussion leads to a simple yet immensely popular two-step procedure as follows (Tripuraneni
et al., 2020; Du et al., 2021). First, we solve a weighted empirical risk minimization problem with
respect to the source tasks:
T	nt
1
(b,{ft}T=ι) ∈ argmin £"/(。,力)，Lbt(φ, ft) ：= — £'(ft ◦ φ(xti),yti),	(2.2)
φ∈Φ,{ft}⊂F t=1	nt i=1
where ω ∈ ∆T -1 is a user-specified vector lying in the T -dimensional probability simplex (i.e.,
T
t=1 ωt = 1 and ωt ≥ 0, ∀1 ≤ t ≤ T). In the second stage, we freeze the representation φ, and
seek the task-specific function that minimizes the empirical risk with respect to the target task:
fb0 ∈ argmin Lb0(φb, f0).	(2.3)
f0∈F
In practice, we can allow φ to slightly vary (e.g., via fine-tuning) to get a performance boost. In
the two-step procedure (2.2)-(2.3), the weight vector ω is usually taken to be a hyperparameter and
is fixed during training. Popular choices include the uniform weights (i.e., ωt = 1/T) or weights
proportional to the sample sizes (i.e., ωt = nt/ PtT0=1 nt0) (Liu et al., 2019; Johnson & Khoshgoftaar,
2019). This reveals the target-agnostic nature of the two-step procedure (2.2)-(2.3): the weights stay
the same regardless the level of proximity between the source tasks and the target task.
Consider the following thought experiment: if we know a priori that the first source task D1 is closer
(compared to other source tasks) to the target task D0 , then we would expect a better performance by
raising the importance of D1, i.e., make ω1 larger. This thought experiment motivates a target-aware
procedure that adaptively adjusts the weights based on the proximity of source tasks to the target. A
natural attempt for developing such a task-aware procedure is a follows:
T
min L0(φ, f0) subject to φ ∈ argmin min	ωtLt(ψ, ft).	(OPT1)
φ∈Φ,f0∈F,	ψ∈Φ {ft}⊂F
ω∈∆T-1	t=1
That is, we seek for the best weights ω such that solving (2.2) with this choice of ω would lead to the
lowest training error when we subsequently solve (2.3).
Despite its conceptual simplicity, the formulation (OPT1) is a complicated constrained optimization
problem. Nevertheless, we demonstrate that it is possible to transform it into an unconstrained form
for which a customized gradient-based optimizer could be applied. To do so, we let (φω, {ftω})
T
be any representation and task-specific functions that minimizes	t=1 ωtLt(φ, ft) over φ ∈ Φ
T
and {ft} ⊂ F. Equivalently, φω minimizes t=1 ωt minft∈F Lt(φ, ft) over φ ∈ Φ. With such
notations, we can re-write (OPT1) as
min	Lb0(φω,f0).	(OPT2)
f0∈F,ω∈∆T-1
The gradient of the above objective with respect to the task-specific function, Vf Lo(φω, fo), is
easy to calculate via back-propagation. The calculation of the gradient with respect to the weights
∂
requires more work, as φω is an implicit function of ω. By the chain rule, we have 怠Lo(φω, fo)=
[VφLbo(φω, fo)]> ∂ωφω. Since φω is a minimizer of φ → PT=I ωt minft∈F Lt(Φ,ft), We have
T
F(φω, ω)=0, ∀ω ∈ ∆τ-1,	F(φ, ω) := Vφ Xωt min Lbt(φ,ft).	(2.4)
t=1	ft∈F
By implicit function theorem, if F(∙, ∙) is everywhere differentiable and the matrix ∂F(φ, ω)∕∂φ is
invertible for any (φ, ω) near some (φ, ωe) satisfying F(φ, ωe) = 0, then we can conclude that the
3
Published as a conference paper at ICLR 2022
Algorithm 1: Target-Aware Weighted Training (TAWT)
Input: Datasets {St }tT=0.
Output: Final pair of representation and task-specific function (φ, f0 ) for the target task.
Initialize parameters ω0 ∈ ∆T-1, φ0 ∈ Φ, {ft0}tT=0 ⊂ F;
for k = 0, . . . , K - 1 do
Starting from (φk, {ftk}tT=1), run a few steps of SGD to get (φk+1, {ftk+1}tT=1);
Use the approximate gradient Nf Lbo(φk+* 1, fo) to run a few steps of SGD from fk to get fk+1;
Run one step of approximate mirror descent (2.7)-(2.8) from ωk to get ωk+1;
end
return φb = φK , fb0 = f0K
map ω 7→ φω is a locally well-defined function near ωe , and the derivative of this map is given by
旦 φω = -( ∂F (φ, ω)	[-1 ( ∂F (φ, ω)
dωt	I dφ	φ=φω)	I dωt	φ=φω
(2.5)
E ♦	1∙ ∕`	.1	1	♦	.1	.	1	1	f .∙	1 X-Z * / I ，，.，、
To simplify the above expression, note that under regularity conditions, we can regard VφLt(φ, ftω)
as a sub-gradient of the map φ 7→ minft∈F Lt(φ, ft). This means that we can write F(φω, ω) =
T
t=1 ωtVφLt(φω, ftω). Plugging this expression back to (2.5) and recalling the expression for
c* / I t.1 C ∖ /C	t ∙ t ∙	.1	1	∙	1	.
∂Lo(φω, fo)∕∂ωt derived via the chain rule, we get
∂	T	-1
d- L0(φωf0) = -[VφLo(φω ,fo)]> [X 3tVφLt(φ3 ,ft)]	∖VφLt(φ3,ft)].	(2.6)
∂ωt	t=1
Now that we have the expressions for the gradients of L0(φ3, f0) with respect to f0 and ω, we can
solve (OPT2) via a combination of alternating minimization and mirror descent. To be more specific,
suppose that at iteration k, the current weights, representation, and task-specific functions are ωk, φk,
and {ftk }tT=0, respectively. At this iteration, we conduct the following three steps:
1. Freeze ωk. Starting from (φk, {ftk}tT=1), run a few steps of SGD on the objective function
(φ, {ft}tT=1) 7→ PtT=13tkLbt(φ,ft) to get (φk+1, {ftk+1}tT=1), which is regarded as an approxi-
mation of (φ3k, {ft3k}tT=1);
2. Freeze (φk+1, {ftk+1}tT=1). Approximate the gradient Vf Lb0(φ3k , f0) by Vf Lb0(φk+1, f0). Us-
ing this approximate gradient, run a few steps of SGD from f0k to get f0k+1;
3. Freeze (φk+1, {fk+1}T=0). Approximate the partial derivative ∂Lbo(φ3k ,f0k+1)∕∂3t by
T	-1
gtk := -[VφLb0(φk+1, f0k+1)]> X 3tV2φLbt(φk+1, ftk+1)	[VφLbt(φk+1, ftk+1)].	(2.7)
t=1
Then run one step of mirror descent (with step size ηk) from ωk to get ωk+1:
3k+1 =	ωk eχp{-ηk gk}
t	PT=I 3k0 eχp{-ηk gko}
(2.8)
We use mirror descent in (2.8), as it is a canonical generalization of Euclidean gradient descent to
gradient descent on the probability simplex (Beck & Teboulle, 2003). Note that other optimization
methods, such as projected gradient descent, can also be used here. The update rule (2.8) has a
rather intuitive explanation. Note that gtk is a weighted dissimilarity measure between the gradients
一 ^ ______^ . .
VφLb0 and VφLbt . This
can further be regarded as a crude dissimilarity measure between the optimal
representations of the target task and the t-th source task. The mirror descent updates 3t along the
direction where the target task and the t-th source task are more similar. The overall procedure is
summarized in Algorithm 1.
A faithful implementation of the above steps would require a costly evaluation of the inverse of
the Hessian matrix PtT=1 3tV2φLbt(φk+1, ftk+1) ∈ Rr×r. In practice, we can bypass this step by
4
Published as a conference paper at ICLR 2022
replacing2 the Hessian-inverse-weighted dissimilarity measure (2.7) with a consine-similarity-based
dissimilarity measure (see Section 4 for details).
The previous derivation has focused on weighted pre-training, i.e., the target data is not used when
defining the constrained set in (OPT1). It can be modified, mutatis mutandis, to handle weighted
joint-training, where we change (OPT1) to
T
min	L0(φ, f0)	subject to φ ∈ argmin min	ωtLt(ψ, ft).	(2.9)
φ∈Φ,f0 ∈F,	ψ∈Φ {ft}⊂F
ω∈∆T	t=0
Compared to (OPT1), we now also use the data from the target task when learning the representation
φ, and thus there is an extra weight ω0 on the target task. The algorithm can also be easily extended
to handle multiple target tasks or to put weights on samples (as opposed to putting weights on tasks).
The algorithm could also be applied to improve the efficiency of learning from cross-domain and
cross-lingual signals, and we postpone such explorations for future work.
3 Theoretical Guarantees
3.1	A Representation-Based Task Distance
In this subsection, we introduce a representation-based task distance, which will be crucial in the
theoretical understanding of TAWT. To start with, let us define the representation and task-specific
functions that are optimal in an "ω-weighted sense" as follows:
T
(φω,{fω}T=ι) ∈ argmin £"4(。, ft)
φ∈Φ,{ft }⊂F t=1
(3.1)
Intuitively, (φω, {fω}) are optimal on the ω-weighted source tasks when only a single representation
is used. Since φω may not be unique, we introduce the function space Φω ⊂ Φ to collect all φω S that
satisfy (3.1). To further simplify the notation, we let L?(φ) := minft∈f Lt(φ, ft), which stands for
the risk incurred by the representation φ on the t-th task. With the foregoing notation, we can write
φω ∈ argminφ∈φ PT=I "tL?(。). The definition of the task distance is given below.
Definition 3.1 (Representation-based task distance). The representation-based task distance between
the ω-weighted source tasks and the target task is defined as
T
dist( X ωtDt, D0) ：= sup L0(φω)-L?(。?),
't=1	φω∈Φω
(3.2)
where the supremum is taken over any φω satisfying (3.1), and φ? is the optimal target representation.
If all the tasks share the same optimal representation, then above distance becomes exactly zero.
Under such an assumption, the only source of discrepancy among tasks arises from the difference in
their task-specific functions. This can be problematic in practice, as the task-specific functions alone
are usually not expressive enough to describe the intrinsic difference among tasks. In contrast, we
relax the shared representation assumption substantially by allowing the optimal representations to
differ and the distance to be non-zero.
The above notion of task distance also allows us to reason about certain important aspects of cross-task
learning. For example, this task distance is asymmetric, capturing the asymmetric nature of cross-task
learning. Moreover, if the task distance is non-zero, then fine-tuning the representation becomes
necessary, because solving (2.2) alone would gives a representation that does not converge to the
correct target φ0? . In addition, an empirical illustration of the representation-based task distance can
be found in Fig. 3 in Appx. B.
This task distance can be naturally estimated from the data by replacing all population quantities with
their empirical version. Indeed, minimizing the estimated task distance over the weights is equivalent
to the optimization formulation (OPT1) of our algorithm (see Appx. A.1 for a detailed derivation).
This observation gives an alternative and theoretically principled derivation of TAWT.
2This type of approximation is common and almost necessary, such as MAML (Finn et al., 2017).
5
Published as a conference paper at ICLR 2022
3.2	Performance Guarantees for TAWT
To give theoretical guarantees for TAWT, we need a few standard technical assumptions. The first one
concerns the Lipschitzness of the function classes, and the second one controls the complexity of the
function classes via uniform entropy (Wellner & van der Vaart, 2013) as follows.
Assumption A (LiPschitzness). The loss function ' : Y × Y → [0,1] is l`-Lipschitz in the first
argument, uniformly over the second argument. Any f ∈ F is LF -Lipschitz w.r.t. the `2 norm.
Assumption B (Uniform entroPy control of function classes). There exist CΦ > 0, νΦ > 0, such
that for any probability measure QX on X ⊆ Rd, we have
N(Φ; L2(Qx)； ε) ≤ (Cφ∕ε)νφ,	∀ε > 0,	(3.3)
where N(Φ; L2(QX); ε) is the L2(QX) covering number of Φ (i.e., the minimum number of L2(QX)
balls3 with radius ε required to cover Φ). In parallel, there exist CF > 0, νF > 0, such that for any
probability measure QZ on Z ⊆ Rr, we have
N (F ； L2(Qz )； ε) ≤ (CF ∕ε)νG,	∀ε > 0,	(3.4)
where N(F; L2(QZ); ε) is the L2(QZ) covering number of F.
Uniform entroPy generalizes the notion of VaPnik-Chervonenkis dimension (VaPnik, 2013) and
allows us to give a unified treatment of regression and classification Problems. For this reason,
function classes satisfying the above assumPtion are also referred to as “VC-tyPe classes” in the
literature (Koltchinskii, 2006). In Particular, if each coordinate of Φ has VC-subgraPh dimension
c(Φ), then (3.3) is satisfied with vφ = Θ(r ∙ c(Φ)) (recall that r is the dimension of the latent space
Z). Similarly, if F has VC-subgraPh dimension c(F), then (3.4) is satisfied with νF = Θ(c(F)).
The following definition characterizes how “transferable” a representation φ is from the ω-weighted
source tasks to the target task.
Definition 3.2 (Transferability). A representation φ ∈ Φ is (ρ, Cρ)-transferable from ω-weighted
source tasks to the target task, ifthere exists ρ > 0,Cρ > 0 such that for any φ ∈ Φω, we have
L0(φ)-L3(φω) ≤ CP(Xωt[L[(φ) -L[(φ)]) "	(3.5)
Intuitively, the above definition says that relative to φω, the risk of φ on the target task can be
controlled by a polynomial of the average risk of φon the source tasks. This can be regarded as an
adaptation of the notions of transfer exponent and relative signal exponent (Hanneke & Kpotufe,
2019; Cai & Wei, 2021) originated from the transfer learning literature to the representation learning
setting. This can also be seen as a generalization of the task diversity assumption (Tripuraneni et al.,
2020; Du et al., 2021) to the case when the optimal representations {。?}乙 do not coincide. In
addition, Tripuraneni et al. (2020) and Du et al. (2021) proved that ρ = 1 in certain simple models
and under some simplified setting.
In this part, we prove a non-asymptotic generalization bound for TAWT. The fact that the weights
are learned from the data substantially complicates the analysis. To proceed further, we make a few
simplifying assumptions. First, we assume that the sample sizes of the source data are relatively
balanced: there exists an integer n such that nt = Θ(n) for any t ∈ {1, . . . , T}. Meanwhile, instead
of directly analyzing (OPT1), we focus on its sample split version. In particular, we let B1 ∪ B2 be a
partition of {1,..., n0}, where |Bi| = Θ(∣B2∣). Define
LOI)(O,	fO)	=	JB-T	X	'(f0 ◦ φ(XOi), yoi),	Lb02)(φ, fO)	=	JB-T	X	'(f0	◦	φ(XOi),	y0i)
|B1 | i∈B1	|B2 | i∈B2
We first solve (OPT1) restricted to the first part of the data:
T
(φb, ωb) ∈	argmin min Lb(O1) (φ, fO)	subject to φ∈ argmin min	ωtLbt(ψ, ft). (3.6)
φ∈Φ,ω∈∆T-1 f0∈F	ψ∈Φ {ft}⊂F t=1
3For two vector-valued functions φ,ψ ∈ Φ, their L2 (Qχ) distance is (R ∣∣φ(x) — ψ(x)k2dQχ(x))1/2.
6
Published as a conference paper at ICLR 2022
Then, we proceed by solving
fb0 ∈ argmin Lb0 (φb, f0).	(3.7)
f0∈F
Such a sample splitting ensures the independence of φ and the second part of target data B2, hence
allowing for more transparent theoretical analyses. Such strategies are common in statistics and
econometrics literature when the algorithm has delicate dependence structures (see, e.g., (Hansen,
2000; Chernozhukov et al., 2018)). We emphasize that sample splitting is conducted only for
theoretical convenience and is not used in practice.
The following theorem gives performance guarantees for the sample split version of TAWT.
Theorem 3.1 (Performance of TAWT with sample splitting). Let (φ, f0) be obtained via solving
(3.6)-(3.7). Let Assumptions A and B hold. In addition, assume that the learned weights satisfy
ω ∈ We := [ω ∈ ∆TT : β-1 ≤ ωt∕ωto ≤ β, ∀t = t0}, where β ≥ 1 is an absolute constant. Fix
δ ∈ (0,1). There exists a constant C = C(L', LF, Cφ, CF) > 0 such that the following holds: if
for any weights ω ∈ We and any representation φ in a Cβ ∙ √(νφlogδ-1)∕nT+(νF- + logT)∕n -
neighborhood of Φω4, there exists a specific φω ∈ Φω such that φω is (ρ, CP)-transferable, then
there exists another C0 = C0(L', LF, Cφ, CF, Cρ, ρ) such that with probability at least 1 一 δ, we
have
1	1
Lo(b,而- Lo(Φ?,/?) ≤ C[(VF +°g≡)2+ β1∕P ( Y产& + 生甘铝厂
T
+ dist X ωbtDt, D0 .	(3.8)
t=1
The upper bound in (3.8) is a superposition of three terms. Let us disregard the log(1∕δ) terms for
now and focus on the dependence on the sample sizes and problem dimensions. The first term, which
scales with VZVF/n, corresponds to the error of learning the task-specific function in the target task.
This is unavoidable even if the optimal representation φ0? is known.
The second term that scales with [(vφ + TVF)∕(nT)]1/2p characterizes the error of learning the
imperfect representation φω from the source datasets and transferring the knowledge to the target
task. Note that this term is typically much smaller than vz(vφ + VF)∕n0, the error that would have
been incurred when learning only from the target data, thus illustrating the potential benefits of
representation learning. This happens because nT is typically much larger than n0 .
The third term is precisely the task distance introduced in Definition 3.1. The form of the task distance
immediately demonstrates the possibility of “matching” φω ≈ φ? via varying the weights ω, under
which case the third term would be negligible compared to the former two terms. For example, in
Appx. A.2, we give a sufficient condition for exactly matching φω = φ?.
The proof of Theorem 3.1 is based on empirical process theory. Along the way, we also establish an
interesting result on multi-task learning, where the goal is to improve the average performance for all
tasks instead of target tasks (see Lemma A.1 in Appx. A). The current analysis can also be extended
to cases where multiple target tasks are present.
4	Experiments
In this section, we verify the effectiveness of TAWT in extensive experiments using four NLP tasks,
including PoS tagging, chunking, predicate detection, and NER. More details are in Appx. B.
Experimental settings. In our experiments, we mainly use two widely-used NLP datasets, Ontontes
5.0 (Hovy et al., 2006) and CoNLL-2000 (Tjong Kim Sang & Buchholz, 2000). Ontonotes 5.0
contains annotations for PoS tagging, predicate detection, and NER, and CoNLL-2000 is a shared
task for chunking. There are about 116K sentences, 16K sentences, and 12K sentences in the
training, development, and test sets for tasks in Ontonotes 5.0. As for CoNLL-2000, there are
4A representation φ is in an ε-neighborhood of Φω if PT=I ωt [L? (φ) — L? (φω)] ≤ ε.
7
Published as a conference paper at ICLR 2022
^ ^	^—_—Target Task Learning Paradigm	PoS	Chunking	Predicate Detection	NER	Avg
Single-Task Learning	-34.37-	-43.05-	6626	-3320-	44.22
Pre-Training	49.43	73.15	74.10	41.22	59.48
Weighted Pre-Training	51.17 ***	73.41	75.77 ***	46.23 ***	61.64
Joint Training	-53.83-	-75.58-	75:42	-43.50-	62.08
Weighted Joint Training	57.34 ***	77.78 ***	75.98 ***	53.44 ***	66.14
Normalized Joint Training	84.14	88.91	77.02	61.15	77.80
Weighted Normalized Joint Training	86.07 ***	90.62 ***	76.67	63.44 ***	79.20
Table 1: The benefits of weighted training for different learning paradigms under different settings.
There are four tasks in total, PoS tagging, chunking, predicate detection, and NER. For each setting,
we choose one task as the target task and the remaining three tasks as source tasks. We randomly
choose 9K training sentences for each source task respectively, because the training size of the
chunking dataset is 8936. As for the target task, we randomly choose 100, 100, 300, 500 training
sentences for PoS tagging, chunking, predicate detection, and NER respectively, based on the
difficulty of tasks. Single-task learning denotes learning only with the small target data. *** indicates
the p-value of the paired sampled t-test is smaller than 0.001.
about 9K sentences and 2K sentences in the training and test sets. As for the evaluation metric, we
use accuracy for PoS tagging, span-level F1 for chunking, word-level F1 for predicate detection,
and span-level F1 for NER. We use BERT5 (Devlin et al., 2019) as our basic model in our main
experiments. Specifically, we use the pre-trained case-sensitive BERT-base PyTorch implementation
(Wolf et al., 2020), and the common hyperparameters for BERT. In the BERT, the task-specific
function is the last-layer linear classifier, and the representation model is the remaining part. As for
cross-task learning paradigms, we consider two popular learning paradigms, pre-training, and joint
training. Pre-training first pre-train the representation part on the source data and then fine-tune the
whole target model on the target data. Joint training uses both source and target data to train the
shared representation model and task-specific functions for both source and target tasks at the same
time. As for the multi-task learning part in both pre-training and joint training, we adopt the same
multi-task learning algorithm as in MT-DNN (Liu et al., 2019). More explanation on the choice of
experimental settings can be found in Appx. C.
Settings for weighted training. For scalability, in all the experiments, we approximate gtk in Eq.
(2.7) by -C X sim(VφLbo(φk+1, fk+1), VφLbt(φk+1 ,fjk+1)), where sim(∙, ∙) denotes the cosine
similarity between two vectors. Note that this type of approximation is common and almost necessary,
such as MAML (Finn et al., 2017). For weighted joint training, we choose c = 1. For weighted
pre-training, we choose the best c among [0.3, 1, 10, 30, 100], because the cosine similarity between
the pre-training tasks and the target task is small in general. In practice, we further simplify the
computation of VφLbt(φk+1, ftk+1) (t = 0, 1, . . . , T) in Eq. (2.7) by computing the gradient over
the average loss of a randomly sampled subset of the training set instead of the average loss of the
whole training set as in Eq. (2.2). In our experiments, we simply set the size of the randomly sampled
subset of the training set as 64, though a larger size is more beneficial in general. In our experiments,
we choose ηk = 1.0 in the mirror descent update (2.8). It is worthwhile to note that there is no need
to tune any extra hyperparameters for weighted joint training, though we believe that the performance
of TAWT can be further improved by tuning extra hyper parameters, such as the learning rate ηk.
Results. The effectiveness of TAWT is first demonstrated for both pre-training and joint training on
four tasks with quite a few training examples in the target data, as shown in Table 1. Experiments
with more target training examples and some additional experiments can be found in Table 2 and
Table 3 in Appx. B. Finally, we note that TAWT can be easily extended from putting weights on tasks
to putting weights on samples (see Fig. 2 in Appx. B).
Normalized joint training. Inspired by the final task weights learned by TAWT (see Table 6 in
Appx. B), we also experiment with normalized joint training. The difference between joint training
and normalized joint training lies in the initialization of task weights. For (weighted) joint training,
the weights on tasks are initialized to be ωt = nt/ PtT0=1 nt0 (i.e, the weights on the loss of each
5While BERT is no longer the SOTA model, all SOTA models are slight improvements of BERT, so our
experiments are done with highly competitive models.
8
Published as a conference paper at ICLR 2022
(％) 一uφluφ>o,lduj-
0
20	21	22	23	24	25	26	27
Ratio
(a) The impact of the ratio between
the training size of the source
dataset and the target dataset.
3 2
(％) 一uφEφ>o,ldE-
(％) 一uφEφ>o,ldE-
The training SiZe of the target dataset
(b) The impact of the training size
of the target dataset (Ratio=8).
(b) The impact of the training size
of the target dataset (Ratio=10).
The training SiZe of the target dataset
Figure 1: Analysis of the weighted training algorithms. We analyze the impact of two crucial factors
on the improvement of the weighted training algorithms: the ratio between the training sizes of the
source and target datasets, and the training size of the target dataset. In this figure, we use NER as
the target task and source tasks include PoS tagging and predicate detection. In the first subfigure,
we keep the training size of the target tasks as 500 and change the ratio from 1 to 128 on a log scale.
In the second (third) subfigure, we keep the ratio as 8 (10) and change the training size of the target
dataset from 500 to 8000 on a log scale. The corresponding improvement from the normalized joint
training to the weighted normalized joint training is shown.
example will be uniform), whereas for the (weighted) normalized joint training, the weights on tasks
are initialized to be ωt = 1/T (i.e., the weight on the loss of each example will be normalized by
the sample sizes). Note that the loss for each task is already normalized by the sample size in our
theoretical analysis (see Eq. 2.2). As a byproduct, we find that normalized joint training is much
better than the widely used joint training when the source data is much larger than the target data.
In addition, TAWT can still be used to further improve normalized joint training. The corresponding
results can be found in Table 1. In addition, we find that dynamic weights might be a better choice in
weighted training compared to fixed weights (see Table 7 in Appx. B).
Analysis. Furthermore, we analyze two crucial factors (i.e., the ratio between the training sizes of
the source and target datasets, and the training size of the target dataset) that affect the improvement
of TAWT in Fig. 1. In general, we find that TAWT is more beneficial when the performance of the
base model is poorer, either because of a smaller-sized target data or due to a smaller ratio between
the source data size and the target data size. More details are in Table 4 and Table 5 in Appx. B.
5	Discussion
In this paper, we propose a new weighted training algorithm, TAWT, to improve the sample efficiency
of learning from cross-task signals. TAWT adaptively assigns weights on tasks or samples in the
source data to minimize the representation-based task distance between source and target tasks. The
algorithm is an easy-to-use plugin that can be applied to existing cross-task learning paradigms,
such as pre-training and joint training, without introducing too much computational overhead and
hyperparameters tuning. The effectiveness of TAWT is further corroborated through theoretical
analyses and empirical experiments. To the best of our knowledge, TAWT is the first weighted
algorithm for cross-task learning with theoretical guarantees, and the proposed representation-based
task distance also sheds light on many critical aspects of cross-task learning.
Limitations and Future Work. There are two main limitations in our work. First, although we
gave an efficient implementation of task-weighted version of TAWT, an efficient implementation of
sample-weighted version of TAWT is still lacking, and we leave it for future work. Second, we are
not aware of any method that can efficiently estimate the representation-based task distance without
training the model, and we plan to work more on this direction. In addition, we also plan to evaluate
TAWT in other settings, such as cross-domain and cross-lingual settings; in more general cases, such
as multiple target tasks in the target data; and in other tasks, such as language modeling, question
answering, sentiment analysis, image classification, and object detection.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This material is based upon work supported by the US Defense Advanced Research Projects Agency
(DARPA) under contracts FA8750-19-2-0201 and W911NF-20-1-0080, NSF through CAREER
DMS-1847415 and an Alfred Sloan Research Fellowship. The views expressed are those of the
authors and do not reflect the official policy or position of the Department of Defense or the U.S.
Government.
References
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
StePhane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic
theory of independence. Oxford university press, 2013.
T Tony Cai and Hongji Wei. Transfer learning for nonParametric classification: Minimax rate and
adaPtive classifier. The Annals of Statistics, 49(1):100-128, 2021.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whit-
ney Newey, and James Robins. Double/debiased machine learning for treatment and structural
Parameters. The Econometrics Journal, 21(1):C1-C68, 2018.
Kurtland Chua, Qi Lei, and Jason D Lee. How fine-tuning allows for effective meta-learning.
Advances in Neural Information Processing Systems, 34, 2021.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for imPortance weighting.
In Nips, volume 10, PP. 442-450. Citeseer, 2010.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deeP
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), PP. 4171-4186, 2019.
Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via
learning the rePresentation, Provably. In International Conference on Learning Representations,
2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaPtation of
deeP networks. In International Conference on Machine Learning, PP. 1126-1135. PMLR, 2017.
Ting Gong, Tyler Lee, Cory StePhenson, Venkata Renduchintala, Suchismita Padhy, Anthony Ndi-
rango, Gokce Keskin, and Oguz H Elibol. A comParison of loss weighting strategies for multi task
learning in deeP neural networks. IEEE Access, 7:141627-141632, 2019.
Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In international conference on machine learning, PP.
1311-1320. PMLR, 2017.
Steve Hanneke and Samory KPotufe. On the value of target data in transfer learning. In Advances in
Neural Information Processing Systems, volume 32, 2019.
Bruce E Hansen. SamPle sPlitting and threshold estimation. Econometrica, 68(3):575-603, 2000.
Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan Roth. Foreseeing the benefits of incidental
suPervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, PP. 1782-1800, 2021.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
10
Published as a conference paper at ICLR 2022
Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes:
the 90% solution. In Proceedings of the human language technology conference of the NAACL,
Companion Volume: Short Papers,pp. 57-60, 2006.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In Proceedings of
the 45th annual meeting of the association of computational linguistics, pp. 264-271, 2007.
Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal
of Big Data, 6(1):1-54, 2019.
MM Kalan and Z Fabian. Minimax lower bounds for transfer learning with linear and one-hidden
layer neural networks. Neural Information Processing Systems, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.
The Annals of Statistics, 34(6):2593-2656, 2006.
Hong Liu, Jeff Z HaoChen, Colin Wei, and Tengyu Ma. Meta-learning transferable representations
with a single target domain. arXiv preprint arXiv:2011.01418, 2020.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for
natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 4487-4496, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 17(1):2853-2884, 2016.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language
decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning?
In Advances in Neural Information Processing Systems, volume 33, pp. 512-523, 2020.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Erik F Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on
Computational natural language learning-Volume 7, pp. 127-132, 2000.
Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. In Advances in Neural Information Processing Systems, volume 33, pp. 7852-7862,
2020.
Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations. In
International Conference on Machine Learning, pp. 10434-10443. PMLR, 2021.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer science & business media,
2013.
Jon Wellner and Aad van der Vaart. Weak convergence and empirical processes: with applications to
statistics. Springer Science & Business Media, 2013.
Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi,
Pierric Cistac, Morgan Funtowicz, Joe Davison, and Sam Shleifer. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38-45, 2020.
11
Published as a conference paper at ICLR 2022
Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 3712-3722, 2018.
Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
Data Engineering, 2021.
12
Published as a conference paper at ICLR 2022
A Omitted Proofs
We start by introducing some notations. For a set S, we let 1S be its indicator function and we
use #S and |S | interchangeably to denote its cardinality. For two positive sequences {an} and
{bn}, we write an . bn or an = O(bn) to denote lim sup an/bn < ∞, and we let an & bn or
an = Ω(bn) to denote bn . a^ Meanwhile, the notation an N bn or an = Θ(bn) means an . bn
and an & bn simultaneously. For a vector x, we let kxk denote its `2 norm. In this section, we
treat l`, LF, Cφ, CF as absolute constants and we hide the dependence on those parameters in our
theoretical results. The exact dependence on those parameters can be easily traced from the proofs.
A.1 TAWT and task distance minimization
T
If we estimate L?(。) = mmf°∈F L0(φ,f0) by mιnf°∈F L0(Φ,f0) and φω ∈ argmmφ∈φ Et=I 3『
T
minft∈F Lt(φ, ft) by the argmin of Et=I ωt ∙ mι□ft∈F Lt(φ,ft), then overall, the quantity L?(。")
can be estimated by
T
min Lb0(φ, f0)	subject to φ ∈ argmin	3tLbt(ψ, ft).
f0∈F	ψ∈Φ t=1
Thus, minimizing the estimated task distance over the weights (note that L?0 (φ?) is a constant) is
equivalent to the optimization formulation (OPT1) of our algorithm.
To further relate TAWT with task distance minimization, we provided an analysis of the two-step
procedure (2.2)-(2.3) with fixed weights. The next theorem gives the corresponding performance
guarantees.
Theorem A.1 (Performance of the two-step procedure with fixed weights). Let (φ, f0) be obtained
via solving (2.2)-(2.3) with fixed ω. Let Assumptions A and B hold. Fix any δ ∈ (0,1) and define
Nω = (PT=I 32/nt)-1. There exists a constant C = C(l`, Lf,Cφ,Cf) > 0 such that the
following holds: if for any representation φ in a C，(νφ + TVF + log δ-1 )∕Nω -neighborhood of
Φω 6 ,there exists a specific φω ∈ Φω such that φ is (ρ, Cρ)-transferable, then there exists another
C0 = C0(L',Lf, Cφ, CF, Cρ,ρ) such that with probability at least 1 一 δ, we have
_1,,
2ρ
,^ ^. . .. ,
L0(φ,f0) -Lo(φ0,f00) ≤ C0
VF + log(1∕δ) [ 2 +
νΦ + TνF + log(1∕δ)
n0
T
+ dist	3tDt,
Nω
(A.1)
Proof. See Appendix A.3.	□
Note that the task distance naturally appears in the upper bound above. This theorem can be regarded
as a predecessor of Theorem 3.1, and we refer readers to Section 3.2 for a detailed exposition on the
meaning of each term in the upper bound.
A.2 A Sufficient Condition FOR Exactly Matching φω = φ?
Proposition A.1 (Weighting and task distance minimization). Suppose that for every φ ∈ Φ, there
exists two source tasks 1 ≤ t1,t2 ≤ T such that L?1 (φ) ≤ L?(。) ≤ L?? (φ). Then there exists a
choice of weights ω, possibly depending on φ, such that dist(PtT=1 3tDt, D0) = 0.
Proof. By construction, it suffices to show the existence of some ω such that
T
X3tL↑(φ) = L0(φ),	∀φ ∈ Φ.	(A.2)
t=1
6Recall that a representation φ is in an ε-neighborhood of Φω if PT=I ωt [L? (φ) — L? (φω)] ≤ ε.
13
Published as a conference paper at ICLR 2022
By assumption, fix any φ ∈ Φ, we can find 1 ≤ t1,t2 ≤ T such that L? (φ) ≤ L?(。)≤ L? (φ).
We set ωt = 0 for any t ∈ {tι, t2}. If L?1 (φ) = L0(φ) = L?2 (φ), then any choice of ω^ ,ωt?2will
suffice. Otherwise, we let
-L?2(0)-L?(。)	_ L?(Φ) —L?i (Φ)
ωt1 = L?2 (φ) -L?1 (φ) ,	ωt2 = L?2 (φ) -L?1 (φ).
It is straightforward to check that such a choice of ω indeed ensures (A.2). The proof is concluded. 口
A.3 Proof of Theorem A.1
We start by stating two useful lemmas.
Lemma A.1 (Error for learning the imperfect representation from source data). Under the setup of
Theorem A.1, there exists a constant Ci = Cι(L', LF, Cφ, CF) > 0 such thatfor any δ ∈ (0,1), we
have	___________________
XXXωt[L?(b) -L7(φω)] ≤ Ci ∙ J +τνF+log(1例	(A.3)
with probability at least 1 - δ.
Proof. See Appendix A.3.1.	口
Lemma A.2 (Error for learning the task-specific function from target data). Under the setup of
TheoremA.1, there exists a constant C2 = C2(L', CF) > 0 such thatfor any δ ∈ (0,1), we have
Lo(b,而-L0(b) ≤ C ∙ SVF + log(1∕δ)	(A.4)
0	n0
with probability at least 1 - δ.
Proof. See Appendix A.3.2.	口
To prove Theorem A.1, we start by writing
^ ^, . . ..	, ^ ^, , , ^ , , ........
Lo(φ,fo) — Lo(Φ0, f0) = Lo(φ, fo) — L?(。) + L?(。) - L0(φω) + £?仍)-L?(。?).
Suppose the two high probability events in Lemmas A.1 and A.2 hold. Then we can bound Lo(φ, fo)-
L0(φ) by (A.4). Meanwhile, since φ is in a C,(νφ + TVF + log δ-i)∕Nω-neighborhood of Φω,
we can invoke the transferability assumption to get
L0(b) -L? (Φω) ≤ CP(X ωt[L0(φ) -L0(φω)])" ≤ CρC7ρ∙ ( "φ + TVF+log(1∕δ) 厂
Moreover, we have the trivial bound that L0(φω) - L0(φ?) ≤ supφω L0(φω) - L?(。?). Assembling
the three bounds above gives the desired result.
A.3. 1 Proof of Lemma A.1
We start by writing
T
X
ωt[L0(φ) -L0(φω)]
t=i
T
X
ωt[Lo(φ,ft) -Lt(φω,ftω)]
t=i
T
X
ωt (Lt(φ,ft) - Lt(φ,ft) + Lt(φ, ft) - Lt(φ3,f) + Lt(φω,ftω) - Lt(φω,ftω)j
14
Published as a conference paper at ICLR 2022
T
X
ωt (Lt(φ,ft) - Lt(φ, fb) + LtWd∕-Lt(φ3,ftω"
≤ SUp	ω∕Q(φ,ft)-Lt(φ,ft) + Lt(φω ,ftω )-Lt(φ3,f))
φ∈Φ,{ft}⊂F
T	1 nt
= sup 52 ωt ∙ - ∑ Lt(φ,ft) - '(fo ◦ φ(xti),yti) + '(ft3 ◦ φ3(xti),yti) - Lt(φ3,
φ∈Φ,{ft}⊂F t=1	nt i=1
where the first inequality is by £?(•) = minft∈F Lt(∙,ft) and the second inequality is by the fact that
(φ, {ft}tT=1) is a minimizer of (2.2). To simplify notations, let zti = (xti, yti) and let the right-hand
side above be G({zti}). Fix two indices 1 ≤ t0 ≤ T, 1 ≤ it0 ≤ nt, and let {zeti} be the source
datasets formed by replacing zt0,it0 with some zet0,it0 = (xet0,it0 , yet0,it0 ) ∈ X × Y. Since {zti} and
{zeti } differ by only one example, we have
nt
^X ^X n Lt(φ,ft)
t6=to i=1 t


-'(ft ◦ φ(χti),yti) +'(ft ◦ Φ3(Xti),yti) + Lt(φ3,f)
+
i6=ito
ωt
Lt0(φ, fto) - '(ft0 ◦ Φ(χto,i), yt0,i) + '(f3 ◦ φ3(χt0,i), yt0,i) + Lto(φ3,fω)
t
+----匚(LtO (φ, fto) - '(fto ◦ φ(xto,ito ), yto,ito ) + '(ft ◦ φ3 (xto,ito ), yto,ito ) + LtO (φ3 ,ft3 )
nto
- G({zeti})
≤ ωo
nto
Lt0(φ,ft0) - '(ft0 ◦ φ(xt0,ii0), yt0,it0)
Lto(Φ3,f3) - '(ft ◦ Φ3"to),yto,ito)
+ Lto (φ, fto) - '(fto ◦ φ(xeto,iio ), yeto,ito )
Lto(Φ3,f3) - '(ft ◦ φ3(Xeto,ito),eto,ito)


≤ 4ωto,
(A.5)
nto
where the last inequality is by the fact that the loss function is bounded in [0, 1]. Taking the supremum
over φ ∈ Φ, {ft} ⊂ F at both sides, we get G({zti}) - G({zeti}) ≤ 4ωto ∕nto . A symmetric
argument shows that the reverse inequality, namely G({zeti}) - G({zti}) ≤ 4ωto ∕nto, is also true.
That is, we have shown
IGHztiD- G({eti})l≤ 于.
This means that we can invoke McDiarmid’s inequality to get
P G({zti}) - E[G({zti})] ≥ ε ≤ exp
for any ε > 0, or equivalently
-2ε2
Pt=I Pn= 1 16ω2∕n2
G({zti}) ≤ E[G({zti})] + 2√2 ∙
八og(1∕δ)
N N
(A.6)
with probability at least 1 - δfor any δ∈ (0, 1). To bound the expectation term, we use a standard
symmetrization argument (see, e.g., Lemma 11.4 in (Boucheron et al., 2013)) to get
-'(ftoφ(χti), yti)+'(ftoφ(χti),yti)
「	T nt
pN3E[G({zti})] ≤ 2pN3E °RsUpkF ΣΣ εti∙ ωt
where the expectation is taken over the randomness in both the source datasets {zti} and the i.i.d. sym-
metric Rademacher random variables {εti}. Consider the function space G := {(φ, {ft}tT=1) : φ ∈
Φ, {ft} ⊂ F}. Let
T nt
Mg := PN3 ∙ XXεti -竺∙ [-'(ft ◦ φ(xti),yti)],	g = (φ, {ft}Lι) ∈ G
nt
t=1 i=1
15
Published as a conference paper at ICLR 2022
be the empirical process indexed by the function space G . Conditional on the randomness in the data
{zti }, this is a Rademacher process with sub-Gaussian increments:
λ2
log Eeλ(Mg-Me) ≤ 工 d2(g, e),	∀λ ≥ 0,g = (φ, {ft}T=ι), e = (φ,{ft }T=ι) ∈ G,
where the pseudometric
d2(g,e) := Nω ∙
`(ft ◦ φ(xti), yti) - `(ft ◦ φ(xti), yti)
2
T2
≤ Nω ^ X = 1.
Thus, we can invoke Dudley’s entropy integral inequality (see, e.g., Corollary 13.2 in (Boucheron
et al., 2013)) to get
E[sup Mg - Mg | {zti}] . Z plogN(G;d; ε)dε,
where g = (φω, {fω}), and N(G； d; ε) is the ε-covering number of G with respect to the PseUdo-
metric d. Taking expectation over the randomness in {zti}, we get
E[G({zti})] . Nω 1/2 ∙ Z √logN(G； d; ε)dε.
0
(A.7)
We now bound the covering number of G . To do so, we define
T	2 nt δ
Q := X NN ∙ ≡ ∙ X ⅛i
nt
δxti,
i=1
where δxti is a point mass at xti. Let {φ(1), . . . , φ(Nε)} ⊂ Φ be an ε-covering of Φ with respect to
L2(Q), where Nε = N(Φ; L2(Q); ε). This means that for any φ ∈ Φ, there exists j ∈ {1, . . . , Nε}
such that
Tn	2
kφ - φ(j)k2L2(Q) :=XXNω ∙ T kφ(Xti)- φj)(Xti)112 ≤ ε .
t=1 i=1	nt
Now for each j ∈ {1, . . . , Nε} and t ∈ {1, . . . , T}, let {ft(j,1), . . . , ft(j,Nε(j))} be an ε-covering of
F with respect to L2(0(j)#Qt), where 0(j)#Qt is the pushforward of Qt by φ(j), and Ny)=
N (F; L2(0(j)#Qt); ε) has no dependence on t due to the uniform entropy control from Assumption
B. This means that for any ft ∈ F, j ∈ {1, . . . , Nε}, there exists k ∈ {1, . . . , Nε(j)} such that
kft - fjkk∖∖2L2(φ("Qt) := — X (ft ◦ φ(j)(Xti)- ftj,k) ◦ φ(j)(Xti))	≤ ε2.
nt i=1
Now, let us fix (φ, {ft}tT=1) ∈ G. By construction, we can find j ∈ {1, . . . , Nε} and kt ∈
{1, . . . , Nε(j)} for any 1 ≤ t ≤ T such that
kφ - φ(j) ∣∣L2(Q) ≤ ε,	kft - f(j,kt) kL2(0(j)#Qt) ≤ ε, ∀1 ≤ t ≤ T.	(A.8)
Thus, we have
d2(φ,{ft}tT=1), (φ(j),{ft(j,kt)}tT=1))
T nt ω2	2
=Nω ∙	Y^n《(ft ◦ Φ(χti), yti) - 0(f"S ◦ Φ(j)(χti), yti))
T nt	2	2
≤ LNω ∙ XX n (ft ◦ Φ(xti) - ft(j,kt) ◦ Φ(j)(χti))
T nt ω2	2
=L2 Nω ∙ £ f T (ft ◦ φ(xti) - ft ◦ φ(j) (xti) + ft ◦ φ(j) (xti) - ftj, Mt) ◦ φ(j)(xti))
t=1 i=1 nt
16
Published as a conference paper at ICLR 2022
T nt 2	T 2	nt	2
≤ 2LQNω XX ω2	∙ LF kφ(xti)	- φj)(xti)k2 + 2LQNω X ωt	∙ ɪ X	(ft。φj(xti)-ftjkt。φj)(xti))
t=1 i=1 nt	t=1 nt nt i=1
T2
=2L2LF kφ - φ(j) kL2(Q) + 2L2 Nω X -L ∙ kft - ftj,自 11：2(0仃)#Qt)
t=1 nt
≤ 2L2 (LF +I)ε2.
This yields
N(G; d; √2L',LF +1 ∙ ε) ≤ ∣∣(φ(j), {ft(j,kt)}T=ι) : 1 ≤ j ≤ Nε, 1 ≤ kt ≤ Nj, ∀1 ≤ t ≤ T
=Nε ∙ (NjT
≤ (与：(CF广
from which we get
log N(G; d; ε) ≤ Vφ log(CφL' Q2(LQF + 1)) + TVF log(Cfl` J2(LF + 1)) + (νφ + Tvf)log(1∕ε)
.(vφ + TVF)(I+ log(1∕ε)).
Plugging the above inequality to (A.7), we get
E[G({zti})] . Nω 1/2 ∙ √VΦ + TVF •(1 + ∣0 1 √i⅞∏τε)dε) . J"φ NTVF.
The proof is concluded by plugging the above inequality to (A.6).
A.3.2 Proof of Lemma A.2
Since φbis obtained from the source datasets {St}tT=1, it is independent of the target data S0. Through-
out the proof, we condition on the randomness in the source datasets, thus effectively treating φ as
fixed. Let f0,φb ∈ argminf0∈F L0(φ, f). We start by writing
L0(φb, fb0)	- L0(φb) = L0(φb, fb0)	- Lb0(φb, fb0)	+ Lb0 (φb, fb0) - Lb0(φb, f0,φb) +	Lb0(φb, f0,φb)	- L0(φb, f0,φb)
≤ L0 (φb, fb0)	- Lb0(φb, fb0)	+ Lb0 (φb, f0,φb) - L0 (φb, f0,φb)
≤ sup L0(φb,f0) - Lb(φb,f0) +Lb0(φb, f0 φb) - L0(φb, f0 φb).
f0∈F
The right-hand side above is an empirical process indexed by f0 ∈ F. Using similar arguments as
those appeared in the proof of Lemma A.1, we have
^ . . ^,^ . . ^ ,^
^
sup L0(φ,f0)-L(φ,f0)+L0(φ,f0φb)-L0(φ,f0φb) .
f0∈F
IVF + lθg(1∕δ)
n	no
with probability at least 1 - δ, which is exactly the desired result.
A.4 Proof of Theorem 3.1
The proof bears similarities to the proof of Theorem A.1, with additional complications in ensuring a
uniform control over the learned weights. Let f0,φb ∈ argminf0∈F L0 (φ, f0). Note that
L0(φ,f0 )-L°(φ?,/?)
=L0(φb,fb0)-Lb(02)(φb,fb0)+Lb(02)(φb,fb0)-Lb(02)(φb,f0,φb)
+ Lbo (φ, f0,φ) - L0(φb, f0,φ) + LO(O)-LO(O ) + LO(φ ) - Lo(φ0)
17
Published as a conference paper at ICLR 2022
≤ L0 (φb, fb0) -
b2)(φ,f0) + Lθ2)(φ,f0,b) - Lo(φ,foφ) + L?(。)- £?(。3) + £?(。3) - L0(φ0).
(A.9)
Let ft ∈ argminft∈F Lt(φ, ft). We then have
T
X &t[L?(b)-L?(d。)]
t=1
T
X
ωt[Lt(φ,ft)-Lt(φω,fω)]
t=1
T
X
ω Lt(φ,ft) - Lt(φ, fb) + Lt(φ, ft) - Lt(φ3,f) + Lt(φ3,ft) - Lt(φω,
t=1
T
X
ω Lt(φ,ft) - Lt(φ,ft) + LtW ,ft) - Lt(φ3 ,ft)
t=1
T
≤ 2 sup ][>t Lt(φ, ft) - Lt(φ, ft) + Lt(φ3,f) - Lt(φω
φ∈Φ,{ft}⊂F,ω∈Wβ t=1
Let zti = (xti, yti), and let {zeti} be the source datasets formed by replacing zt0,it0 with zet0,it0 . Let
G({zti})
T
SUp Vωt Lt(φ,ft)- Lt(φ,ft) + Lt(φω ,ft) - Lt(φω
φ∈Φ,{ft}⊂F,t∈Wβ t=1
Then conducting a similar calculation to what led to (A.5), we get
IGHzti}) - G({zti})| ≤ -- . jτL,
nt0	nT
where the last inequality is by the fact that ω ∈ We implies ωt ≤ β∕T for any 1 ≤ t ≤ T. Now,
invoking McDiarmid’s inequality, we get
G({zti}) ≤ EG({zti}) + θ(β Jl0gn(Tδ))
(A.10)
with probability at least 1 - δ. Now, a standard symmetrization argument plus an application of
Dudley’s entropy integral bound (similar to what led to (A.7)) gives
(nT∕β 2)-1/2
Z1
0
,logN(G; d; ε)dε,
(A.11)
where now G := {(φ, {ft}tT=1, ω) : φ ∈ Φ, {ft} ⊂ F,ω ∈ Wβ}, and
d2 (φ,{ft}tT=1,ω), (φe, {fet}tT=1,ωe)
nT T nt	1	2
X ^β∑∑S 三姆卜t'(ft ◦ φ(xti), yti) - ωt'(ft ◦ φ(xti),yti)J
β2	^X ^X	/ 卜(ft	◦ φ(Xti ), yti) - '(ft	◦ φ(Xti)) +	β2 ^X	^X ^2	(ωt	-	eet)2['(ft	◦	φ(Xti), yti)]2
t=1 i=1	t	t=1 i=1 t
T	nt	2
.nτ XX	`(ft ◦ φ(xti), yti) - `(fet ◦ φe(xti)	+T2kω - ωek2,
nT t=1 i=1
where the last inequality is by n X n, ωt ≤ β∕T for any 1 ≤ t ≤ T and β ≥ 1. This means that We
can construct a Cε-covering of G by the following two steps (where C is an absolute constant only
depending on l` and LF): (1) cover the space {(φ, {ft}T=1)} by the same construction as (A.8);
18
Published as a conference paper at ICLR 2022
(2) construct an ε∕T-covering of We with at most (cT∕ε)T many points, where C is an absolute
constant. Overall, We can construct a Cε covering G with (Cφ∕ε)νφ ∙ (Cf/ε)TνF ∙ (cT∕ε)T many
points. Hence, we have
log N (G; d; ε) . (vφ + Tvf )(log(1∕ε) + 1) + T (1 + log T + log(1∕ε))
.(vφ + Tvf)(log(1∕ε) + 1) + TlogT,
where the last inequality is by νF ≥ 1. Plugging this inequality back to (A.10) and (A.11), we get
T	/-----；--7—77；--------；-TTT
X &t[L;@ -)].βj%+nT(1∕δ)+VF#空.
with probability at least 1 - δ . This means that under this high probability event, we can invoke the
transferability assumption to conclude the existence of a specific φω with
L?(b)- £?您).CP" ∙ ( vφ+l0g(1∕δ) + VF^ogɪ )1/2：
nT	n
Recalling (A.9), we arrive at
^ ^ , . . ..
L0(φ,f0) -Lo(φ0,fO)
≤Lo(Φ,fo)-L2)(Φ,f0) + Lf (Φ,fo b)-L0(Φ,f0 3)+supL0(φω)-L0(Φ0)
,	,	Φω
+ o (Cρβi∕ρ ∙ (vφ +对⑷ + VF Yog T
≤ SUp [L0(φ,f0) - Le(φ,fo) + L02)(b,f0 b) - L0(φ,f0 3)] + SUpL0(φω) - L?(。?)
fo∈F[	)	φω
+ o (Cρβi∕ρ ∙ (vφ +%10 + VF Yog T)1/2)
with probability 1 - δ. Since φis independent of the second batch of target data {(x0i, y0i) : i ∈ B2}
and |B2 |	n0, a nearly identical argument as that appeared in the proof of Lemma A.2 gives
；学 ∣L0(b,f0) - L⑵(b,f0)+b2)(b,f0,3) - L0(b,f0,3)}. SVF+n：1”)
with probability at least 1 - δ. We conclude the proof by invoking a union bound.
B Experimental Details and Additional Results
Target Task Learning Paradigm J^ J^一■一	一	PoS	Predicate Detection	NER	Avg
Single-Task Learning	-85.06-	71:51	-54.96-	70.51
Pre-Training	-88.66-	7857	-5822-	75.15
Weighted Pre-Training	89.31 ***	79.21 ***	60.09 ***	76.20
Joint Training	-87.23-	74:90	-5955-	73.89
Weighted Joint Training	90.71 ***	76.29 ***	64.49 ***	77.16
Table 2: Compared to Table 1, more training examples are considered here. Here we only consider
the three tasks in Ontonotes 5.0, i.e., PoS tagging, predicate detection, and NER. For each setting, we
choose one task as the target task and the remaining two tasks as source tasks. We randomly choose
20K training sentences for each source task respectively. As for the target task, we randomly choose
500, 600, 1000 training sentences for PoS tagging, predicate detection, and NER respectively.
In this section, we briefly highlight some important settings in our experiments. More details can be
found in our released code. It usually costs about half an hour to run the experiment for each setting
(e.g. one number in Table 1) on one GeForce RTX 2080 GPU.
19
Published as a conference paper at ICLR 2022
Setting Learning Paradigm	PoS + NER	PoS + Chunking + NER	PoS + Chunking + Predicate Detection	Avg
Single-Task Learning	6847	6847	7800	71.65
Joint Training	65:70	67712	798	70.88
Weighted Joint Training	69.58		69.73			80.38		73.23
Table 3: Additional experiments to illustrate the benefits of weighted joint training compared to joint
training. For each setting, the task in the bold text is the target task and the remaining tasks are source
tasks. The improvement from weighted training in the joint training paradigm under various settings
indicates the effectiveness of TAWT.
Ratio Learning Paradigm ~~~~~~,~~~,~~~,``Ι~~~	1	2	4	8	16	32	64	128	Avg
Single-Task Learning	30.08	30.08	30.08	30.08	30.08	30.08	30.08	30.08	30.08
Normalized Joint Training	38.55	36.53	42.37	54.18	62.08	63.58	62.85	62.33	52.81
Weighted Normalized Joint Training	42.31	42.13	51.64	59.08	63.46	64.86	64.08	63.65	56.40
Upper Bound	62.69	70.31	75.65	79.39	81.59	83.19	84.18	-	-
Table 4: The performance for the analysis on the ratio between the training sizes of the source and
target datasets in Fig. 1. In this part, we use NER as the target task and source tasks include PoS
tagging and predicate detection. We keep the training size of the target task as 500 and change the
ratio from 1 to 128 on a log scale. Single-task learning denotes learning only with the small target
data, and upper bound denotes learning with the target data that has the same size as the overall size
of all datasets in cross-task learning. "-" means that we do not have enough training data for the upper
bound in that setting. The sampling strategy7 for training examples we used for the analysis is a little
different from that used in the experiments in Table 1, so that the performance of single-task learning
for the same setting can be a little different.
Data. In our experiments, we mainly use two widely-used NLP datasets, Ontontes 5.0 (Hovy et al.,
2006) and CoNLL-2000 (Tjong Kim Sang & Buchholz, 2000). Ontonotes 5.0 is a large multilingual
richly annotated corpus covering alotofNLP tasks, and we use the corresponding English annotations
for three sequence tagging tasks, PoS tagging, predicate detection, and NER. CoNLL-2000 is a shared
task for another sequence tagging task, chunking. There are about 116K sentences, 16K sentences,
and 12K sentences in the training, development, and test sets for tasks in Ontonotes 5.0. The average
sentence length for sentences in Ontonotes 5.0 is about 19. As for CoNLL-2000, there are about 9K
sentences and 2K sentences in the training and test sets. The corresponding average sentence length
is about 24.
Tasks. We consider four common sequence tagging tasks in NLP, PoS tagging, chunking, predicate
detection, and NER. PoS tagging aims to assign a particular part of speech, such as nouns and
verbs, for each word in the sentence. Chunking divides a sentence into syntactically related non-
overlapping groups of words and assigns them with specific types, such as noun phrases and verb
phrases. Predicate detection aims to find the corresponding verbal or nominal predicates for each
sentence. NER seeks to locate and classify named entities mentioned in each sentence into pre-defined
categories such as person names, organizations, and locations. Based on the above datasets, there
are 50 labels in PoS tagging, 23 labels in chunking, 2 labels in predicate detection, and 37 labels in
NER. As for the evaluation metric, we use accuracy for PoS tagging, span-level F1 for chunking,
token-level F1 for predicate detection, and span-level F1 for NER.
The model. We use BERT as our basic model in our main experiments. Specifically, we use the pre-
trained case-sensitive BERT-base PyTorch implementation (Wolf et al., 2020). We use the common
parameter settings for BERT. Specifically, the max length is 128, the batch size is 32, the epoch
number is 4, and the learning rate is 5e-5. In the BERT model, the task-specific function is the
last-layer linear classifier, and the representation model is the remaining part.
7Specifically, in the main experiments, we first randomly sample 9K sentences and then randomly sample
a specific number of sentences (e.g. 500 sentences for NER) among the 9K sentences for the training set of
the target task. In the analysis, we first randomly sample 100K sentences and then randomly sample a specific
number of sentences (e.g. 500 sentences for NER) among the 100K sentences for the training set of the target
task. We use a different sampling strategy for training sentences in main experiments and analysis, simply
because the largest training size of the data we considered in the two situations is different.
20
Published as a conference paper at ICLR 2022
Target Size Leaming Paradigm	^—■— 		500	1000	2000	4000	8000	Avg
Single-Task Learning	30.08	54.26	68.05	74.77	79.20	61.27
Normalized Joint Training (Ratio=8)	54.18	64.69	71.33	76.95	80.37	69.50
Weighted Normalized Joint Training (Ratio=8)	59.08	66.00	72.30	76.95	80.64	70.99
Upper Bound	79.39	81.49	83.53	84.26	-	-
Normalized Joint Training (Ratio=10)	58.06	65.94	71.48	77.33	80.34	70.63
Weighted Normalized Joint Training (Ratio=10)	60.96	67.45	72.52	77.44	79.94	71.66
Upper Bound	79.70	82.32	83.39	84.84	-	-
Table 5: The performance for the analysis on the training size of the target dataset in Fig. 1. In this
part, we use NER as the target task and source tasks include PoS tagging and predicate detection.
We keep the ratio between the training sizes of the source and target datasets as 8 (10) and change
the training size of the target dataset from 500 to 8000 on a log scale. Single-task learning denotes
learning only with the small target data, and upper bound denotes learning with the target data that
has the same size as the overall size of all datasets in cross-task learning. "-" means that we do not
have enough training data for the upper bound in that setting.
'	~~~~~~_______	Target Task Learning Paradigm	~~~~ ~~	PoS	Chunking	Predicate Detection	NER
Weighted Pre-Training	(0.68, 0.01, 0.31)	(0.32, 0.30, 0.37)	-(0.33, 0.35, 0.32)-	-(0.92, 0.05, 0.04)-
Weighted Joint Training	(0.04, 0.04, 0.03) 0.89	(0.04, 0.04, 0.05) 0.87	-(0.04, 0.04, 0.05)- 0.87	-(0.04, 0.04, 0.05)- 0.87
Weighted Normalized Joint Training	(0.0005, 0.0004, 0.0004) 	0.9986		(0.0005, 0.0005, 0.0004) 	0.9987		(0.002, 0.002, 0.002) 	0.995		(0.003, 0.003, 0.003) 	0.990	
Table 6: The final learned weights on tasks in the experiments in Table 1. For each setting, the final
weights on three source tasks are represented by a three-element tuple. The source tasks are always
organized in the following order: PoS tagging, chunking, predicate detection, and NER. For example,
the tuple (0.68, 0.01, 0.31) for the target task Pos tagging in the weighted pre-training indicate that
the final weights on the three source tasks (chunking, predicate detection, NER) are 0.68, 0.01, and
0.31 respectively. As for TAWT in the joint training, we have extra weight on the target task, which is
bold in the second line.
Cross-task learning paradigms. In our experiments, we consider two cross-task learning paradigms,
pre-training, and joint training. Pre-training first pre-trains the representation part on the source data
and then fine-tunes the whole target model on the target data. Joint training uses both source and
target data to train the shared representation model and task-specific functions for both source and
target tasks at the same time. As for the multi-task learning part in both pre-training and joint training,
we adopt the same multi-task learning algorithm as in MT-DNN (Liu et al., 2019).
Experiments with more target training examples. Compared to the main experiments in Sec. 4,
we further experiment with more training examples in the target data for three tasks in Ontonotes
5.0. Without chunking, we also use more training examples in the source tasks. As shown in Table
2, we can see that TAWT is still beneficial even with more training examples, though the relative
improvement of TAWT is smaller compared to that with fewer training examples.
Additional experiments to illustrate the benefits of TAWT. In this part, we conduct additional ex-
periments with TAWT on the joint training with fewer source tasks compared to the main experiments
in Sec. 4. For simplicity, we make the target task bold to distinguish it from source tasks for each
setting. For example, in the setting of PoS + Chunking + NER, NER is the target task and the other
two tasks are source tasks. For PoS + NER, we randomly sample 20K training sentences for PoS
tagging and 2K training sentences for NER. For PoS + Chunking + NER, we randomly sample
20K, 9K8 and 2K training sentences for PoS tagging, chunking, and NER respectively. As for PoS
+ Chunking + Predicate Detection, we randomly sample 20K, 9K, and 2K training sentences for
PoS tagging, chunking, and predicate detection respectively. As shown in Table 3, we can see that
TAWT is still beneficial under these diverse settings, which is a complement for our main experiments
in Sec. 4.
8We choose 9K training sentences for chunking because the training size of the chunking dataset is 8936.
21
Published as a conference paper at ICLR 2022
3ZZ3Z~	Target Task Learning Paradigm	'~~~'~~~,```∙~~~	PoS	Chunking	Predicate Detection	NER	Avg
Weighted Pre-Training (fixed weights)	49.17	-73.05-	74126	39.84	59.08
Weighted Pre-Training (dynamic weights)	51.17	73.41	75.77	46.23	61.64
Weighted Normalized Joint Training (fixed weights)	86.93	-90.12-	74:90	63.60	78.89
Weighted Normalized Joint Training (dynamic weights)	86.07	90.62	76.67	63.44	79.20
Table 7: Comparison between dynamic weights and fixed final weights. There are four tasks in total,
PoS tagging, chunking, predicate detection, and NER. For each setting, we choose one task as the
target task and the remaining three tasks are source tasks.
Figure 2: Extension to weighted-sample training. The weighted training algorithm can be easily
extended from the weights on tasks to the weights on samples. As for the experiments on weighted-
sample training, we use the PoS tagging on entity words as the source task and named entity
classification on entity words as the target task. Note that the settings for the weighted-sample
training are quite different from those for the weighted-task training in the remaining parts because
the weighted-sample training is much more costly compared to the weighted-task training.
Analysis of some crucial factors. As shown in Fig. 1, we analyze two crucial factors that affect the
improvement of TAWT. First, in general, we find that the improvement of TAWT in normalized joint
training is larger when the ratio between the training sizes of the source and target dataset is smaller,
though the largest improvement may not be achieved at the smallest ratio. Note that the improvement
vanishes with a large ratio mainly because the baseline (normalized joint training) is already good
enough with a large ratio. Second, the improvement from TAWT will decrease with the increase of the
training size of the target data. In a word, TAWT is more beneficial when the performance of the base
model is poorer, either with a smaller target data or with a smaller ratio. The specific performance for
experiments in our analysis on two crucial factors for TAWT, the ratio between the training sizes of
the source and target datasets, and the training size of the target dataset, can be found in Table 4 and
Table 5 respectively.
Dynamic weights analysis. As for experiments in Table 1, there are four tasks in total, PoS tagging,
chunking, predicate detection, and NER. For each setting, we choose one task as the target task and
the remaining three tasks are source tasks. The learned final weights on tasks are shown in Table 6.
To better understand our algorithm, we compare TAWT with dynamic weights and TAWT with fixed
final weights. TAWT with dynamic weights is the default setting for our algorithm, where weights on
tasks for each epoch are different. As for TAWT with fixed weights, we simply initialize the weights
as the final weights learned by our algorithm and fix the weights during the training. As shown
in Table 7, we find that TAWT with dynamic weights (the default one) is slightly better than TAWT
with final fixed weights. It indicates that fixed weights might not be a good choice for weighted
training, because the importance of different source tasks may change during the training. In other
words, it might be better to choose the weighted training with dynamic weights, where the weights
are automatically adjusted based on the state of the trained model.
Extension to the weighted-sample training. As shown in Fig. 2, TAWT can be easily extended from
the weights on tasks to the weights on samples. We can see that TAWT based on weighted-sample
training is also beneficial for both pre-training and joint training. Because the weighted-sample
training is much more costly compared to the weighted-task training, we choose a much simpler
22
Published as a conference paper at ICLR 2022
Figure 3: Illustration for the task distance. We find that the source data is more beneficial when the
task distance between the source data and the target data is smaller or the size of the target
data is smaller. In this part, we keep the training size of the source task as 10000 and change
the training size of the target task from 10 to 10000 in a log scale. STL denotes the single-task
learning only with the target data. LTL denotes the learning to learn paradigm, where we first learn
the representations in the source data and then learn the task-specific function in the target data. For
the learning to learn paradigm, we consider the source task with different flip rates from 0.0 to 1.0,
where the flip rate is an important factor in generating the source data and lower flip rate indicates a
smaller task distance between the source data and the target data.
setting here. In this part, we use the PoS tagging on entity words as the source task and named
entity classification on entity words as the target task. Note that both tasks we considered here are
word-level classification tasks, i.e., predicting the label for a given word in a named entity. We still
use the Ontontes 5.0 (Hovy et al., 2006) as our dataset. There are 50 labels in PoS tagging on entity
words, and 18 labels 9 in named entity classification on entity words. There are 37534 examples
in the development set and 23325 examples in the test set for both source and target tasks. As for
the training set, we randomly sample 1000 examples for the source task and 100 examples for the
target task. As for the model, we use two-layer NNs with 5-gram features. The two-layer NNs have a
hidden size of 4096, ReLU non-linear activation, and cross-entropy loss. As for the embeddings, we
use 50 dimensional Glove embeddings (Pennington et al., 2014). The majority baseline for named
entity classification on entity words on the test data is 20.17%. As for training models, the size of the
training batch is 128, the optimizer is Adam (Kingma & Ba, 2015) with a learning rate 3e-4, and
the number of training epochs is 200. As for updating weights on samples, we choose to update the
weights every 5 epoch with the mirror descent in Eq. 2.8 and thus 40 updates on weights in total. In
IPT=I ωtVφ:Lt(φk^1,fk+1)]
this part, we approximate the inverse of the Hessian matrix (
-1
) in
Eq. 2.7 by a constant multiple of the identity matrix as in MAML (Finn et al., 2017), and choose the
constant as 5. According to our experiments, the results of this approximation are similar to those
of the approximation that we used in Sec. 4. The corresponding results are shown in Fig. 2. In the
future, we also plan to group the instances and give each group a weight rather than each sample.
Settings for simulations on task distance. In this part, we first randomly generate 10K examples
D, where the dimension of the inputs are 1000 and the corresponding labels are in 0 - 9 (10 classes).
For each dimension of the input, it is randomly sampled from a uniform distribution over [-0.5, 0.5].
For each flip rate q% ∈ [0, 1], we randomly choose q% of the data and replace their labels with a
random label uniformly sampled from the 10 classes. The data with flip rate q% is denoted as Dq%
(including D0.0 for the original dataset). For each dataset Dq%, we train a 2-layer NNs Mq% with
100% accuracy and almost zero training loss on the dataset. The 2-layer NNs can then be used to
generate data for the task Tq%. We use the T0.0 as the target task and change q% from 0.0 to 1.0 as
9In NER, we have 37 labels because each type of entity have two variants of labels (B/I for beside/inside)
and one more extra-label O for non-entity words is also considered.
23
Published as a conference paper at ICLR 2022
the source task. Based on the process of the data generation, we can see that the flip rate q% plays
an important role in the intrinsic difference between the source task Tq% and the target task T0.0. In
general, we can expect that a smaller flip rate q% indicates a smaller task distance between the source
task Tq% and the target task T0.0 . This data generation process is inspired by the teacher-student
network in (Hinton et al., 2015). As shown in Fig. 3, we keep the training size of the source task as
10000 and change the training size of the target task from 10 to 10000 in a log scale. For simplicity,
we use the same architectures for both the teacher network that we used for generating data and the
student network that we used for learning the data, i.e., a two-layer neural network with a hidden
size of 4096, an input size of 1000, and an output size of 10. As for training, the size of the training
batch is 100, the optimizer is Adam (Kingma & Ba, 2015) with learning rate 3e-4, and the number
of training epochs is 100.
Details on using existing assets. As for the corresponding code and pre-trained models for BERT,
we directly download it from the Github of huggingface whose license is Apache-2.0, and more
details can be found in https://github.com/huggingface/transformers. As for the
Ontonotes 5.0, we obtain the data from LDC. The corresponding license and other details can
be found in https://catalog.ldc.upenn.edu/LDC2013T19. As for the CoNLL-2000
shared task, we download the data directly from the website and more details are in https:
//www.clips.uantwerpen.be/conll2000/chunking.
C On the Choice of Experimental Settings
In this part, we clarify the choice of experimental settings based on the following perspectives.
Signal selection: TAWT can also be applied to cross-domain or cross-lingual signals. Compared to
the above two types of signals, we think cross-task signals are more widespread and more difficult to
be used efficiently. Therefore, in our experiments, we choose the most difficult and crucial signals,
cross-task signals, to verify the effectiveness of TAWT. Another reason is that we didn’t find any
existing weighted training algorithms with theoretical guarantees for cross-task learning, while
weighted training is common for domain adaptation, such as importance sampling.
The choice of similar domains: We choose to select tasks with similar domains. Ideally, TAWT can
be used for cross-task signals in different domains or languages. However, the gap between different
domains or different languages can make cross-task learning more complicated. We instead consider
cross-task learning in similar domains of English to simplify the settings. But similar domains didn’t
mean that datasets overlap because we randomly sample sentences for different tasks independently
without repetition, causing a small ratio of overlap in our case.
Area selection: We choose to experiment with NLP tasks because we think cross-task signals are
more common in the NLP area and NLP tasks are more diverse compared to tasks in other areas.
Dataset selection: We choose Ontonotes 5.0 as our main dataset because it is widely used, and
provides large-scale expert annotations (on 2.9 million words) for a wide range of NLP tasks. This
enables us to focus on learning with various tasks in similar domains. As for CoNLL-2000, we add it
mainly because we want to analyze the impact of chunking on NER.
Task selection: On the one hand, sequence tagging is more challenging than classification tasks. On
the other hand, the evaluation of sequence tagging is more reliable, compared with generative tasks.
Another reason is that Ontonotes 5.0 mainly cover sequence tagging tasks.
Model selection: We choose to use BERT in our main experiments because BERT is widely used
and all SOTA models are slight improvements of BERT. For weighted-sample training, we instead
choose two-layer NNs with 5-gram features because it is simple and fast. Similar results could be
shown even if the model is more complex, but exhaustive experimentation is not our goal.
The choice of the low-resource setting: There is an intrinsic trade-off between the base performance
of single-task learning and the relevant improvement of cross-task learning. Specifically, if the base
performance of single-task learning is good enough, adding cross-task signals can introduce extra
noise compared to its information. In our experiments, we simply consider a simple low-resource
setting where few target examples are available. Actually, we can still see the effectiveness of TAWT
in cross-task learning as long as the base performance is not too high, but the relative improvement of
TAWT will be smaller compared to that in the low-resource setting.
24