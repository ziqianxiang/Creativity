论文题目,会议名称
 Better fine-tuning by reducing representational collapse, In International Conference onLearning Representations (ICLR)
 Deep learning for segmentation of brain tumors:Impact of cross-institutional training and testing, Med Phys
 The evolution ofout-of-distribution robustness throughout fine-tuning, arXiv
 On the optimization of deep networks: Implicitacceleration by overparameterization, In International Conference on Machine Learning (ICML)
" Bartlett, Philip M", Long
" What do neural ma-chine translation models learn about morphology? In Association for Computational Linguistics(ACL), pp", 861-872
 Two models of double descent for weak features, arXiv
 A new look at an old problem: A universal learningapproach to linear regression, In 2019 IEEE International Symposium on Information Theory(ISIT)
" Lee, and Qi Lei", A theory of label propagation for subpopulation shift
 A simple framework forcontrastive learning of visual representations, In International Conference on Machine Learning(ICML)
" Girshick, and Kaiming He", Improved baselines with momentumcontrastive learning
 An empirical study of training self-supervised visiontransformers, arXiv preprint arXiv:2104
 Functional map of the world, InComputer Vision and Pattern Recognition (CVPR)
 How fine-tuning allows for effective meta-learning, arXivpreprint arXiv:2105
 An analysis of single-layer networks in unsuper-vised feature learning, In Proceedings of the Fourteenth International Conference on ArtificialIntelligence and Statistics
" Du, Wei Hu, Sham M", Kakade
 Algorithmic regularization in learning deep homoge-neous models: Layers are automatically balanced, In Advances in Neural Information ProcessingSystems (NeurIPS)
 fastai tutorial on transfer learning, https://github
 Borrowing treasures from the wealthy: Deep transfer learning throughselective joint fine-tuning, In Computer Vision and Pattern Recognition (CVPR)
" Bach, and Simon Lacoste-Julien", Implicit regularization of discretegradient dynamics in deep linear neural networks
 Understanding the difficulty of training deep feedforward neuralnetworks, In International Conference on Artificial Intelligence and Statistics
 Golub and Charles F, Van Loan
Implicit regularization in matrix factorization, In Advances in Neural Information ProcessingSyStemS(NeurIPS) 
 Surprises in high-dimensional ridgeless least squares interpolation, arXiv preprint arXiv:1903
 Momentum contrast for unsuper-vised visual representation learning, In Computer ViSion and Pattern Recognition (CVPR)
 Using pre-training can improve model robustnessand uncertainty, In International Conference on Machine Learning (ICML)
 Natural adversarialexamples, arXiv preprint arXiv:1907
The many faces of robustness: A critical analysis of out-of-distribution generalization, arXivpreprint arXiv:2006
 Manning, A structural probe for finding syntax in word representa-tions
 Parameter-efficient transfer learning forNLP, arXiv
 Universal language model fine-tuning for text classification,In ASSociation for Computational LinguiSticS (ACL)
" Matthew Davis, David B", Lobell
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principledregularized optimization, In International Conference on Learning RepreSentationS (ICLR)
 Partial transfusion: on the expressive influence of trainablebatch norm parameters for transfer learning, In Medical Imaging with Deep Learning
 Removing spurious features can hurt accuracy and affect groups dis-proportionately, In ACM Conference on FairneSS
" Earnshaw, Imran S", Haque
 Le, Do better imagenet models transfer better? InComputer Vision and Pattern Recognition (CVPR)
 Learning multiple layers of features from tiny images, Technical report
 von Brecht, Deep linear neural networks with arbitrary loss: All localminima are global
 The power of scale for parameter-efficient prompttuning, arXiv preprint arXiv:2104
" Levine, Chelsea Finn, Trevor Darrell, and P", Abbeel
 Prefix-tuning: Optimizing continuous prompts for generation, InAssociation for Computational Linguistics (ACL)
 Explicit inductive bias for transfer learning withconvolutional networks, In International Conference on Machine Learning (ICML)
 The generalization error of random features regression: Preciseasymptotics and double descent curve, arXiv preprint arXiv:1908
 Accuracy on the line: on the strong correlationbetween out-of-distribution and in-distribution generalization, In International Conference onMachine Learning (ICML)
 Harmless interpo-lation of noisy data in regression, IEEE Journal on Selected Areas in Information Theory
 In search of the real inductive bias: On therole of implicit regularization in deep learning, arXiv
 Moment matchingfor multi-source domain adaptation, In International Conference on Computer Vision (ICCV)
" Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, andLuke Zettlemoyer", Deep contextualized word representations
 To tune or not to tune? adapting pretrainedrepresentations to diverse tasks, In Proceedings of the 4th Workshop on Representation Learningfor NLP (RepL4NLP-2019)
 Selective entropy optimizationvia committee consistency for unsupervised domain adaptation, In International Conference onComputer Vision (ICCV)
Learning transferable visual models from natural language supervision, In InternationalConference on Machine Learning (ICML)
" Duchi, and Percy Liang", Understandingand mitigating the tradeoff between robustness and accuracy
 Smallest singular value of a random rectangular matrix,Communications on Pureand AppIiedMathematics
 Breeds: Benchmarks for subpopulationshift, arXiv
" Saxe, James L", McClelland
 A theoretical analysis of fine-tuning with linearteachers, In Advances in Neural Information Processing Systems (NeurIPS)
 Class-imbalanced domain adaptation: An empiricalodyssey, arXiv preprint arXiv:1910
 Measuring robustness to natural distribution shifts in image classification, arXiv preprintarXiv:2007
" Jordan, and Chi Jin", On the theory of transfer learning: The importanceof task diversity
 Tropp, An introduction to matrix concentration inequalities
 Avoiding inferenceheuristics in few-shot prompt-based finetuning, arXiv preprint arXiv:2109
 Learning robust global representationsby penalizing local predictive power, In Advances in Neural Information Processing Systems(NeurIPS)
 Robust fine-tuning of zero-shot models, arXivpreprint arXiv:2109
" Zhang, and Christopher R6", Understanding and improving information transferin multi-task learning
In-N-out: Pre-training and self-training using auxiliary information for out-of-distributionrobustness, In International Conference on Learning Representations (ICLR)
 Composed fine-tuning: Freezing pre-traineddenoising autoencoders for improved generalization, In International Conference on MachineLearning (ICML)
 Bdd100k: A diverse driving dataset for heterogeneous multitask learning,In Computer Vision and Pattern Recognition (CVPR)
 A large-scale study of representation learning with the visual task adaptation benchmark,arXiv
 Side-tuning: Abaseline for network adaptation via additive side networks, In European Conference on ComputerVision (ECCV)
 Learning to prompt forvision-language models, arXiv preprint arXiv:2109
 FreeLB: Enhancedadversarial training for natural language understanding, In International Conference on LearningRepresentations (ICLR)
"1 Preliminaries on Important Notations and Principal AnglesBig-Oh Notation: For convenience, we use big-oh notation in a way that differs from standardtheoretical computer science texts", When we say O(<expr1>) we mean that this can be replacedby c <expr1> for some universal constant such that the statement holds
" Since there exists parameters that achieve 0 loss on the training data(namely, B? , v?), this means fine-tuning gets 0 loss on the training data as well", So for all trainingexamples x (that is
