Published as a conference paper at ICLR 2022
Comparing Distributions by Measuring Dif-
ferences that Affect Decision Making
Shengjia Zhao∖ Abhishek Sinha*, Yutong He*, Aidan Perreault, Jiaming Song, Stefano Ermon
Department of Computer Science
Stanford University
{sjzhao,a7b23,kellyyhe,aperr,tsong,ermon}@stanford.edu
Ab stract
Measuring the discrepancy between two probability distributions is a fundamental
problem in machine learning and statistics. We propose a new class of discrepan-
Cies based on the optimal loss for a decision task - two distributions are different
if the optimal decision loss is higher on their mixture than on each individual
distribution. By suitably choosing the decision task, this generalizes the Jensen-
Shannon divergence and the maximum mean discrepancy family. We apply our
approach to two-sample tests, and on various benchmarks, we achieve superior
test power compared to competing methods. In addition, a modeler can directly
specify their preferences when comparing distributions through the decision loss.
We apply this property to understanding the effects of climate change on different
economic activities and selecting features targeting different decision tasks.
1 Introduction
Quantifying the difference between two probability distributions is a fundamental problem in ma-
chine learning. Modelers choose different types of discrepancies (or probability divergences) to
encode their prior knowledge about which aspects are relevant to evaluate the difference. Inte-
gral probability metrics (IPMs, Muller (1997)) and f -divergences (Csiszar, 1964) are widely used
discrepancies in machine learning. IPMs, such as the Wasserstein distance, maximum mean discrep-
ancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012), are based on the idea that if two
distributions are identical, any function should have the same expectation under both distributions.
IPMs are used to define training objectives for generative models (Arjovsky et al., 2017), perform
independence tests (Doran et al., 2014), robust optimization (Esfahani & Kuhn, 2018) among many
other applications. f -divergences, such as the KL divergence and the Jensen Shannon divergence,
are based on the idea that if two distributions are identical, they assign the same likelihood to every
point. One can then define a discrepancy based on how different the likelihood ratio is from one.
KL divergence underlies some of the most commonly used training objectives for both supervised
and unsupervised machine learning algorithms, such as cross entropy loss.
We propose a third category of divergences called H-divergences that overlaps with but also extends
the set of integral probability metrics or the set f -divergences. Intuitively, H-divergence compares
two distributions in terms of the optimal loss for a certain decision task. This optimal loss corre-
sponds to a generalized notion of entropy (DeGroot et al., 1962). Instead of measuring the best
average code length of any encoding scheme (Shannon entropy), the generalized entropy uses arbi-
trary loss function (rather than code length) and set of actions (rather than encoding schemes), and
is defined as the best expected loss among the set of actions. In particular, given two distribution p
and q, we compare the generalized entropy of the mixture distribution (p+ q)/2 and the generalized
entropy of p and q individually. Intuitively, if p and q are different, it is more difficult to minimize
expected loss under the mixture distribution (p + q)/2, and hence the mixture distribution should
have higher generalized entropy; if p and q are identical, then the mixture distribution is identical to
p or q, and hence should have the same generalized entropy.
Our divergence strictly generalizes the maximum mean discrepancy family and the Jensen Shannon
divergence, which can be obtained with specific choices of the loss function. We illustrate this via
* Co-first author
1
Published as a conference paper at ICLR 2022
H-Divergence
Jensen-Shannon	MMD
KL	Wasserstein
f-Divergence	∣pm
Figure 1: Relationship between H-divergence (this paper) and existing divergences. The Jensen
Shannon divergence is an f -divergence but not an IPM; the MMD is an IPM but not always an
f -divergence; both are H-divergences. There are H-divergences that are not f -divergences or IPMs.
the Venn diagram in Figure 1. Our formulation allows us to choose alternative losses to leverage
inductive biases and machine learning models from different problem domains. For example, if we
choose the generalized entropy as the maximum log likelihood of deep generative models, we are
able to leverage recent progress in modeling high dimensional images.
We demonstrate the effectiveness of H-divergence in two sample tests, i.e. to decide whether two
sets of samples come from the same distribution or not. A test based on a probability discrepancy
declares two sets of samples different if their discrepancy exceeds some threshold. We use H-
divergences based on generalized entropy defined by the log likelihood of off-the-shelf generative
models. Compared to state-of-the-art tests based on MMD with deep kernels (Liu et al., 2020), tests
based on the H-divergence achieve better test power (given identical type I error) on a large set of
benchmarks.
More importantly, scientists and policy makers are often interested not only in if two distributions are
different, but how two distributions are different and whether the differences affect decision making.
Typical divergence measures (such as KL) or two sample tests only quantify if two distributions are
different, while we show that H-divergence is a useful tool for quantifying how distributions are
different with three application examples: studying the effect of climate change, feature selection,
and sample quality evaluation. In each of these examples, we compare different aspects of the
distributions by choosing specific decision loss functions. For example, climate change (Figure 3)
might impact agriculture in a region but not energy production, or vice versa. By choosing suitable
loss functions (related to agriculture, energy, etc) we can quantify and test if the change in climate
distribution impact different economic activities.
2	Background
2.1	Probability Divergences
Let X denote a finite set or a finite dimensional vector space, and P(X) denote the set of probability
distributions on X that have a density. We consider the problem of defining a probability divergence
between any two distributions in P(X), where a probability divergence is any function D : P(X) ×
P(X) → R that satisfies D(pkq) ≥ 0, D(pkp) = 0, ∀p, q ∈ P(X). We call the divergence D
“strict” if D(pkq) > 0 ∀p 6= q, and “non-strict“ otherwise. In this paper we consider both types of
divergences.
Integral Probability Metrics Let F denote a set of functions X → R. An integral probability
metric is defined as IPMF(pkq) = supf ∈F |Ep[f(X)] - Eq [f (X)]|. Several important divergences
belong to integral probability metrics. Examples include the Wasserstein distance, where F is the set
of 1-Lipschitz functions; the total variation distance, where F is the set of functions X → [-1, 1].
The maximum mean discrepancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012)
chooses a kernel function k : X × X → R+ and is defined by
MMD2(pkq) = Ep,pk(X, Y) + Eq,qk(X, Y) - 2Ep,qk(X, Y)
MMD is an IPM where F is the unit norm functions in the reproducing kernel Hilbert space (RKHS)
associated with the kernel k .
2
Published as a conference paper at ICLR 2022
f -Divergences Given any convex continuous function f : R+ → R such that f (1) = 0, the
f -Divergence is defined as (assuming densities exist) Df (pkq) = Eq [f (p(X)/q(X))]. Examples
include the KL divergence, where f : t 7→ t log t and the Jensen Shannon divergence, where f :
t → (t + I)Iog (t+2ι) + tlogt.
2.2	H-Entropy
For any action space A and any loss function ` : X × A → R, the H-entropy (DeGroot et al., 1962;
DeGrooL 2005; Grunwald et al., 2004) is defined as
H'(p) = inf Ep['(X,a)]
a∈A
In words, H-entropy is the Bayes optimal loss of a decision maker who must select some action a
not for a particular x, but in expectation for a random x drawn from p(x). H-entropy generalizes
several important notions of uncertainty. Examples include: Shannon Entropy, where A as the set of
probabilities P(X), and '(x, a) = - log a(x); Variance where A = X, and '(x, a) = ∣∣x-ak∣; Pre-
dictive V-entropy, where A ⊂ P(X) is some subset of distributions, and '(x, a) = - log a(x) (XU
et al., 2020).
A key property we will use is that H -entropy is concave (DeGroot et al., 1962).
Lemma 1. For any choice of' : X ×A→ R, h` is a ConcavefUnction.
This Lemma can be proved by observing that inf is a concave function: it is always better to pick an
optimal action for p and q separately rather than a single one for both.
H'(ap +(1 — α)q) = inf (αEp['(X, a)] + (1 — α)Eq['(X, a)])
≥ ainf Ep['(X, a)] + (1 — α) inf Eq['(X, a)] = αH'(p) + (1 — α)H'(q)
This Lemma reflects Why h` can be thought of as a measurement of entropy or uncertainty. If the
distribution is more uncertain (e.g. a mixture ofp and q, rather than p or q separately) then decisions
made under higher uncertainty will suffer a higher loss.
3	Definition and Theoretical Properties
3.1	H-Jensen Shannon Divergence
As a warm up, we present a special case of our divergence.
Definition 1 (H-Jensen Shannon divergence).
DJS(p,q) = H' (号)—2(H'(p) + H'(q))	(1)
DJS is always non-negative because H-entropy is concave (Lemma 1), and clearly DJS (p, q) = 0
whenever P = q. Therefore, DJS is a valid probability divergence. In particular, if We choose H'
as the Shannon entropy, Definition 1 recovers the Jensen Shannon divergence. Other special loss
function choices can recover definitions in (Burbea & Rao, 1982).
3.2	General H-divergence
In addition to the H-Jensen Shannon divergence, there are other functions based on the H-entropy
that satisfy the requirements of a divergence. For example,
DMin = h` P++l) — min(H'(p), H'(q))
(2)
is also a valid divergence (this will be proved later as a special case of Lemma 2). We can define a
general set of divergences that includes the above two divergences with the following definition:
3
Published as a conference paper at ICLR 2022
Definition 2 (H-divergence). For two distributions p, q on X, given any continuous function φ :
R2 → R such that φ(θ, λ) > 0 whenever θ + λ > 0 and φ(0, 0) = 0, define
Dφ(pkq)
Intuitively h` (p++q) - H'(p) and h` (p++q) - H'(q) measure how much more difficult it is to
minimize loss on the mixture distribution (p+ q)/2 than onp and q respectively. φ is a general class
of functions that map these differences into a scalar divergence, while satisfying some desirable
properties described in the next section.
The following proposition shows that the H-divergence generalizes the previous definitions (1) and
(2). Therefore, any property of H-divergence is inherited by e.g. the H-Jensen Shannon divergence.
Proposition 1. If φ(θ, λ) = θ++λ then Dφ(p, q) is the H-Jensen Shannon divergence in Eq.(1). If
φ(θ, λ) = max(θ, λ) then Dφ(p, q) is the H-Min divergence in Eq.(2).
3.3	Properties of the H-divergence
We first verify that Dφ is indeed a (strict or non-strict) probability divergence.
Lemma 2. For any choice of' andfor any choice of φ that satisfy Definition 2, Dφ is non-negative
and Dφ (p, q) = 0 whenever P = q. Furthermore, Dφ is symmetric whenever φ is symmetric.
Depending on the choice of `, H-divergence may or may not be strict (i.e. whenever p 6= q,
D(pkq) > 0). The following proposition characterizes conditions for a strict divergence.
Proposition 2 (Strict Divergence). For any choice of φ the following are equivalent 1) ∀p 6= q,
D'(pkq) > 0. 2) The H-entropy H'(p) := inf a Ep ['(X,a)] is strictly convex in P. 3) Vp = q,
arg inf。Ep ['(X, a)] ∩ arg inf。Eq ['(X, a)] = 0.
In particular, this proposition can be used to characterize all strict H-divergences, because the set of
all losses' that induces strict H-entropy functions h` can be characterized by Fenchel duality (DUChi
et al., 2018).
One important property of the H-divergence is that two distributions have non-zero divergence if and
only if they have different optimal actions, i.e. the optimal solutions for their respective H-entropy
are different. This is shown in the following proposition (proof in Appendix A).
Proposition 3. Dφ (Pkq) > 0 ifand only if arg inf。Ep ['(X, a)] ∩ arg inf。Eq ['(X, a)] = 0.
Intuitively, Dφ only takes into account differences between distributions that lead to different opti-
mal action choices. This property allows us to incorporate prior domain knowledge. By choosing A
and ' we can specify which differences between distributions lead to different optimal actions, and
which differences do not. For example, we can choose A as a set of generative models (e.g., mixture
of Gaussians) and '(x, a) as the negative log likelihood of x under generative model a. If under
two distributions we end up learning the same generative model (by maximizing log likelihood), the
H-divergence between them is zero.
3.4	Relationship to MMD
An important special case of the H-divergence is the set of squared Maximum Mean Discrepencies
(MMD), as shown by the following theorem:
Theorem 1. The set of H-Jensen Shannon Divergences is strictly larger than the MMD2 distances.
To prove this theorem, we show that for each choice of kernel k : X × X → R, there exists an
action space A and loss ' such that the corresponding squared MMD distance and H-divergence are
the same (see proof in Appendix A). In particular, this equivalence can be achieved by choosing A
to be the RKHS H of k(∙, ∙), and '(x, a) = 4∣∣k(x, ∙) - a∣∣H∙ Inclusion is strict because the Jensen
Shannon divergence is a H-Jensen Shannon Divergence but not a squared MMD distance.
4
Published as a conference paper at ICLR 2022
3.5 Estimation and Convergence
Many machine learning tasks can be reduced to the problem of estimating the divergence between
two distributions given samples. Specifically, suppose we are provided with a set ofm i.i.d. samples
p^m = (xι, ∙∙∙ , Xm) drawn from distribution P and ‰ = (x；, ∙∙∙ , xm) drawn from distribution q,
and would like to obtain an estimate of Dtφ(pkq) based on the samples. Here, Pm and qm denote
empirical distributions drawn from p and q respectively. In this section we propose an empirical
estimator for the H-divergence and show that it has favorable convergence properties.
Let Dφ(Pm k^m) be the empirical (random) estimator for Dφ(Pkq) defined by
m	mm	m
inf — X '(χi0,	a)	- inf — X '(xi, a), inf — X '(χi0, a) -	inf —	X '(χi, a)
am	am	am	am
i=1	i=1	i=1	i=1
where x0i0 = xibi + x0i(1 - bi) and bi are i.i.d uniformly sampled from {0, 1}, so that x0i0 is a sample
from the mixture distribution (P + q)/2 of size m.
Using x0i0 as defined above is crucial for the convergence properties we will prove in Theorem 2. It
might be tempting to replace the term m Pm=I '(χi0, a) with ^m Pm=ι('(xi, a) + '(χi, a)) to use
all the available samples. However, optimizing the action based on a finite set of samples (instead of
in expectation) is prone to overfitting, and introduces bias. Intuitively, using m samples (x0i0) ensures
the bias for the mixture is comparable to that of P and q. Without this, Theorem 2 is no longer true,
and empirical performance also degrades.
Before presenting the convergence results, we first must define several assumptions that make con-
vergence possible. In particular, we are going to assume that the loss function ` is C-bounded, i.e.
there exists some C such that 0 ≤ '(x, a) ≤ C, ∀a, x. This assumption seemingly exclude important
special cases such as the Jensen-Shannon divergence (which is associated with the unbounded log
loss). However, we show in the appendix that the Jensen-Shannon divergence cannot be consistently
estimated in general, hence correctly excluded by our theorem. One practical solution is to clip the
log likelihood, which is the approach adopted in (Song & Ermon, 2019) for improved divergence
estimation (for a similar KL divergence estimation problem).
In addition, We assume that φ is 1-Lipschitz under the ∞-norm, i.e. ∣φ(θ + dθ, λ + dλ) - φ(θ, λ) | ≤
max(dθ, dλ), ∀θ, λ, dθ, dλ ∈ R . Both φ(θ, λ) = θ++λ and φ(θ, λ) = max{θ + λ} are 1-Lipschitz
under the ∞-norm. This is a mild assumption because if φ is not 1-Lipschitz we can rescale φ to
make it 1-Lipschitz. Finally, define the Radamacher complexity
Rm(I) = EXi~p,ea ~Uniform({ —1,1})
1m
sup - Tei'(Xi, a)
a∈A m i=1
We define Rm(I) analogously. Based on these assumptions and definitions We can bound the dif-
ference between Dφ(Pm∣∣qm) and Dφ(Pkq).
Theorem 2. IfI is C -bounded, andφ is 1-Lipschitz under the ∞-norm, for any choice of distribution
P, q ∈ P(X) and t > 0 we have
t2m
1.	PrD夕(Pmkqm) ≥ t] ≤ 4e-2C2 if P = q.
2.	Pr [∣DΦ(P^mkqm) - Dφ(Pkq) I ≥ 4max(Rm('), Rm(')) + t] ≤ 4e-2C2
Corollary 1. Jvar[Dφ(Pmkqm)] ≤ 4max(Rm(1),Rm(')) + p2CC2∕m
For proof see Appendix A. Note that when P = q, the convergence of Dφ(Pm kqm) does not depend
on the Radamacher complexity of I, and converges to 0 very quickly. When P 6= q the estimator
Dφ(Pmkqm) is still consistent (under regularity assumptions)
Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a finite
set 2. A is a bounded subset of Rd for some d ∈ N and I is Lipschitz w.r.t. a, then almost surely
iimm→∞ D φ(Pmkqm) = Dφ(P∣∣q).
5
Published as a conference paper at ICLR 2022
For both cases in Corollary 2 the Radamacher complexity Rm(') goes to zero (as sample size
m → ∞) at a rate of O(1∕√m). In other words We can conclude that the estimation error in
Theorem 2 is bounded by O(1 /√m) and the variance of the estimator is also bounded by O(1 /√m)
when the sample size m → ∞.
4	Experiment: Two Sample Test
The first application is to design more powerful two sample tests. We aim to show that H-divergence
allow us to leverage inductive biases for each data type (e.g. image, bio, text) by choosing suitable
actions A and loss `, which leads to improved test power. 1
4.1	Two Sample Test
For the task of two sample test, we would like to decide if two sets of samples are drawn from
the same distribution or not. Specifically, given two sets of samples Pm := (xi, ∙∙∙ , χm) i吟 P
and ^m := (x1, ∙∙∙ ,xm)i九.q we would like to decide if P = q. Typical approaches estimate a
divergence D(Pmkqm) and output P = q if the divergence exceeds some threshold.
There are two types of errors: Type I error happens when the algorithm incorrectly outputs P 6= q;
the probability of type I errors is called the significance level. Type II error happens when the
algorithm incorrectly outputs P = q; the probability of not making a Type II error is called the
test power (higher is better). Note that both the significance level and the test power are relative to
distributions P and q.
We follow the typical setup where we guarantee a certain significance level while empirically mea-
suring the test power. In particular, the significance level can be guaranteed with a permutation
test (Ernst et al., 2004). In a permutation test, in addition to the original set of samples Pm and ‰,
we also uniformly randomly swap elements between Pm and ^m, and sample multiple randomly
swapped datasets (Pm, ^m), (Pm, ^m),….The testing algorithm outputs P = q if D(Pmllqm) is
in the top α-quantile among {D(Pm∣∣qm), D(Pm 1般),…}. Permutation test guarantees the sig-
nificance level (i.e. low Type I error) because if P = q then swapping elements between Pm and
^m should not change its distribution, so each pair (Pm, qm), (P1n, ^m), .一should have the same
distribution. Therefore, D(PmII ^m) happens to be in the top α-quantile with at most α probability.
Note that the significance level guarantee does not rely on accurate estimation of H-divergence in
Theorem 2 (accurate H-divergence estimation is still important because the test power does depends
on it).
When the choice of D' is not a strict divergence (See Proposition 2) we may falsely conclude P = q
(D(Pkq) = 0) when in reality P 6= q. This is true but inconsequential in finite data scenarios.
With finite data, it is generally impossible to guarantee the test power (i.e. bounding the probability
of concluding P = q when in reality P 6= q for any P, q) and prior literature do not provide such
guarantees. Hence our guarantee is no weaker than prior two sample test literature.
4.2	Experiment Setup
Baselines We compare our proposed approach with six other divergences. All methods are
based on the permutation test explained in Section 4.1. MMD-D (Liu et al., 2020) measures
the MMD distance with a deep kernel, while MMD-O (Gretton et al., 2012) measures the MMD
distance with a Gaussian kernel. Mean embedding (ME) and smoothed characteristic functions
(SCF) (Chwialkowski et al., 2015; Jitkrittum et al., 2016) are distances based on the difference in
Gaussian kernel mean embedding at a set of optimized points, or a set of optimized frequencies.
C2STS-S & C2ST-L (Lopez-Paz & Oquab, 2017; Cheng & Cloninger, 2019) use a classifier’s accu-
racy distinguishing between the two distributions.
Comparison Metrics and Setup All methods have the same significance level (which is provably
equal to α = 0.05 because of the permutation test), therefore we only consider the test power. We
follow Liu et al. (2020) and consider four datasets: Blob (Liu et al., 2020), HDGM (Liu et al.,
1The code to reproduce our experiments can be found here.
6
Published as a conference paper at ICLR 2022
2020), HIGGS (Adam-Bourdarios et al., 2014) and MNIST (LeCun & Cortes, 2010). Our method
and all the baseline methods have hyper-parameters. To ensure fair comparison, we follow the same
evaluation setup as (Liu et al., 2020) for all methods. We split each dataset into two equal partitions:
a training set to tune hyper-parameters, and a validation set to compute the final test output.
Implementation Details We choose φ(θ, λ) = (θs+λs )1/s for s > 1 (which includes the H-
Jensen Shannon divergence when s = 1 and the H-Min divergence when s = ∞). We define l(x, a)
as the negative log likelihood of x under distribution a, where a is in a certain model family A. We
experiment with mixture of Gaussian distributions, Parzen density estimtor and Variational Autoen-
coder (Kingma & Welling, 2013). Our hyper-parameters consist of the best parameter s and also the
best generative model family. Choosing these hyper-parameters might seem cumbersome, but com-
pared to the second best baseline (MMD-D which chooses thousands of deep kernel parameters),
we have much fewer hyper-parameters.
We use α = 0.05 in all two-sample test experiments. Each permutation test uses 100 permutations,
and we run each test 100 times to compute the test power (i.e. the percent of times it correctly
outputs p 6= q). Finally we plot and report the performance standard deviation by repeating the
entire experiment 10 times.
4.3	Experiment Results
The average test powers are reported in Figure 4, Figure 2, Table 1 and Table 3. Our approach
achieves superior test power across the board. Notably on Higgs we achieve the same test power
with 2x fewer samples than the second best test, and on MNIST we can achieve perfect test power
even on the smallest sample size evaluated in (Liu et al., 2020).
Following (Liu et al., 2020) we also evaluate the test power as the dimension of the problem increases
(Figure 2). Our test power decreases gracefully as the dimension of the problem increases. We
hypothesize that the test power improvements come from leveraging progress in generative model
research: for each type of data (e.g. bio, image, text) there has been decades of research finding
suitable generative models; we use commonly used generative models (in modern literature) for
each data type (e.g. KDE for low dimensional physics/bio data, VAE for simple images).
Figure 2: Average test power on HDGM dataset. Left: results with the same sample size (4000)
and different data dimensions. Right: results with the same sample dimension (10) and different
sample sizes. Our method (H-Div, dashed line) achieve better test power for almost every setup.
All tests have high test power for low data dimensions, but our method scales better for higher data
dimensions.
N	I ME	SCF	C2ST-S	C2ST-L	MMD-O	MMD-D	H-Div
1000	O.12O±0.007	0.095±0.007	0.082±0.015	0.097±0.014	0.132±0.005	0.113±0.013	0.240±0.020
2000	0.165±0.019	0.130±0.019	0.183±0.026	0.232±0.032	0.291±0.017	0.304±0.012	0.380±0.040
3000	0.197±o.012	0.142±0.025	0.257±0.049	0.399±0.058	0.376±0.022	0.403±0.050	0.685±0.015
5000	0.410±0.041	0.261±0.044	0.592±0.037	0.447±0.045	0.659±0.018	0.699±0.047	0.930±0.010
8000	0.691 ±0.067	0.467±0.038	0.892±0.029	0.878±0.020	0.923±0.013	0.952±0.024	1.000±0.000
10000	0.786±0.041	0.603±0.066	0.974±0.007	0.985±0.005	1.000±0.000	1.000±0.000	1.000±0.000
Avg.	I 0.395	0.283	0.497	0.506	0.564	0.579	0.847
Table 1: Average test power ± standard error for N samples over the HIGGS dataset. The results
on MNIST is similar and presented in Table 3, Appendix B.1.
7
Published as a conference paper at ICLR 2022
Figure 3: Example plots of H-divergence across different geographical locations for losses ` related
to agriculture (left) and energy production (right). Darker color indicates larger H-divergence. Com-
pared to divergences such as KL, H-divergence measures changes relevant to different social and
economic activities (by selecting appropriate loss functions `). For example, even though climate
change significantly impact the high latitude or high altitude areas, this change has less relevance to
agriculture (because few agriculture activities are possible in these areas).
5	Experiment: Decision Dependent Discrepancy Measurement
5.1	Assessing Climate Change
As an illustrative example of how H-divergence can facilitate decision making, we use climate data
and study how climate change affects decision making through the lens of H-divergence. Scien-
tists and policy makers are often interested in how climate change disparately affect different geo-
graphical locations. Existing methods (Preston et al., 2011) focus on one aspect of climate change
(such as the expected economic loss (Burke et al., 2018)) using tailor-designed analysis, while H-
divergence provides a general tool for hypothesis testing and visualization for different aspects of
climate change. In our example, we choose suitable loss functions to quantitatively measure as-
pects of climate change that are relevant to decision making in agriculture and renewable energy
production.2 * *
Setup We use the NOAA database which contains daily weather from thousands of weather sta-
tions at different geographical locations. For each location, we summarize the weather sequence of
each year into a few summary statistics (average yearly temperature, humidity, wind speed and rainy
days). We are interested in assessing changes in weather over this period at each location, from the
perspective of agriculture and renewable energy activities. Further details of these experiments are
in Appendix C.2.
Example: Agriculture It is known that climate changes affect crop suitability (Lobell et al.,
2008). Let A denote the set of possible crops to plant at each location (e.g. wheat/barley/rice),
and '(x, a) denote the loss of planting crop a if the yearly weather is x. We estimate the function '
by matching geographical locations in the FAO crop yield dataset (FAOSTAT et al., 2006) to weather
stations in the NOAA database, and learn a function to predict crop yield from weather data with
kernel ridge regression.
The H-divergence has a natural interpretation: a geographical location could either (1) plant the
same crop for the entire period 1981-2019 that is optimal for the local climate (i.e. choose
a* = arg mina∈A E(p+q)/2 ['(X, a)]); (2) plant the optimal crops for 1981-1999 and for 2000-2019
respectively. H divergence measures the additional loss of option (1) compared to option (2). In
other words, it is the excess loss of not adapting crop type to climate change. For each geographical
location We can compute the H-divergence DJS for the estimated ' (plotted in Figure 3 left).
Example: Energy production Changes in weather also affect electricity generation, since cli-
mate change could affect the amount of wind/solar energy available. Let A denote the number
of wind/solar/fossil fuel power plants built, and '(x, a) denote the loss (negative utility) when the
weather is x. We obtain the function ` using empirical formulas for energy production (Npower,
2012). The H-divergence for this loss function is shown in Figure 3 (right). Intuitively the H di-
2Designing loss functions ` that capture the effect of climate on human activities is a well studied topic in
economics, and beyond the scope of this work. Our results should be taken as an illustrative example of how
domain experts might use H-divergence with more realistic loss functions.
8
Published as a conference paper at ICLR 2022
Loss Selection
Neutral
Upweight low income
Upweight high income
Selected Features
education, cap-gain, sex, age, occupation
education, cap-gain, relationship, marital-status, sex
education, cap-gain, sex, age, race
Table 2: Features selected by different approaches. With H-Divergence we can select different
features that are important in different decision problems. For example, if we assign a high / low
penalty to making incorrect prediction for higher income groups, we select a different set of features.
vergence measures the excess loss of using the same energy generation infrastructure for the entire
time period vs. using different infrastructure that adapts to climate change. While this is only an
illustrative example, comparing the two maps we see that regions and industries are affected by
climate change in different Ways - H divergence provides a quantitative framework for this kind of
assessments.
5.2	Feature Selection
In a feature selection task, we wish to know which input features are most predictive of the label.
Feature selection provides information on which features have the biggest influence on the label,
and can be used in scientific discovery (Jovic et al., 2015; Zhang et al., 2015).
Off-the-shelf feature selection algorithms often do not take into account problem specific require-
ments. For example, denote the input features as Xi, ∙ ∙ ∙ ,Xk and label as Y, the mutual in-
formation feature selection algorithms estimate the Shannon mutual information I(Xi, Y) :=
KL(p(Xi, Y)kp(Xi)p(Y)) and select features with largest mutual information. However, scien-
tists and policies makers often need fine-grained control to answer their specific scientific or policy
questions. For example, social scientists might want to know which features are more important for
high-income as compared to low-income groups (e.g. to understand potential glass ceilings).
With H-Divergence we can select features with large Dcφ(p(Xi, Y)kp(Xi)p(Y)) (i.e. the optimal
action is different under the joint p(Xi, Y) and the product of marginals p(Xi)p(Y)). By choosing
different loss functions ` we can get different feature selection results, each reflecting important
features for that decision problem. For example, in Table 2 we show the features selected for the
UCI income prediction dataset (Blake, 1998). For this dataset, we choose A as the set of logistic
regression functions, l(x, a) as the cross entropy loss for regression function a on the sample x and
φ(θ, λ) = max(θ, λ). Ifwe want to focus on high income groups, we can assign a higher weight to
the loss of high income samples, and vice versa. We observe that gender/race is more predictive of
income for high-income groups, while relationship or marital status is more predictive of income for
lower-income groups. This can help us identify potential inequality or suggest further investigation
into the cause of low income and poverty. For example, our results suggest a connection between
family and relationship status and poverty, and a connection between gender/race and high income.
These connections merit further investigation into the cause and policy remedy.
6	Acknowledgements
SE acknowledges support by NSF(#1651565, #1522054, #1733686), ONR (N000141912145),
AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship.
References
Claire Adam-Bourdarios, Glen Cowan, CeCile Germain, Isabelle Guyon, BalazS Kegl, and David
Rousseau. The higgs boson machine learning challenge. In Proceedings of the 2014 International
Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML’14, pp. 19-55.
JMLR.org, 2014.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Peter Bartlett. Theoretical statistics, lecture 13. https://www.stat.berkeley.edu/
~bartlett/Courses/2 013Spring-stat210b∕notes∕13notes.pdf, 2013.
9
Published as a conference paper at ICLR 2022
Catherine Blake. Uci repository of machine learning databases. http://www. ics. uci. edu∕~
mlearn/MLRepository. html, 1998.
Jacob Burbea and C Radhakrishna Rao. Entropy differential metric, distance and divergence mea-
SUres in probability spaces: A unified approach. Journal OfMultivariate Analysis, 12(4):575-596,
1982.
Jacob Burbea and C Radhakrishna Rao. Differential metrics in probability spaces. Probab. Math.
Stat, 3:241-258, 1984.
Marshall Burke, W Matthew Davis, and Noah S Diffenbaugh. Large potential reduction in economic
damages under un mitigation targets. Nature, 557(7706):549-553, 2018.
X. Cheng and A. Cloninger. Classification logit two-sample testing by neural networks. ArXiv,
abs/1909.11298, 2019.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems
28, pp. 1981-1989. Curran Associates, Inc., 2015.
Imre Csiszar. Eine informationstheoretiSche UngleichUng und ihre anwendung auf beweis der er-
godizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85-108, 1964.
Morris H DeGroot. Optimal statistical decisions, volume 82. John Wiley & Sons, 2005.
Morris H DeGroot et al. Uncertainty, information, and sequential experiments. The Annals of
Mathematical Statistics, 33(2):404-419, 1962.
Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Scholkopf. A permutation-based kernel
conditional independence test. In UAI, pp. 132-141, 2014.
John Duchi, Khashayar Khosravi, Feng Ruan, et al. Multiclass classification, information, diver-
gence and surrogate risk. Annals of Statistics, 46(6B):3246-3275, 2018.
Michael D Ernst et al. Permutation methods: a basis for exact inference. Statistical Science, 19(4):
676-685, 2004.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
FAO FAOSTAT et al. Fao statistical databases. Rome: Food and Agriculture Organization of the
United Nations, 2006.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Peter D Grunwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. the Annals of Statistics, 32(4):1367-1433, 2004.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
Wittawat Jitkrittum, Zoltan Szabo, Kacper P Chwialkowski, and Arthur Gretton. Interpretable dis-
tribution features with maximum testing power. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
181-189. Curran Associates, Inc., 2016.
10
Published as a conference paper at ICLR 2022
Alan Jovic, Karla BrkiC, and Nikola Bogunovic. A review of feature selection methods with appli-
cations. In 2015 38th international convention on information and communication technology,
electronics and microelectronics (MIPRO),pp. 1200-1205. Ieee, 2015.
Diederik P Kingma and Max Welling. Auto-Encoding variational bayes. arXiv preprint
arXiv:1312.6114v10, December 2013.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, A. Gretton, and D. Sutherland. Learning deep
kernels for non-parametric two-sample tests. ArXiv, abs/2002.09116, 2020.
David B Lobell, Marshall B Burke, Claudia Tebaldi, Michael D Mastrandrea, Walter P Falcon, and
Rosamond L Naylor. Prioritizing climate change adaptation needs for food security in 2030.
Science, 319(5863):607-610, 2008.
David Lopez-Paz and M. Oquab. Revisiting classifier two-sample tests. arXiv: Machine Learning,
2017.
Tengyu Ma. Machine learning theory. https://github.com/tengyuma/cs229m_notes/
blob/main/Winter2021/pdf/02-08-2021.pdf, 2021.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, pp. 429-443, 1997.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, pp. 271-279, 2016.
Npower. Wind turbine power calculations. Mechanical and Electrical Engineering Power Industry,
The Royal Academy of Engineering, 2012.
Benjamin L Preston, Emma J Yuen, and Richard M Westaway. Putting vulnerability to climate
change on the map: a review of approaches, benefits, and risks. Sustainability science, 6(2):
177-202, 2011.
C Radhakrishna Rao. Diversity and dissimilarity coefficients: a unified approach. Theoretical
population biology, 21(1):24-43, 1982.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert RG
Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv
preprint arXiv:0901.2698, 2009.
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.
Yudong Zhang, Zhengchao Dong, Preetha Phillips, Shuihua Wang, Genlin Ji, Jiquan Yang, and Ti-
Fei Yuan. Detection of subjects and brain regions related to alzheimer’s disease using 3d mri
scans based on eigenbrain and machine learning. Frontiers in computational neuroscience, 9:66,
2015.
11
Published as a conference paper at ICLR 2022
A Proofs
Lemma 2. For any choice of ' and for any choice of φ that satisfy Definition 2, Dφ is non-negative and Dφ (p, q) = 0
whenever P = q. Furthermore, Dφ is symmetric whenever φ is symmetric.
Proof of Lemma 2. For any choice of p, q by the concavity of the H-entropy in Lemma 1 we have
h`
p+q
h`
2
P+q
-Hg(P) ≥ 2(Hxq)- H`(P))
-He(q) ≥ I(He(P)- H`(q))
2
Therefore by summing the two inequalities we have
p + q
+
≥0
By the requirement on φ we know that Dφ(ρ∣∣q) ≥ 0. In addition when P = q since (p + q)/2 = P = q we have
Dφ(Pkq) = φ(0, 0) = 0.
To show it is symmetric, note that
Dφ(pkq) = Φ
p+q
2
Dφ(qkp)
-h`(p),h`
-H'(q),H'
whenever φ is symmetric.
□
Proposition 3. Dφ(p∣∣q) > 0 if and only if arg inf a Ep['(X,a)] ∩ arg inf a Eq ['(X,a)] = 0.
Proof ofProposition 3. Denote Ap = arg inf a Ep ['(X, a)] and A* = arg inf a Eq ['(X, a)]. Also compute
h`
p+q
inf Ep+q ['(X, a)] = inf 1EEp ['(X, a)] + 1 Eq ['(X,
a 2	a ∖ 2	2
(3)
2
If A*p ∩ Aq* = 0, for any action a0 such that Ep ['(X, a0)] = He(P), we must have a0 ∈ Ap* so a0 6∈ Aq* and Eq ['(X, a0)] >
He(q). Similar if we choose a00 such that Eq['(X, a00)] = He(q) we have similarly have Ep ['(X, a00)] > He(P). In other
words, for any choice of action a ∈ A either a ∈/ Ap* and Ep[l(X, a)] > He(P) or a ∈ Ap* and Eq[l(X, a)] > He(q).
Therefore
inf(1Ep['(X,a)] + 1 Eq ['(X,a)])> 2 He(P)+2 HMq)
(4)
Combining Eq.(3) and Eq.(4) we have
+
>0
By Definition 2 this implies (for any choice of φ that satisfies the requirements in Definition 2) that Dφ(Pkq) > 0.
To prove the converse simply obverse that if Ap ∩A* = φ, let a* ∈ Ap ∩ A* we have a* = arg infa∈A Ep+q [l(X, a)]. This
implies that
2H'
P+q
-H'(q) - H'(p) = 2Ep+q [l(X, a*)] - Eq[l(X, a*)] - Ep[l(X, a*)] = 0
2
By Definition 2 We can conclude that Dφ(p∣∣q) = 0.
□
Theorem 1.	The set of H-Jensen Shannon Divergences is strictly larger than the MMD2 distances.
12
Published as a conference paper at ICLR 2022
Proof of Theorem 1. Let k(x, y) be some kernel on an input space X, and let H be the RKHS induced by the kernel. The
(squared) MMD distance is defined by
MMD2 (p, q) = EX〜p,Y〜pk(X, Y) + EX〜q,Y〜qk(X, Y)- 2Eχ〜p,γ〜qk(X, Y)
which we write more compactly as MMD2 (p, q) = Ep,pk(X, Y) + Eq,qk(X, Y) - 2Ep,qk(X, Y).
Define φ(x, y) = ∣∣k(x, •) - k(y, ∙)kH. We can rewrite this in the following form:
MMD2 (p, q) = Ep,qΦ(X, Y)- | Ep,pΦ(X, Y)- | Eq,q Φ(X, Y)	⑸
=Ep,qkk(X, ∙)kH + kk(Y, ∙)kH - 2k(X, Y) - 2Ep,p∣k(X, ∙)kH + kk(Y, ∙)kH - 2k(X, Y)
-2Eq,q kk(X, ∙)kH + kk(Y, ∙)kH - 2k(X,Y) = Ep,pk(X,Y)+ Eq,q k(X,Y) - 2Ep,q k(X,Y)
We also observe an algebraic relationship for any function f(x, y) such that f(x, y) = f(y, x) for all x, y:
E p+q p+q f (X, Y) = 1 Ep,pf (X, Y) + 1 Ep,q f(X, Y) + 1 Eq,p f (X, Y ) + 1 Eq,q f (X, Y)
2,2	4	4	4	4
=4 Ep,pf (X,Y) + 4 Ep,q f (X,Y) + 4 Eq,p f (Y,X ) + 1 Eq,q f (X,Y )
=1 Ep,pf (X, Y) + 1 Eq,qf (X, Y) + 2Ep,qf (X, Y)	(6)
Furthermore, we have that
Ep,p∣k(X, ∙) - k(Y, ∙)kH = 2Ep∣k(X, ∙) - Epk(Y, ∙)kH	(7)
Based on the above, noting that φ(x, y) = φ(y, x), we can derive
MMD2(P, q) = Ep,qkk(X, ∙) - k(Y, ∙)kH - 2Ep,p∣k(X, ∙) - k(Y, ∙)kH - 2Eq,q∣k(X, ∙) - k(Y, ∙)kH	Eq ⑸
=2Ep+q,p+q kk(X, ∙) - k(Y, ∙)kH - Ep,p∣k(X, ∙) - k(Y, ∙)kH - Eq,qkk(X, ∙) - k(Y, ∙)kH	Eq (6)
=4Ep+q ∣k(X, ∙) - Ep+qk(Y, ∙)∣H - 2Ep∣k(X, ∙) - Epk(Y, ∙)∣H - 2Eq∣k(X, ∙) - Eqk(Y, ∙)∣H Eq ⑺
=4 inf Ep+q ∣∣k(X, ∙) — a∣H — 2 inf Epkk(X, ∙) — a∣H — 2 inf Eq∣∣k(X, ∙) — a∣H.	mean def.
a∈H "ɪ	a∈H 尸	a∈H 丫
Therefore we can define a loss ` : X × H → R where
'(x, a) = 4∣k(x, ∙) — a∣H
Under the new notation we have
MMD2(p, q) = inf Ep+q l(X, a) - 1 ( inf Epl(X, a) + inf Eql(X, a)
a∈H F	2 ∖a∈H F	a∈H
=h` (p+i) - 1(H'(P) + H'(q)) = DJS(Pkq)
Conversely We want to show that not every H-Jensen Shannon divergence is a MMD. For example, take h` to be the Shan-
non entropy, then the corresponding DJS is the Jensen-Shannon divergence, which is not a MMD. This is because the JS
divergence is a type of f -divergence, and the only f -divergence that is also an IPM is total variation distance Sriperumbudur
et al. (2009). Therefore, the set of H-Jensen Shannon Divergences is strictly larger than the set of MMDs.	□
Theorem 2.	If ` is C -bounded, and φ is 1-Lipschitz under the ∞-norm, for any choice of distribution P, q ∈ P(X) andt > 0
we have
2
1.	Pr[Dφ(PmkGm) ≥ t] ≤ 4e-2C2 fP = q.
2. Pr h∣D Φ(Pm∣qm)-Dφ(p∣q)∣ ≥ 4max(Rm('), R为('))+ t] ≤ 4e-2C2
13
Published as a conference paper at ICLR 2022
ProofofTheorem 2. Let Pm be a sequence of n samples (xi, ∙∙∙ ,Xm) drawn from p, and qm a sequence of n sam-
ples (χ1, ∙∙∙ ,xm)drawn from q. Let rm, the sub-sampling mixture (x；0, ∙∙∙ ,xmm) defined in Section 3.5 (i.e. Xi =
Xibi + χi(1 - bi) where b is uniformly sampled from {0, l}). We also overload the notation h` by defining Hχpm,)=
infa∈A m Pm=I l(xi, a), and define He(<jm), He(rm) similarly.
Before proving this theorem we need the following Lemmas
Lemma 3. Under the assumptions of Theorem 2
2
Pr [H'(Pm) - E[H'(Pm)] ≥ t] ≤ e-b
2
Pr [H'(pm) - E[H'(pm)] ≤ -t] ≤ e-b
Lemma 4. Under the assumptions of Theorem 2
Pr[∣H'(p)- H'(Pm)∣≥ 2Rm(')+ t] ≤ e-2Cm
To prove the first statement of the Theorem, when P = q We can denote μ = E[H'(pm,)] = E[He(qm)] = E[H'(rm,)], and
we have
Pr [ddφ(PmkGm) ≥ t]
=Pr[φ(H'(rm) - H'(pm),H'(rm) - Hg(Gm)) ≥ t]	Def2
≤ Pr [max(Hg(rm) - Hg(pm), Hg(rm) - Hg(Gm)) ≥ t]	φ 1-Lipschitz
≤ Pr [H'(rm) - Hg(p^m) ≥ t] + Pr [H'(^m) - H'(qm) ≥ t]	Union bound
≤ Pr [H'(pm) - μ ≤ -1∕2] + 2Pr [H'(rm) - μ ≥ t∕2]+ Pr [H'(qm) - μ ≤ -1∕2]	UniOn bound
-t2
≤ 4e 2C2/m	Lemma 3
The third inequality is because if Hg(rm,) - Hg(pm,) ≥ t then it must be either Hg(Pm) - μ ≤ -t∕2 or Hg(rm,) - μ ≥ t∕2.
Similarly if Hg(rm,) - Hg(qm,) ≥ t then it must be either Hg(Gm) - μ ≤ -t∕2 and Hg (rm) - μ ≥ t∕2.
To prove the second statement of the Theorem, we observe that
|DDφ(pmkqm)- Dφ(pkq)l
=∣φ (H'(rmɔ - h' (Pm) , H'(rm) - H'(qm)) - φ (H' (P_2_B_ ) - H'(p) , H' ( P_2_S_ ) - H'(Q))I
≤ max (∣H'(rm) - H'(Pm)- H' (P 2 Q ) + H'(p)∣ , ∣H'(rm) - H'(qm) - H' (？-2jl) + H'(q)|)
≤ max (∣H'(rm) - H' (0 2 Q ) | + |H'(Pm)- H'(p) | , ∣H'(rm) - H' (P-2-B-) | + |H'(qm) - H'(q)|
Def 2
φ 1-Lip
Jensen
Therefore, the event |DDφ(pm∣∣qm) - Dφ(p∣∣q)∣ ≥ 4max(Rm1('), Rm(')) + t happens only if at least one of the following
events happen
Ηg(^m) - Hg (审)1 ≥ Rm(') + Rm(') +1∕2 ≥ 2Rm+q"2(')+1∕2
R convex
|Hg(Pm) - Hg(p)∣ ≥ 2Rm(') +1∕2
∣Hg(qm) - Hg(q)∣ ≥ 2Rm(') +1∕2
_ t2m
Based on Lemma 4 each of these events only happen with probability at most e 2c2. Therefore We can conclude by union
bound that
2
Pr[∣Dφ(pkq) - Dφ(p∣∣q)∣ ≥4max(Rm('), Rml(')) + t] ≤ 4e-2C2
Finally we prove the two Lemmas used in the theorem. Lemma 4 is a standard result in the Radamacher complexity literature.
For a proof, see e.g. Section 26.1 in (Shalev-Shwartz & Ben-David, 2014). Lemma 3 can also be proved by standard
techniques. We provide the proof here.
14
Published as a conference paper at ICLR 2022
ProofofLemma 3. Consider two sets of samples xi, ∙∙∙ ,xj,…,Xm and x；,…，xj,…，Xm where Xi = Xi for every
index i = 1,…，m except index j.
inf m X 0® ㈤-inf m X '(Xi，a)
ii
≤ sup
a
m χ `(Xi,a)- 嬴 χ `(Xi,a)
i
i
1
m Su
Then we can conclude by Mcdiarmid inequality that
Pr
inf ɪ X '(Xi,a) - E
am
i
inf ɪ X '(Xi ,a)
am
i
≥t
2t2
.---------
≤ e C2/m
_ 2t2 m
e- C2
□
□
Corollary 1. JVar[Df(pmkdm)] ≤ 4max(Rm('),Rm(')) + P2C2∕m
ProofofCorollary 1. For notation convenience denote B = 4 max(Rmm ('), Rm('))
VarD φ(p^mkqm)]
=E [(DDΦ(Pm∣∣qm)- Dφ(p∣∣q))
∞
t=0
Pr ∖(dΦ(Pmk^m) - Dφ(p∣∣q))2 ≥ t] dt
=/ Pr h∣DDφ(pmk^m) - Dφ(pkq)∣ ≥ √] dt
=/ Pr h∣DDφ(Pm∣∣Gm) - Dφ(p∣∣q)∣ ≥ Si 2sds
≤ Z	2sds + Z Pr h∣DDφ(Pmkqm)- Dφ(p∣∣q)∣ ≥ B + Si 2(B + s)ds
∞	∞ zπ	、	s2m T
≤ B2 +	2(B + s)e-2C2 ds
≤ B2 + 「 2(B +Lt \耳dt
t=0	m	m
m
t = sV 2C2
2
B2 + 2B
B2 + B
2C2	,----- C
+ 竺 ≤ (B + √2c27m )2
m
□
Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a finite set 2. A is a bounded
subset of Rd for Some d ∈ N and ' is LiPschitz w.r.t. a, then almost surely limm→∞ DDφ(Pmkqm) = Dφ(Pkq).
Proof of Corollary 2. We can prove the consistency results from Theorem 2 by observing that the expected Radamacher
complexity goes to 0 when m → ∞.
The first statement is a simple consequence of Massart’s Lemma, (e.g. see Eq.(8.44) in (Ma, 2021)). In particular, because A
is finite we have
Rpm(') ≤ √2log |A|/m →m→∞ 0
15
Published as a conference paper at ICLR 2022
To prove the second statement, first observe that because A is bounded, there must exist some r ∈ R such that A ⊂ Br :=
{a, kak2 ≤ r }. In addition, without loss of generality we can assume that there exists L ∈ R such that ∀x ∈ X and a, a0 ∈ A
|'(x, a) - '(x, a0)| ≤ Lka - a0∣∣2
There is no loss of generality because in finite dimensions all norms are equivalent, so if f is Lipschitz under any norm, then `
is Lipschitz under the 2-norm. We can apply the results on Radamacher complexity for smoothly parameterized class proved
in (Bartlett, 2013), and conclude that limm→∞ Rm (') = 0.	□
B Additional Experimental Results
B.1	Blob dataset
Figure 4: Left: Average test power on the Blob dataset for different sample sizes and significance
level α = 0.05. Our method (H-Div, dashed line) has significantly better test power, especially for
setups with small sample sizes. Right: The same plot with significance level α = 0.01.
B.2	Evaluating Sample Quality
Figure 5: The divergence between corrupted image and original image measured by H-divergence
vs. FID. For better comparison we normalize each distance to between [0, 1] by a linear transforma-
tion. For “speckle” and “impulse” corruption, both divergences are monotonically increasing with
more corruption. For “snow” corruption H-divergence is monotonic while FID is not. Other types
of corruptions are provided in Appendix B.2.
The gold standard for evaluating image generative models requires human decision, which is nevertheless expensive. Sev-
eral surrogate measurements are commonly used, such as the Frechet Inception Distance (FID) (Heusel et al., 2017) or the
inception score. Here by formulating such evaluation as an estimation of the discrepancy between the generated and the real
images, we can quantify the quality of image samples by calculating the corresponding H-Divergences.
We choose A as the set of Gaussian mixture distributions on the inception feature space, l(x, a) as the negative log likelihood
of x under distribution a and φ(θ, λ) = max(θ, λ). To evaluate the performance, we use the same setup as (Heusel et al.,
2017), where we add corruption from (Hendrycks & Dietterich, 2019) to a set of images. Intuitively, adding more corruption
degrades the sample quality, so a good measurement of sample quality should assign a lower quality score (higher divergence
from clean images). The results are plotted in Figure 5. The remaining plots of other perturbations are in Appendix B.2.
Both FID and H-divergence are generally monotonically increasing as we increase the amount of corruption. Our method is
slightly better on some perturbations (such as “snow”), where the FID fails to be monotonically increasing, but our method
is still monotonic, better aligning with human intuition.
16
Published as a conference paper at ICLR 2022
Figure 6: Additional plots that extend Figure 5.
C Additional Theory and Experiment Details
C.1	Connection to Optimal Transport
We first show that H-divergence can also have a transportation interpretation. For all the intuitive interpretations we avoid
technical difficulty by assuming X is a finite set, even though all the formulas are applicable when X has infinite cardinality.
17
Published as a conference paper at ICLR 2022
N	ME	SCF	C2ST-S	C2ST-L	MMD-O	MMD-D	H-Div
200	0.414±0.050	0.107±0.018	0.193±0.037	0.234±0.031	0.188±0.010	0.555±0.044	1.000±0.000
400	0.921±0.032	0.152±0.021	0.646±0.039	0.706±0.047	0.363±0.017	0.996±0.004	1.000±0.000
600	1.000±0.000	0.294±0.008	1.000±0.000	0.977±0.012	0.619±0.021	1.000±0.000	1.000±0.000
800	1.000±0.000	0.317±0.017	1.000±0.000	1.000±0.000	0.797±0.015	1.000±0.000	1.000±0.000
1000	1.000±0.000	0.346±0.019	1.000±0.000	1.000±0.000	0.894±0.016	1.000±0.000	1.000±0.000
Avg.	0.867	0.243	0.768	0.783	0.572	0.910	1.000
Table 3: Average test power ± standard error for N samples over the MNIST dataset.
Setup Choose A = X, and let l(x, a) be a symmetric function (l(x, a) = l(a, x)) that denotes the cost of transporting a
unit of goods from x to a. When we say that a unit of goods is located according to p, we mean that there is 1 unit of goods
dispersed in X locations, where p(x) is the amount of goods at location x.
Optimal Transport Distance The optimal transport distance is defined by
O'(p, q) =	inf	ErXY [l(X, Y)]
rXY,rX=pX,rY=qY
Intuitively the optimal transport distance measures the following cost: initially the goods are located according to p, we would
like to move them to locate according to q; O(p, q) denote the minimum cost to accomplish this transportation task.
H-Divergence as Optimal Storage We first consider the intuitive interpretation to the H-entropy
H'(p) = inf Ep['(X, a)]	a* = arg inf Ep['(X, a)]
a	a∈X
Suppose we want to move goods located according top to a storage location (for example, we want to collect all the mail in a
city to a package center), then a* is the optimal location to build the storage location, and H-entropy measures the minimum
cost to transport all goods to the storage location. Similarly 2H' (p++q) measures the minimum cost to transport both goods
located according to p and goods located according to q to the same storage location. The H-divergence
2DJS(Pkq) := 2H' (号)-(H'(q) + H'(p))
measures the reduction of transportation cost with two storage locations (one forp and one for q) rather than a single storage
location (for both p and q).
The H-Divergence is related to the optimal transport distance by the following inequality.
Proposition 4. If' satisfies the triangle inequality ∀x, y,z ∈ X, l(x, y) + l(y, Z) ≥ l(x, Z) then DJS(Pkq) ≤ 2O(p, q)
Proof of Proposition 4. Let aq* = arg inf Eq [l(X, a)] then we have
2H'审
inf (Ep ['(X, a)] + Eq ['(X, a)]) ≤Ep['(X,aq*)]+Eq['(X,aq*)]
a
≤ rXY,rX=inpXf,rY=qY ErXY ['(X, aq*)] +Eq['(X,aq*)]
≤	inf	ErXY ['(X, Y) + '(Y, aq*)] + Eq['(X, aq*)]
rXY,rX =pX ,rY =qY
=O'(P, q) +2H'(q)
Intuitively, to move goods located according to p and goods located according to q to some storage location, one option is to
first transport all goods from p to q (so that the goods at location x will be 2q(x)), then move the goods located according to
2q to the optimal storage location. Similarly we have
2H' "
≤ O(q,p) + 2H'(p)
which combined we have
2DJS(Pkq) = 2H' (?) - (H'(q) + H'(p)) ≤ O(p, q)
□
18
Published as a conference paper at ICLR 2022
C.2 Impossibility of Jensen-Shannon Divergence Estimation
Suppose We have a consistent estimator for the Jenson-Shannon divergence, i.e. a function JS such that for any pair of
distribution p, q and given N i.i.d. samples PN 〜P and qN 〜q we have IimN→∞ JS(PN∣∣qN) = JS(Pkq), then we prove a
contradiction by the probabilistic method.
Let P be a standard Gaussian distribution, let QM be a uniform distribution on a set of M i.i.d. samples from P (hence QM
is itself a random variable that depends on the i.i.d. samples). Let Q* be the limit of QM when M → ∞ (i.e. it is the
uniform distribution on an infinite set of samples). Let q* denote a value that Q* can take. Because q* is always supported
on countably many points, hence JS(Pkq*) = 1. Note that for any N the following two sampling process leads to identical
distribution on PN , qN :
Pn 〜P, qN 〜P
Q* 〜p,pn 〜p, qN 〜Q*
Hence, the expectation of any function (including JS) is also identical.
[EpN ~p,qN ~Q*[JS(pn ,qN)]]
PN 〜p,qN 〜P
[JS(PN, qN)]
E
Hence the limit when N → ∞ must be identical
lim Eq*〜P
N →∞ Q P
~p,qN ~Q*[JS(Pn ,qN)]]
lim EpN 〜p,qN 〜p[JS(PN, qN )]
N→∞
Because the Jenson Shannon divergence is always bounded, any consistent estimator must also be bounded for sufficiently
large N. By the dominated convergence theorem we can exchange the expectation and the limit.
EQ*〜p	lim EpN〜p,qN〜Q* [JS(PN, qN)]
N→∞
lim EpN 〜p,qN 〜p[JS(PN, qN )]
N→∞
By the probabilistic method (i.e. for any function f if Eq*〜p[f (Q*)] = 0 there must exist some q* such that f (q*) ≤ 0)
there must exist some q* such that
*
lim EpN ~p,qN 〜q* [JS(PN, qN )] ≤ lim EpN 〜p,qN 〜p[JS(PN, qN )]=0 = JS(Pkq ) = 1
N→∞	N→∞
Therefore JS cannot be consistent.
C.3 Climate Change Experiment Details
Setup Details In this experiment, we extract the statistics of yearly weather for each year from 1981-2019. We use the
NOAA dataset, which contains daily weather data from thousands of weather stations across the globe. For each year we
compute the following summary statistics: average yearly temperature, average yearly humidity, average yearly wind speed
and average number of rainy days in an year. For example x1990 is a 4 dimensional vector where each dimension correspond
to one of the summary statistics above.
Letp denote the uniform distribution over {χi98i,…，χ1999} and q denote the uniform distribution over {χ2000,…，χ2019}.
For example Ep ['(X, a)] denote the expected loss of action a for a random year sampled from 1981-1999. Note that for many
decision problems, it is possible to make yearly decisions (e.g. decide the best crop to plant for each year). However, because
we want to measure the difference between two time periods 1981-1999 vs. 2000-2019, we choose the action space A to
be a single crop selection that will be used for the entire time period (rather than a different crop selection for each year).
Similarly for energy production we choose the action space A to be the proportion of different energy production methods
that will be used for the entire time period.
Crop yield We obtain the crop yield loss function '(x, a) with the following procedure
1.	We obtain the crop yield dataset from (FAOSTAT et al., 2006), each entry we extract is the following tuple: (country
code, year, crop type, yield per hectare (kg/ha))
2.	We associate each country code with the central coordinate (i.e. the average latitude and longitude) of the country. For
each central coordinate we find the nearest weather station in the NOAA database. We use data for the nearest weather
station as the weather data for the country.
3.	Based on step 2 for each (country code, year) pair we can associate a weather statistics (i.e. the 4 dimensional vector
described in Setup Details). We update each entry in step 1 to be (weather statistics, crop type, yield per hectare).
4.	Based on the data entries we obtain in step 3 we train a Kernel Ridge regression model to learn the function '(x, a)
where X is the weather statistics, a is the crop type, and '(x, a) is learned to predict the yield (normalized by market
price) of the weather x for crop type a.
19
Published as a conference paper at ICLR 2022
Energy production We consider three types of energy production methods: solar, wind and traditional (such as fossil
fuel). Solar energy and wind energy both depend heavily on weather, while traditional energy does not. In particular, we use
empirical formulas for solar and wind energy calculation:
solar H number of sunny days * daylight hour
Wind H Wind velocity3
D Discussions
Future Work In this paper we explored the applications of H-divergence to two sample testing. Future work can explore
other applications of divergences. Potential applications include
•	Generative model training. Many generative models learning algorithm minimize divergences (Nowozin et al., 2016;
Arjovsky et al., 2017), and future work can explore if the new divergence family leads to new generative model learning
algorithms.
•	Independence tests. Independence tests are two sample tests between the joint distribution pXY and the product of
marginal distributions pXpY , therefore the two sample test results from this paper are applicable to independence tests.
•	Robustness. Many robust optimization, estimation or prediction methods aim to achieve good performance even when
the data distribution is perturbed. Typically perturbation is measured by e.g. the KL divergence or the Lp distances.
Future work can measure perturbation with the H-divergence h` by choosing loss functions ' that are tailored for the
problem.
Computation Issues There are several situations where estimating the H-divergence is (provably) computationally feasible:
•	When A is a small finite set, in which case we can enumerate all possible values of a ∈ A.
•	When the loss function '(y, a) is convex in a, in which case We can accurately estimate the H-divergence in polynomial
time by solving the optimization problem inf a E['(Y, a)] by gradient descent.
In general, while it is difficult to guarantee computational feasibility, we use a practical technique that works well in our
experiments: We use the same number of gradient descent steps for evaluating h` (p++q) and H'(p), H'(q). Intuitively, the
“sub-optimality” when estimating the three terms is approximately the same in expectation and cancels out.
20