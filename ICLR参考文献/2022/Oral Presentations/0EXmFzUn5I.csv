论文题目,会议名称
 Etc: Encoding long and structuredinputs in transformers, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
 Longformer: The long-document transformer,arXiv preprint arXiv:2004
 Some recent advances in forecasting and control, Journalof the Royal Statistical Society
 Dilated recurrent neural networks,Advances in Neural Information Processing Systems
 {TVM}: An automated end-to-end optimizingcompiler for deep learning, In 13th {USENIX} Symposium on Operating Systems Design andImplementation ({OSDI} 18)
 J, Choi
 Hierarchical multiscale recurrent neural net-works, In 5th International Conference on Learning Representations
 Character-based neural machine translation, In Pro-ceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers)
 Residual lstm: Design of a deep recurrentarchitecture for distant speech recognition, arXiv preprint arXiv:1701
 Reformer: The efficient transformer, InInternational Conference on Learning Representations
 Modeling long-and short-termtemporal patterns with deep neural networks, In The 41st International ACM SIGIR Conferenceon Research & Development in Information Retrieval
 Enhancing the locality and breaking the memory bottleneck of transformer on time seriesforecasting, Advances in Neural Information Processing Systems
 Document level neuralmachine translation with hierarchical attention networks, In Proceedings of the Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Deepant: A deeplearning approach for unsupervised anomaly detection in time series, Ieee Access
 Deepar: Probabilistic fore-casting with autoregressive recurrent networks, International Journal of Forecasting
 Schuster, Bi-directional recurrent neural networks for speech recognition
 Multi-scaletransformer language models, arXiv preprint arXiv:2005
 Deep high-resolution representation learning forhuman pose estimation, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Forecasting at scale, The American Statistician
 Attention is all you need, In Advances in neural informationprocessing systems
" Wang, E", Xie
 Bp-transformer: Modellinglong-range context via binary partitioning, arXiv preprint arXiv:1911
 Variational wishart approximation for graphical modelselection: Monoscale and multiscale models, IEEE Transactions on Signal Processing
 Temporal regularized matrix factorization forhigh-dimensional time series prediction, Advances in neural information processing systems
Informer: Beyond efficient transformer for long sequence time-series forecasting, In Proceedingsof AAAI
 Transformer hawkesprocess, In International Conference on Machine Learning
		Notation	Size	MeaningL	Constant	The length of historical sequence,G	Constant	The number of global tokens in ETC
", 2017)", However
F Experiment SetupWe set S = 4 and N = 4 for Pyraformer in all experiments, When the historical length L is notdivisible by C
com/zhouhaoyi/ETDataset15Published as a conference paper at ICLR 2022Table 5: Hyper-parameter settings of long-range experiments,Dataset	prediction length	N	S	H	A	C	historical length	168	4	4	6	3	4	168ETTh1	336	4	4	6	3	4	168	720	4	4	6	5	4	336	96	4	4	6	3	5	384ETTm1	288	4	4	6	5	5	672	672	4	4	6	3	6	672	168	4	4	6	3	4	168Elect	336	4	4	6	3	4	168	720	4	4	6	3	5	336OO OOOOOOPublished as a conference paper at ICLR 2022Specifically
029	0,817	58992	1
001	0,802	53208	1
999	0,796	49992	1
forecasting, Parameters introduced by the normalization layers are relatively few
" The sequence length in (a) and(b) is 672, and that in (c) and (d) is 1344", The time series in (a) and (b) corresponds to the latter halfof those in (c) and (d) respectively
