{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The paper analyzes the dynamics of learning under Gaussian input using dynamical systems theory. As two of the reviewers have pointed out, the paper is hard to read, and not written in a way which is accessible to the wider ICLR community. Hence, I cannot recommend its acceptance to the main conference. However, I recommend acceptance to the workshop track, since it has nice technical contributions that can lead to interesting interactions. I encourage the authors to make it more accessible for a future conference.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Optimization of a ReLU network under new assumptions",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units). The work is interesting in the sense that it does not involve some unrealistic assumptions used by previous works with similar goal. Most importantly, this work does not assume independence between input and activations, and it does not rely on noise injection (which can simplify the analysis). Nonetheless, removing these simplifying assumptions comes at the expense of limiting the analysis to:\n1. Only one layer of nonlinear units\n2. Discarding the bias term in ReLU while keeping the input Gaussian (thus constant input trick cannot be used to simulate the bias term).\n3. Imposing strong assumption on the representation on the input/output via (bias-less) ReLU networks: existence of orthonormal bases to represent this relationships.\n\nHaving that said, as far as I can tell, the paper presents original analysis in this new setting, which is interesting and valuable. For example, by exploiting the symmetry in the problem under the assumption 3 I listed above, the authors are able to reduce the high-dimensional dynamics of the gradient descent to a bivariate dynamics (instead of dealing with original size of the parameters). Such reduction to 2D allows the author to rigorously analyze the behavior of the dynamics (e.g. convergence to a saddle point in symmetric case, or to the optimum in non-symmetric case).\n\nClarification Needed: first paragraph of page 2. Near the end of the paragraph you say \"Initialization can be arbitrarily close to origin\", but at the beginning of the same paragraph you state \"initialized randomly with standard deviation of order 1/sqrt(d)\". Aren't these inconsistent?\n\nSome minor comments about the draft:\n1. In section 1, 2nd paragraph: \"We assume x is Gaussian and thus the network is bias free\". Do you mean \"zero-mean\" Gaussian then?\n2. \"standard deviation\" is spelled \"standard derivation\" multiple times in the paper.\n3. Page 6, last paragraph, first line: Corollary 4.1 should be Corollary 4.2\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially new analysis, but hard to read",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes a convergence analysis of some two-layer NNs with ReLUs. It is not the first such analysis, but maybe it is novel on the assumptions used in the analysis, and the focus on ReLU nonlinearity that is pretty popular in practice. \n\nThe paper is quite hard to read, with many English mistakes and typos. Nevertheless, the analysis seems to be generally correct. The novelty and the key insights are however not always well motivated or presented. And the argument that the work uses realistic assumptions (Gaussian inputs for example) as opposed to other works, is quite debatable actually. \n\nOverall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately. The main results and intuition should be more clearly presented, and details could be moved to appendices for example - that could only help to improve the visibility and impact of these interesting results. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Hard to read paper; unclear conclusions.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}