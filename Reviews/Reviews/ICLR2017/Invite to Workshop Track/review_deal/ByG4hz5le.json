{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Reviewers feel the work is well executed and that the model makes sense, but two of the reviewers were not convinced that the proposed method contains enough novelty in light of prior work. The comparison of the soft vs hard attention model variations is perhaps one of the more novel aspects of the work; however, the degree of novelty within these formulations and the insights obtained from their comparison were not perceived as being enough to warrant higher ratings. We would like to invite the authors to submit this paper to the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Clear and efficient space+time+feature attention mechanisms for video captioning",
            "rating": "7: Good paper, accept",
            "review": "1) Summary\n\nThis paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels.\n\n2) Contributions\n\n+ Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions).\n+ Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms.\n+ Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case.\n\n3) Suggestions for improvement\n\nHypercolumns comparison:\nAs mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels.\n\nMinor clarifications in the text and figures as agreed with the authors in our pre-review discussions.\n\n4) Conclusion\n\nAlthough the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "solid work but lack of novelty",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "state-of-the-art results but too incremental",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN.\n\nThe work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}