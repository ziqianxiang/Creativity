{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper studies efficient signal representations to perform bioacoustic classification based on CNNs. Contrary to image classification, where most useful information can be extracted with spatially localized kernels, bioacoustic signatures are more localized in the frequency domain, requiring to rethink the design of convolutional architectures. The authors propose to enforce the lower layers of the architecture with chirplet transforms, which are localized in the time-frequency plane as wavelets, but with time-varying central frequency. They present preliminary numerical experiments showing promising improvements over existing baselines. \n \n The reviewers found interest in the method, but raised concerns on the relatively narrow scope of the method, as well as the clarity and rigor of the presentation. Whereas the first concern is up to debate, I agree that the paper currently suffers from poor english which affects its clarity. \n \n Despite these concerns, the AC finds the contribution useful in the broader context of inductive bias and injecting priors in neural networks. This is an example where the inductive priors that work well on images (localized convolutions rather than generic fully connected layers) are not sufficient unless given massive amounts of data. The AC thus recommends rejection, but invites the contribution to the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Chirplet transforms for small data tasks",
            "rating": "6: Marginally above acceptance threshold",
            "review": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A work to try to structure a deep network",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Pros: \n- Introduction of a nice filter banks and its implementation\n- Good numerical results\n- Refinement of the representation via back propagation, and a demonstration that it speeds up learning\n\nCons:\n- The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great!\n- The link with a scattering transform is not clear\n- Sometimes (as mentionned in some of my comments), the writing could be improved.\n\nFrom a personal point of view, I also believe the negative points I mention can be easily removed.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}