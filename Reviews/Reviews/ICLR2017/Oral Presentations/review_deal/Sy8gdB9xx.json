{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The authors report the experimental findings of a fascinating inquiry on the ability of the deep neural networks to fit randomly labelled data. The investigation is sound, enlightening, and inspiring. The authors propose both a) a theoretical example showing that a simple shallow network with a number of parameters large enough wrt sample size yields perfect finite-sample expressivity; b) a systematic extensive experimental evaluation to support the findings and claims. The experimental evaluation is a model of thoroughness. \n \n This is definitely groundbreaking work, which will inspire many works in the coming years.",
        "decision": "Accept (Oral)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "review": "This paper offers a very interesting empirical observation regarding the memorization capacity of current large deep convolutional networks. It shows they are able to perfectly memorize full training-set input-to-label mapping, even with random labels (i.e. when label has been rendered independent of input), using the same architecture and hyper-parameters as used for training with correct labels, except for a longer time to convergence. \nExtensive experiments support the main argument of the paper. \n\nReflexions and observations about finite-sample expressivity and implicit regularization with linear models fit logically within the main theme and are equally thought-provoking.\n\nWhile this work doesn’t propose much explanations for the good generalization abilities of what it clearly established as overparameterized models,\nit does compel the reader to think about the generalization problem from a different angle than how it is traditionally understood.\n\nIn my view, raising good questions and pointing to apparent paradox is the initial spark that can lead to fundamental progress in understanding. So even without providing any clear answers, I think this work is a very valuable contribution to research in the field.\n\nDetailed question: in your solving of Eq. 3 for MNIST and CIFAR10, did you use integer y class targets, or a binary one-versus all approach yielding 10 discriminant functions (hence a different alpha vector for each class)?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Memorization, overfitting, generalization",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "review": "the authors of this work shed light on the generalization properties of deep neural networks. Specifically, the consider various regularization methods (data augmentation, weight decay, and dropout). They also show that quality of the labels, namely label noise also significantly affects the generalization ability of the network.\n\nThere are a number of experimental results, most of which are intuitive. Here are some specific questions that were not addressed in the paper:\n1. Given two different DNN architectures with the same number of parameters, why do certain architectures generalize better than others? In other words, is it enough to consider only the size (# of parameters) of the network and the size of the input (number of samples and their dimensionality), to be able to reason about the generalization properties of a given network?\n\n2. Does it make sense to study the stability of predictions given added dropout during inference?\n\nFinally, provided a number of experiments and results, the authors do not draw a conclusion or offer a strong insight into what is going on with generalization in DNNs or how to proceed forward. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper presents a set of experiments where via clever use of randomization and noise addition authors demonstrate the enormous modeling (memorization) power of the deep neural networks with large enough capacity.  Yet these same models have very good generalization behavior even when all obvious explicit or implicit regularizers are removed.  These observations are used to argue that classical theory (VC dimension, Rademacher complexity, uniform stability) is not able to explain the generalization behavior of deep neural networks, necessitating novel theory.\n\nThis is a very interesting and thought provoking body of work and I am in complete accord with the observations and conclusions of the paper.  The classical generalization theory is indeed often at a loss with complex enough model families.  As the authors point out, once model families reach a point where they have capacity to memorize train sets, the classical theory does not yield useful results that could give insight into generalization behavior of these models, leaving one to empirical studies and observations.\n\nA minor clarification comment: On page 2, “ … true labels were replaced by random labels.”  Please state that random labels came from the same set as the true labels, to clarify the experiment.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}