{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Accept",
            "rating": "7: Good paper, accept",
            "review": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models.\nThe (hopefully soon) added comparison to Wang and Manning will make it stronger. \nThough it is sad that for sufficiently large datasets, NB-SVM still works better.\n\nIn the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.\n\nMinor comments:\n\n\"The capturing the similarities\" -- typo in line 2 of intro.\n\"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation\n ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting model and analysis",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work.\n\nHere are some comments on technical details:\n\n- The word \"discourse\" is confusing. I am not sure whether the words \"discourse\" in \"discourse vector c_s\" and the one in \"most frequent discourse\" have the same meaning.\n- Is there any justification about $c_0$ related to syntac?\n- Not sure what thie line means: \"In fact the new model was discovered by our detecting the common component c0 in existing embeddings.\" in section \"Computing the sentence embedding\"\n- Is there any explanation about the results on sentiment in Table 2?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Accept",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too.\n\nOverall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}