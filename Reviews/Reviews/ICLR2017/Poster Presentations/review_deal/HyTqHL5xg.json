{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The paper provides a clear application of variational methods for learning non-linear state-space models, which is of increasing interest, and of general relevance to the community.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "interesting way of learning nonlinear state space models",
            "rating": "7: Good paper, accept",
            "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I’d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows. \n\nIs there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI’m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn’t what’s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn’t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Rather incremental, but interesting experiments",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.\n\nMy compliments for the amount of detail put in the paper and appendix.\n\nThe experiments are on toy examples, but show promise.\n\n- Section 2.1: “In our notation, one would typically set beta_t = w_t, though other variants are possible” -> It’s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).\n\n- Section 2.2 last paragraph: “A key contribution is […] forcing the latent space to fit the transition”. This seems rather trivial to achieve.\n\n- Eq 9: “This interpretation implies the factorization of the recognition model:..”\nThe factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}