{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The paper presents a new regularization approach for deep learning that penalizes positive correlations between features in a network. The experimental evaluation is solid, and suggests the proposed regularized may help in learning better convolutional networks (but the gains are relatively small).",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Good results on deep nets",
            "rating": "7: Good paper, accept",
            "review": "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Orthogonality of weight features might not always lead to improved performance",
            "rating": "7: Good paper, accept",
            "review": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. \n\nOrthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better.\n\nAlthough the improvement in performances is not significant the direction of research and the observations made are promising.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid experimental validation",
            "rating": "7: Good paper, accept",
            "review": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}