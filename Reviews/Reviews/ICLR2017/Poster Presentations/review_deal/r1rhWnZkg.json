{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Need justification for why low-rank bilinear pooling is better",
            "rating": "7: Good paper, accept",
            "review": "Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.\n\nStrengths:\n\n1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. \n\n2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).\n\n3. The various design choices made in model development have been experimentally verified. \n\nWeaknesses/Suggestions:\n\n1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).\n\n2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? \n\n3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.\n\n4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.\n\n5. In the caption for Table 1, fix the following: “have not” -> “have no” \n\nReview Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Proposes simple 3 layer network of embedding projection, Hadamard product and linear layer as alternative to recently proposed compact bilinear pooling, achieves state of the art on COCO VQA with ~25% reduction in parameters",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. \n\nMissing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in\tthe compact bilinear model. Since the main contribution of the\npaper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). \nThis formulation is evaluated on the visual question answering (VQA) task together with several other model variants.\n\nStrength:\n1.\tThe paper discusses how the Hadamard product can be used to approximate the full outer product.\n2.\tThe paper provides an extensive experimental evaluation of other model aspect for VQA.\n3.\tThe full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.\n\nWeaknesses:\n1.\tNovelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA.\n2.\tExperimental evaluation:\n2.1.\tAn experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.\n2.2.\tAn experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.\n2.3.\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n2.4.\tComparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.\n3.\tNo theoretical analysis or properties of the approximation are presented.\n4.\tThe paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.\n5.\tRelated work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.\n\n\nMinor\n-\tIt is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.\n-\tSect 2, first sentence: “every pairs” -> “every pair”\n\n\nSummary:\nWhile the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.\n\nTo be more convincing I would like to see the following experiments \n-\tComparison with Outer product in the identical model\n-\tComparison with MCB in the identical model\n-\tComparison with elementwise sum instead of elementwise product\n-\tOne of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}