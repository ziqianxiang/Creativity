{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This work proposes a method for segmenting target generation sequence that is learned as part of the model. Generally all reviewers found this paper novel and interesting.\n \n Pros:\n - Quality: The paper is both containing \"with solid theoretically justified\" and \"present(s) nice improvements over character based result\". One reviewer asked for the \"the experimental study could be more solid.\"\n - Impact: methods like \"BPE\" are now somewhat standard hacks in seq2seq modeling. This type of model could be potentially impactful at disrupting. \n - Clarity: Reviewers found the work to be a \"clearly written paper\" \n \n Cons:\n - Some of the reviewers were not as enthuthiastic about this work compared to other papers. There were several comments asking for further experimental results. However I found that the author's responses clearly explained these away and provided clear justificition for why they were not necessary, already included, or explained by previous results. I feel that this takes care of the major issues, and warrants a small raise in score.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper proposes to learn decomposition of sequences (such as words) for speech recognition. It addresses an important issue and I forsee it being useful for other applications such as machine translation. While the approach is novel and well-motivated, I would very much like to see a comparison against byte pair encoding (BPE). BPE is a very natural (and important) baseline (i.e. dynamic vs fixed decomposition). The BPE performance should be obtained for various BPE vocab sizes. \n\nMinor points\n- Did the learned decompositions correspond to phonetically meaningful units? From the example in the appendix it's hard to tell if the model is learning phonemes or just most frequent character n-grams.\n- Any thoughts on applications outside of speech recognition? If this is shown to be effective in other domains it would be a really strong contribution (but this is probably outside the scope for now).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting paper, but raising a few questions.",
            "rating": "7: Good paper, accept",
            "review": "Interesting paper which proposes jointly learning automatic segmentation of words to sub words and their acoustic models.\n\nAlthough the training handles the word segmentation as hidden variable which depends also on the acoustic representations, during the decoding only maximum approximation is used.\n\nThe authors present nice improvements over character based results, however they did not compare results with word segmentation which does not assume the dependency on acoustic.\n\nObviously, only text based segmentation would result in two (but simpler) independent tasks. In order to extract such segmentation several publicly open tools are available and should be cited. Some of those tools can also exploit the unigram probabilities of the words to perform their segmentations.\n\nIt looks that the improvements come from the longer acoustical units - longer acoustical constraints which could lead to less confused search -, pointing towards full word models. In another way, less tokens are more probable due to less multiplication of probabilities. As a thought experiment for an extreme case: if all the possible segmentations would be possible (mixture of all word fragments, characters, and full-words), would the proposed model use word fragments at all? (WSJ is a closed vocabulary task). It would be good to show that the sub word model could outperform even a full-word model (no segmentation).\nYour model estimates p(z_t|x,z<t;\\theta), but during training both p(z_t|x,y,z<t;\\theta) and p(z_t|x,z<t;\\theta) are needed. Their connections and calculations, in general Section 2 should be more detailed. The experimental setup should be presented in a replicable way.\n\nThe authors use one sample of Z during the gradient calculation. Would not it be more consistent then to use the maximum instead of sampling? Or is even getting the maximum is computationally too expensive? Considering the left-to-right approximation, this might be possible and more consistent with your decoding, please address this issue in details.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}