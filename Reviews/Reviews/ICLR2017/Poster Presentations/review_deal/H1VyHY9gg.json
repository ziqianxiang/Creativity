{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The reviewers are reasonably supportive of this paper. The ideas presented in the paper are nice and the results are encouraging. The authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 Billion Words corpus, Chelba et al.).",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.\n\nComments: \n- p. 3 “can be seen has a way” -> “can be seen as a way” (?)\n- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?\n- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.\n- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simplistic, empirical but practically useful approach to improve LM and MT",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Strengths\n-\tA simple “noising” method for improving LM\n-\t“noise” added to word history by using a probabilistic distribution using N-gram smoothing\n-\tExperimental evidence that such simple techniques improve LM and MT\n\nWeaknesses\n-\tPurely empirical, with no theoretical justification\n-\tRather primitive step so far; would be nice to see future work in modeling different types of LM “noise” as hidden variables and then “denoise” them.\n\n"
        },
        {
            "title": "",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper discusses data noising as a regularization technique for language modelling as an alternative to dropout regularization. The key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise. Through this motivation, the authors present noising analogies to standard discounting, as well as Kneser-Ney smoothing.\n\nThe experiments are convincing in that the smoothed (noised) models outperform their unregularized baselines.\n\nMy main issue with the evaluation in this paper is that there is no comparison between the noising/smoothing idea and more conventional regularizers (such as L2 and dropout) which were discussed in the paper. Likewise, it would have been interesting if the techniques proposed here had been applied to stronger base models (such as the models compared with in Table 2). Seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in Zaremba's or Gal's!\n\nThose weaknesses aside (and I recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly. I strongly recommend the paper for acceptance.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}