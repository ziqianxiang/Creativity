{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Final review",
            "rating": "7: Good paper, accept",
            "review": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting work",
            "rating": "7: Good paper, accept",
            "review": "This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. \n+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. \n+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. \n+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. \n-It will be more interesting to show results in other domains such as texts and images. \n-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work, needs more explanations ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}