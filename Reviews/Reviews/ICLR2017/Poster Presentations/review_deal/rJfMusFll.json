{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "clearly written, natural extension of previous work",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\"\n\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning â€“ training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}