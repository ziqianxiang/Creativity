{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.\n \n Pros\n  - the main idea is well-motivated, from a non-standard perspective.\n  - There are lots of side-experiments supporting the claims for the motivation.\n Cons\n  - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.\n  - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.\n \n In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Well-written paper explores promising direction for training generalizable deep neural network, but empirical results are too preliminary to support some arguments made",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Overview: \n\nThis paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.\n\nPros:\n- Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks\n- Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize “flat” minima\n- Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area\n\nCons / points suggested for a rebuttal:\n(1) One claim of the paper given in the abstract is ”experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.“ This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, “In our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.” It's not clear to me how to reconcile those two claims.\n\n(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be “the number of parameter updates required to run through the dataset once.” It’s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.\n\n(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads “Actively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.“ According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.\n\n(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to “Flat Minima” by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.\n\n(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Insightful Work",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a new term that exploits both width and depth of the objective function. In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss. Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss. The smoothing obviously tends to suppress sharp minima.\n\nOverall, developing such regularization term based on thermodynamics concepts is very interesting. I have a couple of concerns that the authors may want to clarify in the rebuttal.\n\n1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD.\n\n2. The proposed algorithm approximates the smoothed \"exponentiated\" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of https://arxiv.org/pdf/1601.04114\n\n3. Section 4.4. Thank you for revising the statements related to the eigenvalues of the Hessian. However, even in the revised version, there seems to be some discrepancy. You \"assume no eigenvalue of the Hessian lies in the set [−2γ −c, c] for some small c > 0\". This essentially says the eigenvalues are far from zero. Such assumption seems to be in the opposite direction of the reality: the plots of eigenvalues (Figure 1) show most eigenvalues are indeed close to zero.\n\n4. Theorem 3 from Hardt 2015: The way you quote it differs from the original paper. Are you referring to the Theorem 3.12 of Hardt's paper? If so, why the difference, including elimination of dependency on constant c in the exponent of T?  \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "official review",
            "rating": "7: Good paper, accept",
            "review": "The paper introduces a new regularization term which encourages the optimizer \nto search for a flat local minimum of reasonably low loss instead of seeking a \nsharp region of a low loss. This is motivated by some empirical observations that\nlocal minima of good generalization performance tend to have flat shape. \nTo achieve this, a regularization term based on the free local energy is proposed\nand the gradient of this term, which do not have tractable closed-form solution, \nis obtained by performing Monte Carlo estimation using SGLD sampler. In the \nexperiments, the authors show some evidence of the flatness of good local \nminima, and also the performance of the proposed method in comparison to the\nAdam optimizer. \n\nThe paper is well and clearly written. I enjoyed reading the paper. The connection\nto the concept of free energy in optimization framework seems interesting. The \nmotivation of pursuing flatness is also well analyzed with a few experiments. I'm\nwondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)?\nAlso, I'm wondering why the authors did not add the experiment results on RNN in\nthe evaluation of the performance because char-lstm for text generation was \nalready used for the flatness experiments. I think adding more experiments on \nvarious models and applications of deep architectures (e.g., RNN, seq2seq, etc.) \nwill make the author's claim more persuasive. I also found the mixed usage of the\nterminology, e.g., free energy and free entropy, a bit confusing. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}