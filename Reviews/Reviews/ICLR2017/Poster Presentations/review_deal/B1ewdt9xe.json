{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.\n  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.\n Overall, this is an interesting, solid contribution.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, nice example of using the idea of feeding forward error signals.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "an interesting architecture for future prediction inspired by deep predictive coding",
            "rating": "6: Marginally above acceptance threshold",
            "review": "An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}