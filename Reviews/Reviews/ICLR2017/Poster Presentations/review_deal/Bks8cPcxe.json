{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "All reviewers find value in the contributions. ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper presents a domain specific language for the specification of deep learning models. The intermediate representation offers many possibilities for optimization and to focus on speed or runtime.\n\nThe paper is well-written and makes conclusive statements and comparisons. The experiments cover five fundamentally differenct CNN architectures, each evaluated for two batch sizes. They include the two competing frameworks Tensorflow and Caffe and show convincing performance. Overall, the paper is well-written and structured.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "This paper presents and evaluates a Scala-based deep learning framework called “DeepDSL,” describing the language’s syntactic and performance benefits with respect to existing frameworks.\n\n\nPros:\n\nThe use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users.  The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice.\n\nThe ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited.\n\nDeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures.\n\n\nCons:\n\nThere appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow).  This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API.  (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.)\n\nThe paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax.  This is misleading as Caffe has officially supported a compact network definition style called “NetSpec” for years -- see a ~15 line definition of AlexNet [1].  Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format.\n\nDeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework.  Some examples:\n\n“This separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.”  This separation is not unique -- it’s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it’s not the default workflow there).\n\n“The difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as ‘networks’ but as abstract ‘functions’.” There is no notion of a “network” in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL.  I asked about this statement, and the response didn’t convince me otherwise.  The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution.  This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2].  It may be true that DeepDSL is more “batteries included” for writing compact network definitions than these other frameworks, but the paper’s claims seem to go beyond this.\n\n\nOverall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise.\n\n\n[1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24\n[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92\n\n=====================\n\nUpdate: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review.  I've upgraded my rating from 5 (below threshold) to 7 (good paper, accept).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ICLR is not the right venue for this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents DeepDSL, a \"domain specific language (DSL) embedded in\nScala, that compiles deep networks written in DeepDSL to Java source code\". It\nintroduces its syntax and the key concepts which differentiate it\nfrom other existing frameworks, such as Torch7, Theano, Caffe, TensorFlow,\nCNTK, Chainer and MXNet. It also benchmarks speed and memory usage against\nTensorFlow and Caffe on a variety of convolutional neural network architectures.\n\nThe paper is clear and well written and it does a good job of presenting DeepDSL\nin the context of existing deep learning frameworks.\n\nHowever, I don't think ICLR is the right venue for this type of work. Some of\nthe ideas it presents are interesting, but overall the paper lacks novelty and\npotential impact and stays firmly within the realm of deep learning framework\nwhitepapers such as [1,2,3,4], which to my knowledge don't have a precedent of\nbeing accepted at venues like ICLR.\n\n[1]: Bergstra, James, et al. \"Theano: A CPU and GPU math compiler in Python.\"\nProc. 9th Python in Science Conf. 2010.\n\n[2]: Bastien, Frédéric, et al. \"Theano: new features and speed improvements.\"\narXiv preprint arXiv:1211.5590 (2012).\n\n[3]: Abadi, Martın, et al. \"Tensorflow: Large-scale machine learning on\nheterogeneous distributed systems.\" arXiv preprint arXiv:1603.04467 (2016).\n\n[4]: The Theano Development Team et al. \"Theano: A Python framework for fast\ncomputation of mathematical expressions.\" arXiv preprint arXiv:1605.02688\n(2016).\n\nUPDATE: The rating has been revised to a 6 following the authors' reply.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}