{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "Reviewers found this work to be a \"well-motivated\", \"good contribution\", and \"clever\". The idea was clearly conveyed and reviewers were convinced that the approach was simpler than others like NTMs. Experiments are sufficient, and the work will likely be used in the future. \n \n Pros:\n - Well-explained and expected to be widely implemented\n - Experimental results convincing on the tasks.\n \n Cons:\n - Several questions about \"generalization to some unseen entities\". \n - Reliance on synthetic tasks unclear if \"scalable to complex real tasks\"\n - Training process seems quite complicated (although again simpler that NTMs)",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes a new memory augmented neural network (MANN) model called recurrent entity network (EntNet). EntNet can be considered as a bank of RNNs with gating mechanism to update the hidden states of the RNNs and the hidden states act like the memory slots. The model is very much relevant to NTM style architectures. It is known that training the controller in NTM to read/write from memory slots is challenging. EntNet cleverly pushes the complexity of the controller to individual memory slots. It is as if each slot has a controller and they all act in a distributed manner.\n\nAuthors report strong results in bAbI tasks where the model achieves state of the art performance. Synthetic world experiments justify that the model learns to capture the world dynamics. However it is not clear if this will be scalable to complex real tasks. EntNet also achieves reasonable performance with one-shot reading of the passage in CBT task.\n\nI see EntNet as a generalization of RNNs and has some advantage over NTMs when it comes to training complexity. This is definitely a good contribution to the conference. I see that EntNet can have several other applications in future.\n\nAuthors have provided convincing answers to my pre-review questions.\n\nFew more questions:\n1. Do you fix the size of the f vector set in equation (1)? If so, to what size in all the experiments?\n2. There are so many training details in the paper which makes it difficult to reproduce the results. Can the authors release the source code to reproduce all the results in the paper? I am willing to increase my rating if authors can release the code.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "The work proposes a variant of a recurrent neural network that can selectively update a fixed number of multiple memory slots to update entity states.\n\nThe architecture is well motivated, especially with the motivating example, and the operation is shown to validate the intuition as shown in visualizations.\n\nExperimental results, datasets and the baselines used are sufficient to quantitatively show the strength of the proposed architecture.\n\nA limitation is failing to (explicitly) generalize to unseen entities, however this is not a trivial problem on its own and the authors have addressed to this issue and proposed several ideas as workarounds.\n\nI consider the work as a good conference contribution.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "The paper proposed a multi-memory mechanism that memorizes different information into different components/entities. It could be considered as a mixture model in RNN. This is a very interesting model and result is convincing.\n\nA limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}