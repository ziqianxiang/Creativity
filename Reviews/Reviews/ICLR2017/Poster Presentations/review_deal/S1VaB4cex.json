{
    "Decision": {
        "title": "ICLR committee final decision",
        "comment": "The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Unsatisfactory experiments and restrictively large number of parameters",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep” networks while residuals are incidental.\n\nThe main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep” networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.\n\nAuthors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. \n\nTable 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nAlso, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. \n\nAlthough the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. \n\nPros:\nProvides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis\n\ncons:\n     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters\nThe claims are intuitive but not supported well with empirical evidence\nPath regularization does not yield improvement when the data augmentation is used\n     -     The empirical results do not show whether the method is promising for “ultra-deep” networks ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Unconvincing experimental comparisons",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "weak comparison",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper presents a strategy for building deep neural networks via rules for expansion and merging of sub-networks.\npros:\n- the idea is novel\n- the approach is described clearly\ncons:\n- the experimental evaluation is not convincing, e.g. no improvement on SVHN\n- number of parameters should be mentioned for all models for fair comparison\n- the effect of drop-path seems to vanish with data augmentation",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}