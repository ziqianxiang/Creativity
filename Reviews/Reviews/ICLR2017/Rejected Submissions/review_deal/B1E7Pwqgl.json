{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.\n \n (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)"
    },
    "Reviews": [
        {
            "title": "",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors proposes an interesting idea of connecting the energy-based model (descriptor) and \nthe generator network to help each other. The samples from the generator are used as the initialization \nof the descriptor inference. And the revised samples from the descriptor is in turn used to update\nthe generator as the target image. \n\nThe proposed idea is interesting. However, I think the main flaw is that the advantages of having that \narchitecture are not convincingly demonstrated in the experiments. For example, readers will expect \nquantative analysis on how initializing with the samples from the generator helps? Also, the only \nquantative experiment on the reconstruction is also compared to quite old models. Considering that \nthe model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison \nto that model. \n\n** Minor\n- I'm wondering if the analysis on the convergence is sound when considering the fact that samples \nfrom SGLD are biased samples (with fixed step size). \n- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but improperly evaluated",
            "rating": "3: Clear rejection",
            "review": "This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”.\n\nThe above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.\n\nIn a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ? Run comparative experiments.\n\nThe paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.\n\nPROS:\n+ Interesting and novel idea\nCONS:\n- Improper experimental protocols\n- Missing baselines\n- Missing diagnostic experiments\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}