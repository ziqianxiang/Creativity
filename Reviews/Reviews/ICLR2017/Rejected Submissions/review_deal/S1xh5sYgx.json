{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The paper proposes a ConvNet architecture (\"SqueezeNet\") and a building block (\"Fire module\") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method."
    },
    "Reviews": [
        {
            "title": "Are fire modules an application of concepts from GoogLeNet and ResNet?",
            "rating": "7: Good paper, accept",
            "review": "Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn’t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "excellent empirical work with potential impact, but the impact would be greater if insights and analysis can be provided by reading back earlier work aimed to analyse the “by-pass” architecture by mixing linear and nonlinear predictions similar to the proposed architecture ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Strengths\n-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n-- x50 less memory usage than AlexNet, keeping similar accuracy \n-- strong experimental results\n\nWeaknesses\n--Would be nice to test Sqeezenet on multiple tasks\n\n--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable convnet engineering paper",
            "rating": "7: Good paper, accept",
            "review": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}