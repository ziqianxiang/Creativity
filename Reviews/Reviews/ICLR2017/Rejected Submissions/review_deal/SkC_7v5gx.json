{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The reviewers agreed that the main contribution is the first empirical analysis on large-scale convolutional networks concerning layer-to-layer sparsity. The main concerns were that of novelty (connection-wise sparsity being explored previously but not in large-scale domains) and the importance given the current state of fast implementations of sparsely connected CNNs. The authors argued that the point of their paper was to drive software/hardware co-evolution and guide the next generation of, e.g. CUDA tools development. The confident scores were borderline while the less confident reviewer was pleased with the paper so I engaged the reviewers in a discussion post author-feedback. The consensus was that the paper presented promising but not fully developed nor convincing research."
    },
    "Reviews": [
        {
            "title": "Useful analysis ",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper experiments with channel to channel sparse neural networks.\nThe paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.\nThe results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. \nThis paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Less parameters using sparse random connection structure and more discriminative power by gradual densifying of the network",
            "rating": "7: Good paper, accept",
            "review": "The paper is about channel sparsity in Convolution layer.\nThe paper is well written and it elaborately discussed and investigated different approaches for applying sparsity.  The paper contains detailed literature review.\nIn result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices.\nThe main point is that the paper needs more detailed investigation on different dropout schedule.\nAs mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "seems promising, but incomplete without real gains",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.  Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.\n\nUnfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.  In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.  The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.\n\nThe section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.  The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.\n\nOverall, the method and evaluations show that the basic approach has promise.  However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.  Without this last step, it still seems incomplete to me for a conference paper.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}