{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks\n hence the zero-shot generalization, which is considered to be the primary challenge to be solved. \n The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.\n \n With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers. \n At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice. \n \n While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present."
    },
    "Reviews": [
        {
            "title": "Potentially good architecture; insufficient evaluation for \"large-scale\" tasks, no comparison to other state-of-the-art methods",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Description:\n\nThis paper presents a reinforcement learning architecture where, based on \"natural-language\" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks.\n\nThe subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an \"analogy-making\" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).\n\nThe meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.\n\nTraining involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.\n\nThe system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.\nIt is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.\n\n\nEvaluation:\n\nThe proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the \"right\" way to do it.\n\nI do not feel the grid world here really represents a \"large-scale task\": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work.\n\nMoreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel architectural ideas ; algorithmically complex",
            "rating": "7: Good paper, accept",
            "review": "This paper presents an architecture and corresponding algorithms for\nlearning to act across multiple tasks, described in natural language.\nThe proposed system is hierarchical and is closely related to the options\nframework. However, rather than learning a discrete set of options, it learns\na mapping from natural instructions to an embedding which implicitly (dynamically)\ndefines an option. This is a novel and interesting new perspective on options\nwhich had only slightly been explored in the linear setting (see comments below).\nI find the use of policy distillation particularly relevant for this setting.\nThis, on its own, could be a takeaway for many RL readers who might not necessarily\nbe interested about NLP applications.\n\nIn general, the paper does not describe a single, simple, end-to-end,\nrecipe for learning with this architecture. It rather relies on many recent\nadvances skillfully combined: generalized advantage estimation, analogy-making\nregularizers, L1 regularization, memory addressing, matrix factorization,\npolicy distillation. I would have liked to see some analysis but\nunderstand that it would have certainly been no easy task.\nFor example, when you say \"while the parameters of the subtask controller are\nfrozen\", this sounds to me like you're having some kind of two-timescale stochastic gradient\ndescent. I'm also unsure how you deal with the SMDP structure in your gradient\nupdates when you move to the \"temporal abstractions\" setting.\n\nI am inclined to believe that this approach has the potential to scale up to\nvery large domains, but paper currently does not demonstrate this\nempirically. Like any typical reviewer, I would be tempted to say that\nyou should perform larger experiments. However, I'm also glad that you have\nshown that your system also performs well in a \"toy\" domain. The characterization\nin figure 3 is insightful and makes a good point for the analogy regularizer\nand need for hierarchy.\n\nOverall, I think that the proposed architecture would inspire other researchers\nand would be worth being presented at ICLR. It also contains novel elements\n(subtask embeddings) which could be useful outside the deep and NLP communities\ninto the more \"traditional\" RL communities.\n\n# Parameterized Options\n\nSutton et. al (1999) did not explore the concept\nof *parameterized* options originally. It only came later, perhaps first with\n[\"Optimal policy switching algorithms for reinforcement\nlearning, Comanici & Precup, 2010\"] or\n[\"Unified Inter and Intra Options Learning Using Policy Gradient Methods\", Levy & Shimkin, 2011].\nKonidaris also has a line of work  on \"parametrized skills\":\n[\"Learning Parameterized Skills\". da Silva, Konidaris, Barto, 2012)]\nor [\"Reinforcement Learning with Parameterized Actions\". Masson, Ranchod, Konidaris, 2015].\n\nAlso, I feel that there is a very important distinction to be made with\nthe expression \"parametrized options\". In your work, \"parametrized\" comes in\ntwo flavors. In the spirit of policy gradient methods,\nwe can have options whose policies and termination functions are represented\nby function approximators (in the same way that we have function approximation\nfor value functions). Those options have parameters and we might call them\n\"parameterized\" because of that. This is the setting of Comanicy & Precup (2010),\nLevy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and\nMannor (2016) for example.\n\nNow, there a second case where options/policies/skills take parameters *as inputs*\nand act accordingly. This is what Konidaris & al. means by \"parameterized\", whose\nmeaning differs from the \"function approximation\" case above.\nIn your work, the embedding of subtasks arguments is the \"input\" to your options\nand therefore behave as \"parameters\" in the sense of Konidaris.\n\n# Related Work\n\nI CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's\nwork. Branavan's PhD thesis had to do with using control techniques from RL\nin order to interpret natural instructions so as to achieve a goal. For example,\nin \"Reinforcement Learning for Mapping Instructions to Actions\", an RL agent\nlearns from \"Windows troubleshooting articles\" to interact with UI elements\n(environment) through a Softmax policy (over linear features) learned by policy\ngradient methods.\n\nAs you mention under \"Instruction execution\" the focus of your work in\non generalization, which is not treated explicitely (afaik) in Branavan's work.\nStill, it shares some important algorithmic and architectural similarities which\nshould be discussed explicitly or perhaps even compared to in your experiments\n(as a baseline).\n\n## Zero-shot and UVFA\n\nIt might also want to consider\n\"Learning Shared Representations for Value Functions in Multi-task\nReinforcement Learning\", Borsa, Graepel, Shawe-Taylor]\nunder the section \"zero-shot tasks generalization\". \n\n\n# Minor Issues\n\nI first read the abstract without knowing what the paper would be about\nand got confused in the second sentence. You talk about \"longer sequences of\npreviously seen instructions\", but I didn't know what clearly\nmeant by \"instructions\" until the second to last sentence where you specify\n\"instructions described by *natural language*.\" You could perhaps\nre-order the sentences to make it clear in the second sentence that you are\ninterested in NLP problems.\n\nZero-generalization: I was familiar with the term \"one-shot\" but not \"zero-shot\".\nThe way that the second sentence \"[...] to have *similar* zero-shot [...]\" follows\nfrom the first sentence might as well hold for the \"one-shot\" setting. You\ncould perhaps add a citation to \"zero-shot\", or define it more\nexplicitly from the beginning and compare it to the one-shot setting. It could\nalso be useful if you explain how zero-shot relates to just the notion of\nlearning with \"priors\".\n\nUnder section 3, you say \"cooperate with each other\" which sounds to me very much\nlike a multi-agent setting, which your work does not explore in this way.\nYou might want to choose a different terminology or explain more precisely if there\nis any connection with the multi-agent setting.\n\nThe second sentence of section 6 is way to long and difficult to parse. You could\nprobably split it in two or three sentences.\n"
        },
        {
            "title": "RL by learning to take advice ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). \n\nA fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). \n\nNice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review. ",
            "rating": "3: Clear rejection",
            "review": "The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). \n\nOverall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. \n\nHowever, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). \n\nThe paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. \n\nI believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}