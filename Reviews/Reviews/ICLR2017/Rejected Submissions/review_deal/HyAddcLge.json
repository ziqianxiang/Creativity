{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware."
    },
    "Reviews": [
        {
            "title": "Interesting Idea, but can be improved and I have a few concerns about experiments",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right? \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "no title",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}