{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "This work proved to be a controversial submission. This paper has two main components: 1) a neural framework TBRU/DRAGNN, 2) experimental results on some NLP tasks. Generally there was lack of consensus about the originality of (1) and a general feeling that even if there are aspect of novelties, that the paper was lacking clarity about is contributions One reviewer was an outlier, highlighting the benefit of the ability \"incorporate dynamic recurrent connections through the definition of the transition system\" which is claimed to be really novel. Others claim this is specific to the framework used. Negative reviewers felt that (1) is probably not novel within itself and represents a slight departure from stack-lstm. The controversy here is whether DRAGNN is simple \"software engineering with no inherent \"free things\" that would lead to impact within the community. This question of impact is also inherent to (2), in particular whether the authors really got new benefit of using DragNN or whether these are \"reimplementations of things in the literature\". I felt the reviewers did seem to put in due diligence here, so the recommendation would be to put further effort into clarification and further back up of novelty claims in future versions."
    },
    "Reviews": [
        {
            "title": "Final review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Overall, this is a nice paper. Developing a unifying framework for these newer\nneural models is a worthwhile endeavor.\n\nHowever, it's unclear if the DRAGNN framework (in its current form) is a\nsignificant standalone contribution. The main idea is straightforward: use a\ntransition system to unroll a computation graph. When you implement models in\nthis way you can reuse code because modules can be mixed and matched. This is\nnice, but (in my opinion) is just good software engineering, not machine \nlearning research.\n\nMoreover, there appears to be little incentive to use DRAGNN, as there are no\n'free things' (benefits) that you get by using the framework. For example:\n\n- If you write your neuralnet in an automatic differentiation library (e.g.,\n  tensorflow or dynet) you get gradients for 'free'.\n\n- In the VW framework, there are efficiency tricks that 'the credit assignment\n  compiler' provides for you, which would be tedious to implement on your\n  own. There is also a variety of algorithms for training the model in a\n  principled way (i.e., without exposure bias).\n\nI don't feel that my question about the limitations of the framework has been\nsatisfactorily addressed. Let me ask it in a different way: Can you give me\nexamples of a few models that I can't (nicely) express in the DRAGNN framework?\nWhat if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or\nhttp://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the\ndynamic programming components as transition units and (importantly) would it be\nefficient?\n\n disagree that the VW framework is orthogonal, it is a *competing* way to\nimplement recurrent models. The main different to me appears to be that VW's\nimperative framework is more general, but less modular.\n\nThe experimental contribution seems useful as does the emphasis on how easy it\nis to incorporate multi-task learning.\n\nMinor:\n\n- It would be useful to see actual code snippets (possibly in an\n  appendix). Otherwise, its unclear how modular DRAGNN really are.\n\n- The introduction states that (unlike seq2seq+attention) inference remains\n  linear. Is this *necessarily* the case? Users define a transition system that\n  is quadratic, just let attention be over all previous states. I recommend that\n  authors rephrase statement more carefully.\n\n- It seems strange to use A() as in \"actions\", then use d as \"decision\" for its\n  elements.\n\n- I recommend adding i as an argument to the definition of the recurrence\n  function r(s) to make it clear that it's the subset of previous states at time\n  i, otherwise it looks like an undefined variable. A nice terse option is to\n  write r(s_i).\n\n- Real numbers should be \\mathbb{R} not \\mathcal{R}.\n\n- It's more conventional to use t for a time-step instead of i.\n\n- Example 2: \"52 feature embeddings\" -> did you mean \"52-DIMENSIONAL feature\n  embeddings\"?\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "",
            "rating": "7: Good paper, accept",
            "review": "The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google’s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. \n\nThe most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). \n\nThe “raison d’etre,” in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I’d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. \n\nOverall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "official review",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}