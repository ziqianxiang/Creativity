{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference."
    },
    "Reviews": [
        {
            "title": "Insightful theoretical derivation, experiments can be improved.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea, but not very convincing",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theory, but experimental results not (yet) very convincing, unfortunately.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}