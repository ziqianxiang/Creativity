{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline."
    },
    "Reviews": [
        {
            "authors": []
        },
        {
            "title": "a lot of interesting experiments but limited novelty",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.\n\nThe main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,\n\n1) the model compression approach of https://arxiv.org/abs/1510.00149\n2) training with integer/binary weights https://arxiv.org/abs/1511.00363\n\nOverall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Straightforward application of Bloom filter embedding and membership checking, not a lot of novelty; experiments should test more dimensionalities",
            "rating": "3: Clear rejection",
            "review": "Description:\n\nThis paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\".\n\nThe embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.\n\nInputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.\n\nDesired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.\n\nThe paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach.\n\n\nEvaluation:\n\nIt is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test.\n\nThe way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step.\n\nIn the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?)\n\nOverall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable.\n\n\nMinor points:\n\nAs the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned.\n\nThe experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straightforward but marginal impact",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.\n\nPros:\n\n- Can be applied to practically any model, either at the input or hte output.\n- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. \n\nCons:\n\n- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.\n- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.\n\nThis seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}