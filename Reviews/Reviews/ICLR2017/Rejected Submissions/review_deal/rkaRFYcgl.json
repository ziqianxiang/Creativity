{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR committee final decision",
        "comment": "The reviewers seem to agree that the framework presented is not very novel, something I agree with.\n The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR."
    },
    "Reviews": [
        {
            "title": "Exploring a solid idea, but results are not convincing",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks.\n\nThe paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.)\n\nAside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network.\n\nAuthor also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks.\n\nAn empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). \n\nHowever, there are few problems with the evaluation:\n\n- In the highway network experiment, the author does not compare with a baseline.\nWe can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer  (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?.\n\n- It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison.\nAlso the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline.\n\n-Author claims state-of-art in the memory task. However, their approach uses  more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size.\n\n- It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task.\n\n- it is not clear when to use low-rank or low-rank + diagonal from the experiments.\n\nOverall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "my review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.\nThat said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs."
        }
    ]
}