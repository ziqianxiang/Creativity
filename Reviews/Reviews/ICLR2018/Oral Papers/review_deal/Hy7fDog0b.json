{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All three reviewers were positive about the paper, finding it to be on an interesting topic and with broad applicability. The results were compelling and thus the paper is accepted. ",
        "decision": "Accept (Oral)"
    },
    "Reviews": [
        {
            "title": "A nice paper dealing with an important problem.",
            "rating": "7: Good paper, accept",
            "review": "Quick summary:\nThis paper shows how to train a GAN in the case where the dataset is corrupted by some measurement noise process. They propose to introduce the noise process into the generation pipeline such that the GAN generates a clean image, corrupts its own output and feeds that into the discriminator. The discriminator then needs to decide whether this is a real corrupted measurement or a generated one.  The method is demonstrated to the generate better results than the baseline on a variety of datasets and noise processes.\n\nQuality:\nI found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simple. \n\nOriginality:\nI'm pretty sure this is the first paper to tackle this problem directly in general.\n\nSignificance:\nThis is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumstances. \n\nPros:\n* Important problem\n* elegant and simple solution\n* nice results and decent experiments (but see below)\n\nCons:\n* The assumption that the measurement process *and* parameters are known is quite a strong one. Though it is quite common in the literature to assume this, it would have been interesting to see if there's a way to handle the case where it is unknown (either the process, parameters or both).\n* The baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the \"fixed\" version which is fed into them. I can't however, think of other baselines other than \"ignore\" so I guess that is acceptable.\n* I wish the authors would show that they get a *useful* model eventually - for example, can this be used to denoise other images from the dataset?\n\nSummary:\nThis is a nice paper which deals with an important problem, has some nice results and while not groundbreaking, certainly merits a publication.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper exploring GAN training under a linear projection measurement model.",
            "rating": "7: Good paper, accept",
            "review": "The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise. The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models.\nThe AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space. The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training.\nThe theoretical analysis is satisfactory. However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training. For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult. The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data. It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves. Broadly, the approach is to have a generator produce the \"full\" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples. By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements.\n\nThe paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing. The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement). However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative.\n\nI would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.). Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training). I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}