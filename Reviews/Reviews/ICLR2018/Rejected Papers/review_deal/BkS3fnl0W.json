{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper presents a framework where GANs are used to improve detection of outliers (in this context, instances of the “background class”). This is a very interesting and, as demonstrated, promising idea. However, the general feeling of the reviewers is that more work is needed to make the technical and evaluations parts convincing. Suggestions for further work towards this direction include: theoretical analysis, better presentation of the manuscript and, most importantly, stronger experimental section. "
    },
    "Reviews": [
        {
            "title": "Questionable formulation with insufficient experiments",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The idea of using GANs for outlier detection is interesting and the problem is relevant. However, I have the following concerns about the quality and the significance:\n- The proposed formulation in Equation (2) is questionable. The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence. However, then what is learned though the proposed formulation? Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable.\n- In addition to the above point, I guess the expectation is needed as the original formulation of GAN. Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined.\n- In experiments, although the authors say \"lots of datasets are used\", only two datasets are used, which is not enough to examine the performance of outlier detection methods. Moreover, outliers are artificially generated in these datasets, hence there is no evaluation on pure real-world datasets. To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments.\n- As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments.\n  Although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data, this is not always correct.\n  Please see the paper:\n  -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)\n  This paper shows that the performance gets even better for higher dimensional data if each feature is relevant.\n  I recommend to add some distance-based outlier detection methods as baselines in experiments.  \n- Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others). Otherwise in practice how to set these parameters to get better results is not obvious.\n\n* The clarity of this paper is not high as the proposed method is not well explained. In particular, please mathematically formulate each proposed technique in Section 4.\n\n* Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high.\n\nMinor comments:\n- P.1, L.5 in the third paragraph: architexture -> architecture\n- What does \"Cor\" of CorGAN mean?\n\nAFTER REVISION\nThank you to the authors for their response and revision. Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In my view this paper is a clear rejection, a few interesting heuristics are presented without sufficient theoretical or experimental justification",
            "rating": "3: Clear rejection",
            "review": "This paper addresses the problem of one class classification. The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class.\n\nThe suggested techniques are nice and show promising results. But I feel a lot can still be done to justify them, even just one of them. For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values. But, in the experimental section results are shown only for a  single value, alpha_new=0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested. \n\nThe writing of the paper is also very unclear, with several repetitions and many typos e.g.:\n\n'we first introduce you a'\n'architexture'\n'future work remain to'\n'it self'\n\nI believe there is a lot of potential in the approach(es) presented in the paper. In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but paper and experiments need revision",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The idea of the paper is to use a GAN-like training to learn a novelty detection approach. In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers. To establish such a behavior, the authors propose early stopping as well as other heuristics. \n\nI like the idea of the paper, however, this paper needs a revision in various aspects, which I simply list in the following:\n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \"inlier accepted fraction\" is not well motivated in my opinion. This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data. The latter is important for the GAN-like training.\n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. \n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)\n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)\n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)\n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ())\n* The term \"AI-related task\" sounds a bit too broad\n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection.\n* Where is Table 1?\n* There are quite a lot of typos.\n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating. The clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}