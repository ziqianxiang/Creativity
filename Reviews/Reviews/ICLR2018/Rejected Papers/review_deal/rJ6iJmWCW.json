{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a method for accented speech generation using GANs.\nThe reviewers have pointed out the problems in the justification of the method (e.g. the need for using policy gradients with a differentiable objective) as well as its evaluation."
    },
    "Reviews": [
        {
            "title": "Relevant work, but not executed or presented well",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper presents a method for generating speech audio in a particular accent. The proposed approach relies on a generative adversarial network (GAN), combined with a policy approach for joining together generated speech segments. The latter is used to deal with the problem of generating very long sequences (which is generally difficult with GANs).\n\nThe problem of generating accented speech is very relevant since accent plays a large role in human communication and speech technology. Unfortunately, this paper is hard to follow. Some of the approach details are unclear and the research is not motivated well. The evaluation does not completely support the claims of the paper, e.g., there is no human judgment of whether the generated audio actually matches the desired accent.\n\nDetailed comments, suggestions, and questions:\n- It would be very useful to situate the research within work from the speech community. Why is accented modelling important? How is this done at the moment in speech synthesis systems? The paper gives some references, but without context. The paper from Ikeno and Hansen below might be useful.\n- Accents are also a big problem in speech recognition (references below). Could your approach give accent-invariant representations for recognition?\n- Figure 1: Add $x$, $y$, and the other variables you mention in Section 3 to the figure.\n- What is $o$ in eq. (1)?\n- Could you add a citation for eq. (2)? This would also help justifying that \"it has a smoother curve and hence allows for more meaningful gradients\".\n- With respect to the critic $C_\\nu$, I can see that it might be helpful to add structure to the hidden representation. In the evaluation, could you show the effect of having/not having this critic (sorry if I missed it)? The statement about \"more efficient layers\" is not clear.\n- Section 3.4: If I understand correctly, this is a nice idea for ensuring that generated segments are combined sensibly. It would be helpful defining with \"segments\" refer to, and stepping through the audio generation process.\n- Section 4.1: \"using which we can\" - typo.\n- Section 5.1: \"Figure 1 shows how the Wasserstein distance ...\" I think you refer to the figure with Table 1?\n- Figure 4: Add (a), (b) and (c) to the relevant parts in the figure.\n\nReferences that might be useful:\n- Ikeno, Ayako, and John HL Hansen. \"The effect of listener accent background on accent perception and comprehension.\" EURASIP Journal on Audio, Speech, and Music Processing 2007, no. 3 (2007): 4.\n- Van Compernolle, Dirk. \"Recognizing speech of goats, wolves, sheep and… non-natives.\" Speech Communication 35, no. 1 (2001): 71-79.\n- Benzeghiba, Mohamed, Renato De Mori, Olivier Deroo, Stephane Dupont, Teodora Erbes, Denis Jouvet, Luciano Fissore et al. \"Automatic speech recognition and speech variability: A review.\" Speech communication 49, no. 10 (2007): 763-786.\n- Wester, Mirjam, Cassia Valentini-Botinhao, and Gustav Eje Henter. \"Are We Using Enough Listeners? No!—An Empirically-Supported Critique of Interspeech 2014 TTS Evaluations.\" In Sixteenth Annual Conference of the International Speech Communication Association. 2015.\n\nThe paper tries to address an important problem, and there are good ideas in the approach (I suspect Sections 3.3 and 3.4 are sensible). Unfortunately, the work is not presented or evaluated well, and I therefore give a week reject.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Policy gradients are not needed for continuous latent variables",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper considers speech generation conditioned on an accent class.\nLeast Squares GAN and a reconstruction loss is used to train the network.\n\nThe network is using continuous latent variables. These variables are trained by policy gradients.\nI do not see a reason for the policy gradients. It would be possible to use the cleaner gradient from the discriminator.\nThe decoder is already trained with gradient from the discriminator.\nIf you are worried about truncated backpropagation through time,\nyou can bias it by \"Unbiasing Truncated Backpropagation Through Time\" by Corentin Tallec and Yann Ollivier.\n\n\nComments on clarity:\n- It would be helpful to add x, z, y, o labels to the Figure 1.\nI understood the meaning of `o` only from Algorithm 1.\n- It was not clear from the text what is called the \"embedding variable\". Is it `z`?\n- It is not clear how the skip connections connect the encoder and the decoder.\nAre the skip connections not used when generating?\n- In Algorithm 1, \\hat{y}_k is based on z_k, instead of \\hat{z}_k. That seems to be a typo.\n\nComments on evaluation:\n- It is hard to evaluate speech conditioned just on the accent class.\nOverfitting may be unnoticed.\nYou should do an evaluation on a validation set.\nFor example, you can condition on a text and generate samples\nfor text sentences from a validation set.\nPeople can then judge the quality of the speech synthesis.\nA good speech synthesis would be very useful.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper lacks any novel technical insight, contributions are not explained well, exposition is poor, and the evaluations are invalid.",
            "rating": "3: Clear rejection",
            "review": "The contributions made by this paper is unclear. As one of the listed contributions, the authors propose using policy gradient. However, in this setting, the reward is a known differentiable function, and the action is continuous, and thus one could simply backpropagate through to get the gradients on the encoder. Also, it seems the reward is not a function of the future actions, which further questions the need for a reinforcement learning formulation.\n\nThe paper is written poorly. For instance, I don't understand what this sentence means: \"We condition the latent variables to come from rich distributions\". Observed accent labels are referred to as latent (hidden) variables.\n\nWhile the independent Wasserstein critic is useful to study whether models are overfitting (by comparing train/heldout numbers), their use for comparing across different model types is not justified. Moreover, since GAN-based methods optimize the Wasserstein distance directly, it cannot serve as a metric to compare GAN-based models with other models.\n\nAll of the models compared against do not use accent information during training (table 2), so this is not a fair comparison.\n\nOverall, the paper lacks any novel technical insight, contributions are not explained well, exposition is poor, and the evaluations are invalid.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}