{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper reports experiments where a LSTM language model is pretrained on a large corpus of reviews, and then the produced representation is used within a classifier on a number of sentiment classification datasets.  The relative success of the method is not surprising. The novelty is very questionable, the writing quality is mixed (e.g., typos, the model is not even properly described). There are many gaps in evaluation (e.g., from the intro it seems that the main focus is showing that byte level modeling is preferable to more standard set-ups -- characters / BPE / words). However, there are (almost) no experiments supporting this claim. The same is true for the 'sentiment neuron': its effectiveness is also not properly demonstrated. In general, the results are somewhat mixed.\n\nPros:\n-- good results on some datasets\nCons:\n-- limited novelty\n-- some claims are not tested / issues with evaluation\n-- writing quality is not sufficient / clarity issues\n\n\nOverall, the reviewers are in agreement that the paper does not meet ICLR standards.\n\n"
    },
    "Reviews": [
        {
            "title": "Interesting experiments but lack of model description",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose to use a byte level RNN to classify reviews. In the meantime, they learn to generate reviews. The authors rely on the multiplicative LSTM proposed by Krause et al. 2016, a generative model predicting the next byte. They apply this architecture on the same task as the original article: document classification; they use a logistic regression on the extracted representation. The authors propose an evaluation on classical datasets and compare themselves to the state of the art.\nThe authors obtain interesting results on several datasets. They also explore the core of the unsupervised architecture and discover a neuron which activation matches the sentiment target very accurately. A deeper analyze shows that this neuron is more efficient on small datasets than on larger.\nExploiting the generative capacity of the network, they play with the \"sentiment neuron\" to deform a review. Qualitative results are interesting.\n\n\n\n\nThe authors do not propose an original model and they do not describe the used model inside this publication.\n\nNor the model neither the optimized criterion is detailled: the authors present some curve mentioning \"bits per character\" but we do not know what is measured. In fact, we do not know what is given as input and what is expected at the output -some clues are given in the experimental setup, but not in the model description-.\n\nFigure 2 is very interesting: it is a very relevant way to compare authors model with the literature.\n\nUnfortunately, the unsupervised abilities of the network are not really explained: we are a little bit frustrated by section 5.\n\n==\n\nThis article is very interesting and well documented. However, according to me, the fact that it provides no model description, no model analysis, no modification of the model to improve the sentiment discovery, prevents this article from being publicized at ICLR.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper needs more serious work",
            "rating": "2: Strong rejection",
            "review": "First of all, I don't think I fully understand this paper, because it is difficult for me to find answers from this paper to the following questions:\n1) what is the hypothesis in this paper? Section 1 talks about lots of things, which I don't think is relevant to the central topic of this paper. But it misses the most important thing: what is THIS paper (not some other deep learning/representation problems)\n2) about section 2, regardless whether this is right place to talk about datasets, I don't understand why these two datasets. Since this paper is about generating reviews and discovering sentiment (as indicated in the paper)\n3) I got completely confused about the content in section 3 and lost my courage to read the following sections. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper shows that an LSTM language model trained on a large corpus of Amazon product reviews can learn representations that are useful for sentiment analysis. \nGiven representations from the language model, a logistic regression classifier is trained with supervised data from the task of interest to produce the final model.\nThe authors evaluated their approach on six sentiment analysis datasets (MR, CR, SUBJ, MPQA, SST, and IMDB), and found that the proposed method is competitive with existing supervised methods. \nThe results are mixed, and they understandably are better for test datasets from similar domains to the Amazon product reviews dataset used to train the language model.\nAn interesting finding is that one of the neurons captures sentiment property and can be used to predict sentiment as a single unit.\n\nI think the main result of the paper is not surprising and does not show much beyond we can do pretraining on unlabeled datasets from a similar domain to the domain of interest. \nThis semi-supervised approach has been known to improve in the low data regime, and pretraining an expressive neural network model with a lot of unlabeled data has also been shown to help in the past.\nThere are a few unanswered questions in the paper:\n- What are the performance of the sentiment unit on other datasets (e.g., SST, MR, CR)? Is it also competitive with the full model?\n- How does this method compare to an approach that first pretrains a language model on the training set of each corpus without using the labels, and then trains a logistic regression while fixing the language model? Is the large amount of unlabeled data important to obtain good performance here? Or is similarity to the corpus of interest more important?\n- I assume that the reason to use byte LSTM is because it is cheaper than a word level LSTM. Is this correct or was there any performance issue with using the word directly?\n- More analysis on why the proposed method does well on the binary classification task of SST, but performs poorly on the fine-grained classification would be useful. If the model is capturing sentiment as is claimed by the authors, why does it only capture binary sentiment instead of a spectrum of sentiment level?\n\nThe paper is also poorly written. There are many typos (e.g., \"This advantage is also its difficulty\", \"Much previous work on language modeling has evaluated \", \"We focus in on the task\", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}