{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a modification to Adam which is intended to ensure that the direction of weight update lies in the span of the historical gradients and to ensure that the effective learning rate does not decrease as the magnitudes of the weights increase.  The reviewers wanted a clearer justification of the changes made to Adam and a more extensive evaluation, and held to this opinion after reading the authors' rebuttal and revisions.\n\nPros:\n+ The basic idea of treating the direction and magnitude separately in the optimization is interesting.\n\nCons:\n- Insufficient evaluation of the new method.\n- More justification and analysis needed for the modifications.  For example, are there circumstances under which they will fail?\n- The modification to Adam and batch-normalized softmax idea are orthogonal to one another, making for a less coherent story.\n- Proposed method does not have better generalization performance than SGD.\n- Concern that constraining weight vectors to the unit sphere can harm generalization.\n"
    },
    "Reviews": [
        {
            "title": "A variant of ADAM optimization algorithm that normalizes the weights of each hidden unit",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes a variant of ADAM optimization algorithm that normalizes the weights of each hidden unit. They further suggest using batch normalization on the output of the network before softmax to improve the generalization. The main ideas are new to me and the paper is well-written. The arguments and derivations are very clear. However, the experimental results suggest that the proposed method is not superior to SGD and ADAM.\n\nPros: \n\n- The idea of optimizing the direction while ignoring the magnitude is interesting and make sense.\n- Using batch normalization before softmax is interesting.\n\nCons:\n\n- In the abstract, authors claim that the proposed method has good optimization performance of ADAM and good generalization performance of SGD. Such a method could be helpful if one can get to the same level of generalization faster (less number of epochs). However, the experiments suggest that optimization advantages of the proposed method do not translate to faster generalization. Figures 2,3 and Table 1 indicate that the generalization performance of this method is very similar to SGD.\n\n- The paper is not coherent. In particular, direction-preserving ADAM and batch-normalized softmax trick are completely orthogonal ideas. \n\n- In the introduction and Section 2.2, authors claim that weight decay has a significant effect on the generalization performance of DNNs. I wonder if authors can refer to any work on this. My own experience and several empirical works have suggested that weight decay does not improve generalization significantly.\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Some related works should be analyzed. The experimental validation should to be revised. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Method:\n\nThe paper is missing analysis of some important related works such as\n\n\"Beyond convexity: Stochastic quasi-convex optimization\" by E. Hazan et al. (2015) \n\nwhere Stochastic Normalized Gradient Descent (SNGD) was proposed. \n\nThen, normalized gradient versions of AdaGrad and Adam were proposed in \n\n\"Normalized Gradient with Adaptive Stepsize Method for Deep\nNeural Network Training\" by A. W. Yu et al. (2017).\n\nAnother work which I find to be relevant is \n\n\"Follow the Signs for Robust Stochastic Optimization\" by L. Balles and P. Hennig (2017).\n\nFrom my personal experiments, restricting w_i to have L2 norm of 1, i.e., to be +-1 \nleads to worse generalization. One reason for this is that weight decay is not \nreally functioning since it cannot move w_i to 0 or make its amplitude any smaller. \nPlease correct me if I misunderstand something here. \n\nThe presence of +-1 weights moves us to the area of low-precision NNs, \nor more specifically, NNs with binary / binarized weights as in \n\n\"BinaryConnect: Training Deep Neural Networks with\nbinary weights during propagations\" by M. Courbariaux et al. (2015)\n\nand \n\n\"Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1\" by M. Courbariaux et al. (2016). \n\nRegarding\n\"Moreover, the magnitude of each update does not depend on themagnitude of the gradient. Thus, ND-Adam is more robust to improper initialization, and vanishing or exploding gradients.\"\n\nIf the magnitude of each update does not depend on the magnitude of the gradient, then the algorithm heavily depends on the learning rate. Otherwise, it does not have any means to approach the optimum in a reasonable number of steps *when* it is initialized very / unreasonably far from it. The claim of your second sentence is not supported by the paper. \n\nEvaluation:\n\nI am not confident that the presented experimental validation is fair. First, the original WRN paper and many other papers with ResNets used weight decay of 0.0005 and not 0.001 or 0.002 as used for SGD in this paper. It is unclear why this setting was changed. One could just use \\alpha_0 = 0.05 and \\lambda = 0.0005.\n\nThen, I don't see why the authors use WRN-22-7.5 which is different from WRN-28-10 which was suggested in the original study and used in several follow-up works. The difference between WRN-22-7.5 and WRN-28-10 is unlikely to be significant, \nthe former might have about only 2 times less parameters which should barely change the final validation errors. However, the use of WRN-22-7.5 makes it impossible to easily compare the presented results to the results of Zagoruyko who had 3.8\\% with WRN-28-10. I believe that the use of the setup of Zagoruyko for WRN-22-7.5 would allow to get much better results than 4.5\\% and 4.49\\% shown for SGD and likely better 4.14\\% shown for ND-Adam. I note that the use of WRN-22-7.5 is unlikely to be due to the used hardware because later in paper the authors refer to WRN-34-7.5.\n\nMy intuition is that the proposed ND-Adam moves the algorithm back to SGD but with potentially harmful constraints of w_i=+-1. Even the values of \\alpha^v_0 found for ND-Adam (e.g., \\alpha^v_0=0.05 in Figure 1B) are in line of what would be optimal values of \\alpha_0 for SGD. \n\nI find it uncomfortable that BN-Softmax is introduced here to support the use of an optimization algorithm, moreover, that the values of \\gamma_c are different for CIFAR-10 and CIFAR-100. I wonder if the proposed values are optimal (and therefore selected) for all three tested algorithms  or only for Adam-ND. I expect that hyperparameters of SGD and Adam would also need to be revised to account for BN-Softmax.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper extended the Adam optimization algorithm to preserve the update direction. Instead of using the un-centered variance of individual weights, the proposed method adapts the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector. The authors empirically demonstrated the method works well on CIFAR-10/100 tasks.\n\nComments:\n\n- I found the paper very hard to follow. The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest. The authors also combined the proposed method with a few existing deep learning tricks in the paper. All those tricks that, ie. section 3.3 and 4, should go into the background section.\n\n- Overall, the only contribution of the paper seems to be the ad-hoc modification to Adam in Eq. (9). Why is this a reasonable modification? Do we expect this modification to fail in any circumstances? The experiments on CIFAR dataset and one CNN architecture do not provide enough evidence to show the proposed method work well in general.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}