{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Dear authors,\n\nI agree with the reviewers that the paper tries to do several things at once and the results are not that convincing. Overall, this work is mostly incremental, which is fine if there is no issue in the execution. Thus, I regret to inform you that this paper will not be accepted to ICLR."
    },
    "Reviews": [
        {
            "title": "Confusing",
            "rating": "2: Strong rejection",
            "review": "It is very hard to follow this work, it feels like it tries to get several messages across while none of them properly. The work further contains number of unclear or incorrect claims, meaningless comparison with existing work, and unbelievable results (\"0.737% error rate\" on CIFAR-10).\n\nIn introduction, first, the paper seems to be about L1-regularization, with few motivating remarks valid only for convex problems, then about novel optimization method, and suddenly main contribution is reducing memory requirements. Further, part on \"Cumulative l1 regularization\" need to be better explained if, as it seems, plays important role in what you do. In discussion about SVRG, I don't understand how claims about convergence and batch size make sense, please provide reference, and how is it important for what you do later. When you say \"Hence, a promising approach is to use...\" I don't understand how it either follows from discussion above, nor what is the problem that you address.\nIn Main Contributions, 2.1 - \"we analyse non-convex SVRG\" - I don't see any kind of analysis in the paper.\n\nSec 3. you use IFO of Agarwal and Bottou which is known not to include this kind of algorithm - see large red box above abstract in the last version of the cited paper. Even then it is not clear what you try to say in the section, and whether any of it is new.\n\nSec 3.1. What is the notion of \"larger dataset\"? You regard CIFAR-10 as larger than MNIST.\n\nSec 4. After 4 pages of discussion on optimization algorithms, you write (very ambiguous) 4 lines about quantization, and compare against work not related to optimization at all. No explanation of what is presented in the table nor notation used. It requires lot of guessing to see what you try to do.\nIf I guessed correctly, you propose optimization method used together with particular objective function to train a model that is sparse in its final trained form, and then reduce numerical precision used to represent the model. And compare that to Han et al.\n1. If this is what you try to do, it is never clearly stated it up to this point, and much of the preceding text is irrelevant and it is sufficient to just refer to existing work... I now see you have a similar statement in Discussion, but if this is what you try to do and has to be explained at the beginning.\n2. It does not make any sense to compare against Han et al (precisely against the numbers presented in their paper), as you are compressing something else. If applied to your trained model, I believe it would achieve significantly better result.\n\nI did not properly look at the experiments, as it is not clear what you do/propose in first place, and you seems to report 0.737% error rate on CIFAR-10, and in the appendix, plots for CIFAR-10 show convergence to ~3% test error with LeNet-5.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Sparse Regularized Deep Neural Networks For Efficient Embedded Learning",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary: \nPaper proposes the compression method Delicate-SVRG-cumulative-L1 (combining minibatch SVRG with cumulative L1 regularization) which can significantly reduce the number of weights without affecting the test accuracy. Paper provides numerical experiments for MNIST and CIRAR10 on LeNet-300-100 and LeNet-5. \n\nComments: \nUp to my knowledge, Han et. al (2016) is not the leading result. There are (at least) two more results which are better than Han et. al. (2016) and also better than your results for LeNet-300-100 and LeNet-5 (MNIST), which were already published at ICML 2017 and NIPS 2016: \nhttp://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns.pdf\nhttp://proceedings.mlr.press/v70/molchanov17a/molchanov17a.pdf\n\nThere is no theory supporting the proposed method (which is the combination of some existing methods). Therefore, you should provide more experiments to show the efficiency. MNIST and CIFAR10 on LeNet-300-100 and LeNet-5 are quite standard that people have already shown. \n\nMoreover, there is no guarantee for sparsity by using L1 regularization on nonconvex problems.  \n\nMinor comments: \nPage 3, section 2, first paragraph: typo in the last sentence: “dose” -> “does” \nSame typo above for page 5, the sentence right before (2) Bias-based pruning\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors use l-1 regularized SVRG to promotes sparsity in the trained model. However, the paper lacks comparisons with some key literature, and experimentally the benefit of SVRG over SGD does not seem substantial.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors present an l-1 regularized SVRG based training algorithm that is able to force many weights of the network to be 0, hence leading to good compression of the model.  The motivation for l-1 regularization is clear as it promotes sparse models, which lead to lower storage overheads during inference. The use of SVRG is motivated by the fact that it can, in some cases, provide faster convergence than SGD.\n\nUnfortunately, the authors do not compare with some key literature. For example there has been several techniques that use sparsity, and group sparsity [1,2,3], that lead to the same conclusion as the paper here: models can be significantly sparsified while not affecting the test accuracy of the trained model.\n\nThen, the novelty of the technique presented is also unclear, as essentially the algorithm is simply SVRG with l1 regularization and then some quantization. The experimental evaluation does not strongly support the thesis that the presented algorithm is much better than SGD with l1 regularization. In the presented experiments, the gap between the performance of SGD and SVRG is small (especially in terms of test error), and overall the savings in terms of the number of weights is similar to Deep compression. Hence, it is unclear how the use of SVRG over SGD improves things. Eg in figure 2 the differences in top-1 error of SGD and SVRG, for the same number of weights is very similar (it’s unclear also why Fig 2a uses top-1 and Fig 2b uses top-5 error). I also want to note that all experiments were run on LeNet, and not on state of the art models (eg ResNets).\n\nFinally, the paper is riddled with typos. I attach below some of the ones I found in pages 1 and 2\n\nOverall, although the topic is very interesting, the contribution of this paper is limited, and it is unclear how it compares with other similar techniques that use group sparsity regularization, and whether SVRG offers any significant advantages over l1-SGD.\n\ntypos:\n“ This work addresses the problem by proposing methods Weight Reduction Quantisation”\n-> This work addresses the problem by proposing a Weight Reduction Quantisation method\n\n“Beside, applying with sparsity-inducing regularization”\n-> Beside, applying sparsity-inducing regularization\n\n“Our method that minibatch SVRG with l-1 regularization on non-convex problem”\n-> Our minibatch SVRG with l-1 regularization method on non-convex problem\n\n“As well as providing,l1 regularization is a powerful compression techniques to penalize some weights to be zero”\n-> “l1 regularization is a powerful compression technique that forces some weights to be zero”\n\n The problem 1 can\n->  The problem in Eq.(1) can\n\n“it inefficiently encourages weight”\n-> “it inefficiently encourages weights”\n\n————\n\n[1] Learning Structured Sparsity in Deep Neural Networks\nhttp://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf\n\n[2] Fast ConvNets Using Group-wise Brain Damage\nhttps://arxiv.org/pdf/1506.02515.pdf\n\n[3] Sparse Convolutional Neural Networks\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf\n\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}