{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper addresses the problem of learning neural graph representations, based on graph filtering techniques in the vertex domain.\n\nReviewers agreed on the fact that this paper has limited interest in its current form, and has serious grammatical issues. The AC thus recommends rejection at this time. "
    },
    "Reviews": [
        {
            "title": "rejection",
            "rating": "3: Clear rejection",
            "review": "There are many language issues rendering the text hard to understand, e.g.,\n-- in the abstract: \"several convolution on graphs architectures\"\n-- in the definitions: \"Let data with N observation\" (no verb, no plural, etc).\n-- in the computational section: \"Training size is 9924 and testing is 6695. \"\nso part of my negative impression may be pure mis-understanding of what\nthe authors had to say. \n\nStill, the authors clearly utilise basic concepts (c.f. \"utilize eigenvector \nbasis of the graph Laplacian to do filtering in the Fourier domain\") in ways\nthat do not seem to have any sensible interpretation whatsoever, even allowing\nfor the mis-understanding due to grammar. There are no clear insight, \nno theorems, and an empirical evaluation on an ill-defined problem in \ntime-series forecasting. (How does it relate to graphs? What is the graph \nin the time series or among the multiple time series? How do the authors\nimplement the other graph-related approaches in this problem featuring\ntime series?) My impression is hence that the only possible outcome is\n\nrejection.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Added Late Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph.  Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems.\n\nPros: \n-Authors provide some empirical evidence for the benefits of using their technique.\n-Authors are fairly upfront about how, overall, it seems their technique isn't doing *too* much--null results are still results, and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much.\n\nCons: \n-The grammar in the paper is pretty bad.  It could use a couple more passes with an editor.\n-For a, more or less, entirely empirical paper, the choices of experiments are...somewhat befuddling.  Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good.\n-While I mentioned it as a pro, it also seems to be that this technique simply doesn't buy you very much as a practitioner.  If this is true--that learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments.\n\nUltimately, I will have to suggest rejection, unless the authors considerably beef up their manuscript with more experiments, more details, and improve the grammar considerably.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Authors of this paper built a neural network architecture compatible with the novel approach for learning graph representation of the data, motivated by graph filtering in the vertex domain. The learned graph is demonstrated to be richer structures than nearest neighbor graphs.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. A projected sub-gradient descent algorithm is used. The UPS optimizer by itself is not new.\n\nGraph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter. First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. There are some concerns about this network that need to be clarified:\n1. sigma is never clarified in the main context or experiments\n2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random.\n3. another explanation about the weights as the rescaling to matrix A needs to further clarified. As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction. What is the disagreement between L1 penalty and prediction quality? Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?\n4. Authors stated that the last step is to build a mapping from the GPS features into the response Y. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper. In the experiments, authors only stated that “we fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph”, and then the graph is used to train existing models as the input of the graph. Which architecture is used for building the mapping ?\n\nIn the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified. For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on. More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm. \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}