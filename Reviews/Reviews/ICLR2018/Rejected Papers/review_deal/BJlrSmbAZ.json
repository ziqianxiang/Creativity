{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper shows that batch normalization can be cast as approximate inference in deep neural networks.  This is an appealing result as batch normalization is used in practice in a wide variety of models.   The reviewers found the paper well written and easy to understand and were motivated by underlying idea.  However, they found the empirical analysis lacking and found that there was not enough detail in the main text to verify whether the claims were true.\n\nThe authors empirically compared to a recent method showing that dropout can be cast as approximate inference with the claim that by transitivity they were comparing to a variety of recent methods.  AnonReviewer1 casts significant doubt on the results of that work.  This is very unfortunate and not the fault of the authors of this paper.  The authors have since gone to great length to compare to Louizos and Welling, 2017.  Unfortunately, that comparison doesn't appear to be complete in the manuscript.\n\nThe main text was also lacking specific detail relating to fundamental parts of the proposed method (noted by all reviewers).\n\nOverall, this paper seems to be tremendously promising and the underlying idea potentially very impactful.  However, given the reviews, it doesn't seem that this paper would achieve its potential impact.  The response from the authors is appreciated and goes a long way to improving the paper.  Taking the reviews into account, adding specific detail about the methodology and model (e.g. the prior) and completing careful empirical analysis will make this a strong paper that should be much more impactful."
    },
    "Reviews": [
        {
            "title": "Using Batch norm at test time to obtain uncertainty estimate",
            "rating": "5: Marginally below acceptance threshold",
            "review": "*Summary*\n\nThe paper proposes using batch normalisation at test time to get the predictive uncertainty. The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer. This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time. Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout.\n\n*Originality and significance*\n\nAs far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1. However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction. \n\nIt is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w).\n\nFrom a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small \\lambda] -- what is the rationale of this? Can this be explained by comparing the variational free-energy. \n\nThe experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017). The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method.\n\n*Clarity*\nThe paper is in general well written and easy to understand. \n\n*Additional comments*\n\nPage 2: Monte Carlo Droput --> Dropout\nPage 3 related work: (Adams, 2015) should be (Hernandez-Lobato and Adams, 2015)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and relevant but lack of details on prior",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference. The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference). The experiments\nperformed show that the Bayesian view of batch normalization performs similarly\nas MC dropout in terms of the estimates of uncertainty that it produces.\n\nQuality:\n\nI found the quality to be low in some aspects. First, the description of what\nis the prior used by batch normalization in section 3.3 is unsatisfactory. The\nauthors basically refer to Appendix 6.4 for the case in which the weight decay\npenalty is not zero. The details in that Appendix are almost none, they just\nsay \"it is thus possible to derive the prior...\".\n\nThe results in Table 2 are a bit confusing. The authors should highlight in\nbold face the results of the best performing method.\n\nThe authors indicate that they do not need to compare to variational methods\nbecause Gal and Ghahramani 2015 compare already to those methods. However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data. In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54\n\nTherefore, the claim that \n\n\"Since we have established that MCBN performs on par with MCDO, by proxy we\nmight conclude that MCBN outperforms those VI methods as well.\"\n\nis not valid.\n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015). However, Gal and Ghahramani\n(2015) actually follow Hernández-Lobato and Adams, 2015 so the correct\nreference should be the latter one.\n\nClarity:\n\nThe paper is clearly written and easy to follow and understand.\n\nI found confusing how to use the proposed method to obtain estimates of\nuncertainty for a particular test data point x_star. The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch.\nHow is this actually done in practice?\n\nOriginality:\n\nThe proposed contribution is original. This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal.\n\nSignificance:\n\nThe paper's contributions are significant. Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant. Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A dense paper with a few key open questions",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization.\n\nThere is a lot going on in this paper. Although the overall presentation is clean, there are few key shortfalls (see below). Overall, the reported functionality is nice, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive).\n\nSome open questions that I find crucial:\n\n* How exactly is the “stochastic forward-pass” performed that gives rise to the moment estimates? This step is the real meat of the paper, yet I struggle to find a concrete definition in the text. Is this really just an average over a few recent weights during optimization? If so, how is this method specific to batch normalization? Maybe I’m showing my own lack of understanding here, but it’s worrying that the actual sampling technique is not explained anywhere. This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here? In Bayesian Dropout, there is an explicit variational objective. Here, this is replaced by an implicit regularizer. The argument in Section 3.3 seems rather weak to me. To paraphrase it: If the prior vanishes, so does the regularizer. Fine. But what's the regularizer that's vanishing? The sentence that \"the influence of the prior diminishes as the size of the training data increases\" is debatable for something as over-parametrized as a DNN. I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.\n\n* I’m confused about the statements made about the “constant uncertainty” baseline. First off, how is this (constant) width of the predictive region chosen? Did I miss this, or is it not explained anywhere? Unless I misunderstand the definition of CRPS and PLL, that width should matter, no? Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is “quite a reasonable baseline”. That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?). \nOn a related point: What would Figure 2 look like for the constand uncertainty setting? Just a horizontal line in blue and red? But at which level?\n\nI like this paper. It is presented well (modulo the above problems), and it makes some strong points. But I’m worried about the empirical evaluation, and the omission of crucial algorithmic details. They may hide serious problems.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}