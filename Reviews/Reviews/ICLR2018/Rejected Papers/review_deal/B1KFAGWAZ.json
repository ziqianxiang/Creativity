{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The authors present a centralized neural controller for multi-agent reinforcement learning.   The reviewers are are not convinced that there is sufficient novelty, considering the authors setup as essentially a special case of other recent works, with added adjustments to the neural-networks that are standard in the literature.\n\nI personally am more bullish about this paper than the reviewers, as I think engineering an architecture to perform well in interesting scenarios is worth reporting.  However, the reviewers are mostly in agreement, and their reviews were neither sloppy nor factually incorrect.  So I will recommend rejection, following their judgement.\n\nNevertheless, I encourage the authors to continue strengthening the results and the presentation and resubmit.  "
    },
    "Reviews": [
        {
            "title": "Somewhat encouraging results on StarCraft, method is incremental, paper needs work",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management. The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL. \nA gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent.  This contribution is added to the logits coming from each agent. \n\nPros: \n-The results on StarCraft are encouraging and present state of the art performance if reproducible.\n\nCons:\n-The experimental evaluation is not very thorough:\nNo uncertainty of the mean is stated for any of the results. 100 evaluation runs is very low. It is furthermore not clear whether training was carried out on multiple seeds or whether these are individual runs. \n\n-BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution. Thus they represent weak baselines for a fully centralized method such as MS-MARL. \nThe only fully centralized baseline in the paper is GMEZO, however results stated are much lower than what is reported in the original paper (eg. 63% vs 79% for M15v16). The paper is missing further centralized baselines. \n\n-It is unclear to what extends the novelty of the paper (specific architecture choices) are required. For example, the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings (if at all).\n\nDetailed comments:\n\"For all tasks, the number of batch per training epoch is set to 100.\"\nWhat does this mean?\n\nFigure 1: \nThis figure is very helpful, however the colour for M->S is wrong in the legend. \n\nTable 2:\nGMEZO win rates are low compared to the original publication. \nWhat many independent seeds where used for training? What are the confidence intervals? How many runs for evaluation? \n\n\nFigure 4:\nB) What does it mean to feed two vectors into a Tanh? This figure currently very unclear. What was the rational for choosing a vanilla RNN for the slave modules?\n\nFigure 5:\na) What was the rational for stopping training of CommNet after 100 epochs? The plot looks like CommNet is still improving. \nc) This plot is disconcerting. Training in this plot is very unstable. The final performance of the method ('ours') does not match what is stated in 'Table 2'. I wonder if this is due to the very small batch size used (\"a small batch size of 4 \").\n\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "results seem nice, but novelty less clear",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper investigates multiagent reinforcement learning  making used of a \"master slave\" architecture (MSA). On the positive side, the paper is mostly well-written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks. On the negative side, there seems to be relatively limited novelty: we can think of MSA as one particular communication (i.e, star) configuration one could use is a multiagent system. One aspect does does strike me as novel is the \"gated composition module\", which allows differentiation of messages to other agents based on the receivers internal state. (So, the *interpretation* of the message is learned). I like this idea, however, the results are mixed, and the explanation given is plausible, but far from a clearly demonstrated answer.\n\nThere are some important issues that need clarification:\n\n* \"Sukhbaatar et al. (2016) proposed the “CommNet”, where broadcasting communication channel among all agents were set up to share a global information which is the summation of all individual agents. [...] however the summed global signal is hand crafted information and does not facilitate an independently reasoning master agent.\"\n-Please explain what is meant here by 'hand crafted information', my understanding is that the f^i in figure 1 of that paper are learned modules?\n-Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your 'master'.\n\n\n*This relates also to this: \n\n\"Later we empirically verify that, even when the overall in-\nformation revealed does not increase per se, an independent master agent tend to absorb the same\ninformation within a big picture and effectively helps to make decision in a global manner. Therefore\ncompared with pure in-between-agent communications, MS-MARL is more efficient in reasoning\nand planning once trained. [...] \nSpecifically, we compare the performance among the CommNet model, our\nMS-MARL model without explicit master state (e.g. the occupancy map of controlled agents in this\ncase), and our full model with an explicit occupancy map as a state to the master agent. As shown in\nFigure 7 (a)(b), by only allowed an independently thinking master agent and communication among\nagents, our model already outperforms the plain CommNet model which only supports broadcast-\ning communication of the sum of the signals.\"\n\n-Minor: I think that the statement \"which only supports broadcast-ing communication of the sum of the signals\" is not quite fair: surely they have used a 1-channel communication structure, but it would be easy to generalize that.\n\n-Major: When I look at figure 4D, I see that the proposed approach *also* only provides the master with the sum (or really mean) with of the individual messages...? So it is not quite clear to me what explains the difference.\n\n\n*In 4.4, it is not quite clear exactly how the figure of master and slave actions is created. This seems to suggest that the only thing that the master can communicate is action information? It this the case?\n\n* In table 2, it is not clear how significant these differences are. What are the standard errors?\n\n* The section 3.2 explains standard things (policy gradient), but the details are a bit unclear. In particular, I do not see how the Gaussian/softmax layers are integrated; they do not seem to appear in figure 4?\n\n* I cannot understand figure 7 without more explanation. (The background is all black - did something go wrong with the pdf?)\n\n\n\n\nDetails:\n* references are wrongly formatted throughout. \n\n* \"In this regard, we are among the first to combine both the centralized perspective and the decentralized perspective\"\nThis is a weak statement (E.g., I suppose that in the greater scheme of things all of us will be amongst the first people that have walked this earth...)\n\n\n* \"Therefore they tend to work more like a commentator analyzing and criticizing the play, rather than\na coach coaching the game.\"\n-This sounds somewhat vague. Can it be made crisper?\n\n* \"Note here that, although we explicitly input an occupancy map to the master agent, the actual infor-\nmation of the whole system remains the same.\"\nThis is a somewhat peculiar statement. Clearly, the distribution of information over the agents is crucial. For more insights on this one could refer to the literature on decentralized POMDPs.\n\n\n\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients.\nAuthors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines.\n\nOverall, I had a very confusing feeling when reading the paper. First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP? How do different agents perceive their time, is it synchronized or not? Do they (partially) share the incentive or may have completely arbitrary rewards?\nWhat is exactly the communication protocol?\n\nI find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks.\n\nSecond, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me.\nCurrently, it looks \n\nBased on that, I can’t recommend acceptance of the paper.\n\nTo make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients).\nIt would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}