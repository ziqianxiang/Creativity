{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper studies the problem of synthesizing adversarial examples that will succeed at fooling a classification system under unknown viewpoint, lighting, etc conditions. For that purpose, the authors propose a data-augmentation technique (called \"EOT\") that makes adversarial examples robust against a predetermined family of transformations.\n\nReviewers were mixed in their assessment of this work, on the one hand highlighting the potential practical applications, but on the other hand warning about weak comparisons with existing literature, as well as lack of discussion about how to improve the robustness of the deep neural net against that form of attacks.\nThe AC thus believes this paper will greatly benefit from a further round of iteration/review, and therefore recommends rejection at this time. "
    },
    "Reviews": [
        {
            "title": "Review - Accept pending clarifications",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors present a method to enable robust generation of adversarial visual\ninputs for image classification.\n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation.\n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution.\n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains.\n\nThe paper is clear to follow and the objective employed appears to be sound. I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples.\n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each\n   experiment? Is it a PCFG? Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?) And on a related note, how were the number of sampled\n   transformations chosen?\n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input.\n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective. I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required.\n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\n   what effect the monte-carlo approximation of the objective has on things.\n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment.\n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from. There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one.\n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models? Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason? Similar questions for the 5\n   models in the 3D rendering experiment.\n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \"over a variety of viewpoints\". Were these\n   viewpoints chosen randomly?\n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves. For now, I think the\nsubmission is good for a weak accept â€“- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Persuasive real-world results, would benefit from a comparison to universal examples",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations. The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results.\n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive. This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting.\n\nHowever, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al. This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle).\n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al). If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary. This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world.\n\nMinor comments:\nSection 1.1: \"a affine\" -> \"an affine\"\nTypo in section 3.4: \"of a of a\"\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse.\n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but needs a major revision",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network.\nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.\n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting. However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date. \nSee for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.\n- The presentation of the results is not very clear. See specific comments below.\n- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.\n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations. Bold everywhere or nowhere\nResults: The tables are difficult to read and should be clarified:\nWhat does the l2 metric stands for ? \nHow about min, max ?\nAccuracy -> classification accuracy\nModels -> 3D models\nDescribe each metric (Adversarial, Miss-classified, Correct)\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}