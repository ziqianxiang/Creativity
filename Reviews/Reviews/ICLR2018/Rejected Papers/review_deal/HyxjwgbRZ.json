{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Dear authors,\n\nAfter carefully reading the reviews, the rebuttal, and going through the paper, I regret to inform you that this paper does not meet the requirements for publication at ICLR.\n\nWhile the variance analysis is definitely of interest, the reality of the algorithm does not match the claims. The theoretical rate is worse than that of SG but this could be an artefact of the analysis. Sadly, the experimental setup lacks in several ways:\n- It is not yet clear whether escaping the saddle points is really an issue in deep learning as the loss function is still poorly understood.\n- This analysis is done in the noiseless setting despite your argument being based around the variance of the gradients.\n- You report the test error on CIFAR-10. While interesting and required for an ML paper, you introduce an optimization algorithm and so the quantity that matters the most is the speed at which you achieve a given training accuracy. Also, your table lists the value of the test accuracy rather than the speed of increase. Thus, you test the generalization ability of your algorithm while making claims about the optimization performance."
    },
    "Reviews": [
        {
            "title": "Preliminary work that requires further investigation",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Dear Authors,\nAfter reading the revised version I still believe that the assumption about the gradients + their variances to be distributed equivalently among all direction is very non-realistic, also for the case of deep learning applications.\n\nI think that the direction you are taking is very interesting, yet the theoretical work is still too preliminary and I believe that further investigation should be made in order to make a more complete manuscript.\n\nThe additional experiments are nice.  I therefore raised my score by a bit.\n\n\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n The paper explores SignGD --- an algorithm that uses the sign of the gradients instead of actual gradients for training deep models. The authors provide some guarantees regarding the convergence of SignGD to local minima in the stochastic optimization setting, and later compare SignSG to GD in two deep learning tasks.\n\nExploring signSGD is an important and interesting line of research, and this paper provides some preliminary result in this direction.\nHowever, in my view, this work is too preliminary and not ready for publish. This is since the authors do not illustrate any clear benefits of signSGD over SGD neither in theory nor in practice. I elaborate on this below:\n\n-The theory part shows that under some conditions, signGD  finds a local minima. Yet, as the authors themselves \nmention, the dependence on the dimension is much worse compared to SGD.\nMoreover, the authors do not mention that if the noise variance does not scale with the dimension (as is often the case), then the convergence of SGD will not depend on the dimension, while it seems that the convergence of signGD will still depend on the dimension.\n\n-The experiments are nice as a preliminary investigation, but not enough in order to illustrate the benefits of signSGD over SGD. In order to do so, the authors should make a more extensive experimental study.\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "[UPDATED] Would rate more confidently, if stronger numerical experiments are present :) and Assumption 3 is more explained and defended",
            "rating": "4: Ok but not good enough - rejection",
            "review": "UPDATED REVIEW:\n\nI have checked all the reviews, also checked the most recent version.\nI like the new experiments, but I am not impressed much with them to increase my score. The assumption about the variance is fixing my concern, but as you have pointed out, it is a bit more tricky :) I would really suggest you work on the paper a bit more and re-submit it.\n\n--------------------------------------------------------------------\nIn this paper, authors provided a convergence analysis of Sign SGD algorithm for non-covex case.\nThe crucial assumption for the proof was Assumption 3, otherwise, the proof technique is following a standard path in non-convex optimization.   \n\nIn general, the paper is written nicely, easy to follow.\n\n==============================================\n\"The major issue\":\nWhy Assumption 3 can be problematic in practice is given below:\nLet us assume just a convex case and assume we have just 2 kids of function in 2D:  f_1(x) = 0.5 x_1^2 and f_2(x) = 0.5 x_2^2.\nThen define the function f(x) = E [ f_i(x)  ].   where $i =1$  with prob 0.5 and $i=2$ with probability 0.5. \nWe have that   g(x) = 0.5 [ x_1, x_2 ]^T.\nLet us choose $i=1$ and choose $x = [a,a]^T$, where $a$ is some parameter.\n\nThen (4) says, that there has to exist a $\\sigma$ such that\nP [   | \\bar g_i(x) - g_i(x) | > t ] \\leq 2 exp( - t^2 / 2\\sigma^2).  forall \"x\".\n\nplugging our function inside it should be true that\n\nP [   | [ B ] - 0.5 a | > t ] \\leq 2 exp( - t^2 / 2\\sigma^2).  forall \"x\".\nwhere B is a random variable which has value \"a\" with probability 0.5 and value \"0\" with probability 0.5.\n\nIf we choose $t = 0.1a$ then we have that it has to be true that\n\n1 = P [   | [ B ] - 0.5 a | > 0.1a ] \\leq 2 exp( - 0.01 a^2 / 2\\sigma^2)   ---->  0 as $a \\to \\infty$.\n\nHence, even in this simple example, one can show that this assumption is violated unless $\\sigma = \\infty$.\n\nOne way to ho improve this is to put more assumption + maybe put some projection into a compact set?\n==============================================\n\nHence, I think the theory should be improved.\n\nIn terms of experiments, I like the discussion about escaping saddle points, it is indeed a good discussion. However, it would be nicer to have more numerical experiments.\nOne thing I am also struggling is the \"advantage\" of using signSGD: one saves on communication (instead of sending 4*8 bits per dimension, one just send only 1 bit, however, one needs \"d\"times more iterations, hence, the theory shows that it is much worse then SGD (see (11) ).\n\n\n\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not correct",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents convergence rate of a quantized SGD, with biased quantization - simply taking a sign of each element of gradient.\n\nThe stated Theorem 1 is incorrect. Even if the stated result was correct, it presents much worse rate for a weaker notion of convergence.\n\nMajor flaws:\n1. As far as I can see, Theorem 1 should depend on 4th root of N_K, the last (omitted) step from the proof is done incorrectly. This makes it much worse than presented.\n2. Even if this was correct, the main point is that this is \"only\" d times worse - see eq (11). That is enormous difference, particularly in settings where such gradient compression can be relevant. Also, it is lot more worse than just d times:\n3. Again in eq (11), you compare different notions of convergence - E[||g||_1]^2 vs. E[||g||_2^2]. In particular, the one for signSGD is the weaker notion - squared L1 norm can be d times bigger again. If this is not the case for some reason, more detailed explanation is needed.\n\nOther than that, the paper contains several attempts at intuitive explanation, which I don't find correct. Inclusion of Assumption 3 would in particular require better justification.\n\nExperiments are also inconclusive, as the plots show convergence to significantly worse accuracy than what the models converged to in original contributions.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}