{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Dear authors,\n\nWhile the reviewers appreciated your analysis, they all expressed concerns about the significance of the paper. Indeed, given the plethora of GAN variants, it would have been good to get stronger evidence about the advantages of the Dudley GAN. Even though I agree it is difficult to provide a clean comparison between generative models because of the lack of clear objectives, the LL on one dataset and images generated is limited. For instance, it would have been nice to show robustness results as this is a clear issue with GANs."
    },
    "Reviews": [
        {
            "title": "Marginally below acceptance threshold",
            "rating": "5: Marginally below acceptance threshold",
            "review": "It is clear that the problem studied in this paper is interesting. However, after reading through the manuscript, it is not clear to me what are the real contributions made in this paper. I also failed to find any rigorous results on generalization bounds. In this case, I cannot recommend the acceptance of this paper. ",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "There are certain contributions in literatures, but the novelty may be not significant.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Ensuring Lipschitz condition in neural nets is essential of stablizing GANs. This paper proposes two contraint-based optimzation to ensure the Lips condtions , and these proposed approaches maintain suffcient capacity, as well as expressiveness of the network.  A simple theoritical result is given by emprical risk minimization. The content of this paper\nis written clearly, and there are certain contribution and orginality in the literature. However, I am not sure that the novelty is\nsignificant, since I think that the idea of proposing their optimization is trival. Here I am concerned with the following two questions:\n(1) How to parameterize the function space of f_w or h_w, since they are both multivariate and capacity of the network will be\nreduced if the used way of parametering functions is adopted inappropriatily.\n(2) The theoretical result in (4)  doesnot contain the information of Rademacher complexity, and it may be suboptimal in some sense. Besides, the parameter $\\gamma$ appears in the discriminator, which contradicts its role on the contraint of functions space.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-written, but unlikely a big step forward for GANs",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors propose a different type of GAN--the Dudley GAN--that is related to the Dudley metric. In fact, it is very much like the WGAN, but rather than just imposing the function class to have a bounded gradient, they also impose it to be bounded itself. This is argued to be more stable than the WGAN, as gradient clipping is said not necessary for the Dudley GAN. The authors empirically show that the Dudley GAN achieves a greater LL than WGAN for the MNIST and CIFAR-10 datasets.\n\nThe main idea [and its variants] looks solid, but with the plethora of GANs in the literature now, after reading I'm still left wondering why this GAN is significantly better than others [BEGAN, WGAN, etc.]. It is clear that imposing the quadratic penalty in equation (3) is really the same constraint as the Dudley norm? The big contribution of the paper seems to be that adding some L_inf regularization to the function class helps preclude gradient clipping, but after reading I'm unsure why this is \"the right thing\" to do in this case. We know that convergence in the Wasserstein metric is stronger than the Dudley metric, so why is using the weaker metric overweighed by the benefits in training?\n\nNits: Since the function class is parameterized by a NN, the IPM is not actually the Dudley metric between the two distributions. One would have to show that the NN is dense in Dudley unit ball w.r.t. L_inf norm, but this sort of misnaming had started with the \"Wasserstein\" GAN.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}