{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper proposes to combine Depthwise separable convolutions developed for 2d grids with recent graph convolutional architectures. The resulting architecture can be seen as learning both node and edge features, the latter encoding node similarities with learnt weights.\nReviewers agreed that this is an interesting line of work, but that further work is needed in both the presentation and the experimental front before publication. In particular, the paper should also compare against recent models (such as the MPNN from Gilmer et al) that also propose edge feature learning. THerefore, the AC recommends rejection at this time.\n"
    },
    "Reviews": [
        {
            "title": "Depthwise convolution for GCN, seems to improve performance but requires more work",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents a Depthwise Separable Graph Convolution network that aims\nat generalizing Depthwise convolutions, that exhibit a nice performance in image\nrelated tasks, to the graph domain. In particular it targets\nGraph Convolutional Networks.\n\nIn the abstract the authors mention that the Depthwise Separable Graph Convolution\nthat they propose is the key to understand the connections between geometric\nconvolution methods and traditional 2D ones. I am afraid I have to disagree as\nthe proposed approach is not giving any better understanding of what needs to be\ndone and why. It is an efficient way to mimic what has worked so far for the planar\ndomain but I would not consider it as fundamental in \"closing the gap\".\n\nI feel that the text is often redundant and that it could be simplified a lot.\nFor example the authors state in various parts that DSC does not work on\nnon-Euclidean data. Section 2 should be clearer and used to better explain\nrelated approaches to motivate the proposed one.\nIn fact, the entire motivation, at least for me, never went beyond the simple fact\nthat this happens to be a good way to improve performance. The intuition given\nis not sufficient to substantiate some of the claims on generality and understanding\nof graph based DL.\n\nIn 3.1, at point (2), the authors mention that DSC filters are learned from the\ndata whereas GC uses a constant matrix. This is not correct, as also reported in\nequation 2. The matrix U is learned from the data as well.\n\nEquation (4) shows that the proposed approach would weight Q different GC\nlayers. In practical terms this is a linear combination of these graph\nconvolutional layers.\nWhat is not clear is the \\Delta_{ij} definition. It is first introduced in 2.3\nand described as the relative position of pixel i and pixel j on the image, but\nthen used in the context of a graph in (4). What is the coordinate system used\nby the authors in this case? This is a very important point that should be made\nclearer.\n\nWhy is the Related Work section at the end? I would put it at the front.\n\nThe experiments compare with the recent relevant literature. I think that having\nless number of parameters is a good thing in this setting as the data is scarce,\nhowever I would like to see a more in-depth comparison with respect to the number\nof features produced by the model itself. For example GCN has a representation\nspace (latent) much smaller than DSCG.\nNo statistics over multiple runs are reported, and given the high variance of\nresults on these datasets I would like them to be reported.\n\nI think the separability of the filters in this case brings the right level of\nsimplification to the learning task, however as it also holds for the planar case\nit is not clear whether this is necessarily the best way forward.\nWhat are the underlying mathematical insights that lead towards selecting\nseparable convolutions?\n\nOverall I found the paper interesting but not ground-breaking. A nice application\nof the separable principle to GCN. Results are also interesting but should be\nfurther verified by multiple runs.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Incremental yet interesting advance in geometric CNNs. But, some core technical aspects and experiments are missing.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Paper Summary:\nThis work proposes a new geometric CNN model to process spatially sparse data. Like several existing geometric CNNs, convolutions are performed on each point using nearest neighbors. Instead of using a fixed or Gaussian parametric filters, this work proposes to predict filter weights using a multi-layer perception. Experiments on 3 different tasks showcase the potential of the proposed method.\n\nPaper Strengths:\n- An incremental yet interesting advance in geometric CNNs.\n- Experiments on three different tasks indicating the potential of the proposed technique.\n\nMajor Weaknesses:\n- Some important technical details about the proposed technique and networks is missing in the paper. It is not clear whether a different MLP is used for different channels and for different layers, to predict the filter weights. Also, it is not clear how the graph nodes and connectivity changes after the max-pooling operation.\n- Since filter weight prediction forms the central contribution of this work, I would expect some ablation studies on the MLP (network architecture, placement, weight sharing etc.) that predicts filter weights. But, this is clearly missing in the paper.\n- If one needs to run an MLP for each edge in a graph, for each channel and for each layer, the computation complexity seems quite high for the proposed network. Also, finding nearest neighbors takes time on large graphs. How does the proposed technique compare to existing methods in terms of runtime?\n\nMinor Weaknesses:\n- Since this paper is closely related to Monti et al., it would be good if authors used one or two same benchmarks as in Monti et al. for the comparisons. Why authors choose different set of benchmarks? Because of different benchmarks, it is not clear whether the performance improvements are due to technical improvements or sub-optimal parameters/training for the baseline methods.\n- I am not an expert in this area. But, the chosen benchmarks and datasets seem to be not very standard for evaluating geometric CNNs.\n- The technical novelty seems incremental (but interesting) with respect to existing methods.\n\nClarifications:\n- See the above mentioned clarification issues in 'major weaknesses'. Those clarification issues are important to address.\n- 'Non-parametric filter' may not be right word as this work also uses a parametric neural network to estimate filter weights?\n\nSuggestions:\n- It would be great if authors can add more details of the multi-layer perceptron, used for predicting weights, in the paper. It seems some of the details are in Appendix-A. It would be better if authors move the important details of the technique and also some important experimental details to the main paper.\n\nReview Summary:\nThe proposed technique is interesting and the experiments indicate its superior performance over existing techniques. Some incomplete technical details and non-standard benchmarks makes this not completely ready for publication.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Extension of Depth-Wise-Convolution (Chollet et al. 2016) with improved performance.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs. The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel. The weights within a depth channel is shared thus maintaining the stationary requirement. The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception. \n\nThe paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016). \n\nThe paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification. The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters. For image classification, the performance of proposed method is below its predecessor Xception network. \n\nPros:\ni) Detailed review of the existing work and comparison with the proposed work.\nii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings.\niii) Superior performance with fewer number of parameters compared to other methods. \nCons:\ni) The architecture of the 2 layer MLP used to learn weights for a particular depth channel is not provided.\nii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}