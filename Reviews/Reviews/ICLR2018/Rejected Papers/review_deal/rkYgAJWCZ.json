{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper is looking at an interesting problem, but it seems too early. The approach requires training a new language model  from scratch for each new word, rendering it completely impractical for real use. The main evaluation therefore only considers four words - \"bonuses\", \"explained\", \"marketers\", \"strategist\" (expanded to 20 during the rebuttal). This is not sufficient for ICLR."
    },
    "Reviews": [
        {
            "title": "Review",
            "rating": "3: Clear rejection",
            "review": "The paper proposes a technique for exploiting prior knowledge to learn embedding representations for new words with minimal data. The authors provide a good motivation for the task and it is also a nice step in the general direction of learning deep nets and other systems with minimal supervision. \n\nThe problem is useful and very relevant to natural language applications, especially considering the widespread use of word embeddings within NLP systems. However, the demonstrated experimental results do not match the claims which seems a little grand. Overall, the empirical results is unsatisfactory. The authors pick a few example words and provide a detailed analysis. This is useful to understand how the test perplexity varies with #training examples for these individual settings. However, it is hardly enough to draw conclusion about the general applicability of the technique or effectiveness of the results. Why were these specific words chosen? If the reason is due to some statistical property (e.g., frequency) observed in the corpus, then why not generalize this idea and demonstrate empirical results for a class of words exhibiting the property. Such an analysis would be useful to understand the effectiveness of the overall approach. Another idea would be to use the one/few-shot learning to learn embeddings and evaluate their quality on a semantic task (as suggested in Section 3.3), but on a larger scale.\n\nThe technical contributions are also not novel. Coupled with the narrow experimentation protocol, it does not make the paper’s contributions or proposed claims convincing.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "5: Marginally below the acceptance threshold.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Paper Summary\n\nFrom just seeing a word used in a sentence, humans can infer a lot about this word by leveraging the surrounding words. Based on this idea, this work tries to obtain a better understanding of words in the one-shot or few-shot setting by leveraging surrounding word. They do this by language modeling sentences which contain rarely seen or never seen words. They evaluated their model using percent change in perplexity on test sentences containing new word by varying the number of training sentences containing this word. 3 Proposed Methods to model few-shot words: (1) beginning with random embedding, (2) beginning with zero embedding (3) beginning with the centroid of other words in the sentence. They compare to 2 Baseline Methods: (1) centroid of other words in the sentence, and (2) full training including the sparse words. Their results show that learning from centroids of other words can outperform full training on the new words. \n\nExplanation\nThe paper is well written, and the experiments are well explained.  It is an interesting paper, and a research topic which is not well studied. The experiments are reasonable. The method seems to work well. \n\nHowever, the method provides a very marginal difference between the previous method in Lazaridou et al. (2017). They just use backdrop to learn from this starting position. The main contribution of this work is the evaluation section. \n\nWhy only use the PTB language modeling task. Why not use the task in Gauthier & Mordatch or Hermann et al. The one task of language modeling shows promising results, but it’s not totally convincing. \n\nOne of the biggest caveats is that the experiments are only done in a few words. I’m not sure why more couldn’t have been done. This is discussed in section 4.1, but I think some of these differences could have been alleviated if there were more experiments done. Regardless, the experiments on the 8 words that they did chose were well done. \n\nI don’t think that section 3.3 (embedding similarity) is particularly useful. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Worthwile goal of using backpropogation and weight-clamping to perform few-shot learning; questionable evaluation",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I am highly sympathetic to the goals of this paper, and the authors do a good job of contrasting human learning with current deep learning systems, arguing that the lack of a mechanism for few-shot learning in such systems is a barrier to applying them in realistic scenarios. However, the main evaluation only considers four words - \"bonuses\", \"explained\", \"marketers\", \"strategist\" - with no explanation of how these words were chosen. Can I really draw any meaningful conclusions from such an experimental setup? Even the authors acknowledge, in footnote 1, that, for one of the tests, getting lower perplexity in three out of the four casess \"may just be chance variation, of course\". I wonder why we can't arrive at a similar conclusion for the other results in the paper. At the very least I need convincing that this is a reasonable experimental paradigm.\n\nI don't understand the first method for initializing the word embeddings. How can we use the \"current\" embedding for a word if it's never been seen before? What does \"current\" mean in this context?\n\nI also didn't understand the Latin square setup. Training on ten different permutations of the ten sentences suggests that all ten sentences are being used, so I don't see how this can lead to a few-shot or one-shot scenario.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}