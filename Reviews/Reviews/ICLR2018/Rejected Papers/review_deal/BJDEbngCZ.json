{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper studies the global convergence for policy gradient methods for linear control problems.  Multiple reviewers point out strong concerns about the novelty of the results."
    },
    "Reviews": [
        {
            "title": "GLOBAL CONVERGENCE OF POLICY GRADIENT METHODS FOR LINEARIZED CONTROL PROBLEMS",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective. I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved.\n2) The authors state in the abstract (and elsewhere): \"... showing that (model free) policy gradient methods globally converge to the optimal solution ...\". This is misleading and NOT true. The authors show the convergence of the objective but not of the iterates sequence. This should be rephrased elsewhere.\n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "I find this paper not suitable for ICLR. All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Global Convergence of Policy Gradient Methods for Linearized Control Problems",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper studies the global convergence for policy gradient methods for linear control problems. \n(1) The topic of this paper seems to have minimal connection with ICRL. It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully. \n\n(2) I am not convinced if the main results are novel. The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming. The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. I am not sure if it is a good idea to examine the convergence purely from an optimization perspective.\n\n(3) The main results of this paper seem technical sound. However, the results seem a bit limited because it does not apply to neural-network function approximator. It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted. I might have missed something here. I strongly suggest that these results be submitted to a more suitable venue.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}