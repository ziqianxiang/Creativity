{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All three reviewers are in agreement that this paper is not ready for ICLR in its current state. Given the pros/cons, the committee feels the paper is not ready for acceptance in its current form."
    },
    "Reviews": [
        {
            "title": "review",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks\n- introduces a simple branch-and-bound approach\n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one\n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference.\n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read.\n\nSoundness: As far as I can tell, the work is sound.\n\nNovelty: This is in my opinion the weakest point of the paper. There isn't really much novelty in the work. The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is). The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well).\n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods. Unfortunately, there isn't much insight to be gained from them. I couldn't see any emerging trend/useful recommendations (like \"if your problem looks like X, then use algorithm B\"). This is unfortunately often the case when dealing with combinatorial search/optimization. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting study with some flaws",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper studies methods for verifying neural nets through their piecewise\nlinear structure. The authors survey different methods from the literature,\npropose a novel one, and evaluate them on a set of benchmarks.\n\nA major drawback of the evaluation of the different approaches is that\neverything was used with its default parameters. It is very unlikely that these\ndefaults are optimal across the different benchmarks. To get a better impression\nof what approaches perform well, their parameters should be tuned to the\nparticular benchmark. This may significantly change the conclusions drawn from\nthe experiments.\n\nFigures 4-7 are hard to interpret and do not convey a clear message. There is no\nclear trend in many of them and a lot of noise. It may be better to relate the\nstructure of the network to other measures of the hardness of a problem, e.g.\nthe phase transition. Again parameter tuning would potentially change all of\nthese figures significantly, as would e.g. a change in hardware. Given the kind\nof general trend the authors seem to want to show here, I feel that a more\ntheoretic measure of problem hardness would be more appropriate here.\n\nThe authors say of the proposed TwinStream dataset that it \"may not be\nrepresentative of real use-cases\". It seems odd to propose something that is\nentirely artificial.\n\nThe description of the empirical setup could be more detailed. Are the\nproperties that are being verified different properties, or the same property on\ndifferent networks?\n\nThe tables look ugly. It seems that the header \"data set\" should be \"approach\"\nor something similar.\n\nIn summary, I feel that while there are some issues with the paper, it presents\ninteresting results and can be accepted.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "no original contribution",
            "rating": "3: Clear rejection",
            "review": "The paper compares some recently proposed method for validation of properties\nof piece-wise linear neural networks and claims to propose a novel method for\nthe same. Unfortunately, the proposed \"branch and bound method\" does not explain\nhow to implement the \"bound\" part (\"compute lower bound\") -- and has been used \nseveral times in the same application, incl.:\n\nRuediger Ehlers. Planet. https://github.com/progirep/planet,\nChih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis\nAlessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351\n\nSpecifically, the authors say: \"In our experiments, we use the result of \nminimising the variable corresponding to the output of the network, subject \nto the constraints of the linear approximation introduced by Ehlers (2017a)\"\nwhich sounds a bit like using linear programming relaxations, which is what\nthe approaches using branch and bound cited above use. If that is the case,\nthe paper does not have any original contribution. If that is not the case,\nthe authors may have some contribution to make, but have not made it in this\npaper, as it does not explain the lower bound computation other than the one\nbased on LPs.\n\nGenerally, I find a jarring mis-fit between the motivation (deep learning\nfor driving, presumably involving millions or billions of parameters) and\nthe actual reach of the methods proposed (hundreds of parameters).\nThis reach is NOT inherent in integer programming, per se. Modern solvers\nroutinely solve instances with tens of millions of non-zeros in the constraint\nmatrix, but require a strong relaxation. The authors may hence consider\nimproving the LP relaxation, noting that the big-M constraint are notorious\nfor producing weak relaxations.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}