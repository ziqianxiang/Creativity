{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers were quite unanimous in their assessment of this paper.\n\nPROS:\n1. The paper is relatively clear and the approach makes sense\n2. The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks.\n3. Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation.\n4. The multi-stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.\n\nCONS\n1. Lack of novelty e.g. wrt to Finn et al. in \"Deep Spatial Autoencoders for Visuomotor Learning\"\n2. The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem.\n3. The contribution on reward shaping would benefit from a more detailed description and investigation.\n4. There is concern that results may be specific to the chosen task.\n5. Experiments using real robots are needed for practical evaluation."
    },
    "Reviews": [
        {
            "title": "Multiple minor extensions",
            "rating": "3: Clear rejection",
            "review": "The title is too generic and even a bit misleading. Dexterous manipulation usually refers to more complex skills, like in-hand manipulation or using the fingers to turn an object, and not simple pick and place tasks. Reinforcement learning methods are generally aiming to be data-efficient, and the method does not seem designed specifically for dexterous manipulation (which is actually a positive point, as it is more general).\n\nThe paper presents two extensions for DDPG: multiple network updates per physical interactions, and asynchronous updates from multiple robots. As the authors themselves state, these contributions are fairly straightforward, and the contributions are largely based on prior works. The  authors do evaluate the methods with different parameter settings to see the effects on learning performance. \n\nThe simulation environment is fairly basic and seems unrealistic. The hand always starts close to the blocks, which are close together, so the inverse kinematics will be close to linear. The blocks are always oriented in the same direction and they can connect easily with no need to squeeze or wiggle them together. The task seems more difficult from the description in the paper, and the authors should describe the environment in more detail.\n\nDoes the robot learn to flip the blocks over such that they can be stacked? The videos show the \nblocks turning over accidentally, but then the robot seems to give up. Having the robot learn to turn the blocks  would make for a more challenging task and a better policy.\n\nThe paperâ€™s third contribution is a recipe for constructing shaped reward functions for composite tasks. The method relies on a predefined task structure (reach-grasp-stack) and is very similar to reward shaping already used in many other reinforcement learning for manipulation papers. A comparison of different methods for defining the rewards and a more formal description of the reward generation procedure would improve the impact of this section.  The authors should also consider using tasks with longer sequences of actions, e.g., stacking four blocks. \n\nThe fourth and final listed contribution is learning from demonstrated states. Providing the robot with prior knowledge and easier partial tasks will result in faster learning. This result is not surprising. It is not clear though how applicable this approach is for a real robot system. It effectively assumes that the robot can grasp the block and pick it up, such that it can learn the stacking part, while simultaneously still learning how to grasp the block and pick it up. For testing the real robot applicability, the authors should try having the robot learn the task without simulation resets.  \n\nWhat are the actual benefits of using deep learning in this scenario? The authors mention skill representations, such as dynamic motor primitives, which employ significantly more prior knowledge than a deep network. However, as demonstrations of the task are provided, the task is divided into steps, the locations of the objects and finger tips are given, a suitable reward function is provided, and the generalization is only over the object positions, why not train a set of DMPs and optimize them with some additional reinforcement learning? The authors should consider adding a Cartesian DMP policy as a benchmark, as well as discussing the benefits of the proposed approach given the prior knowledge. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Neither very innovative nor very strong evaluations",
            "rating": "4: Ok but not good enough - rejection",
            "review": "I already reviewed this paper for R:SS 2017. There were no significant updates in this version, see my largely identical detailed comment in \"Official Comment\"\n\nQuality\n======\nThe proposed approaches make sense but it is unclear how task specific they are.\n\nClarity\n=====\nThe paper reads well. The authors cram 4 ideas into one paper which comes at the cost of clarity of each of them.\n\nOriginality\n=========\nThe ideas on their own are rather incremental.\n\nSignificance\n==========\nIt is unclear how widely applicable the ideas (and there combination) are an whether they would transfer to a real robot experiment. As pointed out above the ideas are not really groundbreaking on their own.\n\nPros and Cons (from the RSS AC which sums up my thoughts nicely)\n============\n+ The paper presents and evaluates a collection of approaches to speed learning of policies for manipulation tasks.\n+ Improving the data efficiency of learning algorithms and enabling learning across multiple robots is important for practical use in robot manipulation.\n+ The multi-stage structure of manipulation is nicely exploited in reward shaping and distribution of starting states for training.\n\n- The techniques of asynchronous update and multiple replay steps may have limited novelty, building closely on previous work and applying it to this new problem.\n- The contribution on reward shaping would benefit from a more detailed description and investigation.\n- There is concern that results may be specific to the chosen task.   \n- Experiments using real robots are needed for practical evaluation.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The contributions already appear in prior work",
            "rating": "2: Strong rejection",
            "review": "The authors propose to learn to pick up a block and put it on another block using DDPG. A few tricks are described, which I believe already appear in prior work. The discussion of results presented in prior work also has a number of issues. The claim of \"data efficient\" learning is not really accurate, since even with demonstrations, the method requires substantially more experience than prior methods. Overall, it's hard to discern a clear contribution, either experimentally or conceptually, and the excessive claims in the paper are very off-putting. This would perhaps make a reasonable robotics paper if it had a real-world evaluation and if the claims were scoped more realistically, but as-is, I don't think this work is ready for publication.\n\nMore detailed comments:\n\nThe two main contributions -- parallel training and asynchrony -- already appear in the Gu et al. paper. In fact, that paper demonstrates learning entirely in the real world, and substantially more efficiently than described in this paper. The authors don't discuss this at all, except a passing mention of Gu et al.\n\nThe title is not appropriate for this paper. The method is data-efficient compared to what? The results don't look very data efficient: the reported result is something on the order of 160 robot-hours, and 16 robot-hours with demonstration. That's actually dramatically less efficient than prior methods.\n\n\"our results on data efficiency hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots\": Prior work already shows successful stacking policies on real robots, as well as successful pick-and-place policies and a variety of other skills. The funny thing is that many of these papers are actually cited by the authors, but they simply pretend that those works don't exist when discussing the results.\n\n\"We assess the feasibility of performing analogous experiments on real robotics hardware\": I assume this is a typo, but the paper does not actually contain any real robotics hardware experiments.\n\n\"To our knowledge our results provide the first demonstration of end-to-end learning for a complex manipulation problem involving multiple freely moving objects\": This was demonstrated by Finn et al. in \"Deep Spatial Autoencoders for Visuomotor Learning,\" with training times that are a tiny fraction of those reported in this paper, and using raw images and real hardware.\n\n\"both rely on access to a well defined and fully observed state space\": This is not true of the Finn et al. paper mentioned above.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}