{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes to use absolute value activations, in a joint supervised + unsupervised training (classification + deep autoencoder with tied encoder/decoder weights).\nPros:\n + simple model and approach on ideas worth revisiting\nCons:\n- The paper initially approached these old ideas as novel, missing much related prior work\n- It doesn't convincingly breathe novel insight into them.\n- Empirical methodology is not up to standards (non-standard data split, lack of strong baselines for comparison)\n- Empirical validation is too limited in scope (MNIST only)."
    },
    "Reviews": [
        {
            "title": "Preliminary report on a classification/reconstruction network with absolute value activation function",
            "rating": "2: Strong rejection",
            "review": "SUMMARY \n\nThe model is an ANN whose units have the absolute value function abs as their activation function (in place of ReLU, sigmoid, etc.). The network has bi-directional connections (with equal weights) between consecutive layers, but it operates only in one direction at a time. In the forward direction, it is a feed-forward net from image to classification (say); in the reverse direction, it is a feed-forward net from classification to image. In both directions it operates in supervised fashion, trained with backpropagation (subject to the constraint that the weight matrix is symmetric). In the forward direction, the activation vector y over the classification layer is L2-normalized so the activation of a class c is the cosine of the angle between y and the 1-hot vector for c.\nAlthough there is a reverse pass through the net, the training loss function is not adversarial; the loss is just the classification error in the forward pass plus the reconstruction error in the backward pass.\nThe generalization accuracy in classification on 42k-image MNIST is 97.4%.\n\nSTRENGTHS\n\n* Comparisons are made of the reconstruction performance with the proposed abs activation function and with ReLU on one or both passes, and a linear activation function.\n* The model has the virtue of simplicity, as the authors point out.\n\nWEAKNESSES\n\n* The discussion of evaluation of the model is weak. \n  - No baselines are given. (A kaggle leaderboard shows the 50th-ranked model at 97.8% and the top 8 models at 100%.)\n  - The paper talks of a training set and a \"dev\" set, but no test set, and generalization performance is given for the dev set rather than a test set.\n  - No quantitative evaluation of the reconstruction (backward pass) performance is given, just by-eye comparison of the reconstruction error through figures.\n  - Some explanation is needed of why the ReLU cases were fatally plagued with NaN errors.\n* Claims of interpretability advantage seem unwarranted since the claimed interpretability applies to any classification ANN, as far as I can see.\n* The work seems to be at too preliminary a stage to warrant acceptance at ICLR.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "incremental idea, insufficient experimental evaluation",
            "rating": "3: Clear rejection",
            "review": "The paper proposes using the absolute value activation function in (what seems to be) an autoencoder architecture with an additional supervised learning term in the objective function that encourages the bottleneck layer representation to be discriminative. A few examples of reconstructed images and classification performance are reported for the MNIST dataset.\n\nThe contribution of the paper is not clear. The idea of combining autoencoders with supervised learning has been explored before, see e.g., \"Learning Deep Architectures for AI\" by Bengio, 2009, and many other papers. Alternative activation functions have also been studied in many papers, see https://arxiv.org/pdf/1710.05941.pdf for a recent example. Even without novel algorithmic contributions, the paper would have been interesting if there was an extensive evaluation across several challenging datasets of different ways of combining autoencoders with supervised learning and different activation functions that gives better insight into what works and why.\n\nIt would be helpful not to introduce new terminology like \"bidirectional artificial neuron\" unless there is a significant difference from existing concepts. It is not clear from the paper how a network of bidirectional neurons is different from an autoencoder.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting ideas but not fully explored",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper introduces a reversible network with absolute value used as\nthe activation function.  The network is run in the forward direction\nto classify and in the reverse direction to generate.\n\nThe key points of the network are the use of the absolute value\nactivation function and the use of (free) normalization to match\ntarget output. This allows the network to perfectly map inputs to any\npoint on a vector that goes through the one-hot encoding, allowing for\ndeterministic generation from different vectors (of different lengths)\nwith the same normalized output.\n\nI think there are a lot of novel and interesting ideas in this paper\nthough they have not been fully explored.  The use of the absolute\nvalue transfer function is new to me, though I was able to find a couple of old\nreferences to its use.   In a paper by Gad et al. (2000), it is stated \n\" For example, the algorithm presented in Lin and\nUnbehauen (1995) < I think they mean Lin and Unbehauen 1990)> \n is used to train networks with a single hidden layer\nemploying the absolute value as the activation function of the hidden\nneuron. This algorithm was further generalized to multilayer networks\nwith cascaded structures in Batruni (1991).\"   Exploring the properties \nof the abs activation function seems worth exploring.\n\nMore details on the training are needed for full clarity in the paper.\n(Though it is recognized that some of these could be determined from\nlinks when made active, they should be included in the paper).  How\ndid you select the training parameters given at the bottom of page 5?\nHow many layers and units/layer did you use? And how were these\nselected?  (The use of the links for providing code and visualizations (when active)\n is a nice feature of this paper).\n\nAlso, did you compare to using the leaky ReLU activation function --\nThat would be interesting as it also doesn't have any areas of zero\nslope?  Did you compare the generated digits to those obtained using GANs?\n\nI am also curious, how does accuracy on digit classification differ\nwhen trained only to optimize the forward error?\n\nThe MNIST site referenced lists 60,000 training data and test data of\n10,000.  How/why did you select 42,000 and then split it to 39900 in\nthe train set and 2100 in the dev set?\n\nAlso, the goal for the paper is presented as creating highly\ninterpretable representations of the input data.  My interpretation of\ninterpretable is that the hidden units are \"interpretable\" and that it\nis clear how the combined hidden unit representations allow for\naccurate classification.  Towards that end, it would be nice to see\nsome of the interpretations of the hidden unit representations.  In\nthe abstract it states \" ...These representations are generated by\npenalizing the learning of the network in such a way that those\nlearned representations correspond to the respective labels present in\nthe labelled dataset used for supervised training\".  Does this\nstatement refer only to the encoding of the representation vector or\nalso the hidden layers?  If the former, isn't that true for all\nsupervised algorithms.  If the latter, you should show this.\n\nBatruni, R. (1991). A multilayer neural network with piecewise-linear\nstructure and backpropagation learning. IEEE Transactions on Neural\nNetworks, 2, 395–403.\n\nLin, J.-N., & Unbehauen, R. (1995). Canonical piecewise-linear neural\nnetworks. IEEE Transactions on Neural Networks, 6, 43–50.\n\nLin, J.-N, & Unbehauen, R. (1990). Adaptive Nonlinear Digital Filter with Canonical Piecewise-Linear Structure,\nIEEE Transactions on Circuits and Systems, 37(3) 347-353.\n\nGad, E.F et al (2000). A new algorithm for learning in piecewise-linear neural networks.\nNeural Networks 13,  485-505.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}