{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The pros and cons of this paper cited by the reviewers can be summarized below:\n\nPros:\n* Empirical results demonstrate decent improvements over other reasonable models\n* The method is well engineered to the task\n\nCons:\n* The paper is difficult to read due to grammar and formatting issues\n* Experiments are also lacking detail and potentially difficult to reproduce\n* Some of the experimental results are suspect in that the train/test accuracy are basically the same. Usually we would expect train to be much better in highly parameterized neural models\n* The content is somewhat specialized to a particular task in NLP, and perhaps of less interest to the ICLR audience as a whole (although I realize that ICLR is attempting to cast a broad net so this alone is not a reason for rejection of the paper)\n\nIn addition to the Cons cited by the reviewers above, I would also note that there is some relevant work on morphology in sequence-to-sequence models, e.g.:\n* \"What do Neural Machine Translation Models Learn about Morphology?\" Belinkov et al. ACL 2017.\n\nand that it is common in sequence-to-sequence models to use sub-word units, which allows for better handling of morphological phenomena:\n* \"Neural Machine Translation of Rare Words with Subword Units\" Sennrich et al. ACL 2016.\n\nWhile the paper is not without merit, given that the cons seem to significantly outweigh the pros, I don't think that it is worthy of publication at ICLR at this time, although submission to a future conference (perhaps NLP conference) seems warranted."
    },
    "Reviews": [
        {
            "title": "Unreadable paper",
            "rating": "2: Strong rejection",
            "review": "The paper is a pain to read. Most of the citation styles are off (i.e., without parentheses). Most of the sentences are not grammatically correct. Most, if not all, of the determiners are missing. It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct.\n\nThe experimental numbers look skeptical. For example, 1/3 of the training results are worse than the test results in Table 1. It also happens a few times in Table 5. Either the models are not properly trained, or the models are heavily tuned on the test set.\n\nThe running times in Table 9 are also skeptical. Why are the Concorde models faster than unigrams and bigrams? Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Borderline paper on morphological agreement",
            "rating": "5: Marginally below acceptance threshold",
            "review": "In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence. The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task. The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results.\n\nThe morphological agreement task would be an interesting contribution of the paper, with wider potential. But one concern that I have is regarding the evaluation metrics used for it. Firstly, word accuracy rate doesn't seem appropriate, as it does not measure morphological agreement. Secondly, sentence accuracy (w.r.t. the sentences from which the normalized sentences are derived) is not indicative of morphological agreement: even \"wrong\" sentences in the output could be perfectly valid in terms of agreement. A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure.\n\nAnother concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported. Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input? Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models).\n\nAlso, many experimental details are missing from the draft:\n-- What are the sizes of the train/test sets derived from the OpenSubtitles database?\n-- Details of the validation sets used to tune the models.\n-- In Section 5.4, no details of the question-answer corpus are provided. How many pairs were extracted? How many were used for training and testing?\n-- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated?\n-- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages).\n\nPros:\n-- Efficient model\n-- Proposed architecture is general enough to be useful for other sequence-to-sequence problems\n\nCons:\n-- Evaluation metrics for the morphological agreement task are unsatisfactory\n-- It would appear that the baselines could be improved further using standard techniques",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical results are convincing, contribution to representational learning is not much ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \"standard\" form and then into their correct morphological form,\n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,\n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.\n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences. \n\nAs an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}