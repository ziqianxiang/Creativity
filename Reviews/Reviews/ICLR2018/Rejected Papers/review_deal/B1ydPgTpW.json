{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Reviewers concur that the paper and the application area are interesting but that the approaches are not sufficiently novel to justify presentation at ICLR.\n"
    },
    "Reviews": [
        {
            "title": "A successful application of RNN to a real value sentiment analysis task; well written but does not provide a novel network",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits. The method uses a learnable character embedding to transform the data, but is an end-to-end approach. The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features. \nHere are my concerns:\n1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model. That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000. \n\n2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2. If so, the evaluation setting of dividing data into three *random* sets of training, validation, and test, in 5.3 doesn't seem to be the right and most appropriate choice. It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A nicely written paper on an interesting applied problem, but lacking some of the novelty and originality to be expected at a top conference.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors present a deep neural network that evaluates plate numbers. The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context. I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow. There are some typos and grammatical problems (indicated below), but nothing really serious.\n\nSo, the paper is relevant and well presented. However, I find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality. Even though the significance of the work is apparent given the good results of the proposed neural network, I believe that such material is more appropriate to a focused applied meeting. However, even for that sort of setting I think the paper requires some additional work, as some final parts of the paper have not been tested yet (the interesting part of explanations). Hence I don't think the submission is ready for publication at this moment.\n\nConcerning the text, some questions/suggestions:\n- Abstract, line 1: I suppose \"In the Chinese society...\"--- are there many Chinese societies?\n- The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text. \n- Footnote 1, line 2: \"an exchange\".\n- Page 2, line 12: \"prices. Among\".\n- Please add commas/periods at the end of equations.\n- There are problems with capitalization in the references. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "RNNs applied to predicting auction prices from license plate numbers in China, Overall approach is not very scientific. ",
            "rating": "4: Ok but not good enough - rejection",
            "review": "Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \"luckiness\" of the license plate number.  The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings. Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.  The proposed pipeline converts each character to an embedding with the only sentence of description being \"Each character is converted by a lookup table to a vector representation, known as character embedding.\"   Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined. Finally they find a \"a feature vector for each plate by summing up the output of the last recurrent layer overtime.\" and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates. In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better. \n\nSpecific Comments on Sections: \nComments: Sec 1,2\nIn these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data.\n\nSec 3\nThe author does not mention the following reference: \"Deep learning for stock prediction using numerical and textual information\" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks.\n\nSec 4\nWhat are the characters embedded with? This is important to specify. Is it Word2vec or something else? What does the lookup table consist of? References should be added to the relevant methods. \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison. LSTMs as well could have been a point of comparison. \n\nSec 6\n Nothing too insightful is said about the RNN Model. \n\nSec 7\nThe ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table.\n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN. However comparisons are lacking and the paper is not presented very scientifically.  The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}