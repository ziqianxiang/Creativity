{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Pros\n-- Nice way to formulate domain adaptation in a Bayesian framework that explains why autoencoder and domain difference losses are useful.\n\nCons\n-- Model closely follows the framework, but the overall strategy is similar to previous models (but with improved rationale).\n-- Experimental section can be improved. It would interesting to explore and develop the relationship between the proposed technique and Tzeng et al.\n\nGiven the aforementioned cons, the AC is recommending that the paper be rejected.\n"
    },
    "Reviews": [
        {
            "title": "An enjoyable paper, with some room for improvements",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al.\nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem.\n\nThe only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model.  \n\nThe authors perform numerous empirical experiments on several types of problems. They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks. On the downside, all these experiments concern predictive (discriminative) problems. Given the paper title, I would have expected some experiments in a generative context. Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus.\nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016).\n\nMinor comments:\n- Paragraph below Equation 6:  The meaning of $\\phi(\\psi)$ is unclear \n- Equation (7): phi and psi seems inverted \n- Section 4: The acronym MLP is used but never defined.\n\n=== update ===\nI lowered my score and confidence, see my new post below.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Different angle to explain the DA methods, but the novelty is insufficient ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances.  \nHowever, I have the following concerns on novelty. \n\n1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation.  At this point, the novelty is weaken.\n2. In the introduction, the authors mentioned “limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings.” However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features. It is not clear why mSDA cannot handle time-series data but DAuto can.  DAuto does not consider the sequence/ordering of data either. \n3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss). As far as I know, this kind of loss is commonly used in most existing methods. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A probabilistic framework for domain adaptation, some more recent baselines missing",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors propose a probabilistic framework for semi-supervised learning and domain adaptation. By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation. A domain confusion loss is added to learn domain-invariant feature representations. The authors compared Dauto with several baseline methods on several datasets and showed improvement. \n\nThe paper is well-organized and easy to follow. The probabilistic framework itself is quite straight-forward. The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme. \n\nThe proposed DAuto is essentially DANN+autoencoder.  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator. It would be interesting to see if the additional auto-encoder part help address the issue. \n\nThe experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017). \n\nIt could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block. DAuto does seem to offer more boost in domain pairs that are less similar. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}