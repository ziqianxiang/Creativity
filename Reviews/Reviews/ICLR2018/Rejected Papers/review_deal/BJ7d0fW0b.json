{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper proposes a simple idea for using expert data to improve a deep RL agent's performance. Its main flaw is the lack of justification for the specific techniques used. The empirical evaluation is also fairly limited.\n"
    },
    "Reviews": [
        {
            "title": "A novel approach to learning from state trajectories with some unmotivated details and confusing exposition",
            "rating": "6: Marginally above acceptance threshold",
            "review": "SIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose to accelerate the learning of complex tasks by exploiting traces of experts.\nUnlike the most common form of imitation learning or behavioral cloning, the authors \nformulate their solution in the case where the expert’s state trajectory is observable, \nbut the expert’s actions are not. This is an important and useful problem in robotics and other\napplications. Within this specific setting the authors differentiate their approach from others \nby developing a solution that does NOT estimate an explicit dynamics model ( e.g.,  P( S’ | S, A ) ).\nThe benefits of not estimating an explicit action model are not really demonstrated in a clear way.\n\nThe author’s articulate a specific solution that provides heuristic guidance rewards that cause the \nlearner to favor actions that achieve subgoals calculated from expert behavior\nand refactors the representation of the Q function so that it \nhas a component that is a function of the subgoal extracted from the expert.\nThese subgoals are linear functions of the expert’s change in state (or change in state features).\nThe resultant policy is a function of the expert traces on which it depends.\nThe authors show they can retrain a new policy that does not require the expert traces.\nAs far as I am aware, this is a novel approach to the problem. \nThe authors claim that this factorization is important and useful but the paper doesn’t\nreally illustrate this well.\n\nThey demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems.\nThe algorithm learns faster than unassisted DQN as shown by learning curve plots. \nThey also evaluate the algorithms on the quality of the final policies for their approach, DQN, \nand  a supervised learning from demonstration approach ( LfD ) that requires expert actions.\nThe proposed approach does as well or better than competing approaches.\n\n\nQUALITY\n\nAblation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way. However, it would also be useful to do an ablation study of the “factorization” of action values.  Is this important to achieving better results as well or is the guidance reward enough? This seems like a key claim to establish.\n\n\nCLARITY\n\nThe details of the memory based kernel density estimation and neural gradient training seemed\ncomplicated by the way that the process was implemented. Is it possible to communicate\nthe intuitions behind what is going on?\n \nI was able to work out the intuitions behind the heuristic rewards, but I still don’t clearly get \nwhat the Q-value factorization is providing:\n\nTo keep my text readable, I assume we are working in feature space\ninstead of state space and use different letters for learner and expert:\n\n   Learner: S = \\phi(s) \n   Expert’s i^th state visit:  Ei = \\phi( \\hat{s}_i }  where Ei’ is the successor state to Ei\n\nThe paper builds upon approximate n-step discrete-action Q-learning \nwhere the Q value for an action is a linear function of the state features:\n\n    Qp(S,a) = Wa S + Ba\n\nwhere parameters p = ( Wa, Ba ).\n\nAfter observing an experience ( S,A,R,S’ ) we use Bellman Error as a loss function to optimize Qp for parameter p.\nI ignore the complexities of n-step learning and discount factors for clarity.\n\n    Loss = E[    R + MAXa’ Qp(S’,a’)    -   Qp(S,a)   ]  \n\nThe authors suggest we can augment the environment reward R \nwith a heuristic reward Rh proportional to the similarity between \nthe learner “subgoal\" and the expert “subgoal\" in similar states. \n\nThe authors propose to use cosine distance between representations \nof what they call the “subgoals” of learner and expert. \nA subgoal is defined as a linear transformation of the distance traveled by an agent during a transition.\nThe heuristic reward is proportional to the cosine distance between the learner and expert “subgoals\"\n\n   Rh = B  <   Wv LearnerDirectionInStateS,   \n                     Wv ExpectedExpertDirectionInStatesSimilarToS   >\n\nThe learner’s direction in state S is just (S-S’) in feature space.\n\nThe authors model the behavior of the expert as a kernel density type approximator\ngiving the expected direction of the expert starting from a states similar to the one the learner is in. \nLet < Wk S, Wk Ej > be a weighted similarity between learner state features S and expert state features Ej\nand Ej’ be the successor state features encountered by the expert.\nThen the expected expert direction for learner state S is:\n\n     SUMj  < Wk S, Wk Ej > ( Ej - Ej’ ) \n\nPresumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej.\n\nMapping the learner and expert directions into subgoal space using Wv, the heuristic reward is\n\n   Rh = B <   Wv (S-S’),  \n                    Wv SUMj  < Wk S, Wk Ej > ( Ej - Ej’ ) >\n\nI ignore the ReLU here, but I assume that is operates element-wise and just clips negative values?\nThere is only one layer here so we don’t have complex non-linear things going on?\n\nIn addition to introducing a heuristic reward term, the authors propose to alter the Q-function\nto be specific to the subgoal.\n\n   Q( s,a,g ) = g(S) Wa S + Ba\n\nThe subgoal is the same as the first part, namely a linear transform of the expected expert direction in \nstates similar to state S.\n\n    g(S) =  Wv   SUMj  < Wk S, Wk Ej >  ( Ej - Ej’ ) \n\nSo in some sense, the Q function is really just a function of S, as g is calculated from S.\n\n    Q( S,a ) = g(S) Wa S + Ba \n\nSo this allows the Q-function more flexibility to capture each subgoal in a different linear space?\nI don’t really get the intuition behind this formulation. It allows the subgoal to adjust the value \nof the underlying model? Essentially the expert defines a new Q-value problem at every state \nfor the learner? In some sense are we are defining a model for the action taken by the expert?\n\n\nADDITIONAL THOUGHTS\n\nWhile the authors compare to an unassisted baseline, they don’t compare to methods that use an action model\nwhich is not a fatal flaw but would have been nice. \n\nOne can imagine there might be scenarios where the local guidance rewards of this \nform could be problematic, particularly in scenarios where the expert and learner are not identical\nand it is possible to return to previous states, such as the grid worlds the authors discuss:\nIf the expert’s first few transitions were easily approximable,\nthe learner would get local rewards that cause it to mimic expert behavior.\nHowever, if the next step in the expert’s path was difficult to approximate, \nthen the reward for imitating the expert would be lower.\nWould the learner then just prefer to go back towards those states that it can approximate and endlessly loop?\nIn this case, perhaps expressing heuristic rewards as potentials as described in Ng’s shaping paper might solve the problem.\n\n\nPROS AND CONS\n\nImportant problem generally. Avoiding the estimation of a dynamics model was stated as a given, but perhaps more could be put into motivating this goal. Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Works for deterministic controlable dynamics, lack of related work",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper presents a method that leverages demonstrations from experts provided in the shape of sequences of states (actually, state transitions are enough, they don't need to come in sequences) to faster learn reinforcement learning tasks. The authors propose to learn subgoals (actually local rewards) to encourage the agent to go towards the same direction as the expert when encountering similar states. The main claimed advantage is that it doesn't require the knowledge of the actions taken by the expert, only observations of states. \n\nTo me, there is a major flaw in the approach. Ho and Ermon 2016 extensively study the fact that imitation is not possible in stochastic environment without the knowledge of the actions. As the author say, learning the actions from state transitions in a standard stochastic MDP would require to learn the model. Yet, the authors demonstrate their approach in environments where the controlable dynamics is mainly deterministic (if one decides to turn right, the agents indeed turns right). So by subtracting features from successive states, the method mainly encodes the action as it almost encodes the one step dynamics in one shot. \n\nAlso the main assumption is that there is an easy way to compute similarity between states. This assumption is not met in the HealthGathering environment as several different states may generate very similar vision features. This causes the method not to work. This brings us back to the fact that features encoding the actual dynamics, potentially on many consecutive states (e.g. feature expectations used in IRL or occupancy probability used in Ho and Ermon 2016), are mandatory. \n\nThe method is also very close to the simplest IRL method possible which consists in placing positive rewards on every state the expert visited. So I would have liked a comparison to that simple method (using similar regression technique to generalize over states with similar features). \n\nFinally, I also think that using expert data generated by a pre-trained network makes the experimental section very weak. Indeed, it is unlikely that this kind of data can be obtained and training on this type of data is just a kind of distillation of the optimal network making the weights of the network close to the right optimum. With real data, acquired from humans, the training is likely to end up in a very different minima. \n\nConcerning the related work, the authors didn't mention the Universal Value Function Approximation (Schaul et al, @ICML 2015) which precisely extends V and Q functions to generalize over goals. This very much relates to the method used to generalize over subgoals in the paper. Also, the state if the art in IRL and learning from demonstration is lacking a lot of references. For instance, learning via RL + demonstrations was already studied into papers by Farahmand et al (APID, @NIPS 2013), Piot et al (RLED, @ ECML 2014) or Chemali & Lazaric (DPID, @IJCAI 2015) before Hester et al (DQfD @AAAI 2018). Some work is cited in the wrong context. For instance, Borsa et al 2017 doesn't do inverse RL (as said in the related work section) but learn to perform a task only from the extrinsic reward provided by the environment (as said in the introduction). BTW, I would suggest to refer to published papers if they exist instead of their Arxiv version (e.g. Hester et al, DQfD). ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper proposes a hierarchical approach for speeding up RL through subgoals learned from expert demonstrations. Experiments on the game Doom using OpenAI Gym show some improvements in learning rate over DQN.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations. The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning. The goal of this process is to extract a function that maps any given state to a subgoal. Subgoals are then used to learn different Q-value functions, one per subgoal. \nTo learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal. A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal. Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned).\nFinally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation.\nThe paper is overall well-written, and the proposed idea seems interesting. However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them. From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration. It is not clear how this method would generalize to new situations. There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration. \nThe architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g).\nThe empirical evaluation seems rather mixed. Figure 3 shows that the proposed method learns faster than DQN, but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition. Are these the results after all agents had converged? \nOverall, this is a good paper, but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game. Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}