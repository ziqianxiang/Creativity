{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This work develops a methodology for exploration in deep Q-learning through Thompson sampling to learn to play Atari games.  The major innovation is to perform a Bayesian linear regression on the last layer of the deep neural network mapping from frames to Q-values.  This Bayesian linear regression allows for efficiently drawing (approximate) samples from the network.  A careful methodology is presented that achieves impressive results on a subset of Atari games.\n\nThe initial reviews all indicated that the results were impressive but questioned the rigor of the empirical analysis and the implementation of the baselines.  The authors have since improved the baselines and demonstrated impressive results across more games but questions over the empirical analysis remain (by AnonReviewer3 for instance) and the results still span only a small subset of the Atari suite.  The reviewers took issue with the treatment of related work, placing the contributions of this paper in relation to previous literature.\n\nIn general, this paper shows tremendous promise, but is just below borderline.  It is very close to a strong and impressive paper, but requires more careful empirical work and a better treatment of related work.  Hopefully the reviews and the discussion process will help make the paper much stronger for a future submission.\n\nPros:\n- Very impressive results on a subset of Atari games\n- A simple and elegant solution to achieving approximate samples from the Q-network\n- The paper is well written and the methodology is clearly explained\n\nCons:\n- Questions remain about the rigor of the empirical analysis (comparison to baselines)\n- Requires more thoughtful comparison in the manuscript to related literature\n- The theoretical justification for the proposed methods is not strong"
    },
    "Reviews": [
        {
            "title": "A nice algorithm. Decent preliminary results, but some more validation would be welcome.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors propose a new algorithm for exploration in Deep RL. They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action. Posterior weights are sampled to select actions during execution (Thompson Sampling style). I generally liked the paper and the approach, here are some more detailed comments.\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets. It’s not immediately clear what the semantics of this posterior are then. Take for example the case where a particular transition (s,a,r,s’) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced. \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode. But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling? It would be useful to comment on that aspect.\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN). DDQN wasn’t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods). The authors argue they could not reproduce Osband’s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores.  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging. But it would be good to know whether the approach works well across games and is competitive against other stronger baselines. Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice.\n\nIn addition, your reported score on Atlantis of ~2M seems too big. Did you cap the max episode time to 30mins? As is done in the baselines usually.\n\n\nMinor things:\n-“TS finds the true Q-function very fast” But that contradicts the previous statements, I think you mean something different. If TS does not select certain actions, the Q-function would not be updated for these actions. It might find the optimal policy quickly though, even though it doesn’t resolve the entire value function completely.\n-Which epsilon did you use for evaluation of DDQN in the experiments? It’s a bit suspicious that it doesn’t achieve 20+ in Pong.\n-The history of how to go from a Bellman equation to a sample-based update seems a bit distorted. Sample-based RL did not originate in 2008. Also, DQN does not optimize the Bellman residual, it’s a TD update. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but lack of baselines, strong theoretical justification and reference to previous work",
            "rating": "5: Marginally below acceptance threshold",
            "review": "\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning. The Bayesian neural networks are only\nBayesian in the last layer. That is, the authors learn all the previous layers\nby finding point estimates. The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model. The resulting method is\ncalled BDQL. The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN.\n\nQuality:\n\nI am very concern about the authors stating on page 1 \"we sample from the\nposterior on the set of Q-functions\". I believe this statement is not correct.\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions. In this paper there is no generative model for the data and the\ndata obtained is not actually sampled from the model. The data are just targets\nobtained by the q-learning rule. This means that the authors are adapting\nQ-learning methods so that they look Bayesian, but in no way they are dealing\nwith a principled posterior distribution over Q-functions. At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$. In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?\n\nI am also concerned about the hyper-parameter tuning for the baselines. In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL. However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration. This makes me\nwonder if the comparison in table 2 is fair. Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution. This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling. You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem.\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016. What is the purpose of including two rows in table 2 with the same\nmethod? It feels a bit that the authors want to hide the fact that they only\ncompare with a singe epsilon-greedy baseline (DDQN). Epsilon-greedy methods\nhave already been shown to be less efficient than Bayesian methods with\nThompson sampling for exploration in q learning (Lipton et al. 2016).\n\nThe authors do not compare with variational approaches to Bayesian learning\n(Lipton et al. 2016). They indicate that since Lipton et al. \"do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\". This statement seems completely unjustified. The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline. \n\nThe method proposed by the authors is very similar to Lipton's approach. The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights. The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network. It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights. If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems.\n\nThe plots in Figure 2 include performance in terms of episodes. However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method. One could imagine that computing the posterior\napproximation in equation 6 has some additional cost. How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?\n\nClarity:\n\nThe paper is clearly written. However, I found a lack of motivation for the\nspecific design choices made to obtain equations 9 and 10. What is a_t in\nequation 9? The parameters \\theta are updated just after equation 10 by\nfollowing the gradient of the loss in which the weights of the last layer are\nfixed to a posterior sample, instead of the posterior mean. Is this update rule\nguaranteed to produce convergence of \\theta? I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients. Does this create any instability in learning? \n\nI found the paragraph just above section 5 describing the maze-like\ndeterministic game confusing and not very useful. The authors should improve\nthis paragraph.\n\nOriginality:\n\nThe proposed approach in which the weights in the last layer of the neural\nnetwork are the only Bayesian ones is not new. The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June). Scalable Bayesian optimization using deep neural\nnetworks. In International Conference on Machine Learning (pp. 2171-2180).\n\nwhich the authors fail to cite. The use of Thompson sampling for efficient\nexploration in deep Q learning is also not new since it has been proposed by\nLipton et al. 2016. The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice.\n\nSignificance:\n\nIt is hard to determine how significant the work is since the authors only\ncompare with a single baseline and leave aside previous work on efficient\nexploration with Thompson sampling based on variational approximations.\n\nAs far as the method is described, I believe it would be impossible to\nreproduce their results because of the complexity of the hyper-parameter tuning\nperformed by the authors. I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach for Thompson sampling in DQN, some concerns over baseline.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "(Last minute reviewer brought in as a replacement).\n\nThis paper proposed \"Bayesian Deep Q-Network\" as an approach for exploration via Thompson sampling in deep RL.\nThis algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.\nThe agent then samples from this posterior for an approximate Thompson sampling.\nExperimental results show that this outperforms an epsilon-greedy baseline.\n\nThere are several things to like about this paper:\n- The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways.\n- The paper is mostly clear and well written.\n- The experimental results are impressive in their outperformance.\n\nHowever, there are also some issues, many of which have already been raised:\n- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example).\n- There are some loose and misleading descriptions of the algorithm computing \"the posterior\" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away.\n- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.\n- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.\n\nOverall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network.\nHowever, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach.\nFor example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example).\n\nIn it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.\n\n\n===============================================================\n\nRevising my review following the rebuttal period and also the (ongoing) revisions to the paper.\n\nI've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest. Given the ongoing review decisions/issues I'm putting my review slightly below accept.\n\n## Relation to literature on \"randomized value functions\"\nIt's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm). This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm! If you say something like \"they did not estimate a true posterior\" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior. In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN.\n\n## Quality/science of experiments\nThe experimental results have been updated, and the performance of the baseline now seems much more reasonable. However, the procedure for \"selecting arbitrary number of frames\" to report performance seems really unnecessary... it would be clear that BDQN is outperforming DDQN... you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific. This type of stuff smells like overfitting!",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}