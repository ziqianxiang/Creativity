{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All of the reviewers have found some aspects of the formulation interesting, but they raised concerns regarding the practical use of the experimental setup.\n"
    },
    "Reviews": [
        {
            "title": "Unconvincing formalization of a challenge.",
            "rating": "3: Clear rejection",
            "review": "The paper addresses the problem of a mismatch between training classification loss and a loss at test time. This is motivated by use cases in which multiclass classification problems are learned during training, but where binary or reduced multi-class classifications is performed at test time. The question for me is the following: if at test time, we have to solve \"some\" binary classification task, possibly drawn at random from a set of binary problems (this is not made precise in the paper), then why not optimize the same classification error or a surrogate loss at training time? Instead, the authors start with a multiclass problem, which may introduce a computational burden. when the number of classes is large as one needs to compute a properly normalized softmax. The authors now seem to ask, what if one were to use a multi-classification loss at training time, but then decides at test time that a binary classification of one-vs-all is asked for. \n\nIf one buys into the relevance of the setting, then of course, one is faced with the problem that the multiclass logits (aka raw scores) may not be calibrated to be used for binary classification by applying a fixed threshold. The authors call this sententiously \"Principle of logit separation\". Not too surprisingly, the standard multiclass losses do not have the desired property, however approaches that reduce multi-class to binary classification at training time do, namely unnormalized models with penalized log Z (self-normalization), the NCE approach, as well as (the natural in the proposed setting) binary classification loss. I find this almost a bit circular in the line of argumentation, but ok. It remains odd that while usually one has tried to reduce multiclass to binary, the authors go the opposite direction.\n\nThe main technical contribution of the paper is the batch-nornalization that makes sure that multiclass logits across mini-batches of data are better calibrated. One can almost think of that as an additional regularization. This seems interesting and does not create much overhead, if one applies mini-batched SGD optimization anyway. However, I feel this technique would need to be investigated with regard to general improvements in a multiclass setting and as such also benchmarked relative to other methods that could be applied. \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Neat basic idea, but not enough",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper explores a neat, simple idea intended to learn models suitable for fast membership queries about single classes (\"is this data point a member of this class [or set of classes]?\"). In the common case when the class prediction is made with a softmax function minimizing 1-of-K multiclass cross-entropy loss, this cannot in general be determined without essentially evaluating all K logits (inputs to the softmax). This paper describes how other losses (such as the natural multilabel cross-entropy) do not suffer this problem because all true labels' logits rank above all false labels' (so that any membership query can be answered by choosing a threshold), and models trained to minimize these losses perform better on class membership metrics. One of the new losses suggested, the batch cross-entropy, is particularly interesting in keeping with the recent work on using batch statistics; I would like to see this explored further (see below). The paper is well-written.\n\nI am not sure of the relevance of this work as written. The authors discuss how related work (e.g. Grave et al.) scales computationally with K, which is undesirable; however, training the entire network with a non-CE objective function is an end-to-end model change, and practical uptake may suffer without further justification. The problem (and the proposed solution by changing training objective) is of interest because standard approaches ostensibly suffer unfavorable runtime-to-performance tradeoffs, so this should be demonstrated. I would be more comfortable if the authors actually evaluated runtime, preferably against one or two of the other heuristic baselines they cite. \nThe notation is a little uneven. The main idea is stated given the premise of Fig. 1, that there exist logits which are computed and passed through a softmax neuron, but this is never formally stated. (There are a few other very minor quibbles, e.g. top of pg. 6: sum should be over 1...k). ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good solution to the problem of speeding up test-time classification is given. More motivation for the importance of the problem is needed",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper is well-written which makes it easy to understand its main\nthrust - choosing loss functions so that at test time one can\naccurately (and speedily) determine whether an example is in a given\nclass, ie loss functions which are aligned with the \"Principle of Logit\nSeparation (PoLS)\". \n\nWhen the \"Principle of logit separation\" was first given (second page)\nI found it confusing and difficult to parse (too many \"any\"s, I could\nnot work out how the quantification worked). However, the formal\ndefinition (Definition 2.1) was fine. Why not just use this - and drop\nthe vague, wordy definition?\n\nThe paper is fairly 'gentle'. For example, we are taken through\nexamples of loss functions which satisfy \"PoLS\" and those which don't.\nNo 'deep' mathematical reasoning is required - but I don't see this as\na deficiency.\n\nThe experiments are reasonably chosen and, as expected, show the\nbenefits of using PoLS-aligned loss functions.\n\nMy criticism of the paper is that I don't think there is enough\nmotivation. We have that normal classification is linear in the number\nof classes. This modest computational burden (ie just linear),\napparently, is too slow for certain applications.  I would like more\nevidence for this, including some examples of this problem including\nin the paper. This is lacking from the current version.\n\n\ntypos, etc\n\nmax-maring -> max-margin\nthe seconds term -> the second term\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}