{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Pros:\n- The authors propose a new algorithm to train GAN based on Cramer distance arguing that this eases optimization compared to Wasserstein GAN.\n-  Reviewers agree that the paper reads well and provides a good overview of the properties of divergence measures used for GAN training.\n\nCons:\n- It is not clear how much the central arguments about scale sensitivity, sum invariance, and unbiased sample gradients of the distances hold true in practice and generalize.\n- The reviewers do not agree the benefits of the new algorithm is clear from the experiments shown.\nGiven the pros/cons ,the committee feels the paper falls short of acceptance in its current form."
    },
    "Reviews": [
        {
            "title": "Nice read; provides some understanding for GAN training",
            "rating": "7: Good paper, accept",
            "review": "The authors investigate how the properties of different discrepancies for distributions affect the training of parametric model with SGD. They argue that in order for SGD to be a useful training procedure, an ideal metric should be scale sensitive, sum invariant and also provide unbiased gradients. The KL-divergence is not is scale sensitive, and the Wasserstein metric does not provide unbiased gradients. The authors thus posit the Cramer distance as a foundation for the discriminator in the GAN, and then generalize this to an energy based discriminator. The authors then test their Cramer GAN on the CelebA dataset and show comparable results to the Wasserstein GAN, with less mode collapse.\n\nFrom what I can gather, the Cramer GAN is unlikely to be a huge improvement in the GAN literature, but the mathematical relationships investigated in the paper are illuminating. This brings some valuable understanding for improving upon previous GANs [e.g. WGAN]. As energy based GANs and MMD GANs have become more prominent, it would be nice to see how these ideas interplay with those GANs. However, overall I thought the paper did a nice job presenting some context for GAN training.\n\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The paper proposes a too vague discussion of the (dis-)advantages of metrics in statistical learning, which discussion has already taken place in the 50's-60's in the domain of Statistics",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The contribution of the article is related to performance criteria, and in particular to the Wasserstein/Mallows metric, which has received a good deal of attention these last few years in the machine learning literature. The paper starts with a discussion about desirable properties of a loss function and points out the fact that (plug-in) empirical versions of the gradient of this quantity are biased, which limits its interest, insofar as many learning techniques are based on (stochastic) gradient descent. In its current state, this argument looks artificial. Indeed, zero bias can be a desirable properties for an estimate but being biased does not prevent it from being accurate. In contrast, in many situations like ridge regression, incorporating bias permits to drastically reduce variance.It quite depends on the structural assumptions made. For this reason, the worst case result (Theorem 1) is not that informative in my opinion. As they are mainly concerned by the L_1 version of the Wasserstein distance, rather than focussing on the bias, the authors could consider the formulation in terms of inverse cumulative distribution functions in the 1-d setup and the fact that the empirical cdf is never invertible: even if the theoretical cdf is invertible (which naturally guarantees uniqueness of the optimal transport) the underlying mass transportation problem is not as well-posed as that related to its statistical counterpart (however, smoothing the empirical distribution may remedy this issue).\nThe authors propose to use instead the Cramer distance, which is a very popular distance in Statistics and on which many statistical hypothesis testing procedures rely, and review its appealing properties. The comparisons between KL, Wasserstein and Cramer distances is vain in my opinion and willing to come to a general conclusion about the merits of one against the others is naive. In a nonparametric setup, it is always possible to find distributions such that certain of its properties are hidden by certain distances and highlighted by others. This is precisely why you are forced to specify the type of deviations between distributions in nonparametric hypothesis testing (a shift, a change in scale, etc.), there is no way of assessing universally that two distributions are close: optimality can only be assessed for sequences of contiguous hypotheses. The choice of the distance is part of the learning problem. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nicely written paper, nice review of some interesting properties of divergence measures, narrow scope w.r.t the problem addressed,  and on-the-threshold experiments",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The manuscript proposes to use the Cramer distance as a measure between distributions (acting as a loss) when optimizing\nan objective function using stochastic gradient descent (SGD). Cramer distance is a Bregman divergence and is a member of the Lp family of divergences.  Here a \"distance\" means a symmetric divergence measure that satisfies the relaxed triangle inequality. The motivation for using the Cramer distance is that it has unbiased sample gradients while still enjoying some other properties such as scale sensitivity and sum invariant. The authors also proof that for the Bernoulli distribution, there is a lower bound independent of the sample size for the deviation between the gradient of the Cramer distance, and the expectation of the estimated gradient of the Cramer distance. Then, the multivariate case of the Cramer distance, called the energy distance, is also briefly presented. The paper closes with some experiments on ordinal regression using neural networks and training GANs using the Cramer distance. \n\nIn general, the manuscript is well written and the ideas are smoothly presented. While the manuscript gives some interesting insights, I find that the contribution could have been explained in a more broader sense, with a stronger compelling message.\n\nSome remarks and questions:\n\n1.\tThe KL divergence considered here is sum invariant but not scale sensitive, and has unbiased sample gradients. The \n\tauthors are considering here the standard (asymmetric) KL divergence (sec. 2.1). Is it the case that losing scale\n\tsensitivity make the KL divergence insensitive to the geometry of the outcomes? or is it due to the fact the KL \n\tdivergence is not symmetric? or ?\n\n2.\tThe main argument for the paper is that the simple sample-based estimate for the gradient using the Wasserstein \n\tmetric is a biased estimate for the true gradient of the Wasserstein distance, and hence it is not favored with\n\tSGD-type algorithms. Are there any other estimators in the literature for the gradient of the Wasserstein distance?\n\tWas this issue overlooked in the literature?\n\n3.\tI am not sure if a biased estimate for the gradient will lead to a ``wrong minimum'' in an energy space that has \n\tinfinitely many local minima.  Of course one should use an unbiased estimate for the gradient whenever this is possible.\n\tHowever, even when this is possible, there is no guarantee that this will consistently lead to deeper and ``better''\n\tminima, and there is no guarantee as well that these deep local minima reflect meaningful results.\n\n4.\tTo what extent can one generalize theorem 1 to other probability distributions (continuous and discrete) and to the \n\tmultivariate cases as well?\n\n5.\tI also don't think that the example given in sec. 4.2 and depicted in Fig. 1 is the best and simplest way to illustrate\n\tthe benefit of Cramer distance over Wasserstein. Similarly, the experiments for the multivariate case using GANs and\n\tNeural Networks do not really deliver tangible, concrete and conclusive results. Partly, these results are very  \n       qualitative, which can be understood within the context of GANs. However, the authors could have used other       \n       models/algorithms where they can obtain concrete quantitative results (for this type of contribution). In addition, \n\tsuch sophisticated models (with various hyper-parameters) can mask the true benefit for the Cramer distance, and can \n\talso mask the extent of how good/poor is the sample estimate for the Wasserstein gradient.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}