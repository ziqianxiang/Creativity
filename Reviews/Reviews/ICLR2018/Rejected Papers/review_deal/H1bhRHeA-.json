{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.  While the authors point out that their method is O(ND) instead of O(KND), the reviewers really wanted to see graphs demonstrating this, given that the implicit SGD method requires an iterative solver. The revised paper is otherwise much improved from the original submission, but falls a bit short of ICLR acceptance because of the lack of a measurement of convergence vs. time.\n\nPros:\n+ Promising unbiased algorithms for optimizing the log-likelihood of a model using a softmax without having to repeatedly compute the normalizing factor.\n\nCons:\n- The key concern from the reviewers that was not addressed is that none of the experimental results illustrate convergence vs. time instead of convergence vs. number of iterations.\n"
    },
    "Reviews": [
        {
            "title": "Interesting, but with flaws.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper develops an interesting approach for solving multi-class classification with softmax loss.\n\nThe key idea is to reformulate the problem as a convex minimization of a \"double-sum\" structure via a simple conjugation trick.  SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum.  The main contributions of this paper are: \"U-max\" idea (for numerical stability reasons) and an \"\"proposing an \"implicit SGD\" idea.\n\nUnlike the first review, I see what the term \"exact\" in the title is supposed to mean. I believe this was explained in the paper. I agree with the second reviewer that the approach is interesting. However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here. I will stress though that the statement about Newton in the paper is not justified. Newton method does not converge globally with linear rate. Cubic regularisation is needed for global convergence. Local rate is quadratic.  \n\nI believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed.\n\nI apologise for short and late review: I got access to the paper only after the original review deadline.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper explores SGD based explorations to reduce instability in soft-max minimization",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The problem of numerical instability in applying SGD to soft-max minimization is the motivation. It would have been helpful if the author(s) could have made a formal statement. \nSince the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary. The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms.\n\nThe proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies.\nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\"Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\"\n \nwhich could have shed more light on the matter. \n\nThe title is also misleading in using the word \"exact\". I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\n\nIn summary the algorithms are novel variants of SGD but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing. The choice of word exact is also not clear.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents interesting algorithms for minimizing softmax with many classes. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi-class classification problem (using softmax loss) and with linear model. The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it. At each iteration, SGD samples a subset of training samples and labels. The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach. It seems the implicit SGD approach is better in the experimental comparisons. \n\nI found the paper quite interesting, but meanwhile I have the following comments and questions: \n\n- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD. The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch. However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck? Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed. \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? Also, I guess we are assuming the obj is strongly convex?\n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data. As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data). \n\n- All the comparisons are based on \"epochs\", but the competing algorithms are quite different and can have very different running time for each epoch. For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. \n\n- The claim that \"implicit SGD never overshoots the optimum\" needs more supports. Is it proved in some previous papers? \n\n- The presentation can be improved. I think it will be helpful to state the algorithms explicitly in the main paper.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}