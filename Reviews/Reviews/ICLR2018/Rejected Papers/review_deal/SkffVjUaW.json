{
    "Decision": {
        "decision": "Reject",
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous."
    },
    "Reviews": [
        {
            "title": "simple idea that is shown to work well in practice, preliminary ImageNet results demonstrate scalability",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. The authors then introduce a greedy algorithm that expands the different layers in a neural network until the metric indicates that additional features will end up not being used effectively.\n\nThe application of this algorithm is shown to lead to architectures that differ substantially from hand-designed models with the same number of layers: most of the parameters end up in intermediate layers, with fewer parameters in earlier and later layers. This indicates that common heuristics to divide capacity over the layers of a network are suboptimal, as they tend to put most parameters in later layers. It's also nice that simpler tasks yield smaller models (e.g. MNIST vs. CIFAR in figure 3).\n\nThe experimental section is comprehensive and the results are convincing. I especially appreciate the detailed analysis of the results (figure 3 is great). Although most experiments were conducted on the classic benchmark datasets of MNIST, CIFAR-10 and CIFAR-100, the paper also includes some promising preliminary results on ImageNet, which nicely demonstrates that the technique scales to more practical problems as well. That said, it would be nice to demonstrate that the algorithm also works for other tasks than image classification.\n\nI also like the alternative perspective compared to pruning approaches, which most research seems to have been focused on in the past. The observation that the cross-correlation of a weight vector with its initial values is a good measure for effective filter use seems obvious in retrospect, but hindsight is 20/20 and the fact is that apparently this hasn't been tried before. It is definitely surprising that a simple method like this ends up working this well.\n\nThe fact that all parameters are reinitialised whenever any layer width changes seems odd at first, but I think it is sufficiently justified. It would be nice to see some comparison experiments as well though, as the intuitive thing to do would be to just keep the existing weights as they are.\n\nOther remarks:\n\nFormula (2) seems needlessly complicated because of all the additional indices. Maybe removing some of those would make things easier to parse. It would also help to mention that it is basically just a normalised cross-correlation. This is mentioned two paragraphs down, but should probably be mentioned right before the formula is given instead.\n\npage 6, section 3.1: \"it requires convergent training of a huge architecture with lots of regularization before complexity can be introduced\", I guess this should be \"reduced\" instead of \"introduced\".",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Greedy network feature depth optimization",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose an approach to dynamically adjust the feature map depth of a fully convolutional neural network. The work formulates a measure of self-resemblance, to determine when to stop increasing the feature dimensionality at each convolutional layer. The experimental section evaluates this method on MNIST, CIFAR-10/100 and a limited evaluation of ImageNet. Generally, I am a very big proponent of structure learning in neural networks. In particular, we have seen a tremendous boost in performance in going from feature engineering to feature learning, and thus can expect similar effects while learning architectures rather than manually designing them. One important work in this area is \"Self-informed neural network structure learning\" by Farley et al. that is missing from the citations. \nHowever, this work falls short of its promises.\n\n1. The title is misleading. There really isn't much discussion about the architecture of networks, but rather the dimensionality of the feature maps. These are very different concepts.\n2. Novelty of this work is also limited, as the authors acknowledge, that much of the motivation is borrowed from Hao et al., while only the expansion mechanism is now normalized to avoid rescaling issues and threshold tuning.\n3. The general approach lacks global context. All decisions about individual feature depths are made locally both temporally and spatially. In particular, expanding the feature depth at layer f at time t, may have non trivial effect on layer f-1 at time t + 1. In other words, there must be some global state-space manifold to help make decisions globally. This resembles classical dynamic programming paradigms. Local decisions aren't always globally optimal.\n4. Rather than making decision on per layer basis at each iteration, one should wait for the model to converge, and then determine what is useful and what is not.\n5. Finally, the results are NOT promising. In table 1, although the final error has reduced in most cases, it comes at the expense of increases capacity, in extreme cases as much as ~5x, and always at the increased training time, in the extreme case ~14x, An omitted citation of \"Going deeper with Convolution\" is an example, where a much smaller footprint leads to a higher performance, further underlying the importance of a smaller footprint network as stated in the abstract.\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not sure about the novelty / contribution of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network. Experiments are carried out on small-scale datasets such as MNIST and CIFAR, as well as an exploratory run on ImageNet (AlexNet).\n\nOverall, I find the approach proposed in the paper interesting but a little bit thin in content. Essentially, one increases or decreases the number of features based on equation 2. It would be much valuable to see ablation studies to show the effectiveness of such criterion: for example, simple cases one can think of is to model (1) a data distribution of known rank, (2) simple MLP/CNN models to show the cross-layer relationships (e.g. sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc.\n\nThe experimentation section uses small scale datasets and as a result, it is relatively unclear how the proposed approach will perform on real-world applications. One apparent shortcoming of such approach is that training takes much longer time, and the algorithm is not easily made parallel (the sgd steps limit the level of parallelization that can be carried out). As a result, I am not sure about the applicability of the proposed approach.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}