{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "sadly, none of the reviewers seem to have been able to fully appreciate and check the proofs.\n\nbut in the words of even the least positive reviewer:\nIn general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.\n\ni think we can all gain from fresh perspectives of LSTMs and DL for NLP :)\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Interesting paper",
            "rating": "7: Good paper, accept",
            "review": "The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized.\n\n-In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. ",
            "rating": "6: Marginally above acceptance threshold",
            "review": "My review reflects more from the compressive sensing perspective, instead that of deep learners.\n\nIn general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.\n\nThe paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review.\nFinally, extensive experiments are conducted and they are in accordance with the theory.\n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5.\n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery.\n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC.\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.\n\nSeveral minor comments:\n\n1. Please avoid the use of “information theory”, especially “classical information theory”, in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there’s no need to mention information theory here.\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen.\n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs.\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?\n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017].\n\n6. Page 2, first paragraph of related work, the sentence “Our method also closely related to ...” is incomplete.\n\n7. Page 2, second paragraph of related work, “Pagliardini also introduceD a linear ...”\n\n8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous.\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, “The Convex Geometry of Linear Inverse Problems”, Foundations of Computational Mathematics, 2012.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper applies techniques from compressed sensing to analyze the classification performance of LSTM word embeddings",
            "rating": "7: Good paper, accept",
            "review": "The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition.  Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.\n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}