{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The pros and cons of this paper cited by the reviewers can be summarized below:\n\nPros:\n* The method proposed here is highly technically sophisticated and appropriate for the problem of program synthesis from examples\n* The results are convincing, demonstrating that the proposed method is able to greatly speed up search in an existing synthesis system\n\nCons:\n* The contribution in terms of machine learning or representation learning is minimal (mainly adding an LSTM to an existing system)\n* The overall system itself is quite complicated, which might raise the barrier of entry to other researchers who might want to follow the work, limiting impact\n\nIn our decision, the fact that the paper significantly moves forward the state of the art in this area outweighs the concerns about lack of machine learning contribution or barrier of entry.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Although the search method chosen was reasonable, the only real innovation here is to use the LSTM to learn a search heuristic.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents a branch-and-bound approach to learn good programs\n(consistent with data, expected to generalise well), where an LSTM is\nused to predict which branches in the search tree should lead to good\nprograms (at the leaves of the search tree). The LSTM learns from\ninputs of program spec + candidate branch (given by a grammar\nproduction rule) and ouputs of quality scores for programms. The issue\nof how greedy to be in this search is addressed.\n\nIn the authors' set up we simply assume we are given a 'ranking\nfunction' h as an input (which we treat as black-box). In practice\nthis will simply be a guess (perhaps a good educated one) on which\nprograms will perform correctly on future data. As the authors\nindicate, a more ambitious paper would consider learning h, rather\nthan assuming it as a given.\n\nThe paper has a number of positive features. It is clearly written\n(without typo or grammatical problems). The empirical evaluation\nagainst PROSE is properly done and shows the presented method working\nas hoped. This was a competent approach to an interesting (real)\nproblem. However, the 'deep learning' aspect of the paper is not\nprominent: an LSTM is used as a plug-in and that is about it. Also,\nalthough the search method chosen was reasonable, the only real\ninnovation here is to use the LSTM to learn a search heuristic.\n\n\nThe authors do not explain what \"without attention\" means.\n\n\nI think the authors should mention the existence of (logic) program\nsynthesis using inductive logic programming. There are also (closely\nrelated) methods developed by the LOPSTR (logic-based program\nsynthesis and transformation) community. Many of the ideas here are\nreminiscent of methods existing in those communities (e.g. top-down search\nwith heuristics). The use of a grammar to define the space of programs\nis similar to the \"DLAB\" formalism developed by researchers at KU\nLeuven.\n\nADDED AFTER REVISIONS/DISCUSSIONS\n\nThe revised paper has a number of improvements which had led me to give it slightly higher rating.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental paper but well-written",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper extends and speeds up PROSE, a programming by example system, by posing the selection of the next production rule in the grammar as a supervised learning problem.\n\nThis paper requires a large amount of background knowledge as it depends on understanding program synthesis as it is done in the programming languages community. Moreover the work mentions a neurally-guided search, but little time is spent on that portion of their contribution. I am not even clear how their system is trained.\n\nThe experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy. It is difficult to conclude overall if the technique helps in synthesis.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong paper; accept",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This is a strong paper. It focuses on an important problem (speeding up program synthesis), it’s generally very well-written, and it features thorough evaluation. The results are impressive: the proposed system synthesizes programs from a single example that generalize better than prior state-of-the-art, and it does so ~50% faster on average.\n\nIn Appendix C, for over half of the tasks, NGDS is slower than PROSE (by up to a factor of 20, in the worst case). What types of tasks are these? In the results, you highlight a couple of specific cases where NGDS is significantly *faster* than PROSE—I would like to see some analysis of the cases were it is slower, as well. I do recognize that in all of these cases, PROSE is already quite fast (less than 1 second, often much less) so these large relative slowdowns likely don’t lead to a noticeable absolute difference in speed. Still, it would be nice to know what is going on here.\n\nOverall, this is a strong paper, and I would advocate for accepting it.\n\n\nA few more specific comments:\n\n\nPage 2, “Neural-Guided Deductive Search” paragraph: use of the word “imbibes” - while technically accurate, this use doesn’t reflect the most common usage of the word (“to drink”). I found it very jarring.\n\nThe paper is very well-written overall, but I found the introduction to be unsatisfyingly vague—it was hard for me to evaluate your “key observations” when I couldn’t quite yet tell what the system you’re proposing actually does. The paragraph about “key observation III” finally reveals some of these details—I would suggest moving this much earlier in the introduction.\n\nPage 4, “Appendix A shows the resulting search DAG” - As this is a figure accompanying a specific illustrative example, it belongs in this section, rather than forcing the reader to hunt for it in the Appendix.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}