{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This work proposes a hybrid system for large-scale distributed and federated training of commonly used deep networks. This problem is of broad interest and these methods have the potential to be significantly impactful, as is attested by the active and interesting discussion on this work. At first there were questions about the originality of this study, but it seems that the authors have now added extra references and comparisons.\n\nReviewers were split about the clarity of the paper itself. One notes that \"on the whole clearly presented\", but another finds it too dense, disorganized and needing of more clear explanation. Reviewers were also concerned that methods were a bit heuristic and could benefit from more details. There were also many questions about these details in the discussion forum, these should make it into the next version.  The main stellar aspect of the work were the experimental results, and reviewers call them \"thorough\" and note they are convincing. ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "A useful contribution",
            "rating": "7: Good paper, accept",
            "review": "I think this is a good work that I am sure will have some influence in the near future. I think it should be accepted and my comments are mostly suggestions for improvement or requests for additional information that would be interesting to have.\n\nGenerally, my feeling is that this work is a little bit too dense, and would like to encourage the authors in this case to make use of the non-strict ICLR page limit, or move some details to appendix and focus more on more thorough explanations. With increased clarity, I think my rating (7) would be higher.\n\nSeveral Figures and Tables are never referenced in the text, making it a little harder to properly follow text. Pointing to them from appropriate places would improve clarity I think.\n\nAlgorithm 1 line 14: You never seem to explain what is sparse(G). Sec 3.1: What is it exactly that gets communicated? How do you later calculate the Compression Ratio? This should surely be explained somewhere.\n\nSec 3.2 you mention 1% loss of accuracy. A pointer here would be good, at that point it is not clear if it is in your work later, or in another paper. The efficient momentum correction is great!\n\nAs I was reading the paper, I got to the experiments and realized I still don't understand what is it that you refer to as \"deep gradient compression\". Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments.\n\nI feel the presentation of experimental results is somewhat disorganized. It is not clear what is immediately clear what is the baseline, that should be somewhere stressed. I find it really confusing why you sometimes compare against Gradient Dropping, sometimes against TernGrad, sometimes against neither, sometimes include Gradient Sparsification with momentum correction (not clear again what is the difference from DGC). I recommend reorganizing this and make it more consistent for sake of clarity. Perhaps show here only some highlights, and point to more in the Appendix.\n\nSec 5: Here I feel would be good to comment on several other things not mentioned earlier. \nWhy do you only work with 99.9% sparsity? Does 99% with 64 training nodes lead to almost dense total updates, making it inefficient in your communication model? If yes, does that suggest a scaling limit in terms of number of training nodes? If not, how important is the 99.9% sparsity if you care about communication cost dominating the total runtime? I would really like to better understand how does this change and what is the point beyond which more sparsity is not practically useful. Put differently, is DGC with 600x size reduction in total runtime any better than DGC with 60x reduction?\n\n\nFinally, a side remark:\nUnder eq. (2) you point to something that I think could be more discussed. When you say what you do has the effect of increasing stepsize, why don't you just increase the stepsize? \nThere has recently been this works on training ImageNet in 1 hour, then in 24 minutes, latest in 15 minutes... You cite the former, but highlight different part of their work. Broader idea is that this is trend that potentially makes this kind of work less relevant. While I don't think that makes your work bad or misplaced, I think mentioning this would be useful as an alternative approach to the problems you mention in the introduction and use to motivate your contribution.\n...what would be your reason for using DGC as opposed to just increasing the batch size?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "good empirical results, but requires work to justify the proposed techniques",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes additional improvement over gradient dropping(Aji & Heafield) to improve communication efficiency.  \n\n- First of all, the experimental results are thorough and seem to suggest the advantage of the proposed techniques.\n- The result for gradient dropping(Aji & Heafield) should be included in the ImageNet experiment.\n- I am having a hard time understanding the intuition behind v_t introduced in the momentum correction. The authors should provide some form of justifications.\n   - For example, provide an equivalence provide to the original update rule or some error analysis would be great\n   - Did you keep a running sum of v_t overall history? Such sum without damping(the m term in momentum update) is likely lead to the growing dominance of noise and divergence.\n- The momentum masking technique seems to correspond to stop momentum when a gradient is synchronized. A discussion about the relation to asynchronous update is helpful.\n- Do you do non-sparse global synchronization of momentum term? It seems that local update of momentum is likely going to diverge,  and the momentum masking somehow reset that.\n- In the experiment, did you perform local aggregations of gradients between GPU cards before send out to do all0reduce in a network? since doing so will reduce bandwidth requirement.\n\nIn general, this is a paper shows good empirical results. But requires more work to justify the proposed correction techniques.\n\n\n---\n\nI have read the authors updates and changed my score accordingly(see series of discussions)\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Study on gradient compression",
            "rating": "7: Good paper, accept",
            "review": "The paper is thorough and on the whole clearly presented. However, I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle. The methods proposed are heuristic in nature, and it's not clear what the guiding principle is. E.g., \"momentum correction\". What exactly is the problem without this correction? The authors describe it qualitatively, \"When the gradient sparsity is high, the interval dramatically increases, and thus the significant momentum effect will harm the model performance\". Can the issue be described more precisely? Similarly for gradient clipping, \"The method proposed by Pascanu et al. (2013) rescales the gradients whenever the sum of their L2-norms exceeds a threshold. This step is conventionally executed after gradient aggregation from all nodes. Because we accumulate gradients over iterations on each node independently, we perform the gradient clipping locally before adding the current gradient... \" What exactly is the issue here? It reads like a story of what the authors did, but it's not really clear why they did it.\n\nThe experiments seem quite thorough, with several methods being compared. What is the expected performance of the 1-bit SGD method proposed by Seide et al.?\n\nre. page 2: What exactly is \"layer normalization\"?\n\nre. page 4: What are \"drastic gradients\"?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}