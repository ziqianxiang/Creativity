{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper contributes to a body of empirical work towards understanding generalization in deep learning. They do this  through a battery of experiments studying \"single directions\" or selectivity of small groups of neurons. The reviewers that have actively participated agree that the revision is of high quality, impact, originality, and significance. The issue of a lack of prescriptiveness was raised by one reviewer. I agree with the majority that this is not necessary, but nevertheless, the revision makes some suggestions.  I urge the authors to express the appropriate amount of uncertainty regarding any prescriptions that have not been as thoroughly vetted!\n\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "An important piece of the generalization puzzle ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "article summary: \nThe authors use ablation analyses to evaluate the reliance on single coordinate-aligned directions in activation space (i.e. the activation of single units or feature maps) as a function of memorization. They find that the performance of networks that memorize more are also more affected by ablations. This result holds even for identical networks trained on identical data. The dynamics of this reliance on single directions suggest that it could be used as a criterion for early stopping. The authors discuss this observation in relation to dropout and batch normalization. Although dropout is an effective regularizer to prevent memorization of random labels, it does not prevent over-reliance on single directions. Batch normalization does appear to reduce the reliance on single directions, providing an alternative explanation for the effectiveness of batch normalization. Networks trained without batch normalization also demonstrated a significantly higher amount of class selectivity in individual units compared to networks trained without batch normalization. Highly selective units were found to be no more important than units that were not selective to a particular class. These results suggest that highly selective units may actually be harmful to network performance. \n\n* Quality: The paper presents thorough and careful empirical analyses to support their claims.\n* Clarity: The paper is very clear and well-organized. Sufficient detail is provided to reproduce the results.\n* Originality: This work is one of many recent papers trying to understand generalization in deep networks. Their description of the activation space of networks that generalize compared to those that memorize is novel. The authors throughly relate their findings to related work on generalization, regularization, and pruning. However, the authors may wish to relate their findings to recent reports in neuroscience observing similar phenomena (see below).\n* Significance: The paper provides valuable insight that helps to relate existing theories about generalization in deep networks. The insights of this paper will have a large impact on regularization, early stopping, generalization, and methods used to explain neural networks. \n\nPros:\n* Observations are replicated for several network architectures and datasets. \n* Observations are very clearly contextualized with respect to several active areas of deep learning research.\nCons:\n* The class selectivity measure does not capture all class-related information that a unit may pass on. \n\nComments:\n* Regarding the class selectivity of single units, there is a growing body of literature in neurophysiology and neuroimaging describing similar observations where the interpretation has been that a primary role of any neural pathway is to “denoise” or cancel out the “distractor” rather than just amplifying the “signal” of interest. \n    * Untuned But Not Irrelevant: The Role of Untuned Neurons In Sensory Information Coding, https://www.biorxiv.org/content/early/2017/09/21/134379\n    * Correlated variability modifies working memory fidelity in primate prefrontal neuronal ensembles https://www.ncbi.nlm.nih.gov/pubmed/28275096\n    * On the interpretation of weight vectors of linear models in multivariate neuroimaging http://www.sciencedirect.com/science/article/pii/S1053811913010914\n        * see also LEARNING HOW TO EXPLAIN NEURAL NETWORKS https://openreview.net/forum?id=Hkn7CBaTW\n* Regarding the intuition in section 3.1, \"The minimal description length of the model should be larger for the memorizing network than for the structure- finding network. As a result, the memorizing network should use more of its capacity than the structure-finding network, and by extension, more single directions”. Does reliance on single directions not also imply a local encoding scheme? We know that for a fixed number of units, a distributed representation will be able to encode a larger number of unique items than a local one. Therefore if this behaviour was the result of needing to use up more of the capacity of the network, wouldn’t you expect to observe more distributed representations? \n\nMinor issues:\n* In the first sentence of section 2.3, you say you analyzed three models and then you only list two. It seems you forgot to include ResNet trained on ImageNet.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "7: Good paper, accept",
            "review": "\nSummary:\n- nets that rely on single directions are probably overfitting\n- batch norm helps not having large single directions\n- high class selectivity of single units is a bad measure to find \"important\" neurons that help a NN generalize.\n\nThe experiments that this paper does are quite interesting, somewhat confirming intuitions that the community had, and bringing new insights into generalization. The presentation is good overall, but many minor improvements could help with readability.\n\n\nRemarks:\n- The first thing you should say in this paper is what you mean by \"single direction\", at least an intuition, to be refined later. The second sentence of section 2 could easily be plugged in your abstract.\n- You should already mention in section 2.1 that you are using ReLUs, otherwise clamping to 0 might take a different sense.\n- considering the lack of page limit at ICLR, making *all* your figures bigger would be beneficial to readability.\n- Figure 2's y values drop rapidly as a function of x, maybe make x have a log scale or something that zooms in near 0 would help readability.\n- Figure 3b's discrete regimes is very weird, did you actually look at how much these clusters converged to the same solution in parameter space?\n- Figure 4a is nice, but an additional figure zooming in on the first 2 epochs would be really great, because that AUC curve goes up really fast in the beginning.\n- Arpit et al. find that there is more cross-class information being shared for true labels than random labels. Considering you find that low class selectivity is an indicator of good generalization, would it make sense to look at \"cross-class selectivity\"? If a neuron learns a feature shared by 2 or more classes, then it has this interesting property of offering a discrimination potential for multiple classes at the same time, rather than just 1, making it more \"useful\" potentially, maybe less adversary prone?\n- You say in the figure captions that you use random orderings of the features to perform ablation, but nowhere in the main text (which would be nice).\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review of \"On the importance of single directions for generalization\"",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This is an \"analyze why\" style of paper:  the authors attempt to explain the relationship between some network property (in this case, \"reliance on single directions\"), and a desired performance metric (in this case, generalization ability).   The authors quantify a variety of related ways to measure \"reliance on single directions\" and show that the more reliant on a single directions a given network is, the less well it generalizes.   \n\nClarity:  The paper is fairly clearly written.  Sometimes key details are in the footnotes (e.g. see footnote 3) -- not sure why -- but on the  whole, I think the followed the paper reasonably well.  \n\nQuality: The work makes a good-faith attempt to be fairly systematic -- e.g evaluating several different types of network structures, with reasonable numbers of random initializations, and also illustrates the main point in several different comparatively independent-seeming ways.  I feel fairly confident that the results are basically right within the somewhat limited domain that the authors explore. \n\nOriginality: This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks. I don't know that the concept of \"reliance on a single direction\" seems especially novel to me, but on the other hand, I can't think of another paper that precisely investigates this notion the way it is done here.   \n\nSignificance: The work touches on some important issues.  I think the demonstration that the existence of strongly class-selective neurons is not a good correlate for generalization is interesting.   This point illustrates something that has made me a bit uncomfortable with the trend toward \"interpretable machine learning\" that has been arising recently:  in many of those results, it is shown that some fraction of the units at various levels of a trained deepnet have optimal driving stimuli that seem somewhat interpretable, with the implication that the existence of such units is an important correlate of network performance.  There has even been some claims that better-performing networks have more \"single-direction\" interpretable units [1].  The fact that the current results seem directly in contradiction to that line of work is interesting, and the connections to batch normalization and dropout are for the same reason interesting.  However, I wish the authors had grappled more directly with the apparent contradiction with (e.g.) [1].   There is probably a kind of tradeoff here.   The closer the training dataset is to what is being tested for \"generalization\", the more likely that having single-direction units is useful; and vice-versa.   I guess the big question is: what types of generalization are actually demanded / desired in real deployed machine learning systems (or in the brain)?  How does those cases compare with the toy examples analyzed here?   The paper doesn't go far enough in really addressing these questions, but it is sort of beginning to make an effort. \n\nHowever, for me the main failing of the paper is that it's fairly descriptive without being that prescriptive. Does using their metric of reliance on a single direction, as a regularizer in and of itself, add anything above any beyond existing regularizers (e.g. batch normalization or dropout)?  It doesn't seem like they tried. This seems to me the key question to understanding the significance of their results.   Is \"reliance on single direction\" actually a good regularizer as such, especially for \"real\" problems like (e.g.) training a deep Convnet on (e.g.) ImageNet or some other challenging dataset?  Would penalizing for this quantity improve the generalization of a network trained on ImageNet to other visual datasets (e.g. MS-COCO)?  If so, this would be a very significant result and would make me really care about their idea of \"reliance on a singe direction\".  If such results do not hold, it seems to me like one more theoretical possibility that would bite the dust when tested at scale.  \n\n[1] http://netdissect.csail.mit.edu/final-network-dissection.pdf",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}