{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Thank you for submitting you paper to ICLR. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "The paper proposes an approach to perform inference in models that combine probabilistic graphical models with deep networks",
            "rating": "7: Good paper, accept",
            "review": "The paper seems to be significant since it integrates PGM inference with deep models. Specifically, the idea is to use the structure of the PGM to perform efficient inference. A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part. Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference.\nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates. As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution.\n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved. One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs. Can this be generalized to arbitrary PGM structures? How about cases when computing Z is intractable? Could the proposed approach be adapted to such cases. I was not very sure as to why the proposed method is more general than existing approaches.\n\nRegarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets. the approach shows that the proposed methods converge faster than existing methods. However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "More efficient variational inference with structured inference networks",
            "rating": "7: Good paper, accept",
            "review": "The authors adapts stochastic natural gradient methods for variational inference with structured inference networks. The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter. The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters. In the experiments the authors generally show improved convergence over SVAE.\n\nThe idea seems promising but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation. The main motivation seems to be that it is easier to optimize.\n\n- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta. Might this also suffer from non-convergence issues like you argue SVAE does? Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward?\n- It was not clear to me why we should use a Gaussian approximation for the \\theta_NN parameters? The prior might be Gaussian but the posterior is not? Is this more of a simplifying assumption?\n- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models. Some examples of related work missing in this area is \"Variational Sequential Monte Carlo\" by Naesseth et al. (2017) / \"Filtering Variational Objectives\" by Maddison et al. (2017) / \"Auto-encoding sequential Monte Carlo\" Le et al. (2017).\n-  Section 2.1, paragraph nr 5, \"algorihtm\" -> \"algorithm\"\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents an interesting algorithm to perform variational inference with amortized and natural gradient updates of models that contain deep neural network and probabilistic graphical model components. The contributions of the paper need to be more clearly presented and the experiments could be more thorough.",
            "rating": "7: Good paper, accept",
            "review": "This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents.\nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components. The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms. Overall, I think that the paper proposes some\ninteresting ideas, however, in its current form I do not think that the novelty\nof the contributions are clearly presented and that they are not thoroughly\nevaluated in the experiments.\n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components. However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm. As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters. These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017),\nbut I think the authors may be overselling these ideas as a brand new algorithm.\nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better.\n\nAnother concern that I have is that there are a lot of conditiona-conjugacy\nassumptions baked into the algorithm that the authors only mention at the end\nof the presentation of their algorithm. Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation. Though one could do\nthis, the authors do not adequately show that one should, or that one can do this\nwithout suffering a lot of error in the posterior approximation. I think that\nwithout an experiment the small section on non-conjugacy should be removed.\n\nFinally, I found the experimental evaluation to not thoroughly demonstrate the\nadvantages and disadvantages of the proposed algorithm. The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels. The experiments do not however demonstrate why the algorithm is\nperforming better. For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient? It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm.\n\nI also have some minor comments on the paper:\n- There are a lot of typos.\n- The first two sentences of the abstract do not really contribute anything\n  to the paper. What is a powerful model? What is a powerful algorithm?\n- DNN was used in Section 2 without being defined.\n- Using p() as an approximate distribution in Section 3 is confusing notation\n  because p() was used for the distributions in the model.\n- How is the covariance matrix parameterized that the inference network produces?\n- The phrases \"first term of the inference network\" are not clear. Just use The\n  DNN term and the PGM term of the inference networks, and better still throw\n  in a reference to Eq. (4).\n- The term \"deterministic parameters\" was used and never introduced.\n- At the bottom of page 5 the extension to the non-conjugate case should be\n  presented somewhere (probably the appendix) since the fact that you can do\n  this is a part of your algorithm that's important.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}