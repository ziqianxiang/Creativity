{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This is another paper, similar in spirit to the Wasserstein GAN and Cramer GAN, which uses ideas from optimal transport theory to define a more stable GAN architecture. It combines both a primal representation (with Sinkhorn loss) with a minibatch-based energy distance between distributions.\nThe experiments show that the OT-GAN produces sharper samples than a regular GAN on various datasets. While more could probably be done to distinguish the model from WGANs and Cramer GANs, this paper seems like a worthwhile contribution to the GAN literature and merits publication.\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Design choices",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents a variant of GANs in which the distance measure between the generator's distribution and data distribution is a combination of two recently proposed metrics. In particular, a regularized Sinkhorn loss over a mini-batch is combined with Cramer distance \"between\" mini-batches. The transport cost (used by the Sinkhorn) is learned in an adversarial fashion. Experimental results on CIFAR dataset supports the usefulness of the method.\n\nThe paper is well-written and experimental results are supportive (state-of-the-art ?)\n\nA major practical concern with the proposed method is the size of mini-batch. In the experiment, the size is increased to 8000 instances for stable training. To what extent is this a problem with large models? The paper does not investigate the effect of small batch-size on the stability of the method. Could you please comment on this?\n\nAnother issue is the adversarial training of the transport cost. Could you please explain why this design choice cannot lead instability? \n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An interesting paper on how to push optimal transport in the \"classical\" GAN framework",
            "rating": "6: Marginally above acceptance threshold",
            "review": "There have recently been a set of interesting papers on adapting optimal transport to GANs. This makes a lot of sense. The paper makes some very good connections to the state of the art and those competing approaches. The proposal makes sense from the generative standpoint and it is clear from the paper that the key contribution is the design of the transport cost. I have two main remarks and questions.\n\n* Regarding the transport cost, the authors say that the Euclidean distance does not work well. Did they try to use normalised vectors with the squared Euclidean distance ? I am asking this question because solving the OT problem with cost defined as in c_eta is equivalent to using a *normalized squared* Euclidean distance in the feature space defined by v_eta. If the answer is yes and it did not work, then there is indeed a real contribution to using the DNN. Otherwise, the contribution has to be balanced. In either case, I would have been happy to see numbers for comparison.\n\n* The square mini batch energy distance looks very much like a maximum mean discrepancy criterion (see the work of A. Gretton), up to the sign, and also to regularised approached to MMD optimisation (see the paper of Kim, NIPS'16 and references therein). The MMD is the solution of an optimisation problem which, I suppose, has lots of connections with the dual Wasserstein GAN. The authors should elaborate on the relationships, and eventually discuss regularisation in this context.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New algorithm motivated theoretically, good results, honest writing",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "The paper introduces a new algorithm for training GANs based on the Earth Moverâ€™s distance. In order to avoid biased gradients, the authors use the dual form of the distance on mini-batches, to make it more robust. To compute the distance between mini batches, they use the Sinkhorn distance. Unlike the original Sinkhorn distance paper, they use the dual form of the distance and do not have biased gradients. Unlike the Cramer GAN formulation, they use a mini-batch distance allowing for a better leverage of the two distributions, and potentially decrease variance in gradients.\n\nEvaluation: the paper shows good results a battery of tasks, including a standard toy example, CIFAR-10 and conditional image generation, where they obtain better results than StackGAN. \n\nThe paper is honest about its shortcomings, in the current set up the model requires a lot of computation, with best results obtained using a high batch size.\n\nWould like to see: \n  * a numerical comparison with Cramer GAN, to see whether the additional  computational cost is worth the gains. \n  * Cramer GAN shows an increase in diversity, would like to see an analog experiment for conditional generation, like figure 3 in the Cramer GAN paper.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}