{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper presents a fairly straightforward algorithm for learning a set of sub-controllers that can be re-used between tasks.  The development of these concepts in a relatively clear way is a nice contribution.  However, the real problem is how niche the setup is.  However, it's over the bar in general.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Metalearning with shared hierarchies",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper considers the reinforcement learning problem setup in which an agent must solve not one, but a set of tasks in some domain, in which the state space and action space are fixed. The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution.\n\nI found the paper to be generally well written and the key ideas easy to understand on first pass. The authors should be commended for this. Aside from a few minor grammatical issues (e.g. missing articles here and there), the writing cannot be too strongly faulted.\n\nThe problem setup is of general interest to the community. Metalearning in the multitask setup seems to be gaining attention and is certainly a necessary  step towards building rapidly adaptable agents.\n\nWhile the concepts were clearly introduced, I think the authors need to make, much more strongly, the case that the method is actually valuable. In that vein, I would have liked to see more work done on elucidating how this method works ‘under the hood’. For example, it is not at all clear how the number of sub policies affects performance (one would imagine that there is a clear trade off), nor how this number should be chosen. It seems obvious that this choice would also affect the subtle dynamics between holding the master policy constant while updating the sub policies and vice versa. While the authors briefly touch on some of these issues in the rationale section, I found these arguments largely unsubstantiated. Moreover, this leads to a number of unjustified hyper-parameters in the method which I suspect would affect the training catastrophically without significant fine-tuning.\n\nThere are also obvious avenues to be followed to check/bolster the intuitions behind the method. By way of example, my sense is that the procedure described in the paper uncovers a set of sub policies that form a `good’ cover for the task space - if so simply plotting out what they policies look like (or better yet how they adapt in time) would be very insightful (the rooms domain is perhaps a good candidate for this).\n\nWhile the key ideas are clearly articulated  the practical value of the procedure is insufficiently motivated. The paper would benefit hugely from additional analysis.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting.  The setting is one where the agent must solve some task for many episodes in a sequence, after which the task will change and the process repeats.  The proposed solution method splits the agent into two components, a master policy which is reset to random initial weights for each new task, and several sub-policies (motor primitives) that are selected between by the master policy every N steps and whose weights are not reset on task switches.  The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task, whereas learning the motor primitives occurs slowly over many different tasks.  To push this motivation into the learning process, the master policy is updated always but the sub-policies are only updated after an extended warmup period (called the joint-update or training period).  This experiments include both small domains (moving to 2D goals and four-rooms) and more complex physics simulations (4-legged ants and humanoids).  In both the simple and complex domains, the proposed method (MLSH) is able to robustly achieve good performance.\n\nThis approach to obtaining complex structured behavior appears impressive despite the amount of temporal structure that must be provided to the method (the choice of N, the warmup period, and the joint-update period).  Relying on the temporal structure for the hierarchy, and forcing the master policy to be relearned from scratch for each new task may be problematic in general, but this work shows that in some complex settings, a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of meta-policies over these motor-primitives.  Moreover, the results show that these temporal hierarchies are helpful in these domains, as the corresponding non-hierarchical methods failed on the more challenging tasks.\n\nThe paper could be improved in some places (e.g. unclear aliases of joint-update or training periods, describing how the parameters were chosen, and describing what kinds of sub-policies are learned in these domains when different parameter choices are made).\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "very vague paper",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Please see my detailed comments in the \"official comment\"\n\nThe extensive revisions addressed most of my concerns\n\nQuality\n======\nThe idea is interesting, the theory is hand-wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don't evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED).\n\nClarity\n=====\nThe paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader's best guess (ADDRESSED).\n\nOriginality\n=========\nThe idea looks original.\n\nSignificance\n==========\nIf it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution.\n\nPros and Cons\n============\n+ interesting idea\n- we do everything asynchronously and in parallel and it magically works (ADDRESSED)\n- many open questions / missing details (ADDRESSED)",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}