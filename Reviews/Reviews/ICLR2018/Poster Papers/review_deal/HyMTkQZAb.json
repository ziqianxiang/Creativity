{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This clearly written paper extends the Kronecker-factored approximate curvature optimizer to recurrent networks.  Experiments on Penn Treebank language modeling and training of differentiable neural computers on a repeated copy task show that the proposed K-FAC optimizers are stronger than SGD, Adam, and Adam with layer normalization. The most negative reviewer objected to a lack of theoretical error bounds on the approximations made, but the authors successfully argue that obtaining such bounds would require making assumptions that are likely to be violated in practice, and that strong empirical performance on real tasks is sufficient justification for the approximations.\n\nPros:\n+ \"Completes\" K-FAC training by extending it to recurrent models.\n+ Experiments show effects of different K-FAC approximations.\n\nCons:\n- The algorithm is rather complex to implement.\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Nice extension of K-FAC for RNNs, but missing validation/testing performances",
            "rating": "7: Good paper, accept",
            "review": "\nSummary of the paper\n-------------------------------\n\nThe authors extend the K-FAC method to RNNs. Due to the nature of BPTT, the approximation that the activations 'a' are independent from the gradients 'Ds' doesn't hold anymore and thus other approximations have to be made. They present 3 ways of approximating F, and show optimization results on 3 datasets, outperforming ADAM in both number of updates and computation time.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Above average. The mathematical notations is overly verbose, so it makes the paper harder to understand. Note that it is not the author's fault only, since they followed the notation of the other K-FAC papers.\nFor instance, the notations goes from 'E[xy^T]' to 'cov(x, y)' to 'V_{x,y}'. I don't think introducing the 'cov' notation helps with the understanding of the paper (unless they explicitly wanted to stress out that the covariance of the gradients of the outputs of the model are centered). Also the 'V' in equation (4) could be confused with the 'V' in the first equation. Moreover, for the gradients with respect to the activations, we go from 'dL/ds' to 'Ds' to 'g', and for the weights we go from 'dL/dW' to 'DW' to 'w'. Why not keeping the 'Ds' and 'Dw' notation throughout the paper, and defining Dx as vec(dL/dx)?\n\nSignificance: This paper aims at helping with the optimization of RNNs and is thus and important contribution for our community.\n\nCorrectness: The paper is technically correct.\n\nQuestions\n--------------\n\n1. In figure 1, how does it compare to Adam instead of SGD? I think it would be a more fair comparison since SGD is rarely used to train RNNs (as RMSprop and ADAM might help with the vanishing/exploding gradients problem). Also, does the SGD baseline has momentum (since your method does)?\n2. In all experiments, how do the validation / testing curves look like?\n3. How does it compare to different reparametrization techniques, such as Layer Normalization or Batch Normalization?\n\nPros\n------\n\n1. This paper completes the K-FAC family.\n2. It addresses the optimization of RNNs, which is an important research direction in our field.\n3. It shows different levels of approximations of the Fisher, with the corresponding performances.\n\nCons\n-------\n\n1. No validation / test curves for any experiments, which makes it hard to asses if one should use this method in practice or not.\n2. The notation is a bit verbose and can become confusing.\n3. Small experimental setting (only PTB and DNC).\n\nTypos\n--------\n\n1. Sec 1, par 5: \"a new family curvature\" -> \"a new family of curvature\"",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel method, incremental but sufficient improvements",
            "rating": "7: Good paper, accept",
            "review": "In this paper, the authors present a second-order method that is specifically designed for RNNs. The paper overall is well-written and I enjoyed reading the paper. \n\nThe main idea of the paper is to extend the existing kronecker-factored algorithms to RNNs. In order to obtain a tractable formulation, the authors impose certain assumptions and provide detailed derivations. Even though the gain in the convergence speed is not very impressive and the algorithm is quite complicated and possibly not very accessible by deep learning practitioners, I still believe this is a novel and valuable contribution and will be of interest to the community. \n\nI only have some minor corrections:\n\n1) Sec 2.1: typo \"is is\"\n2) Sec 2.2: typo \"esstentiallybe\"\n3) Sec 2.2: (F+lambda I) --> should be inverse\n4) The authors should include a proper conclusion",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Assumptions used in approximations are not well justified",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper extends the Kronecker-factor Approximate Curvature (K-FAC) optimization method to the setting of recurrent neural networks. The K-FAC method is an approximate 2nd-order optimization method that builds a block diagonal approximation of the Fisher information matrix, where the block diagonal elements are Kronecker products of smaller matrices. \n\nIn order to approximate the Fisher information matrix for RNNs, the authors assume that the derivative of the loss function with respect to each weight matrix at each time step is independent of the length of the sequence, that these derivatives are temporally homogeneous, that the input and derivatives of the output are independent across every point in time, and that either the one-step cross-covariance of these derivatives is symmetric or that the training sequences are effectively infinite in length. Based on these assumptions, the authors show that the Fisher information can be reduced into a form in which the derivatives of the weight matrices can be approximated by a linear Gaussian graphical model and in which the approximate 2nd order method can be efficiently carried out. The authors compare their method to SGD on two language modeling tasks and against Adam for learning differentiable neural computers.\n\nThe paper is relatively clear, and the authors do a reasonable job of introducing related work of the original K-FAC algorithm as well as its extension to CNNs before systematically deriving their method for RNNs. The problem of extending the K-FAC algorithm is natural, and the steps taken in this paper seem natural yet also original and non-trivial.  \n\nThe main issue that I have with this paper is the lack of theoretical justification or even intuition for the many approximations carried out in the course of approximating the Fisher information matrix. In many instances, it seemed like these approximations were made purely for convenience and tractability without much regard for (even approximate) correctness. This quality of this paper would be greatly  strengthened if it had some bounds on approximation error or even empirical results testing the validity of the assumptions in the paper. Moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work.  \n\nSpecific comments and questions\nPage 2, \"r is is\". Typo.\nPage 2, \"DV\". I found the introduction of V without any explanation to be confusing.\nPage 2, \"P_{y|x}(\\theta)\". The relation between P_{y|x}(\\theta) and f(x,\\theta) is never explained.\nPage 3, \"common practice of computing the natural gradient as (F + \\lambda I) \\nabla h instead of F^{-1} \\nabla h\". I don't see how the former can serve as a replacement for the latter.\nPage 3, \"approximate g and a as statistically independent\". Even though K-FAC already exists, it would be good to explain why this assumption is reasonable, since similar assumptions are made for the work presented in this paper.\nPage 4, \"This new approximation, called \"KFC\", is derived by assuming....\". Same as previous comment. It would be good to briefly discuss why these assumptions are reasonable.\nPage 5, Independence of T and w_t's, temporal homogeneity of w_t's,, and independence between a_t's and g_t's. I can see why these are convenient assumptions, but why are they reasonable? Moreover, why is it further natural to assume that A and G are temporally homogeneous as well?\nPage 7, \"But insofar as the w_t's ... encode the relevant information contained in these external variables, they should be approximately Markovian\". I am not sure what this means.\nPage 7, \"The linear-Gaussian assumption meanwhile is a more severe one to make, but it seems necessary for there to be any hope that the required expectations remain tractable\". I am not sure that this is a good enough justification for such an idea, unless there are compelling approximation error bounds. \nPage 8, Option 1. In what situations is it reasonable to assume that V_1 is symmetric? \nPages 8-9, Option 2. What is a good finite sample size in which the assumption that the training sequences are infinitely long is reasonable in practice? Can the error |\\kappa(x) - \\zeta_T(x)| be translated into a statement on the approximation error?\nPage 9, \"V_1 = V_{1,0} = ...\". Typos (that appear to have been caught by the authors already).\nPage 9, \"The 2nd-order statistics ... are accumulated through an exponential moving average during training\". How sensitive is the performance of this method to the decay rate of the exponential moving average? \nPage 10, \"The additional computations required to get the approximate Fisher inverse from these statistics ... are performed asynchronously on the CPU's\". I find it a bit unfair to compare SGD to K-FAC in terms of wall clock time without also using the extra CPU's for SGD as well (e.g. via Hogwild or synchronous parallel SGD).\nPage 10, \"The hyperparameters of our approach...\". What is the sensitivity of the experimental results to these hyperparameters? Moreover, how sensitive are the results to initialization?\nPage 10, \"we found that each parameter update of our method required about 80% more wall-clock time than an SGD update\". How much of this is attributed to the fact that the statistics are computed asynchronously?\nPages 10-12, Experiments. There are no error bars in any of the plots, so it is impossible to ascertain the statistical significance of any of these results. \nPage 11: Figure 2. Where is the Adam batchsize 50 line in the left plot? Why did the Adam batchsize 200 line disappear halfway through the right plot?\n  \n\n\n  ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}