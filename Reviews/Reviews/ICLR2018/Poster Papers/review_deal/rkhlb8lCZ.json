{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The idea of using wavelet pooling is novel and will bring many interesting research work in this direction. But more thorough experimental justification such as those recommended by the reviewers would make the paper better. Overall, the committee feels this paper will bring value to the conference.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "An interesting thought but not well justified or tested.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes to use discrete wavelet transforms combined with downsampling to achieve arguably better pooling output compared to average pooling or max pooling. The idea is tested on small-scale datasets such as MNIST and CIFAR.\n\nOverall, a major issue of the paper is the linear nature of DWT. Unless I misunderstood the paper, linear DWT is being adopted in the paper, and combined with the downsampling and iDWT stage, the transform is linear with respect to the input: DWT and iDWT are by definition linear, and the downsampling can be viewed as multiplying by 0. As a result, if my understanding is correct, this explains why the wavelet pooling is almost the same as average pooling in the experiments (other than MNIST). See figures 10, 12 and 14.\n\nThe rest of the paper reads reasonable, but I am not sure if they offset the issue above.\n\nOther minor comments:\n\n- I am not sure if the issue in Figure 2 applies in general to image classification issues. It surely is a mathematical adversarial to the max pooling principle, but note that this only applies to the first input layer, as such behavior in later layers could be offset by switching the sign bits of the previous layer's filter.\n\n- The experiments are largely conducted with very small scale datasets. As a result I am not sure if they are representative enough to show the performance difference between different pooling methods.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written/motivated paper, some extra polishing would make it solid.",
            "rating": "7: Good paper, accept",
            "review": "The paper proposes \"wavelet pooling\" as an alternative for traditional subsampling methods, e.g. max/average/global pooling, etc., within convolutional neural networks. \nExperiments on the MNIST, CIFAR-10, SHVN and KDEF datasets, shows the proposed wavelet-based method has\ncompetitive performance with existing methods while still being able to address the overfitting behavior of max pooling.\n\nStrong points\n- The method is sound and well motivated.\n- The proposes method achieves competitive performance.\n\nWeak points\n- No information about added computational costs is given.\n- Experiments are conducted in relatively low-scale datasets.\n\n\nOverall the method is well presented and properly motivated. The paper as a good flow and is easy to follow. The authors effectively demonstrate with few toy examples the weaknesses of traditional methods, i.e max pooling and average pooling. Moreover, their extended evaluation on several datasets show the performance of the proposed method in different scenarios.\n\nMy main concerns with the manuscript are the following.\n\nCompared to traditional methods, the proposed methods seems to require higher computation costs. In a deep neural network setting where operations are conducted a large number of times, this is a of importance. However, no indication is given on what are the added computation costs of the proposed method and how that compares to existing methods. A comparison on that regard would strengthen the paper.\n\nIn many of the experiments, the manuscript stresses the overfitting behavior of max pooling. This makes me wonder whether this is caused by the fact that experiments are conducted or relatively smaller datasets. While the currently tested datasets are a good indication of the performance of the proposed method, an evaluation on a large scale scenario, e.g. ILSVRC'12, could solidify the message sent by this manuscript. Moreover, it would increase the relevance of this work in the computer vision community.\n\nFinally, related to the presentation, I would recommend presenting the plots, i.e. Fig. 8,10,12,14, for the training and validation image subsets in two separate plots. Currently, results for training and validation sets are mixed in the same plot, and due to the clutter it is not possible to see the trends clearly.\nSimilarly, I would recommend referring to the Tables added in the paper when discussing the performance of the proposed method w.r.t. traditional alternatives.\n\nI encourage the authors to address my concerns in their rebuttal",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea for \"pooling\"",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "I think this paper presents an interesting take on feature pooling. In particular, the idea is to look at pooling as some form of a lossy process, and try to find such a process such that it discards less information given some decimation criterion. Once formulating the problem like this, it becomes obvious that wavelets are a very good candidate.\n\nPros:\n- The nice thing about this method is that average pooling is in some sense a special case of this method, so we can see a clear connection.\n- Lots of experiments, and results, which show the method both performing the best in some cases, and not the best in others. I applaud the authors for publishing all the experiments they ran because some may have been tempted to \"forget\" about the experiments in which the proposed method did not perform the best.\n\nCons:\n- No comparison to non-wavelet methods. For example, one obvious comparison would have been to look at using a DCT or FFT transform where the output would discard high frequency components (this can get very close to the wavelet idea!).\n- This method has the potential to show its potential on larger pooling windows than 2x2. I would have loved to see some experiments that prove/disprove this.\n\nOther comments:\n- Given that this method's flexibility, I could imagine this generate a new class of pooling methods based on lossy transforms. For example, given a MxNxK input, the wavelet idea can be made to output (M/D)x(N/D)x(K/D) (where D is decimation factor). Of interest is the fact that channels can be treated just like any other dimension, since information will be preserved!\n\nFinal comments:\n- I like the idea and it seems novel it may lead to some promising research directions related to lossy pooling methods/channel aggregation. As such, I think it will be a nice addition to ICLR, especially if the authors decide to run some of the experiments I was suggesting, namely: show what happens when larger pooling windows are used (say 4x4 instead of 2x2), and compare to other lossy techniques (such as Fourier or cosine-transforms).",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}