{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Overall this paper seems to make an interesting contribution to the problem of subtask discovery, but unfortunately this only works in a tabular setting, which is quite limiting.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Further development needed before the method can be applied in RL",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper builds upon the work of Saxe et al on multitask LMDP and studies how to automatically discover useful subtasks. The key idea is to perform nonnegative matrix factorization on the desirability matrix Z to uncover the task basis.\n\nThe paper does a good job in illustrating step by step how the proposed algorithms work in simple problems. In my opinion, however, the paper falls short on two particular aspects that needs further development:\n\n(1) As far as I can tell, in all the experiments the matrix Z is computed from the MDP specification. If we adopt the proposed algorithm in an actual RL setting, however, we will need to estimate Z from data since the MDP specification is not available. I would like to see a detailed discussion on how this matrix can be estimated and also see some RL experiment results. \n\n(2) If I understand correctly, the row dimension of Z is equal to the size of the state space, so the algorithm can only be applied to tabular problem as-is. I think it is important to come up with variants of the algorithm that can scale to large state spaces.\n\nIn addition, I would encourage the authors to discuss connection to Machado et al. Despite the very different theoretical foundations, both papers deal with subtask discovery in HRL and appeal to matrix factorization techniques. I would also like to point out that this other paper is in a more complete form as it clears the issues (1) and (2) I raised above. I believe the current paper should also make further development in these two aspects  before it is published.\n\nMinor problems:\n- Pg 2, \"... can simply be taken as the resulting Markov chain under a uniformly random policy\". This statement seems problematic. The LMDP framework requires that the agent can choose any next-state distribution that has finite KL divergence from the passive dynamics, while in a standard MDP, the possible next-state distribution is always a convex combination of the transition distribution of different actions.\n\nReferences\nMachado et al. ICML 2017. A Laplacian Framework for Option Discovery in Reinforcement Learning.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": " Extends previous work that considered multitask learning in RL and proposed a hierarchical learner based on concurrent execution of many actions in parallel. Current work proposes an autonomous way to build the hierarchy and discover subtasks. ",
            "rating": "7: Good paper, accept",
            "review": "The present paper extends a previous work by Saxe et al (2017) that considered multitask learning in RL and proposed a hierarchical learner based on concurrent execution of many actions in parallel. That framework made heavy use of the framework of linearly solvable Markov decision process (LMDP) proposed by Todorov, which allows for closed form solutions of the control due to the linearity of the Bellman optimality equations. The simple form of the solutions allow them to be composed naturally, and to form deep hierarchies through iteration. The framework is restricted to domains where the transitions are fixed but the rewards may change between tasks.  A key role is played in the formalism by the so-called ‘free dynamics’ that serves to regularize the action selected. \n\nThe present paper goes beyond Saxe et al. in several ways. First, it renders the process of deep hierarchy formation automatic, by letting the algorithm determine the new passive dynamics at each stage, as well as the subtasks themselves. The process of subtask discovery is done via non-negative matrix factorization, whereby the matrix of desirability functions, determined by the solution of the LMDPs with exponentiated reward. Since the matrix is non-negative, the authors propose a non-negative factorization into a product of non-negative low rank matrices that capture its structure at a more abstract level of detail. A family of optimization criteria for this process are suggested, based on a subclass if Bregman divergences. Interestingly, the subtasks discovered correspond to distributions over states, rather than single states as in many previous approaches. The authors present several demonstrations of the intuitive decomposition achieved. A nice feature of the present framework is that a fully autonomous scheme (given some assumed parameter values) is demonstrated for constructing the full hierarchical decomposition.  \n\nI found this to be an interesting approach to hierarchical multitask learning, augmenting a previous approach with several steps leading to increased autonomy, an essential agent for any learning agent. Both the intuition behind the construction and the application to test problem reveal novel insight. The utilization on the analytic framework of LMDP facilitates understanding and efficient algorithms. \n\nI would appreciate the authors’ clarification of several issues. First, the LMDP does not seem to be completely general, so I would appreciate a description of the limitations of this framework. The description of the elbow-joint behavior around eq. (4) was not clear to me, please expand. The authors do not state any direct or indirect extensions – please do so. Please specify how many free parameters the algorithm requires, and what is a reasonable way to select them. Finally, it would be instructive to understand where the algorithm may fail.   \n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel model to discover subtasks in probabilistic planning (LMDP).",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a formulation for discovering subtasks in Linearly-solvable MDPs. The idea is to decompose the optimal value function into a fixed set of sub value functions (each corresponding to a subtask) in a way that they best approximate (e.g. in a KL-divergence sense) the original value.\n\nAutomatically discovering hierarchies in planning/RL problems is an important problem that may provide important benefits especially in multi-task environments. In that sense, this paper makes a reasonable contribution to that goal for multitask LMDPs. The simulations also show that the discovered hierarchy can be interpreted. Although the contribution is a methodological one, from an empirical standpoint, it may be interesting to provide further evidence of the benefits of the proposed approach. Overall, it would also be useful to provide a short paragraph about similarities to the literature on discovering hierarchies in MDPs.  \n\nA few other comments and questions: \n\n- This may be a fairly naive question but given your text I'm under the impression that the goal in LMDPs is to find z(s) for all states (and Z in the multitask formulation). Then, your formulation for discovery subtasks seems to assume that Z is given. Does that mean that the LMDPs must first be solved and only then can subtasks be discovered? (The first sentence in the introduction seems to imply that there's hope of faster learning by doing hierarchical decomposition).\n\n- You motivate your approach (Section 3) using a max-variance criterion (as in PCA), yet your formulation actually uses the KL-divergence. Are these equivalent objectives in this case?\n\n\nOther (minor) comments: \n\n- In Section it would be good to define V(s) as well as 'i' in q_i (it's easy to mistake it for an index). ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}