{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper presents the Stein gradient estimator, a kernelized direct estimate of the score function for implicitly defined models. The authors demonstrate the estimator for GANs, meta-learning for approx. inference in Bayesian NNs, and approximating gradient-free MCMC. The reviewers found the method interesting and principled.  The GAN experiments are somewhat toy-ish as far as I am concerned, so I'd encourage the authors to try out larger-scale models if possible, but otherwise this should be an interesting addition to ICLR.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Interesting idea",
            "rating": "7: Good paper, accept",
            "review": "Post rebuttal phase (see below for original comments)\n================================================================================\nI thank the authors for revising the manuscript. The methods makes sense now, and I think its quite interesting. While I do have some concerns (e.g. choice of eta, batching may not produce a consistent gradient estimator etc.), I  think the paper should be accepted. I have revised my score accordingly.\n\nThat said, the presentation (esp in Section 2) needs to be improved. The main problem is that many symbols have been used without being defined. e.g. phi, q_phi, \\pi,  and a few more. While the authors might assume that this is obvious, it can be tricky to a reader - esp. someone like me who is not familiar with GANs. In addition, the derivation of the estimator in Section 3 was also sloppy. There are neater ways to derive this using RKHS theory without doing this on a d' dimensional space.\n\nRevised summary: The authors present a method for estimating the gradient of some training objective for generative models used to sample data, such as GANs. The idea is that this can be used in a training procedure. The idea is based off the Stein's identity, for which the authors propose a kernelized solution. The key insight comes from rewriting the variational lower bound so that we are left with having to compute the gradients w.r.t a random variable and then applying Stein's identity. The authors present applications in Bayesian NNs and GANs.\n\n\nSummary\n================================================================\nThe authors present a method for estimating the gradient of some training objective\nfor generative models used to sample data, such as GANs. The idea is that this can be\nused in a training procedure. The idea is based off the Stein's identity, for which the\nauthors propose a kernelized solution. The authors present applications in Bayesian NNs\nand GANs.\n\n\n\n\nDetailed Reviews\n================================================================\n\nMy main concern is what I raised via a comment, for which I have not received a response\nas yet. It seems that you want the gradients w.r.t the parameters phi in (3). But the\nline immediately after claims that you need the gradients w.r.t the domain of a random\nvariable z and the subsequent sections focus on the gradients of the log density with\nrespect to the domain. I am not quite following the connection here.\n\nAlso, it doesn't help that many of the symbols on page 2 which elucidates the set up\nhave not been defined. What are the quantities phi, q, q_phi, epsilon, and pi?\n\nPresentation\n- Bottom row in Figure 1 needs to be labeled. I eventually figured that the colors\n  correspond to the figures above, but a reader is easily confused.\n- As someone who is not familiar with BNNs, I found the description in Section 4.2\n  inadequate.\n \nSome practical concerns:\n- The fact that we need to construct a kernel matrix is concerning. Have you tried\n  batch verstions of these estimator which update the gradients with a few data points?\n- How is the parameter \\eta chosen in practice? Can you comment on the values that you\n  used and how it compared to the eigenvalues of the kernel matrix?\n\nMinor\n- What is the purpose behind sections 3.1 and 3.2? They don't seem pertinent to the rest\n  of the exposition. Same goes for section 3.5? I don't see the authors using the\n  gradient estimators for out-of-sample points?\n\nI am giving an indifferent score mostly because I did not follow most of the details.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting paper with novel and significant contribution!",
            "rating": "7: Good paper, accept",
            "review": "In this paper, the authors proposed the Stein gradient estimator, which directly estimates the score function of the implicit distribution. Direct estimation of gradient is crucial in the context of GAN because it could potentially lead to more accurate updates. Motivated by the Stein’s identity, the authors proposed to estimate the gradient term by replacing expectation with the empirical counterpart and then turn the resulting formulation into a regularized regression problem. They also showed that the traditional score matching estimator (Hyvarinen 2005) can be obtained as a special case of their estimator. Moreover, they also showed that their estimator can be obtained by minimizing the kernelized Stein discrepancy (KSD) which has been used in goodness-of-fit test. In the experiments, the proposed method is evaluated on few tasks including Hamiltonian flow with approximate gradients, meta-learning of approximate posterior samplers, and GANs using entropy regularization. \n\nThe novelty of this work consists of an approach based on score matching and Stein’s identity to estimate the gradient directly and the empirical results of the proposed method on meta-learning for approximate inference and entropy regularized GANs. The proposed method is new and technically sound. The authors also demonstrated through several experiments that the proposed technique can be applied in a wide range of applications.\n\nNevertheless, I suspect that the drawback of this method compared to existing ones is computational cost. If it takes significantly longer to compute the gradient using proposed estimator compared to existing methods, the gain in terms of accuracy is questionable. By spending the same amount of time, we may obtain an equally accurate estimate using other methods. For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case. Hence, the comparison in terms of computational cost should also be included either in the text or in the experiment section.\n\nWhile the proposed Stein gradient estimator is technically interesting, the experimental results do not seem to evident that it significantly outperforms existing techniques. In Section 4.2, the authors only consider four datasets (out of six UCI datasets). Also, in Section 4.3, it is not clear what the point of this experiment is: whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators.\n\nSome comments:\n\n- Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.\n- Stein gradient estimator vs KDE: What if the kernel is not translation invariant? \n- In Section 4.3, why did you consider the entropy regularizer? How does it help answer the main hypothesis of this paper?\n- The experiments in Section 4.3 seems to be a bit out of context.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "an interesting paper",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.\n\nThis is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. \n\n- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term \"Stein identity\" is appropriate to express the equation (8). \n- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. ",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}