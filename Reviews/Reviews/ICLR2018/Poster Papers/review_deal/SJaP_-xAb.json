{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "In this paper the authors show how to allow deep neural network training on logged contextual bandit feedback. The newly introduced framework comprises a new kind of output layer and an associated training procedure. This is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible. ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "The paper proposes a counterfactual risk minimization objective to perform learning from bandit feedback using a deep neural network.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "In this paper, the authors propose a new output layer for deep networks allowing training on logged contextual bandit feedback. They propose a counterfactual risk minimization objective which makes the training procedure different from the one that uses conventional cross-entropy in supervised learning. The authors claim that this is the first attempt where Batch Learning from Bandit Feedback (BLBF) is performed using deep learning. \n\nThe authors demonstrate the derivation steps of their theoretical model and present 2 empirical results. The first result is on visual object classification using CIFAR-10.  To simulate logged bandit feedback for CIFAR-10, the authors perform the standard supervised to bandit conversion using a hand-coded logging policy that achieves 49 % error rate on training data. Using the logged bandit feedback data for the proposed bandit model, the authors are able to achieve substantial improvement (13% error rate) and given more bandit feedback, the model is able to compete with the same architecture trained on the full-information using cross entropy (achieving 8.2% error rate).\n\nThe second result is a real-world verification of the proposed approach (as the logged feedback are real and not synthesized using a conversion approach) which is an advertisement placing task from Criteo’s display advertising system (Lefortier et al., 2016). The task consists of choosing the product to display in the ad in order to maximize the number of clicks. The proposed deep learning approach improve substantially on state-of-the-art. The authors show empirically that the proposed approach is able to have substantial gain compared to other methods. The analysis is done by performing ablation studies on context features which are not effective on linear models.\n\nThe paper is well written. The authors make sure to give the general idea of their approach and its motivation, detail the related work and position their proposed approach with respect to it. The authors also propose a new interpretation of the baseline in “REINFORCE-like” methods where it makes the counterfactual objective equivariant (besides the variance reduction role). The authors explain their choice of using importance sampling to estimate the counterfactual risk. They also detail the arguments for using the SNIPS estimator, the mathematical derivation for the training algorithm and finally present the empirical results.\n\nBesides the fact that empirical results are impressive, the presented approach allows to train use deep nets when manually labelling full-information feedback is not viable.\n\nIn section 4.2, the neural network that has been used is a 2 layer network with tanh activation. It is clear that the intention of the authors is to show that even with a simple neural architecture, the gain is substantial compared to the baseline method, which is indeed the right approach to go with. Still, it would have been of a great benefit if they had used a deeper architecture using ReLU-based activations.\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A decent paper with some issues",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes a new output layer in neural networks, which allows them to use logged contextual bandit feedback for training. The paper is well written and well structured. \n\n\nGeneral feedback:\n\nI would say the problem addressed concerns stochastic learning in general, not just SGD for training neural nets. And it's not a \"new output layer\", but just a softmax output layer (Eq. 1) with an IPS+baseline training objective (Eq. 16).\n\n\nOthers:\n\n- The baseline in REINFORCE (Williams'92), which is equivalent to introduced Lagrange multiplier, is well known and well defined as control variate in Monte Carlo simulation, certainly not an \"ad-hoc heuristic\" as claimed in the paper [see Greensmith et al. (2004). Variance Reduction for Gradient Estimates in Reinforcement Learning, JMLR 5.]\n\n- Bandit to supervised conversion: please add a supervised baseline system trained just on instances with top feedbacks -- this should be a much more interesting and relevant strong baseline. There are multiple indications that this bandit-to-supervised baseline is hard to outperform in a number of important applications.\n\n- The final objective IPS^lambda is identical to IPS with a translated loss and thus re-introduces problems of IPS in exactly the same form that the article claims to address, namely:\n    * the estimate is not bounded by the range of delta\n    * the importance sampling ratios can be large; samples with high such ratios lead to larger gradients thus dominating the updates. The control variate of the SNIPS objective can be seen as defining a probability distribution over the log, thus ensuring that for each sample that sample’s delta is multiplied by a value in [0,1] and not by a large importance sampling ratio.\n    * IPS^lambda introduces a grid search which takes more time and the best value for lambda might not even be tested. How do you deal with it?\n\n- As author note, IPS^lambda is very similar to an RL-baseline, so results of using IPS with it should be reported as well:\n    In more detail, Note:\n    1. IPS for losses<0 and risk minimization: raise the probability of every sample in the log irrespective of the loss itself\n    2. IPS for losses>0 and risk minimization: lower the same probability\n    3. IPS^lambda: by the translation of the loss, it divides the log into 2 groups: a group whose probabilities will be lowered and a group whose probabilities will be raised (and a third group for delta=lambda but the objective will be agnostic to these)\n    4. IPS with a baseline would do something similar but changes over time, which means the above groups are not fixed and might work better. Furthermore, there is no hyperparameter/grid search required for the simple RL-baseline\n    -> results of using IPS with the RL-baseline should be reported for the BanditNet rows in Table 1 and in CIFAR-10 experiments.\n\n- What is the feedback in the CIFAR-10 experiments? Assuming it's from [0..1], and given the tested range of lambdas, you should run into the same problems with IPS and its degenerate solutions for lambdas >=1.0. In general, how are your methods behaving for lambda* (corresponding to S*) such that makes all difference (delta_i - lambda*) positive or negative?\n\n- The claim of Theorem 2 in appendix B does not follow from its proof: what is proven is that the value of S(w) lies in an interval [1-e..1+e] with a certain probability for all w. It says nothing about a solution of an optimization problem of the form f(w)/S(w) or its constrained version. Actually, the proof never makes any connection to optimization.\n\n- What the appendix C basically claims is that it's not possible to get an unbiased estimate of a gradient for a certain class of non-convex ratios with a finite-sum structure. This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013. Variance Reduction for Stochastic Gradient Optimization, NIPS. On the other hand, there seem to be no need to prove such a claim in the first claim, since the difficulty of performing self-normalized IPS on GPU should be evident, if one remembers that the normalization should run over the whole logged dataset (while only the current mini-batch is accessible to the GPU).",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straighforward extension of existing techniques. Nevertheless, this is important work and I can see it being used by others.",
            "rating": "7: Good paper, accept",
            "review": "Learning better policies from logged bandit feedback is a very important problem, with wide applications in internet, e-commerce and anywhere it is possible to incorporate controlled exploration. The authors study the problem of learning the best policy from logged bandit data. While this is not a brand new problem, the important and relevant contribution that the authors make is to do this using policies that can be learnt via neural networks. The authors are motivated by two main applications: (i) multi-class classification problems with bandit feedback (ii) ad placements problem in the contextual bandit setting. \n\nThe main contributions of the authors is to design an output layer that allows training on logged bandit feedback data. Traditionally in the full feedback setting (setting where one gets to see the actual label and not just if our prediction is correct or incorrect) one uses cross-entropy loss function to optimize the parameters of a deep neural network. This does not work in a bandit setting, and previous work has developed various methods such as inverse-propensity scoring, self-normalized inverse propensity scoring, doubly robust estimators to handle the bandit setting. The authors in this paper work with self-normalized inverse propensity scoring as the technique to deal with bandit feedback data. the self normalized inverse propensity estimator (SNIPS) that the authors use is not a new estimator and has been previously studied in the work of Adith Swaminathan and co-authors. However, this estimator being a ratio is not an easy optimization problem to work with. The authors use a fairly standard reduction of converting ratio problems to a series of constrained optimization problems. This conversion of ratio problems to a series of constrained optimization problems is a standard textbook problem, and therefore not new. But, i like the authors handling of the constrained optimization problems via the use of Lagrangian constraints. It would have been great if the authors connected this to the REINFORCE algorithm of Williams. Unfortunately, the authors do not do a great job in establishing this connection, and I hope they do this in the full version of the paper.  The experimental results are fairly convincing and i really do not have any major comments. Here are my \"minor\" comments.\n\n1. It would be great if the authors can establish connections to the REINFORCE algorithm in a more elaborate manner. It would be really instructive to the reader.\n\n2.  On page 6,  the authors talk about lowering the learning rate and the learning rate schedule. I am guessing this is because of the intrinsic high variance of the problem. It would be great if the authors can explain in more detail why they did so.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}