{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper uses known methods for learning a differentially private models and applies it to the task of learning a language model, and find they are able to maintain accuracy results on large datasets. Reviewers found the method convincing and original saying it was \"interesting and very important to the machine learning ... community\", and that in terms of results it was a \"very strong empirical paper, with experiments comparable to industrial scale\". There were some complaints as to the clarity of the work, with requests for more clear explanations of the methods used.\n\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "I like the experimental strength of the paper, but I have mild concerns about the new algorithmic ideas in the paper.",
            "rating": "7: Good paper, accept",
            "review": "Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy. The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example. The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework.\n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale. The paper uses the right composition tools like moments accountant to get strong privacy guarantees. The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm. Both these contributions are important in the effectiveness of the overall algorithm.\n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models. I did not find strong algorithmic ideas in the paper. I found the paper to be lacking in that respect.  ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " Nice extensions to FederatedAveraging, with strong experimental setup.",
            "rating": "7: Good paper, accept",
            "review": "\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee. The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent\n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point.\n\nCorrectness: The paper is technically correct.\n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline.\n2. Clipping is supposed to help with the exploding gradients problem. Do you have an idea why a low threshold hurts the performances? Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?\n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee.\n2. Strong experimental setup that analyses in details the proposed extensions.\n3. Experiments performed on public datasets.\n\nCons\n-------\n\nNone\n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \"is given in Figure 1\" -> \"is given in Algorithm 1\"\n\nNote\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Nice work",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.\n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility.\n\nThe presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}