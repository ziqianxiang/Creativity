{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers agree that the method is original and mostly well communicated, but have some doubts about the significance of the work.   ",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Novel idea with seemingly a large amount of moving parts and a considerable amount of hyper parameters.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper presents GANITE, a Generative Adversarial Network (GAN) approach for estimating Individualized Treatment Effects (ITE). This is achieved by utilising a GAN to impute the `missing` counterfactuals, i.e. the  outcomes of the treatments that were not observed in the training (i.e. factual) sample, and then using another GAN to estimate the ITE based on this `complete` dataset. The authors then proceed in combining the two GAN objectives with extra supervised losses to better account for the observed data; the GAN loss for the `G` network has an extra term for the `G` network to better predict the factual outcome `y_f` (which should be easy to do given the fact that y_f is an input to the network) and the GAN loss for the `I` network has an extra term w.r.t. the corresponding performance metric used for evaluation, i.e. PEHE for binary treatment and MSE for multiple treatments. This model is then evaluated on extensive experiments.\n\nThe paper is reasonably well-written with clear background and diagrams for the overall architecture. The idea is novel and seems to be relatively effective in practice although I do believe that it has a lot of moving parts and introduces a considerable amount of hyperameters (which generally are problematic to tune in causal inference tasks). Other than that, I have the following questions and remarks:\n- I might have misunderstood the motivation but the GAN objective for the `G` network is a bit weird; why is it a good idea to push the counterfactual outcomes close to the factual outcomes (which is what the GAN objective is aiming for)? Intuitively, I would expect that different treatments should have different outcomes and the distribution of the factual and counterfactual `y` should differ.\n- According to which metric did you perform hyper-parameter optimization on all of the experiments? \n- From the first toy experiment that highlights the importance of each of the losses it seems that the addition of the supervised loss greatly boosts the performance, compared to just using the GAN objectives. What was the relative weighting on those losses in general? \n- From what I understand the `I` network is necessary for out-of-sample predictions where you don’t have the treatment assignment, but for within sample prediction you can also use the `G` network. What is the performance gap between the `I` and `G` networks on the within-sample set? Furthermore, have you experimented with constructing `G` in a way that can represent `I` by just zeroing the contribution of `y_f` and `t`? In this way you can tie the parameters and avoid the two-step process (since `G` and `I` represent similar things).\n- For figure 2 what was the hyper parameters for CFR? CFR includes a specific knob to account for the larger mismatches between treated and control distributions. Did you do hyper-parameter tuning for all of the methods in this task?\n- I would also suggest to not use “between” when referring to the KL-divergence as it is not a symmetric quantity.\n\nAlso it should be pointed out that for IHDP the standard evaluation protocol is 1000 replications (rather than 100) so there might be some discrepancy on the scores due to that.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good empirical results, the explanation of the methods can be improved",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Summary:\nThis paper proposes to estimate the individual treatment effects (ITE) through\ntraining two separate conditional generative adversarial networks (GANs). \n\nFirst, a counterfactual GAN is trained to estimate the conditional distribution \nof the potential outcome vector, which consists of factual outcome and all \nother counterfactual outcomes, given 1) the feature vector, 2) the treatment \nvariable, and 3) the factual outcome. After training the counterfactual GAN, \nthe complete dataset containing the observed potential outcome vector can be \ngenerated by sampling from its generator.\n\nSecond, a ITE GAN is trained to estimate the conditional distribution of the\npotential outcome vector given only the feature vector. In this way, for any\ntest sample, its potential outcomes can be estimated using the generator of the\ntrained ITE GAN. Given its pontential outcome vector, its ITE can be estimated\nas well.\n\nExperimental results on the synthetic data shows the proposed approach, called\nGANITE, is more robust to the existence of selection bias, which is defined as\nthe mismatch between the treated and controlled distributions, compared to\nits competing alternatives. Experiments on three real world datasets show\nGANITE achieves the best performance on two datasets, including Twins and Jobs.\nIt does not perform very well on the IHDP dataset. The authors also run\nexperiments on the Twins dataset to show the proposed approach can estimate the\nmultiple treatment effects with better performance.\n\nComments\n1) This paper is well written. The background and related works are well\norganized. \n\n2) To the best of my knowledge, this is the first work that applies \nGAN to ITE estimation.\n\n3) Experiments on the synthetic data and the real-world data demonstrate the\nadvantage of the proposed approach.\n\n4) The authors directly present the formulation without providing sufficient \nmotivations. Could the authors provide more details or intuitions on why GAN \nwould improve the performance of ITE estimation compared to approaches that\nlearn representations to minimize the distance between the distributions of\ndifferent treatment groups, such as CFR_WASS?\n\n5) As is pointed out by the authors, the proposed approach does not perform\nwell when the dataset is small, such as the IHDP data. However, in practice, a\nlot of real-world datasets might have small sample size, such as the LaLonde\ndataset. Did the authors plan to extend the model to handle those small-sized \ndata sets without completely changing the model.\n\n6) When training the ITE GAN, the objective is to learn the conditional\ndistribution of the potential outcome vector given the feature vector. Did the\nauthors try the option of replacing ITE GAN with multi-task regression? Will\nthe performance become worse using multi-task regression?  I think this \ncomparison would be a sanity check on the utility of using GAN instead of \nregression models for ITE estimation.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Re: GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper introduces a generative adversarial network (GAN) for estimating individualized treatment effects (ITEs) by (1) learning a generator that tries to fool a discriminator with feature, treatment, potential outcome- vectors, and (2) by learning a GAN for the treatment effect. In my view, the counterfactual component is the interesting and original component, and the results show that the ITE GAN component further improves performance (marginally but not significant). The analysis is conducted on semi-synthetic data sets created to match real data distributions with synthetically introduced selection bias and conducts extensive experimentation. While the results show worse performance compared to existing literature in the experiment with small data sizes, the work does show improvements in larger data sets. However, Table 5 in the appendix suggests these results are not significant when considering average treatment effect estimation (eATE and eATE).\n\nQuality: good. Clarity: acceptable. Originality: original. Significance: marginal.\n\nThe ITE GAN does not significantly outperform the counterfactual GAN alone (in the S and GAN loss regime), and in my understanding the counterfactual GAN is the particularly innovative component here, i.e., can the algorithm effectively enough generate indistinguishable counterfactual outcomes from x and noise. I wonder if the paper should focus on this in isolation to better understand and characterize this contribution.\n\nWhat is the significance of bold in the tables? I'd remove it if it's just to highlight which method is yours.\n\nDiscussion section should be called \"Conclusion\" and a space permitting a Discussion section should be written.\nE.g. exploration of the form of the loss when k>2, or when k is exponential e.g. a {0,1}^c hypercube for c potentially related treatment options in an order set. \nE.g. implications of underperformance in settings with small data sets. We have lots of large data sets where ground truth is unknown, and relatively more small data sets where we can identify ground truth at some cost.\nE.g. discussion of Table 2 (ITEs) where GANITE is outperforming the methods (at least on large data sets) and Table 5 (ATEs) which does not show the same result is warranted. Why might we expect this to the case?",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}