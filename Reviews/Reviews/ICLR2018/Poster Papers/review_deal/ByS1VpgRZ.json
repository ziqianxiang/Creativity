{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log-linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super-resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state-of-the-art (AC-GAN).\n",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Review of cGAN with projection discriminator",
            "rating": "6: Marginally above acceptance threshold",
            "review": "\nI thank the authors for the thoughtful response and updated manuscript. After reading through both, my review score remains unchanged.\n\n=================\n\nThe authors describe a new variant of a generative adversarial network (GAN) for generating images. This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models.\n\nMajor comments:\n1) Spatial resolution. What spatial resolution is the model generating images at? The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling. It would be great to see the authors address this issue in a serious manner.\n\n2) FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large). Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard'.\n\n3) Conditional batch normalization.  I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator. The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629\n). If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful.\n\nMinor comments:\n- I believe you have the incorrect reference for conditional batch normalization on Page 5.\nA Learned Representation For Artistic Style\nDumoulin, Shlens and Kudlur (2017)\nhttps://arxiv.org/abs/1610.07629\n\n- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images.\n\n- Please add citations for Figures 1a-1b. Do these correspond with some known models?\n\nDepending on how the authors respond to the reviews, I would consider upgrading the score of my review.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "simple, interesting GAN modification; great results",
            "rating": "7: Good paper, accept",
            "review": "The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results.\n\n+ Quality:\n- Simple method leading to great results on ImageNet!\n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes.\n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs.\n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID)\n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs.\n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger.\n\nOverall, I really enjoy reading this paper and recommend for acceptance!\n\n\n\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An unusually thorough GAN paper.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.  It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.\n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward.\n\nThe sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}