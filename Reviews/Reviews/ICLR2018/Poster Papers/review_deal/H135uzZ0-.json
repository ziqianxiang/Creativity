{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "Mixed precision application of CNNs is being explored for e.g. hardware implementations of networks trained at full precision.  Mixed precision at training time is less common.  This submission primarily concerns itself with the practical implementation details of training with mixed precision, and focuses primarily on representation of mixed precision floating point and algorithmic issues for learning.  In the end the support for the approach is primarily empirical, with the mixed precision approach giving a factor of two speedup with half the precision, while accuracies remain effectively statistically tied on the ImageNet 1k database.  Table 1 should avoid the use of bold as there is likely no statistical significance.\n\nThe reviewers appreciated the paper. The proposed approach is sensible, and appears correct.",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Mixed Precision Training",
            "rating": "7: Good paper, accept",
            "review": "This paper is about low-precision training for ConvNets. It proposed a \"dynamic fixed point\" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format. The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K. According to the paper, this is the first time such kind of performance are demonstrated for limited precision training.\n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance.\n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New setup for CNN with half precision that gets 2X speedup on training",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training. The work is clearly presented and the evaluations seem convincing. The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation.  I'm not an expert in this area but the contribution seems relevant to me, and enough for being published.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "SOTA with reduced precision on large CNNs",
            "rating": "7: Good paper, accept",
            "review": "This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations. These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10. However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported. In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16. \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs. However, I was hoping to see a direct comparison between FP16 and INT16.  \n\nThe paper is written clearly and the English is fine.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}