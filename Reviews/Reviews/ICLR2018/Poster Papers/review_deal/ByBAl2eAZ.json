{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This paper proposes adding noise to the parameters of a deep network when taking actions in deep reinforcement learning to encourage exploration.  The method is simple but the authors demonstrate its effectiveness through thorough empirical analysis across a variety of reinforcement learning tasks (i.e. DQN, DDPG, and TRPO).  Overall the paper is clear, well written and the reviewers enjoyed it.  However, a common trend among the reviews was that the authors overstated their claims and contributions.  The reviewers called out some statements in particular (e.g. the discussion of ES and RL) which the authors appear to have addressed when comparing their revisions (thank you).  Overall, a clear, well written paper conveying a simple but effective idea for exploration that often works across a variety of RL tasks.  The authors also released open-source code along with their paper for reproducibility (as evidenced by the reproducibility study below), which is appreciated.\n\nPros:\n- Clear and well written\n- Thorough experiments across deep RL domains\n- A simple strategy for exploration that is effective empirically\n\nCons:\n- Not a panacea for exploration (although nothing really is)\n- Claims are somewhat overstated\n- Lacks a strong justification for the method other than that it is empirically effective and intuitive",
        "decision": "Accept (Poster)"
    },
    "Reviews": [
        {
            "title": "Nice exploration of parameter noise",
            "rating": "7: Good paper, accept",
            "review": "In recent years there have been many notable successes in deep reinforcement learning. However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem. For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration. This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets. This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks. By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space).\n\nThis work is well-written and cites previous work appropriately. Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms. The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards.\n\nOne empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3]).\n\nAs with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1]. It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication. As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah.\n\nMinor points:\n- The definition of a stochastic policy (section 2) is unusual (it is defined as an unnormalized distribution). Usually it would be defined as $\\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$\n\n- This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy. Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target.\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016). Combining policy gradient and Q-learning.\n\n[3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015). Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Random exploration at the policy level, rather than the action level - a good paper, but needs to be a bit more careful with the hype.",
            "rating": "7: Good paper, accept",
            "review": "This paper proposes a method for parameter space noise in exploration.\nRather than the \"baseline\" epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy.\nIn some domains this can be a much better approach and this is supported by experimentation.\n\nThere are several things to like about the paper:\n- Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better.\n- The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper.\n- This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.\n- The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up.\n- I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances).\n\nHowever, there are also a few things to be cautious of... and some of them serious:\n- At many points in the paper the claims are quite overstated. Parameter noise on the policy won't necessarily get you efficient exploration... and in some cases it can even be *worse* than epsilon-greedy... if you just read this paper you might think that this was a truly general \"statistically efficient\" method for exploration (in the style of UCRL or even E^3/Rmax etc).\n- For instance, the example in 4.2 only works because the optimal solution is to go \"right\" in every timestep... if you had the network parameterized in a different way (or the actions left/right were relabelled) then this parameter noise approach would *not* work... By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment. I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of \"deep exploration\" and you should be clear that your parameter noise does *not* address this issue.\n- That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration. Essentially you perform a local exploration rule in parameter space... and sometimes this is great - but you should be careful to distinguish this type of method from other approaches. This must be mentioned in section 4.2 \"does parameter space noise explore efficiently\" because the answer you seem to imply is \"yes\" ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D\n- The demarcation of \"RL\" and \"evolutionary strategies\" suggests a pretty poor understanding of the literature and associated concepts. I can't really support the conclusion \"RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually\". This sort of sentence is clearly wrong and for many separate reasons:\n    - Parameter noise exploration is not a separate/new thing from RL... it's even been around for ages! It feels like you are talking about DQN/A3C/(whatever algorithm got good scores in Atari last year) as \"RL\" and that's just really not a good way to think about it.\n    - Parameter noise exploration can be *extremely* bad relative to efficient exploration methods (see section 2.4.3 https://searchworks.stanford.edu/view/11891201)\n\n\nOverall, I like the paper, I like the algorithm and I think it is a valuable contribution.\nI think the value in this paper comes from a practical/simple way to do policy randomization in deep RL.\nIn some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy.\n\nHowever, I hope that you address some of the concerns I have raised in this review.\nYou shouldn't claim such a universal revolution to exploration / RL / evolution because I don't think that it's correct.\nFurther, I don't think that clarifying that this method is *not* universal/general really hurts the paper... you could just add a section in 4.2 pointing out that the \"chain\" example wouldn't work if you needed to do different actions at each timestep (this algorithm does *not* perform \"deep exploration\").\n\nI vote accept.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "How far is parameter noise really going to get us?",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.\n\nHowever, the results are weak. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well. E-greedy approaches will always struggle to choose the same random action repeatedly. Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns. Similarly for the continuous control with sparse rewards environments – if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning. However, for complex domains with sparse reward (e.g. Montezuma’s Revenge) parameter space noise is just not going to get you very far.\n\nOverall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don’t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn’t really gain you much more than policies that are biased towards particular actions. Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise.\n\nA non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}