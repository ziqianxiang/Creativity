{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers agree that the proposed architecture is novel. However, there are issues in terms of the motivation. It would be helpful in future drafts to strengthen the argument about why the architecture is expected to be better than others. Most importantly, the gains at this stage are still incremental. A larger improvement from the new architecture would motivate more researchers to focus on this architecture.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Good intuition and derivation. Consistent increase in performance.",
            "rating": "7: Good paper, accept",
            "review": "Summary\n- This paper draws analogy from numerical differential equation solvers and popular residual network-like deep learning architectures. It makes connection from ResNet, FractalNet, DenseNet, and RevNet to different numerical solvers such as forward and backward Euler and Runge-Kunta. In addition, inspired by the Linear Multi-step methods (LM), the authors propose a novel LM-ResNet architecture in which the next residual block takes a linear combination of the previous two residual blocks’ activations. They also propose a stochastic version of LM-ResNet that resembles Shake-shake regularization and stochastic depth. In both deterministic and stochastic cases, they show a positive improvement in classification accuracy on standard object classification benchmarks such as CIFAR-10/100 and ImageNet.\n\nPros\n- The intuition is good that connects differential equation and ResNet-like architecture, also explored in some of the related work.\n- Building upon the intuition, the author proposes a novel architecture based on a numerical ODE solver method.\n- Consistent improvement in accuracy is observed in both deterministic and stochastic cases.\n\nCons\n- The title is a little bit misleading. “Beyond Finite Layer Neural Networks” sounds like the paper proposes some infinite layer neural networks but the paper only studies finite number of layers.\n- One thing that needs to be clarified is that, if the network is not targeted at solving certain ODEs, then why is the intuition from ODE matters? The paper does not motivate readers in this perspective.\n- Given the widespread use of ResNet in the vision community, the incremental improvement of 1% on ImageNet is less likely to push vision research to switch to a completely different architecture. Therefore, the potential impact of the this paper to vision community is probably limited.\n\nConclusion\n- Based on the comments above, I think the paper is a good contribution which links ODE with Deep Networks and derivation is convincing. The proposed new architecture can be considered in future architecture designs. Although the increase in performance is small, I think it is good enough to accept.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful experiment results and not so clear insights",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Originality\n--------------\nThe paper takes forward the idea of correspondence between  ResNets and discretization of ODEs. Introducing multi-step discretization is novel.\n\nClarity\n---------\n1)  The paper does not define the meaning of u_n=f(u).\n2) The stochastic control problem (what is the role of controller, how is connected to the training procedure) is not defined\n\nQuality\n---------\nWhile the experiments are done in CIFAR-10 and 100,  ImageNet and improvements are reported, however, connection/insights to why the improvement is obtained is still missing. Thus the evidence is only partial, i.e., we still don't know why the connection between ODE and ResNet is helpful at all.\n\nSignificance\n-----------------\nStrength: LM architectures reduce the layers in some cases and achieve the same level of accuracy.\nWeakness: Agreed that LM methods are better approximations of the ODEs. Where do we gain? (a) It helps if we faithfully discretize the ODE. Why does (a) help? We don't have a clear answer; which takes back to the lack of what the underlying stochastic control problem is.\n",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": " [Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations]",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The authors proposed to bridge deep neural network design with numerical differential equations. They found that many effective networks can be interpreted as different numerical discretization of differential equations and provided a new perspective on the design of effective deep architectures. \n\nThis paper is interesting in general and it will be useful to design new and potentially more effective deep networks. Regarding the technical details, the reviewer has the following comments:\n\n- The authors draw a relatively comprehensive connection between the architecture of popular deep networks and discretization schemes of ODEs. Is it possible to show stability of the architecture of deep networks based on their associated ODEs? Related to this, can we choose step size or the number of layers to guarantee numerical stability?\n\n- It is very interesting to consider networks as stochastic dynamic systems. Are there any limitations of this interpretation or discrepancy due to the weak approximation? ",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "further comparisons & inconsistent reported baselines",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The authors cast some of the most recent CNN designs as approximate solutions to discretized ODEs. On that basis, they propose a new type of block architecture which they evaluate on CIFAR and ImageNet. They show small gains when applying their design on the ResNet architectures. They also draw a comparison between a stochastic learning process and approximation to stochastic dynamic systems.\n\nPros:\n(+) The paper presents a way to connect NN design with principled approximations to systems\n(+) Experiments are shown on compelling benchmarks such as ImageNet\nCons:\n(-) It is not clear why the proposed approach is superior to the other designs\n(-) Gains are relatively small and at a price of a more complicated design\n(-) Incosistent baselines reported\n\nWhile the effort of presenting recent CNN designs as plausible approximations to ODEs, the paper does not try to draw connections among the different approaches, compare them or prove the limits of their related approximations. In addition, it is unclear from the paper how the proposed approach (LM-architecture) compares to the recent works, what are the benefits and gains from casting is as a direct relative to the multi-step scheme in numerical ODEs. How do the different approximations relate in terms of convergence rates, error bounds etc.?\n\nExperimentwise, the authors show some gains on CIFAR 10/100, or 0.5% (see ResNeXt Table1), while also introducing slightly more parameters. On ImageNet1k, comparisons to ResNeXt are missing from Table3, while the comparison with the ResNets show gains in the order of 1% for top-1 accuracy. \n\nTable3 is concerning. With a single crop testing scheme, ResNet101 is yielding top-1 error of 22% and top-5 error of 6% (see Table 5 of Xie et al, 2017 (aka ResNeXt)). However, the authors report 23.6% and 7.1% respectively for their ResNet101. The performance stated by the authors of ResNe(X)t weakens the empirical results of LM-architecture.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}