{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "In general, the reviewers and myself find this work of some interest, though potentially somewhat incremental in terms of technical novelty compared to the work for Makhzani et al. Another bothersome aspect is the question of evaluation and understanding how well the model actually does; I am not convinced that the interpolation experiments are actually giving us a lot of insights. One interesting ablation experiment (suggested privately by one of the reviewers) would be to try AAE with Wasserstein and without a learned generator -- this would disambiguate which aspects of the proposed method bring most of the benefit. As it stands, the submission is just shy of the acceptance bar, but due to its interesting results in the natural language domain, I do recommend it being presented at the workshop track.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Little methodological novelty but an interesting set of tasks considered, empirical evaluation could still be better.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "I was asked to contribute this review rather late in the process, and in order\nto remain unbiased I avoided reading other reviews. I apologize if some of\nthese comments have already been addressed in replies to other reviewers.\n\nThis paper proposes a regularization strategy for autoencoders that is very\nsimilar to the adversarial autoencoder of Makhzani et al. The main difference\nappears to be that rather than using the classic GAN loss to shape the\naggregate posterior of an autoencoder to match a chosen, fixed distribution,\nthey instead employ a Wasserstein GAN loss (and associated weight magnitude\nconstraint, presumably enforced with projected gradient descent) on a system\nwhere the matched distribution is instead learned via a parameterized sampler\n(\"generator\" in the GAN lingo). Gradient steps that optimize the encoder,\ndecoder and generator are interleaved. The authors apply an extension of this\nmethod to topic and sentiment transfer and show moderately good latent space\ninterpolations between generated sentences.\n\nThe difference from the original AAE is rather small and straightforward, making the\nnovelty mainly in the choice of task, focusing on discrete vectors and sequences.\n\nThe exposition leaves ample room for improvement. For one thing, there is the\nirksome and repeated use of \"discrete structure\" when discrete *sequences* are\nconsidered almost exclusively (with the exception of discretized MNIST digits).\nThe paper is also light on discussion of related work other than Makhzani et al\n-- the wealth of literature on combining autoencoders (or autoencoder-like\nstructures such as ALI/BiGAN) and GANs merits at least passing mention.\n\nThe empirical work is somewhat compelling, though I am not an expert in this\ntask domain. The annealed importance sampling technique of Wu et al (2017) for\nestimating bounds on a generator's log likelihood could be easily applied in\nthis setting and would give (for example, on binarized MNIST) a quantitative\nmeasurement of the degree of overfitting, and this would have been preferable\nthan inventing new heuristic measures. The \"Reverse PPL\" metric requires more\njustification, and it looks an awful lot like the long-since-discredited Parzen\nwindow density estimation technique used in the original GAN paper.\n\nHigh-level comments:\n\n- It's not clear why the optimization is done in 3 separate steps. Aside\nfrom the WGAN critic needing to be optimized for more steps, couldn't the\nremaining components be trained jointly, with a weighted sum of terms for the\nencoder?\n- In section 2, \"This [pre-training or co-training with maximum likelihood]\n  precludes there being a latent encoding of the sentence.\" It is not at all\n  clear to me why this would be the case.\n- \"One benefit of the ARAE framework is that it compresses the input to a\n  single code vector.\" This is true of any autoencoder.\n- It would be worth explaining, in a sentence, the approach in Shen et al for\n  those who are not familiar with it, seeing as it is used as a baseline.\n- We are told that the encoder's output is l2-normalized but the generator's\n  is not, instead output units of the generator are squashed with the tanh\n  activation. The motivation for this choice would be helpful. Shortly\n  thereafter we are told that the generator quickly learns to produce norm 1\n  outputs as evidence that it is matching the encoder's distribution, but this\n  is something that could have just as easily have been built-in, and is a\n  trivial sort of \"distribution matching\"\n- In general, tables that report averages would do well to report error bars as\n  well. In general some more nuanced statistical analysis of these results\n  would be worthwhile, especially where they concern human ratings.\n- The dataaset fractions chosen for the semi-supervised experience seem\n  completely arbitrary. Is this protocol derived from some other source?\n  Putting these in a table along with the results would improve readability. \n- Linear interpolation in latent space may not be the best choice here\n  seeing as e.g. for a Gaussian code the region near the origin has rather low\n  probability. Spherical interpolation as recommended by White (2016) may\n  improve qualitative results.\n- For the interpolation results you say \"we output the argmax\", what is meant?\n  Is beam search performed in the case of sequences?\n- Finally, a minor point: I will challenge the authors to justify their claim\n  that the learned generative model is \"useful\" (their word). Interpolating\n  between two sentences sampled from the prior is a neat parlour trick, but the\n  model as-is has little utility. Even some speculation on how this aspect\n  could be applied would be appreciated (admittedly, many GAN papers could use\n  some reflection of this sort).",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea; maybe helpful to present more intuition",
            "rating": "6: Marginally above acceptance threshold",
            "review": "the paper presents a way to encode discrete distributions which is a challenging problem. they propose to use a latent variable gan with one continuous encoding and one discrete encoding. \n\ntwo questions linger around re practices:\n1. gan is known to struggle with discriminating distributions with different supports. the problem also persists here as the gan is discriminating between a continuous and a discrete distribution.  it'll interesting to see how the proposed approach gets around this issue.\n\n2. the second question is related. it is unclear how the optimal distribution would look like with the latent variable gan. ideally, the discrete encoding be simply a discrete approximation of the continuous encoding. but optimization with two latent distributions and one discriminator can be hard. what we get in practice is pretty unclear. also how this could outperform classical discrete autoencoders is unclear. gan is an interesting idea to apply to solve many problems; it'll be helpful to get the intuition of which properties of gan solves the problem in this particular application to discrete autoencoders.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very nice paper, very clearly presented, on using a learned distribution to regularize the embedding space of a discrete-space autoencoder. ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "review": "The authors present a new variation of autoencoder, in which they jointly train (1) a discrete-space autoencoder to minimize reconstuction loss, and (2) a simpler continuous-space generator function to learn a distribution for the codes, and (3) a GAN formulation to constrain the distributions in the latent space to be similar.\n\nThe paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid.\n\nMy primary suggestion is that I would like to know a lot more (even qualitatively, does not need to be extensively documented runs) about how sensitive the results were--- and in what ways were they sensitive--- to various hyperparameters. Currently, the authors mention in the conclusion that, as is known to often be the case with GANS, that the results were indeed sensitive. More info on this throughout the paper would be a valuable contribution. Clearly the authors were able to make it work, with good results. When does it not work? Any observations about how it breaks down?\n\nIt is interesting how strong the denoising effect is, as simply a byproduct of the adversarial regularization.\n\nSome of the results are quite entertaining indeed. I found the yelp transfer results particularly impressive.\n\n(The transfer from positive->negative on an ambiguous example was interesting: Original \"service is good but not quick\" -> \"service is good but not quick, but the service is horrible\", and \"service is good, and horrible, is the same and worst time ever\". I found it interesting to see what it does with the mixed signals of the word \"but\": on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence. I guess the most accurate opposite would have been \"The service is quick but not good\"... )\n\nI really like the reverse perplexity measure. Also, it was interesting how that was found to be high on AAE due to mode-collapse.\n\nBeyond that, I only have a list of very insignificant typos:\n-p3, end of S3, \"this term correspond to minimizing\"\n-p3, S4, \"to approximate Wasserstein-1 term\" --> \"to approximate the Wasserstein-1 term\"\n-Figure 1, caption \"which is similarly decoded to $\\mathbf{\\~x}$\" . I would say that it is \"similarly decoded to $\\mathbf{c}$\", since it is \\mathbf{c} that gets decoded. Unless the authors meant that it \"is similarly decoded to produce $\\mathbf{\\~x}$. Alternately, I would just say something like \"to produce a code vector, which lies in the same space as \\mathbf{c}\", since the decoding of the generated code vector does not seem to be particularly relevant right here.\n\n-p5, beginning of Section 6.1:  \"to regularize the model produce\" --> \"to regularize the model to produce\" ?\n-p6, end of first par. \"is quite high for the ARAE than in the case\" --> quite a bit higher than? etc...\n-p7, near the bottom \"shown in figure 6\". --> table, not figure...\n-p8  \"ability mimic\" -->\"ability to mimic\"\n-p9 Fig 3 -- the caption is mismatched with the figure.. top/bottom/left/right/etc.... Something is confusing there...\n-p9 near the bottom \"The model learns a improved\" --> \"The model learns an improved\"\n-p14 left side, 4th cell up, \"Cross-AE\"-->\"ARAE\"\n\nThis is a very nice paper with a clear idea (regularize discrete autoencoder using a flexible rather than a fixed prior), that makes good sense and is very clearly presented. \n\nIn the words of one of the paper's own examples: \"It has a great atmosphere, with wonderful service.\" :)\nStill, I wouldn't mind knowing a little more about what happened in the kitchen...\n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper but disregards formatting suggestions, making fair evaluation impossible",
            "rating": "3: Clear rejection",
            "review": "This paper introduces a model for learning robust discrete-space representations with autoencoders. The proposed method jointly trains an RNN encoder with a GAN to produce latent representations which are designed to better encode similarity in the discrete input space. A variety of experiments are conducted that demonstrate the efficacy of the proposed methodology.\n\nGenerally speaking, I like the overall idea, which, as far as I know, is a novel approach for dealing with discrete inputs. The generated textual samples look good and offer strong support for the model. However, I would have preferred to see more quantitative evaluation and less qualitative evaluation, but I understand that doing so is challenging in this domain.\n\nI will refrain from adding additional detailed commentary in this review because I am unable to judge this paper fairly with respect to other submissions owing to its large deviation from the suggested length limits. The call for papers states that \"we strongly recommend keeping the paper at 8 pages\", yet the current submission extends well into its 10th page. In addition (and more importantly), the margins appear to have been reduced relative to the standard latex template. Altogether, it seems like this paper contains a significant amount of additional text beyond what other submissions enjoyed. I see no strong reason why this particular paper needed the extra space. In fact, there are obvious places where the exposition is excessively verbose, and there are clear opportunities to reduce the length of the submission. While I fully understand that the length suggestions are not requirements, in my opinion this paper did not make an adequate effort to abide by these suggestions. Moreover, as a result, I believe this extra length has earned this paper an unfair advantage relative to other submissions, which themselves may have removed important content in order to abide by the length suggestions. As such, I find it difficult or impossible to judge this paper fairly relative to other submissions. I regrettably cannot recommend this paper for acceptance owing to these concerns.\n\nThere are many good ideas and experiments in this paper and I would strongly encourage the authors to resubmit this work to a future conference, making sure to reorganize the paper to adhere to the relevant formatting guidelines.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}