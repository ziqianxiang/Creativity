{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "This is a good contribution, with the potential to become extremely good and significant if presentation is substantially improved.\nAll reviewers comment on the lack of clarity of the paper, especially concerning its central contributions (Section 4 and 5), as illustrated also by the relatively low confidence scores.\nReviewers also mention the current imbalance between the generality of high-order compositional networks and the motivation and empirical evaluation of these models. Generalizations of graph neural representations based on higher order local interactions are particularly interesting in contexts such as combinatorial optimization, where heuristics typically exploit high-order interactions.\n\nIn summary, we believe this work deserves a further iteration before it can be in proceedings in order to improve the exposition and the motivation of compositional networks, that will greatly improve its exposure to the community.  That said, the idea it lays forward is of potential interest, and thus the AC recommends resubmission to the workshop track. ",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Nice technical contribution with some presentation issues",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper introduces a formalism to perform graph classification and regression, so-called \"covariant compositional networks\", which can be seen as a generalization of the recently proposed neural message passing algorithms.\n\nThe authors argue that neural message passing algorithms are not able to sufficiently capture the structure of the graph since their neighborhood aggregation function is permutation invariant. They argue that relying on permutation  invariance will led to some loss of structural information.\n\nIn order to address this issue they introduce covariant comp-nets, which are a hierarchical decompositon of the set of vertices, and propose corresponding aggregation rules based on tensor arithmetic.\n\nTheir new method is evaluated on several graph regression and classification benchmark data sets, showing that it improves the state-of-the-art on a subset of them.\n\nStrong points:\n+ New method that generalizes existing methods\n\nWeak Points:\n- Paper should be made more accessible, especially pages 10-11\n- Should include more data sets for graph classification experiments, e.g., larger data sets such as REDDIT-*\n- Paper does not include proofs, should be included in the appendix\n- Review of literature could be extended\n\nSome Remarks:\n* Section 1: The reference Feragen et al., 2013 is not adequate for kernels based on walks.\n* Section 3 is rather lengthy. I wonder if its contents are really needed in the following.\n* Section 6.5, 2nd paragraph: The sentence is difficult to understand. Moreover, the reference (Kriege et al., 2016) appears not to be adequate: The vectors obtained from one-hot encodings are summed and concatenated, which is different from the approach cited. This step should be clarified.\n* unify first names in references (Marion Neumann vs. R. Kondor)\n* P. 5 (bottom) broken reference",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Verbose paper that lacks lots of details on experiments",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Thank you for your contribution to ICLR. The paper covers a very interesting topic and presents some though-provoking ideas. \n\nThe paper introduces \"covariant compositional networks\" with the purpose of learning graph representations. An example application also covered in the experimental section is graph classification. \nGiven a finite set S, a compositional network is simply a partially ordered set P where each element of P is a subset of S and where P contains all sets of cardinality 1 and the set S itself. Unfortunately, the presentation of the approach is extremely verbose and introduces old concepts (e.g., partially ordered set) under new names.  The basic idea (which is not new) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations. Moreover, the hierarchical order of the nodes should be invariant to valid permutations of the nodes, that is, two isomorphic graphs should have the same hierarchical order on their nodes and the same feature representations. Since this is the case for graph embedding methods that collect feature representations from their neighbors in the graph (and where the feature aggregation functions are symmetric) it makes sense that \"compositional networks\" generalize graph convolutional networks (and the more general message passing neural networks framework). \n\nThe most challenging problem, however, namely the problem of finding a concrete and suitable permutation invariant hierarchical decomposition of the nodes plus some aggregation/pooling functions to compute the feature representations is not addressed in sufficient detail. The paper spends a lot of time on some theoretical definitions and (trivial) proofs but then fails to make the connection to an approach that works in practice. The description of the experiments and which compositional network is chosen and how it is chosen seems to be missing. The only part hinting at the model that was actually used in the experiments is the second paragraph of the section 'Experimental Setup', consisting of one long sentence that is incomprehensible to me. \n\nInstead of spending a lot of effort on the definitions and (somewhat trivial) propositions in the first half of the paper, the authors should spend much more time on detailing the experiments and the actual model that they used. In an effort to make the framework as general as possible, you ended up making the paper highly verbose and difficult to follow. \n\nPlease address the following points or clarify in your rebuttal if I misunderstood something:\n\n- what precisely is the novel contribution of your work (it cannot be \"compositional networks\" and the propositions concerning those because these are just old concepts under new names)?\n- explain precisely (and/or more directly/less convoluted) how your model used in the experiments looks like; why do you think it is better than the other methods?\n- given that compositional network is a very general concept (partially ordered set imposed on subsets of the graph vertices), what is the principled set of steps one has to follow to arrive at such a compositional network tailored to a particular graph collection? isn't (or shouldn't) that be the contribution of this work? Am I missing something?\n\nIn general, you should write the paper much more to the point and leave out unnecessary math (or move to an appendix).  The paper is currently highly inaccessible.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN).",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN). Although, the paper is well structured and quite well written, its dense information    and its long size made it hard to follow in depth. Some parts of the paper should have been moved to appendix. As far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to SoA methods, but it is not really clear whether the out-performance is of statistical significance. Moreover,  in Table 1, training performances shouldn't be shown, while in Table 3, RMSE it would be nice to be shown in order to gain a complete image of the actual performance.",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}