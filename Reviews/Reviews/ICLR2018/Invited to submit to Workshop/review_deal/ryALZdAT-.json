{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": " + An intriguing novel regularization method: encouraging larger norms for the feature vector input to the last softmax layer of a classifier.\n + Resonably extensive experimental validation shows that it improves test accuracy to some degree.\n - While a motivation is given, the formal analysis of what is really going on remains very superficial and limited.\n\nTechnical note: Simply scaling the softmax layer's input would not change class rankings, so any positive effect of this regularizer on classification performance is due to it changing the learning dynamic in the upper layers as well. The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer's feature incay.\n",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "The idea is interesting, but motivation requires more justification. Results are good, but not very impressive",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The manuscript proposes to increase the norm of the last hidden layer to promote better classification accuracy. However, the motivation is a bit less convincing. Here are a few motivations that are mentioned.\n(1) Increasing the feature norm of correctly classified examples helps cross entropy, which is of course correct. However, it only decreases the training loss. How do we know it will not lead to overfitting?\n(2) Increasing the feature norm of mis-classified examples will make gradient larger for self-correction. And the manuscript proves it in property 2. However, the proof seems not complete. In Eq (7), increasing the feature norm would also affect the value of the term in parenthesis. As an example, if a negative example is already mis-classified as a positive, and its current probability is very close to 1, then further increasing feature norm would make the probability even closer to 1, leading to saturation and smaller gradient.\n(3) Figure 1 shows that examples with larger feature norm tend to be predicted well. However, it is not very convincing since it is only a correlation rather than causality. Let's use simple linear softmax regression as a sanity check, where features to softmax are real features rather than hidden units. Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1.\n\nThe manuscript states that the feature norm won't be infinitely increased since there is an upper bound. However, the proof of property 3 seems to only apply to the certain cases where K<2D. In addition, alpha is in the formula of upper bound, but what is the upper bound of alpha?\n\nThe manuscript does comprehensive experiments to test the proposed method. The results are good, since the proposed method outperforms other baselines in most datasets. But the results are not impressively strong.\n\nMinor issues:\n(1) For proof of property 3, it seems that alpha and beta are used before defined. Are they the radius of two circles?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper investigates how to finetune feature norms of correctly-classified and mis-classified feature vectors to improve learning process. Based on the analysis, they proposed feature incay to encourage larger feature norm. Experimental results on four datasets demonstrate the effectiveness of the proposed method.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Pros:\n1. It provided theoretic analysis why larger feature norm is preferred in feature representation learning.\n\n2. A new regularization method (feature incay) is proposed.\n\nCons:\nIt seems there is not much comparison between this proposed method and the concurrent work \"COCO(Liu et al. (2017c))\".",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting analysis of feature norm, but the paper needs improvements",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The analyses of this paper (1) increasing the feature norm of correctly-classified examples induce smaller training loss, (2) increasing the feature norm of mis-classified examples upweight the contribution from hard examples, are interesting. The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses. \n\nHowever, the presentation of this paper need to be largely improved. For example, Figure 3 seems to be not relevant to Property2 and may be show the feature norm is lower when the samples is hard example. Therefore, the author used reciprocal norm loss which increases feature norm as shown in Figure 4. However, both Figures are not explained in the main text, and thus hard to understand the relation of Figure 3 and 4. The author should refer all Figures and Tables. \n\nOther issues are:\n-Large-margin Soft max in Figure 2 is not explained in the introduction section. \n-In Eq.(7), P_j^I is not defined. \n- In the Property3, The author wrote “ where r is lower bound of feature norm”. \n However, r is not used.\n-In the experimental results, “RN” is not defined.\n-In the Table3, the order of \\lambda should be increasing or decreasing order. \n- Table 5 is not referred in the main text. \n\n== Updated review == \nThe presentation has been improved, I have increased the rate from 5 to 6. \nFollowing are further comments for presentation. \n\n-\tFig.2 “ the increasing L2 norm “ seems to  “the order of L2 norm ”\n-\tPp.4 the first sentence above Eq.(7) “According to definition …”  should be improved . \n-\tpp.5, the first sentence of the second paragraph “The feature norm can be optimized ..” is not clear. \n-\tIt would be better put Figure 5 under Property3. \n-\tD should be defined in Property3. \n-\tpp.8 wrote “However, 259-misclassfied examples are further introduced”. However, in Table 5, it seems to be 261. \n-\tSection 5. is “Conclusion and future work”. However, future work is not mentioned. \n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}