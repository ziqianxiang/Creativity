{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "I am somewhat of two minds from the paper. The authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. The tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution..  On the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search (the last shouldn't be surprising if that was indeed the training criterion).\n\nAll in all, I think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the Workshop.\n\n(Side note: There's a reference missing on page 4, first paragraph)",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Interesting experiments, wrong conclusions.",
            "rating": "3: Clear rejection",
            "review": "This work presents an empirical study aiming at improving the understanding of the vulnerability of neural networks to adversarial examples. Paraphrasing the authors, the main observation of the study is that the vulnerability is due to an inherent uncertainty that neural networks have about their predictions ( the difference between the logits). This is consistent across architectures, datasets. Further, the authors note that \"the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.\"\n\nWhile this empirical study contains valuable information, its above conclusions are factually wrong. It can be theoretically proven at least using two routes. They are also in contradiction with other empirical observations consistent across several previous studies. \n\n1-Constructive counter-argument: Consider a neural network that always outputs a constant prediction. It (1) is by definition independent of any dataset (2) generalizes perfectly (3) has zero adversarial error, hence contradicting the central statement of the paper. \n\n2- Analysis-based counter-argument: Consider a neural network with one hidden layer and two classes. It is easy to show that the difference between the scores (logits) of the two classes is linear in the operator norm of the hidden weight matrix and linear in the L2-norm of the last weight vector. Therefore, the robustness of the model indeed depends on its capability to generalize because the latter is essentially governed by the geometric margin of the linear separator and the spectral norm of the weight matrix (see [1,2,3]). QED.\n\n3- Further, the lack of calibration of neural networks and its causes are well known. Among other things, it is due to the use of building blocks (such as batch-norm [4]), regularization (e.g., weight decay) or the use of softmax+cross-entropy during training. While this is convenient for optimization reasons, it indeed hurts the calibration. The authors should try to train a neural network with a large margin criteria and see if the same phenomenon still holds when they measure the geometric margin. Another alternative is to use a temperature with the softmax[4]. Therefore, the observations of the empirical study cannot be generalized to neural networks and should be explicitly restricted to neural networks using softmax with cross-entropy as criteria. \n\nI believe the conclusions of this study are misleading, hence I recommend to reject the paper. \n\n\n[1] Spectrally Normalized Margin-bounds Margin bounds for neural networks (Bartlett et al., 2017)\n[2] Parseval Networks: Improving Robustness to Adversarial Examples (Cisse et al., 2017) \n[3] Formal Guarantees on the Robustness of a classifier against adversarial examples (Hein et al., 2017)\n[4] On the Calibration of Modern Neural Networks (Guo et al., 2017)",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Their explanation seems to be done by non-strict argument,  and their proposed methods do not seem related to their discovery so much.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper insists that adversarial error for small adversarial perturbation follows power low as a function of the perturbation size, and explains the cause by the logit-difference distributions using mean-field theory.\nThen, the authors propose two methods for improving adversarial robustness (entropy regularization and NAS with reinforcement learning).\n\n[strong points]\n* Based on experimental results over a broad range of datasets, deep network models and their attacks.\n* Discovery of the fact that adversarial error follows a power low as a function of the perturbation size epsilon for small epsilon.\n* They found entropy regularization improves adversarial robustness.\n* Their neural architecture search (NAS) with reinforcement learning found robust deep networks.\n\n[weak points]\n* Unclear derivation of Eq. (9). (What expansion is used in Eq. (21)?)\n* Non-strict argument using mean-field theory.\n* Unclear connection between their discovered universality and their proposals (entropy regularization and NAS with reinforcement learning).",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "review": "Very intriguing paper and results to say the least. I like the way it is written, and the neat interpretations that the authors give of what is going on (instead of assuming that readers will see the same). There is a well presented story of experiments to follow which gives us insight into the problem. \n\nInteresting insight into defensive distillation and the effects of uncertainty in neural networks.\n\nQuality/Clarity: well written and was easy for me to read\nOriginality: Brings both new ideas and unexpected experimental results.\nSignificance: Creates more questions than it answers, which imo is a positive as this topic definitely deserves more research.\n\nRemarks:\n- Maybe re-render Figure 3 at a higher resolution?\n- The caption of Figure 5 doesn't match the labels in the figure's legend, and also has a weird wording, making it unclear what (a) and (b) refer to.\n- In section 4 you say you test your models with FGSM accuracy, but in Figure 7 you report stepll and PGD accuracy, could you also plot the same curves for FGSM?\n- In Figure 4, I'm not sure I understand the right-tail of the distributions. Does it mean that when Delta_ij is very large, epsilon can be very small and still cause an adversarial pertubation? If so does it mean that overconfidence in the extreme is also bad?\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}