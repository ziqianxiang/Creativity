{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The method proposed in the paper for latent disentanglement and attribute-conditional image generation is novel to the best of my understanding but reviewers (Anon1 and Anon3) have expressed concerns on the quality of results (CelebA images) as well as on the technical presentation and claims in the paper.\n\nGiven the novelty of the proposed method, I would *not* like to recommend a \"reject\" for this paper but the concerns raised by the reviewers on the quality of results and lack of quantitative results seem valid. Authors rule out possibility of any quantitative results in their response but I am not fully convinced -- in particular, effectiveness of attribute-conditional image generation can be captured by first training an attribute classifier on the generated images and then measuring how often the predicted attributes are flipped when conditioning signal is changed. There are also other metrics in the literature for evaluating generative models.\n\nI would recommend inviting it to the workshop track, given that the work is novel and interesting but has scope for improvements.\n",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "This paper proposed a new method to disentangle different attributes of images.  A novel DNA structure GAN is proposed to manipulate attributes in images.",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Pros:\n1. A new DNA structure GAN is utilized to manipulate/disentangle attributes.\n\n2. Non attribute part (Z) is explicitly modeled in the framework.\n\n3. Based on the experiment results, this proposed method outperformed previous methods (TD-GAN, IcGAN).\n\nCons:\n1. It assumes that each individual piece represents an independent factor of variation, which can not hold all the time. The authors also admit that when two factors are dependent, this method might fail.\n\n2. In Lreconstruct, only min difference between A and A1 is considered. How about A and A2 here? It seems that A2 should also be similar with A since only one bit in A2 and A1 is different.\n\n3. Only one attribute can be \"manipulated\" each time? Is it possible to change more than one attribute each time in this method?",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but unclear description of the method and not very convincing results",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes. \n\nPros: \n+ The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel. \n+ A theoretical guarantee of the efficiency of an aspect of the proposed method is given.\n\nCons: \n- The results are not very appealing visually. The results from the proposed method do not seem much better than the baselines. What is the objective for the images in Fig. 4? For example I'm looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top-left but adding the attributes of choice.\n- Quantitative results are missing. \n- Some unclarity in the description of the method; see below.\n\nQuestions/other:\n- What is meant by \"implicit\" models? By \"do not anchor a specific meaning into the disentanglement\"? By \"circumscribed in two image domains\"?  \n- Why does the method require two images? \n- In the case of images, what is a dominant vs recessive pattern? \n- It seems artificial to enforce that \"the attribute-irrelevant part [should] encode some information of images\". \n- Why are (1, 0) and (1, 1) not useful pairs?\n- Need to be more specific: \"use some channels to encode the id information\". \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review summary \"DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images\"",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\nThis paper investigated the problem of attribute-conditioned image generation using generative adversarial networks. More specifically, the paper proposed to generate images from attribute and latent code as high-level representation. To learn the mapping from image to high-level representations, an auxiliary encoder was introduced. The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss. To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline. Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.\n\n== Novelty and Significance ==\nMulti-attribute image generation is an interesting task but has been explored to some extent. The integration of generative adversarial networks with auto-encoding loss is not really a novel contribution.\n-- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016.\n\n== Technical Quality == \nFirst, it is not clear how was the proposed annihilating operation used in the experiments (there is no explanation in the experimental section). Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution). I would appreciate the authors to elaborate this a bit.\n\nSecond, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network). The proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribution.\n\nThird, it seems that the proposed multi-attribute generation pipeline works for binary attribute only. However, such assumption limits the generality of the work. Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes and (2) not specific to face images.\n-- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015.\n-- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016.\n-- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.\n\nAdditionally, considering the generation quality, the CelebA samples in the paper are not the state-of-the-art. I suspect the proposed method only works in a more constrained setting (such as Multi-PIE where the images are all well aligned).\n\nOverall, I feel that the submitted version is not ready for publication in the current form.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}