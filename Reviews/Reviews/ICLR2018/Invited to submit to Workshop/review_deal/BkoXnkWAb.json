{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "the reviewers were not fully convinced of the setting under which the proposed bipolar activation function was found by the authors to be preferable, and neither am i.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Simple idea to encourage zero mean activations, but results focus on accuracy instead of learning speed-up",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks. The proposal is to replace the non-linearity in half of the units in each layer with its \"bipolar\" version -- one that is obtained by flipping the function on both axes.\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained. \n\nClarity:\nThe paper is easy to read. The plots in Fig. 2 and the appendix are quite helpful in improving presentation. The experimental setups are explained in detail. \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive. However, the experiments to support the idea do not seem to match the motivation of the paper. As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent. However, the presented results focus on the performance on held-out data instead of improvements in training speed. This is especially the case for the RNN experiments.\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning. However, it is unclear that overall training time can be reduced with the help of this technique. To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered.\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases. The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset), but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice idea but weak empirical performances",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes a self-normalizing bipolar extension for the ReLU activation family. For every neuron out of two, authors propose to preserve the negative inputs. Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU.\n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN.\n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.  They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions. They also show that they can train deep residual network architecture on CIFAR without the use of BN.\n\nQuestion:\n- Which layer mean and variance are reported in Figure 2? What is the difference between the left and right plots?\n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances. It would be nice to report the training loss to see if this is an optimization or a generalization problem.\n- How does bipolar activation compare to model train with BN on CIFAR10?\n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?\n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB. However, BN-LSTM outperforms BELU-RNN on Text8. Do you know why the trend is not consistent across datasets?\n\n-Clarity/Quality\nThe paper is well written and pleasant to read\n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel.\n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost. In this paper, bipolar activations are used to train very deep stacked RNN. However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures. It is not clear what are the advantage of deep stacked RNN in that context.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting paper, would like to see more experiments",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs. The activation function keeps the activation roughly zero-centered. \n\nIn general, this is an interesting direction to explore, the idea is interesting, however, I would like to see more experiments\n\n1. The authors tested out this new activation function on RNNs. It would be interesting to see the results of the new activation function on LSTM.\n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers. For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.  \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.\n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm.\n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without.\n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}