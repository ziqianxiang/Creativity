{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "All the reviewers noted that the dual formulation, as presented, only applies to the logistic family of classifiers. The kernelization is of course something that *can* be done, as argued by the authors, but is not in fact approached in the submission, only in the rebuttal. The toy-ish nature of the problems tackled in the submission limits the value of the presentation.\n\nIf the authors incorporate their domain adaptation results (SVHN-->MNIST and others) using the kernelized approach and do the stability analysis for those cases, and obtain reasonable results on domain adaptation benchmarks (70% on SVHN-->MNIST is for instance on the low side compared to the pixel-transfer-based GAN approaches out there!) then I think it'd be a great paper.\n\nAs such, I can only recommend it as an invitation to the workshop track, as the dual formulation is interesting and potentially useful.",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "A very specific and restricted fix to GANs ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The paper deals with “fixing GANs at the computational level”, in a similar sprit to f-GANs and WGANs. The fix is very specific and restricted. It relies on the logistic regression model as the discriminator, and the dual formulation of logistic regression by Jaakkola and Haussler. \n\nComments: \n1) Experiments are performed by restricting alternatives to also use a linear classifier for the discriminator. It is mentioned that results are expected to be lower than those produced by methods with a multi-layer classifier as the discriminator (e.g. Shen et al., Wasserstein distance guided representation learning for domain adaptation, Ganin et al., Domain-adversarial training of neural networks?).  \n2) Considering this is an unsupervised domain adaption problem, how do you set the hyper-parameters lambda and the kernel width? The “reverse validation” method described in Ganin et al., Domain-adversarial training of neural networks, JMLR, 2016 might be helpful. \n\nMinor comments: on the upper-bound of the distance, alpha_i instead of alpha^\\top, and please label the axes in your figures. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting but limited dual approach to adversarial distance",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear). They motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation.\n\nThe dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the Hessian is y_i y_j <x_i, x_j>, thus highlighting the pairwise similarities between the points x_i & x_j; here the labels represent whether the point x comes from the samples A from the target distribution or B from the proposal distribution. This paper then compare this objective with the MMD distance between the samples A & B. It points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the MMD distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the MMD has been observed in the past to perform poorly for face generation [Dziugaite et al. UAI 2015]). From this analogy, one could expect the method to improves over MMD, but not necessarily significantly in comparison to an approach which would use more powerful discriminators.\n\nThis paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task. They observe better performance for their approach in comparison to ADDA, improved WGAN and MMD, when restricting the discriminators to be a linear classifier.\n\n== Evaluation \n\nI found this paper quite clear to read and enjoyed reading it. The observations are interesting, despite being on the toyish side. I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper).\n\nOne concern about the paper (but this is an unfortunate common feature of most GAN papers) is that it ignores the vast knowledge on saddle point optimization coming from the optimization community. The instability of a gradient method on non-strongly convex-concave saddle point problems (like the bilinear form of Figure 1) is a well-known property, and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this paper. Moreover, the transformation to the minimization form crucially required the closed form computation of the dual function (with w* just defined above equation (2)), and this is limited to linear discriminators,  thus ruling out the use of the proposed approach to more powerful discriminators like deep neural nets. Thus the significance appears a bit limited to me.\n\n== Other comments\n\n1) Note that d(A, B'_theta) is *equal* to min_alpha max_w  (...)  above equation (2) (it is not just an upper bound). This is a standard result coming from the fact that the Fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective as (2).  See e.g. Section 2.2 of [Collins et al. JMLR 2008] (this is for the more general multiclass logistic regression problem, but (2) is just the binary special case of equation (4) in the [Collins ... ] reference). And note that the \"w(u)\" defined in this reference is the lambda*w*(alpha) optimal relationship defined in this paper (but without the 1/lambda factor because of just slightly different writing; the point though is that strong duality holds there and thus one really has equality).\n\n\n[Collins et al. JMLR 2008] Michael Collins, Amir Globerson, Terry Koo , Xavier Carreras, Peter L. Bartlett, Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks, , JMLR 2008.\n\n [Dziugaite et al. UAI 2015] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting dual formulation of an adversarial loss, but its applicability remains to be fully demonstrated",
            "rating": "6: Marginally above acceptance threshold",
            "review": "This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss. This allows the authors to turn the standard min max problem of adversarial training into a single minimization problem, which is easier to solve. The method is demonstrated on a toy example and on the task of unsupervised domain adaptation.\n\nStrengths:\n- The derivation of the dual formulation is novel\n- The dual formulation simplifies adversarial training\n- The experiments show the better behavior of the method compared to adversarial training for domain adaptation\n\nWeaknesses:\n- It is unclear that this idea would generalize beyond a logistic regression classifier, which might limit its applicability in practice\n- It would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which GANs are often used\n- It would be interesting to see if the DA results with a kernel classifier are better (comparable to the state of the art)\n- The mathematical derivations have some errors\n\n\nDetailed comments:\n- The upper bound used to derive the formulation applies to a logistic regression classifier. While effective, such a classifier might not be as powerful as multi-layer architectures that are used as discriminators. I would be interested to know if they authors see ways to generalize to better classifiers.\n\n- The second weakness listed above might be related to the first one. Did the authors tried their approach to non-DA tasks, such as generating images, as often done with GANs? Showing such results would be more convincing. However, I wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other tasks.\n\n- The DA results are shown with a linear classifier, for the comparison to the baselines to be fair, which I appreciate. However, to evaluate the effectiveness of the method, it would be interesting to also report results with a kernel-based classifier, so as to see how it compares to the state of the art.\n\n- There are some errors and unclear things in the mathematical derivations:\n* In the equation above Eq. 2, \\alpha should in fact be \\alpha_i, and it is not a vector (no need to transpose it)\n* In Eq. 2, it should be \\alpha_i \\alpha_j instead of \\alpha^T\\alpha\n* In Eq. 3, it is unclear to me where the constraint 0 \\leq \\alpha \\leq 1 comes from. The origin of the last equality constraints on the sums of \\alpha_A and \\alpha_B is also unclear to me.\n* In Eq. 3, it is also not clear to me why the third term has a different constant weight than the first two. This would have an impact on the relationship to the MMD\n\n- The idea of sample reweighting within the MMD was in fact already used for DA, e.g., Huang et al., NIPS 2007, Gong et al., ICML 2013. What is done here is quite different, but I think it would be worth discussing these relationships in the paper.\n\n- The paper is reasonably clear, but could be improved with some more details on the mathematical derivations (e.g., explaining where the constraints on \\alpha come from), and on the experiments (it is not entirely clear how the distributions of accuracies were obtained).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}