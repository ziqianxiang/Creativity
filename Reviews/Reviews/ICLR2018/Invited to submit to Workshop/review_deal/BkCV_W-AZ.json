{
    "Decision": {
        "title": "ICLR 2018 Conference Acceptance Decision",
        "comment": "The reviewers agree this is a really interesting paper, with an interesting idea (in particular\nthe use of regret clipping might provide a benefit over typical policy gradient methods). However,\nthere are two major concerns: 1) clarity / exposition and more importantly 2) lack of a strong\nempirical motivation for the new approach (why do standard methods work just as well on these\npartially observable domains?).",
        "decision": "Invite to Workshop Track"
    },
    "Reviews": [
        {
            "title": "Interesting results on applying counterfactual regret minimization results to Deep RL.",
            "rating": "7: Good paper, accept",
            "review": "This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL. Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better. The results is interesting and novel. This paper should be accepted.\n\nThe presentation of the paper can be improved a bit. Much of the notation introduced in section 3.1 is not used later on. There seems to be a bit of a disconnect before and after section 3.3. The algorithm in deep RL could be explained a bit better.\n\nThere are some papers that could be connected. Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments.\n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals.\n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train. An experiment illustrating this effect could be illuminating.\n\nCan the proposed approach help when we have recurrent value functions? Since recurrence does not guarantee that all information needed is captured.\n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al.\n\nPotential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A regret minimization approach to policy gradient algorithms.",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE.\nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions).\nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN.\n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \"regret minimization\" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives.\n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \"deep reinforcement learning\".\n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks.\n\nHowever, there are also some negatives:\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused.\n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc. At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed.\n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability) but we only see results compared to DQN... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods. Yes A3C and TRPO seem like they perform very poorly compared to ARM... but I'm left wondering how/why?\n\nI feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper.\nA lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting.\nHowever, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods... I also didn't feel the experimental evaluations drove a clear message except \"ARM did better than all other methods on these experiments\"... I'd want to understand how/why and whether we should expect this universally.\nThe focus on \"regret minimization perspectives\" didn't really get me too excited...\n\nOverall I would vote against acceptance for this version.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. However the results only show that the algorithm converges, in some cases, faster than the previous work.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance. Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs. \n\nThe paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain. To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption . The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle. This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy. To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently.\n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original. However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification). I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence. However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones. Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling. More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.\n ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}