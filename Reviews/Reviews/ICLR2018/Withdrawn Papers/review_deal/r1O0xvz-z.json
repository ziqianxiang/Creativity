{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper proposes to address the adversarial attach problem by learning the neural network over hyperspheres. Experiments are weak.",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes to address the adversarial attach problem by learning the neural network over hyperspheres. The topic is important and the authors visually illustrate the mechanism of the proposed approach. \n\nMy major concerns are as follows.\n\ni.\tAs pointed out by the authors, the high level intuition of the proposed approach relies on the observation that \"The black-box adversarial attack assumes that the inputs of each layer lie in the Euclidean space, and perturbs them in the Euclidean space.\". However, an important principle in designing a defense system (in cyber security) is that it does not know in advance how the attackers will attack the system. I think the principle also applies here. In other words, the authors may want to discuss when we should learn the NN over hyperspheres. Or, shall we learn the NN over hyperspheres all the time?\n\nii.\tThe authors conduct experiments on two small datasets. To make the proposed approach more convincing, they may want to perform experiments on large real-world data sets.\n\niii.\tThe authors may want to make this paper self-contained. For example, the authors may want to briefly describe PGD or at least provide references.\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose to use a new form of layers for deep networks, in which the dot product + nonlinearity is replaced by some function of the angle between the weightsand the input. The authors propose three variants of these so-called hypersphere networks, including the cosine. In addition to modifying the convolutional operations, the authors modify the loss function to include constraints on the maximum 2-norm of the weight vectors for each class in the last layer. They also use an additional term to encourage orthogonality of weight matrices, alter the training with loss annealing which gives more weight at the beginning of training with a non-normalized last layer. The authors also propose a means to perform \"robust inference\", rejecting examples based on the distance of the angle to a mean angle on the natural data. A theoretical motivation is given, where the only relationship to adversarial examples seem to be Theorem 5, where using normalized inputs might improve by a factor (p-1)/p under some assumptions, on a linear problem. Experiments are carried out on MNIST and CIFAR-10, comparing the different variants of the approach.\n\nWhile the paper presents rather extensive experiments, the approach is only weakly motivated and the results are mostly unconclusive. The baseline has rather poor performances, and different variants of the model are better than others on different evaluation measures. Whereas it is expected since the different evaluation measures may be conflicting, it is unclear what model is supposed to improve on the state-of-the-art on what criterion, and for what reason. Moreover, whereas the paper is motivated by defending against adversarial examples, the approach improves on the vanilla CNN as well, so the improved performances may simply be because the model is better overall. But since the final results are far from the state-of-the-art, this is not really sufficient to make a case for the approach.\n\ndetailed comments (in order of appearance in the paper):\n- the motivation for using something else than the cosine is unclear, and the results do not seeem very conclusive since, as the authors say \"D-SphereNets with weight multiplier and loss annealing have the best performance against PGD attacks.\"\n- the term to encourage orthogonality seems to be the regularization proposed by Cisse et al. (2017). Is there any difference?\n- for the robust inference, the authors propose the formula p(theta_{mu} + theta_{\\sigma}). Why this formula? I would expect a formula of the form theta_mu + p theta_sigma by analogy with confidence intervals\n- the \"theoretical insights\" are not really convincing. The factor (p-1)/p is most likely very small in practice, and it is unclear whether it can actually be significant on multilayer networks. Moreover, it is only about normalizing inputs under very strong assumptions, and nothing really argues in favor of normalizing the weight vector at this point, nor justifies anything else than the cosine.\n- the baseline on CIFAR-10 seems way below the standards. Wide-resnets achieve ~ 95% accuracy on clean data (whereas models with 92% accuracy are the standard), while in the paper the baseline has 85% accuracy. In general, all improvements w.r.t. the baseline are not strong arguments in favor of the approach, because they are still way below the sota.\n- it is very hard to get a clear winner from Table 2. The \"D-SphereNet + WM + Input-truncated SphereCorr\" seems to be the best on clean data, but then it loses robustness when learnt on clean and performs worse against PGD. The \"D-SphereNet + WM + Loss Annealing\" performs better against FGSM, but worse than \"D-SphereNet + WM\" against PGD. As there doesn't seem to be any good reason a priori to believe that one of the approaches is better against a certain type of opponent, it is difficult to extract a take-home message from these results.\n- how much of the good performances are due to the use of near-orthogonality constraints?\n- at what moment in the experiment is the approach of section \"4.3 ROBUST GEODESIC INFERENCE\" used? As it is designed to \"[...] reject this testing sample and regard it as adversarial attack.\", it doesn't seem to fit in the experimental protocol.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Repurposing of the work in [Liu et al 2017] to gain adversarial attack resistance; good experimental results but lacking sound theoretical justification",
            "rating": "5: Marginally below acceptance threshold",
            "review": "The author(s) propose a network very similar to that in [Liu et al 2017] along with minor changes as a solution to white and blackbox adversarial attacks. The idea is to project the data onto a hypersphere not only in the input layer, but also other layers, which leads to better resistance against those attacks. They provided several theorems regarding a simplified architecture (a single linear neuron with additive noise) to support the idea. They also observed that the proposed network shows better performance when attacked by FGSM and PGD methods. \nMy main concern is about the novelty of the work. It looks like the only major difference between the work and that in [Liu et al 2017] is the weight multiplier loss in (eq. 5) and robust geodesic inference to identify samples which are adversarial. \nI also noticed several problems and gaps in the the theoretical justification. To name a few:\n1) In theorem 2, they claimed a bound about the *true* expected loss with respect to the samples and noise. This is not realistic, given that the *empirical* loss is minimized in the training. \n2) In the proof of theorem 2, in line 4 to 5 of eq. 16, the logic is flawed. Although E(Sum x_i^2) = 1, but one can not say E(Sum (w*_i - w_i)^2 x_i^2) = Sum (w*_i - w_i)^2. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}