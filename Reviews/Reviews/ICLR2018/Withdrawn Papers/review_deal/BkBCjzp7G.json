{
    "Decision": "",
    "Reviews": [
        {
            "title": "Because the paper is hard to understand, does not provide any significant technical contributions, and includes experiments that are not well analyzed and explained, the reviewer suggests the paper be rejected in its current form. The authors can take the reviewer's comments into consideration to put together a stronger submission. ",
            "rating": "3: Clear rejection",
            "review": "* Paper Summary\nThis paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs). It makes use of the CP or rank-1 tensor decomposition to define the meaning of rank for a tensor. When a tensor is decomposed as a sum of rank-1 tensors (outer products), the number of operations in a DNN forward pass decreases leading to a faster testing runtime. This form of network compression has been worked on before. The contribution of this paper seems to be the specific way the decomposition is used in training the DNN. It seems that this training process follows a projected gradient descent procedure, where the filter weights of the network are iteratively updated using regular (stochastic) gradient descent and then they are projected onto the set of rank-R tensors. The authors devise a heuristic way (based on an innovated measure that combines computational complexity with performance) to select the tensor rank to be used. Experiments are conducted on the task of image classification for a couple well-known DNN architectures (VGG and Resnet) to show a speedup of runtime in testing, significant compression of the network, and minimal degradation in performance.\n\n* Related Work\nThe authors do a good job describing and listing the papers most related to the current submission.\n\n* Technical Novelty\nOne main limitation of the paper is the lack of technical contribution. The idea of using rank-1 tensor decomposition for training low-rank filtering operations in DNNs has already been proposed and used in several other work. From what I understood from the paper, the only technical contribution is the use of a so called 2-pass decomposition, which is simply an implementation of projected gradient descent on the set of rank-R tensors. In particular, if we seek to minimize f(W) such that W belongs to asset that can be easily projected on, then projected gradient descent would apply traditional gradient descent on the current iterate, followed by a projection step onto this set. This paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f(.). This iterative projection tends to perform better than iteratively optimizing f(W) and then applying the projection step only once at the very end of the optimization (assumedly the CP-ALS method that is used for comparison). Put in this light, the proposed paper does not contribute much.\n\n* Paper Presentation\nIn the reviewer’s opinion, the primary limitation of the paper is how it is written and organized. The paper is badly written. It is riddled with grammar, choice of word, and spelling mistakes. The paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work. These issues make the paper hard to read. For example, the authors spend quite a bit of space focusing on the rank-1 (CP) decomposition, which is well known, as opposed to focusing on the merits of their technical contributions. Also, the experiments are not clearly explained. It is hard to understand the experimental setup of each experiment and what the conclusions are. For example, it is unclear whether the baseline in Table 3 also uses the two-pass decomposition or not. Also, the authors should provide a clear and standard description of the experimental setup for each experiment (e.g. which network, which dataset, which task/loss, which measure, etc.).\n\n* Experimental Results\nFrom what I understood from the experiments, it seems that using the “two-pass decomposition” (i.e. projected gradient descent) is better than CP-ALS (gradient descent ended with a single projection step). This conclusion seems to be intuitive and expected. However, as mentioned earlier, the paper writing and organization makes it hard to understand what exactly is being shown. For example, Table 1 shows that the baseline method uses less filters than the proposed method that selects the number of filters through an innovated heuristic measure. Then in Table 3, we see that the baseline is less stable (i.e. its performance decreases across the different iterations of projected gradient descent). Isn’t this expected since the baseline uses less filters? It is unclear from the text if this is the case. \n                   The authors should do a better job explaining and comparing the overall experimental results.  For example, it seems that the proposed projected gradient descent method leads to better speedup results in VGG as opposed to Resnet, with very similar reduction in accuracy. The authors do not comment on this. It would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases (e.g. cases when the low-rank decomposition leads to a significant decrease in performance). All of this analysis provided more insight into the method and helps the reader understand its extents. \n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Useful extension, but not written clearly and evaluated rigorously enough. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "General comment\n==============\nLow-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction performance. The authors a) extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights, and b) and algorithm to determine the rank of each convolutional filter. The authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to VGG16. The proposed method is a useful extension of existing methods but needs to evaluated more rigorously. The manuscript is hard to read due to unclear descriptions and grammatical errors.\n\nMajor comments\n=============\n1. The authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to VGG16. The authors should analyze if this also holds true for ResNet and Inception, which are more widely used than VGG16.\n\n2. The authors measured the actual speedup on a single CPU (Intel Core i5). The authors should measure the actual speedup also on a single GPU.\n\n3. It is unclear how the actual speedup was measured. Does it correspond to the seconds per update step or the overall training time? In the latter case, how long were models trained?\n\n4. How and which hyper-parameters were optimized? The authors should use the same hyper-parameters for all methods (Jaderberg, Zhang, Rank selection). The authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for ‘Rank selection’.\n\n5. Figure 4: the authors should show the same plot for more convolutional layers at varying depth from both VGG and ResNet.\n\n6. The manuscript is hard to understand and not written clearly enough. In the abstract, what does ‘two-pass decomposition’, ‘proper ranks’, ‘the instability problem’, or ‘systematic’ mean? What are ‘edge devices’, ‘vanilla parameters’? The authors should also avoid uninformative adjectives, clutter, and vague terms throughout the manuscript such as ‘vital importance’ or ‘little room for fine-tuning’.\n\nMinor comments\n=============\n1. The authors should use ‘significantly’ only if a statistical hypothesis was performed.\n\n2. The manuscript contains several typos and grammatical flaws, e.g. ‘have been widely applied to have the breakthrough’, ‘The CP decomposition factorizes the tensors into a sum of series rank-one tensors.’, ‘Our two-pass decomposition provides the better result as compared with the original CP decomposition’. \n\n3. For clarity, the authors should express equation 5 in terms of Y_1, Y_2, Y_3, and Y_4.\n\n4. Equation 2, bottom: C_in, W_f, H_f, and C_out are undefined at this point.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Still have questions",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes a novel workflow for acceleration and compression of CNNs. The proposed workflow consists of the novel two-pass decomposition of a group of layers, and the fine-tuning of the remaining network. This process is applied iteratively to different groups of layers. The authors also propose a way to determine the target rank of each layer given the target overall acceleration. The authors report the highest measured acceleration of VGG16 using low-rank approximation techniques (6.2x vs 5x previously) with a similar accuracy drop (1.2% vs 1.0% previously).\nThe paper is well-structured, and the proposed method is clearly described. However it would be nice to see the difference to other related methods more clearly. Some results are counterintuitive if the reader is not familiar with related works (e.g. the Zhang et al. 2016 achieves a lower acceleration with much lower ranks).\nThe main concern is the motivation of the two-pass decomposition. It is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor. There are no theoretical results regarding this question in the paper, and the empirical justification is also lacking. It would be necessary to see the tensor reconstruction error during the following 2 scenarios:\nWe apply the CP decomposition to a pretrained network\nWe apply the CP decomposition to a pretrained network, then restore it back into the dense format, optimize it, and then apply the CP decomposition again\nWhat is the reconstruction error in case 1? What is the reconstruction error during the second CP decomposition in 2? What is the accuracy drop after fine-tuning in both scenarios? Figure 4 could have answered this question, however it is not clear from the paper whether the CP-ALS procedure was followed by fine-tuning or not. If it wasn’t, then the comparison is unfair, as the results for CP-ALS are drastically underestimated.\nIt would also be nice to see the full learning curves for all experiments, where different stages (decompose->optimize->decompose->finetune->...) are explicitly marked. The reported tables seem to ignore a lot of the relevant information.\nAlso Astrid and Lee 2017 do not seem to report the instabilities during fine-tuning of the decomposed layers, and argue that these layers should not be freezed. As they use a very similar iterative fine-tuning workflow, it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freezing. These two methods seem to be closely related and should be thoroughly compared.\nThe improvement w.r.t. other methods seems marginal. The previous SotA result on VGG16 was 5x acceleration with 1% accuracy drop, and here the reported result is 6.2x acceleration with 1.2% accuracy drop. The authors claim that the previous SotA result was carefully fine-tuned with a low learning rate, and that in this paper they used only default fine-tuning with a high learning rate. Is it possible to further improve the accuracy by a more careful fine-tuning? Right now the results are not very convincing.\nI would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with Astrid and Lee 2017 are answered.\nOther comments and remarks:\nThe meaning of the following sentence is not clear, it probably should be rephrased: “We observed that if the network is trained in the restored dense form, the training result can be more stable because of its smoother convex.”. What does “smoother convex” mean?\nIt should be stated more clearly how the results from Figure 4 were obtained.\nIt would be interesting to see the accuracy of the fitness approximation during the rank selection procedure.\nIs it possible to perform the CP decomposition by minimizing the activation reconstruction loss (like proposed by Zhang et al. 2016), and not the tensor reconstruction loss (as usual)? It seems as a more natural way to do it.\nThe convergence constraint procedure from Table 4 is not clear. “our experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than 0.1%.” - what does “the accuracy improvement is smaller than 0.1%” mean?",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}