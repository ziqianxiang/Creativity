{
    "Decision": "",
    "Reviews": [
        {
            "title": "A machine learning based approach to fault diagnosis is impractical in modern communication systems. The paper proposes to use both unlabeled and labeled data for the task of learning. ",
            "rating": "4: Ok but not good enough - rejection",
            "review": "1. The paper is hard to read - it contains several \"?\" generated from latex. For e.g. 1) a traditional two-phase framework, using expert features and some general-propose classifiers (?); \n2. Spelling errors need to be corrected. For e.g. 'artical' -> article and so on.\n3. The related work needs to be improved with more details.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper mainly combined 3 data sources and applied the semi-supervised and transferred learning to realized the packet flow classification. ",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposed to combine three kinds of data sources: real, simulated and unlabeled, to help solve \"small\" data issue occurring in packet stream. A directed information flow graph was constructed, a multi-headed network was trained by using Keras and GAN library.  Its use on the packet sequence classification can archive comparable accuracy while relieve operation engineers from heavy background learning. \n\nThe presentation of this paper can be improved. \n* With the missing citations as \"(?)\" and not clearly defined concepts, including property of function H (any function? convex?) in (3), full name of TCP/abbr of GAN when first appear, etc. reader might need to make guesses to follow. \n* P2: You can draw your audience by expanding the \"related work\" like a story: more background of GAN etc. and one or two highlight formula to help clear the idea \n* P3: What's the purpose of inserting \"dummy packets to denote the timeout between two time stamps\"? \n* P3: Help sell to \"non-engineer\" by maybe having image example or even plainer language to describe the meaning (deep difference/purpose) of \"3 levels of feature engineering\"; and when addressing features, mentioned as 1,2,3, while in Table 1, shown as Feature=0,1,2; \n* P6: section 4.2 mentioned \"only metrics cared by operators\", is this what you mean by \"relieve operation engineers ...\", and which is or how to decide the cutoff accuracy the engineers should make a Go or No Go decision? \n\n",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice work that could benefit from clarifications and the use of a public dataset",
            "rating": "6: Marginally above acceptance threshold",
            "review": "Comments:\n\nI reviewed the updated version.\nThe authors manage to significantly improve the detection of anomalies using generative adversarial networks to simulate new samples and therefore improve the training of their final classifier. I liked the fact that even if the overall system seems difficult to train (3 GANs + 2 generative networks plus one final classifier), the number of hyperparameters remains low (same learning rate used everywhere, 3 discriminators trained jointly).\n\nHowever, there is still room for improvement:\n\n1) Reproducible research and comparison to previous work :\nThe data employed in this work are from \"a wireless telecom website in service\". Could you be more specific?\nAs in Niyaz et al. (Apparently same paper as Javaid et al with different authors orderning), why did not you use public datasets? Iam not an expert in this field of application, bu it seems like the dataset they test on : \"KDD Cup 99 dataset, benchmark dataset for network\nintrusion\", would be appropriate and would allow for future approcah to compare to your work.\n\n2) Clarification of the experimental section needed:\nThe graph \\mathcal{G} is not properly define but we manage to understand that it is depicted in Figure 2 a), you should make that clearer. \nThe algorithm misses multiple symbols definitions making it difficult to understand. We finally get it but it could be easier to read with the addition of clarifications.\nWhat is J?\nwhat is the masking m?\nclarify \\lambda L\nHow is the threshold chosen?\ncross entropy, etc -> what is etc? did you try other losses? If yes, where is the comparative study?\n(must exist based on graph theory) -> could you clarify?\n\n3) The related work section may be improved:\n- You state that prior art seldom use generative models. It would be nice to add some details about the specific works using them.\n- The section begins stating that works after 2012 use deep learning are are going to be reviewed in this section. We are thus expecting related work post 2012 but it's not what is reviewed. We understand that this paragraph is about works using neural networks. But then, the next pargraph starts with \"on the other hand, the neural networks models advance...\" On which other hand? So are we back to NN based approches ? It seems not because the next cited works are about PCA and wavelets? I was really confused trying to understand this section.\nSome missing related works on GANs with classification: Odena et al, 2016 Auxilary classifier https://arxiv.org/abs/1610.09585\n\nListed contribution 3 is not a contribution: verifying that the propose approach work is an inherent part of a research paper.\n\nStability of training -> Did you try any data normalization?\n\nMinor:\np 2 Even *if* we\nIn spite of *the fact* there\nuse the generative -> use generative\nAnd they -> They\nsetting( -> setting (\nby ourselves -> here\np3\nthe generative -> generative\ntransfers->transfer\ncompose -> are composed of\nHoang et al addressed -> I would opt for a weaker word here\nthe event symbol e is never used, I would suggest to remove it\ndefine N\nto focus *on* the\nthe ... applying of -> the ... application of\np4\neq 1: put a comma after the equation, Where -> where\ndefined \\pi, and p\neq 5,7 : put the dot in the formula otherwise it goes to the next line\np5\nclassifier -> classifier output probability\nis viable -> viable\nheuristics-> heuristic\n\\lambda_S -> \\lambda_s\np7\nnoise X-> noise z?\nonly metrics -> the only metric\nfeature levels and data sources -> feature levels as described in Section 3.1, and data sources organized in 5 groups\nIt would be nice to make appear in the table 1 the worlds \"baseline\" and \"upper bound\" for the 2 first lines\np8 fig 3 d loss -> dr loss? \nfig 3 loss: overall loss",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}