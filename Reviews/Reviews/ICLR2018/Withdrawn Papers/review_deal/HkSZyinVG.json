{
    "Decision": "",
    "Reviews": [
        {
            "title": "Plausible motivation w/o enough justification; Experimental results need to be stronger ",
            "rating": "3: Clear rejection",
            "review": "This is an emergency review, as the replacement of an overdue review. \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit. This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor. \n\nHowever, the motivation above is not justified well. No theoretic results are present to support this design. Figure 4 shows the only experimental results to “support” the motivation. However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU. As a result, the motivation does not seem clear.\n\nThe shift variables seem only useful when they are not shared for different pixels. Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used). The question is if it is worth adding so many pixel-wise parameters. Moreover, the proposed formulation does not seem useful for the fully connected layer at any time. \n\nThe experiments are limited. Only the basic LeNet and another network are considered on Cifar-100. The results are not as good as the state-of-the-art. More importantly, the proposed activation functions reduce the errors only a bit (<0.5%). Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper proposes a piecewise linear activation function that is build on ELU. In general it was an OK paper and there are many to be improved.\n\n+ Novelty seems minor. In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable. The idea proposed in the paper is just a stack of \"better\" experiments. Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape?\n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated. In  section 3, the shifted version, \\delta, is abruptly proposed only based on \"results presented in 4.1\" could improve learning. This is not a professional ML paper looks like. \n\n+ Experiment not strong to support the idea. Experiments are only conducted in CIFAR100. This is obviously not enough. In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins). And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%.\n\nAlso, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR. ",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Little explanation on why proposed ShElU should work better compared to others. ",
            "rating": "1: Trivial or wrong",
            "review": "The authors propose using piecewise linear activation functions with contraints to make it continous. They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article. Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron.\n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required. Authors present no clear explanation on why the shift should result in improved performance.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}