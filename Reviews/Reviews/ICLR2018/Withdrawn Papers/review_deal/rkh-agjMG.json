{
    "Decision": "",
    "Reviews": [
        {
            "title": "Some interesting themes and tools, but the problem definition is unclear",
            "rating": "3: Clear rejection",
            "review": "Summary:\nThe submission describes a method to predict goals for robotic planning applications. The goal prediction task is formulated as a supervised learning problem, and it is learned using a encoder-decoder-type CNN. Uncertainty in the goals is modeled by making multiple predictions and using a multiple-hypothesis loss. Additionally, state (cost-to-go) values and “action priors” are predicted. The method is applied to a robot navigation task and a manipulation task.\n\nStrengths:\n+ Multimodal goal prediction is an important problem to solve in robotic planning applications\n+ Some of the results seem qualitatively interesting\n\nWeaknesses:\n+ The problem is very ill-defined\n+ Key aspects of the method are unclear\n+ It is unclear how predicting images helps to solve the problem\n+ The method is not evaluated against any comparable baselines\n\nUnfortunately, I really struggled to understand the context and motivation for this work, as well as other key aspects of the submission.  The work seems related to reinforcement learning, imitation learning, and what I would call \"learning to plan” (i.e., imitation learning where the expert is a planner); however, I cannot confidently say that the work is attempting to solve any of these problems. If the submission is proposing a novel problem, then the problem definition should really be absolutely clear. On the other hand, if the method is trying to solve one of the aforementioned problems, then that too should be made clear, and the method should be motivated with respect to existing attempts to solve that problem.\n\nOne possible interpretation of this work is that it is attempting to solve an RL problem, and the submission is really about learning a model of the environment using a particular supervised learning method, to be applied in the context of model-based RL. Parts of section 3 support this interpretation, as we see that the method learns a hidden state representation, a state transition function, and an observation function (f_dec). The statement “Our goal is to learn a set of encoders and decoders that project our available features into the latent world state h that can be used for planning,” also seems to support this interpretation. However, other aspects contradict this interpretation: since this is a continuous problem, doing RL with the learned world model is highly nontrivial, for instance. Also, the introduction mentions that the predicted goals are “connected via traditional trajectory optimization or motion planning approaches,” which presumably have no way of using the learned hidden state or transition functions.\n\nInstead, I believe that the learned MDP components are actually incidental to the task of supervised goal prediction, in the sense that only the goals, and not the learned form of the MDP, are available to the planner. If this is so, why do we need to learn the MDP at all? Indeed, it seems like this “MDP-like” latent structure is actually just proposed as some kind of way to regularize the structure of the goal prediction function. But I fail to see the motivation for this. Why should predicting a one-step transition function help to predict a long-term goal?\n\nIn retrospect, the paper seems mainly to be proposing a very simple supervised learning task (predict goals and actions from current state, action) with some very confusing and questionable semantic interpretations attached to the structure of the predictor. However, even the motivation and context for this prediction task are unclear. Why is it necessary to predict both goals and actions? If the method consists of using a motion planner to connect the goals, then the predicted actions are redundant. \n\nAnother critical point is the generation of training data for this approach. If a planner is used to generate the training data, why not just use that planner to solve the task instead of having to predict sub-goals and connect them using a motion planner? If a planner is not being used to generate the training data, then how do we define a goal? If the planning problem is nontrivial, how do we ensure that the training data reaches the end goal enough to provide useful examples? \n\nThe image-generation aspect of this work was also a great source of confusion for me, because it seems entirely unnecessary to solve the task. Presumably, the motion planner just uses the predicted end-effector position and ignores the predicted goal image, so why go through the trouble of generating it? It is unfortunately also difficult to interpret the meaning of the many images showing predicted images.\n\nThe experiments should have a comparison to some kind of baseline method. I believe that doing so would also greatly clarify the scope and purpose of this work. For example, if deep RL methods could be applied to solve the same task, then this is probably best described as a kind of RL, and it should be compared to other RL methods. If imitation learning methods are more comparable, then those should be provided as baselines.\n\nTo summarize, I believe that the problem definition and the nature and purpose of the method are insufficiently clear at this point. Currently, the submission seems to expound on some general themes and tools that might be interesting and ultimately useful for some task, but these are not brought together in a cohesive enough way to make a clear impact on the community.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "3: Clear rejection",
            "review": "This paper proposes a type of generative predictive model to generate plans for robot execution. The evaluates the generative model in terms of the training objective, but does not demonstrate the model being used for planning or control. \n\nOriginality & Significance:\nThe paper is motivated in terms of the interesting and worthwhile problem of learning to generate task plans from data. Unfortunately, the experiments in the paper do not actually demonstrate the proposed method for executing task plans. Instead, they only evaluate the generation quality of the proposed generative model. To effectively validate the approach, the paper must demonstrate how the method works in the experiments through controlling a robot.\nAs such, the main contribution of this paper is the proposed generative model architecture, which is largely a combination of existing methods: comparing an encoder-decoder model and a U-net model, using a multiple hypothesis loss function, and learning a value function. However, the value function does not appear to be learned in the experiments section. Also, note that the multiple hypothesis loss function has previously been used for predictive modeling by Fragiadaki et al. [1]. Furthermore, the evaluation of the generative model does not include a comparison to standard models such as a conditional VAE and/or conditional GAN.\nThe related work section includes a number of relevant works, but should clearly explain the difference between the prior methods and the proposed method. Currently, the last two paragraphs of the related work largely lists the prior methods, without such a discussion. There are also a number of highly-relevant related works that are missing. The paper states that \"none of these approaches provide a way to predict or evaluate possible futures\". There is significant literature in model-based RL and model-predictive control that combined learned models and planning. In terms of models that can handle raw images, there are a several methods that have planned using learned models or learned latent spaces. For example, see [2,3,4,5].\n\nClarity:\nCurrently, the problem set-up and assumptions are not clearly stated anywhere, making the method difficult to understand. For example, does the method use a reward function (e.g. to train the value function)? Does it assume demonstrations of the task? Are the high-level actions pre-defined by a human? Is the state and action space continuous or discrete? Also, it would be helpful to include an algorithm box or overview of the entire algorithm somewhere in the paper. I provide more minor feedback on clarity below.\n\nIn summary, this paper does not seem to evaluate the proposed method in the experiments. Thus, it is impossible to judge the merit of the proposed approach. The paper does evaluate the proposed generative model, which is largely a combination of prior methods, but it does not provide any comparisons to standard state-of-the-art models such as conditional VAEs or conditional GANs.\n\n[1] Motion Prediction Under Multimodality with Conditional Stochastic Networks https://arxiv.org/abs/1705.02082\n[2] Embed-to-Control https://arxiv.org/abs/1506.07365\n[3] Deep Visual Foresight for Planning Robot Motion https://arxiv.org/abs/1610.00696\n[4] Learning to Act by Predicting the Future https://arxiv.org/abs/1611.01779\n[5] Self-Supervised Visual Planning with Temporal Skip Connections https://arxiv.org/abs/1710.05268\n\n\nMore minor feedback:\n-- In the listed contributions, what is meant by \"supervised dataset\"? What are the labels? What kind of supervision? demonstrations?\n-- Is a the high-level goal, e.g. provided as input, or is it the action that the agent will take? Currently, this is not clear. \n-- section 3.3: how is success determined? how is the value function trained?\n-- In the experiments, was the data collected by a human or autonomously?\n-- In the experiment figures, the axes on all of the images should be removed for readability\n-- predictive models is terminology that is used more commonly than prospective models. Might be worth using \"predictive models\" in the introduction at least once\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but insufficient evaluation",
            "rating": "3: Clear rejection",
            "review": "Summary: This paper proposes a method for learning a high-level transition function that is useful for task planning; for example, being able to generate the state resulting from the high-level goal of `pickup(block)`.\n\nPros:\n- The idea of learning a high-level transition model (rather than a low-level transition model) is interesting and has the potential to be very useful.\n- Experiments are performed on reasonably realistic and complex tasks (albeit in simulation).\n\nCons:\n- Many of the details of the approach are not clearly explained or seem to be missing.\n- The evaluation of the approach is weak and unclear in places.\n- There is not much discussion about other current approaches to combining learning and planning.\n\nQuality and Clarity:\n\nIt is difficult to evaluate the quality of the work as there a number of details missing from the experiments. There is a good deal of text spent on discussing what seem like relatively minor architectural details (e.g. the effect of dropout and skip connections) rather than a systematic evaluation of how well the approach works across many different scenarios:\n\n- The only evaluation in the husky simulation experiment is the qualitative result given in Figure 3, which I can't even really interpret---I do not know what the \"observed goal\" is supposed to be (is it the action? if so, why is it an image and not a high-level action, which is how it is defined in Section 3?), or why the predicted destinations would be useful in solving the task. The text states that \"we also analyzed the effect of varying network architectures on the final pose error prediction\" but I couldn't find these results reported anywhere in the text.\n- There is no comparison with even the simplest baseline architecture; the text in Section 3.1 says that there was a comparison made between two alternate models but I couldn't find the results of these comparisons anywhere.\n- There is very little evaluation of whether predicting multiple hypotheses has any effect on performance. In some cases it seems there is little variance between the different hypotheses, such as in the bottom row of Figure 1.\n- Although the approach is motivated in terms of being able to do task planning, as far as I can tell there is no actual evaluation with a planning system. The paper discusses how you would use the model for planning (that you can learn V(h) and p(a|h)) and says this was done in Section 3.3, but there does not seem to be any evaluation of this. The lack of evaluation within a larger planning framework somewhat diminishes the impact as it is unclear how well this approach would work in practice.\n\nI recommend that the authors move the discussion of architectural details into an appendix and expand their evaluations in the main text.\n\nOriginality and Significance:\n\nThe paper discusses a few previous works at the intersection of planning and learning and claims that \"none of these approaches provide a way to predict or evaluate possible futures\" (pg. 2). However, there have been a number of other recent works exploring the intersection of planning and learning beyond those that the current paper cites, such as [1-3], which all explicitly involve being able to predict and evaluate possible futures. While these works focus more on learning low-level transition models rather than high-level transition models for task plans, they are quite related and should be discussed in the related work.\n\nThe idea of learning a high-level transition function is incredibly important and it is encouraging to see an exploration of how this might be done. However, given the weaknesses in the evaluation of this approach, it is difficult to say how significant the work is.\n\n\n[1] Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., … Degris, T. (2016). The Predictron: End-To-End Learning and Planning. Retrieved from http://arxiv.org/abs/1612.08810\n\n[2] Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racanière, S., … Battaglia, P. (2017). Learning model-based planning from scratch. arXiv Preprint arXiv: 1707.06170. Retrieved from https://arxiv.org/abs/1707.06170\n\n[3] Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., … Wierstra, D. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. arXiv Preprint arXiv: 1707.06203. Retrieved from http://arxiv.org/abs/1707.06203",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}