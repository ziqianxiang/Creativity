{
    "Decision": "",
    "Reviews": [
        {
            "title": "No title",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposed an approach for detecting adversarial examples using saliency maps. The key idea is exploiting saliency maps as additional inputs to detector, which reveals importance of each pixel in classification.\n\nOverall, this reviewer leans towards rejecting this paper due to its limited contribution/novelty and the incomprehensive experiment results. The proposed method simply concatenates a saliency map with the corresponding raw pixel image as an input to adversarial perturbation detector. Despite of its simplicity, there are no discussions/empirical comparisons with other approaches, and simple ablative analysis such as performance of detector with and without saliency maps.   \n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but needs a bit more work",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper describes a method for detecting adversarial examples using a second detector classifier. The key innovation here is to use pixel saliency as a channel for the adversarial example detector. Pixel saliency, in this case, is defined as the partial derivative of the output of the classification network with respect to each input pixel. Several examples are used to show that even visually similar adversarial examples can have very different saliency maps, which motivates using these saliency maps to detect adversaries. The experiments show that this approach can be used to successfully detect adversaries for several datasets, including MNIST, CIFAR10, and a small subset of ImageNet, and also investigates the robustness across different variations of attack.\n\nIn general, I think the idea of using saliency to detect adversarial attacks is very good and represents an interesting avenue of research. However, the paper as it stands seems rather preliminary, and there are several issues with the paper I think need to be addressed.\n\n1) Sec 2.2 introduces the C&W attack. However, this does not appear to be used anywhere else in the paper. It is not clear from the paper why the authors do not test their systems resiliency to this form of attack. More advanced attacks need to be considered.\n2) I would have liked to have seen an ablative study where the detectors are trained on pixels alone and directly compared with the detectors trained with saliency as an input channel. This would give a much more convincing account of the value of saliency in this context.\n3) It is easy to envision a (full knowledge) attack where the attacker knows the architecture and parameters of the detector and then devises an attack to fool both the detector and the original classifier. This could, for example, be done by optimizing a joint objective which modifies the input to both maximize error in the classifier and in the detector simultaneously, while remaining close to the original input. Unfortunately, there is no investigation of this kind of attack in the paper. Of course, it is fine if the assumption is that the attacker has no knowledge of the detector (black box case), but if this is the case, this assumption needs to be explicitly acknowledged up front and attention drawn to the obvious limitations of this approach.\n4) There is no direct comparison with other state-of-the-art in the paper. Related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful.\n5) The mathematical notation needs a bit of improvement. E.g. argmin, sgn, min, etc. should be set in roman and parentheses should be enlarged to match the size of the enclosed expressions. There is also a bit of overloading going on: at the start of 2.1 it appears f_theta refers to the true function, then later the classification model, and then in eq (4) a linear approximation of the classification model.\n6) The writing needs improvement and the paper in general needs some copy editing. There are quite a few grammatical errors and typos which, unfortunately, can make the text difficult to comprehend in parts. Captions also run into the text in a few places.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "rating": "3: Clear rejection",
            "review": "The paper adopts the concept of saliency to explain how the deep model makes decisions with adversarial perturbations. The paper is not written well and the idea seems to be trivial compared with previous methods.\n\nThe paper uses three manners of adversarial attacks and formulate saliency as the gradient that is particularly influential to the final classification output. From what I can see, there is barely new that is proposed by the paper. The experiment lacks the C&W's attack. The second paragraph of introduction listed some related work and yet failed to compare with them well. The method resembles quite a lot the Grad-CAM method and the conclusions from the experiment (\"shallow layers are robust enough to adversarial examples and middle layers ....\") seem shallow as well.\n\nThe presentation of the paper is poor. Many syntax errors/ format issues. For example, in the abstract, \"saliency simply explain how\" -> explains. Discussion section, \"how xxx contributing to wrong xxx\" -> contributes to.",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lacks comparison with other adversarial detection methods",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors propose to use methods for explaining neural network decisions (\"Saliency methods\") to detect and reject adversarial examples. They use the gradient of the image w.r.t. to the network prediction as saliency map and train a classifier to detect adversarial examples. They show that this method shows good detection performance when using some of the most common attack methods for producing adversarial examples and explore how well detectors trained on one attack method generalize to other attack methods.\n\nThe idea to detect and reject adversarial examples has seen quite some work in the last time. One thing that is crucially missing from the paper to make it's contribution clear is a comparison with other state-of-the-art methods to detect adversarial examples. Especially since there is even previous work on using saliency methods to detect adversarial examples (e.g. Fong et al, Interpretable Explanations of Black Boxes by Meaningful Perturbation, ICCV 2017).\n\nThe used saliency method (gradient based) is pretty old and many supposedly better methods have been proposed in the mean time. Why has none of those methods been used? Would it affect the performance? A comparison might make the paper much more interesting.\n\nThe same holds for the attack methods: Currently DeepFool is usally refered to as state-of-the-art attack (see e.g. https://robust.vision), but it is not included. How would black-box-attack methods do?\nin the paper.\n\nMinor points\n\n* \"Given an image x with ground truth y = f_theta(x)\": As written, y would be the model prediction.\n  But later on f_theta is derived w.r.t. to x, which doesn't work if y is the class prediction instead\n  of e.g. the probability of the true label.\n* I don't understand the discussion (which seems to be another experiment). What is alpha? Which gradient\n  are you taking?\n* The C&W attack in introduced but never used. \n* The second imagenet adversarial example in Figure 1 is visually different from the original\n  image. Usually, for VGG on imagenet it should be easy to find adversarials that are visually\n  indistinguishable from the original.\n* Many typos and syntactically wrong sentences make the paper hard to read.\n* Citations often not correctly embedded into text (\\pcite vs \\cite?)\n* Bad styling: Often the figure captions are indistinguishable from the main text, no spacing at all.\n  Also some figure captions are incomplete (Figure 2, Figure 4). References incomplete (doesn't say whether\n  Section or Figure).\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}