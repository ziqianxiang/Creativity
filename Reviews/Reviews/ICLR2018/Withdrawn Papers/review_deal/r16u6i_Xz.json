{
    "Decision": "",
    "Reviews": [
        {
            "title": "interesting idea but experiments not convincing enough",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes interactive boosting to train two complimentary networks for relative small image classification. This is an interesting idea for small sample training, although I am not sure whether it can be applied to \"really\"  small sample datasets since UIUC or LabelMe contains at least thousands of images.  \n\nThe experimental results are not very convincing, and I wish the final paper could compare with more baselines, including\n1. adapting pretrained model (e.g., from ImageNet) to the proposed problem\n2. deep CNN with dropout \n3. deep CNN with distillation\n4. deep CNN with data augmentation\nwhich all are popular techniques to apply deep network for small sample problems. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A method for building complementary networks - interesting, but many foundational questions haven't been addreesed",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes an iterative method to computing complementary weights between two networks for small datasets.\n\nAll in all, there might be something interesting in this topic, but since there's no theoretical analysis and background in the paper, it's not clear why this method should work or be better than others.\n\nComments:\n- On the theoretical front, I'm not sure what is it that is being optimised by the iterative complementary reweighting updates. What is the loss function? What would the best two models? \n- Why is the reweighting only reliant on the posteriors rather than on mistakes (i.e. labels are not being used at all in the computation of sample weights, only the posterior)? \n- Why not to ensemble all the models up the end for the prediction (i.e. what is so special about the last models that you want to only keep them - this relates to the question about what is it that is being optimised)?\n- Since the application is computer vision here, the question is that why not to use/compare to CNNs for this task?\n\n",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The proposed method is quite a lot like mixtures of experts, and this needs to be addressed",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The manuscript aims to improve the performance of neural networks on small datasets, by using an ensemble approach to reduce the variance.  This is a worthy goal, and the overall approach of using an ensemble method makes sense, based on the variance-reducing properties of bagging.  Postive results are reported on two datasets, indicating that the method may work as intended.  The experiments varying the size of the training set are also useful for exploring the authors' hypothesis.  It is appreciated that the authors made their code available.\n\nHowever, while the proposed ideas are interesting, there are concerns about the novelty in relation to a long line of previous work.  The approach assigns weights that sum to one for each instance, where the weights associate that instance with different models, and these weights are updated via an iterative procedure.  Thus, each model is allowed to specialize on a subset of the data, according to the weights.  This should immediately bring to mind the notion of a mixture model, trained via EM.  Indeed, the mixture of experts model of Jacobs et al., dating back to 1991, does exactly this in a supervised learning context, as is studied here.  This type of model has been extensively studied for well over two decades:\n\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.\nYuksel, S. E., Wilson, J. N., & Gader, P. D. (2012). Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems, 23(8), 1177-1193.\n\nI am unsure of whether the proposed updates, which are suggested heuristically, will exactly correspond to the EM algorithm for mixtures of experts (MoE).  Regardless, they are very closely related, and this needs to be addressed in the manuscript.  If they are different, the paper must demonstrate that the approach has superior performance to MoE.  If they are not, the novelty of the paper is greatly reduced, although this may help to justify the proposed approach, and could lead to new analyses.  Mixtures of experts have a model-based theoretical basis, while the present work is heuristically motivated.\n\nOnly two datasets are used in the experiments, which leaves questions on whether the results will generalize to other datasets.  Results on at least one more dataset would greatly strengthen confidence in the results.\n\nTo summarize, while the manuscript has some interesting ideas, the relationship to a long line of previous work, the mixture of experts model, is not properly addressed, which means that the paper cannot presently be accepted in its current form.  The experimental results need to be extended, as comparisons to mixtures of experts are important to include (even if the proposed approach ends up being different to that work), and only two datasets are used.  The lack of theoretical justification of the proposed approach becomes more of a concern in light of the fact that its primary competitor, mixtures of experts, has a model-based justification.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}