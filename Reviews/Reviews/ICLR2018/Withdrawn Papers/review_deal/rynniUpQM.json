{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review: Learning with mental image",
            "rating": "3: Clear rejection",
            "review": "== Summary ==\nThis paper investigated the task of few shot recognition via generative modeling. More specifically, DCGAN was used to generate a “mental image” as intermediate representation given the input image. The mental image share the same semantic information with the input image but differs in style (e.g., color, observation viewpoint). Experimental evaluations were conducted on MNIST, SVHN, and BigBIRD database.\n\n== Technical Assessment ==\nFew shot recognition with generative modeling is an interesting topic. But I feel there is not much novelty proposed in this paper and the technical details are not consistent. Additionally, I feel this paper’s experiments are poorly designed.\n\nFirst, unsupervised domain adaptation (using GAN or auto-encoder) has been explored to some extent. The proposed auto-encoding pipeline with adversarial loss is not a novel contribution.\n\nSecond, looking at in Figure 2, discriminator distinguished between input image X and generated mental image G(E(X)). But it is not clear how does the paper enforce generated mental image to be close to true mental image, as the ground-truth mental image Y is never used in algorithm 1.\n\nBased on my understanding, the pairs (input image, mental image) are fully given during training where mental image provides direct class information (e.g., the canonical view for handwritten digits). If this is true, the comparisons throughout the paper are not valid since the baseline models work in relatively unsupervised/semi-supervised manner. By using additional pairs during training, it actually uses strong class information for supervision. \n\nOverall, I believe the submission is not ready for publication in the current form. In the future, please clarify the exact training pipeline and how it is compared to baseline methods (e.g., DCGAN, Salimans et al). \n\nMissing reference:\n-- Domain Separation Networks. Bousmalis et al., In NIPS 2016.\n-- Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks. Bousmalis et al., In CVPR 2017. \n-- Unsupervised Image-to-Image Translation Networks. Liu et al., In NIPS 2017.\n-- Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis. Yang et al., In NIPS 2015.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The paper presents an adversarial auto-encoder architecture for few shot learning.\n\n+ well written\n+ exact setup is novel\n- combination of existing methods\n- no comparison or ablation\n- mental image story is a distraction\n\nThe papers is well written, and the presented architecture and setup seems novel. Unfortunately the components used in the architecture are all well known: DCGAN, auto-encoders, and reconstruction loss. It would help if the authors could highlight their contributions on their response.\n\nSecond, the results only evaluate their exact algorithm, and no ablation or prior few show learning (or unsupervised/self-supervised feature learning) approaches. For example, it would be good to know if the adversarial loss actually helps, or if an auto-encoder is all we need. Additionally, it would help if the authors compare to other unsupervised or self-supervised learning approaches, which can be used for few shot learning. Examples include Wang & Gupta 2015, Pathak etal 2016, both of which the authors cite.\n\nFinally the mental image story seems distracting. The paper would be easier to read if the authors just explain what they actually do: Train an adversarial auto-encoder. It would shorten the paper, and provide room for additional experiments.\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: Does not deliver what I thought was advertised",
            "rating": "3: Clear rejection",
            "review": "Summary:\n\nThe paper proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose. \nThe central idea is to create a feature representation from any given image of an object from which an image of the same object in a canonical pose and situation (lighting, background, …), a “mental image”, can be reconstructed. The feature representation should thus contain the relevant information about the identity of the object, but abstract away information that is irrelevant for recognition.\n\nFor a given input image the features are computed as the output of a “bottleneck” layer of an encoder network in a feed-forward manner. The loss for training the feature encoder is defined by several additional components. First a decoder network takes the features and produces an output the size of the input image. This output image is then compared to a given canonical reference image for the corresponding input image by two different loss functions. One is a simple L2 loss, the other one is an adversarial loss defined by a discriminator network trained to distinguish canonical images from the output of the decoder.\n\nThe trained feature encoder is then used to construct a classifier by computing features for a given set of training inputs on top of which an SVM is trained.\n\nGeneral assessment:\n\nPros:\n\nThe central idea seems relevant and interesting, although not all novel.\n\nCons:\n\nThe whole way the paper is written, the work it claims to be related to, the methods it compares to, seem to suggest that it is about “self-supervised” learning of generalizable feature embeddings.\n\nIn fact, it is not. It is pure supervised training. Since the task is instance recognition, the canonical image which must be provided is de facto the class label.\n\nThere is thus, a very important comparison/experiment missing: What happens if one just attached a linear layer + softmax + cross-entropy loss to the end of the encoder and trained it on the ID of the canonical images as target labels?\n\nThe necessity of framing the problem in a General Adversarial Network framework is unclear. Importantly, there is no unknown target distribution to be matched. All images of one class are supposed to be mapped to exactly one output, which is also known because it was manually specified. There is, thus, also no need to learn a similarity metric. There is no reason the network should not be required to produce exactly the ‘canonical image’. In fact, one perfectly valid solution for a trained encoder/decoder pair would be to map each input to exactly one descriptor per respective class (e.g., a one-hot encoding), from which the decoder reproduces the “canonical image” it memorized for that exact descriptor.\n\nOf course, such a representation would likely not be suitable to encode previously unseen object (categories). But nothing in the formulation prevents it from doing exactly that.\n\nOn a side note, I do not see why there a weighting necessary, between the two types of losses. They do not oppose each other. If the L2 loss gets to zero then certainly the loss from the discriminator must also be zero, because the images from both “domains” are exactly the same. If changing the weighting changes anything in the results that is just an indicator of the optimization not being very good, or the decoder network not having enough capacity to produce the canonical images.\n\nEliminating the GAN part, the paper is much closer related to auto encoders, which are mostly ignored in the discussion, and completely in the experimental evaluation.\nOne very relevant related work (at least in what the paper claims to attempt) for instance would be Kulkarni et al. “Deep Convolutional Inverse Graphics Network”, NIPS 2015.\nThere is a whole suit of related work under the topic of “disentangling representations”.\n\nGiven the above, the comparison in the experimental evaluation is insufficient. As pointed out, the real comparison is to other supervised methods.\n\nThe only experiment trying to show the ability of the learned features to generalize to anything beyond the objects it was trained on in a supervised way is in the very last paragraph. Unfortunately, but also unsurprisingly, the generalization does not work very well.\n(Also, why is it shown on a different dataset?)\n\nThe choice of just taking any arbitrary (the first) image as the “mental image” is likely suboptimal, because there is no guarantee that this is an image that is suitable for recognition. The real interesting thing would be to let the system identify or learn a canonical image which it can infer from all other views and is also a good representation for recognition.\n\nOther details:\n\nThe related work is just list and not discussed (relevance, relation, differences to proposed work).\n\nThe Bibliography needs to be formatted consistently (names of conferences/journals (full, abbreviated, ...), page numbers)\n\nSome of the details of the supplemental material could be included in the main paper, or at least referenced.\nIf a bottleneck size of 1024 delivered the best result in the supplemental material, why was this result not chosen for the main paper? \n\nWhy are all the images of the BigBIRD dataset rotated 90 deg to the left? And looking at the text, horizontally flipped?\n\nThe images generated by MIDCGAN and shown in Figure 1 demonstrate that it is trained only for recognition of known objects. How else would the decoder be able to generate the exact label on the different bottles just from looking at them from the bottom (row 1 column 3 vs row 2 column 1)?\n\nTable 2 contains almost no real comparison due to the lack of data points.\n\nMinor details: \n\nThe term “double convolutional … unit” is not really clear.\nThe second sentence in 3.2 is incomplete.\n\nThe description of the last experiment on generalization capabilities of the features is somewhat unclear. I assume that the encoder was trained on BigBIRD and then fixed to produce features to train an SVM on with labeled data from a split of RGBD. If that is the case it should be stated more clearly.\n\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}