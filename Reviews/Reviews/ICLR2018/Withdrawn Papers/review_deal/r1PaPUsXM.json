{
    "Decision": "",
    "Reviews": [
        {
            "title": "Unclear whether the considered models produce informative features.",
            "rating": "3: Clear rejection",
            "review": "The paper presents a new deep neural network architecture based on generalized hamming distance operations. A number of properties of the basic operation and compositions of them are shown. The authors further explain that the proposed architecture provides additionally some interpretation of the network with fuzzy logic.\n\nA main issue with the paper is that the error of the produced models is not reported, which makes it difficult to assess whether good representations of the data were used. Visualizations given in the appendix suggest that the produced features are not very informative.\n\nA mechanistic understanding of a network (e.g. via fuzzy logic) can be useful for neural networks composed of few neurons, but it is unclear whether the technique remains interpretable for a human when considering larger networks.",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insightful theoretical and technical paper on Generalized Hamming Networks",
            "rating": "7: Good paper, accept",
            "review": "This insightful theoretical and technical paper aims to help us interpret Generalized Hamming Networks (GHNs) using fuzzy logic, with the goal of more widespread understanding of neural networks. \n\nGHNs were introduced in Fan, 2017, which are networks that use the generalized hamming distance as a similarity metric between weights and data. A key component of GHNs is analytically setting the bias term so that every layer is performing a generalized hamming distance (GHD) operation. This has many nice fuzzy logic interpretations, and it provides a justification for not having to use ReLUs. \n\nFan, 2017 had state of the art performances on various image tasks, and the authors seek to interpret this performance. They review hamming convolutions, and give a fuzzy logic approach for discerning the importance of input pixels in classification problems. The main result is combining multiple convolutions into a single layer, which they call a deep epitome. This has a corresponding fuzzy interpretation, where we can quantify the degree of equivalence between inputs and weights.\n\nThe experiments produce interpretable results of what features the network is learning at each layer. Due to the equivalence of deep epitomes and stacked convolutions, this has the unique characteristic that the features extracted do not depend on those of the previous layer, which is a unique feature that the authors assert is worth exploring.\n\nI think this is a strong paper, not only for the theoretical equivalence given for stacked hamming convolutions and single convolutions, but also for the resulting interpretability results which allow us to visualize features without any extra optimization. \n\nThe main points of improvement are coherence. I found some of the paper difficult to follow -- a lot of the key steps are in the appendix, and I believe some of the earlier explanations can be cut out to make room for this. For example, I would recommend either cutting the beginning of the Related Work section or merging the first two sections since the first two paragraphs of Related Work are a summary of the introduction. As a disclaimer, the fuzzy logic background was unfamiliar for me, so a brief overview would've been helpful. Additionally, there were terms -- such as banks and channels -- that were unfamiliar to me, and may benefit from some more explanation. Finally, there were various typos throughout. \n\nPros: Insightful theoretical results, interpretation of experiments, large step toward understanding GHNs\nCons: Coherence/clarity\n",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Accuracy on cifar?",
            "rating": "4: Ok but not good enough - rejection",
            "review": "pros :\n- interesting topic\ncons :\n- experiments are not convincing\n- writing & clarity could be improved sometimes\n\nThe paper is a bit hard to read sometimes, because it does not refer to usual notions. \n\nFirst of all, the Equation (1) of the subsection 3.1 could be more precise. Use $\\mathbb{R}$ and not $\\mathcal{R}$ and define the \"dot\" operation. I have assumed the paper was using real valued networks and the euclidean scalar product. I do not see the interest to use the notation \"g\" if the circplus is used. The definitions 1 and 2 of 3.2 do not help the reader so maybe they could be removed..\n\nSecondly, it is shown that the operations performed by the network are associative, and equivalent to a single \"epitome convolution\". I dot not agree with (on page 7) the \"manifestation of the universal approximation\" theorem. The formulations are rigorously different (2 linear+point wise non linearity), or I do not see the link and thus a comment would be appreciated.\n\nThirdly, there is the \"data independent\" and \"data dependent\" visualisation, which correspond, if I'm correct, to a visualisation of the filters and the representations respectively. I do not understand what are the histograms of the normalised deep epitoms, or to which quantity they refer to. Similarly, I do not understand what is displayed on the Figures in the appendix: if I am correct, the equivalent {g_n} are visualised, but this is not specified and there are a lot of redundant filters, which could be commented. The figures of the appendix should be put in the main paper.\n\nI am finally very concerned with the experimental section. No classification accuracy is reported. It is important to report it, as one can not judge the technique: it is possible to reach 80% with mathematically predefined filters (wavelets) on CIFAR10, which are interpretable. It is very important to have competitive results w.r.t. classification to convince a reader that the technique is well-founded to study CNN.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}