{
    "Decision": "",
    "Reviews": [
        {
            "title": "Reasonable idea, but poorly written with critical missing literature and experimental baselines",
            "rating": "4: Ok but not good enough - rejection",
            "review": "This paper proposes a method for using Gaussian process (GP) based Bayesian optimization (BayesOpt) to tune optimization hyperparameters (namely the learning rate) in an online manner during neural net training – choosing the parameters to use at the next iteration on the fly. To combat the non-stationarity of the resulting optimization, it suggests the use of a separable GP kernel that is the product of a kernel over the hyperparameters and a kernel of the iteration number, the latter of which is taken from Swersky et al. [2014]. A further heuristic is suggested to combat introduced instabilities. Results are presented that suggest improvements over a method for manually tuning the learning rate, albeit without repeated runs to fully quantify the robustness of the approach.\n\nThe topic of the paper is interesting, timely, and of clear relevance to the ICLR community. The suggested approach seems sensible and the performance promising. Unfortunately, the paper has a number of shortcomings that make it difficult to argue for acceptance in its current form. Perhaps most critically, the paper omits much of the existing literature for online optimization of the learning rate and subsequently fails to compare to the appropriate baselines in the experiments, such that it is not possible to assess whether the approach actually provides benefits to existing approaches. The paper is also poorly written and structured, missing many key details about the algorithm. For example, the fact that the approach replaces the original problem of choosing the optimization hyperparameters with the, potentially equally difficult, one of choosing the GP hyperparameters is completely glossed over. I further have concerns about the experimental procedure, with the only the outcomes of single runs presented for each experiment and different experiments run for different numbers of iterations without justification. If it can be demonstrated that the approach provides advantages to other existing methods for adaptively setting the learning rate and the writing of the paper is improved, the work could make a solid contribution to a future venue, but in its current form, the paper falls comfortably below the acceptance threshold.\n\nIn the rest of this review, I will go into more detail on these shortfalls.\n\n\n%%%% Missing Literature and Comparisons %%%%\n\nThe biggest shortfall of this work is that it is written as if it is the first approach to optimize the learning rate on-the-fly, when in practice multiple competing approaches have been considered by, for example, Almeida et al. [1999], Baydin et al. [2017], Plagianakos et al. [2001], Schaul et al. [2013] and the references therein, some of which are in the stochastic gradient descent, rather than neural network, literature. In fact, this is not even the first approach to consider using Bayesian optimization in the context of online adaptation of stochastic gradient descent [Mahsereci and Hennig, 2015]. None of these alternative approaches are cited and thus inevitably the experiments do not provide any comparisons to the true competitors of this work. Further, many of these alternatives are far more simple to implement than the suggested approach and do not have the problem of introducing additional hyperparameters through the GP. Demonstrating advantages of these alternatives in some scenarios is thus a prerequisite for this paper to be accepted. Given its performance, simplicity, and close relationship the submitted work (because it explicitly targets the neural net training context), [Baydin et al., 2017] probably constitutes the most appropriate baseline to compare against.\n\nBeyond the missing comparisons, I still have some grievances with the experiments. Most importantly, the paper commits the cardinal sin of only reporting results for a single run. This is not really acceptable for a conference paper as it is necessary to show variability in the results to demonstrate that the given run was not simply lucky or (potentially unintentionally) cherry-picked. To convince the reader that the method works, it is necessary to show the average over multiple runs (either mean or median) along with error bars on the performance to show a statistically significant improvement. It also worries me that the number of epochs seems to vary between experiments without explanation. In many of the experiments, Hyperdyn and the competitor are actually interleaving for superiority and so, in addition to providing multiple runs, I think the experiments should be updated to a) show runs with more epochs to see the long run behavior and b) fix the number of epochs across experiments (at least within the same dataset) – choosing the stopping point after seeing the results can substantially bias the results.\n\n\n%%%% Clarity %%%%\n\nThe writing of the paper is generally sloppy with a lot of typos and other poor presentation (e.g. algorithm blocks and figures in the middle of paragraphs, inconsistent figure formats, untidy algorithm blocks, etc). One particularly infuriating thing it the use of cite (or citet) almost everywhere that citep should be used instead, meaning that cited author’s names often appear randomly in the middle of sentences, e.g. “good configurations Snoek et al. (2012) or to speed up configuration evaluations Li et al. (2016)”. A couple of mistakes like this would, of course, be fine, but when it is almost every citation it gets annoying.\n\nMore importantly, the structuring of the paper is poor in terms of the prioritization of space to different components. The first half of the paper is effectively just introduction and background, with the approach not introduced in any detail until page 5. The description of the approach is then very condensed with significant components completely glossed over. For example, what does acc represent on line 14 and why is the validation accuracy being evaluated with the hyperparameters set to 0? Line 7 of Algorithm 4 also seems critical but is never explained – does this actually mean you have to evaluate the performance twice for each iteration? It seems like the approach here might actually double the training cost (or even triple once line 14 is included)! Maybe the cost of this call is actually insignificant, but this needs to be made clear (with real timing information) and this would then also mean the later suggestion in the conclusion that improvements can be made by parallelizing these calls is hogwash. There also seems to be no details on what kernel is used on the hyperparameters (K1) including its own hyperparameters (e.g. length scale etc), upon which the BayesOpt performance will itself, ironically, be critically dependent.\n\nSome complex papers do not have space for all the algorithmic details in the main paper and have to consign some elements to the supplement. However, this is not the case here as there is no supplement and the approach is relatively straightforward to explain. Thus there is not really an excuse for providing insufficient detail. In particular, the introduction and background are needlessly long and would actually be stronger from a substantial trim. In addition to giving the full paper a purge of unnecessary sentences, my recommendation would be to shorten section 1.1 to a couple of paragraphs (moving some material to the results section if necessary), reduce the BayesOpt related work paragraph to a sentence or a most two, cut the LSTM related work paragraph (I find the link strenuous at best but that might be my misunderstanding), and cut the intro the BayesOpt and GPs down to a short paragraph (just give the key high-level idea then reference to existing work). It should thus be easy to find a full page of space, if not more, that can be instead used to explain the algorithm more carefully in Section 3 and add in missing elements of the literature review.\n\nAnother qualm I had with the clarity of the paper is that I feel the results are overstated throughout. The most obvious case of this is the sentence\n\n“As shown in Fig. 1(a)1(c), Hyperdyn tuning is always faster (at reaching superior generalization performance) than the manually tuned one [sic].”\n\nOnly two datasets have been considered, only a single evaluation shown for each experiment is given, no real-time information is provided, and even the referenced figures do not show ubiquitous improvement, so claiming Hyperdyn is “always faster” is bordering on the ridiculous. Relatedly, it is strange and a little misleading to repeatedly talk about the method speeding up training when what you are really claiming is a better performance for a limited epoch budget. Though this indirectly means that the same performance can be achieved in fewer iterations, real applications will almost\nnever have the scenario where it is both possible and desired to train until a target performance is reached. I feel like this strange performance metric was chosen with an eye to making the results seem more impressive, rather than for objective evaluation – I do not feel Table 1 provides a very accurate reflection of the actual results.\n\nRelatedly, the phrase “it is computationally efficient since it uses Gaussian processes (GP)” made me chuckle. GPs are notoriously one of the least computationally efficient methods around and the fact that their computational cost will explode as the number of iterations increases is actually a weakness of the method, while the fact that no timing information is provided is similarly a weakness of the paper.\n\nReferences\nL. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In On-line learning in neural networks, pages 111–134. Cambridge University Press, 1999.\nA. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782, 2017.\nM. Mahsereci and P. Hennig. Probabilistic line searches for stochastic optimization. In Advances In Neural Information Processing Systems, pages 181–189, 2015.\nV. Plagianakos, G. Magoulas, and M. Vrahatis. Learning rate adaptation in stochastic gradient descent. In Advances in convex analysis and global optimization, pages 433–444. Springer, 2001.\nT. Schaul, S. Zhang, and Y. LeCun. No more pesky learning rates. In International Conference on Machine Learning, pages 343–351, 2013.\nK. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw bayesian optimization. arXiv preprint arXiv:1406.3896, 2014.",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising experiments, unclear method",
            "rating": "5: Marginally below acceptance threshold",
            "review": "This paper proposes to dynamically adapt hyperparameters during an optimization run based on Bayesian optimization and additional heuristics.\nIts main claim is that it leads to better anytime performance than traditional optimizers.\n\nThe method uses Bayesian optimization to set the learning rate based on the last step, but it is somewhat unclear to me, particularly since it is not described much in text, but mostly as pseudocode (which also has several problems).\nAlgorithm 3 (Random Start) seems to be a very simple concept, not really something I would have allocated 10 lines of pseudocode to. Also in this, I don't know which units k is in: #batches? #epochs? Why is the performance of the k-1 suboptimal hyperparameters not saved and returned? \nThe pseudocode for Algorithm 4 (Hyperdyn) seems broken since Validation_Accuracy is supposed to update the weights and output validation accuracy, but it's called in several places where one would obviously want to use a cached value. Because of this I am uncertain where it is really called and where it's just cached. Algorithm 4 is also not really described in the text, and I, e.g., do not know what acc_diff_0 and acc_diff_1 are about. \nFinally, the inner workings of the heuristic Check_Accuracy_Trend are not described in the text, and I am missing intuition for the formula in its line 4 (which is the core of this procedure), and for the apparently quite important offset/bias b.\nThe bottom line is that, because of these issues, I don't really understand the algorithm. I do understand from Section 2 that the authors use the standard GP framework with standard expected improvement, and the kernel across epochs defined in the Freeze-Thaw Bayesian optimization paper. All of these are reasonable choices (although neither of them are novel). Unfortunately, the novel part (Section 3) is the one I don't understand since there are only 13 lines of text, along with almost a page of pseudocode.\n\nI see a few papers missing in the related work section: \n1) Learned Optimizers that Scale and Generalize by Wichrowska et al, ICML 2017 (http://proceedings.mlr.press/v70/wichrowska17a/wichrowska17a.pdf).\n2) Learning to Optimize Neural Nets by Li and Malik (https://arxiv.org/abs/1703.00441).\nThese two are the follow-up papers of Learning to learn and of learning to optimize, respectively, and both of them have been applied to large and standard problems now (CIFAR, ImageNet). So this is not unique about the current submission.\n3) SGDR: Stochastic Gradient Descent with Warm Restarts by Loshchilov and Hutter, ICLR 2017 (https://arxiv.org/abs/1608.03983)\nThis improves anytime performance on the CIFAR datasets by more than 5x by controlling the learning rate (compared to the 1.5x or 2x reported in Table 1). It also introduced cosine annealing, which is typically used now to obtain state-of-the-art methods for CIFAR (see, e.g. Shake-shake regularization by Gastaldi, https://arxiv.org/abs/1705.07485). An adaptive learning rate schedule would optimally perform better than that.\nThroughout, the authors are missing several citations to concepts they use, e.g., MNIST, CIFAR, ImageNet, SMAC, TPE, and Spearmint.\n\nThe experiments seem interesting, althought I am missing a few things:\nFigures 1 (a-c) seems to cleanly show improved performance compared to the training used by He et al (2015). I don't know whether that training was particularly optimal.\nFigure 2a should have a log-y value to see interesting learning rates chosen.\nFigure 1d shows that standard Bayesian optimization without Check_Accuracy_Trend seems to stagnate on this example. I don't know what random search x5 is in this context -- take 5 random steps at each step and pick the best??\nI am missing a comparison to standard Hyperband (with standard parameters), not a special variant by the authors as mentioned at the bottom of page 7. \nIn Figure 3, tuning Adam seems to work well.\nIn Figure 4, I don't understand why different batch sizes are used for the two methods; that makes them incomparable. Does this invalidate the most impressive number in Table 1?\n\nWhat do the authors mean by \"We can also exploit the temporal gains obtained by using a backtrack algorithm at the later stages of the algorithm [...]\"?\nThere are a few English issues, and all citations are inline -- please use citep for parenthetical citations (most of yours).\n\nOverall, the experimental results appear promising (albeit some baselines are missing). But unfortunately, in the current version, the algorithm desciption remains unclear, and I cannot judge whether an improvement should be expected.\n\nPros: \n- promising experimental results\n\nCons: \n- method not described well \n- method appears adhoc ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experimental setup and results are questionable",
            "rating": "4: Ok but not good enough - rejection",
            "review": "The authors use a ResNet 20 which is a small network with 0.27M parameters discussed by He at al. (2015), it demonstrated 8.75\\% error rate on CIFAR-10 in the original paper.  Note that the network architecture was improved in a set of follow-up papers. ResNet 20 was not powerful enough 2 years and is rather irrelevant today. \nUnfortunately, in contrast to the originally reported 8.75\\% , the network failed to approach 10\\% in all experiments of the current paper (see Figure 1). That result alone makes me feel that all presented CIFAR-10 results are rather irrelevant. \n\n\"Comparison to Manually Tuned Networks\"\n\nThis section does not contain enough information to understand the experimental setup. One option to compare with manually tuned networks is to consider optimal settings of hyperparameters as a baseline. This baseline is not described, it is probably not the one by He et al. which otherwise would achieve 8.75\\%. Moreover,  \n\na) He at al. used to drop learning rates, it leads to clear jumps of performance but is not the case in Figure 1. Do you schedule learning rates?\nb) AFAIK, He at al. didn't use / suggest any settings for batch size 1000 as given in Figure 1 b.\n\nHyperparameter values of the learning rate and L2-norm regularization are missing.\n\nIf I understand correctly, your method first performs Random_Start which spends some computational resources (note that you use k for solutions there and k for iterations in the paper) and then runs the remaining part of Hyperdyn which also spends some resources on computing the validation accuracy, something that SGD does not need. Overall, the use of epochs for all x-axes of the paper is misleading and time should be additionally provided.\n\nFor CIFAR-10 it is common (unfortunately) to have the validation set equal to the test set of 10k samples. Is it also the case in your setup which is based on He et al.? In that case, you essentially online optimize on / overfit to the test set. \n\n\"This suggests that it is more important to find good learning rates for a simple algorithm, rather than attempt to design a more sophisticated algorithm\"\n\nHave you tuned the learning rate for large-batch studies of SGD presented here? I wonder, because \n\na) Random_Start is essentially doing it for your approach.\nb) The network used here is hardly comparable in complexity with the ones used for large-batch studies and thus the learning rates and effects discussed there might be irrelevant here.\n\n\"beta_1 -> 0 and beta_2 is around 1, i.e. the momentum coefficient beta_1 becomes very small\"\n\nbeta_2 is not around 1 in Figure 2 B, instead it randomly fluctuates in its initial range of [0,1] with the last seen value randomly appeared to be 0 and mean value constantly being around 0.5. beta_1 decreases because the initial expected value of 0.5 is a huge learning rate and its decrease is easy to detect, a different effect would be expected if the range is chosen appropriately on log10 scale.\n\n\n\"Additionally, results in You et al. (2017) suggest that (upto 40 epochs) Hyperdyn gives the best performance.\"\n\nIt is not fair to take some convergence curve and cut in your preferred location to claim that your results are better given the cut because the author of the original curve most likely also could run her / his experiment for your cut-budget and potentially obtain a better result. ",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}