{
    "Decision": "",
    "Reviews": [
        {
            "title": "A Matrix Approximation View of NCE that Justifies Self-Normalization",
            "rating": "3: Clear rejection",
            "review": "It is hard to interpret this work as the authors do not mention the original work by Gutmann and his colleague on the NCE in the required details. Their paper provides a proof that in the non-parametric case the optimum on NCE objective function is at the data distribution with normalisation constant either learned or held fixed (0 or any value you like). What exactly is the purpose of this paper? \n\nThere are a number of minor issues as well. In language modelling we do not compute normalisation term during NCE training or testing as explicitly stated by the authors you are referring to (Chen 2016) - that is the whole point of using NCE. What is p(c) in equation 4 and where it comes from?",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesing problem but nothing really new",
            "rating": "2: Strong rejection",
            "review": "This paper considers the problem of self-normalizing models. This kind\nof approaches, such as NCE (Noise Contrastive Estimation) is very\npromising and important to provide efficient and large vocabulary\nlanguage models.\n\nBy interpreting the NCE in terms of matrix factorization allows the\nauthors to better explain this learning criterion and more\nspecifically the self-normalizing mechanism.\n\nHowever, the first (theoritical) contribution is to make the link\nbetween matrix decomposition and sampling based objective. This was\nalready shown for negative sampling in the paper of Melamud et al. in\nEMNLP 2017. Therefore, nothing new here, the difference is slight.\nMoreover, this paper is only cited in the experimental part, while the\ncontribution should be far more emphasized by the authors.\n\nThe second part makes the link with the self-normalization. This is\nnot really surprising. This was already explained in the same way in\npapers from Pihlaja,Gutmann and Hyvarinen published in 2010/12. See\nsome references below. \n\n\n\n@InProceedings{melamud-dagan-goldberger:2017:EMNLP2017,\n  author    = {Melamud, Oren  and  Dagan, Ido  and  Goldberger, Jacob},\n  title     = {A Simple Language Model based on PMI Matrix Approximations},\n  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},\n  month     = {September},\n  year      = {2017},\n  address   = {Copenhagen, Denmark},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1861--1866},\n  abstract  = {In this study, we introduce a new approach for learning language models by\n\ttraining them to estimate word-context pointwise mutual information (PMI), and\n\tthen deriving the desired conditional probabilities from PMI at test time.\n\tSpecifically, we show that with minor modifications to word2vec's algorithm, we\n\tget principled language models that are closely related to the well-established\n\tNoise Contrastive Estimation (NCE) based language models. A compelling aspect\n\tof our approach is that our models are trained with the same simple negative\n\tsampling objective function that is commonly used in word2vec to learn word\n\tembeddings.},\n  url       = {https://www.aclweb.org/anthology/D17-1198}\n}\n\n@InProceedings{pmlr-v9-gutmann10a,\n  title = \t {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},\n  author = \t {Michael Gutmann and Aapo Hyv√§rinen},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {297--304},\n  year = \t {2010},\n  editor = \t {Yee Whye Teh and Mike Titterington},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher = \t {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},\n  url = \t {http://proceedings.mlr.press/v9/gutmann10a.html},\n  abstract = \t {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}\n}\n\n\n\n@inbook{Pihlaja10Unnorm,\n\tAuthor = {M. Pihlaja and M. Gutmann and A. Hyv{\\\"a}rinen},\n\tPages = {442--449},\n\tPublisher = {AUAI Press},\n\tTitle = {A Family of Computationally Efficient and Simple Estimators for Unnormalized Statistical Models},\n\tYear = {2010}}\n",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Self-normalization results from NCE being a low rank approximation",
            "rating": "6: Marginally above acceptance threshold",
            "review": "The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.  However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words. However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution. Does the argument hold for general noise distributions ? With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof. \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising. The paper is interesting but lacks strong empirical results. It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline. ",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}