{
    "Decision": {
        "metareview": "This paper presents an interesting method for code generation using a graph-based generative approach.  Empirical evaluation shows that the method outperforms relevant baselines (PHOG).\n\nThere is consensus among reviewers that the methods are novel and is worth acceptance to ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Novel Model for Programs and Impressive Results",
            "review": "In this paper, authors propose a conditional generative model which predicts the missing expression given the surrounding code snippet. Authors represent programs as graphs and use some off-the-shelf encoder to obtain representations for all nodes. Inspired from the attribute grammar, authors augment every node in AST with two new nodes which contain inherited and synthesized information. Based on GGNN, a grammar-driven decoder is further proposed to sequentially generate the AST and the corresponding program. Authors also propose a large dataset which is built from open sourced projects. Experimental results on this dataset show that the proposed method achieves better predictive performance compared to several recent work. \n\nStrength:\n\n1, The problem this paper tries to tackle, i.e., building generative models of code, is very challenging and of great significance. \n\n2, The overall model is a novel and successful attempt to incorporate the structure information of the program into neural networks. I think it will be inspiring for other machine learning based programming applications.\n\n3, The results are very promising and impressive, especially given the large size of the proposed dataset. For example, the top 5 accuracy of predicting correct expression on unseen projects is 57%.\n\nWeakness:\n\n1, I think it would be great to provide more statistics of the proposed dataset, e.g., the average number of tokens, the average size of ASTs. \n\n2, Given the dynamic nature of the graph generation process, I am curious about the efficiency of the proposed method. It would be great to provide some run time information. Also, since recurrent networks are heavily used throughout the model, I wonder how difficult the training process is. \n\n3, It would be great to also compare the log likelihood on the test set.\n\n4, It is unclear from the paper that whether authors use a pre-trained GGNN as encoder or train the encoder end-to-end with the decoder from scratch.\n\n5, It would be great to improve figure 2 as it is not easy to read. Maybe draw another graph to illustrate the temporal evolution of AST?\n\nOverall, I think this paper has made a great progress towards neural modelling of programs and recommend it to be accepted for ICLR.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting task and dataset.",
            "review": "The paper introduces a 'code generation as hole completion' task and associated dataset, ExprGen. The authors proposed a novel extension of AST code generation which uses what they call Neural Attribute Grammars. They show the proposed method does well on this task, compared to ablations of their model (which are similar to previous AST approaches).\n\nThe task and dataset are interesting, and the comparison of the proposed method to baselines seems thorough. \n\n*Details to Improve*\nThe authors have a qualitative evaluation section describing the differences in errors made by various methods. Making this more quantitative by categorizing the errors and computing their frequency would be quite interesting.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall good ideas, but not quite ready",
            "review": "The paper proposes a code completion task that given the rest of a program, predicts the content of an expression. This task has similarity to code completion tasks in the code editor of an IDE. The paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.\n\nThe work builds on prior research by Allamanis et al. 2018b that performs such completions of single variables by picking from the variables in the scopes. The difference here is that portions of parse trees are predicted as opposed to a single variables, where the algorithm from the prior research is used to predict single variables.\n\nWriting-wise the paper is hard to read on the technical part with many unclear details and this portion needs a good amount of extra explanations. The Epsilon set includes triples which are not described and need understanding equation (2). The first element of this triple is an edge label <edge>($a$, $v$) where $a$ is an AST and $v$ is a node. Thus, edges of the graph end up between entire ASTs and nodes? While I can see how could this make sense, there is certainly lack of explanation going on here. Overall, this part is hard to parse and time-consuming to understand except at high level. Furthermore, the text has many functions without signatures and they seem to be used before they are defined (e.g. getRepresentation).\n\nTechnically, the approach also seems very similar to N3NN by Parisotto et al, ICLR 2017. There should be more elaboration on what is new here. Otherwise, the novelty of the paper really is just combining this work with Allamanis et al. 2018b.\n\nIn terms of evaluation, the task seems to be on a different set of expressions than the one explained in the exposition. How many expressions where there in the evaluation programs and how many were chosen to evaluate on and based on what criteria. It seems from the exposition that expressions with field accessed and function calls are not possible to be generated, but then some completions show method calls. How much of the full task is actually solved? In particular, several of the cited prior works solve specific problems like constants that are ignored here.\n\nThe evaluation is mostly an ablation studies of the proposed approach by removing edges from the final idea. \nBesides this, the paper also introduces a new dataset for showcasing the technique and does not report sizes and running times, essentially not answering basic questions like what is the trade-off between the different techniques. Comparison to actual prior works on similar tasks is also lacking (some TODO is left in the paper), but there is the claim that existing neural techniques such as seq2seq perform \"substantially worse\". I guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}