{
    "Decision": {
        "metareview": "\nThe authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation-like domains. The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting idea with relevance to some common settings"
    },
    "Reviews": [
        {
            "title": "Great idea, promising results, some confusing text",
            "review": "This paper proposes a new method to give exploration bonuses in RL algorithms by giving larger bonuses to observations that are farther away (> k) in environment steps to past observations in the current episode, encouraging the agent to visit observations farther away. This is in contrast to existing exploration bonuses based on prediction gain or prediction error, which do not work properly for stochastic transitions.\n\nOverall, I very much like the idea, but I found many little pieces of confusing explanations that could be further clarified, and also some questionable implementation details. However the experimental results are very promising, and the approach should be modular and slotable into existing deep RL methods.\n\nSection Introduction: I’m confused by how you can define such a bonus if the memory is the current episode. Won’t the shortest-path distance of the next observation always be 1 because it is immediately following the current step, and thus this results in a constant bonus? You explain how you get around this in practice, but intuitively and from a high-level, this idea does not make sense. It would perhaps make more sense if you used a different aggregation, such average, in which case you would be giving bonuses to observations that are farther away from the past on average.\n\nAlso, while eventually this idea makes sense, it only makes sense within a single episode. If you clear the memory between episodes, then you are relying on some natural stochasticity of the algorithm to avoid revisiting the same states as in the previous episode. Otherwise, it seems like there is not much to be gained from actually resetting and starting a new episode; it would encourage more exploration to just continue the same episode, or not clear memory when starting a new episode.\n\nSection 2.2: You say you have a novelty threshold of 0 in practice, doesn’t this mean you end up always adding new observations to the memory? In this case, then it seems like your aggregation method of taking the 90th percentile is really the only mechanism that avoids the issue of always predicting a constant distance of 1 (and relying on the function approximator’s natural errors). \n\nI do think you should rework your intuition. It seems to me what you are actually doing is creating some sort of implicit discretization of the observation space, and rewarding observations that you have not seen before under this discretization. This is what would correspond to a shortest-path distance aggregation.\n\nExperiments: I like your grid oracle, as it acts as a baseline for using PPO and provides a point of reference for how well an exploration bonus could potentially be. But why aren’t grid oracle results put into your graphs? Your results look good and are very promising.\n\nOther points:\n- The pre-training of the R-network is concerning, but you have already responded with preliminary results.\n- I do share some of the concerns other reviewers have brought up about generality beyond navigation tasks, e.g. Atari games. To me, it seems like this method can run into difficulty when reachability is not as nice as it is in navigation tasks, for example if the decisions of the task followed a more tree-like structure. This also does not work well with the fact that you reset every episode, so there is nothing to encourage an agent to try different branches of the tree every episode.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple novel idea for improving exploration in DRL",
            "review": "The main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning. The work is fairly innovative in its approach, where an episodic memory is used to store agent’s observations while rewarding the agent for reaching novel observations not yet stored in memory. The novelty here is determined by a pre-trained network that computes the within k-step-reachability of current observation to the observations stored in memory. The method is quite simple but promising and can be easily integrated with any RL algorithm.\n\nThey test their method on a pair of 3D environments, VizDoom and DMLab. The experiments are well executed and analysed. \n\nPositives:\n-\tThey do a rigorous analysis of parameters, and explicitly count the pre-training interactions with the environment in their learning curves.\n-\tThis method does not hurt when dense environmental rewards are present.\n-\tThe memory buffer is smaller than the episode length, which avoids trivial solutions.\n-\tThe idea of having a discriminator assess distance between states is interesting.\n\nQuestions and critics:\n-\tThe tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation? \n-\tMy main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics.\n-\tIt was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what “novelty” means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered?\n-\tThe DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound.\n-\tThe architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)?\n-\tHaving the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this?\n-\tThe fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode? \n-\tThe embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks.\n-\tI think that alluding that their method is similar to babies’ behaviour in their cradle is stretched at best and not a constructive way to motivate their work…\n-\tIn Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results?\n\nOverall, it is a simple and interesting idea and seems quite easy to implement. However, everything is highly dependent on how varying the environment is, how bad the exploration policy used for pre-training is, how good the embeddings are once frozen, and how k, action repeat and memory buffer size interact. Given that the experiments are all navigation based, it makes it hard for me to assess whether this method can work as well in other domains with harder exploration setups.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough motivation why crustily driven approach is in interest.",
            "review": "In this paper, the authors study the problem of exploration in RL when the reward process is sparse. They introduce a new curiosity based approach which considers a state novel if it was not visited before and is far from the visited states. They show that their methods perform better than two other approaches, one without curiosity-driven exploration and the second one a one-step curiosity-driven approach. \n\nThe paper is well-written and easy to follow. The authors motivate this work by bringing an example where the state observation might be novel but important. They show that if part of the environment just changes randomly, then there is no need to explore there as much as vanilla curiosity-driven approaches to suggest. The approach in this paper partially addresses this drawback of curiosity-driven approaches. The authors miss the point why the curiosity-driven exploration approaches as in this work are in interest. \n\nThe problem mentioned in this paper can be also solved based on efficient exploration-exploration methods where the distribution of next states is considered rather than the samples themselves. An efficient explorative/exploitative RL agent explores part of state space more if there is uncertainty in the reward and state distribution rather than not being able to predict a particular sample. In curiosity-driven approaches, if the predictability of the next state is considered, all methods are sentenced to failure in stochastic environments. The approach in this paper partially mitigate this problem but for a very specific environment setup, but still, fails if the environment is stochastic. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. \n\nThe paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem.\n\nIt would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn’t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately)\n\nAre there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area.\n\nThe Grid-Oracle result is very interesting and a contribution on it’s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this.\n\nOverall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}