{
    "Decision": {
        "metareview": "The paper proposes a new method to improve exploration in sparse reward problems, by having two agents competing with each other to generate shaping reward that relies on how novel a newly visited state is.\n\nThe idea is nice and simple, and the results are promising.  The authors implemented more baselines suggested in initial reviews, which was also helpful.  On the other hand, the approach appears somewhat ad hoc.  It is not always clear why (and when) the method works, although some intuitions are given.  One reviewer gave a nice suggestion of obtaining further insights by running experiments in less complex environments.  Overall, this work is an interesting contribution.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Nice idea with good empirical results, but ad hoc approach"
    },
    "Reviews": [
        {
            "title": "Interesting idea; lack of comparisons with current methods.",
            "review": "The author proposes to use a competitive multi-agent setting for encouraging exploration.\n\nI very much agree with most of previous reviewers, and their constructive suggestions. However, I find a major issue with this paper is the lack of baseline comparisons. The paper shows that CER + HER > HER ~ CER. I do not think CER should be compared to HER at all. CER to me attacks the exploration problem in a very different way than HER. It is not trying to \"reuse\" experience, which is the core in HER; instead, it uses 2 agents and their competition for encouraging visiting new states. This method should be compared to method that encourages exploration via some form of intrinsic motivation. There are methods proposed in the past, such as [1]/[2] that uses intrinsic motivation/curiosity driven prediction error to encourage exploration. Note that these methods are also compatible with HER. I'd suggest comparing CER with one of these methods (if not all) both with and without HER.\n\nMinor:\nIn the beginning paragraph of 3.1, the paper states: \n\"\nWhile the re-labelling strategy introduced by HER provides useful rewards for training a goal-conditioned\npolicy, it assumes that learning from arbitrary goals will generalize to the actual task goals. As such,\nexploration remains a fundamental challenge for goal-directed RL with sparse reward. We propose a relabelling\nstrategy designed to overcome this challenge.\n\"\nI think overcoming this particular challenge is a bit overstating. The method proposed in this paper is not guaranteed to address the \"fundamental challenge\" either --- i.e., why can you assume that learning from arbitrary goals that results from the dynamics of two agents will generalize to the actual task goals?\n\nI will change my rating accordingly if there are more meaningful comparisons made in the rebuttal.\n\n[1] Curiosity-driven Exploration by Self-supervised Prediction, Pathak et. al.\n[2] Large-Scale Study of Curiosity-Driven Learning. Burda et. al.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The authors propose a new method for learning from sparse rewards in model-free reinforcement learning settings. This is a challenging and important problem in model-free RL, mainly due to the lack of effective exploration. They propose a new way of densifying the reward by encouraging a pair of agents to explore different states (using competitive self-play) while trying to learn the same task. One of the agents (A) receives a penalty for visiting states that the other agent (B) also visits, while B is rewarded for visiting states found by A. They evaluate their method on a few tasks with continuous action spaces such as ant navigation in a maze and object manipulation by a simulated robotic arm.  Their method shows faster convergence (in some cases) and better performance than comparable algorithms.\n  \n\nStrengths:\nAttempts to solve a long-standing problem in model-free RL (effective exploration in sparse reward environments)\nClear writing and structure, easy to understand (except for some minor details)\nNovel, intuitive, and simple method building on ideas from previous works\nGood empirical results (better than state of the art, in terms of performance) on some challenging tasks\n\nWeaknesses:\nNot very clear why (and when) the method works -- more insight from experiments in less complex environments or some theoretical analysis would be helpful\nIt would also be useful to better understand the conditions under which we can expect this to bring significant gains and when we can expect this to fail (or not help more than other methods) \nNot clear how stable (to train) and robust (to different environment dynamics) the method is\n\n\nMain Comments / Questions:\nThe paper makes the claim that their technique “automatically generates a curriculum of exploration” which seems to be based more on intuition rather than clear experiments or analysis. I would suggest to either avoid making such claims or include stronger evidence for that. For example, you could consider visualizing the visited states by A and B (for a fixed goal and initial state) at different training epochs. Other such experiments and analysis would be very helpful.\nIt is known that certain reward shaping approaches can have negative consequences and lead to undesired behaviors (Ng et al., 1999; Clark & Amodei, 2016). Why can we expect that this particular type of reward shaping doesn’t have such side effects? Can it be the case that due to this adversarial reward structure, A learns a policy that takes it to some bad states from which it will be difficult to recover or that A & B get stuck in a cyclic behavior? Have you observed such behaviors in any of your experiments?\nDo you train the agents with using the shaped reward (from the exploration competition between A and B) for the entire training duration? Have you tried to continue training from sparse reward only (e.g. after the effect ratio has stabilized)? One problem I see with this approach is the fact that you never directly optimize the true sparse reward of the tasks, so in the late stages of training your performance might suffer because the agent A is still trying to explore different parts of the state space. \nCan you comment on how stable this method is to train (given its adversarial nature) and what potential tricks can help in practice (except for the discussion on batch size)?\nPlease make clear the way you are generating the result plots (i.e. is A evaluated on the full task with sparse reward and initial goal distribution with no relabelling?).\nIn Algorithm 1, can you include the initialization of the goals for A and B? Does B receive identical goals as A?\nIt would also be helpful to more clearly state the limitations and advantages of this method compared to other algorithms designed for more efficient exploration (e.g. the need for a resettable environment for int-CER but not for ind-CER etc.).\n\n\nMinor Comments / Questions:\nYou might consider including more references in the Related Work section that initializing from different state distributions such as Hosu & Rebedea (2016), Zhu et al. (2016), and Kakade & Langford (2002), and perhaps more papers tackling the exploration problem. \nCan you provide some intuition on why int-CER performs better than ind-CER (on most tasks) and why in Figure 1, HER + int-CER takes longer to converge than the other methods on the S maze?\nIn Figure 4, why are you not including ind-CER (without HER)?\nHave you considered training a pool of agents with self-play (for the competitive exploration) instead of two agents? Is there any intuition on expecting one or the other to perform better?\n\n\nPlots:\nWhat is the x-axis of the plots? Number of samples, episodes, epochs? Please label it.\nPlease be explicit about the variance shown in the plots. Is that the std?\nIt would be helpful if to have larger numbers on the xy-axes. It is difficult to read when on paper.\nCan you explain how you smoothed the curves -- whether before or after taking the average and perhaps include the min and max as well. I believe this could go in the Appendix.\n\nNotation:\nI don’t understand the need for calling the reward r_g instead of r. I believe this introduces confusion since the framework already has r taking as argument the goal g (eq. 1) while the g in the subscript doesn’t seem to refer to a particular g but rather to a general fact (that this is a reward for a goal-oriented task with sparse reward, where the goals are a subset of the states) (eq. 4)\nPlease use a consistent notation for Q. In sections 2.1 and 2.2, at times you use Q(s,a,g), Q(a,s,g) or Q(s,a).\n\nTypos:\nPage 6, last paragraph of section 4.1: Interestingly, even the … , is enough to support …\nPage 7, last paragraph of section 4.3: Interestingly, … adversely affects both ...\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "To address the sparse reward problems, the authors propose a relabeling strategy called Competitive Experience Reply (CER).  This strategy relabels states, and places learning in the context of an exploration competition between a pair of agents.  The experiments support some parts of authors’ claim well.  However, the experiments are insufficient. ",
            "review": "The authors propose a states relabeling strategy (CER) to encourage exploration in RL algorithms by organizing a competitive game between a pair of agents. \nTo verify their strategy, they extend MADDPG as their framework. Then, they compare the performance of agents trained with HER, and both variants of CER, and both variants of CER with HER. The experiments show that CER can improve the performance of HER with faster converge and higher accuracy.\n\nMy major concerns are as follows.\n1.\tThe authors may want to conduct more experiments to compare CER with other state-of-the-art methods such as PPO[1]. As illustrated in Figure 1, the performance of HER is better than that of CER. The authors may want to analyze whether CER strategy alone could properly address the sparse reward problems, and why CER strategy can improve HER. The authors have mentioned that CER is “orthogonal” to HER. I suggest authors provide more discussions on this statement. \n2.\tThe authors may want to improve the readability of this paper. \nFor example, in Figure 1, the authors may want to clarify the meanings of the axes and the plots. \nThe results shown in Figure 3 are confusing. How can the authors come to the conclusion that the optimal conﬁguration requires balancing the batch sizes used for the two agents? \nTo better illustrate the framework of CER, the authors may want to show its flow chart.\n3.\tThere are some typos. For example, in Section 2.1, the authors use T(s’|s,a) without index t; in Section 2.2, the authors use both Q(a,s,g) and Q(s,a,g). \nThere is something wrong with the format of the reference (“Tim Salimans and Richard Chen … demonstration/, 2018.”) in the bottom of page 10.\n\n[1] Schulman J, Wolski F, Dhariwal P, et al. Proximal Policy Optimization Algorithms[J]. 2017.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "clear simple idea and good results",
            "review": "The paper is well written and easy to read. Exploration is one of the fundamental problems in RL, and the idea of using two agents for better exploration is interesting and novel. However, an explanation of the intuition behind the method would be useful. The experimental results show that the method works well in complex tasks. Since states are compared to each other in L2 distance, the method might not generalize to other domains where L2 distance is not a good distance metric.\n\nPros:\n- well written\n- a simple and novel idea tackling a hard problem\n- good results on hard tasks\n\nCons: \n- an explanation of why the method should work is missing\n- plot text is too small (what is the unit of X-axis?)\n\nQuestions:\n- what is the intuition behind the method?\n- during training, randomly sampled two states are compared. why it is a good idea? how the replay buffer size will affect it?\n- since it is a two-player game, is there anything you can say about its Nash equilibrium? \n- why A is better than B at the task?\n- when comparing states, are whole raw observations (including velocity etc.) used?\n- section 4.2 doesn't seem to be that relevant or helpful. is it really necessary? \n- fig 4 is missing CER alone results? why is that? it doesn't work by itself on those tasks? ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}