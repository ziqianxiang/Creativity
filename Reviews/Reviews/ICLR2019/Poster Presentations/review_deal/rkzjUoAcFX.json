{
    "Decision": {
        "metareview": "The paper benchmarks three strategies to adapt an existing TTS system (based on WaveNet) to new speakers.\n\nThe paper is clearly written. The models and adaptation strategies are not very novel, but still a scientific contribution. Overall, the experimental results are detailed and convincing. The rebuttals addressed some of the concerns.\n\nThis is a welcomed contribution to ICLR 2019.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Limited novelty but sound experimental work"
    },
    "Reviews": [
        {
            "title": "nice paper, publish",
            "review": "This paper presents an approach to customize or adapt a text-to-speech synthesis system to a new speaker, given relatively small amount of data from that speaker.  It is a very well written paper with rather strong results indicating high quality, naturalness, and similarity with real speech from a speaker can be achieved with the authors' proposed approach.  I think the paper should be accepted for presentation at the conference.\n\nFew comments:\na) In second equation in Section 2 authors state speaker identity “s” is part of conditioning inputs “h” but it is not shown in the Equation where “h” is replaced with “l, f_0”\nb) In related work, I think the speaker code work of Abdel-Hamid et al., e.g. Ossama Abdel-Hamid, Hui Jiang, “Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code,” ICASSP 2013 is worth citing.\nc) The result that synthesized speech performs better than real speech in speaker verification task is interesting.  To me this points to a potential weakness in the verification methodology.  Please comment if this may be the case.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Wavenet solution to speaker adaptation - accept ",
            "review": "This paper proposes  an adaptation technique for TTS using wavenet as the speech backend, with the adaptation carried out on small data. The work is extremely significant in that speech data is hard to produce  (we need many hours of speaker data), and techniques to adapt (transfer learning?) data from large networks would be quite valuable. The main idea is that we train a network containing a large amount of data, and (assuming that we have a trained model), we adapt this network to the task of generating speech from text for a much smaller dataset. \n\nIn general (insofar as we can use that term), one trains such a network using <text/speech> pairs,  with speaker conditioning as added input so as to produce voice from a given speaker. The input text is converted to linguistic features in the ‘front end’, which is then injected with voice features to be synthesized into a voice output in the backend. More recent efforts in speech modeling have used RNN or wavenet based systems to carry out these transformations in the front/backends. The present work seems to use an SPSS technique (Zen et al 2016) to generate the linguistic features, while the task of converting to voice is carried out by a Wavenet. \n\nThe work is quite (conceptually) similar to \"Neural voice cloning with a few samples\" (Arik et al, https://arxiv.org/abs/1802.06006) in proposing techniques for few shot adaptation described below but with the significant difference that the latter used autoregressive DNNs (loosely speaking, seq2seq a la Tacotron) for the task in both the front and back ends, while in the current work, the linguistic features are computed with SPSS as in Zel et al (2016).\n\nThe paper proposes three quite related techniques for adaptation as shown clearly in Figure 2 of the paper. These techniques are again ‘roughly’ analogous to those described in the Baidu work “Neural Voice Cloning with a few samples”, with the difference in front and backend setups noted in the previous paragraph.\n\nWe take as text as input, and convert them to a representation for linguistic features as described in Zen et al (2016). To this, we now add the fundamental frequency F_0 for the sample voice. The key piece needed is the speaker embeddings (a vector), which is to be obtained by training. In addition to all this, we also have available the weights of a trained wavenet network (probably quite large) trained on many speakers, which we will modify (or not) using the strategies outlined for the few data dataset.\n\nSEA-EMB - Train embeddings, but not the network. We expect this to be ‘fast’, but not particularly accurate. \nSEA-ALL - Train embeddings, and network. This would be a much more accurate, if slower task. The authors note that since we train a very large network in this case, it could be prone to overfitting. They employ early stopping (as a practitioner, I would make note of the issue) with 10 % of the dataset being held out. Additional ideas such as initializing the emeddings - possibly with those that SEA-EMB calculates - are also stated to be useful.\nSEA-ENC - In this third version, they predict speaker embeddings from the trained larger network (the recipe is provided in the appendix). This task of predicting speaker embeddings is one of training a classifier.\n\n\nResults\nThe paper presents evaluations conducted with subjective, MOS based enrolment and with an evaluation metric from TI-SV d-vectors. Comparisons are made for all three models with human evaluated MOS scores, and it is seen that SEA-ALL outperforms the other two models, while performance in SEA-EMB depends on the amount of data used. Nevertheless, humans are still able to detect the difference between synthetic voices and real samples. \n\nThe TI-SV evaluations from Wan et al show t-SNE embeddings of ‘clusters’ of d-vectors for human and synthetic voices, where it is seen that inter-cluster distance (i.e. between different speakers) is high, showing that the model is able to discern speakers, and the intra-cluster distance (i.e. between real and synthetic voices) is low, showing that synthetic voices are ‘similar’ to real voices. In addition, three other measures - cosine similarity, and statistical measures for detection error trade off, ROC curves and cosine similarity measures are also presented, which show that that the adaptation models perform quite well. \n\n\nClarifications and comments:\n\nHave there been efforts to compare this model (with the SPSS based frontend) with seq2seq (Bahdanau/transformer) DNN based systems as in “Neural Voice cloning with few samples”?. How do they compare (is it even a valid comparison?)?\n\nI think the model for computing linguistic features could be elaborated upon further. \n\nRepresentations: I assume that the output audio representation is an audio waveform\n\nTypo 1 (minor): The reference  for “Bornschein et al” in section 4 “Related work”\n“Variable inference for memory addressing”. \nCorrection “Variational memory addressing in generative models”\n\nTypo 2 (minor): Figure 6: Lower curve indicate that the verification system is having a harder time distinguishing real from generated samples. \nCorrection (minor): Lower curve “indicates” ...\n\nSummary\n-------------\nIn summary, I am in favor of accepting this paper as it proposes a solution to adapt a trained network to one with has limited number of samples. A big issue in speech modeling is that datasets are tiny, and it is difficult to obtain good quality data at reasonable cost. It would be extremely useful to have a trained network that we can adapt for our own experiments. The related paper by Arik et al (Neural Voice cloning with a few samples) also operates with similar strategic aims, but uses a a different methodology using attention based DNNs. The paper under review should be a good addition to the toolbox of few shot adaptation/transfer learning for speech with much potential for practical use. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good work; limited novelty but solid results",
            "review": "This paper investigates speaker adaption with a few samples based on an existing (pre-trained) multi-speaker TTS system. The three approaches in this paper are almost the same as the voice cloning work in Arik et al. (2018). However, it is still very beneficial to demonstrate these approaches for linguistic feature conditioned WaveNet.\n\nDetailed comments:\n\n1) This manuscript is not self-contained, as it omits the important details for acquiring linguistic features (e.g., phoneme duration model) and fundamental frequency (F0) at training and test time. The only information is that it uses existing model (Zen et al., 2016) to predict linguistic features and F0. What type of linguistic features are used in this work? Is the existing model (Zen et al., 2016) trained on the same training set as WaveNet model?\n\n2) It seems the only speaker-dependent part of the system is the embedding table for WaveNet. Actually, both linguistic features (e.g., phoneme duration) and fundamental frequency sequence are highly speaker-dependent. The authors normalize F0 to make it as speaker-independent as possible. What about the speaker-dependent linguistic features? Why not keep them as speaker dependent, and do speaker-adaption for the new speaker at inference?\n\n3) In my opinion, it’s a bit superfluous to name fine tuning as non-parametric few-shot adaption, and auxiliary network (speaker encoding) as parametric few-short adaption. Both ideas are quite natural as in Arik et al. (2018).\n\n4) The abbreviations SEA-ALL, SEA-EMB and SEA-ENC are appeared without explanation. \n\n5) It would be better to provide more details about early termination criterion in Section 3.1. Is it simply the validation loss?\n\n6) In Table 1, the MOS from Arik et al. (2018) and Jia et al. (2018) are not comparable. The experimental settings are different. Perhaps more importantly, these MOS evaluations are done by different group of people.\n\n7) In Section 5.3, Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.\n\nOverall, this is a good work with limited novelty but solid results. However, it can be improved in many ways as detailed  in previous comments. I would like to raise my rating if these comments can be addressed properly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}