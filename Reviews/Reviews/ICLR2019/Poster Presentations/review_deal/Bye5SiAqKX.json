{
    "Decision": {
        "metareview": "The method presented here adapts an SGD preconditioner by minimizing particular cost functions which are minimized by the inverse Hessian or inverse Fisher matrix. These cost functions are minimized using natural (or relative) gradient on the Lie group, as previously introduced by Amari. This can be extended to learn a Kronecker-factored preconditioner similar to K-FAC, except that the preconditioner is constrained to be upper triangular, which allows the relative gradient to be computed using backsubstitution rather than inversion. Experiments show modest speedups compared to SGD on ImageNet and language modeling.\n\nThere's a wide divergence in reviewer scores. We can disregard the extremely short review by R2. R1 and R3 each did very careful reviews (R3 even tried out the algorithm), but gave scores of 5 and 8. They agree on most of the particulars, but just emphasized different factors. Because of this, I took a careful look, and indeed I think the paper has significant strengths and weaknesses. \n\nThe main strength is the novelty of the approach. Combining relative gradient with upper triangular preconditioners is clever, and allows for a K-FAC-like algorithm which avoids matrix inversion. I haven't seen anything similar, and this method seems potentially useful. R3 reports that (s)he tried out the algorithm and found it to work well. Contrary to R1, I think the paper does use Lie groups in a meaningful way.\n\nUnfortunately, the writing is below the standards of an ICLR paper. The title is misleading, since the method isn't learning a preconditioner \"on\" the Lie group. The abstract and introduction don't give a clear idea of what the paper is about. While some motivation for the algorithms is given, it's expressed very tersely, and in a way that will only make sense to someone who knows the mathematical toolbox well enough to appreciate why the algorithm makes sense. As the reviewers point out, important details (such as hyperparameter tuning schemes) are left out of the experiments section.\n\nThe experiments are also somewhat problematic, as pointed out by R1. The paper compares only to SGD and Adam, even though many other second-order optimizers have been proposed (and often with code available). It's unclear how well the baselines were tuned, and at the end of the day, the performance gain is rather limited. The experiments measure only iterations, not wall clock time. \n\nOn the plus side, the experiments include ImageNet, which is ambitious by the standards of an algorithmic paper, and as mentioned above, R3 got good results from the method.\n\nOn the whole, I would favor acceptance because of the novelty and potential usefulness of the approach. This would be a pretty solid submission of the writing were improved. (While the authors feel constrained by the 8 page limit, I'd recommend going beyond this for clarity.) However, I emphasize that it is very important to clean up the writing.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "clever idea, but experiments and writing need much improvement"
    },
    "Reviews": [
        {
            "title": "explanation could use more work, but a solid idea that seems to work in practice",
            "review": "Author proposes general framework to use gradient descent to learn a preconditioner related to inverse of the Hessian, or the inverse of Fisher Information matrix, where the inverse may take a particular form, ie, Kronecker-factored form like in KFAC. I have tracked down the implementation of this method by author from earlier paper Li 2018 and verified that it works and speeds up convergence of convolutional networks in terms of number of iterations needed. In particular, Kronecker Factored preconditioner using approach in the paper worked better in terms of wall-clock time on MNIST LeNet5, comparing against an existing PyTorch implementation of KFAC from CÃ©sar Laurent.\n\n\nSome comments on the paper:\n\nSection 2\nThe key seems to be equation 8. The author provides loss function, the minimum is what is achieved by inverse of the Hessian. Given the importance of the formula, it feels like proof should be included (perhaps in Appendix).\n\nJustification of the criterion is relegated to earlier work in Li (https://arxiv.org/pdf/1512.04202.pdf), but I failed to fully grasp the motivation. There are simpler criteria being introduced, such as criterion 1, equation 17, which simply minimizes the difference between predicted gradient delta and observed, why not use that criterion?\n\nThe justification is given that using inverse Hessian may \"amplify noise\", which I don't buy. When using SGD to solve least-square regression, dividing by Hessian does not have a problem of amplifying noise, so why is this a concern here?\n\n\nSection 3\n\nThe paper should make it clear that empirical Fisher matrix is used, unlike \"unbiased estimate of true Fisher\" which used in many natural gradient papers.\n\nSection 4\nIs \"Lie group\" used anywhere in the derivations? It seems the same algebra holds even without that assumption. The motivation for using \"natural gradient for learning Q\" seems to come from Amari. I have not read that paper, how important it is to use the \"natural\" gradient for learning Q? What if we use regular gradient descent for Q?\n\nSection 7\nFigure 1 showed that Fisher-type criterion didn't work for toy problem, it would be more informative if it used square root of Fisher-type criterion. The square root comes out of regret-analysis (ie, AdaGrad uses square root of gradient covariance)\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Possibly interesting ideas but badly presented and justified, and with poor experimental design",
            "review": "This paper proposes a preconditioned SGD method where the preconditioner is adapted by performing some type of gradient descent on some secondary objective \"c\".  The preconditioner lives in one of a restricted class of invertible matrices (e.g. symmetric, diagonal, Kronecker-factored) constituting a Lie group (which is where the title comes from). \n\nI think the idea of designing a preconditioner based on considerations of gradient noise and as well as the Hessian is interesting. However most of that work was done in the Li paper, and including the design of \"c\".  This paper's contribution seems to be to work out some of the details for various restricted classes of matrices, to construct a \"Fisher version\" of c, and to run some experiments. \n\nThe problem is that I don't really buy the original motivation for the \"c\" function from the Li paper, and the newer Fisher version of c proposed in this paper doesn't seem to have any justification at all.  I also find that the paper in general doesn't do a good job of explaining its various choices when designing the algorithm.  This could be somewhat forgiven if the experimental results were strong, but unfortunately they are too limited, and marred by overly-simplistic baselines that aren't properly tuned.\n\n\nMore detailed comments below\n\nTitle:\n\nI think the title is poorly chosen.  The paper doesn't use Lie groups or their properties in any significant way, and \"learning\" is a bad choice of words too, since it involves generalization etc (it's not merely the optimization of some function).  A better title would be \"A general framework for adaptive preconditioners\" or something.\n\nIntro:\n\nCitation of Adagrad paper is broken\n\nThe literature review contained in the intro needs works. I wouldn't call methods like quasi-Newton methods \"convex optimization methods\".  Those algorithms were around a long time ago before \"convex optimization\" was a specific topic of study and are probably *less* associated with the convex optimization literature than, say, Adagrad is. And methods like Adagrad aren't exactly first-order methods either. They use adaptively chosen preconditioners (that happen to be diagonal) which puts them in a similar category to methods like LBFGS, KFAC, etc.\n\nIt's not clear at this point in the paper what it means for a preconditioner to be \"learned on\" something.  \n\nSection 2:\n\nThe way you discuss quadratic approximations is confusing.  Especially  the sentence \"is the sum of approximation error and constant term independent of theta\" where you then go on to say that a_z does depend on theta.  I know that this second theta is the \"current theta\" separate from the theta as it appears in the formula for the approximation but this is really sloppy. Usually people construct the quadratic approximation in terms of the *change in theta* which makes such things cleaner.\n\nYou should explain how eqn 8 was derived since it's so crucial to everything that follows.  Citing a previous paper with no further explanation really isn't good enough here.  Surely with all of the notation you have already set up it should be possible to motivate this criterion somehow. The simple fact that it recovers P = H^-1 in the noiseless quadratic case isn't really good enough, since many possible criteria would do the same.\n\nI've skimmed the paper you cited and their justification for this criterion isn't very convincing.  There are other possible criteria that they give and there doesn't seem to be a strong reason to prefer one over the other.\n\n\nSection 3:\n\nThe way you define the Fisher information matrix corresponds to the \"empirical Fisher\", since z includes the training labels.  This is different from the standard Fisher information matrix.\n\nHow can you motivate doing the \"replacement\" that you do to generate eqn 12? Replacing delta theta with v is just notation, but how can you justify replacement of delta g with g + lambda v?  This isn't a reasonable approximation in any sense that I can discern. Once again this is an absolutely crucial step that comes out of nowhere.  Honestly it feels contrived in order to produce a connection to popular methods like Adam.\n\nSection 4: \n\nThe prominent use of the abstract mathematical term \"Lie group\" feels unnecessary and like mathematical name-dropping. Why not just talk about certain \"classes\" of invertible matrices closed under standard operations (which would also help people that don't know what a Lie group is)?  If you are going to invoke some abstract mathematical framework like Lie groups it needs to actually help you do something you couldn't otherwise. You need to use some kind of advanced Theorem for Lie groups. \n\nWithout knowing the general form of R equation 18 is basically vacuous. *any* matrix (in the same class) could be written this way.\n\nI've never heard of the natural gradient being defined using a different metric than the Fisher metric.  If the metric can be arbitrary then even standard gradient descent is a \"natural gradient\" too (taking the Euclidean metric).  You could argue for a generalized definition that would include only parametrization independent metrics, but then your particular metric wouldn't obviously work.\n\n\nSection 6:\n\nRather than comparing to Batch Normalization you would be better off comparing to the old centering and normalization work of Schraudolph et al which the former was based on, which is actually a well-defined preconditioner.\n\nSection 7: \n\nYou really need to sweep over the learning rate parameters for optimizers like SGD with momentum or Adam.   Otherwise the comparisons aren't very interesting. \n\n\"Tikhonov regularization\" should just be called L2-regularization\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Solid work",
            "review": "The authors suggest and analyse two types of preconditioners for optimization, a Newton type and a Fisher type preconditioner.\n\nThe paper is well written, the analysis is clear and the significance is arguably given. The authors run their optimizers on a synthetic benchmark data set and on imagenet.\nThe originality is not so high as the this line of research exists for long. \nThe \"Lie\" in the title is (technically correct, but) a bit misleading, as only matrix groups were used.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}