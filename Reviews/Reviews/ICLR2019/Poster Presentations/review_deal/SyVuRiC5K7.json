{
    "Decision": {
        "metareview": "As far as I know, this is the first paper to combine transductive learning with few-shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \\sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A new transductive few-shot learning algorithm with strong empirical results"
    },
    "Reviews": [
        {
            "title": "interesting empirically",
            "review": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.  Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. \n\nThe merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results.\n\nThe drawbacks  of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.  (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel idea, but important details and deeper analysis are missing",
            "review": "Summary\nThis paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.  The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. \n\nPros. \n-This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. \n-The empirical results show improvement over the baselines, which are expected. \n\nCons.\n-Some technical details  are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\n-Does episode training help label propagation? How about the results of label propagation without the episode training? \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Transductive few-shot by meta-learning to propagate labels for . Solid work.",
            "review": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. \n\nThere is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. \n\nComments: \n-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \n-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\n-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}