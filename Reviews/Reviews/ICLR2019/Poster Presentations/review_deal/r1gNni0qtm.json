{
    "Decision": {
        "metareview": "AR1 finds that extension of the previously presented ICLR'18 paper are interesting and sufficient due to the provided analysis of universality and depth efficiency. AR2 is concerned with the lack of any concrete toy example between the proposed architecture and RNNs. Kindly make an effort to add such a basic step-by-step illustration for a simple chosen architecture e.g. in the supplementary material. AR3 is the most critical (the analysis TT-RNN based on the product non-linearity done before, particular case of rectifier non-linearity is used, etc.)\n\nDespite the authors cannot guarantee the existence of corresponding weight tensor W in less trivial cases, the overall analysis is very interesting and it is the starting point for further modeling. Thus, AC advocates acceptance of this paper. The review scores do not indicate this can be an oral paper, e.g. it currently is unlikely to be in top few percent of accepted papers. Nonetheless, this is a valuable and solid work.\n\nMoreover, for the camera-ready paper, kindly refresh your list of citations as a mere 1 page of citations feels rather too conservative. This makes the background of the paper and related work obscure to average reader unfamiliar with this topic, tensors, tensor outer products etc. There are numerous works on tensor decompositions that can be acknowledged:\n- Multilinear Analysis of Image Ensembles: TensorFaces by Vasilescou et al.\n- Multilinear Projection for Face Recognition via Canonical Decomposition by Vasilescou et al.\n- Tensor decompositions for learning latent variable models by Anandkumar et al.\n- Fast and guaranteed tensor decomposition via sketching by Anandkumar et al.\n\nOne good example of the use of the outer product (sums over rank one outer products of higher-order) is paper from 2013. They perform higher-order pooling on encoded feature vectors (although this seems to be the shallow setting) similar to Eq. 2 and 3 (this submission):\n- Higher-order occurrence pooling on mid-and low-level features: Visual concept detection by Koniusz et al. (e.g. equations equations 49 and 50 or 1, 16 and 17 realize Eq. 3 and 13 in this submission)\n- Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection (similar follow-up work)\n\nOther related papers include:\n- Long-term Forecasting using Tensor-Train RNNs by Anandkumar et al.\n- Tensor Regression Networks with various Low-Rank Tensor Approximations by Cao et al.\n\nOf course, the authors are encouraged to cite even more related works.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Solid work + references should be extended."
    },
    "Reviews": [
        {
            "title": "The paper analyze connection between RNN and TT decomposition by incorporating nonlinearity. The theoretical results are very interesting while novelty is limited. ",
            "review": "This paper extends the work of TT-RRN [Khrulkov et al., 2018] to further analyze the connection between RNN and TT decomposition by incorporating generalized nonlinearity, i.e., RELU, into the network architectures. Specifically, the authors theoretically study the influence of generalized nonlinearity on the expressivity power of TTD-based RNN, both theoretical result and empirical validation show that generalized TTD-based RNN is more superior to CP-based shallow network in terms of depth efficiency. \nPros:\n1. This work is theoretically solid and extend the analysis of TT-RNN to the case of generalized nonlinearity, i.e. ReLU.\n\n2. The paper is well written and organized.\n \nCons:\n1. The contribution and novelty of this paper is incremental and somehow limited, since the analysis TT-RNN based on the product nonlinearity already exists, which make the contribution of this paper decreased.\n\n2. The analysis is mainly on the particular case of rectifier nonlinearity. I wonder if the nonlinearities other than the RELU hold the similar properties? The proof or discussion on the general nonlinearities is missing.\n\nOther comments:\n1. The authors said that the replacement of standard outer product with its generalized version leads to the loss of conformity between tensor networks and weight tensors, the author should clarify this in a bit more details.\n\n2. The theoretical analysis relies on grid tensor and restricts the inputs on template vectors. It is not explained why to use and how to choose the those template vectors in practice?\n\n3. A small typo: In Figure 2, ‘m' should be ‘M'",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper that would benefit from a meaningful and concrete toy example.",
            "review": "This paper would benefit from a meaningful and concrete toy example. \n\nThe toy example from section 3.1 eq(3) amounts to stating that the Kronecker product  of a set of eigenparts is equal to the PCA eigenpixels for  a set of complete images, or more generally that the kronecker product of a set of  image-part feature tensors is equal to the complete image-feature tensor  (assuming no pixel overlap).  Sure.  What does that buy?   Hierarchical Tucker (including Tensor Train) does indeed compute the standard Tucker mode representation of a tensor in an efficient manner using a set of sequential SVDs rather than using a single SVD per mode.   Is there anything else?  Depending on how the data is organized into a data tensor, the object representation and its properties can differ dramatically.  Section 3.1 needs further clarification.\n\nQuestions:\n1. What is data tensor organization and what tensor decomposition model are you using? Tucker but implemented as a TT? \nWhat is the resulting object representation?\n2. In the context of the toy example, please give a concrete mapping between your tensor decomposition (object representation)  and RNN.\n\nThe rest of the paper is a lot of mathematical manipulation which looks correct.\n\n\nPlease reference the first papers to employ tensor decompositions for imaging.\n\nM. A. O. Vasilescu, D. Terzopoulos, \"Multilinear Analysis of Image Ensembles: TensorFaces,\"  Proc. 7th European Conference on Computer Vision (ECCV'02), Copenhagen, Denmark, May, 2002, in Computer Vision -- ECCV 2002, Lecture Notes in Computer Science, Vol. 2350, A. Heyden et al. (Eds.), Springer-Verlag, Berlin, 2002, 447-460. \n\nM.A.O. Vasilescu, \"Multilinear Projection for Face Recognition via Canonical Decomposition \",  In Proc. Face and Gesture Conf. (FG'11), 476-483.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good incrementally theoretical paper with supporting experimental results. The presentation could be improved (see comments).",
            "review": "The authors extend the theoretical results of a paper previously presented in the last edition of ICLR (2018), where it was demonstrated that Recurrent Neural Network can be interpreted as a tensor network decomposition based on the Tensor-Train (TT, Oseledets et al, 2011).\nWhile previous results covered the multiplicative nonlinearity only, the contribution of the current paper is the extension of the analysis of universality and depth efficiency (Cohen et al, 2016) to different nonlinearities, for example ReLU (Rectified Linear Unit), which is very important from the practical point of view.\nThe paper is well written and have a good structure. However, I found that some deep concepts are not well introduced, and maybe other more trivial results are discussed with unnecessary details. The following comments could help authors to improve the quality of presentation of their paper:\n-\tSection 3.1 (Score Functions and Feature Tensor) is a bit short and difficult to read. \no\tMaybe, a more motivating introduction could be included in order to justify the definition of score functions (eq. 2). \no\tIt would be also nice to state that, according to eq. (3), the feature tensor is a rank-1 tensor. \no\tI would suggest moving the definition of outer product to the Appendix, since most readers know it very well.\no\tIt is said that eq. 2 possesses the universal approximation property (it can approximate any function with any prescribed precision given sufficiently large M). It is not clear which is the approximation function.\n-\tA Connection with Tensor-Ring (TR) format, if possible, could be helpful: It is known that TR format (Zhao et al, 2016, arXiv:1606.05535), which is obtained by connecting the first and last units in a TT model, helps to alleviate the requirement of large ranks in the first and last the core tensors of a TT model reaching to a decomposition with an evenly distributed rank bounds. I think, it would be interesting to make a connection of RNN to TR because the assumption of R_i < R for all i becomes more natural. I would like to see at least some comment from the authors about the applicability of TR in the context of analysis of RNN, if possible. Maybe, also the initial hidden state defined in page 5 can be avoided if TR is used instead of TT.\n-\tFig 2 shows that Test accuracy of a shallow network (CP based) is lower and increases with the number of parameters approaching to the one for RNN (TT based). It would be necessary to show the results for an extended range in the number of parameters, for example, by plotting the results up to 10^6. It is expected that, at some point, the effect of overfitting start decreasing the test accuracy.\n-       When scores functions are presented (eq. 2) it is written the term \"logits\" between brackets. Could you please clarify why this term is introduced here? Usually, logit of a probability p is defined as L(p)=p/(1-p). What is the usage of this term in this work? \n-      I think the theory is presented for a model with the two-classes only but used for multiple classes in the experimental sections. It should be necessary to make some comment about this in the paper.\n-      Details about how the RNN based on TT is applied must be added. More specifically, the authors should provide answers to clarify the following questions: \n(i) Are patches overlapped or non-overlapped? \n(ii) What value of M is used? and is there any general rule for this choice? \n(iii) How the classification in the 10-classes is obtained? Are you using a softmax function in the last layer? Are you using one weight tensor W_c per class (c=1,2,...,10). Please provide these technical details. \n(iv) Please, specify which nonlinear activation sigma is used in the feature map f_\\theta(x).\n(v) How many feature maps are used? and, Are the matrix A and vector b learned from training dataset or only the TT-cores need to be learned? ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}