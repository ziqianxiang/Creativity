{
    "Decision": {
        "metareview": "This paper presents an RL agent which progressively synthesis programs according to syntactic constraints, and can learn to solve problems with different DSLs, demonstrating some degree of transfer across program synthesis problems. Reviewers agreed that this was an exciting and important development in program synthesis and meta-learning (if that word still has any meaning to it), and were impressed with both the clarity of the paper and its evaluation. There were some concerns about missing baselines and benchmarks, some of which were resolved during the discussion period, although it would still be good to compare to out-of-the-box MCTS.\n\nOverall, everyone agrees this is a strong paper and that it belongs in the conference, so I have no hesitation in recommending it.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Exciting work"
    },
    "Reviews": [
        {
            "title": "Good paper",
            "review": "This paper presents a (meta-)solver for particular program synthesis problems, where the model has access to a (logic) specification of the program to be synthesized, and a grammar that can change from one task instance to another. The presented model is an RL-based model that jointly trains 1) the joint graph-based embedding of the specification and the grammar, and 2) a policy able to operate on different (from instance to instance) grammars. Interestingly, not only can the model operate as a stand-alone solver, but it can be run as a meta-solver - trained on a subset of tasks, and applied (with tuning) on a new task. Experiments show that the model outperforms two baselines (one being a (near-to-)SOTA model) in the stand-alone setting and that the model successfully transfers knowledge (considers fewer candidates) in the meta-solving mode.\n\nFirst, I enjoyed reading the paper. I think the problem is interesting, particularly due to the model being able to train and operate on various grammars (from task to task), and not on a single, pre-specified grammar. The additional bonus is that the problem the paper solves does not require program as supervision, but an external verifier.\nThe evaluation shows that this approach not only makes sense but (significantly) outperforms, under same conditions, specialized program synthesis programs. However, there’s one issue here, and that’s what the comparison hasn’t been done to SOTA model but to a less performant model (see issues). \nThe particular approach of jointly training a specification+grammar graph embedding and learning a policy that acts on different grammars seems original and significant enough for publication.\nThe paper is well (with a few kinks) written, and mostly clear. There are still some issues in the paper.\n\nIssues:\n- The dataset used is 210 cryptographic circuit synthesis tasks from SyGuS 2017. Why only this particular subset of all the tasks, and not the other tasks/categories (there is 569 of them in total, no)?\n- Alur et al mention 214 examples in the said tasks, yet the paper says 210. Why?\n- The SyGuS results paper https://arxiv.org/abs/1711.11438 mentions EUSolver as the SOTA model, solving 152 tasks (out of 214). Why didn’t you compare your model to EUSolver?\n- The same paper reports CVC4 solving 117 tasks (out of 214), as opposed to 129 (out of 210) reported in your paper. Could you comment on the (possible) differences in the experimentation protocol?\n- you mention global graph embedding, but you never describe how you calculate it\n- abstract mentions outperforming two SOTA engines, but later you say ESymbolic is a baseline (which it seems by description)\n\nQuestions:\n- W for different edge types and different propagation steps t? Why is there a need for such a large number of parameters? What is the number of propagation steps?\n- In the extreme case where all inputs can be enumerated - how often does this happen in the tasks you solve?\n- figure 2 is not clear. There is too much information on one side (grammar) and too little on the other (what is the meaning of \\tau^(t-1)?)? Is the tree on the right a generated subtree?\n- details of the state s are unclear - it is tracked by an LSTM? Is there a concrete training signal for s, or is it a part of the architecture and everything is end-to-end trainable from the final reward? The same for s0=MLP(h(G)) - is that also trained in the same way?\n- can you provide some intuition on why you chose that particular architecture (state-tracking LSTM,  s0 as such, instead of something simpler?)\n- can you provide details on the state value estimator MLP architecture, as well as the s0 MLP, and the state-tracking LSTM?\n- the probability of each action (..) is defined as ….H_\\alpha^(i) - what does the i stand for? Was that supposed to be the t or \\alpha_t was supposed to be \\alpha_i?\n\nMinor stuff:\n- Figure 5a is referred to as Table 5a in the text\n- out-of-out-solver\n- global graph embedding, figure 1 - G(phi, G), figure 2 - h(G)\n- a figure of the policy architecture would be beneficial\n- Figure 1\n  - d_1 ->X OR Y in the graph is d1T, why isn’t it d1_OR, and connected to the OR node?\n  - why isn’t d1_OR connected to OR node?\n  - AST edge - but grammar is a DAG - (well, multigraph)\n  - what are the reversed links? e.g. if A->B, reversed link is B->A ?\n  - what is the meaning of the concrete figures in ‘one step’?\n- consider relating to ‘DREAMCODER: Bootstrapping Domain-Specific Languages for Neurally-Guided Bayesian Program Learning’ (https://uclmr.github.io/nampi/extended_abstracts/ellis.pdf), as it’s another model that steps away from the fixed-DSL story",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting technique for a challenging synthesis domain, but some details are not clear",
            "review": "This paper presents a reinforcement learning based approach to learn a search strategy to search for programs in the generic syntax-guided synthesis (SyGuS) formulation. Unlike previous neural program synthesis approaches, where the DSL grammar is fixed or the specification is in the form of input-output examples only, the SyGuS formulation considers different grammars for different synthesis problems and the specification format is also more general. The main idea of the approach is to first learn a joint representation of the specification and grammar using a graph neural network model, and then train a policy using reinforcement learning to guide the search with a grammar adaptive policy network that is conditioned on the joint representation. Since the specifications considered here are richer logical expressions, it uses a SAT solver for checking the validity of the proposed solution and to also obtain counterexamples for future rewards. The technique is evaluated on 210 SyGuS benchmarks coming from the cryptographic circuit synthesis domain, and shows significant improvements in terms of number of instances solved compared to CVC4 and ESymbolic baseline search techniques from the formal methods community. Moreover, the learnt policy is also showed to generalize beyond the benchmarks on which it is trained and the meta-solver performs reasonably well compared to the per-task out-of-box solver.\n\nOverall, this paper tackles a more challenging synthesis problem than the ones typically considered in recent neural synthesis approaches. The previous synthesis approaches have mostly focused on learning programs in a fixed grammar (DSL) and with specifications that are typically based on either input-output examples or natural language descriptions. In the SyGuS formulation, each task has a different grammar and moreover, the specifications are much richer as they can be arbitrary logical expressions on program variables. The overall approach of using graph neural networks to learn a joint representation of grammars with the corresponding logical specifications, and then using reinforcement learning to learn a search policy over the grammar is quite interesting and novel. The empirical results on the cryptographic benchmarks compare favorably to state of the art CVC4 synthesis solver.\n\nHowever, there were some details in the model description and evaluation that were not very clear in the current presentation.\n\nFirst, the paper mentions that it uses the idea of Static Single Assignment (SSA) form for the graph representation. What is the SSA form of a grammar and of a specification? \n\nIt was also not very clear how the graphs are constructed from the grammar. For example, for the rule d1 -> X OR Y | d2 OR d2 in Figure 1, are there two d_OR nodes or a single node d_OR shared by both the rules? Similarly, what is the d_T node in the figure? It would be good to have a formal description of the nodes and edges in the graph constructed from the spec and grammar.\n\nSince the embedding matrix H_d can be of variable size (different sizes of expansion rules), it wasn’t clear how the policy learns a conditional distribution over the variable number of actions. Is there some form of padding of the matrix and then masking being used?\n\nFor the reward design, the choice of using additional examples in the set B_\\phi was quite interesting. But there was no discussion about how the interpolation technique works to generate more examples around a counterexample. Can you provide some more details on how the interpolation is being performed? \n\nAlso, how many examples were typically used in the experiments? It might be interesting to explore whether different number of examples lead to different results. How does the learning perform in the absence of these examples with the simple binary 0/1 reward?\n\nFrom last year’s SyGuS competition, it seems that the EUSolver solves 152 problems from the set of 214 benchmarks (Table 4 in http://sygus.seas.upenn.edu/files/SyGuSComp2017.pdf). For the evaluation, is ESymbolic baseline solver different that the EUSolver? Would it be possible to evaluate the EUSolver on the same hardware and timeout to see how well it performs on the 210 benchmarks? \n\nThe current transfer results are only limited to the cryptographic benchmarks. Since SyGuS also has benchmarks in many other domains, would it be interesting to evaluate the policy transfer to some other non-cryptographic benchmark domain?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Generating (syntactic and functional) specification-satisfying programs via Reinforcement Learning",
            "review": "The authors design a program synthesizer that tries to satisfy per-instance specific syntactic and functional constraints,\nbased on sampling trajectories from an RL agent that at each time-step expands a partial-program.\n\nThe agent is trained with policy gradients with a reward shaped as the ratio of input/output examples that the synthesized program satisfies.\n\nWith the 'out-of-box' evaluation, the authors show that their agent can explore more efficiently the harder problems than their non-learning alternatives even from scratch.\n(My intuition is that the agent learns to generate the most promising programs)\nIt would be good to have a Monte Carlo Tree Search baseline on the'out-of-box' evaluation, to detect exploration exploitation trade-offs.\n\nThe authors show with the 'meta-solver' approach that the agent can generalize to and also speed up unseen (albeit easy-ish in the authors words) instances.\n\nClarity: Paper is clear and nicely written.\n\nSignificance: Imagine a single program synthesizer that could generate C++/Java/Python/DSLs  programs and learn from all its successes and failures! This is a step towards that.\n\nPros:\n+ Generating spec-following programs for different grammars.\n+ partial tree expansion takes care of syntactic constraints.\nNeutral\n· The grammar and specification diversity may be too low to feel impressive.\n· It would have been nicer by computing likelihood for unseen instances with unique and known solutions (that is, without finetuning).\nCons:\n- No Tree Search baseline.\n- No results on programs with control flow/internal state.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}