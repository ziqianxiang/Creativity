{
    "Decision": {
        "metareview": "\npros:\n- well-written and clear\n- good evaluation with convincing ablations\n- moderately novel\n\ncons:\n- Reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas.\n\n(Reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision)\n(Reviewer 3 suggests an additional comparison to related work which was addressed in revision)\n\nI appreciate the authors' revisions and engagement during the discussion period.  Overall the paper is good and I'm recommending acceptance.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Good paper; a little incremental"
    },
    "Reviews": [
        {
            "title": "Interesting work which makes Gumbel-softmax relaxation work in GAN-based text generation using a relational memory",
            "review": "Overall:\nThis paper proposes RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal\nfor the generator updates.\n\nQuality and Clarity:\nThe paper is well-written and easy to read. \n\nOriginality :\nAlthough each of the components (relational memory, Gumbel-softmax) was already proposed by previous works, it is interesting to combine these into a new GAN-based text generator. \nHowever, the basic setup is not novel enough. The model still requires pre-training the generator using MLE. The major difference are the architectures (relational memory, multi-embedding discriminator) and training directly through Gumbel-softmax trick which has been investigated in (Kusner and Hernandez-Lobato, 2016). \n\nSignificance:\nThe experiments in both synthetic and real data are in detail, and the results are good and significant.\n\n-------------------\nComments:\n-- In (4), sampling is known as non-differentiable which means that we cannot get a valid definition of gradients. It is different to denote the gradient as 0.\n-- Are the multiple representations in discriminator simply multiple “Embedding” matrices?\n-- Curves using Gumbel-softmax trick + RM will eventually fall after around 1000 iterations in all the figures. Why this would happen?\n-- Do you try training from scratch without pre-training? For instance, using WGAN as the discriminator\n\n\nRelated work:\n-- Maybe also consider to the following paper which used Gumbel-softmax relaxation for improving the generation quality in neural machine translation related?\nGu, Jiatao, Daniel Jiwoong Im, and Victor OK Li. \"Neural machine translation with gumbel-greedy decoding.\" arXiv preprint arXiv:1706.07518 (2017).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good Paper",
            "review": "Update: the authors' response and changes to the paper properly addressed the concerns below. Therefore the score was improved from 6 to 8.\n\n----\n\n\nThe paper makes several contributions: 1. it extends GAN to text via Gumbel-softmax relaxation, which seems more effective than the other approaches using REINFORCE or maximum likelihood principle. 2. It shows that using relational memory for LSTM gives better results. 3. Ablation study on the necessity of the relational memory, the relaxation parameter and multi-embedding in the discriminator is performed.\n\nThe paper's ideas are novel and good in general, and would make a good contribution to ICLR 2019. However, there are a few things in need of improvement before it is suite for publication. I am willing to improve the scores if the following comments are properly addressed.\n\nFirst of all, the paper does not compare with recurrent networks trained using only the \"teacher-forcing\" algorithm without using GAN. This means that at a high level, the paper is insufficient to show that GAN is necessary for text generation at all. That said, since almost every other text GAN paper also failed to do this, and the paper's contribution on using Gumbel-softmax relaxation and the relational memory is novel, I did not get too harsh on the scoring because of this.\n\nSecondly, whether using BLEU on the entire testing dataset is a good idea for benchmarking is controversial. If the testing data is too large, it could be easily saturated. On the other hand, if the testing data is small, it may not be sufficient to capture the quality well. I did not hold the authors responsible on this either, because it was used in previously published results. However, the paper did propose to use an oracle, and it might be a good idea to use a \"teacher-forcing\" trained RNN anyways since it is necessary to show whether GAN is a good idea for text generation to begin with (see the previous comment).\n\nA third comment is that I had wished the paper did more exploration on the relaxation parameter \\beta. Ideally, if \\beta is too large, the output would be too skewed towards a one-hot vector such that instability in the gradients occurs. On the other hand, if \\beta is too small, the output might not be close enough to one-hot vectors to make the discriminator focus on textual differences rather than numerical differences (i.e., between a continuous and a one-hot vector). It would make sense for the paper to show both ends of these failing cases, which is not apparent with only 2 hyper-parameter choices.\n\nFinally, the first paragraph in section 2.2.2 suggests that the gap between discrete and continuous outputs is the reason for mode collapsing. This is false. For image generation, when all the outputs are continuous, there is still mode collapsing happening with GANs. The authors could say that the discrete-continuous gap contributes to mode-collapsing, but this is not too good either because it will require the paper to conduct experiments beyond text generation to show this. Authors should make changes here.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea and experiments well-executed",
            "review": "==========================\nI have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation. \n==========================\nContributions:\n\nThe main contribution of this paper is the proposed RelGAN. First, instead of using a standard LSTM as generator, the authors propose using a relational memory based generator. Second, instead of using a single CNN as discriminator, the authors use multiple embedded representations. Third, Gumbel-softmax relaxation is also used for training GANs on discrete textual data. The authors also claim the proposed model has the ability to control the trade-off between sample quality and diversity via a single adjustable parameter. \n\nDetailed comments:\n\n(1) Novelty: This paper is not a breakthrough paper, mainly following previous work and propose new designs to improve the performance. However, it still contains some novelty inside, for example, the model choice of the generator and discriminator. I think the observation that the temperature control used in the Gumbel-softmax can reflect the trade-off between quality and diversity is interesting. \n\nHowever, I feel the claim in the last sentence of the abstract and introduction is a little bit strong. Though this paper seems to be the first to really use Gumbel-softmax for text generation, similar techniques like using annealed softmax to approximate argmax has already been used in previous work (Zhang et al., 2017). Since this is similar to Gumbel-softmax, I think this may need additional one or two sentences to clarify this for more careful discussion.  \n\nFurther, I would also recommend the authors discuss the following paper [a] to make this work more comprehensive as to the discussion of related work. [a] also uses annealed softmax approximation, and also divide the GAN approaches as RL-based and RL-free, similar in spirit as the discuss in this paper. \n\n[a] Adversarial Text Generation via Feature-Mover's Distance, NIPS 2018.\n\n(2) Presentation: This paper is carefully written and easy to follow. I enjoyed reading the paper. \n\n(3) Evaluation: Experiments are generally well-executed, with ablation study also provided. However, human evaluation is lacked, which I think is essential for this line of work. I have a few questions listed below. \n\nQuestions:\n\n(1) In section 2.4, it mentions that the generator needs pre-training. So, my question is: does the discriminator also need pre-training? If so, how the discriminator is pre-trained?\n\n(2) In Table 1 & 2 & 3, how does your model compare with MaskGAN? If this can be provided, it would be better. \n\n(3) Instead of using NLL_{gen}, a natural question is: what are the self-BLEU score results since it was used in previous work?\n\n(4) The \\beta_max value used in the synthetic and real datasets is quite different. For example, \\beta_max = 1 or 2 in synthetic data, while \\beta_max = 100 or 1000 is used in real data. What is the observation here? Can the authors provide some insights into this?\n\n(5) I feel Figure 3 is interesting. As the authors noted, NLL_gen measures diversity, NLL_oracle measures quality. Looking at Figure 3, does this mean GAN model produces higher quality samples than MLE pretrained models, while GAN models also produces less diverse samples than MLE models? This is due to NLL_gen increases after pretraining, while NLL_oracle further decreases after pretraining. However, this conclusion also seems strange. Can the authors provide some discussion on this? \n\n(6) Can human evaluation be performed since automatic metrics are not reliable enough?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}