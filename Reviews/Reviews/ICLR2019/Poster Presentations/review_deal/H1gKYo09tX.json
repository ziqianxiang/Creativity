{
    "Decision": {
        "metareview": "Overall this paper presents a few improvements over the code2vec model of Alon et al., applying it to seq2seq tasks. The empirical results are very good, and there is fairly extensive experimentation.\n\nThis is a relatively crowded space, so there are a few natural baselines that were not compared to, but I don't think that comparison to every single baseline is warranted or necessary, and the authors have done an admirable job. One thing that still is quite puzzling is the strength of the \"AST nodes only baseline\", which the authors have given a few explanations for (using nodes helps focus on variables, and also there is an effect of combining together things that are close together in the AST tree). Still, this result doesn't seem to mesh with the overall story of the paper all that well, and again opens up some obvious questions such as whether a Transformer model trained on only AST nodes would have done similarly, and if not why not.\n\nThis paper is very much on the borderline, so if there is space in the conference I think it would be a reasonable addition, but there could also be an argument made that the paper would be stronger in a re-submission where the above questions are answered.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Marginally novel method, extensive experiments, reasons for experimental results not extremely clear."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The authors present a method for generating sequences from code. To achieve this, they parse the code and produce a syntax tree. Then, they enumerate paths in the tree along leaf nodes. Each path is encoded via an bidirectional LSTM and a (sub)token-level LSTM decoder with attention over the paths is used to produce the output sequence.  The authors compare their model with other models and show that it outperforms them on two code-to-sequence tasks. An ablation study shows how different components affect the model's performance.\n\nOverall, the task seems very interesting and the results positive. My main concern is wrt the novelty of this work: the novelty of the proposed model seems limited compared to code2vec (Alon 2018b). To my understanding the core idea of both code2vec and code2seq is similar in many respects. The core difference is that paths, instead of treated as single units (code2vec), they are treated as sequences whose representation is computed by an LSTM.\n\nTo understand the work better, additional evaluation seem be necessary:\n\nQ1: Could the authors compare code2seq with an ablation of a 2-layer BiLSTM where the decoder predicts the output as a single token (similar to the \"no decoder\" ablation of code2vec)?\n\nComparing this result to the \"no decoder\" ablation of code2seq will show the extent to which code2seq's performance is due to its code encoding or if code2vec with an LSTM decoder output would have sufficed.\n\nQ2: Using the BiLSTM and the Transformer as baselines seems reasonable but there are other existing models such as Tree LSTMs, Graph Convolutional Neural Networks [a] and TBCNNs [b] that could also be strong baselines which take tree structure into account. Have the authors experimented with any of those?\n\nQ3: I find the results in Table 1 very confusing when comparing them with those reported in Alon et al(2018b): code2vec achieves the best performance in Alon et al (2018b) but it seems to be performing badly in this work. The empirical comparisons to the same baseline methods used in Alon et al. (2018b) yield very different results. Why is that so? It would be worth performing an additional evaluation on the datasets of Alon et al (2018b) using the code2seq model. This would clarify if the results observed here generalize to other datasets.\n\nQ4: The strategy of enumerating paths in the tree seems to be problematic for large files of code. It is unclear how the authors (a) do an unbiased sample of the paths. Do they need to first enumerate all of them and pick at uniform? (b) since the authors pick $k$ paths for each sample, this may imply that the larger the tree, the worse the performance of code2seq. It would be useful to understand if code2seq suffers from this problem more/less than other baselines.\n\n[a] Kipf, T.N. and Welling, M., 2016. Semi-supervised classification with graph convolutional networks.\n[b] Mou, L., Men, R., Li, G., Xu, Y., Zhang, L., Yan, R. and Jin, Z., 2015. Natural language inference by tree-based convolution and heuristic matching.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice evaluation of AST-based encoding for code summarization tasks",
            "review": "This paper introduces an AST-based encoding for programming code and\nshows the effectivness of the encoding in two different task of code\nsummarization:\n\n1. Extreme code summarization - predicting (generating) function name from function body (Java)\n2. Code captioning - generating a natural language sentence for a (short) snippet of code (C#)\n\nPros:\n- Simple idea of encoding syntactic structure of the program through random paths in ASTs\n- Thorough evaluation of the technique on multiple datasets and using multiple baselines\n- Better results than previously published baselines\n- Two new datasets (based on Java code present in github) that will be made available\n- The encoding is used in two different tasks which also involve two different languages\n\nCons:\n- Some of the details of the implementation/design are not clear (see some clarifying questions below)\n- More stats on the collected datasets would have been nice\n- Personally, I'm not convinced \"extreme code summarization\"\nis a great task for code understanding (see more comments below)\n\nOverall, I enjoyed reading this paper and I think the authors did a\ngreat job explaining the technique, comparing it with other baselines,\nbuilding new datasets, etc.\n\nI have several clarifying questions/points (in no particular order):\n\n* Can you provide some intuition on why random paths in the AST encode\n  the \"meaning\" of the code? And perhaps qualitatively compare it with\n  recording some other properties from the tree that preserve its\n  structure more?\n\n* When you perform the encoding of the function body, one sample in a\n  training step contains all the k (k = 200) paths and all the 2*k\n  terminals (end of Section 2)? Or one path at a time (Section 3.2)?\n  I'm guessing is the latter, but not entirely sure. Figure 3 could\n  improve to make it clear.\n\n* Can you explain how you came up with k = 200? I think providing some\n  stats on the dataset could be helpful to understand this number.\n\n* The results for the baselines - do you train across all projects?\n  (As you point out, ConvAttention trained separately, curious whether\n  it makes a difference for the 2 datasets med and large not present\n  in the original paper).\n\n* I'm not sure I understand parts of the ablation study. In particular\n  for point 1., it seems that instead of the AST, only the terminal\n  nodes are used. Do you still use 200 random pairs of terminal? Is\n  this equivalent to a (randomly shuffled) subset of the tokens in the\n  program? Also could you explain why you do the ablation study on the\n  validation set of the medium dataset? In fact, the caption of Table\n  3 says it's done on the dev set. This part was a bit confusing.\n\n* I would have liked to see more details on the datasets introduced,\n  in particular wrt metrics that are relevant for training the model\n  you describe (e.g., stats on the ASTs, stats on the number of random\n  paths in ASTs, code length in tokens, etc.)\n\n* I'm not convinced that the task of \"extreme code summarization\" is a\n  meaningful task. My main problem with it is that the performance of\n  a human on this task would not be that great. On one hand humans\n  (I'm referring to \"programming humans\" :) ) have no problem in\n  coming up with a name for a function body; however, I'm not\n  convinced they could predict the \"gold\" standard. Or, another way of\n  thinking about this, if you have 3 humans who provided names for the\n  same function, my guess it that there will be a wide degree of\n  (dis)agreement. Some of the examples provided in the supplementary\n  material can serve as confirmation bias to my thought :): Fig 7. I\n  claim \"choose random prime\" and \"generate prime number\" are\n  semantically close, however, the precision and recall for this\n  example are both low. All this being said, I understand that it's a\n  task for which data can be generated fairly quickly to feed the\n  (beast) NN and that helps pushing the needle in understanding code\n  semantics.\n\n* It would be nice to see \"exact match\" as one of the metrics (it is\n  probably low, judging by F1 scores, but good to be reported).\n\n* Most likely the following paper could be cited in the related work:\nNeural Code Comprehension: A Learnable Representation of Code Semantics\nhttps://arxiv.org/abs/1806.07336\nhttps://nips.cc/Conferences/2018/Schedule?showEvent=11359\n\nPage 5 first phrase at the top, perhaps zi is a typo and it is\nsupposed to be z1?\n\n----\n\nUpdate: after all the discussion, I'm lowering my score a bit while still hoping the paper will get published. I'm satisfied with the results and the improvement of the paper. I still find it a bit surprising that the pairs of literals/leaves in the tree are a good approximation for the program itself (as shown in one of the ablation study).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice model using path summaries, but some baseline comparisons are missing",
            "review": "This paper presents a new code-to-sequence model called code2seq that leverages the syntactic structure of programming languages to encode source code snippets, which is then decoded to natural language using a sequence decoder. The key idea of the approach is to represent a program using a set of randomly sample k paths in its abstract syntax tree. For each path, the path is encoded using a recurrent network and concatenated with the embeddings of the two leaf terminal values of the path. The path encodings are then averaged to obtain the program embedding, which is then used to initialize a sequence decoder that also attends over the path embeddings. The code2vec model is evaluated over two tasks: 1) Code summarization: predicting a method’s name from its body, and 2) Code captioning: generating a natural language sentence from method’s body depicting its functionality. The code2seq model significantly outperforms the other baseline methods, and the ablation study shows the importance of various design choices.\n\nThis paper presents an elegant way to represent programs using a set of paths in the AST, which are then weighted using an attention mechanism to attend over relevant path components. The code2seq model is extensively evaluated over two domains of code summarization and code captioning, and results show significant improvements.\n\nThe novelty of the code2seq model is somewhat limited compared to the model presented in code2vec (Alon et al. 2018a) paper. In code2vec, a program is encoded as a set of paths, where each path comes from a fixed vocabulary. The code2seq model instead uses an LSTM to encode individual paths, which allows it to generalize to new paths. This is a more natural choice for embedding paths, but it doesn’t appear to be a big conceptual advance in the model architecture. The use of subtoken embeddings for encoding/decoding identifier names is different in code2seq, but it has been proposed earlier in other code embedding models.\n\nFor the code summarization evaluation, would it be possible to evaluate the code2seq model on the dataset used by the code2vec paper? On that dataset, the code2vec approach gets a precision score of 63.1, recall of 54.4, and F1 score of 58.4, [Table 3 on page 18] which are comparable to overall scores of the code2seq model.\n\nOne of the key findings of the paper is that syntactic structure of programs is important to encode. Similar observations have been made in other program embedding papers that use for example Tree-RNN [1] or graph neural networks (GNN) [Allamanis et al. 2018]. It would be quite valuable to compare the current results with the Tree-RNN or GNN models (without performing additional dataflow and control-flow post processing) to see how well the paths-based embeddings work in comparison to these models.\n\nThe value of k=200 seems a bit large for the examples presented in the paper. What happens when smaller values of k are used (e.g. k=10, 20?) What are the average number of paths in the java programs in the dataset?\n\n1. Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas. Learning Program Embeddings to Propagate Feedback on Student Code\nICML 2015\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}