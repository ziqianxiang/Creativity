{
    "Decision": {
        "metareview": ". Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- The proposed method is novel and effective\n- The paper is clear and the experiments and literature review are sufficient (especially after revision).\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\nThe original weaknesses (mainly clarity and missing details) were adequately addressed in the revisions.\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, itâ€™s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "novel and effective method for more effective quantisation of vectorial representations "
    },
    "Reviews": [
        {
            "title": "Spreading vectors for similarity search",
            "review": "The authors propose a method to adapt the data to the quantizer, instead of having to work with a difficult to optimize discretization function. The contribution is interesting.\n\nAdditional comments and suggestions:\n\n- in the related work overview it would be good to also check possible connections with optimal transport methods using entropy regularization.\n\n- at some points in the paper, e.g. section 3.3, the authors mention Voronoi cells. However, in the related work in section 2 vector quantization and self-organizing maps have not been mentioned.\n\n- more details on the optimization or learning algorithms for eq (3)(4) should be given. The loss function is non-smooth and rather complicated. What are the implications on the learning algorithm when training neural networks? Is it important to have a good initialization or not?\n\n- How reproducible are the results? In Table 1 only one number in each column is shown while eqs (3)(4) are non-convex problems. Is it the best result of several runs or an average that is reported in the Table? \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well motivated novel idea; excellent results",
            "review": "Pros\n----\n\n[Originality]\nThe authors propose a novel idea of learning representations that improves the performance of the subsequent fixed discretization method.\n\n[Clarity]\nThe authors clearly motivate their solution and explain the different ideas and enhancements introduced. The manuscript is fairly easy to follow. The different terms in the optimization problem are clearly explained and their individual behaviour are presented for the better understanding.\n\n[Significance]\nThe empirical results for the proposed scheme are compared against various baselines under various scenarios and the results demonstrate the significant utility of the proposed scheme.\n\nLimitations\n-----------\n\n[Clarity]\nThe training times for the catalyzer is never discussed in this manuscript (even relative to the training times of the considered baselines). Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1. Even if, PQ and the catalyzer+lattice might have comparable search recalls, it would be good to understand the relative search times to get similar accuracy especially since the inference time for the catalyzer (which is part of the search time) can be fairly significant.\n\n[Clarity/Significance]\nOne important point not discussed in this manuscript is the choice of the structure (architechture) of the catalyzer. Is the catalyzer architecture dependent on the data?\n  - If yes, how to find an appropriate architecture?\n  - If no, what is it about the proposed architecture that makes it sufficient for all data sets?\nIn my opinion, this is extremely important since this drives the applicability of the proposed scheme beyond the presented examples.\n\n[Minor question]\n- Is the parameter r in the rank loss same as the norm r in the lattice quantizer? This is a bit confusing.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Problematic  experimental results",
            "review": "The idea, transforming the input data to an output space in which the data is distributed uniformly and thus indexing is easier, is interesting. \n\nMy main concerns come from experimental results.\n\n(1) Table 1: where are the results of OPQ and LSQ from? run the codes by the authors of this paper? or from the original paper?\n\nIt is not consistent to the LSQ paper (https://www.cs.ubc.ca/~julm/papers/eccv16.pdf). For BigANN1M, from the LSQ paper, the result is >29 recall at 1 for 64 bits. \n\n(2) Figure 5: similarly, how did you get the results of PQ and OPQ?\n\n(3) There are some other advanced algorithms: e.g.,  additive quantization (Babenko & Lempitsky, 2014) and composite quantization (https://arxiv.org/abs/1712.00955)\n\nThe above points make it hard to judge this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}