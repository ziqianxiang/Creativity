{
    "Decision": {
        "metareview": "The paper presents \"recall traces\", a model based approach designed to improve reinforcement learning in sparse reward settings. The approach learns a generative model of trajectories leading to high-reward states, and is subsequently used to augment the real experience collected by the agent. This novel take on combining model-based and model-free learning is conceptually well motivated and is empirically shown to improve sample efficiency on several benchmark tasks.\n\nThe reviewers noted the following potential weaknesses in their initial reviews: the paper could provide a clearer motivation of why the proposed approach is expected to lead to performance improvements, and how it relates to learning (and uses of) a forward model. Details of the method, e.g., model parameterization is unclear, and the effect of hyperparameter choices is not fully evaluated.\n\nThe authors provided detailed replies to all reviewer suggestions, and ran extensive new experiments, including experiments to address questions about hyperparameter settings, and an entirely new use of the proposed model in a learning from demonstration setting. The authors also clarified the paper as requested by the reviewers. The reviewers have not responded to the rebuttal, but in the AC's assessment their concerns have been adequately addressed. The reviewers have updated their scores in response to the rebuttal, and the consensus is to accept the paper.\n\nThe AC notes that the authors seem unaware of related work by Oh et al. \"Self Imitation Learning\" which was published at ICML 2018. The paper is based on a similar conceptual motivation but imitates high-value traces directly, instead of using a generative model. The authors should include a discussion of how their paper relates to this earlier work in their camera ready version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel take on model-based improvement on model-free RL"
    },
    "Reviews": [
        {
            "title": "Well-presented idea but evaluation seems preliminary",
            "review": "Revision:\nThe authors have thoroughly addressed my review and I have consequently updated my rating accordingly.\n\nSummary:\nModel-free reinforcement learning is inefficient at exploration if rewards are\nsparse / low probability.\nThe paper proposes a variational model for online learning to backtrack\nstate / action traces that lead to high reward states based on best previous\nsamples.\nThe backtracking models' generated recall traces are then used to augment policy\ntraining by imitation learning, i.e. by optimizing policy to take actions that\nare taken from the current states in generated recall traces.\nOverall, the methodology seems akin to an adaptive importance sampling\napproach for reinforcement learning.\n\nEvaluation:\nThe paper gives a clear (at least mathematically) presentation of the core idea\nbut it some details about modeling choices seem to be missing.\nThe experimental evaluation seems preliminary and it is not fully evident when\nand how the proposed method will be practically relevant (and not relevant).\n\nMy knowledgable of the previous literature is not sufficient to validate the\nclaimed novelty of the approach.\n\nDetails:\nThe paper is well written and easy to follow in general.\n\nI'm not familiar enough with reinforcment learning benchmarks to judge the\nquality of the experiments compared to the literature as a whole.\nAlthough there are quite a few experiments they seem rather preliminary.\nIt is not clear whether enough work was done to understand the effect of the\nmany different hyperparameters that the proposed method surely must have.\n\nThe authors claim to show empirically that their method can improve sample\nefficiency.\nThis is not necessarily a strong claim as such and could be achieved on\nrelatively simple tests.\nIn the discussion the authors claim their results indicate that their approach\nis able to accelearte learning on a variety of tasks, also not a strong claim.\n\nThe paper could be improved by adding a more clear explanation of the exact way\nby which the method helps with exploration and how it affects finding sparse\nrewards (based on e.g. Figure 1).\nIt seems that since only knowledge of seen trajectories can be used to generate\npaths to high reward states it only works for generating new trajectories\nthrough previously visited states.\n\nQuestions that could be clarified:\n- It is not entirely obvious to me what parametric models are used for the\nbacktracking distributions.\n- Does this method not also potentially hinder exploration by making the agent\nlearn to go after the same high rewards / Does the direction of the variational\nproblem guarantee coverage of the support of the R > L distribution by samples?\n- What would be the effect of a hyperparameter that balances learning the recall\ntraces and learning the true environment?\n- Are there also reinforcement learning tasks where the proposed methods'\nimprovement is marginal and the extra modeling effort is not justified (e.g.\ndue to increase complexity).\n\nPage 1: iwth (Typo)\nPage 2: r(s_t) -> r(s_t, a_t)\nPage 6: Prioritize d (Typo)\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "This paper nicely proposes a back-tracking model that predicts the trajectories that may lead to high-value states. The proposed approach was shown to be effective in improving sample efficiency for a number of environments and tasks.\n\nThis paper looks solid to me, well-written motivation with theoretical interpretations, although I am not an expert in RL.\n\nComments / questions:\n- how does the backtracking model correspond to a forward-model? And it doesn't seem to be contradictory to me that the two can work together.\n- could the authors give a bit more explanation on why the backtracking model and the policy are trained jointly? Would it still work if to train the backtracking model offline by, say, watching demonstration?\n\nOverall this looks like a nice paper. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "adding another direction to the model increases the sampling efficiency ",
            "review": "The authors propose a bidirectional model for learning a policy. In particular, a backtracking model was proposed to start from a high-value state and sample back the sequence of actions and states that could lead to the current high-value state. These traces can be used later for learning a good policy. The experiments show the effectiveness of the model in terms of increase the expected rewards in different tasks. However, learning the backtracking model would add some computational efforts to the entire learning phase. I would like to see experiments to show the computational time for these components. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}