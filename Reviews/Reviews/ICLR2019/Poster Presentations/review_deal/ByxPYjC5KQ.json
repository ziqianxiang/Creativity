{
    "Decision": {
        "metareview": "The paper received unanimous accept over reviewers (7,7,6), hence proposed as definite accept. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Unanimous accept."
    },
    "Reviews": [
        {
            "title": "Solid analytical and experimental exploration concerning GAN generalizability",
            "review": "The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.\n\nThe authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results. Specifically, they address the problem of gradient explosion in discriminators. \n\nThe authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue. 0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability. Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting. A 0-GP can give the same guarantees but without allowing overfitting to occur.\n\n\nThe authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet. My only issue here is that very little information was given about the size of the training sets. Did they use all the samples? Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.\n\nAll in all, however, the authors provide a convincing  combination of analysis and experimentation. I believe this paper should be accepted into ICLR.\n\nNote: there is an error on page 9, in Figure 3. The paragraph explanation should list that the authors' 0-GP is figure 3(e). They list (d) twice.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "good discussion of generalization and stability of GAN and the gradient penalty method is promising",
            "review": "The paper discusses the generalization capability of GAN especially from the discriminator's perspective. The explanation is clear and the method is promising. The proposed gradient penalty method that penalizes the unseen samples is novel and reasonable from the explanation, although these methods has been proposed before in different forms. \n\nPros:\n1. Nice explanation of why the training of GAN is not stable and the modes often collapse.\n2. Experiments show that the new 0-gradient penalty method seems promising to improve the generalization capability of GAN and helps to resist mode collapsing.\n\nCons:\n1. The paper does not have a clear definition of the generalization capability of the network.\n2. The straight line segment between real and fake images seems not a good option as the input images may live on low-dimensional manifolds. \n3. Why samples alpha in (7) uniformly? It seems the sampling rate should relate with its value. Intuitively, the closer to the real image the sampling point is, the larger the penalty should be.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting read on the convergence of GANs with gradient penalties, lacking comparisons to WGAN-GP",
            "review": "Summary: \nThe paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper. It also provides an analysis on the mode collapse and lack of stability of classical GANs. The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties. \n\nPositive points:\nThe paper is interesting to read and well illustrated. \nAn experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.\n\nPoints to improve: \n\nIf I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting. Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem. WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few. \nThe related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction. \nThe submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments\nThe experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN. \nThe imagenet experiment lacks details.   ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}