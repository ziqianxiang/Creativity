{
    "Decision": {
        "metareview": "This is a well written paper that contributes a clear advance to the understanding of how gradient descent behaves when training deep linear models.  Reviewers were unanimously supportive.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Solid advance in convergence analysis of gradient descent for deep linear networks"
    },
    "Reviews": [
        {
            "title": "nice theoretical result about deep linear neural networks",
            "review": "This paper continues the recent line of study on the convergence behaviour of gradient descent for deep linear neural networks. For more than 2 layers, the optimization problem is nonconvex and it is known strict saddle points exist. The main contribution is a relaxation of the balancedness condition in previous work by Arora et al and a new deficiency margin condition, which together allowed the authors to prove that gradient descent will converge to an epsilon solution in at most O(log 1/epsilon) iterations (under reasonable assumptions on step size and other parameters). Examples on how to satisfy the two conditions are discussed. Overall, the obtained results appear to be a solid contribution beyond our current understanding of deep linear neural networks, and potentially may be helpful towards our understanding of deep nonlinear neural networks.\n\nThis paper is very well-written. The authors gave an elegant short proof for the gradient flow case and spent efforts in proving the discretized version as well. The discussion of related works seems to be appropriate and thorough. \n\nOne thing I would love to see more discussions about is the deficiency margin assumption. I know the authors provided some argument about its necessity in the appendix, but is it possible that under the deficiency margin assumption, the nonconvex optimization problem is really \"trivial\" hence the linear convergence of gradient descent? For instance, can one prove that on this level set there still could be some (strict) saddle-point? And what if Phi is rank-deficient?\n\nIn the paragraph proceeding Section 3.2, the authors mentioned that \"overly small standard deviation will render high magnitude for the deficiency margin, and therefore fast convergence improbable.\" Is there a typo here? Do you mean  a smaller deficiency margin? If not, can you please provide more details?\n\nLastly, it would be great if the authors could complement the theoretical results with some numerical experiments, especially to test the initialization strategies in Section 3.3.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-written paper, I would vote for acceptance, but I have some concerns as well.",
            "review": "This paper studies the convergence of gradient descent on the squared loss of deep linear neural networks. The authors prove linear convergence rate if (1) the network dimensions are big enough so that the full product can have full rank, (2) the singular values of each weight matrices are approximately the same, (3) the initialized point is “close enough” to the target.\n\nFirst of all, this paper is well-written. It reads smoothly, effectively presents the key ideas and implications of the result, and properly answers to possible concerns that arise while reading. The improvement over the previous work ([Bartlett et al 18’]) is quite substantial.\n\nDeep linear neural networks are important, and having a good understanding of linear neural networks can provide us useful insights for understanding the more complex ones, i.e., the nonlinear neural networks. In that regard, I really liked the discussion at the end of Section 3.1. My general opinion for this paper is acceptance, but I also have a number of concerns and questions.\n\nMy main concern about the study of GD on linear neural network is whether we really get any “benefit” or “acceleration” from depth, i.e., is GD on linear neural nets any faster than GD on linear models. It’s been shown that we get acceleration in some cases (e.g., $\\ell_p$ regression when $p>2$ [Arora et al. 18’]), but some other results (e.g., [Shamir 18’] mentioned in Section 5) show that GD on linear neural nets (when weight matrices are all scalar) suffer exponential (in depth) increase in convergence time at near zero region, due to the vanishing gradient phenomenon. From my understanding, this paper circumvents this problem by assuming deficiency margin, because in the setting of [Shamir 18’], deficiency margin means that the initialized product ($W_{1:N}$) has the same sign as $\\Phi$ and far enough from zero, so we don’t have to pass through the near-zero region.\n\nEven with the deficiency margin assumption, the exponential dependence in depth can also be observed in this paper, if we use independent initialization of each weight matrices. In Claim 3, in order to get the probability 0.49 result, the margin $c$ must be very small (O(1/N^N)) as N goes to infinity, resulting in very small $\\delta$ and $\\eta$ in Theorem 1, and convergence time $T$ exploding in depth. On the other hand, if we fix $0 < c < 1$, then the probability of satisfying deficiency margin will be smaller and smaller as $N$ increases. Is this “blow-up in N” problem due to the fact that the loss is l2? Or am I making false claims? I would like to hear the authors’ opinion about this.\n\nThe paper proposes a balanced initialization scheme that doesn’t suffer exponential blow up (Procedure 1 and Theorem 2), but even with this, the learning rate must decay to zero in polynomial rate in N, also resulting in polynomial increase in convergence time as depth increases. Moreover, this type of initialization scheme (specifically tailored for linear neural networks) is not what people would do in practice; we normally would initialize each layer at random, and may suffer the problems discussed in the above paragraph. That is why I’d love to hear about the authors’ future work on layer-wise independent initialization, as noted in the conclusion section.\n\nBelow, I’ll list specific concerns/questions/comments.\n* In my opinion, the statements about “necessity” of two key assumptions are too strong, because the authors only provide counterexamples of non-convergence. As [Theorem 3, Shamir 18’] shows (although in scalar case), even when the assumptions are not satisfied, a convergence rate $O(exp(N) * log(1/\\epsilon))$ is possible. It will be an interesting future work to clearly delineate the boundary between convergence and non-convergence.\n\n* In Thm 2 and Claim 3, what happens if dimension $d_0$ is smaller? What is the reason that you had to restrict it to high dimension? Is it due to high variance with few samples?\n\n* In Thm 2, constants $d’_0$ and $a$ hide the dependence of the result on p, but I would suggest stating the dependence of those parameters on p, and also dependence on other parameters such as N.\n\n* In Section 5, there is a statement “This negative result, a theoretical manifestation of the “vanishing gradient problem”, is circumvented by balanced initialization.” Can you elaborate more on that? If my understanding is correct, there is still $\\sigma_min$ multiplier in Eq (9), which means that at near-zero regions, the gradient will still vanish.\n\nI appreciate the authors for their efforts, especially on the heavy math in the proof of the main theorem. I would like to hear your comments and/or corrections (especially on my “T blowing up in N” claim) and discuss further.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks",
            "review": "Summary: \n \nThe paper provides the convergence analysis at linear rate of gradient descent to global minima for deep linear neural networks – the fully-connected neural networks with linear activation with l2 loss. The convergence only works under two necessary assumptions on initialization: “weight matrices at initialization are approximately balanced” and “the initial loss is smaller than the loss of any rank-deficient solution”. The result of this work is similar to that of Barlett et al. 2018, but the difference is that, in Barlett et al. 2018, they consider a subclass of linear neural networks (linear residual networks – a subclass of linear neural networks which the input, output and all hidden layers are the same dimensions). \n \nComments: \n \nThis paper focuses on theoretical aspect of Deep Learning. Yes, theoretical study of gradient-based optimization in deep learning is still open and needs to spread more. I have the following comments and questions to the author(s) and hope to discuss further during the rebuttal period: \n \n1) Most of the deep learning applications are well-known used the neural networks with non-linear activation (specifically ReLU). Could you please provide any successful applications that linear neural networks could achieve better performance over the “non-linear” one? Yes, more layers may lead to better performance since we have more parameters. However, it is still not clear that which one is better between “linear” and “non-linear” with the same size of networks. I am not sure if this linear neural networks could generalize well. \n \n2) For N=1, the problem should become linear regression with strongly convex loss, which means that there exists a unique W: y = W*x in order to minimize the loss. Hence, if W = W_N*....*W_1, the problem becomes non-convex w.r.t parameters W_N, ...., W_1 but all the minima could be global. Can you please provide some intuitions why the loss function could have saddle points? Also, is not easier to just solve the minimization problem on W?\n\n3) Similar with l2 loss, it seems that the problem needs to be restricted on l2 loss. In understand that it could have in some applications. Do you try to think of different loss for example in binary classification problems? \n \n4) I wonder about the constant “c > 0” in the definition 2 and it would use it to determine the learning rate. Do you think that in order to satisfy the definition 2 for the most cases, constant c would be (arbitrarily) small or may be very close to 0? If so, the convergence rate may be affected in this case. \n \n5) The result of Theorem 2 is nice and seems new in term of probabilistic bound.  I did not see the similar result in the existing literature for neural networks. \n \n6)  It would be nice if the author(s) could provide some experiments to verify the theory. I am also curious to know what performance it could achieve for this kind of networks. \n \nI would love to discuss with the author(s) during the rebuttal period. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}