{
    "Decision": {
        "metareview": "All reviewers agree that the proposed method interesting and well presented. The authors' rebuttal addressed all outstanding raised issues. Two reviewers recommend clear accept and the third recommends borderline accept. I agree with this recommendation and believe that the paper will be of interest to the audience attending ICLR. I recommend accepting this work for a poster presentation at ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "metareviw"
    },
    "Reviews": [
        {
            "title": "Covariates factors are learned from voice and image data using CNNs. A logistic classifier is trained for cross-modal matching from covariates. ",
            "review": "Authors aim to reveal relevant dependencies between voice and image data (under a cross-modal matching framework) through common covariates (gender, ID, nationality). Each covariate is learned using a CNN from each provided domain (speak recordings and face images), then, a classifier is determined from a shared representation, which includes the CNN outputs from voice-based and image-based covariate estimations. The idea is interesting, and the paper ideas are clear to follow.\n\nPros:\n- New insights to support cross-modality matching from covariates.\n- Competitive results against state-of-the-art.\n-Convincing experiments.\n\nCons:\n-Fixing the output dimension to d (for both voice and image-based CNN outputs) could lead to unstable results. Indeed, the comparison of voice and face-based covariate estimates are not entirely fair due to the intrinsic dimensionality can vary for each domain. Alternatives as canonical correlation analysis can be coupled to joint properly both domains.\n- Table 4 - column ID results are not convincing (maybe are not clear for me).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Disjoint Mapping Network for Cross-modal Matching of Voices and Faces",
            "review": "# Summary\n\nThe article proposes a deep learning-based approach aimed at matching face images to voice recordings belonging to the same person. \n\nTo this end, the authors use independently parametrized neural networks to map face images and audio recordings -- represented as spectrograms -- to embeddings of fixed and equal dimensionality. Key to the proposed approach, unlike related prior work, these modules are not directly trained on some particular form of the cross-modal matching task. Instead, the resulting embeddings are fed to a modality-agnostic, multiclass logistic regression classifier that aims to predict simple covariates such as gender, nationality or identity. The whole system is trained jointly to maximise the performance of these classifiers. Given that (face image, voice recording) pairs belonging to the same person must share equal for these covariates, the neural networks embedding face images and audio recordings are thus indirectly encouraged to map face images and voice recordings belonging to the same person to similar embeddings.\n\nThe article concludes with an exhaustive set of experiments using the VGGFace and VoxCeleb datasets that demonstrates improvements over prior work on the same set of tasks.\n\n# Originality and significance\n\nThe article follows-up on recent work [1, 2], building on their original application, experimental setup and model architecture. The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. While the fact that these covariates are strongly associated to face images and audio recordings had already been discussed in [1, 2], the idea of actually using them to drive the learning process is novel in this particular task.\n\nWhile the article does not present substantial, general-purpose methodological innovations in machine learning, I believe it constitutes a solid application of existing techniques. Empirically, the proposed covariate-driven architecture is demonstrated to lead to better performance in the (VGGFace, VoxCeleb) dataset in a comprehensive set of experiments. As a result, I believe the article might be of interest to practitioners interested in solving related cross-modal matching tasks.\n\n# Clarity\n\nThe descriptions of the approach, related work and the different experiments carried out are written clearly and precisely. Overall, the paper is rather easy to read and is presented using a logical, easy-to-follow structure.\n\nIn my opinion, perhaps the only exception to that claim lies in Section 3.4. If possible, I believe the Seen-Heard and Unseen-Unheard scenarios should be introduced in order to make the article self-contained. \n\n# Quality\n\nThe experimental section is rather exhaustive. Despite essentially consisting of a single dataset, it builds on [1, 2] and presents a solid study that rigorously accounts for many factors, such as potential confounding due to gender and/or nationality driving prediction performance in the test set. \n\nMultiple variations of the cross-modal matching task are studied. While, in absolute terms, no approach seems to have satisfactory performance yet, the experimental results seem to indicate that the proposed approach outperforms prior work.\n\nGiven that the authors claimed to have run 5 repetitions of the experiment, I believe reporting some form of uncertainty estimates around the reported performance values would strengthen the results.\n\nHowever, I believe that the success of the experimental results, more precisely, of the variants trained to predict the \"covariate\" identity, call into question the very premise of the article. Unlike gender or nationality, I believe that identity is not a \"covariate\" per se. In fact, as argued in Section 3.1, the prediction task for this covariate is not well-defined, as the set of identities in the training, validation and test sets are disjoint. In my opinion, this calls into question the hypothesis that what drives the improved performance is the fact that these models are trained to predict the covariates. Rather, I wonder if the advantages are instead a \"fortunate\" byproduct of the more efficient usage of the data during the training process, thanks to not requiring (face image, audio recording) pairs as input.\n\n# Typos\n\nSection 2.4\n1) \"... image.mGiven ...\"\n2) Cosine similarity written using absolute value |f| rather than L2-norm ||f||_{2}\n3) \"Here we are give a probe input ...\"\n\n# References\n\n[1] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. \"Learnable PINs: Cross-Modal Embeddings for Person Identity.\" arXiv preprint arXiv:1805.00833 (2018).\n[2] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. \"Seeing voices and hearing faces: Cross-modal biometric matching.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Networks that predict covariates of multimodal inputs like identity and gender produce better representations for cross-modal matching and retrieval tasks than directly predicting cross-modal matches.  Paper and well written and experiments are thorough.",
            "review": "This paper aims at matching people's voices to the images of their faces. It describes a method to train shared embeddings of voices and face images. The speech and image features go through separate neural networks until a shared embedding layer. Then a classification network is built on top of the embeddings from both networks.  The classification network predicts various combinations of covariates of faces and voices: gender, nationality, and identity.  The input to the classification network is then used as a shared representation for performing retrieval and matching tasks.\n\nCompared with similar work from Nagrani et al (2018) who generate paired inputs of voices and faces and train a network to classify if the pair is matched or not, the proposed method doesn't require paired inputs.  It does, however, require inputs that are labeled with the same covariates across modalities.  My feeling is that paired positive examples are easier to obtain (e.g., from unlabeled video) than inputs labeled with these covariates, although paired negative examples require labeling and so may be as difficult to obtain.\n\nSeveral different evaluations are performed, comparing networks that were trained to predict all subsets of identity, gender, and nationality.  These include identifying a matching face in a set of faces (1,2 or N faces) for a given voice, or vice versa. Results show that the network that predicts identity+gender tends to work best under a variety of careful examinations of various stratifications of the data.  These stratifications also show that while gender is useful overall, it is not when the gender of imposters is the same as that of the target individual.  The results also show that even when evaluating the voices and faces not shown in the training data, the model can achieve 83.2% AUC on unseen/unheard individuals, which outperforms the state-of-the-art method from Nagrani et al (2018).\n\nAn interesting avenue of future work would be using the prediction of these covariates to initialize a network and then refine it using some sort of ranking loss like the triplet loss, contrastive loss, etc.\n\n\nWriting:\n* Overall, ciations are all given in textual form Nagrani et al (2018) (in latex this is \\citet{} or \\cite{}), when many times parenthetical citations (Nagrani et al, 2018) (in latex this is \\citep{}) would be more appropriate.\n* The image of the voice waveform in Figures 1 and 2 should be replaced by log Mel-spectrograms in order to illustrate the network's input.\n* \"state or art\" instead of \"state-of-the-art\" on page 3. \n* In subsection 2.4: \"mGiven\" is written instead of \"Given\". \n* On Page 6 Section 3.1 \"1:2 matching\" paragraph. \"Nagrani et al.\" is written twice. * * Page 6 mentions that there is a row labelled \"SVHF-Net\" in table 2, but there is no such row is this table. \n* Page 7 line 1, “G,N” should be \"G, N\".\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}