{
    "Decision": {
        "metareview": "The paper presents interesting idea, but the reviewers ask for improving further paper clarity - that includes, but is not limited to, providing in-depth explanation of assumptions and also improving the writing that is too heavy and difficult to understand.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "An innovative paper to assess equilibration in SGD",
            "review": "The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. \n\nOne of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations.\n\nThis is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments.\n\nI have only a few questions / comments:\n\n1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0?\n2. In Fig. 2, the distinction between solid and dotted curves could be made better visible.\n3. For completeness, it would be good to add the following citation:\nStephan Mandt, Matthew D. Hoffman, and David M. Blei. \"Continuous-time limit of stochastic gradient descent revisited.\"Â NIPS 2015 Workshop on Optimization for Machine Learning.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Inspired by statistical mechanics, the authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD. They further use the relations to set training schedule adaptively and analyze the loss-function landscape. However, the analysis about the stationarity assumption is insufficient and some experiments are weak.",
            "review": "The authors establish a stationary fluctuation-dissipation theorem and derive two specific fluctuation-dissipation relations. The authors use the first relation to check the stationarity and the second relation to delineate the shape of the loss-function landscape.\nTo verify their claim, the authors further use the relations to set the learning-rate schedule adaptively in SGD. \n\nMy major concerns are as follows.\n\n1. The experiments in subsection 3.3 are not convincing. The authors compare the proposed adaptive training schedule with a preset training schedule. However, the improvement by the proposed schedule is insignificant.\nTo make this paper more convincing, the authors may want to compare the proposed adaptive training schedule with other approaches that have dynamic learning rates, such as those mentioned in [1].\n\n2. The derived relations are based on the stationarity assumption. However, there are few discussions on when this assumption will hold. The authors may want to analyze the conditions for the assumption to hold and explain why imposing L^2-regularization can ensure stationarity.\n\nThis paper will be more convincing if the above issues are addressed properly, and I will be happy to raise my score.\n\n[1] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv: 1609.04747, 2017.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice attempt to understand the stationary equilibrium of SGD but the paper not easy to follow",
            "review": "Understanding the stationary equilibrium helps to understand the practical performance of stochastic gradient descent. In this paper, the authors propose two fluctuation-dissipation relation to link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. An advantage over the existing study is that the results here hold for any stationary state and do not need the analogy with continuous-time differential equations. Empirical results are also reported to verify these fluctuation relation.\n\nComments:\n\n(1) I do not quite understand the second identity of (16). In particular, it seems that the authors replace the first two $\\theta(t+1)$ with $\\theta(t)$, and do not use this replacement for the third $\\theta(t+1)$ (this was addressed by (1)).\n\n(2) It would be helpful to the readers if the authors can give the deduction process of (12). It is not easy for me to understand how it holds.\n\n(3) It is not clear to me how the second fluctuation-dissipation relation helps to determine the properties of loss function landscape.\n\n(4) In Section 2.3.1, can you give some explanation for the harmonic approximation. Also the notation $\\theta^*$ seems not to be defined.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}