{
    "Decision": {
        "metareview": "This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously-experienced roll-outs that achieve high reward. The approach is intuitive, and the results in the paper are convincing. The authors addressed nearly all of the reviewer's concerns. The reviewers all agree that the paper should be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "intuitive/elegant idea, well-written, convincing results",
            "review": "The paper describes a method to improve reinforcement learning for task with sparse rewards signals.\n\nThe basic idea is to select the best episodes from the system's experience, and learn to imitate them step by step as the system evolves, aiming at providing a less sparse learning signal.\n\nThe math works out to a gradient that is of similar form as a policy gradient, which makes it easy to interpolate both of them. The resulting training procedure is a policy gradient that gets additional reinforcement of the system's best runs.\n\nThe experiments show the validity especially for the most extreme case (episodic rewards), while, as expected, for the other extreme of dense rewards, the method's effect is not consistently positive.\n\nThe paper then critiques its own method and identifies a critical weakness: the reliance on good exploration. I like that a lot. The paper goes on to suggest an extension to address this by training an ensemble, and shows the effectiveness of this for a number of tasks. However, I feel that the description of this extension is less clear than that of the core idea, and introduces too many new ideas and concepts in a too condensed text.\n\nThe paper seems a significant in that it provides a notable improvement for sparse-rewards tasks, which are a common sub-class of real-world problems.\n\nMy background is not RL. While I am quite confident in my understanding of the paper's math, I am not 100% familiar with the typical benchmark sets. Hence, I cannot judge whether the results include good baselines, or whether the task selection is biased. I can also not judge the completeness of the related work, and how novel the work is. For these questions, I hope that the other reviewers can provide more information.\n\nPros:\n - intuitive idea for a common problem\n - solution elegantly has the form of a modified policy gradient\n - convincing experimental results\n - self-critique of core idea, and extension to address its main weakness\n - nicely written text, does not leave a lot of questions\n\nCons:\n - while the core idea is nicely motivated and described and good to follow, Section 2.3 feels very dense and too short.\n\nOverall, I find the core idea quite intuitive and elegant. The paper's background, motivation, and core method are well-written and, with some effort, quite readable for someone who is not an RL expert. I found that several questions I had during reading were preempted promptly and addressed. However, the description of the secondary method (Section 2.3) is too dense.\n\nTo me, the paper solidly meets the threshold of publication. Since I have no good comparison to other papers, I rate it a \"clear accept\" (8).\n\nMinor points:\n\nI noticed a few superfluous \"the\", please double-check.\n\nIn Table 1, please use the same exponent for directly comparable numbers, e.g. instead of \"1.8e5 4.4e4\", say \"18e4 4.4e4\". Or best just print the full numbers without exponent, I think you have the space.\n\nWhen reading Table 1, I could bnot immediately line up \"PPO\" and \"Self-imitation\" in the caption with the table columns. It took a while to infer that PPO refers to \\nu=0, and SI to \\nu=0.8. Can you add PPO and SI to the table headings?\n\nYou define p as \"the masking probability\", but it is not clear whether that is the probability for keeping a \"1\" in the mask,\nor for masking out the value. I can only guess from the results. I suggest to rephrase as \"the probability of retaining a reward\". Also, how about using plain words in Table 1's heading, such as \"Noisy rewards\\nSuppressing 10% of rewards\", so that one can understand the table without having to search for its description in the text?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Well written paper that explores an interesting idea, weak experimental evaluation",
            "review": "The paper proposes how previously experienced high reward trajectories can be used to generate dense reward functions for more for efficient training of policies in context of reinforcement learning. The paper does this by computing the state-action pair distribution of high rewarding trajectories in the replay buffer, and using a surrogate reward that measures the distance between this distribution and the current state-action pair distribution. The paper derives approximate policy gradients for this surrogate reward function. The paper then describes limitations of doing this: possibility of getting stuck in the local neighborhood of currently well-performing trajectories. It also describes an extension based on Stein variational policy gradients to diversify behavior of an ensemble of policies that are learned together. The paper shows experimental results on a number of MuJoCo tasks.\n\nStrengths:\n1. Adequately leveraging high-return roll-outs for effective learning of policies is an important problem in RL. The paper proposes and empirically investigates a reasonable approach for doing this. The paper shows how using the proposed additional rewards leads to better performance on the choses benchmarks than baseline methods without the proposed rewards.\n\n2. I also like that the paper details the short-comings of the proposed approach, and how these could be fixed.\n\nWeaknesses:\n1. The paper uses sparse rewards in RL as a motivation. However, the proposed approach crucially relies on the fact that a good trajectory has at least been encountered once in the past to be of any use. I am not sure if how the proposed approach does justice to the motivation in the paper. The paper should re-write the motivation, or better explain why the proposed method addresses the motivation.\n\n2. Additionally, the paper does not provide adequate experimental validation. The experiment that I think will make the case for the paper is one that shows the sample efficiency of the proposed approach over other baseline methods, when given a successful past roll-out. The current experimental setup emphasizes the sparse reward scenario in RL, and it is just not clear to me as to why this is a good benchmark to study the effects of the proposed method. \n\n3. The paper primarily makes comparisons to on-policy methods. This may not be a fair comparison, as the proposed method uses past trajectories from a replay buffer (to compute reward). Perhaps improvements are coming because of use of this off-policy information. The paper should design experiments to de-conflate this: perhaps by also comparing to how these additional rewards will compare in context of off-policy methods (like Q-learning).\n\n4. I also do not understand how the benchmark tasks were chosen? Are the MuJoCo tasks studied here a fair representative of MuJoCo tasks studied in literature, or are these selected in any manner? While selecting and modifying benchmarks for the purpose of making a specific point is acceptable, it is important to include benchmark results on a full suite of tasks. This can help understand (desirable or un-desirable) side-effects of proposed ideas.\n\nAfter reading author response and the extra experiments, I have changed my rating to 6 (from the original rating of 5).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good paper, accept",
            "review": "Overall impression: \nI think that this is a well written interesting paper with strong results. One thing I’d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I’ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. \n\n\nAbstract:\nWe demonstrate its effectiveness on a number of challenging tasks. -> be more specific.\n\nThe term single-timestep optimization is not very clear. Can this be clarified?\n\nthey are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task.\n\nRelated works: \nGuided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. \n\n“we learn shaped, dense rewards”-> too early in the paper for this to make sense. can provide some contextt\n\nSection 2.2:\nfully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit.\n\nSmall typos in appendix 5.1 (r should be replaced by the density ratio)\n\nThe update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from “self” experiences. \n\nHow is the priority list threshold and size chosen?\n Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards?\n\nAppendices are very clear and very informative while being succinct!\n\nI would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm \n\nWhat is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification\n\nExperiments. \nOnly 1 question to answer in this section is labelled? Put 2) and 3) appropriately. \n\nCan a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes?\n\nCan the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}