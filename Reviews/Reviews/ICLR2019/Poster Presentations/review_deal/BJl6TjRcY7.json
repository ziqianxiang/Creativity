{
    "Decision": {
        "metareview": "Strengths:  One-shot physics-based imitation at a scale and with efficiency not seen before.\nClear video, paper, and related work.\n\nWeaknesses described include:  the description of a secondary contribution (LFPC) \ntakes up too much space (R1,4); results are not compelling (R1,4); prior art in graphics and robotics (R2,6);\nconcerns about the potential limitations of the linearization used by LFPC.\n\nThe original reviews are negative overall (6,3,4). The authors have posted detailed replies.\nR1 has posted a followup, standing by their score. We have not heard more from R2 and R3.\n\nThe AC has read the paper, watched the video, and read all the reviews.\nBased on expertise in this area, the AC endorses the author's responses to R1 and R2. \nBeing able to compare LFPC to more standard behavior cloning is a valuable data point for the community; \nthere is value in testing simple and efficient models first.\nThe AC identifies the following recent (Nov 2018) paper as being the closest work, which is not identified by the authors or the reviewers. The approach being proposed in the submitted paper demonstrates equal-or-better scalability,\nlearning efficiency, and motion quality, and includes examples of learned high-level behaviors.\nAn elaboration on HL/LL control: the DeepLoco work also learns mocap-based LL-control with learned HL behaviors.\n       although with a more dedicated structure.\n       Physics-based motion capture imitation with deep reinforcement learning\n       https://dl.acm.org/citation.cfm?id=3274506\n\nOverall, the AC recommends this paper to be accepted as a paper of interest to ICLR. \nThis does partially discount R3 and R1, who may not have worked as directly on these specific problems before.\n\nThe AC requests is rating the confidence as \"not sure\" to flag this for the program committee chairs, in light of the fact that this discounts the R1 and R3 reviews.\nThe AC is quite certain in terms of the technical contributions of the paper.\n",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Accept (Poster)",
        "title": "reviews on balance lean negative, but recommend accept  (is this excessive influence of the AC opinion?)"
    },
    "Reviews": [
        {
            "title": "The idea is oversimplified, which may limit its applications.",
            "review": "This paper mainly focuses the imitation of expert policy as well as compression of expert skills via a latent variable model. Overall, I feel this paper is not quite readable, albeit that the prosed methods are simple and straightforward. \n\nAs one major contribution of this paper, the authors introduce a first-order approximation to estimate the action of an expert, where perturbations are considered. However, this linear treatment could yield large errors when the residuals in (1) are still large, which is very common in high-dimensional and highly-nonlinear cases. Specifically, the estimation of “J” could be hard. In addition, just below (1), the authors mention (1) yields a “stabilized policy”, so what do you mean “stabilized”?\n\nAnother crucial issue lies on the treatment of “\\Delta(s)”, which is often unknown and hard to modeled, Thus, various optimal controllers are introduced so as to obtain robust controllers. Similarly, in (9) it is also difficult to decide what is “suitable perturbation distribution”.\n\nOverall, the linear treatment in (2) and assumption on “\\Delta(s)” in (5) actually oversimplify the imitation learning problem, which may not be applicable in real robot applications.\n\nOthers small comments:\n-Section 2.1 could be moved to supplementary material or appendix, as this part is indeed not a contribution.\n\n- in (5), it should be “-J_{i}^{*}”\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Sound approach, but very similar to prior work",
            "review": "The paper tackles the problem of distilling large numbers of expert demonstrations into a single policy that can both recreate original demonstrations in a physically-simulated environment and humanoid platform, and to generalize to novel motions. Towards this, the paper presents two approaches learn policies from expert demonstrations without involving costly closed loop RL training, and distilling these individual experts into a shared policy by learning latent time-varying codes.\n\nThe paper is well-written and the method is well-evaluated in the scope that it is proposed. Both components of the proposed approach have previously been explored in the literature - there is extensive work on learning local controllers for physics based evironments from demonstrations in both open loop and closed loop settings as well as work on mixtures of these controllers in machine learning, robotics and computer graphics communities. While the paper proposes these two components as a contribution, I would like to see a more detailed argument of what this work contributes over previous such approaches. \n\nAnother part  where I wish the paper could make a more compelling argument is that distilled policy can perform non-trivial generalization. Target following is a good illustrative example, but has been showcased by multitude of prior work. The paper talks about compositionality, and it would have been compelling to see examples of that if the method can achieve it. For example, simultaneously performing locomotion skills with upper body manipulation skills is something mixture of expert demonstrations approaches still struggle with and it would have been great to see this paper investigate the approach on this problem. \n\nOverall, this is a sound and well-written submission, but the existence of very related prior work with similar capabilities makes me reluctant to recommend this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns with proposed approach and results",
            "review": "This paper considers the problem of transferring motor skills from multiple experts to a student policy. To this end, the paper proposes two approaches: (1) an approach for policy cloning that learns to mimic the (local) linear feedback behavior of an expert (where the expert takes the form of a neural network), and (2) an approach that learns to compress a large number of experts via a latent space model. The approaches are applied to the problem of one-shot imitation from motion capture data (using the CMU motion capture database). The paper also considers an extension of the proposed approach to the problem of high-level planning; this is done by treating the learned latent space as a new action space and training a high-level policy that operates in this space. \n\nStrengths:\nS1. The supplementary video was clear and helpful in understanding the setup.\nS2. The paper is written in a generally readable fashion.\nS3. The related work section does a thorough job of describing the context of the work.  \n\nHowever, I have some significant concerns with the paper. These are described below. \n\nSignificant concerns:\nC1. My biggest concern is that the paper does not make a strong case for the benefits of LPFC over simpler strategies. The results in Figure 3 demonstrate that a linear feedback policy computed along the expert's nominal trajectory performs as well as (and occasionally even better than) LPFC. This is quite concerning.\nC2. Moreover, as the authors themselves admit, \"while LPFC did not work quite as well in the full-scale model as cloning from noisy rollouts, we believe it holds promise insofar as it may be useful in rollout-limited settings...\". However, the paper does not present any theoretical/experimental evidence that would suggest this.\nC3. Another concern has to do with the two-step procedure for LPFC (Section 2.2), where the first step is to learn an expert policy (in the form of a neural network) and the second step is to perform behavior cloning by finding a policy that tries to match the local behavior of the expert (i.e., finding a policy that attempts to produce similar actions as the expert policy linearized about the nominal trajectory). This two-step procedure seems unnecessary; the paper does not make a case for why the expert policies are not chosen as linear feedback controllers (along nominal trajectories) in the first place.\nC4. The linearization of the expert policy produced in (1) may not lead to a stabilizing feedback controller and could easily destabilize the system. It is easy to imagine cases where the expert neural network policy maintains trajectories of the system in a tube around the nominal trajectory, but whose linearization does not lead to a stabilizing feedback controller. Do you see this in practice? If not, is there any intuition for why this doesn't occur? If this doesn't occur in practice, this would suggest that the expert policies are not highly nonlinear in the neighborhood of states under consideration (in which case, why learn neural network experts in the first place instead of directly learning a linear feedback controller as the expert policy as suggested in C3?)\nC5. I would have liked to have seen more implementation details in Section 3. In particular, how exactly was the linear feedback policy along the expert's nominal trajectory computed? Is this the same as (2)? Or did you estimate a linear dynamical model (along the expert's nominal trajectory) and then compute an LQR controller? More details on the architecture used for the behavioral cloning baseline would also have been helpful (was this a MLP? How many layers?)\n\nMinor comments:\n- There are some periods missing at the end of equations (eqs. (1), (2), (6), (8), (9)).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}