{
    "Decision": {
        "metareview": "The paper addresses a variant of multi-agent reinforcement learning that aligns well with real-world applications - it considers the case where agents may have individual, diverging preferences. The proposed approach trains a \"manager\" agent which coordinates the self-interested worker agents by assigning them appropriate tasks and rewarding successful task completion (through contract generation). The approach is empirically validated on two grid-world domains: resource collection and crafting. The reviewers point out that this formulation is closely related to the principle-agent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep RL space.\n\nThe reviewers noted several potential weaknesses: They asked to clarify the relation to prior work, especially on the principle-agents work done in other areas, as well as connections to real world applications. In this context, they also noted that the significance of the contribution was unclear. Several modeling choices were questioned, including the choice of using rule-based agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. They asked the authors to provide additional details regarding scalability and sample complexity of the approach.\n\nThe authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. The consensus is to accept the paper.\n\nThe AC is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. The AC also notes that the figures in the paper are very small, and often not readable in print - please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Deep reinforcement learning for principle agent problems"
    },
    "Reviews": [
        {
            "title": "Multi agent RL with self interesting agents: New formulation and solution in simple environments.",
            "review": "Summary:\n\nThis paper proposes a way to train a manager agent which would manage a bunch of worker agents to achieve a high-level goal. Each worker has its own set of skills and preferences and the manager tries to assign sub-tasks to these agents along with bonuses such that the agents can even perform tasks that are not preferred by them. Authors achieve this by training a manager which tracks the skills and preferences of the agents on the fly. Authors have done an extensive analysis of the proposed approach in two simple domains: resource collection and crafting.\n\nMajor comments:\n\nThis paper focuses on multi-agent settings with self-interested agents. The problem formulation and the solution are novel enough. Experiments are on toy domains with very few goals and sub-task dependencies. However, authors have done a good job in doing an extensive analysis of the proposed approach.\n\n1.\tCan you comment about the scalability of the proposed solution when the number of possible subtasks increases? When the sub-task dependency graph size increases?\n\n2.\tWhat is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents. It would also make the paper stronger.\n\n3.\tAre the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper. I would increase my rating if the authors are willing to release the code to reproduce all the results reported in the paper.\n\n\nMinor comments:\n\n1.\tPage 3, line 9: “typical” -> “typically”\n2.\tPage 3, “intention” section: “Based on the its reward ..” Check grammar.\n3.\tPage 5, last line: “the total quantitative is 10” check grammar.\n4.\tPage 8, conclusions, second line: “nad” -> “and”\n5.\tPage 8, conclusions, 4th line: “combing” -> “combine”\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting exploration of RL for principle-agent problems",
            "review": "\nThis paper studies the problem of generating contracts by a principal to incentive agents to optimally accomplish multiagent tasks. The setup of the environment is that the agents have certain skills and preferences for activities, which the principal must learn to act optimally. The paper takes a combined approach of agent modeling to infer agent skills and preferences, and a deep reinforcement learning approach to generate contracts. The evaluation of the approach is fairly thorough.\n\nThe main novel contribution of the paper is to introduce the principal-agent problem to the deep multiagent reinforcement learning literature.\n\nMy concerns are:\n- The paper should perform a literature search on related work from operations research, including especially principal-agent problems, which are not currently surveyed, and perhaps also optimal scheduling problems.\n- How do the problems introduced either map onto real applications or map onto environments studied in existing literature (such as in operations research)?\n- More details should be given on the mind tracker module.\n- Is it necessary to use deep reinforcement learning for contract generation?  If the agent modeling is good, the optimal contracts look like they are probably simple to compute directly in the environments studied.\n\nOverall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal. The problems studied seem pulled out a hat, when they could be situated in specific existing literature.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Multi-agent Management using RL",
            "review": "This paper studies the problem of coordinating many strategic agents with private valuation to perform a series of common goals. The algorithm designer is a manager who can assign goals to various agents but cannot see their valuation or control them explicitly. The manager has a utility function for various goals and wants to maximize the total revenue. The abstract problem is well-motivated and significant and is an entire branch of study called algorithmic mechanism design. However often many assumptions have to be made to make the problem mathematically tractable. In this paper, the authors take an empirical approach by designing an RL framework that efficiently maximizes rewards across many episodes. Overall I find the problem interesting, well-motivated. The paper is well-written and contains significant experiments to support its point. However, I do not have the necessary background in the related literature to assess the significance of the methods proposed compared to prior work and thus would refrain from making a judgment on the novelty of this paper in terms of methodology. Here are some of my comments/questions to the author on this paper.\n\n\n(1) I want to clarify how the skills of the agents play a role in the problem setup. Does it show up in the expression for the manager's reward? In particular, does it affect the Indicator for whether a goal is completed Eq. (2) via a process that need not be explicitly modeled but can be observed via a feedback of whether or not the goal is completed? So in the case of resource collection example, the skill set is a binary value for each resource, whether it can be collected or not? \n\n(2) Related to the first point, the motivation for modeling the agents as maximizing their utility is the assumption that agents do not know their skills. I am wondering, is this really justified? Over the course of episodes, can the agents learn their skills based on the relationship between their intention and the goals they achieve? In the resource collection example, when they reach a resource and are not able to collect it, they understand that they do not have the corresponding skill. Is there a way to extrapolate the results from this paper to such a setting? \n\n(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps). Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps? \n\nSome minor comments on the presentation.\n\n(1) What are the units for rewards in the plots? Is it the average per episode reward? It would be good to mention this in the caption.\n\n(2) There are a few typos in the paper. Some I could catch was,\n\n- Last line in Page 5: \"quantitative\" -> \"quantity\"\n- Page 8: skills nad preferences -> skills and preferences\n- Page 8: For which we combining -> for which we combine",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}