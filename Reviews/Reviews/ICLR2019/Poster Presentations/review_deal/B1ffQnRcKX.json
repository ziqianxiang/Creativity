{
    "Decision": {
        "metareview": "\npros:\n- the paper is well-written and presents a nice framing of the composition problem\n- good comparison to prior work\n- very important research direction\n\ncons:\n- from an architectural standpoint the paper is somewhat incremental over Routing Networks [Rosenbaum et al]\n- as Reviewers 2 and 3 point out, the experiments are a bit weak, relying on heuristics such as a window over 3 symbols in the multi-lingual arithmetic case, and a pre-determined set of operations (scaling, translation, rotation, identity) in the MNIST case.\n\nAs the authors state, there are three core ideas in this paper (my paraphrase):\n\n(1) training on a set of compositional problems (with the right architecture/training procedure) can encourage the model to learn modules which can be composed to solve new problems, enabling better generalization. \n(2) treating the problem of selecting functions for composition as a sequential decision-making problem in an MDP\n(3) jointly learning the parameters of the functions and the (meta-level) composition policy.\n\nAs discussed during the review period, these three ideas are already present in the Routing Networks (RN) architecture of Rosenbaum et al.  However CRL offers insights and improvements over RN algorithmically in a several ways:\n\n(1) CRL uses a curriculum learning strategy.  This seems to be key in achieving good results and makes a lot of sense for naturally compositional problems.\n(2) The focus in RN was on using the architecture to solve multi-task problems in object recognition. The solutions learned in image domains while \"compositional\" are less clearly interpretable.  In this paper (CRL) the focus is more squarely on interpretable compositional tasks like arithmetic and explores extrapolation.\n(3) The RN architecture does support recursion (and there are some experiments in this mode) but it was not the main focus.  In this paper (CRL) recursion is given a clear, prominent role.\n\nI appreciate that the authors' engagement in the discussion period. My feeling is that  the paper offers nice improvements, a useful framing of the problem, a clear recursive formulation, and a more central focus on naturally compositional problems.  I am recommending the paper for acceptance but suggest that the authors remove or revise their contributions (3) and (4) on pg. 2 in light of the discussion on routing nets.\n\nRouting Networks, Adaptive Selection of Non-Linear Functions for Multi-task Learning, ICLR 2018",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Nice framing of the problem; architecturally somewhat incremental over routing nets"
    },
    "Reviews": [
        {
            "title": "Well-written paper; second experiment could be made stronger.",
            "review": "Summary: This paper is about trying to learn a function from typed input-output data so that it can generalize to test data with an input-output type that it hasn't seen during training. It should be able to use \"analogy\" (if we want to translate from French to Spanish but don't know how to do so directly, we should translate from French to English and English to Spanish). It should also be able to generalize better by learning useful \"subfunctions\" that can be composed together by an RL agent. We set up the solution as having a finite number of subfunctions, including \"HALT\" which signifies the end of computation. At each timestep an RL agent chooses a subfunction to apply to the current representation until \"HALT\" is chosen. The main idea is we parameterize these subfunctions and the RL agent as neural networks which are learned based on input -output data. RL agent is also penalized for using many subfunctions. The algorithm is called compositional recursive learner (CRL). Both analogy and meaningful subfunctions should arise purely because of this design.\n\nMultilingual arithmetic experiment. I found this experiment interesting although it would be helpful to specify that it is about mod-10 arithmetic. I was very confused for some time since the arithmetic expressions didn't seem to be evaluated correctly. It also seems that it is actually the curriculum learning that helps the most (vanilla CRL doesn't seem to perform very well) although authors do note that such curriculum learning doesn't help the RNN baseline. It also seems that CRL with curriculum doesn't outperform the RNN baseline that much on test data with the same length as training data. The difference is larger when tested on longer sequences. However here, the CRL learning curve seems to be very noisy, presumably due to the RL element. The qualitative analysis illustrates well how the subfunctions specialize to particular tasks (e.g. translation or evaluating a three symbol expression) and how the RL agent successively picks these subfunctions in order to solve the full task.\n\nImage transformations experiment. This experiment feels a bit more artificial although the data is more complicated than in the previous experiment. Also, in some of the examples in Figure 2, the algorithms seems to perform translation (action 2) twice in a row while it seems like this could be achieved by only one translation. How does this perform experimentally in comparison to an RNN (or other baseline)?\n\nI found this paper to be well-written. Perhaps it could be stronger if the \"image transformations\" experiment quantitatively compared to a baseline. I'm not an expert in this area and don't know in detail how this relates to existing work (e.g. by Rosenbaum et al; 2018).\n\nEdit: change score to 7 in light of revisions and new experiment.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Trying to learn composition",
            "review": "This is a good review paper. I am not sure how much it adds to the open question of how to learn representation with high structure. \n\nI would like to see more detail on what is communicated between the controller and the evaluator. Is it a single function selected or a probability distribution that is sent? How does the controller know how many function the evaluator has created? Or visa versa. \n\nThere is a penalty for the complexity of the program, is there a penalty for the number of functions generated? \n\nHaving just read Hudson and Manning's paper using a separate controller and action/answer generator they make strong use of attention. It is not clear if you use attention? Maybe in that you can operate on a portion of X. What role does attention play in your work?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to compositionally",
            "review": "==== Summary ====\n\nThis paper proposes a model for learning problems that exhibit compositional and recursive structure, called Compositional Recursive Learner (CRL). The paper approaches the subject by first defining a problem as a transformation of an input representation x from a source domain t_x to a target domain t_y. If t_x = t_y then it is called a recursive problem, and otherwise a translational problem. A composite problem is the composition of such transformations. The key observation of the paper is that many real-world problems can be solved iteratively by either recursively transforming an instance of a problem to a simpler instance, or by translating it to a similar problem which we already know how to solve (e.g., translating a sentence from English to French through Spanish). The CRL model is essentially composed of two parts, a set of differential functions and a controller (policy) for selecting functions. At each step i, the controller observes the last intermediate computation x_i and the target domain t_y, and then selects a function and the subset of x_i to operate on. For each instance, the resulting compositional function is trained via back-propagation, and the controller is trained via policy gradient. Finally, the paper presents experiments on two synthetic datasets, translating an arithmetic expression written in one language to its outcome written in another language, and classifying MNIST digits that were distorted by an unknown random sequence of affine transformations. CRL is compared to RNN on the arithmetic task and shown to be able to generalize both to longer sequences and to unseen language pairs when trained on few examples, while RNN can achieve similar performance only using many more examples. On MNIST, it is qualitatively shown that CRL can usually (but not always) find the sequence of transformations to restore the digit to its canonical form.\n\n==== Detailed Review ====\n\nI generally like this article, as it contains a neat solution to a common problem that builds on and extends prior work. Specifically, the proposed CRL model is a natural evolution of previous attempts at solving problems via compositionally, e.g. Neural Programmer [1] that learns a policy for composing predefined commands, and Neural Module Networks [2] that learns the parameters of shared differential modules connected via deterministically defined structure (found via simple parse tree). The paper contains a careful review of the related works and highlights the similarities and differences from prior approaches. Though the experiments are mostly synthetic, the underlying method seems to be readily applicable to many real-world problems.\n\nHowever, the true contributions of the paper are somewhat muddied by presenting CRL as more general than what is actually supported by the experiments. More specifically, the paper presents CRL as a general method for learning compositional problems by decomposing them into simpler sub-problems that are automatically discovered, but in practice, a far more limited version of CRL is used in the experiments, and the suggested translational capabilities of CRL, which are important for abstract sub-problem discovery, are not properly validated:\n\n1. In both experiments, the building-block functions are hand-crafted to fit to the prior knowledge on the compositionally of the problem. For the arithmetic task, the functions are limited to operate each step just on a single window of encompassing 3 symbols (e.g., <number> <op> <number>,  <op> <number> <op>) and return a distribution over the possible symbols, which heavily forces the functions to represent simple evaluators for simple expressions of the form <number> <op> <number>. For the distorted MNIST task, the functions are limited to neural networks which choose the parameter of predetermined transformations (scaling, translation, or rotation) of the input. In both cases, CRL did not *found* sub-problems for reducing the complexity of the original instance but just had to *fine tune* loosely predefined sub-problems. Incorporating expert knowledge into the model like so is actually an elegant and useful trick for solving real problems, and it should be emphasized far clearly in the article. The story of “discovering subproblems” should be left for the discussion / future research section, because though it might be a small step towards that goal, it is not quite there yet.\n2. The experiments very neatly show how recursive transformations offer a nice framework for simplifying an instance of a problem. However, the translation capabilities of the model are barely tested by the presented experiments, and it can be argued that all transformations used by the model are recursive in both experiments. First, only the arithmetic task has a translation aspect to it, i.e., the task is to read an expression in one language and then output the answer in a different language. Second, this problem is only weakly related to translation because it is possible to translate the symbols independently, word by word, as opposed to written language that has complex dependencies between words. Third, the authors report that in practice proper translation was only used in the very last operation for translating the computed value of the input expression to the requested language, and not as a method to translate one instance that we cannot solve into another that we can. Finally, all functions operate and return on all symbols and not ones limited to a specific language, and so by the paper’s own definition, these are all recursive problems and not translational ones.\n\nIn conclusion, I believe this paper should be accepted even with the above issues, mostly because the core method is novel, clearly explained, and appears to be very useful in practice. Nevertheless, I strongly suggest to the authors to revise their article to focus on the core qualities of their method that can be backed by their current experiments, and correctly frame the discussion on possible future capabilities as such.\n\n[1] Reed et al. Neural Programmer-Interpreters. ICLR 2016.\n[2] Andreas et al. Neural Module Networks. CVPR 2016.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}