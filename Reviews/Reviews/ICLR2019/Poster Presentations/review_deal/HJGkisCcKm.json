{
    "Decision": {
        "metareview": "The paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. The model is an auto-encoder with a WaveNet-like domain-specific decoder and a shared encoder, trained with an adversarial \"domain confusion loss\". Even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "interesting problem and promising results"
    },
    "Reviews": [
        {
            "title": "promising results, well-written",
            "review": "A method is presented to modify a music recording so that it sounds like it was performed by a different (set of) instrument(s). This task is referred to as \"music translation\". To this end, an autoencoder model is constructed, where the decoder is autoregressive (WaveNet-style) and domain-specific, and the encoder is shared across all domains and trained with an adversarial \"domain confusion loss\". The latter helps the encoder to produce a domain-agnostic intermediate representation of the audio.\n\nBased on the provided samples, the translation is often imperfect: the original timbre often \"leaks\" into the output. This is most clearly audible when translating piano to strings: the percussive onsets of the piano (due to the hammers hitting the strings) are also present in the translated audio, even though instruments like the violin and the cello are not supposed to produce percussive onsets. This gives the result an unusual sound, which can be interesting from an artistic point of view, but it is undesirable in the context of the original goal of the paper.\n\nNevertheless, the results are quite impressive and for some combinations of instruments/styles it works surprisingly well. The question of whether the approach is equivalent to pitch estimation followed by rendering with a different instrument is also addressed in the paper, which I appreciate.\n\nThe paper is well written and the related work section is comprehensive. The experimental evaluation is thorough and extensive as well (although a few potentially interesting experiments seemingly didn't make the cut, see other comments). I also like that the authors went through the trouble of doing some experiments on a publicly available dataset, to facilitate reproduction and future comparison experiments.\n\n\nOther comments:\n\n* \"autoregressive\" should be one word everywhere\n\n* In section 2 it is stated that attempts to use a unified decoder with style/instrument conditioning all failed. I'm curious about what was tried specifically, it would be nice to discuss this.\n\n* The same goes for experiments based on VQ-VAE, the paper simply states that they were not able to get this working, but not what experiments were run to come to this conclusion.\n\n* The authors went through the trouble to modify the nv-wavenet inference kernels to support their modified architecture, which I appreciate -- will the modified kernels be made available as well?\n\n* The audio augmentation by pitch shifting is a surprising ingredient (but according to the authors it is also crucial). Some more insight as to why this is so important (rather than simply stating that it is important) would be a welcome addition.\n\n* Section 3.2: \"out off tune\" should read \"out of tune\".\n\n* The formulation on p.7, 2nd paragraph is a bit confusing: \"AMT freelancers tended to choose the same domain as the source, regardless of the real source and the presentation order.\" Does that mean they got it right every time? I suspect that is not what it means, but that is how I read it initially.\n\n* I don't quite understand the point of the semantic blending experiments. As a baseline, the same kind of blending in the raw audio space should be done, I suspect it would probably be hard to hear the difference. This is how cross-fading is already done in practice, and it isn't clear to me why this method would yield better results in that respect. The paper is strong enough without them so these could probably be left out.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "\nThe paper proposes a multi-domain music translation method. The model presents a Wavenet auto-encoder setting with a single (domain independent) encoder and multiple (domain specific) decoders.  From the model perspective, the paper builds up on several exciting ideas such as Wavenet and autoencoder based translation models that can perform the domain conversion without relying on parallel datasets. The two main modifications are the use of data augmentation, the use of multiple decoders (rather a single decoder conditioned on the output domain identity) and the use of a domain confusion loss to prevent the latent space to encode domain specific information. This last idea has been also used on prior work.\n\nUp to my knowledge, this is the first autoencoder-based music translation method. While this problem is very similar to that of speaker conversion, modeling musical audio signal (with many instruments) is clearly more challenging. \n\nSummarizing, I think that the contributions in terms of methods are limited, but the results are very interesting. The paper gives an affirmative answer to the question of whether existing models could be adapted to handle the case of music translation, which is of value. The paper would be stronger in my view, if stronger baselines would be included. This would show that the technical contributions are better than alternative methods. Please read bellow some further comments and questions.\n\nThe authors perform two ablation studies: eliminating data augmentation and the domain confusion network. In both cases, the model without this add on fails to train. However, it seems to me that different studies are important. \n\nThe paper seems to be missing baselines. The authors could compare their work with that of VQ-VAE. The authors claim that they could not make VQ-VAE work on this problem. The cited work by Dieleman et al provides some improvements to adapt VQ-VAE to be better suited to the music domain. Did you evaluate also autoregressive discrete autoencoders?\n\nThe proposed method uses an individual decoder per domain. This is unlike other conversion methods (such as the speech conversion studied in VQ-VAE). This modification is very costly and provides a very large capacity. Have you tried having a single decoder which is also conditioned on a one-hot vector indicating the domain? Is it reasonable to expect some transfer between domains or are they too different? Maybe this is the motivation behind using many decoders. It would be good to clarify. \n\nI understand that the emphasis of this work is on music translation, however, the model doesn't have anything specific to music. In that regard, maybe a way to compare to VQ-VAE is to run the proposed method to the voice conversion of the VQ-VAE.\n\nHave you tried producing samples using the decoder in an unconditional setting?\n\nThe authors claim that the learned representation is disentangled. Why is this the case? Normally a representation is said to be disentangled if different properties are represented in different (disjoint) coordinates. I might not be understanding what is meant here.\n\nThe loss used by the authors, encourages the latent representation to not have domain specific information. The authors should cite the work [A], which has very similar motivation. It would be interesting to report the classification accuracy of the classifier to see how much of the domain information is left in the latent codes. Is it reduced to chance?\n\nIn Section 3.1 the authors describe some modifications to nv-wavenet. I imagine that this is because it leads to better performance or faster training. It would be good to give some more information. Did you perform ablation studies for these?\n\nIn the human lineup experiment (Figure 2 b,c and d). While the listeners fail to select the correct source, many of the domains are never chosen. This could suggest that some translations are consistently poorer than others or the translations themselves are poor. This cannot be deduced from this experiment. Have you evaluated this?  Maybe it would be better to present pairs of audios with reconstruction and a translation. \n\nWhile I consider the results quite good, I tend to agree with the posted public comment. It is very hard to claim that the model is effectively transferring styles. A perceptual test should include the question: is this piece on this given style? As the authors mentioned, it is clearly very difficult to evaluate generative models. But maybe the claims could be toned down.\n\n[A] Louizos, Christos, et al. \"The variational fair autoencoder.\" arXiv preprint arXiv:1511.00830 (2015). ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review of the paper",
            "review": "This paper talks about music translation using a WaveNet-based autoencoder architecture.  The models are trained on diverse training sets and evaluated under multiple settings.  What reported in this paper seems to be interesting and the performance sounds good. However, I have following comments/concerns. \n\n1. The paper is not clearly written. Its exposition needs significant improvement.  There are numerous inconsistent definitions and vague descriptions that make the reading sort of difficult. \n    a)  It would be very helpful if the authors can put up a figure for the description of  the WaveNet  autoencoder instead of just using words in Section 3.1\n    b) The paper itself should be self-contained instead of referring readers to other references for the details of model architectures.\n    c) The math symbols are poorly defined.  What is the definition of C in Section 3.3?   It is defined or referred to as \"domain classification network\" and also \"domain confusion network\" but nowhere to find in Fig. 1.\n   d) \"C is minimizes\" -> \"minimizes\"\n   e)  In Section 4,  it says that \"Each batch is first used to train the adversarial discriminator\".  Which adversarial discriminator? Where to find in Fig. 1 as it is the only description of the network architecture?  \n\n2.  The authors mentioned a couple of observations that left unanswered.  \n    a)   I am surprised to see that without data augmentation, the training does not even converge. \n    b)  The conversion from unseen domains is more successful than the learned domains.\n    c)  The decoder starts to be creative when the size of the latent space is reduced. \n   I sense that these observations seem to point to some (serious) generalization issues of the proposed model.  I would like to hear explanations from the authors. \n\n\nAfter reading the rebuttal:\nThe authors have addressed my major concerns with regard to this paper.   I have lifted my score.  Thanks for the nice response.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}