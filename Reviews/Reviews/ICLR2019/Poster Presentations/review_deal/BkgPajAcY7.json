{
    "Decision": {
        "metareview": "This paper provides a new family of untrained/randomly initialized sentence encoder baselines for a standard suite of NLP evaluation tasks, and shows that it does surprisingly well—very close to widely-used methods for some of the tasks. All three reviewers acknowledge that this is a substantial contribution, and none see any major errors or fatal flaws.\n\nOne reviewer had initially argued the experiments and discussion are not as thorough as would be typical for a strong paper. In particular, the results are focused on a single set of word embeddings and a narrow class of architectures. I'm sympathetic to this concern, but since there don't seem to be any outstanding concerns about the correctness of the paper, and since the other reviewers see the contribution as quite important, I recommend acceptance. [Update: This reviewer has since revised their review to make it more positive.]\n\n(As a nit, I'd ask the authors to ensure that the final version of the paper fits within the margins.)",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Limited but worthwhile contribution"
    },
    "Reviews": [
        {
            "title": "interesting investigation with worthwhile contribution; some suggested areas of improvement",
            "review": "This paper is about exploring better baselines for sentence-vector representations through randomly initialized/untrained networks. I applaud the overall message of this paper that we need to evaluate our models more thoroughly and have better baselines. The experimentation is quite thorough and I like that you\n1) explored several different architectures\n2) varied the dimensionality of representations\n3) examine representations with probing tasks in the Analysis section. \n\nMain Critique\n- In your takeaways you say that, “For some of the benchmark datasets, differences between random and trained encoders are so small that it would probably be best not to use those tasks anymore.” I don’t think this follows from your results. Just because current trained encoders do not perform better than random encoders on these tasks doesn’t in itself mean these tasks aren’t good evaluation tasks. These tasks could be faulty for other reasons, but just because we have no better technique than random encoders currently, doesn’t make these evaluation tasks not worthwhile. Perhaps you could further examine what features (n-gram, etc.) it takes to do well on these tasks in order to argue that they shouldn’t be used.\n- In your related work section you say that “We show that a lot of information may be crammed into vectors using randomly parameterized combinations of pre-trained word embeddings: that is, most of the power in modern NLP systems is derived from having high-quality word embeddings, rather than from having better encoders.” Did you run experiments with randomly initialized embeddings? This paper (https://openreview.net/forum?id=ryeNPi0qKX) finds that representations from LSTMs with randomly initialized embeddings can perform quite well on some transfer tasks. I think in order to make such a claim about the power of high-quality word embeddings you should include numbers comparing them to randomly initialized embeddings.\n\nQuestions\n- Did you find that your results were sensitive to the initialization technique used for your random LSTMs / projections?\n- Do you have a sense of why random non-linear features are able to perform well on these tasks? What kind of features are the skip-thought and InferSent representations learning if they do not perform much better? It’s interesting that many of the random encoder methods outperform the trained models on word content. I think you could discuss these Analysis section findings more.\n\nOther Critiques\n- In the introduction, instead of simply describing what is commonly done to obtain and evaluate sentence embeddings, it would be better to include a sentence or two about the motivation for sentence embeddings at all.\n- The first sentence, “Sentence embeddings are learned non-linear recurrent combinations of pre-trained word embeddings”, doesn’t seem to be true as BOE representations are also sentence embeddings and CNNs/transformers could also work. “Non-linear” and “recurrent” are not inherent requirements for sentence embeddings, but just techniques that researchers commonly use.\n- In the second paragraph of introduction instead saying “Natural language processing does not yet have a clear grasp on the relationship between word and sentence embeddings…” it might be better to say “NLP researchers” or the “NLP community” instead of “NLP” as a field doesn’t have a clear grasp.\n- In the introduction: “It is unclear how much sentence-encoding architectures improve over the raw word embeddings, and what aspect of such architectures is responsible for any improvement.” It would be also good to mention that it’s unclear how much the training task / procedure also is affects improvements.\n- You could describe more about applications of reservoir computing in your related work section as it’s been used in NLP before.\n- I don’t think you actually ever describe the type of data that InferSent is trained on, only that it is “expensive” annotated data. It might be useful to add a sentence about natural language inference for clarity.\n- In the conclusion, change “performance improvements are less than 1 and less than 2 points on average over the 10 SentEval tasks, respectively” to  “performance improvements are less than 2 percentage points on average over the 10 SentEval tasks, respectively”\n- It would be nice if you bolded/underlined the best performing numbers in your results tables.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting results though lacks thorough analysis",
            "review": "This paper proposes that randomly encoding a sentence using a set of pretrained word embeddings is almost as good as using a trained encoder with the same embeddings. This is shown through a variety of tasks where certain tasks perform well with a random encoder and certain ones don't.\n\nThe paper is well written and easy to understand and the experiments show interesting findings. There is a good analysis on how the size of the random encoder affects performance which is well motivated by Cover's theorem.\n\nHowever, the random encoders that are tested in the paper are relatively limited to random projections of the embeddings, a randomly initialized LSTM and an echo state network. Other comparisons would make the results significantly more interesting and would move away from the big assumption stated in the first sentence, i.e. that sentence embeddings are: \"learned non-linear recurrent combinations\". Some major models that are missed by this include paragraph vectors (which do not require any initial training if initialized with pretrained word embeddings), CNNs and Transformers. Given this, the takeaways from this paper seem quite limited to recurrent representations and it's unclear how it would generalize to other representations.\n\nAn additional problem is that the paper states that ST-LN used different and older word embeddings which may make the comparison flawed when compared with the random encoders. In this case, the only fairly trained sentence encoder that is compared with is InferSent. The RandLSTM also has an issue in that the biases are intialized around zero whereas it's well known that using an initially higher forget gate bias significantly improves the performance of the LSTM.\n\nFinally, the analysis of the results seems weak. The tasks are very different from each other and no reason or potential explanation is given why certain tasks are better than others with random encoders, except for SOMO and CoordInv. E.g. Could some tasks be solved by looking at keywords or bigrams? Do some tasks intrinsically require longer term dependencies? Do some tasks have more data?\n\nOther comments:\n- The results and especially random encoder results should be shown with confidence intervals.\n- Section 3.1.3 the text refers to W^r but that does not appear in any equations.\n\n=== After rebuttal ===\nThanks for adding the additional experiments (particularly with fully random embeddings) and result analyses to the paper. I feel that this makes the paper stronger and have raised my score accordingly.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Strong, clear paper with worthwhile contribution",
            "review": "This paper tests a number of untrained sentence representation models - based on random embedding projections, randomly-initialized LSTMs, and echo state networks - and compares the outputs of these models against influential trained sentence encoders (SkipThought, InferSent) on transfer and probing tasks. The paper finds that using the trained encoders yields only marginal improvement over the fully untrained models.\n\nI think this is a strong paper, with a valuable contribution. The paper sheds important light on weaknesses of current methods of sentence encoding, as well as weaknesses of the standard evaluations used for sentence representation models - specifically, on currently-available metrics, most of the performance achievements observed in sentence encoders can apparently be accomplished without any encoder training at all, casting doubt on the capacity of these encoders - or existing downstream tasks - to tap into meaningful information about language. The paper establishes stronger and more appropriate baselines for sentence encoders, which I believe will be valuable for assessment of sentence representation models moving forward. \n\nThe paper is clearly written and well-organized, and to my knowledge the contribution is novel. I appreciate the care that has been taken to implement fair and well-controlled comparisons between models. Overall, I am happy with this paper, and I would like to see it accepted. \n\nAdditional comments:\n\n-A useful addition to the reported results would be confidence intervals of some kind, to get a sense of the extent to which the small improvements for the trained encoders are statistically significant.\n\n-I wonder about how the embedding projection method would compare to simply training higher-dimensional word embeddings from the start. Do we expect substantial differences between these two options?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}