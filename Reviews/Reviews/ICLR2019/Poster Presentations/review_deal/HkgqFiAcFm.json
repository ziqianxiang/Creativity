{
    "Decision": {
        "metareview": "The paper introduces a new variance reduced policy gradient method, for directional and clipped action spaces, with provable guarantees that the gradient is lower variance. The paper is clearly written and the theory an important contribution. The experiments provide some preliminary insights that the algorithm could be beneficial in practice. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Variance reduction for Policy Gradients with meaningful theory"
    },
    "Reviews": [
        {
            "title": "Comprehensive analysis and evaluated algorithms on realistic experiments.",
            "review": "In this paper the authors proposed a new policy gradient method, which is known as the angular policy gradient (APG), that aims to provide provably lower variance in the gradient estimate. Here they presented a stochastic policy gradient method for directional control. Under the set of parameterized Gaussian policies, they presented a unified analysis of the variance of APG and showed how it theoretically outperform (in terms of having lower variance) than other state-of-the art methods. They further evaluated the APG algorithms on a grid-world navigation domain as well as the King of Glory task, and showed that the APG estimator significantly out-performs the standard policy gradient.\n\nIn general I think this paper addressed an important issue in policy gradient in terms of deriving a lower variance gradient estimate. In particular the authors showed that under the parameterized marginal distribution, such as the angular Gaussian distribution, the corresponding APG estimate has a lower variance estimate than that of CAPG. Furthermore, I also appreciate that they evaluated these results in realistic experiments such as the RTS game domains. \n\nMy only question is on the possibility of deriving realistic APG algorithms beyond the class of angular Gaussian policy. In terms of the layout of the paper, I would also recommend including the exact algorithm pseudo-code used in the main paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Limited setting of directional RL, but interesting approach and results.",
            "review": "This paper introduces policy gradient methods for RL where the policy must choose a direction (a.k.a., the navigation problem).\n\nMapping techniques from \"non-directional\" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big). The authors propose to sample directly on the sphere, using the fact that the likelyhood of an angular Gaussian r.v. has *almost* a closed form and its gradient can almost be computed, up to some normalization term (the integral which is constant in the standard Gaussian case).\n\n\nThis can be seen as a variance reduction techniques.\n\nThe proofs are not too intricate, for someone used to variance reduction (yet computations must be made quite carefully).\n\n\nThe result is coherent, interesting from a theoretical point of view and the experiment are somehow convincing. The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Fun, albeit incremental paper",
            "review": "Summary\n\nThis paper derives a new policy gradient method for when continuous actions are transformed by a\nnormalization step, a process called angular policy gradients (APG). A generalization based on\na certain class of transformations is presented. The method is an instance of a \nRao-Blackwellization process and hence reduces variance.\n\n\nDetailed comments\n\nI enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications. I am not convinced that the measure theoretic perspective is always\nnecessary to convey the insights, although I appreciate the desire for technical correctness. Still,\nappealing to measure theory does reduces readership, and I encourage the authors to keep this in\nmind as they revise the text.\n\nGenerally speaking it seems like a lot of technicalities for a relatively simple result:\nmarginalizing a distribution onto a lower-dimensional surface.\n\nThe paper positions itself generally as dealing with arbitrary transformations T, but really is \nabout angular transformations (e.g. Definition 3.1). The generalization is relatively \nstraightforward and was not too surprising given the APG theory. The paper would gain in clarity\nif its scope was narrowed. \n\nIt's hard for me to judge of the experimental results of section 5.3, given that there are no other \nbenchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.\n\nDef 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information \nDef 3.1 mu is overloaded: parameter or measure?\n4.4, law of total variation -- define \n\n\nOverall\n\nThis was a fun, albeit incremental paper. The method is unlikely to set new SOTA, but I appreciated\nthe appeal to measure theory to formalize some of the concepts.\n\n\nQuestions\n\nWhat does E_{pi|s} refer to in Eqn 4.1?\nCan you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)\nExperiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian\non the angle?\n\n\nSuggestions\n\nParagraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.\nI would include a short 'measure theory' appendix or equivalent reference for the lay reader.\n\nI wonder if the paper's main aim is not actually to bring measure theory to the study of policy\ngradients, which would be a laudable goal in and of itself. ICLR may not in this case be the right\nvenue (nor are the current results substantial enough to justify this) but I do encourage authors to\nconsider this avenue, e.g. in a journal paper.\n\n= Revised after rebuttal =\n\nI thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to\n1) Provide stronger empirical results (these are not too convincing).\n2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}