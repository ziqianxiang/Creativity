{
    "Decision": {
        "metareview": "The submission proposes two new things: a repulsive loss for MMD loss optimization and a bounded RBF kernel that stabilizes training of MMD-GAN. The submission has a number of unsupervised image modeling experiments on standard benchmarks and shows reasonable performance. All in all, this is an interesting piece of work that has a number of interesting ideas (e.g. the PICO method, which is useful to know). I agree with R2 that the RBF kernel seems somewhat hacky in its introduction, despite working well in practice.\n\nThat being said, the repulsive loss seems like something the research community would benefit from finding out more about, and I think the experiments and discussion are sufficiently extensive to warrant publication.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "OVERALL COMMENTS:\n\nI haven't had much time to write this, so I'm giving a low confidence score and you should feel free to correct me.\n\nI didn't think this paper was very clear. \nI had trouble grasping what the contributions were supposed to be\nand I had trouble judging the significance of the experiments. \n\nThat said, now that (I think) I understand what's going on,\nthe idea seems well motivated, the connection between the repulsion and the use of label information in other\nGAN variants makes sense to me, and the statements you are making seem (as much as I had time to check them) correct. \n\nThis leaves the issue of scientific significance. \nI feel like I need to understand what specifically contributed to the improvements in table 1 to evaluate significance. \nFirst of all, it seems like there are a lot of other 'good-scoring' models left out of this table. \nI understand that you make the claim that your improvement is orthogonal, but that seems like something that needs to\nbe tested empirically. You have orthogonal motivation but it might be that in practice your technique works for a reason\nsimilar to the reason other techniques work. I would like to see more exploration of this. \nSecond, are the models below the line the only models using spectral norm? I can't tell.\nOverall, it's hard for me to envision this work really seriously changing the course of research on GANs,\nbut that's perhaps too high a bar for poster acceptance.\n\nFor these reasons, I am giving a score of 6.\n\nDETAILED COMMENTS ON TEXT:\n\n> their performance heavily depends on the loss functions used in training.\nThis is not true, IMO. See [1]\n\n\n> may discourage the learning of data structures\nWhat does 'data structures' mean in this case?\nIt has another more common usage that makes this confusing.\n\n> Several loss functions have been proposed\nIMO this list doesn't belong in the main body of the text.\nI would move it to an appendix.\n\n> We assume linear activation is used at the last layer of D\nI'm not sure what this means?\nMy best guess is just that you're saying there is no activation function applied to the logits.\n\n> Arjovsky et al. (2017) showed that, if the supports of PX and PG do not overlap, there exists a perfect discriminator...\nThis doesn't affect your paper that much, but was this really something that needed to be shown?\nIf the discriminator has finite capacity it's not true in general and if it has infinite capacity its vacuous.\n\n\n> We propose a generalized power iteration method...\nWhy do this when we can explicitly compute the singular values as in [2]?\nGenuine question.\n\n> MS-SSIM is not compatible with CIFAR-10 and STL-10 which have data from many classes;\nJust compute the intra-class MS-SSIM as in [3].\n\n> Higher IS and lower FID scores indicate better image quality\nI'm a bit worried about using the FID to evaluate a model that's been trained w/ an MMD loss where \nthe discriminator is itself a neural network w/ roughly the same architecture as the pre-trained image classifier\nused to compute the FID. What can you say about this?\nAm I wrong to be worried?\n\n> Table 1: \nWhich models use spectral norm?\nMy understanding is that this has a big influence on the scores.\nThis seems like a very important point.\n\n\n\nREFERENCES:\n\n[1] Are GANs Created Equal? A Large-Scale Study\n[2] The Singular Values of Convolutional Layers\n[3] Conditional Image Synthesis With Auxiliary Classifier GANs",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "official review for \"Improving MMD-GAN Training with Repulsive Loss Function\"",
            "review": "This paper proposed two techniques to improve MMD GANs: 1) a repulsive loss for MMD loss optimization; 2) a bounded Gaussian RBF kernel instead of original Gaussian kernel. The experimental results on several benchmark shown the effectiveness of the two proposals. The paper is well written and the idea is somehow novel. \n\nDespite the above strong points, here are some of my concerns:\n1.The two proposed solutions seem separated. Do the authors have any clue that they can achieve more improvement when combined together, and why?\n\n2. They are limited to the cases with spectral normalization. Is there any way both trick can be extended to other tricks (like WGAN loss case or GP).\n\n3. Few missed references in this area:\na. On gradient regularizers for MMD GANs\nb. Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport\n\nRevision: after reading rebuttal (as well as to other reviewers), I think they addressed my concerns. I would like to keep the original score.  ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but more evidence to show the significance of the work would be appreciated.",
            "review": "The paper proposes a new discriminator loss for MMDGAN which encourages repulsion between points from the target distribution. The discriminator can then learn finer details of the target distribution unlike previous versions of MMDGAN. The paper also proposes an alternative to the RBF kernel to stabilize training and use spectral normalization to regularize the discriminator. The paper is clear and well written overall and the experiments show that the proposed method leads to improvements. The proposed idea is promising and a better theoretical understanding would make this work more significant. Indeed, it seems that MMD-rep can lead to instabilities during training while this is not the case for MMD-rep as shown in Appendix A. It would be good to better understand under which conditions MMD-rep leads to stable training. Figure 3 suggests that lambda should not be too big, but more theoretical evidence would be appreciated.\nRegarding the experiments: \n- The proposed repulsive loss seems to improve over the classical attractive loss according to table 1, however, some ablation studies might be needed: how much improvement is attributed to the use of SN alone? The Hinge loss uses 1 output dimension for the critic and still leads to good results, while MMD variants use 16 output dimensions. Have you tried to compare the methods using the same dimension?\n-The generalized spectral normalization proposed in this work seems to depend on the dimensionality of the input which can be problematic for high dimensional inputs. On the other hand, Myato’s algorithm only depends on the dimensions of the filter. Moreover, I would expect the two spectral norms to be mathematically related [1]. It is unclear what advantages the proposed algorithm for computing SN has.\n- Regarding the choice of the kernel, it doesn’t seem that the choice defined in eq 6 and 7 defines a positive semi-definite kernel because of the truncation and the fact that it depends on whether the input comes from the true or the fake distribution. In that case, the mmd loss loses all its interpretation as a distance. Besides, the issue of saturation of the Gaussian kernel was already addressed in a more general case in [2]. Is there any reason to think the proposed kernel has any particular advantage?\n\nRevision:\n\nAfter reading the author's response, I think most of the points were well addressed and that the repulsive loss has interesting properties that should be further investigated. Also, the authors show experimentally the benefit of using PICO ver PIM which is also an interesting finding.\nI'm less convinced by the bounded RBF kernel, which seems a little hacky although it works well in practice. I think the saturation issues with RBF kernel is mainly due to discontinuity under the weak topology of the optimized MMD [2] and can be fixed by controlling the Lipschitz constant of the critic.\nOverall I feel that this paper has two interesting contributions (Repulsive loss + highlighting the difference between PICO and PIM) and I would recommend acceptance.\n\n\n\n\n\n\n[1]: Sedghi, Hanie, Vineet Gupta, and Philip M. Long. “The Singular Values of Convolutional Layers.” CoRR \n[2]: M. Arbel, D. J. Sutherland, M. Binkowski, and A. Gretton. On gradient regularizers for MMD GANs.\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}