{
    "Decision": {
        "metareview": "Important problem (explainable AI); sensible approach, one of the first to propose a method for the counter-factual question (if this part of the input were different, what would the network have predicted). Initially there were some concerns by the reviewers but after the author response and reviewer discussion, all three recommend acceptance (not all of them updated their final scores in the system). ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Unclear improvement over state-of-the-art saliency maps extractors",
            "review": "This paper introduces a new saliency map extractor to visualize which input features are relevant for the deep neural network to recognize objects in the image. The proposed saliency map extractor searches over a big space of potentially relevant image features and in-fills the irrelevant image regions using generative models.\n\nThe algorithmic machinery in the paper is poorly justified, as it is presented as a series of steps without providing much intuition why these steps are useful (especially compared to previous works). Also, I would like to know how this paper compares to Fan et al. \"Adversarial localization network\" (NIPS workshop, 2017), which has not been cited and it proposes similar ideas.\n\nAlso, the results are not convincing. Only one previous work (among many) has been compared with the proposed algorithm, and the qualitative examples are not enlightening showing the advantages of the introduced saliency map extractor. What are the new insights into the functioning of deep networks that were gained from the proposed saliency map extractor?\n\nIn summary, it is unclear to me if there is any novelty in the approach (missing references, lack of motivation of the algorithm) and if the results show any improvement over previous works (only one previous work has been compared and the qualitative examples do not show anything particularly interesting).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very well-written paper that introduces to important innovations to the problem of interpreting black-box NNs",
            "review": "The paper is aimed at answering the following question: \"for model M, given an instance input and a predicted label, what parts of the input are most relevant for making the M choose the predicted label?\". \nThis is by far not the first paper aimed at answering this question, but it makes important innovations to the best of my knowledge. The most important one is proposing a stronger approach to the counterfactual question \"had this part of the input been different, what would have been the output?\". Because the input can be different in many ways, an important question is addressing in what specific way would it have been different. \n\nSpecifically in the domain of images, most models assume a blurring or simple local in-painting approach: \"if this patch were just a blurry average, what would have been the output?\". However, ss the current paper correctly points out, blurring or other simple in-painting methods leads to an image which is outside the manifold of natural images and outside the domain of the training set. This can lead to biased or inaccurate results. \n\nThe paper therefore propose two innovations on top of existing methods, most closely building on work by Fong & Vedaldi (2017): \n(1) Optimizing an inference network for discovering image regions which are most informative\n(2) Using a GAN to in-paint the proposed regions, leading to a much more natural image and a more meaningful counterfactual question.\n\nThe presentation is crisp, especially the pseudo-code in Figure 5. In addition, the paper includes several well-executed experiments assessing the contributions of different design choices on different metrics and making careful comparisons with several recent methods addressing the same problem. \n\nSpecific comments:\n\n1. In sec. 4.5, the comparison is not entirely fair because FIDO was already trained with CA-GAN, and therefore might be better adapted for it.\n2. Related to the point above: could one train BBMP with a CA-GAN in-painting model?\n3. I would have liked to see an ablation experiment where either one of the two innovations presented in this paper is missing.\n\n\nMinor:\n1. In eq. (2), wouldn't it be more accurate to denote it as \\phi(x,z,\\hat{x}) ? \n2. I would like to know the true labels for all the examples presented in the paper.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A paper with good motivation and results although the novelty and justifications are somewhat lacking. ",
            "review": "Summary: This paper aims to find important regions to classify an image. The main algorithm, FIDO, is trained to find a saliency map based on SSR or SDR objective functions. The main novelty of this work is that it uses generative models to in-fill masked out regions by SSR or SDR. As such, compared to existing algorithms, FIDO can synthesize more realistic samples to evaluate.\n\nI like the motivation of this paper since existing algorithms have clear limitations, i.e., using out-of-distribution samples. This issue can be addressed by using a generative network as described in this paper.\n\nHowever, I think this approach yields another limitation: the performance of the algorithm is bound by the generative network. For example, letâ€™s assume that a head region is important to classify birds. Also assume that the proposed algorithm somehow predicts a mask for the head region during training. If the generative network synthesizes a realistic bird from the mask, then the proposed algorithm will learn that the head region is a supporting region of SSR. In the other case, however, the rendered bird is often not realistic and classified incorrectly. Then, the algorithm will seek for other regions. As a result, the proposed method interprets a classifier network conditioned on the generative network parameters. Authors did not discuss these issues importantly in the paper.\n\nAlthough the approach has its own limitation, I still believe that the overall direction of the paper is reasonable. It is because I agree that using a generative network to in-fill images to address the motivation of this paper is the best option we have at this current moment. In addition, authors report satisfactory amount of experimental results to support their claim.\n\nQuality: The paper is well written and easy to follow.\n\nClarify: The explanation of the approach and experiments are clear. Since the method is simple, it also seems that it is easy to reproduce their results.\n\nOriginality: Authors apply off-the-shelf algorithms to improve the performance of a known problem. Therefore, I think there is no technical originality except that authors found a reasonable combination of existing algorithms and a problem.\n\nSignificance: The paper has a good motivation and deals with an important problem. Experimental results show improvements. Overall, the paper has some amount of impact in this field.\n\nPros and Cons are discussed above. As a summary,\nPros: \n+ Good motivation.\n+ Experiments show qualitative and quantitative improvements.\n\nCons: \n- Lack of technical novelty and justification of the approach.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}