{
    "Decision": {
        "metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- The paper is clear and well-motivated.\n- The experimental results indicate that the proposed method outperforms the SOTA\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n \n- The novelty is somewhat minor.\n- An interesting (but not essential) ablation study is missing (but the authors promised to include it in the final version).\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n \nThere were no major points of contention.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "somewhat limited novelty but good performance"
    },
    "Reviews": [
        {
            "title": "This paper proposes to resolve this issue by adding an attention mechanism to the deterministic path.",
            "review": "Neural process (NP) is a recent probablistic method for modeling distributions of functions. The authors claim that one substantial weakness of NP is the tendency of under-fitting. The authors give a hypoethesize: the under-fitting behaviour of NP is because the mean-aggregation step in the encoder acts as a bottleneck, as a result, it is difficult for the decoder to learn the relevant information for a give target prediction. This paper proposes to resolve this issue by adding an attention mechanism to the deterministic path. The experimental results show that the proposed method converge faster and give better results on various tasks.\n\nOne major concern about the paper is the lack of analysis of the true cause of under-fitting in NP. The authors give the hypoethesize about the potential cause of the under-fitting issue and proposes to resolve it with attention, however, without theoretical or empirical analyses, it is hard to understand the true cause of the under-fitting issue. Although the proposed method give better performance, it is not clear whether the better performance is due to the added complexity to the model (the attention mechanism) or truely resolving the under-fitting issue. Some analyses along this line can make the paper clearer and more convincing.\n\nA lot of technical details are missing in the paper, which makes the method not reproducible. Please add more details about the proposed attention mechanism and how they are implemented into NP.\n\nIn the GP literature, there are also methods tackling meta-learning or multi-task learning or few shot learning. These works are known as multi-output / multi-tasks Gaussian processes. A few works on this topic are listed:\n* Z Dai, MA Álvarez, ND Lawrence, Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes, NIPS 2017\n* MA Alvarez, L Rosasco, ND Lawrence, Kernels for vector-valued functions: A review, Foundations and Trends® in Machine Learning 2012\n* EV Bonilla, KM Chai, C Williams, Multi-task Gaussian process prediction, NIPS 2007\n\nFor the 1D regression experiments (Figure 1left, Figure 3right), it is not clear which fitting is better. It largely depends on the prior of kerel parameters. As the data points are generated from a GP, plotting the Gaussian process fit with the ground truth parameters can show what a ground truth fitting would look like.\n\nThe Bayesian optimization experiment is very nice and gives some good insights about the quality of the uncertainty of prediction. Maybe consider it to include it in the main text.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose an extension to the recently established framework of Neural Processes by adding an attention-based conditioning mechanism which allows the model to better capture dependencies in its conditioning set. ",
            "review": "This paper is a joy to review, as it is clearly written and has a crisp idea that the authors try to motivate consistently.\nIt extends the framework of neural processes and conditional neural processes by an incremental seeming idea: self attention on the conditioning set and cross attention. What this means in practice is that the model is able to learn a more detailed and structured 'kernel' between query and past data which allows it to identify and model conditional structure better.\n\nThe authors try three main prongs of such attention mechanisms with the multi-head attention appearing to be the most successful one in the experiments.\n\nRegarding experiments, the authors show a 1d function gitting example and various conditional image generation ones, similar to the original examples in the paper. While I find the function fitting exampole quite unconvin cing, it arguably also contains less interesting structure for the model to pick up.\nIn the image generation examples both he quantitative and the qualitative illustrations appear to indicate that a very rich conditioning apparatus (stacked multi head attention) manages to give the model more detailed generative abilities.\nWhile introducing all this machinery seems a bit over-engineered at times, the results do show a benefit. \n\nOverall I find the exposition of the effects of the attention mechanism very well executed and the paper clearly positioned and written. My main complaint would be the incremental nature of the work, as the contributions here are not as significant advances as some preceding ideas that have gone into this work, but still steadily improve on the vision of NP and appear to be necessary steps to push the model forward giving this work validity on its own.\nThe authors discuss a similar mechanism for generation, which while more involved would be a very exciting change from the current framework.  I would have enjoyed seeing more of that in this paper to discuss input and output attention jointly.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "Summary:\nThe authors extend neural processes by incorporating two types of attention processes: self-attention for enriching the features of the context points and cross-attention for producing a query-specific representation. By replacing MLPs and mean pooling with these attention processes, the authors resolve the underfitting problem of NPs. The experimental results show that ANPs converge better and faster than NPs.\n\nOverall, I had fun to read the paper and have not much to complain. Below are some comments and questions.\n\n1. It is intuitive and reasonable that the cross-attention process makes ANPs fit with smaller predictive uncertainty for those regions with many context points. This is well illustrated in the qualitative results in the experiment section.\n\n2. I would like to see an ablation study with the two separate techniques (self- and cross-attention processes) on NPs since the two techniques aim to improve different aspects of NPs. More specifically, I wonder the results of just adding cross-attention with the vanilla MLPs for feature encoding and just replacing the MLPs with self-attention modules while keeping using mean pooling.\n\n3. While the dot product improves the performance significantly, the gain of Laplace is much lower. Also, qualitatively it fails to overcome the underfitting problem. Do you have any intuition about why performs worse than other models?\n\n4. Do you have any specific application in mind? I just wonder some example tasks where contexts are given as inputs.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}