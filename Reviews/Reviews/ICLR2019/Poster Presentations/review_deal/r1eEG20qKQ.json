{
    "Decision": {
        "metareview": "The paper proposes an approach to hyperparameter tuning based on bilevel optimization, and demonstrates promising empirical results. Reviewer's concerns seem to be addressed well in rebuttals and extended version of the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A useful approach to hyperparameter tuning, promising results"
    },
    "Reviews": [
        {
            "title": "Principled approach to hyperparameter tuning but only evaluated on small scale problems to-date.",
            "review": "The paper proposes a bilevel optimization approach for hyperparameter tuning. This idea is not new having been proposed in works prior to the current resurgence of deep learning (e.g., Do et al., 2007, Domke 2012, and Kunisch & Pock, 2013). However, the combination of bilevel optimization for hyperparameter tuning with approximation is interesting. Moreover, the proposed approach readily handles discrete parameters.\n\nExperiments are run on small scale problems, namely, CIFAR-10 and PTB. Results are encouraging but not stellar. More work would need to be done to validate the utility of the proposed approach on larger scale problems.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea, not clear if it is easy to apply.",
            "review": "\n========\\\\\nSummary\\\\\n========\\\\\n\nThe paper deals with hyper-parameter optimization of neural networks. The authors formulate the problem as a bilevel optimization problem: minimizing the validation loss over the hyperparameters, subject to the parameters being at the minimum of the training loss. The authors propose an approximation of the so-called best-response function, that maps the hyperparameters to the corresponding optimal parameters (w.r.t the minimization of the training loss), allowing a formulate as a single-level optimization problem and the use gradient descent algorithm. The proposed\napproximation is based on shifting and scaling the weights and biases of the network. There are no guarantee on its quality except in some very simple cases. The approach assumes a distribution on the hyperparameters, governed by a parameter, which is adapted during the course of the training to achieve a compromise between the flexibility of the best-response function and the quality of its local approximation around the current hyperparameters. The authors show\nthat their approach beats grid-search, random search and Bayesian optimization on the CIFAR-10 and PTB datasets. They point out that the dynamic update of the hyperparameters during the training allows to reach a better performance than any fixed hyperparameter. \\\\\n\n\n======================\\\\\nComments and questions\\\\\n======================\\\\\n\nCan cross-validation be adapted to this approach? \\\\\n\nCan this be used to optimize the learning rate? Which is of course a crucial hyperparameter and that needs an update schedule during the training. \\\\\n\nSection 3.2:\\\\\n\n\"If the entries are too large, then θ̂ φ will not be flexible enough to capture the best- response over the sampled neighborhood. However, its entries must remain sufficiently large so that θ̂ φ captures the local shape around the current hyperparameter values.\" Not clear why -- more explanations would be helpful. \\\\\n\n\"minimizing the first term eventually moves all probability mass towards an optimum λ∗ ,resulting in σ = 0\". I can't see how minimizing the first term w.r.t \\phi (as in section \"2.2.Local approximation\") would alter \\sigma. \\\\\n\n\"τ must be set carefully to ensure...\". The authors still do not explain how to set \\tau. \\\\\n\nSection 3.3: \\\\\n\nIf the hyperparameter is discrete and falls in Case 2, then REINFORCE gradient estimator is used. What about the quality of this gradient? \\\\\n\nSection 5, paragraph Gradient-Based HO: \"differentiating gradient descent\" needs reformulation -- an algorithm cannot be differentiated. \\\\\n\nPros \\\\\n- The paper is pretty clear \\\\\n- Generalizes a previous idea and makes it handle discrete hyperparameters and scale better. \\\\\n- I like the idea of hyperparameters changing dynamically during the training which allows to explore a much larger space than one value \\\\\n- Although limited, the experimental results are convincing \\\\\n\nCons \\\\\n- The method itself depends on some parameters and it is not clear how to choose them. Therefore it might be tricky to make it work in practice. I feel like there is a lot of literature around HO but very often people still use the very simple grid/random search, because the alternative methods are often quite complex to implement and make really work. So the fact that the method depends on \"crucial\" parameters but that are not transparently managed may be a big drawback to its applicability. \\\\\n- No theoretical guarantee on the quality of the used approximation for neural networks \\\\\n- Does not handle the learning rate which is a crucial hyperparameter (but maybe it could) \\\\\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The idea is interesing, but the explaination and experiment can be better",
            "review": "First, the writing can be better. I had a hard time to understand the paper. It has many symbols, but some of them are not explained. For instance, in  formula (9), what are Q or s? Also, formula (14). I probably can guess them. Is it possible to simplify the notations or use a table to list the symbols? \n\nFinding good models is a bi-level or tri-level optimization problem. The paper describes a gradient-based hyperparameter optimization method, which finds model parameters, hyperparameter schedules, and network structure (limited) the same time. It is a interesting idea. Comparing random search, grid search and Spearmint, it seems to be better them. The paper rules out the performance gain is from the randomness of the hyperparameters, which is a good thought. \n\nMore evidences are needed to show this method is superior. The paper doesn't explain well why it works, and the experimental results are just ok. The network architecture search part is limited to number of filters in the experiments. Certainly, the results is not as good as  PNASNet or NASNet. \n\nEvolution algorithm or GA shows good performance in hyperparameter optimization or neural architecture search. Why not compare with them? Random and grid search are not good generally, and Bayesian optimization is expensive and its performance depends on implementation.   \n\nIn Table 2 and figure 4, should \"Loss\" be \"Error\"? \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}