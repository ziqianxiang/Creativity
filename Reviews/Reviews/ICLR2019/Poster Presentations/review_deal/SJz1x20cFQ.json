{
    "Decision": {
        "metareview": "Strengths \n\nThe paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well.\nThe challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the\nseparation of internal state from external state is a clean principle that can potentially be broadly employed. \nThe method does well in outperforming the alternative baselines.\n\nWeaknesses\n\nThere is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses \na policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as \"Virtual Windup Toys for Animation\" \nexploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help. \nThe separation of internal and external state is an assumption that may not hold in many cases.\nThe results are locomotion focussed. There are only two timescales.\n\nDecision\n\nThe reviewers are largely in agreement to accept the paper. \nThere are fairly-simple-but-useful lessons to be found in the paper\nfor those working on HRL problems, particularly those for movement and locomotion. \nThe AC sees the novely with respect to different pieces of related work is the weakest point of the paper.  \nThe reviews contain good suggestions for revisions and improvements;  the latest version may take care\nof these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution\nto ICLR 2019.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Intuitive ideas; good results; some related work"
    },
    "Reviews": [
        {
            "title": "Interesting approach to hierarchical RL",
            "review": "This paper proposes a method to train high- and low-level controllers to tackle hierarchical RL.  The novelty is framing hierarchical RL as a problem of training a diverse set of LL controllers such that they can be used by a HL controller to solve high-level tasks.  By dividing state representation into proprioceptive and task-specific, the reward used to train LL and HL controllers are simplified.  Experimental result shows that the method is effective at solving maze environments for both ant and humanoid.\n\nThe good parts:\n- The method of training diversified LL controller and a single high-level controller seems to work unreasonably well.  And one benefit of this approach is that the rewards for both high (sparse) and low (difference) level controllers can be trivially defined.\n\n- The separation of proprioceptive and task-specific states seems to be gaining popularity.  For the maze environment (and any task that involves locomotion), this can be done intuitively.\n\nPlace to improve:\n- Terrain-Adaptive Locomotion (Peng et al. 2016) used a similar approach of phase-indexed motion, as well as selecting from a mixture of experts to generate action sequence for the next cycle.  Perhaps worthwhile to cite.\n\n- In fact, it seems that phase in this work only benefited training of low-level controller for humanoid.  But it should be possible to train humanoid locomotion with using phase information.\n\n- This hierarchical approach shouldn't depend on the selection of state space.  What would happen when LL and HL controllers all receive the same inputs?\n\n- The paper is difficult to follow at places.  Ex. b_phi element of R^d in Section 3.3.  I'm still not sure what is b_phi, and what is d here.\n\n- The choice of K = 10 feels arbitrary.  Since K corresponds to the length of a cycle, it should make sense to choose K such that the period is reasonable compared to average human stride period, etc.  What is the simulation step length?\n\n- Since LL policies control the style of the motion and the only reward it gets is to keep moving, presumably the resulting motion would look unnatural or exhibit excessive energy consumption.  Does \"keep moving\" reward work with other common rewards like energy penalty, etc?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results, major assumption",
            "review": "Brief summary: \nHRL method which uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation, and are trained to maximize change in the non-proprioceptive part of the state as reward. The higher level policy is trained as usual by commanding lower level policies. \n\nOverall impression:\nI think the paper has a major assumption about the separation of internal and external state, thereby setting the form of the low level primitives. This may not be fully general, but is particularly useful for the classes of tasks shown here as seen from the strong results. I would like to see the method applied more generally to other robotic tasks, and a comparison to Florensa et al. And perhaps the addition of a video which shows the learned behaviors. \n\nIntroduction: \nthe difficulty of learning a high-level controller when the low-level policies shifts -> look at “data efficient hierarchical reinforcement learning” (Nachum et al)\n\nThe basic assumption that we can separate out observations into proprioceptive and not proprioceptive can often be difficult. For example with visual inputs or entangled state representations, this might be very challenging to extract. This idea seems to be very heavily based on what is “internal” and what is “external” to the agent, which may be quite challenging to separate. \n\nThe introduction of phase functions seems to be very specific to locomotion?\n Related work: \nThe connection of learning diverse policies should be discussed with Florensa et al, since they also perform something similar with their mutual information term. DeepMimic, DeepLoco (Peng et al) also use phase information in the state, worthwhile to cite. \n\nSection 3.1:\nThe pros and cons of making the assumption that representation is disentangled enough to make this separation, should be discussed. \n\nAlso, the internal and external state should be discussed with a concrete example, for the ant for example. \n\nSection 3.2:\nThe objective for learning diverse policies is in some sense more general than Florensa et al, but in the same vein of thinking. What are the pros and cons of this approach over that?  The objective is greedy in the change of external state. We’d instead like something that over the whole trajectory maximizes change?\n\nSection 3.3:  How well would these cyclic objectives work in a non-locomotion setting? For example manipulation\n\nSection 3.4: This formulation is really quite standard in many HRL methods such as options framework. The details can be significantly cut down, and not presented as a novel contribution. \n\nExperiments:\nIt is quite cool that Figure 2 shows very significant movement, but in some sense this is already supervised to say “move the CoM a lot”. This should be compared with explicitly optimizing for such an objective, as in Florensa et al. I’m not sure that this would qualify as “unsupervised” per se. As in it too is using a particular set of pre-training tasks, just decided by the form of choosing internal and external state.\n\nall of the baselines fail to get close to the goal locations.-> this is a bit surprising? Why are all the methods performing this poorly even when rewarded for moving the agent as much as possible.\n\nOverall, the results are pretty impressive. A video would be a great addition to the paper. \n\nComparison to Eysenbach et al isn’t quite fair since that method receives less information. If given the extra information, the HRL method performs much better (as indicated by the ant waypoint plot in that paper).",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting paper; but I found the algorithm description very hard to parse!",
            "review": "This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers.\nFrom what I can tell, you train K randomly initialized models to maximize displacement (optionally with a periodic implementation).\nThis ensemble of low-level models is then presented to a high-level controller, that can use them for actions.\nWhen you do this, the resultant algorithm performs well on a selection of deep RL tasks.\n\nThere are several things to like about this paper:\n- Hierarchical RL is an important area of research, and this algorithm appears to make progress beyond the state of the art.\n\n- The ideas of using ensemble of low-level policies is intuitive and appealing.\n\n- The authors provide a reasonable explanation of their \"periodicity\" ideas, together with evidence that it can be beneficial, but is not always essential to the algorithm.\n\n- Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out.\n\n\nThere are several places this paper could be improved:\n- First, the statement of the *main* algorithm needs to be brought together so that people can follow it clearly. I understand one of the main reasons this is complicated is because the authors have tried to make this \"general\" or to be used with DQN/PPO/A3C... but if you present a clear implementation for *one* of them (PPO?) then I think this will be a huge improvement.\n\n- Something *feels* a little hacky about this... why are there only two timescales? Is this a general procedure that we should always expect to work? *why* are we doing this... and what can its downsides be? The ablation studies are good, but I think a little more thought/discussion on how this fits in with a bigger picture of RL/control would be good.\n\nOverall, I hope that I understood the main idea correctly... and if so, I generally like it.\nI think it will be possible to make this much clearer even with some simple amendments.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}