{
    "Decision": {
        "metareview": "This paper proposes an input-dependent baseline function to reduce variance in policy gradient estimation without adding bias. The approach is novel and theoretically validated, and the experimental results are convincing. The authors addressed nearly all of the reviewer's concerns. I recommend acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "Interesting premise, needs more clarity/comparisons",
            "review": "\nIntroduction: \n“Since the state dynamics and rewards depend on the input process” -> why do the rewards depend on the input process conditioned on the state? \n\nDoes the scenario being considered basically involve any scenario with stochastic dynamics? Or is the fact that the disturbances may come from a stateful process what makes this distinct?\n\nif the input sequence following the action -> vague, would help if this would just be written a bit more clearly. \n\nIs just the baseline input dependent or does the policy need to be input dependent as well? From later reading, this point is still quite confusing. One line says “At time t, the policy only depends only on (st, zt).”. Another line says that the policy is pi_theta(a|s), with no mention of z. I’m pretty confused by the consistency here. This is also important in the proof of Lemma 1, because P(a|s,z) = pi_theta(a|s). Please clarify this.\n\nSection 4:\n Is the IID version of Figure 3 basically the same as stochastic dynamics? (Case 2)\n\nSection 4.1\n“In input-driven MDPs, the standard input-agnostic baseline is ineffective at reducing variance” -> can you give some more intuition/proof as to why. \n\nIn Lemma 2, how come the Q function is dependent on z, but the policy is only dependent on s (not even the current and past z’s). \n\nI think the proof of theorem 1 should be included in the main paper rather than unnecessary details about policy gradient. \n\nTheorem 1 and theorem 2 are really some of the most important parts of the paper, and they deserve a more thorough discussion besides the 2 lines that are in there right now. \n\n\nAlgorithm 1 -> should it be eqn 4?\n\nThe meta-algorithm provided in Section 5 is well motivated and well described. An experimental result including what happens with LSTM baselines would be very helpful. \n\nOne question is whether it is actually possible to know what the z’s are at different steps? In some cases these might be latent and hard to infer?\n\nCan you compare to Clavera et al 2018? It seems like it might be a relevant comparison. \n\nThe difference between MAML and the 10 value network seems quite marginal. Can the authors discuss why this is? And when we would expect to see a bigger difference. \n\nRelated work: Another relevant piece of work\nMeta-Learning Priors for Efficient Online Bayesian Regression\n\nMajor todos:\n1. Improve clarity of what z's are observed, which are not and whether the policy is dependent on these or not. \n2. Compare with other prior work such as Clavera et al, Harrison et al. \n3. Add more naive baselines such as training an LSTM, etc. \n4. Provide more analysis of the meta-learning component, how much does it actually help.\n\nOverall impression:  I think this paper covers an interesting problem, and proposes a simple, straightforward approach conditioning the baseline and the critic on the input process. What bothers me in the current version of the paper is the lack of clarity about the observability of z, where it comes from and also some lack of comparisons with other prior methods. I think these would make the paper stronger.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Strong paper for environment in which outcomes are strongly influenced by exogenous factors",
            "review": "The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL.\n\nThe insight developed in the paper is clear: in environments such as data centers or outside settings external factors (traffic load or wind) constitute high magnitude perturbations that ultimately strongly change rewards.\nLearning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact).\n\nThe authors propose different methods to train the input dependent baseline function:\n   o) a multi-value network based approach\n   o) a meta-learning approach\nThe performance of these two methods is compared on simulated robotic locomotion tasks as well as a load balancing and video bitrate adaptation task.\nThe input dependent baseline strongly outperforms the state dependent baseline in both cases.\n\nStrengths:\n   o) The paper is well written\n   o) The method is novel and simple while strongly reducing variance in Monte Carlo policy gradient estimates without inducing bias.\n   o) The experiment evidence is strong.\n\nWeaknesses:\n   o) Vehicular traffic has been the subject of recent development through deep reinforcement learning (e.g. https://arxiv.org/pdf/1701.08832.pdf and https://arxiv.org/pdf/1710.05465.pdf). In this particular setting exogenous noise (demand for throughput and accidents) could strongly benefit from input dependent baselines. I believe the authors should mention such potential applications of the method which may have major societal impact.\n   o) There is a lot of space dedicated to well know facts about policy gradient methods. I believe it could be more impactful to put the proof of Theorem 1 in the main body of the paper as it is clearly a key theoretical property.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting research problem (input-driven MDPs), but I think they are missing the most interesting and practically relevant scenario.",
            "review": "\n\nSummary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see. The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one. My main concerns are highlighted in points (3) and (4) in the detailed comments below. \n\nClarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side. Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion.\n\nOriginality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments. Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables. The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising. The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update. \n\nDisclaimer: I have not checked the proofs in the appendix.\n\nDetailed comments:\n\n1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice. Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task. Can you comment on why something like that was not used at least as baseline?\n\n2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it’s harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle. I think it’s meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C. It is also not clear in this example how the comparison was done. For instance, are the value functions used, input-dependent? Is the policy input aware? \n\n3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP. I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting. This might still be information available in hindsight and used in training, but won’t be available ‘online’ -- similar to slack variable, or privileged information at training time.  And in this case it’s not clear to me if this would still result in a variance reduction in the policy update. Case 2 has some of that flavour, but restricts z_t to an iid process. Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work.\n  \n4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden. From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables. Thus we are training a policy that should be able to deal with a distribution of inputs z. How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline? Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z? \n\nReferences:\n[1] Schaul, T., Horgan, D., Gregor, K. and Silver, D., 2015, June. Universal value function approximators. In International Conference on Machine Learning (pp. 1312-1320).\n\n[POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}