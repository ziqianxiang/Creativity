{
    "Decision": {
        "metareview": "This paper combines two ideas: MAML, and the hierarchical Bayesian inference approach of Amit and Meir (2018). The idea is fairly straightforward but well-motivated, and it seems to work well in practice.  The paper is well-written and includes good discussion of the relevant literature. The experiments show improvements on various tests of Bayesian inference, and include some good analysis beyond simply reporting better numbers.\n\nOn the whole, the reviewers are fairly positive about the paper. (While the numerical scores are slightly below the cutoff, the reviewers are more positive in the discussion.) The reviewers' main complaint is the lack of comparisons against recently published methods, especially Gordon et al. (2018). The lack of comparison to this paper doesn't strike me as a big problem; the preprint was released only a few months before the deadline, their approach was very different from the proposed one, and the proposed approach has some plausible advantages (simplicity, computational efficiency), so I don't think a direct comparison is required for acceptance.\n\nOverall, I recommend acceptance.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "simple method for Bayesian meta-learning; paper is well-executed"
    },
    "Reviews": [
        {
            "title": "Simple and interesting, but missing some Bayesian baselines",
            "review": "The authors consider meta-learning to learn a prior over neural network weights. This is done via amortized variational inference. This means that a good initialisation of the variational parameters are learned across tasks, such that a good set of hyperparameters per task can be found in a few gradient steps. The proposed approach is evaluated on a toy and several popular benchmarks (like miniImagenet).\n\nThe topic is timely. The contribution is modest, essentially applying the same idea as the one proposed in MAML to a variational objective, but well executed. The paper is relatively well-written and the contributions clearly stated/motivated. Section 2 and 3 could be written in a more compact way (in particular the math), but it does not harm the flow. The authors conducted a good set of experiments, but are missing comparisons Bayesian versions of MAML.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited baselines in comparison, cost not clear",
            "review": "The authors proposed a meta-learning approach which amortizes hierarchical variational inference across tasks, learning an initial variational distribution such that, after a few steps of stochastic optimization with the reparametrization trick, they obtain a good task-specific approximate posterior. The optimization is performed by applying backpropagation through\ngradient updates. Experiments on a contextual bandit setting and on miniImage net show how the proposed approach can outperform a baseline based on the method MAML. Although in miniImagenet the proposed method does not produce\ngains in terms of accuracy, it does produce gains in terms of uncertainty estimation.\n\nQuality:\n\nThe derivation of the proposed method is rigorous and well justified. The experiments performed show that the proposed method can result in gains. However, the comparison is only with respect to MAML and other techniques could have also be included to make it more meaningful. For example,\n\nGordon, Jonathan, et al. \"Decision-Theoretic Meta-Learning: Versatile and\nEfficient Amortization of Few-Shot Learning.\" arXiv preprint arXiv:1805.09921\n(2018).\n\nor the methods included in the related work section, or Garnelo et al. 2018.\n\nThe authors do not comment on the computational cost of the proposed method.\n\nClarity:\n\nThe paper is clearly written and easy to read.\n\nNovelty:\n\nThe proposed method is new up to my knowledge. This is one of the first methods to do Bayesian meta-learning.\n\nSignificance:\n\nThe experimental results show that the proposed method can produce gains. However, because the authors only compare with a non-Bayesian meta-learning method (MAML), it is not clear how significant the results are. Furthermore, the computational cost of the proposed method is described well enough.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper. Unclear that it makes a significant contribution, either in terms of novelty or empirical results",
            "review": "This work proposes an adaptation to MAML-type models that accounts for posterior uncertainty in task specific latent variables. This is achieved via a hierarchical Bayesian view of MAML, employing variational inference for the task-specific parameters. The key intuition of this paper is that one can perform fast and efficient test-time variational inference for the task-specific latent variables by learning a good initialization during meta-training. This is achieved in a very similar fashion to MAML, and allows for an interesting form of amortization of test-time inference.\n\nPros:\n- For the most part, the approach presented is principled and well justified.\n- The motivation is clear: in the few-shot learning regime we expect to have little data to infer the task-specific latent variables, and so we should perform posterior inference to account for uncertainty.\n- The paper is well written, clear, and easy to follow.\n\nCons (more details below):\n- It is not clear what the significant contributions of this paper are, as a number of methods have been proposed to account for uncertainty in the task-specific latent variables, and results for many of these methods appear to be better than those presented here.\n- Experimental section does compare to many of the existing related methods \n- There are some conceptual issues that need to be addressed by the authors. \n\nI enjoyed reading this paper, and I think the ideas and work presented are, for the most part, solid. However, I am not sure to what extent the novel contribution of this paper is significant. Several papers, including Grant et al. (2018), but going back to Heskes (2000), have proposed the hierarchical Bayesian view of meta-learning. Grant et al. (2018) used a Laplace approximation to learn in such a model with MAML-type settings, presenting a method that accounts for uncertainty in this family of models. More recently, Finn et al. (2018) and Kim et al. (2018) have done this in a variational manner, albeit with variations in the implementation details. Gordon et al. (2018) proposed a more general presentation, unifying the above works (and others) in a Bayesian framework that allows for different functional forms of posterior inference (both point estimates and distributional) of the task-specific parameters, including gradient based procedures. All of these papers have been publicly available for a few months at the time of submission, such that this view of meta-learning as (amortized) Bayesian inference is not novel.\n\nHere are some points that I would ask the authors to address during the rebuttal period:\n\n- The method presented in the paper does not account for the meta-training splits into query and test sets, other than to mention that these led to empirical performance gains (this is somewhat typical of probabilistic meta-learning papers). However, it not clear that this is justified from a probabilistic inference perspective, which would favour conditioning on all available data at inference time. Further, in the experimental section, the authors state that \"For the few-shot learning experiments, we found it necessary to downweight the inner KL term for better performance in our model\". Put together, it is not quite clear exactly what form of approximate inference is being conducted here. Can the authors comment on this?\n\n- I am not sure I agree with the authors' interpretation of the term \"amortized Bayesian inference\", at least in that it deviates from the way the term is typically used in the related literature. The method negates the need to maintain variational parameters for each latent variable, and approximate posterior inference for unseen tasks may be performed relatively efficiently, which is highly desirable. However, a gradient optimization procedure must still be performed for inference of task-specific variables for new tasks at test time. Thus, new variational parameters must be introduced and optimized at test time. It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process, but this implies that the cost of inference at test time has been reduced, not fully amortized to a fixed cost (unless one fixes the number of gradient steps, which is a further deviation from variational inference and requires a prior of the form used in Grant et al. (2018)). Full amortization of inference for the task-specific variables is proposed by Garnelo et al. (2018) and Gordon et al. (2018), as well as Edwards and Storkey (2016), all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables. In these cases, posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network, rather than several forward-backward passes, and does not require introducing new variational parameters to be optimized. Further, these methods negate the need for differentiating through gradient-based procedures at meta-training time, which is not avoided in this paper, but rather dealt with in the standard Hessian-vector product form. It would be highly useful in the paper (perhaps in the related work section) for the authors to conduct a more thorough comparison of their proposed method and the existing literature employing amortized inference for meta-learning, to put their work in context.\n\nI also have a number of concerns regarding the experimental section of the paper, which I find to be lacking both in details and the empirical comparison of the method to existing works.\n- The authors' cite recent works on meta-learning that take into account uncertainty in the local latent variables (e.g., Grant et al. (2018), Finn et al. (2018), Kim et al. (2018)), but do not compare to these methods.\n- Results from Garnelo et al. (2018) are not provided for the contextual bandits experiment. Their results seem to be comparable or better to those presented in this paper. Can the authors comment on this?\n- The same is true for the few-shot learning case, where MAML is the only method compared to, despite there being, at the time of submission, many papers which have significantly improved upon these results.\n- In terms of details, it is unclear how many gradient steps were taken at test time, and how this affects performance of the model.\n- In terms of accuracy, the proposed method appears to be under-performing significantly (i.e., below confidence bounds in almost all cases).\n- The statement \"...we believe improvements could be made with better variance reduction methods for stochastic gradients\" should, in my opinion, either be investigated or omitted from the paper.\n- In terms of uncertainty quantification, I find this experimental evaluation highly interesting. However, there is not a comparison to much of the existing work. The comparison to MAML is only of moderate interest in this case, as MAML is a deterministic method and is not expected to perform well in this regard. A comparison to Probabilistic or Bayesian MAML (at the least) would be more convincing if uncertainty calibration proved to be better for this method.\n\nOverall, the paper proposes a principled approach to performing approximate posterior inference for task specific latent variables in meta-learning settings. The paper is well-written, and the method is clearly derived. However, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) meta-learning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works. As the method is, for the most part, principled and well-derived, and the paper well written, I am willing to reconsider my overall score if the authors can demonstrate either (i) significant novelty or (ii) that this particular flavour of inference for the task-specific parameters provides significant benefits over existing approaches.\n\n[1] - T. Heskes. Empirical Bayes for learning to learn. 2000.\n[2] - E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as hierarchical Bayes. 2018.\n[3] - C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. 2018.\n[4] - T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning. 2018.\n[5] - J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner. Decision-theoretic meta-learning: versatile and efficient amortization of few-shot learning. 2018.\n[6] - M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. 2018.\n[7] - H. Edwards, and A. Storkey. Towards a neural statistician. 2016.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}