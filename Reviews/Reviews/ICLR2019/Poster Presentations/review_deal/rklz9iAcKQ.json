{
    "Decision": {
        "metareview": "Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1's concerns should be taken seriously. Although I disagree with the reviewer that a general \"framework\" method is a bad thing, I agree with them that additional experiments would be valuable.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "borderline paper due to concerns remain about the thoroughness of the experiments"
    },
    "Reviews": [
        {
            "title": "Solid work, will have high impact",
            "review": "This paper describes an approach for unsupervised learning of node features on a graph (with known structure), so that learned local representations represent community information that has high mutual info with a graph-level summary. The general idea is they apply InfoMax to graphs via graph convolutional networks (GCN), and report impressive results, including rivaling supervised learning methods for node classification. The 3 experiments are on paper topic classification, social network modeling, and protein classification.\n\nThe idea of using InfoMax with GCNs for unsupervised node learning is clever and timely, the technical contribution is solid, the experiments are executed well, and the paper is clear and easy to read.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Idea is interesting; realization is graph-specific",
            "review": "This paper proposes an unsupervised approach to learning node representations. The basic steps are: (1) use an encoder E to learn node vectors, (2) use a readout function R to summarize node vectors into the graph vector, (3) use a scoring function D to score how much the node vectors are aligned with the graph vector, and (4) maximize the scores for the given graph meanwhile minimize the those from the negative distribution.\n\nI feel that the idea is interesting; however, the paper is less well written and the realization of the idea has drawbacks as well.\n\n1. Presentation of Section 3.2 can be improved. The proposed approach becomes clear only toward the end.\n\n2. Naming and wording is misleading. The title and the whole paper use the wording \"mutual information\", whereas in reality, the loss function is a cross entropy.\n\n3. In equation (1), it is unclear why the authors take expectation with respect to the distribution of graphs before summing the scores for one particular graph. Should the order of the expectation and summation be swapped?\n\n4. The proposal is more like a framework than a specific method. The encoder and the negative distribution need to be separately designed for different graphs.\n\nGood things about the proposal:\n\n5. The downstream classification results are quite comparable to those of supervised methods (except for the PPI data).\n\n6. The learned node representations possess a clear clustering structure (Figure 3).\n\nMinor comments:\n\n7. In the third paragraph of section 4.3, the authors state that \"... for the GCN model in the fully supervised setting\". GCN should be a semi-supervised method rather than a fully-supervised one.\n\n8. In the last paragraph of section 4.3, what is a \"randomly initialized graph convolutional network\" and how is it different from the proposal?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Alternative information-theoretic objective for unsupervised graph representation learning",
            "review": "This paper adapts the Deep Informax (DIM; Hjelm et al. 2018) method, which was used on\nimage data, into the graph domain. The architecture of the neural network and\nthe learning cost function are given by figure 1 and eq.(1), respectively.\n\nThe idea is to maximize the mutual information between a local representation\n(of a \"patch\" defined by graph adjacency) and a global representation (of the entire graph),\nso those different local patches are encouraged to carry some shared\nglobal information.\n\nThis is in contrast to most unsupervised graph encoders, where the objective is\nto fit the random walk similarities (node adjacency on the graph).\n\nIn an unsupervised learning scenario, where the graph structure and node features\nare given, the authors achieved state-of-the-art performance on transductive and\ninductive node classification tasks, in some cases even better than supervised baselines. \n\nThe paper is well written. I recommend acceptance and have the following concerns.\n\nMain comment 1\n\nThe title suggests that there are some information theory contents. \nHowever, section 3 does not include much information theory.\nRather, the author(s) directly give eq.(1) with pointers to references and informal discussions.\nThis is not so helpful. It is not straightforward for\nthe reader to relate eq.(1) with the definition of mutual information.\nIdeally, before eq.(1) there should be one or two equations (with text)\nto introduce the Jesen-Shannon MI estimation and information theoretic bounds etc.\n\nOverall, due to this, the contribution is mainly on adapting the DIM method info the graph domain. Although the experimental results are good, there is not much theoretical insight or \"recreative\" introduction of the DIM method from the authors' perspectives. This is the main reason for that it is not a strong accept.\n\nMain comment 2\n\nA motivation of the proposition is to \"not rely on random walks\", or graph node adjacency.\nNotice that random walks can be intuitively regarded as higher order node adjacency.\nHowever, the encoder, which is based on GCN, does rely on the adjacency matrix,\nas the convolution is done in local neighborhoods (that can also be defined based on\nrandom-walk similarities). The authors are therefore suggested to make it\nclear in related places that, it is the cost function which is not based\non node adjacency, although the neural network structure does rely on it.\n\nAs a related question, in the inductive experiments, in the mini-batch of 256 nodes\nrandomly selected, or selected by a local patch of the graph which is connected or nearby?\nIf it is the latter case, the cost function does rely on random-walk similarities,\nas the summary vector will be a local patch average.\n\nQuestions:\n\n-The summary vector is the average of all node features. On large graphs, the\naverage may carry less information as compared to small graphs. It can be\nobserved that on Pubmed and Reddit, the performance improvement is not as\nhigh as the other small graphs. Could you comment on this?\n\n-In the baseline \"DeepWalk+features\", are the two different types of features directly concatenated?\n\n-Is it straightforward to apply DGI to link prediction tasks?\n\n-It that a concern that the random corruption function will cause a high variance of the gradient?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}