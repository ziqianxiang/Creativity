{
    "Decision": {
        "metareview": "The paper proposes a decision-theoretic framework for meta-learning. The ideas and analysis are interesting and well-motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "A novel meta-learning framework",
            "review": "This paper proposes both a general meta-learning framework with approximate probabilistic inference, and implements an instance of it for few-shot learning. First, they propose Meta-Learning Probabilistic inference for Prediction (ML-PIP) which trains the meta-learner to minimize the KL-divergence between the approximate predictive distribution generated from it and predictive distribution for each class. Then, they use this framework to implement Versatile Amortized Inference, which they call VERSA. VERSA replaces the optimization for test time with efficient posterior inference, by generating distribution over task-specific parameters in a single forward pass. The authors validate VERSA against amortized and non-amortized variational inference which it outperforms. VERSA is also highly versatile as it can be trained with varying number of classes and shots.\n\nPros\n- The proposed general meta-learning framework that aims to learn the meta-learner that approximates the predictive distribution over multiple tasks is quite novel and makes sense.\n- VERSA obtains impressive performance on both benchmark datasets for few-shot learning and is versatile in terms of number of classes and shots.\n- The appendix section has in-depth analysis and additional experimental results which are quite helpful in understanding the paper.\n\nCons\n- The main paper feels quite empty, especially the experimental validation parts with limited number of baselines. It would have been good if some of the experiments could be moved into the main paper. Some experimental results such as Figure 4 on versatility does not add much insight to the main story and could be moved to appendix.\n- It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. \n\nIn sum, since the proposed meta-learning probabilistic inference framework is novel and effective I vote for accepting the paper. However the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Meta-Learning Probabilistic Inference for Prediction",
            "review": "This paper presents two different sections:\n1. A generalized framework to describe a range of meta-learning algorithms.\n2. A meta-learning algorithm that allows few shot inference over new tasks without the need for retraining. The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. This reduces the number of parameters to amortize during meta-training. More importantly, it makes it independent of the number of classes in a task, and effectively doing meta-training across class inference instead of each task. The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical. \n\nOverall, I feel the paper makes some progress in important aspects of meta-learning.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Few-shot learning, based on amortized inference network for parameters of logistic regression head models. Uses learning criterion based on predictive distributions on train/test splits. Extensive comparison, achieves state-of-the-art despite simpler setup than many competitors",
            "review": "Summary:\nThis work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.\n\nIt provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.\n\n- Quality: Several interesting differences to prior work. Well-done experiments\n- Clarity: Clean derivation, easy to understand. Some details could be spelled out better\n- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to \"neural processes\" work, but this happened roughly at the same time\n- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive\n\nInteresting about this work:\n- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether\n   an inference network of this simple structure (no correlations, sum combination\n   of datapoints, same network for each class) can deliver a good approximation to\n   the true posterior.\n- Different to previous work, task-specific inference is done only on the weights of\n   single-layer head models (logistic regression models, with shared features).\n   Highly encouraging that this is sufficient for state-of-the-art few-shot classification\n   performance. The authors could be more clear about this point.\n- Simple and efficient amortized inference model, which along with the neural\n   network features, is learned on all data jointly\n- Optimization criterion is based on predictive distributions on train/test splits, not\n   on the log marginal likelihood. Has some odd consequences (question below),\n   but clearly works better for few-shot classification\n\nExperiments:\n- 5.1: Convincing results, in particular given the simplicity of the model setup and\n   the inference network. But some important points are not explained:\n   - Which of the competitors (if any) use the same restricted model setup (inference\n      only on the top-layer weights)? Clearly, MAML does not, right? Please state this\n      explicitly.\n   - For Versa, you use k_c training and 15 test points per task update during\n      training. Do competitors without train/test split also get k_c + 15 points, or\n      only k_c points? The former would be fair, the latter not so much.\n- 5.2: This seems a challenging problem, and both your numbers and reconstructions\n   look better than the competitor. I cannot say more, based on the very brief\n   explanations provided here.\n   The main paper does not really state what the model or the likelihood is. From\n   F.4 in the Appendix, this model does not have the form of your classification\n   models, but psi is input at the bottom of the network. Also, the final layer has\n   sigmoid activation. What likelihood do you use?\n   One observation: If you used the same \"inference on final layer weights\" setup\n   here, and Gaussian likelihood, you could compute the posterior over psi in closed\n   form, no amortization needed. Would this setup apply to your problem?\n\nFurther questions:\n- Confused about the input to the inference network. Real Bayesian inference would\n   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in\n   then?\n   Please do improve the description of the inference network, this is a major\n   novelty of this paper, and even the appendix is only understandable by reading\n   other work as well. Be clear how it depends on theta (I think nothing is lost by\n   feeding in the h_theta(x)).\n- The learning criterion based on predictive distributions on train/test splits seem\n   to work better than ELBO-like criteria, for few-shot classification.\n   But there are some worrying aspects. The marginal likelihood has an Occam's\n   razor argument to prevent overfitting. Why would your criterion prevent overfitting?\n   And it is quite worrying that the prior p(psi | theta) drops out of the method\n   entirely. Can you comment more on that?\n\nSmall:\n- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more\n   general notation early on, if you do not do it later on. This is confusing\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}