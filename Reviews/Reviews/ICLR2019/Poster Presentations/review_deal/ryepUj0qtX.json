{
    "Decision": {
        "metareview": "The conditional network embedding approach proposed in the paper seems nice and novel, and consistently outperforms state-of-art on variety of datasets; scalability demonstration was added during rebuttals, as well as multiple other improvements; although  the reviewers did not respond by changing the scores, this paper with augmentations provided during the rebuttal appears to be a useful contribution  worthy of publishing at ICLR. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An intresting, novel approach to the network embedding problem on challenging graph structures, with uniformly better than state-of-art empirical results"
    },
    "Reviews": [
        {
            "title": "use prior distribution to constrain network embedding. novelty may not be high enough",
            "review": "The paper proposed to use a prior distribution to constraint the network embedding. The paper used very restricted Gaussian distributions for the formulation. The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose a generative model of networks via embeddings with the addition of a prior distribution over networks which facilitates learning more semantic embeddings. They use this model successfully in a variety of tasks.",
            "review": "The authors propose a generative model of networks by learning embeddings and pairing the embeddings with a prior distribution over networks. The idea is that the prior distribution may explain structure that the embeddings would not have to capture.\n\nThe motivation for doing this is that this structure is typically hard to model for network embeddings.\nThe authors propose a clean -if improper- prior on networks and proceed to perform maximum likelihood inference on it.\nThe experiments show that the approach works fine for link porediction and can be used for visualization.\n\nTwo points: \na) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.\nb) The prior seems to be picked according to properties of the observed data and expressed in a product of constraints. This seems clunky, I would have been more impressed with a prior structure that ties in closer with the embeddings and requires less hand-engineering.\n\nA key point of interest is the following: very exciting recent work (GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models by You et al ICML2018) has proposed neural generative models of networks with a high degree of fidelity and much less hand-picked features.  The work here tries to not learn a lot of these structures but impose them. Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?\nThe drawback in this model here is that ultimately networks are embedded, but not really generated during test time. A more predictive generative model that makes less hard assumptions on graph data would be interesting.\n\nUpdate After rebuttal:\nGiven the authors' rebuttal to all reviews, I am upgrading my score to a 6. I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but some parts of the paper are not clear",
            "review": "This paper studied learning unsupervised node embeddings by considering the structural properties of networks. Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings. \n\nStrength:\n- important problem and interesting idea\n- the proposed approach seems to be effective according to the experiments\nWeakness:\n- some parts of the paper are quite unclear\n- the complexity of the proposed algorithm seems to be very high\n- the data sets used in the experiments are very small\n\nDetails:\n-In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", why do we have to make v and v'(and u, and u') far from each other?\n- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part\n- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, rightï¼Ÿ\n- In Table 2 and 3, how are the degree and block information leveraged into the model?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}