{
    "Decision": {
        "metareview": "This paper is about unsupervised learning for ASR, by matching the acoustic distribution, learned unsupervisedly, with a prior phone-lm distribution. Overall, the results look good on TIMIT. Reviewers agree that this is a well written paper and that it has interesting results.\n\nStrengths\n- Novel formulation for unsupervised ASR, and a non-trivial extension to previously proposed unsupervised classification to segmental level.\n- Well written, with strong results. Improved results and analysis based on review feedback.\n\nWeaknesses\n- Results are on TIMIT -- a small phone recognition task.\n- Unclear how it extends to large vocabulary ASR tasks, and tasks that have large scale training data, and RNNs that may learn implicit LMs. The authors propose to deal with this in future work.\n\nOverall, the reviewers agree that this is an excellent contribution with strong results. Therefore, it is recommended that the paper be accepted.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel formulation with strong results"
    },
    "Reviews": [
        {
            "title": "Novel unsupervised cost function for phoneme recognition with promising results",
            "review": "This paper presents a method to learn an acoustic model for phoneme recognition with only the training input acoustic features and a pretrained phoneme LM. This is done by matching the output phoneme sequence distribution of the training set with the phoneme LM distribution. The cost function is proposed by extending a previously proposed unsupervised cost function (Empirical-ODM) to the segmental level, and integrating an intra-segment cost function to encourage the frame-wise output distribution to be similar to each other within a segment. The authors conducted thorough experiments on TIMIT phoneme recognition and demonstrated impressive results.\n\nThe paper is technically sound and the presentation is generally clear. The idea is interesting and novel by extending a previous unsupervised sequence modeling approach to speech recognition and exploiting the segmental structure of the problem. Unsupervised learning is an important research topic, and its application to potentially save high cost of human labeling for developing ASR systems is important to the community.\n\nHere are a few general comments/questions:\n\n1. It would be interesting to see whether and how much using a larger acoustic training set and a phoneme LM trained on more data can close the gap between unsupervised and supervised performance. Also it would be great to see how well the learned acoustic model performs in a full ASR system together with the lexicon and LM to predict words, which could generate more accurate unsupervised transcript than the acoustic model itself for refining the model further. These could be done in future work.\n\n2. The current cost function is based on matching the N-gram distribution in the phoneme LM and that in the DNN acoustic model output of the training set, where N is relatively small. How could the framework be extended for the state-of-the-art LM and AM with RNNs where the history is arbitrarily long?\n\n3. Why can this paper just use a larger mini-batch size to alleviate the effect that SGD is intrinsically biased for the Empirical-ODM functional form, while Liu et al. 2017 needed to propose the Stochastic Primal-Dual Gradient approach?\n\n4. The paper compares the unsupervised cost function with the supervised cross-entropy function in terms of quality. How about training time? The computation looks expensive for the unsupervised case since it needs to go through all possible N-grams (which is approximated by the most frequent 10000 5-grams according to Appendix B but still a large space).\n\n5. If the segmentation quality affects the learned acoustic model quality, why not also report the segmentation accuracy for all unsupervised systems and iterations, including the Wang et al. 2017 system?\n\nMore specific comments:\n\n7. The outer summation in Eq(1) seems to indicate summing over all possible \\tau, which is infeasible. Please clarify how it is computed.\n8. Eq(5): \"p(y_t | y_1 ... y_t)\" should be \"p(y_t | y_1 ... y_{t-1})\".\n9. Why are there periodic spikes in both self-validation loss and validation FER in Figure 2(a)? What training stage do they correspond to?\n10. In Figure 2, \"validation error\" in the y-axis should probably be \"validation FER\". In Figure 2(b), the number ranges on the left and right of the y-axis were probably swapped.\n11. Section 2.5: why is Eq(1) instead of Eq(3) used for the self-validation loss?\n12. Conclusion: \"the a potential\" -> \"the potential\".",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Success on practical task using unsupervised learning, interesting problem, comprehensive study and experiments. ",
            "review": "This paper proposes fully unsupervised learning algorithm for speech recognition. It involves two alternating trained component, a phoneme classifier, and a boundary refining model. The experiment results demonstrate that it achieves first success on speech recognition that approaches the supervised learning performance.  \n\nPros:\n+ The paper propose to use a frame-wise smoothing term J_FS added on J_ODM cost. In the new cost function, J_ODM controls the coarse-grained inter-segment distribution using a prepared language model P_LM, while J_FS controls the fine-grained intra-segment distribution. It is actually benefit to take use of this hierarchical 2-level scopes than only 1-level scope on evaluate the distribution mismatch in the cost function. Because otherwise, if only focus on fine-grained frame level,  much larger number of frame labels and longer N-gram have to be considered to evaluate the distribution of phoneme. Consequently, the computation can be exploding. \n+ The proposed unsupervised phoneme classification method is superior to the baseline (Liu et al., 2018) because the baseline relies on a clustering which is upper-bounded by cluster purity. Directly optimize on \\theta using an end-to-end scheme is preferred. \n+ I like the idea to use an iterative training algorithm to jointly improve classifier parameter \\theta and segment boundaries b. \n+ It is quite impressive that unsupervised learning system get close to performance of supervised system on speech recognition. The proposed system also outperforms state-of-the-art baseline with large margin. \n+ The settings of experiments are rather comprehensive. Especially the “non-matching language model”, tests the case where language model cannot directly estimated from training set.  \n\nQuestions:\n1.\tIn Appendix B you mentioned that for the N-gram you choose N=5. So the original language model P_LM can be a high-dim matrix with exactly 39^5 elements. How sparse is the original P_LM? It describes that 10000 elements are chosen, which are only 0.001%(=10000/39^5) of elements in the original one. How representative are they?\n\n2.\tI notice for the balance weight of J_FS in (3), you empirically take the best \\lambda=1e-5 during experiment. To me, the scale of optimal \\lambda is such small value maybe because the order of J_FS is improperly determined. My suggestion is, could you try using square root on the current J_FS, or using standard deviation of intra-segment outputs. The reasons are, first, minimizing std is a more interpretable penalty on diversion in a same segment; second, since you have used mean of outputs in J_ODM, then it is better to use a same dimension statistics, such as std of outputs in J_FS rather than sum of squared differences, when you combine J_ODM and J_FS in a uniform cost.\n\n3.\tWhat is the time complexity of running a comparable supervised speech recognition task with unsupervised learning method? \n\nMinor issues:\nMaybe it is a typo that the second term of Eqn (2) should be “-p_\\theta(y_(t+1)=y|x_(t+1))” instead? Since the p_\\theta is defined as posterior probability of the frame label given the corresponding input. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but should improve ",
            "review": "Overview:\n\nThis paper proposes a new approach to do unsupervised phoneme recognition by learning from unlabelled speech in combination with a trained phoneme language model. The proposed loss function is a combination of a term encouraging the language model of predicted phonemes to match the given language model distribution, and a term to encourage adjacent speech frames to be assigned to the same phoneme class. Phoneme boundaries are iteratively refined using a separate model. Experiments where a hidden Markov model is applied on top of the predicted phonemes are also performed.\n\n\nMain strengths:\n\nThe paper is clear and addresses a very important research problem. The approach and losses proposed in Section 2 have also not been proposed before, and given that an external language model is available, are very natural choices.\n\n\nMain weaknesses:\n\nThe main weakness of this paper is that it does not situate itself within the rich body of literature on this problem.  I give several references below, but I think the authors can include even more studies -- there are several studies around \"zero-resource\" speech processing, and I would encourage the authors to work through the review papers [1, 6].\n\nConcretely, I do not think the authors can claim that \"this is the first fully unsupervised speech recognition method that does not use any oracle segmentation or labels.\" I think it could be argued that the system of [3] is doing this, and there are even earlier studies. I also don't think this claim is actually necessary since the paper has enough merit to stand on its own, as long as the related work is discussed properly.\n\nFor instance, the proposed approach shares commonalities with several other approaches: [2] also used two separate steps for acoustic modelling and boundary segmentation; [4, 7, 8] builds towards the setting where non-matching text data is available (for language model training) together with untranscribed speech for model development; the approach of [5] uses a very similar refinement step to the one described in Section 3, where an HMM model is initialised and retrained on noisy predicted labels.\n\nIn the experiments (Section 4), it would also be useful to report more fine-grained metrics. [6] gives an overview of several of the standard metrics used in this area, but at a minimum phoneme boundary recall, precision and F-scores should be reported in order to allow comparisons to other studies.\n\n\nOverall feedback:\n\nGiven that this paper is situated within the broader context of this research area, which already has a small community around it, I think the novelty in the approach is strong enough to warrant publication given that the additional metrics are reported in the experiments.\n\n\nPapers/links that should be reviewed and cited:\n\n1. E. Dunbar et al., \"The Zero Resource Speech Challenge 2017,\" in Proc. ASRU, 2017.\n2. H. Kamper, K. Livescu, and S. Goldwater. An embedded segmental k-means model for unsupervised segmentation and clustering of speech. in Proc. ASRU, 2017.\n3. Lee, C.-y. and Glass, J. R. A nonparametric Bayesian approach to acoustic model discovery. ACL, 2012.\n4. Ondel, Lucas, Lukaš Burget, Jan Černocký, and Santosh Kesiraju. \"Bayesian phonotactic language model for acoustic unit discovery.\" In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pp. 5750-5754. IEEE, 2017.\n5. Walter, O., Korthals, T., Haeb-Umbach, R., and Raj, B. (2013). A hierarchical system for word discovery exploiting DTW-based initialization. ASRU, 2013.\n6. M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, \"The Zero Resource Speech Challenge 2015: Proposed approaches and results,” in Proc. SLTU, 2016.\n7. https://www.clsp.jhu.edu/wp-content/uploads/sites/75/2018/05/jsalt2016-burget-building-speech-recognition.pdf\n8. https://www.clsp.jhu.edu/workshops/16-workshop/building-speech-recognition-system-from-untranscribed-data/\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}