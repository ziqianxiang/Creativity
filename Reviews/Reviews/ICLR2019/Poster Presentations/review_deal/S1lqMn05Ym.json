{
    "Decision": {
        "metareview": "Strengths\n\nThe paper introduces a promising and novel idea, i.e., regularizing RL via an informationally asymmetric default policy \nThe paper is well written.  It has solid and extensive experimental results.\n\nWeaknesses\n\n\nThere is a lack of benefit on dense-reward problems as a limitation, which the authors further\nacknowledge as a limitation. There also some similarities to HRL approaches. \nA lack of theoretical results is also suggested. To be fair, the paper makes a number of connections\nwith various bits of theory, although it perhaps does not directly result in any new theoretical analysis.\nA concern of one reviewer is the need for extensive compute, and making comparisons to stronger (maxent) baselines.\nThe authors provide a convincing reply on these issues.\n\nPoints of Contention\n\nWhile the scores are non-uniform (7,7,5), the most critical review, R1(5), is in fact quite positive on many\naspects of the paper, i.e., \"this paper would have good impact in coming up with new \nlearning algorithms which are inspired from cognitive science literature as well as mathematically grounded.\"\nThe specific critiques of R1 were covered in detail by the authors.\n\nOverall\n\nThe paper presents a novel and fairly intuitive idea, with very solid experimental results.  \nWhile the methods has theoretical results, the results themselves are more experimental than theoretic.\nThe reviewers are largely enthused about the paper.  The AC recommends acceptance as a poster.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "intuitive idea & theoretical connections; solid experimental results"
    },
    "Reviews": [
        {
            "title": "Novel approach",
            "review": "This paper shows that significant speed-up gains can be achieved by using KL-regularization with information asymmetry in sparse-reward settings.  Different from previous works, the policy and default policy are learned simultaneously.  Furthermore, it demonstrates that the default policy can be used to perform transfer learning.\n\nPros:\n\n- Overall the paper is well-written and the organization is easy to follow.  The approach is novel and most relevant works are compared and contrasted.  The intuitions provided nicely complements the concepts and experiments are thorough.\n\nCons:\n\n- The idea of separating policy and default policy seems similar to having high and low level controller (HLC and LLC) in hierarchical control -- where LLC takes proprioceptive observations as input, and HLC handles task specific goals.  In contrast, one advantage of the proposed method in this work is that the training is end-to-end.  Would have liked to see comparison between the proposed method and hierarchical control.\n\n- As mentioned, the proposed method does not offer significant speed-up in dense-reward settings.  Considering that most of the tasks experimented in the paper can leverage dense shaping to achieve speed-up over sparse rewards, it'd be nice to have experiments to show that for some environments the proposed method can out-perform baseline methods even in dense-reward settings.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This is a very interesting piece of work. We know from cognitive science literature, that there are 2 distinct modes of decision making - habit based and top-down control (goal directed) decision making. The paper proposes to use this intuition by using information theoretic objective such that the agent follows \"default\" policy on average and agent gets penalized for changing its \"default\" behaviour, and the idea is to minimize this cost on average across states.\n\nThe paper is very well written. I think, this paper would have good impact in coming up with new learning algorithms which are inspired from cognitive science literature as well as mathematically grounded. But I dont think, paper in its current form is suitable for publication. \n\nThere are several reasons, but most important:\n\n1) Most of the experiments in this paper use of the order of  10^9 or even 10^10 steps. Its practically not possible for anyone in academia to have such a compute. Now, that said, I do think this paper is pretty interesting. Hence, Is it possible to construct a toy problem which has similar characteristics, and then show similar results using like 10^6 or 10^7 steps ? I think it would be easy to construct a 2D POMPD maze navigation env and test similar results. This would improve the paper, as well as could provide a baseline which people in the future can compare to.\n\n2) It becomes more important to compare to stronger baselines like maximum entropy RL ( for ex. Soft Actor Critic). And spend some good of amount time getting these baselines right on these new environments. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good work in general",
            "review": "\n-- Originality --\n\nThis paper studies how to use KL-regularization with information asymmetry to speed up and improve reinforcement learning (RL). Compared with existing work, the major novelty in the proposed algorithm is that it uses a default policy learned from data, rather than a fixed default policy. Moreover, the proposed algorithm also limits the amount of information the default policy receives, i.e., there is an \"information asymmetry\" between the agent policy and the default policy. In many applications, the default policy is purposely chosen to be \"goal agnostic\" and hence conducts the \"transfer learning\". To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n-- Clarify --\n\nThe paper is well written in general and is easy to follow.\n\n-- Significance --\n\nI think the idea of regularizing RL via an informationally asymmetric default policy is interesting. It might be an efficient way to do transfer learning (generalization) in some RL applications. This paper has also done extensive and rigorous experiments. Some experiment results are thought-provoking.\n\n-- Pros and Cons\n\nPros:\n\n1)  The idea of regularizing RL via an informationally asymmetric default policy is interesting. To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n2) The experiment results are extensive, rigorous, and thought-provoking.\n\nCons:\n\n1) My understanding is that this \"informationally asymmetric\" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain.\n\n2) This paper does not have any theoretical results. I fully understand that it is highly non-trivial or even impossible to analyze the proposed algorithm in the general case. However, I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.\n\n3) The experiment results of this paper are interesting, but I think the authors can do a better job of intuitively explaining the experiment results. For instance, the experiment results show that when the reward is \"dense shaping\", the proposed method and the baseline perform similarly. Might the authors provide an intuitive explanation for this observation? I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}