{
    "Decision": {
        "metareview": "Strengths: This paper is \"thorough and well written\", exploring the timbre transfer problem in a novel way. There is a video accompanying the work and some reviewers assessed the quality of the results as being good relative to other approaches. Two of the reviewers were quite positive about the work.\n\nWeaknesses: Reviewer 2 (the lowest scoring reviewer) felt that the paper was a little too far from solving the problem to be of high significance and that there was:\n - too much focus on STFT vs. CQT\n - too little focus on getting WaveNet synthesis right\n - too limited experimental validation (too restricted choice of instruments)\n - poor resulting audio quality\n - feels too much of combining black boxes\n\nAMT listening tests were performed, but better baselines could have been used.\nThe author response addressed some of these points.\n\nContention: \nAn anonymous commenter noted that the revised manuscript added some names in the acknowledgements, thereby violating double blind review guidelines. However, the aggregated initial scores for this work were past the threshold for acceptance. Reviewer 2 was the most critical of the work but did not engage in dialog or comment on the author response. \n\nConsensus:\nThe two positive reviewers felt that this work is worth of presentation at ICLR. The AC recommends accept as poster unless the PC feel the issue of names in the Acknowledgements in an updated draft is too serious of an issue.\n ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting paper, has some issues, but would be of interest to the community"
    },
    "Reviews": [
        {
            "title": "interesting idea, but weak experimental validation, and too much of combining black-boxes",
            "review": "The paper proposes a method for converting recordings of a specific musical instrument to another. The proposed approach is apply CycleGAN, which was developed for image style transfer, to transfer spectrograms. The synthesis is done using WaveNet.\n\nThe paper is interesting in the core idea. It demonstrates that this combination of building blocks can indeed map recordings while achieving certain characteristics of the target instrument.\n\nThe paper correctly describes \"timbre\" as a catch-all term for characterizing instruments besides pitch and volume. The success of the method should be judged along two dimensions, which are both very subjective:\n - Does the method transfer \"enough\" of the target instrument's characteristics?\n - Is the resulting audio quality sufficient?\n\nThe paper is easy to follow for someone with background in signal processing. I believe it is sufficiently easy to follow for readers with general computer-science and machine-learning background.\n\nThe paper focusses a lot on the choice of spectral representation. It compares short-term Fourier transform (STFT), which is the generic standard, and Constant-Q Transforms (CQT), a variant of STFT that uses a logarithmic frequency axis. To someone with signal-processing background, the choice of CQT seems logical and not something that would be challenged strongly as long as a simple comparison to STFT confirms that it works a bit better. I find the comparison between the two too dominant in the paper, and distracting from the other issues that I feel are more important (see below). For example, Section 6.2 states \"We demonstrate that the aforementioned translation does indeed result in a perceivable pitch shift when fed to our conditional WaveNet.\" But that is trivial: Since changing the playback sample rate by a few half steps does not fundamentally alter the perceived timbre of an instrument, and such a change will, by construction of CQT, shift the CQT representation, and since the WaveNet has seen examples of the source instrument for all notes, shifting the CQT respresentation must necessarily result in a perceived pitch change in the re-synthesized wveform that does not fundamentally change its timbre. The real question here is whether the reconstructed signal is of the same quality as, for example, a simple PSOLA-based pitch change would be.\n\nA larger problem of the paper is that the result section seems to only test two instrument mappings, violin to flute, and piano to harpsichord. One notes that these instrument pairs mostly differ in spectral envelope, while they are rather similar in longer-term temporal variations, such as what is sometimes characterized as the ADSR curve (attack-decay-sustain-release) and vibrato. These, in my view, are very important aspects of a musical instrument's characteristics, which are not addressed by the paper. (Whether they are considered part of \"timbre\" is not clear, but without mapping these, one cannot meaningfully speak of mapping instruments, which is the end goal of this paper.)\n\nAnother big problem in my view is that the audio quality is just not good. I hear a lot of musical-noise artifacts and local timbre modulations. Also it is not clear why the source material is of poor quality (sounds quite noisy, most likely in part due to mu-law 8-bit encoding, and they sound like 11 kHz recordings), for which there is no justification in 2018.\n\nLastly, I am not happy with the \"beam-search\" approach. That approach is used to post-correct imperfections in the WaveNet synthesis. It samples multiple generation hypotheses, and re-weights hypotheses by how well they match the original CQT when converted back. The need for this indicates a fundamental flaw in the WaveNet synthesizer. The authors explicitly say they did not want to fix the WaveNet algorithm itself. In my view, this is what should have been done.\n\nThe authors should focus much more on how to achieve sufficient WaveNet synthesis quality. This should be the main bulk of the paper, and would be a requirement for me to accept the paper.\n\nSo overall, the paper feels a little too much of combining black boxes.\n\nIn terms of significance, I would not think that this paper is getting near solving this problem, hence I rate it of less significance in the current state of results.\n\nPros:\n - interesting idea\n - reasonable approach by combining existing building blocks\n\nCons:\n - too much focus on STFT vs. CQT\n - too little focus on getting WaveNet synthesis right\n - too limited experimental validation (too restricted choice of instruments)\n - poor resulting audio quality\n - feels too much of combining black boxes\n\nAs a result I rate the paper \"not good enough\" in its current form.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Timbre can be tranferred pretty well using a constant-Q transform for features, followed by a CycleGAN to do the transfer, followed by a Wavenet to resynthesize it to audio.",
            "review": "Main Idea: The authors use multiple techniques/tools to enable neural timbre transfer (converting music from one instrument to another, ex: violin to flute) without paired training examples. The authors are inspired by the success of CycleGANs for image style transfer, and by the success of Wavenet for generating realistic audio waveforms. Even without the CycleGAN, the use of CQT->WaveNet for time stretching and pitch shifting of a single piece is an interesting and valuable contribution.\n\nMethodology: Figure 1 captures the overall timbre-conversion methodology concisely. In general the details of the methodology look sound. The lengthy appendices offer additional implementation details, but without access to a source code repository, it is hard to say if the results are perfectly reproducible.\n\nExperiment and Results: Measuring the quality of generated audio is challenging.  To do so, subjective listening tests are conducted on Amazon mechanical turk, but without a comparison to a baseline system except for another performance of the target piece. Note that there are few published timbre-transfer methods (see Similar Work).\n\nOne issue with the AMT survey is that the total number of workers is not reported, and as such the significance of the results can be questioned.\n\nSignificance: In my mind, the paper offers validation of the three techniques used. CycleGANs, originally designed for images,  are shown to work for style transfers on audio spectrograms. Wavenet's claim to be a generic technique for audio generation is tested and validated for this domain (CQT spectrogram to audio). That CQT outperforms STFT on musical data seems to be a well established result already, but this offers further validation.\n\nThis paper also offers practical advice for adapting the techniques/tools (Wavenet, CycleGAN, CQT) to the timbre-transfer task.\n\n\nSimilar Work:\n\nI have only found 2 papers dedicated to timbre transfer in the field of Learning Representations.\n\nBitton, Adrien, Philippe Esling, and Axel Chemla-Romeu-Santos. \"Modulated Variational auto-Encoders for many-to-many musical timbre transfer.\" arXiv preprint arXiv:1810.00222 (2018).\n\nwhich was published on sept 29th 2018, so less than 30 days ago, which is fine according to the reviewer guidelines.\n\n\nVerma, Prateek, and Julius O. Smith. \"Neural style transfer for audio spectograms.\" arXiv preprint arXiv:1801.01589 (2018).\n\nwhich is a short 2 page exploratory paper.\n\n \nIt could be useful to cite:\n\nShuqi Dai, Zheng Zhang, Gus G. Xia.  \"Music Style Transfer: A Position Paper.\" arXiv preprint arXiv:1803.06841 (2018)\n\n\nWriting Quality\n\nOverall the paper is written well with clear sentences.\n\nCertain key information would be useful to move from the appendices to the main body of the paper.  This includes the number of AMT workers, the size of the CQT frame/hop over which they are summarized, and the set of instruments that are being used in the experiments.\n\n\nSome minor nitpicks: \n\nsection 6.3, sentence 2 needs to be reworked. ('After moving on to real world data, we noticed that real world data is harder to learn because compared to MIDI data it’s more irregular and more noisy, thus makes it a more challenging task.') \n\nsection 3.2 sub-section 'Reverse Generation', sentence 1 uses the word 'attacks' for the first time. Please explain this for those not familiar.\n\nsection 3.1, sentence 3 has a typo, 'Thanks' is wrongly capitalized.\n\ntable 1 (and other tables in appendix), 'Percentage' (top left) does not add anything to the table.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Compelling results on timbre transfer, backed up by human evaluation",
            "review": "Summary\n-------\nThis paper describes a model for musical timbre transfer.\nThe proposed method uses constant-Q transform magnitudes as the input representation, transfers between domains (timbres) by a CycleGAN-like architecture, and resynthesizes the generated CQT representation by a modified WaveNET-like decoder. The system is evaluated by human (mechanical turk) listening studies, and the results indicate that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.\n\n\nHigh-level comments\n-------------------\n\nThis paper is extremely well written, and the authors clearly have a great attention to detail in both the audio processing and machine learning domains.  Each of the modifications to prior work was well motivated, and the ablation study at the end, while briefly presented, provides a good sense of the contributions of each piece.\n\nI was unable to listen to the examples provided by the link in section 6, which requires a Microsoft OneDrive login to access.  However, the youtube link provided in the ICLR comments gave a reasonable sample of the results of the system.  Overall, the outputs sound compelling, and match my expectations given the reported results of the listening studies.\n\nOn the quantitative side, it would have been nice to see a measurement of phase retrieval of the decoder component, which could be done in isolation from the transfer components by feeding in original CQT magnitudes.  This might help give a sense of how well the model can be expected to perform, particular as it breaks down along target timbres.  I would expect some timbres to be easier to model than others, and having a quantitative handle on that could help put the listener study in a bit more perspective.\n\nDetailed comments\n-----------------\n\nThe paper contains numerous typos and grammatical quirks, e.g.:\n    - page 5: \"GP can stable GAN training\"\n    - page 7: \"CQT is equivalent to pitch\"\n\n\nThe reverse-generation trick in section 3.2 was clever!\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}