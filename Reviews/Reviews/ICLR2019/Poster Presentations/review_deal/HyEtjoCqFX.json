{
    "Decision": {
        "metareview": "The paper proposes a new RL algorithm (MIRL) in the control-as-inference framework that learns a state-independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.\n\nThe paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.\n\nIn a revised version, the authors are encouraged to\n  (1) include a discussion of when MIRL might fail, and\n  (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:\n    https://arxiv.org/abs/1705.07798\n    http://proceedings.mlr.press/v70/asadi17a.html\n    http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning\n    http://proceedings.mlr.press/v80/dai18c.html",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting contribution that improves on the widely used entropy regularized algorithms"
    },
    "Reviews": [
        {
            "title": "Interesting idea, more experimental results needed",
            "review": "** Summary: **\n\nThe authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions.\n\n** Quality: **\nThe paper is mathematically detailed and correct.\n\n** Clarity: **\nThe paper is sufficiently easy to follow and explains all the necessary background.\n\n** Originality & Significance: **\nThe paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. \n\nMy two main points where I think the paper could improve are:\n- More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution?\n- A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple approach that appears to work well",
            "review": "This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective.\n\nA more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution.\n\nThey also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games.\n\nThis work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information?\n\nEmpirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result.\n\nThe fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion.\n\nOverall it appears this work provides:\n- An algorithm for Soft Q learning with a learned independent prior\n- Moderate evidence for gains compared with a flat prior on Atari.\n- A connection with this approach and regularization by constraining the mutual information between state and action distributions.\n\nIt could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, but motivation and experiments need improvements",
            "review": "The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.\n\nGenerally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.\n\nIn Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \\log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? \n\nThe experiments could be strengthened by addressing the following:\n* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.\n* What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.\n* Plotting beta over time\n* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. \n* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it?\n* How many seeds were run per game?\n* How and why were the 19 games selected from the full set?\n\nComments:\n\nThe abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.\n\nWith a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.\n\nCould state that the stationary distribution is assumed to exist and be unique.\n\nIn Sec 3.1, why is the prior state independent?\n\nIn Sec 3.1, p(R = 1|\\tau) is defined to be proportional to exp(\\beta \\sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \\tau) is not defined?\n\nThroughout, I suggest that the authors not use the phrases \"closed form\" and \"analytic\" for expressions that are in terms of intractable quantities. \n\nIt should be noted that Sec 3.2 Optimal policy for a fixed prior \\rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.\n\nIn Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?\n\nI believe that the connection to MI can be simplified. Plugging in the optimal \\rho into Eq 3, we can see that Eq 3 simplifies to \\max_\\pi E_q[ \\sum_t \\gamma^t r_t] - (1 - gamma)/\\beta MI_p(s, a) where p(s, a) = d^\\pi(s) * \\pi(a | s) and d^\\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.\n\nIn Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?\n\nSec 5 references the wrong Haarnoja reference in the first paragraph.\n\nIn Sec 5, alpha_beta = 3 * 10^5. Is that correct?\n\n=====\n11/26\nAt this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.\n\n====\n12/7\nThe authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}