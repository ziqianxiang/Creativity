{
    "Decision": {
        "metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- novel approach to audio synthesis\n- strong qualitative and quantitative results\n- extensive evaluation\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- small grammatical issues (mostly resolved in the revision).\n \n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, itâ€™s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n \n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "novel approach with good results shown by extensive evaluation"
    },
    "Reviews": [
        {
            "title": "This paper proposes an approach that uses GAN framework to generate audio.",
            "review": "This paper proposes an approach that uses GAN framework to generate audio through modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Experiments on NSynth dataset show that it gives better results then WaveNet. The most successful deep generative models are WaveNET,  Parallel WaveNet and Tacotran that are applied to speech synthesis, the method should be tested for speech synthesis and compared with WaveNet, Parallel WaveNet as well as Tacotran.\n\nFor WaveNet, the inputs are text features, but for Tacotran, the inputs are mel-spectrogram. Here the inputs are log magnitudes and instantaneous frequencies. So the idea is not that much new.\n\nGAN has been used in speech synthesis, see \nStatistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks\nIEEE/ACM Transactions on Audio, Speech, and Language Processing ( Volume: 26 , Issue: 1 , Jan. 2018 )\n\nSo for this work, GAN's application to sound generation is not new.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting take on GAN audio synthesis - accept",
            "review": "This paper proposes a strategy to generate audio samples from noise with GANs. The treatment is analogous to image generation with GANs, with the emphasis being the changes to the architecture and representation necessary to make it possible to generate convincing audio that contains an interpretable latent code and is much faster than an autoregressive Wavenet based model (\"Neural Audio Synthesis of Musical Notes with WaveNet AutoEncoders\" - Engel et al (2017)). Like the other two related works (WaveGAN - \"Adversarial Audio Synthesis\" - Donahue et al 2018) and the Wavenet model above, it uses the NSynth dataset for its experiments. \n\nMuch of the discussion is on the representation itself - in that, it is argued that using audio (WaveGAN) and log magnitude/phase spectrograms  (PhaseGAN) produce poorer results as compared with the version with the unrolled phase that they call 'IF' GANs, with high frequency resolution and log scaling to separate scales.  \n\nThe architecture of the network is similar to the recently published paper  (Donahue et al 2018), with convolutions and transpose convolutions adapted for audio. However, there seem to be two important developments. The current paper uses progressive growing of GANs (the current state of the art for producing high resolution images), and pitch conditioning (Odena et al, where labels are used to help training dynamics). \n\nFor validation, the paper presents several metrics, with the recently proposed \"NDB\" metric figuring in the evaluations, which I think is interesting. The IF-Mel + high frequency resolution model seems to outperform the others in most of the evaluations, with good phase coherence and interpolation between latent codes. \n\nMy thoughts: \nOverall, it seems that the paper's contributions are centered around the representation (with \"IF-Mel\" being the best). The architecture itself is not very different from commonly used DCGAN variants - the authors say that using PGGAN is desirable, but not critical, and the use of labels from Odena et al. \n\nMany of my own experiments with GANs were plagued by instability (especially at higher resolution) and mode collapse problems without special treatment (largely documented, such as adding noise, adjusting learning rates and so forth). To this end, what do the authors see as 'high' resolution vis a vis audio signals? \n\nI am curious if we can adapt these ideas for recurrent generators as might appear in TTS problems. \n\nI rate this paper as an accept since this is one of the few existing works that demonstrate successful audio generation from noise using GANs, and  owing to its novelty in exploring representation for audio. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Exciting work",
            "review": "This is an exciting paper with a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied. The authors demonstrate the reliability of their method on a large dataset of acoustic instruments and report human evaluation metrics. I expect their proposed method of preprocessing audio to become standard practice.\n\nWhy didn't you train a WaveNet on the high-resolution instantaneous frequency representations? In addition to conditioning on the notes, this seems like it would be the right fair comparison. \n\nI'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?\n\nSmall comments:\n\n- Figure 1 & 2: label the x-axis as time. Makes it a lot easier to understand.\n\n- I appreciate the plethora of metrics. The inception score you propose is interesting. Very cool that number of statistically-different bins tracks human eval!\n\n- sentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar. \n\n- Figure 5 is low-res. Please fix. All the other figures are beautiful - nice work!",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}