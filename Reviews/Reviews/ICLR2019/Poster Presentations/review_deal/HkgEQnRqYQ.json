{
    "Decision": {
        "metareview": "This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self-adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.\n\nThe reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.\n\nThe reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation.\n\nWith the revision, the reviewers agreed that this is a worthy paper to include in the conference.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting idea, solid results, good analysis"
    },
    "Reviews": [
        {
            "title": "This paper is an important new contribution to the field.   The results should be compared to TorusE.",
            "review": "The authors propose to model the relations as a rotation in the complex vector space. They show that this way one can model symmetry/antisymmetry, inversion and composition. Another contribution is the so-called self-adversarial negative sampling.\n\nPros:  The problem that they raise is important and the solution is relevant. The results considering the simplicity of the proposed model are impressive. The experiments, proof of lemmas and general overview are easy to follow, well-written and well-organized.  The improvement given the negative sampling approach is also noteworthy.\n\nCons: Nevertheless, this approach is very similar to TorusE [1], since the element-wise rotation on the complex plane is somehow related to transformation on high-dimensional Torus. Therefore, it is expected from the authors to investigate the differences between these two approaches.\n\nSuggestions:\nAlso, it is important to note the result of ablation study on Table 10 in supplementary materials, since part of the improvement does not come only from how the authors model the relation but also from the negative sampling(which could improve the results of other works as well). Maybe it is even better if Table 10 is presented in the main paper. \nAnother suggestion is to mention the negative sampling contribution also in the abstract.\n\n\n[1] Ebisu, Takuma, and Ryutaro Ichise. \"Toruse: Knowledge graph embedding on a lie group.\" arXiv preprint arXiv:1711.05435 (2017).\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid work",
            "review": "The paper proposes a method for graph embedding to be used for link prediction, in which each entity is represented as a vector in complex space and each relation is modeled as a rotation from the head entity to the tale entity. \nFrom the modeling perspective, the proposed model is rich as many type of relations can be modeled with it. In particular, symmetric and anti-symmetric relations can be modeled. It is also possible to model the inverse of a relation and the composition of two relations with this setup. Empirical evaluation demonstrates that method is effective and beats a number of well known competitors.\n\nThis is a solid work and could be of interest in the community. Modeling is elegant and experimental results are strong.\nI have not seen it proposed before.\n\n- The presentation of paper could be improved, in particular the first paragraph of page 2 where the representation in complex domain is introduced is hard to follow and could be improved by inserting formulations instead of merely text.  \nIt would be nice to explicitly mention the number of real and imaginary dimensions of the complex vectors and provide explicit formulation for the Hadamard product on the complex domain, since the term elementwise could be ambiguous.\n- The optimization section does not mention how constraints are imposed. This is an important technicality and should be clarified.\n- In experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number? Each complex number is presented with two parameters and each real number with one parameter. How is that taken into account in experiments\n- Since the method is reported to beat several number of competitors, it is useful to provide the code.\n\n \nBased on the results above, I vote for the paper to be accepted.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Is it the RotatE scoring function or the adversarial sampling?",
            "review": "# Summary\nThis paper presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base, whereas previous methods were only able to support a subset. The method achieves state of the art on FB15k-237, WN18RR and Countries benchmark knowledge bases. I think this will be interesting to the ICLR community. I particularly enjoyed the analysis of existing methods regarding the expressiveness of relational patterns mentioned above.\n\n# Strengths\n- Improvements over prior neural link prediction methods\n- Clearly written paper\n- Interesting analysis of existing neural link prediction methods\n\n# Weaknesses\n- As the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, I believe a more careful ablation study should have been carried out. There is an ablation study showing the impact of the negative sampling on the baseline TransE, as well as another ablation in the appendix demonstrating the impact of negative sampling on TransE and the proposed method, RotatE, for the FB15k-237. However, from Table 10 in the appendix, one can see that the two competing methods, TransE and RotatE, in fact, perform fairly similarly once both use adversarial sampling it still remains unclear whether the gains observed in table 4 and 5 are due to adversarial sampling or a better scoring function. Particularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach. Ideally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results (as it has been done for Countries in Table 6).\n\n# Minor Comments\n- Eq 5: Already introduce gamma (the fixed margin) here.\n- While I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention as some of them can also model symmetry, anti-symmetry, inversion and composition patterns of relations as well (though they might be less scalable and therefore of less practical relevance), e.g. the following come to mind:\n  - Lao et al. (2011). Random walk inference and learning in a large scale knowledge base.\n  - Neelakantan et al. (2015). Compositional vector space models for knowledge base completion.\n  - Das et al. (2016). Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks. \n  - Rocktaschel and Riedel (2017). End-to-end Differentiable Proving.\n  - Yang et al. (2017). Differentiable Learning of Logical Rules for Knowledge Base Completion.\n- Table 6: How many repeats were used for estimating the standard deviation?\n\n\nUpdate: I thank the authors for their response and additional experiments. I am increasing my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}