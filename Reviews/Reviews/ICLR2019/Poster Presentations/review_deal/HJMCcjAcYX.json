{
    "Decision": {
        "metareview": "The paper proposes an architecture to learn over sets, by proposing a way\nto have permutations differentiable end-to-end, hence learnable by gradient\ndescent. Reviewers pointed out to the computational limitation (quadratic in\nthe size of the set just to consider pairwise interactions, and cubic overall).\nOne reviewer (with low confidence) though the approach was not novel but\ndidn't appreciate the integration of learning-to-permute with a differentiable\nsetting, so I decided to down-weight their score. Overall, I found the paper\nborderline but would propose to accept it if possible.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "An interesting method to learn (latent) permutations based on pairwise costs.",
            "review": "The authors introduce a method to learn to permute sets end-to-end. They define the cost of a permutation as the sum of pairwise costs induced by the permutation, where the pairwise costs are learned. Permutations are made differentiable by relaxing them to doubly stochastic matrices which are approximated with the Sinkhorn operator. In the forward pass of the algorithm, a good permutation (ie one with low cost) is obtained with a few steps of gradient descent (the forward pass itself contains an optimization procedure). This permutation is then either used directly as the output of the algorithm or is used to permute the original inputs and feed the permuted sequence to another module (such as an RNN or a CNN). The method can easily be adapted to other structures such as lattices by considering row-wise and column-wise pairwise relations.\n\nThe proposed method is benchmarked on 4 tasks:\n1. Sorting numbers, where they obtain very strong generalization results.\n2. Re-assembling image mosaics, on which they obtain encouraging results.\n3. Image classification through image mosaics.\n4. Visual Question Answering where the permuted inputs are fed to an LSTM  whose final latent state is fed back into the baseline model (a bilinear attention network). Doing so improves over feeding the inputs to an LSTM without learning the order.for which the output is the permutation itself and  classification from image mosaics and visual question answering which require to learn an implicit permutation.\n\nThe method is most similar to Learning Latent Permutations with Gumbel-Sinkhorn Networks (Mena et al) but considers pairwise relations when producing the permutation. This can have important advantages (such as taking local relations into account, as shown by the strong sorting results) but also drawbacks (inability to differentiate inputs with similar content), but in any case this represents a good step towards exploring with different cost functions.\n\nThe method can be quite unpractical (cubic time complexity in set cardinality, optimization in forward pass, having to preprocess the set into a sequence for another module can be resource expensive). \nExperimental results on toy tasks (tasks 1, 2 and 3) are encouraging. The approach improves over a relatively strong baseline (task 4) although it isn't clear that it would still hold true when controlling for number of parameters and compute.\n\nI have a few comments about the presentation (for which I would be willing to change my score to a 6):\n- When possible, please use the numbers reported by Mena et al and consider reporting error (instead of accuracy) as they do to ease comparison. The results that you report using their method are quite worse than what they report, so I think it would be fair to include both your reimplementation and the initial results in the table.\n- It would be interested to have some insights on what function f is learned (for the sorting task and re-assembling image mosaics for example).\n- Clarity would be improved with figures representing which neural networks are used at what part of the process.\n\n\n###########################################\nUpdated review:\n\nThe authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time. I have changed my score to a 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reinventing the (methodological) wheel?",
            "review": "Update: From the perspective of a \"broader ML\" audience, I cannot recommend acceptance of this paper. The paper does not provide even a clear and concrete problem statement due to which it is difficult for me to appreciate the results. This is the only paper out of all ICLR2019 papers that I have reviewed / read which has such an issue. Of course for the conference, the area chair / program chairs can choose how to weigh the acceptance decisions between interest to the broader ML audience and the audience in the area of the paper. \n\n----------------------------------------------------------------------------------------------------------------------------------\n\n This paper addresses the problem that often features are obtained as a set, whereas certain orders of these features are known to allow for easier learning. With this motivation the goal of this paper is to learn a permutation of the features. This paper makes the following three main contributions:\n1. The idea of using pairwise comparison costs instead of position-based costs\n2. The methodological crux of how to go from the pairwise comparison costs to the permutation (that is, solving Eqn. (2) using  Eqn. (1) )\n3. An empirical evaluation\n\nI like the idea and the empirical evaluations are promising. However, I have a major concern about the second contribution on the method. There is a massive amount of literature on this very problem and a number of algorithms are proposed in the literature. This literature takes various forms including rank aggregation and most popularly the (weighted) minimum feedback arc set problem.  The submitted paper is oblivious to this enormous literature both in the related work section as well as the empirical evaluations. I have listed below a few papers pertaining to various versions of the problem (this list is by no means exhaustive). With this issue, I cannot give a positive evaluation of this submitted paper since it is not clear whether the paper is just re-solving a solved problem. That said, I am happy to reconsider if the related work and the empirical evaluations are augmented with comparisons to the past literature on the methodological crux of the submitted paper (e.g., why off-the-shelf use of previously proposed algorithms may or may not suffice here.)\n\n\nUnweighted feedback arc set:\n\nA fast and effective heuristic for the feedback arc set problem, Eades et al.\n\nEfficient Computation of Feedback Arc Set at Web-Scale, Simpson et al.\n\nHow to rank with few errors, Kenyon-Mathieu et al.\n\nAggregating Inconsistent Information: Ranking and Clustering, Ailon et al.\n\n\nHardness results:\n\nThe Minimum Feedback Arc Set Problem is NP-hard for Tournaments, Charbit et al.\n\n\nWeighted feedback arc set:\n\nA branch-and-bound algorithm to solve the linear ordering problem for weighted tournaments, Charon et al.\n\nExact and heuristic algorithms for the weighted feedback arc set problem: A special case of the skew‚Äêsymmetric quadratic assignment problem, Flood\n\nApproximating Minimum Feedback Sets and Multicuts in Directed Graphs, Even et al.\n\n\nRandom inputs:\n\nNoisy sorting without resampling, Braverman et al.\n\nStochastically transitive models for pairwise comparisons: Statistical and computational issues, Shah et al.\n\nOn estimation in tournaments and graphs under monotonicity constraints, Chatterjee et al. \n\n\nSurvey (slightly dated):\n\nAn updated survey on the linear ordering problem for weighted or unweighted tournaments, Charon et al.\n\n\nConvex relaxation of permutation matrices:\n\nOn convex relaxation of graph isomorphism, Afalo et al.\n\nFacets of the linear ordering polytope, Grotschel\n\n",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea on learning representations of sets",
            "review": "This paper proposed an interesting idea of learning representations of sets by permutation optimizations. Through learning a permutation of the elements of a set, the proposed algorithm can learn a permutation-invariant representation of that set. To deal with the underlying difficult combinatorial optimization problem, the authors proposed to relax the optimization constraints and instead optimize over the set of doubly-stochastic matrices with reparameterization using the Sinkhorn operator. The cost function of this optimization is related to a pairwise ordering cost, which compares the order for each pair of the elements.\n\nThe idea of using pairwise comparison information to learn permutations is interesting. The total cost function utilizes the comparison information and optimization over this cost function can lead to a permutation-invariant representation of the set. The idea of using the Sinkhorn operator to reparameterize the doubly-stochastic matrices makes the optimization objective differentiable. Also, the experiment results compared with some baseline algorithms showed the success of the proposed methods in many different tasks.\n\nMy major concern of the proposed method is on whether this method can be applied to large sets. Since the algorithm compares all pairs of elements in the set, we need O(N^2) comparisons for a set of size N and hence the proposed method might be slow if N is large. Is it possible to improve the efficiency for large sets?\n\nQuestions and Suggestions:\n\n1. Since the authors wants to approximately solve the objective function in Equation (2), it is better if we can see a proof showing why this optimization problem is difficult.\n\n2. For the experiment in Section 4.2, it seems that all methods (including the proposed methods and the baseline methods) are not performing well if the images are split to at least 4 * 4 equal-size tiles. I understand that currently the authors applied their method to the case of grid permutation by simply adding all cost functions of all rows and columns. Is it possible to extend the proposed method to the grid case in another way so that the results under this setting is better? \n\n3. It will be better if the authors can propose some more insights (probably with some theoretical analysis) when can the PO-U method performs better and when can the PO-LA method performs better.\n\n4. The authors mentioned that, the proposed method can get good permutations even for only T=4 steps. What if we continue running the algorithm? Will the permutation converges stably?\n\n5. The authors proposed to update the permutation matrix parameters in an alternative way (Equation (7)) and mentioned that this update works significantly better in the experiments. It will be great if the authors can have a theoretical analysis on why this is true since P and \\tilde P can be quite different from each other for an arbitrary \\tilde P matrix.\n\n\nMinor comment:\n\nI think there is a typo in Equation (5). The entry \\tilde P_{pq} is related to not only the entry P_{pq}, but also the other entries of the matrix P. Hence, I think Equation (5) should be modified as a matrix multiplication.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}