{
    "Decision": {
        "title": "meta-review",
        "metareview": "The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted. ",
        "recommendation": "Accept (Poster)",
        "confidence": "4: The area chair is confident but not absolutely certain"
    },
    "Reviews": [
        {
            "title": "Interesting Approach to Route Instruction Following with Thorough Evaluation",
            "review": "The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed (\"self-aware\") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a \"progress monitor\" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction.\n\n\nSTRENGTHS\n\n+ The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a \"progress monitor\" allows the method to reason over whether the navigational progress matches the instruction.\n\n+ The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture.\n\n+ The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results.\n\n\nWEAKNESSES\n\n- The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next.\n\n- The paper emphasizes the use of images, the visual grounding reasons over visual features.\n\n- The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal.\n\nC. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, “Learning to parse natural language commands to a robot control system,” in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012.\n\nS. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, “Learning models for following natural language directions in unknown environments,” in Proc. IEEE Int’l Conf. on Robotics and Automation (ICRA), 2015\n\nF. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, “Inferring maps and behaviors\nfrom natural language instructions,” in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014.\n\n- While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)⁠\n\nJ. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. \"Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots,\" In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017\n\n- The paper misses the large body of literature on grounded language acquisition for robotics.\n\nQUESTIONS\n\n* What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016?\n\n* Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good idea, unclear results",
            "review": "This submission introduces a new method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module. The method is shown to perform well on a standard benchmark. Ablation tests indicate the importance of each component of the model. Qualitative examples show that the proposed method attends to different parts of the instruction as the agent moves. \n\nHere are some comments/questions:\n- I like the underlying idea behind the method. The manuscript is written well for most parts.\n- The qualitative examples and Figure 2 are really helpful in understanding the reasons behind the improved performance.\n- There is a lot of confusion regarding the use of beam search. It's unclear from the current manuscript which results are with and without beam search. It seems like beam search was added from Ours 1 to Ours 2 in Table 2. It's not clear which rows involve beam search in Table 1. Some concerns about beam width were raised in the comments which I agree with. Please modify the submission to clearly indicate the use of beam search for each result and specify the beam width.\n- The use of beam search seems unrealistic to me as I can not think of any way a navigational model using beam search can be transferred or applied to real-world. I understand that one of the baselines uses beam search, so it's fair for performance comparison purposes, but could you provide any justification of how it might be useful in real-world? If there's no reasonable justification, could you also provide all the results (along with SPL metric) without beam search, including ablation, comparing with only methods without beam search? \n- I do not understand why the OSR in the submission is 0.64 and 0.70 for Speaker-Follower and proposed method and 0.96 and 0.97 in the comments.\n- It seems like the proposed method is tailored for the VLN task. In many real-world scenarios, an agent might be given an instruction which only describes the goal (such as in Chaplot et al. 2017 and Hermann et al. 2017) and not the path to the goal, could the authors provide their thoughts on whether the proposed would work well for such instructions? What would the progress monitor and textual attention distribution learn in such a scenario?\n\nDue to confusion about results and concerns about beam search, I give a rating of 5. I am willing to increase the rating if the authors address the above concerns.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper describes a model for vision-and-language navigation. The proposed\nmodel adds two components to the baseline model proposed by Fried et al. (2018):\n\n- a panoramic visual attention (referred to in this paper as \"visual--textual\n  co-grounding\"), in which the full scene around the agent's current position is\n  attended to prior to selecting a direction to follow\n\n- an auxiliary \"progress monitoring\" loss which encourages the agent to to\n  produce textual attentions from which the distance to the goal can be directly\n  inferred\n\nThe two components combine to give state-of-the-art results on the Room2Room\ndataset: small improvements over existing approaches on the \"-seen\" evaluation\nset and larger improvements on the \"-unseen\" evaluation sets. These improvements\nalso stack with the data-augmentation approach of Fried et al.\n\nI think this is a reasonable submission and should probably be accepted. However, I\nhave some concerns about presentation and a number of specific questions about\nmodel implementation and evaluation.\n\nPRESENTATION AND NAMING\n\nFirst off: I implore the authors to find some descriptor other than \"self-aware\"\nfor the proposed model. \"Self-aware\" is an imprecise description of the agent in\nthis paper---the agent is specifically \"aware\" of its visual surroundings and\nits distance from the goal, neither of which is meaningfully an aspect of\n\"self\". Moreover, self-awareness means something quite different in adjacent\nareas of cognitive science and philosophy; overloading the term in the specific\n(and comparatively mundane) way used here creates confusion. See section 3.4 of\nhttps://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something\nlike \"visual / temporal context-sensitivity\" to describe what's new here? A bit\nclunky, but I think it makes the contributions of this work much clearer.\n\nAs suggested in the summary above, I also think \"visual--textual co-attention\"\nis also an unhelpfully vague description of this aspect of the contribution. The\ntextual attention mechanism used in this paper is the same as in all previous\nwork on the task. Representations of language don't even interact with the\nvisual attention mechanism except by way of the hidden state, and the salient\nnew feature of the visual attention is the fact that it considers the full\npanoramic context before choosing a direction.\n\nMODELING QUESTIONS\n\n- p4: $y_t^{pm}$ is defined as the \"normalized distance from the current\n  viewpoint to the goal\". Is this distance in units of length (as defined by the\n  simulator) or units of time (i.e. the number of discrete \"steps\" needed to\n  reach the goal)?\n\n  The authors have already clarified on OpenReview that the progress monitor\n  objective uses an MSE loss rather than a likelihood loss. Do I understand\n  correctly that ground-truth distances are in [0, 1] but model predictions are\n  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search\n  candidates as $p_t^{pm} \\times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?\n\n- The input to the progress monitor is formed by concatenating the attention\n  vector $\\alpha_t$ to a vector of state features, and then multiplying by a\n  fixed weight matrix. How is this possible? The size of $\\alpha_t$ varies\n  depending on the length of the instruction sequence. Are attentions padded out\n  to the length of the longest instruction in the training set? If so, how can\n  the model learn when it's reached the end of a short instruction sequence?\n  What would happen if the agent encountered a sequence that was too long?\n\nEVALUATION QUESTIONS\n\n- The progress monitor is used both as an auxiliary training objective and as a\n  beam search heuristic. Is it possible to disentangle these two contributions?\n  (E.g. by ignoring the scores during beam search, or by doing augmented beam\n  search in a model that was trained without the auxiliary objective.)\n\n- Not critical, but it would be nice to know if the contributions here stack\n  with the pragmatic inference procedure in Fried et al.\n\n- While, as pointed out on OpenReview, it is not required to include SPL\n  evaluations, I think it would be informative to do so---the preliminary\n  results with no beam search look good!\n\nMISCELLANEOUS\n\np1: \"without a map\" If you can do beam search, you effectively have a map.\n\np1: \"...smoothly\" What does \"smoothly\" mean in this context?\n\np2: \"the position of grounded instruction can follow past and future\n    instructions\". Is the claim here that if instructions are of the form \"ACB\"\n    and the agent is supposed to do \"ABC\", that the proposed model will execute\n    these instructions successfully and the baseline will not? This claim does\n    not appear to be evaluated anywhere in the body of the paper.\n\np4: \"intelligently prunes\" \"Intelligently\" is unnecessary.\n\np4: \"for empirical reasons\" What does this mean?\n\np5: \"Intuitively, an instruction-following agent is required...\" The existence\n    of non-attentive models that do reasonably well at these\n    instruction-following tasks suggest that this is not actually a requirement.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}