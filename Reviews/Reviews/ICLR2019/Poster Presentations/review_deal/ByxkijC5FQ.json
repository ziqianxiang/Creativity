{
    "Decision": {
        "metareview": "The paper presents a topological complexity measure of neural networks based on persistence 0-homology of the weights in each layer. Some lower and upper bounds of the p-norm persistence diagram are derived that leads to normalized persistence metric. The main discovery of such a topological complexity measure is that it leads to a stability-based early stopping criterion without a statistical cross-validation, as well as distinct characterizations on random initialization, batch normalization and drop out. Experiments are conducted with simple networks and MNIST, Fashion-MNIST, CIFAR10, IMDB datasets. \n\nThe main concerns from the reviewers are that experimental studies are still preliminary and the understanding on the observed interesting phenomenon is premature. The authors make comprehensive responses to the raised questions with new experiments and some reviewers raise the rating. \n\nThe reviewers all agree that the paper presents a novel study on neural network from an algebraic topology perspective with interesting results that has not been seen before. The paper is thus suggested to be borderline lean accept. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A topological complexity measure of neural networks based on persistent 0-homology of weights, with a new early stopping criterion."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper proposes the notion of \"neural persistence\", i.e., a topological measure to assign scores to fully-connected layers in a neural network. Essentially, a simplicial complex is constructed by considering neurons as 0-simplices and connections as 1-simplices. Using the (normalized) connection weights then facilitates to define a filtration. Persistent homology (for 0-dim. homology groups) then provides a concise summary of the evolution of the 0-dim. features over the filtration in the form of a barcode. The p-norm of the persistence diagram (containing points (1,w_i)) is then used to define the \"neural persistence\"  NP(G_k) of a layer G_k; this measure is averaged over all layers to obtain one final neural persistence score. Thm. 1 establishes lower and upper bounds on N(G_k); Experiments show that neural persistence, measured for small networks on MNIST, aligns well with previous observations that batch-norm and dropout are benefical for generalization and training. Further, neural persistence it can be used as an early stopping criterion without having to rely on validation data.\n\nOverall, I think this is an interesting and well-written paper with a good overview of related work in terms of using TDA approaches in machine learning. The theoretical aspects of the work (i.e., the bounds) are fairly obvious. The bounds \nare required, though, for proper normalization. Using 0-dim. persistent homology is also appropriate in this context, as I tend to agree with the authors that this aspect is the most interesting one (and also the only computationally feasible one if this needs to be done during training). \n\nThe only major concern at this point, is the experimental evaluation on small fully-connected networks. \nWhile reading the paper, I was wondering how this could be generalized, e.g., to convolution layers, as the strategy seems to be also applicable in this context as well. I do think that the results on MNIST are convincing, however, already on CIFAR-10 the early stopping criterion seems to be very sensitive to the choice of g (from what I understood). So, this raises the obvious question of how this behaves for larger networks with more layers and larger datasets. If the contribution boils down to a confirmation that dropout and batch-norm are beneficial, this would substantially weaken the paper. Specifically, I would be interested in having full-connected networks with more layers (possibly less neurons per layer). Maybe the authors can comment on that or perform experiments along this direction.\n\nMinor comments:\n\n- What is the subscript d in \\mathcal{D}_d intended to denote?\n- In Thm.1 - why should \\phi_k be unique? This is not the only choice?\n- End of Sec. 4. - \"it is beneficial to free validation data ...\" - What does that mean?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting idea, but insufficient.",
            "review": "This paper proposes to analyze the complexity of a neural network using its zero-th persistent homology. Each layer is considered a bipartite graph with edge weights. As edges are being added in a monotonically decreasing order, each time a connected component is merged with others will be recorded as a new topological feature. The persistence of each topological feature is measured as the weight difference between the new edge and the maximal weight (properly normalized). Experiments show that by monitoring the p-norm of these persistence values one can stop the training a few epochs earlier than the validation-error-based early stopping strategy, with only slightly worse test accuracy.\n\nThe proposed idea is interesting and novel. However, it is needs a lot of improvement for the following reasons.\n\n1) The proposed idea can be explored much deeper. Taking a closer look, these zero-th persistence are really the weights of the maximum spanning tree (with some linear transformation). So the proposed complexity measure is really the p-norm of the MST. This raises other related questions: what if you just take all the weights of all edges? What if you take the optimal matching of the bipartite graph? How about the top K edges? I am worried that the p-norms of these edge sets might have the same effect; they converge as the training converges. These different measurements should be at least experimentally compared in order to show that the proposed idea is crucial. \n\nNote also that most theoretical proofs are straightforward based on the MST observation.\n\n2) The experiment is not quite convincing. For example, what if we stop the training as soon as the improvement of validation accuracy slows down (converges with a much looser threshold)? Wouldn’t this have the same effect (stop slightly earlier with only slightly worse testing accuracy)? Also shouldn’t the aforementioned various alternative norms be compared with?\n\n3) Some other ideas/experiments might worth exploring: taking the persistence over the whole network rather than layer-by-layer, what happens with networks with batch-normalization or dropout?\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting new measure of network complexity",
            "review": "The authors, motivated by work in topological graph analysis, introduce a new broadly applicable complexity measure they call  neural persistence--essentially a sum over norms of persistence diagrams (objects from the study of persistent homology).  The also provide experiments testing their parameter, primarily on MNIST with some work on CIFAR-10.\n\nI'd like to preface my criticism with the following: this work is extremely compelling, and the results and experiments are sound.  I'm very interested to see where this goes.  Figure 2 is particularly compelling!\n\nThat said, I am extremely suspicious of proposals for measures of generalization which (1) do not make contact with the data distribution being studied, and (2) which are only tested on MNIST and CIFAR-10.  Additionally, (3) it is not clear what a \"good\" neural persistence is, a priori, and (4) I'm not entirely sure I agree with the author's assessment of their numerical data.\n\nIn more detail below:\n\n1. At this point, there's a tremendous number of different suggested ways to measure \"generalization\" by applying different norms and bounds and measures from all of the far reaches of mathematics.  A new proposed measure **really needs** to demonstrate a clear competitive measure against other candidates.  The authors make a strong case that this measure is better than competitors from TGA, but I'm not yet convinced this measure is doing enough legwork.  For example, is it possible that a network has high neural persistence, but still has terrible test or validation error?  Why or why not?  Are there obvious counterexamples?  Are there reasons to think those obvious counterexamples aren't like trained neural networks?  These are all crucial questions to ask and answer if you want this sort of measure to be taken seriously.\n\n2.  Most of your numerical experiments were on MNIST, and MNIST is weird.  It's getting to be a joke now in the community that your idea works on MNIST, but breaks once you try to push it to something harder.  Even Cifar-10 has its quirks, and observations that are true of some networks absolutely do not generalize to others.\n\n3. While I'm convinced that neural persistence allows you to distinguish between networks trained in different ways, it isn't clear why I should expect a particular neural persistence to mean anything at all w.r.t. validation loss.  Are there situations in which the neural persistence has stopped changing, but validation loss is still changing appreciably?  Why or why not?\n\n4. I'm concerned that the early stopping procedure used as a benchmark wasn't tuned as carefully as neural persistence was.  I also honestly cannot determine anything from Figure 4 except that your \"Fixed\" baseline is bad, and that persistence seems to do about the same as validation loss.  It even seems that Training loss is a better early stopping criteria (better than both validation and persistence!) from this plot, because it seems to perform just as well, and systematically stop earlier.  Am I reading this plot right (particularly for 1.0 fraction MNIST)?\n\n\nThis work currently seems like a strong candidate for the workshop track.  I would have difficulty raising my score above much more than a 6 without much more numerical data, and analysis of when the measure fails.\n\nEdit: The authors have made a significant effort to address my concerns, and I'm updating my score to 7 from 5 in response.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}