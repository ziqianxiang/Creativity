{
    "Decision": {
        "metareview": "There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.\n\nThere were two main questions in my mind.\n1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.\n\nHere I believe evidence has been presented, but the case remains undecided.\n\n2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?\n\nThis question I would lean towards answering in the affirmative.\n\nI believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down-weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.\n\nUltimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "borderline paper"
    },
    "Reviews": [
        {
            "title": "Interesting but limited contribution",
            "review": "The thurst behind this paper is that graph convolutional networks (GCNs) are constrained by construction\nto focus on small neighborhoods around any given node. Large neighborhoods introduce in principle\na large number of parameters (while as the authors point out, weight sharing is an option to avoid this issue), \nplus even worse oversmoothing may occur. Specifically, Xu et al. (2018) showed that for a k-layer GCN one can \nthink of the influence score of a node x on node y as the probability  that a walker that starts at x, \nlands on y after k steps of random walk (modulo some details). \n\nTherefore, as k increases the random walks reaches its stationary distribution, forgetting any local information that is useful, \ne.g., for node classification. To avoid this problem, the authors propose the following: use personalized Pagerank\ninstead of the standard Markov chain of Pagerank. In PPR there is a restart probability, which allows \ntheir algorithm to avoid “forgetting” the local information around a walk, thus allowing for an arbitrary \nnumber of steps in their random walk. The authors define two methods PEP, and PEPa based on PPR. The latter \nmethod is faster in practice since it approximates the PPR.   \n\nA key advantage of the proposed method is the separation of the node embedding part from the propagation scheme. In this sense, \nfollowing the categorization of existing methods into three categories, PEP is a hybrid of message passing algorithms,\nand random walk based node embeddings. The experimental evaluation tests certain basic properties of the proposed method. One interesting performance feature of \nPEP and PEPa is that they can perform well using few training examples. This is valuable especially when obtaining labeled\nexamples is expensive.  Finally, the authors compare their proposed methods against state-of-the-art GCN-based methods.  \n\nSome remarks follow. \n\n- The idea of using PPR for node embeddings has been suggested in recent prior work “LASAGNE: Locality and structure aware graph node embeddings” \nBy Faerman et al.  While according to the authors’ categorization of the existing methods in the intro, LASAGNE \nfalls under the “random walk” family  of methods, the authors should compare against it. \n \n- Continuing the previous point,  even simpler baselines would be desirable. How inferior is for instance \nan approach on one-vs-all classification using the approximate personalized Pagerank node embedding and \nsupport vector machines?  \n \n- Also, the authors mention “since our datasets are somewhat similar…”. Please clarify with respect to \nwhich aspects? Also, please use datasets that are different. For instance, see the LASAGNE paper for \nmore datasets that have different number of classes.  \n\n- In the experiments the authors use two layers for fair comparison. Given that one of the advantages of the \nproposed method is the  ability to have more layers without suffering from the GCN shortcomings \nwith large neighborhood exploration, it would be interesting to see an experiment where the number of layers is a variable. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review on \"Personalized Embedding Propagation: Combining Neural Networks on Graphs with Personalized PageRank\"",
            "review": "This paper proposed a variant of graph neural network, which added additional pagerank-like propagations (with constant aggregation weights), in additional to the normal message-passing like propagation layers. Experiments on some benchmark transductive node classification tasks show some empirical gains.\n\nUsing more propagations with constant aggregation weights is an interesting idea to help propagate the information in a graph. However, this idea is not completely new. In the very first graph neural network [1], the propagation is done until convergence. If the operator in each layer is a contraction map, then according to the Banach Fixed Point theorem [2], a unique solution can be guaranteed. The constant operator used in this paper is thus a special case of this contraction map.\n\nAlso, the closed form solution in (3) is not practical. It may not be suitable for large graphs (e.g., graphs with >10k nodes). And that’s why this approach is not suitable for Pubmed and Microsoft dataset. The PEP_A is more practical. However, in this case I’m curious how it would compare with a GNN having same number of layers, but with proper gating/skip connections like ResNet. \n\nThe experiments show some marginal gains on the small graphs. However, I think it would be important to test on large graphs. Since small graphs typically have small diameter, thus several GNN layers would already cover the entire graph, and the additional propagation done by pagerank here might not be super helpful. \n\nFinally, I think the author should properly cite another relevant paper [3], which uses fixed point iteration to help propagate the local information. \n\n[1] Scarselli et.al, “The Graph Neural Network Model”, IEEE Transactions on Neural Networks, 2009\n[2] Mohamed A. Khamsi, An Introduction to Metric Spaces and Fixed Point Theory\n[3] Dai et.al, Learning Steady-States of Iterative Algorithms over Graphs, ICML 2018",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Idea is interesting; experiments are convincing",
            "review": "This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node.\n\nI find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range.\n\nQuestion: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}