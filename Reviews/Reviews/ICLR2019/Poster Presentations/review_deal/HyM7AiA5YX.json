{
    "Decision": {
        "metareview": "This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as \"maximizing the complement entropy.\" Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Novel training objective for deep learning with strong empirical results"
    },
    "Reviews": [
        {
            "title": "Nice idea but leaves several questions not answered",
            "review": "In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems.\n\nThis is an interesting point of view but the manuscript lacks discussion on several important questions:\n\n1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. \n2) Would this method also complement from overfitting?\n3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair  if the regularization option is turned on for the baseline methods.\n4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?\n5) How does alternating between two objectives change the training time? Do the authors use backpropagation?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and sensible heuristic with impressive improvement",
            "review": "This paper considers augmenting the cross-entropy objective with \"complement\" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. \n\nThe paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting.\n\nOne small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting new idea, good experimental results, some points to clarify.",
            "review": "\n========\nSummary\n========\n\nThe paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a \"complementary entropy\" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.\n\nThe procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.\n\nAdversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit\nthat the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.\n\n===========================\n Main comments and questions\n===========================\n\nEnd of page 1: \"the model behavior for classes other than the ground  truth stays unharnessed and not well-defined\". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?\n\nPage 3, sec 2.1: \"optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)\". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g.\n\nThis indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? \n\nFor example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:\nSuppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 \nAfter step 2:  0.1 0.3 0.3 0.3\nThen step 1: 0.5 0.3 0.1 0.1\nThen step 2: 0.1 0.3 0.3 0.3\nAnd so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?\n\nSec 3.1:\n\"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?\n\nSec 3.2:\nThe additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.\n\nSec 3.4:\nAs the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?\n\n===========================\nSecondary comments and typos\n===========================\n\nPage 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer.\n\nIn the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \\hat_y as an argument in this case?\n\nAlgorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}\n(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?\n\nSec 3:\n\"We perform extensive experiments to evaluate COT on the tasks\" --> COT on tasks\n\n\"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain.\" --> domainS\n\n\"to evaluate the model’s robustness trained by COT when attacked\" needs reformulation.\n\n\"we select a state- of-the-art model that has the open-source implementation\" --> an open-source implementation\n\nSec 3.2:\nFigure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?\n\nTable 3 and 4: why is it the validation error that is reported and not the test error?\n\nSec 3.3:\n\"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning.\n\n\"We apply the same pre-processing steps as shown in the model\" --> in the paper?\n\nSec 3.4:\n\"We believe that the models trained using COT are generalized better\" --> \"..using COT generalize better\"\n\n\"using both FGSM and I-FGSM method\" --> methodS\n\n\"The baseline models are the same as Section 3.2.\" --> as in Section 3.2.\n\n\"the number of iteration is set at 10.\" --> to 10\n\n\"using complement objective may help defend adversarial attacks.\" --> defend against\n\n\"Studying on COT and adversarial attacks..\" --> could be better formulated\n\nReferences: there are some inconsistencies (e.g.: initials versus first name)\n\n\nPros\n====\n- Paper is clear and well-written\n- It seems to me that it is a new original idea\n- Wide applicability\n- Extensive convincing experimental results\n\nCons\n====\n- No theoretical guarantee that the procedure should converge\n- The training time may be twice longer (to clarify)\n- The adversarial section, as it is,  does not seem relevant for me\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}