{
    "Decision": {
        "metareview": "The reviewers agree  that the paper is worthy of publication at ICLR, hence I recommend accept.\n\nRegarding section 4.3 of the submission and the claim that this paper presents the first insight for existing work from a divergence minimization perspective, as pointed out by R2, I went and checked the details of RAML and they have similar insights in their equations (5) and (8). Please make this clearer in the paper. Regarding evaluation using greedy search instead of beam search, please consider using beam search for reporting test performance as this is the standard setup in sequence prediction. Please take my comments and the reviews into account an prepare the final version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Accept"
    },
    "Reviews": [
        {
            "title": "This paper compares several well known methods and one new method based on Hellinger distance",
            "review": "This paper compares several well known methods and illustrates some connections among these methods, and proposes one new method based on Hellinger distance for seq2seq models' training, experimental results on machine translation and sentence summarization show that the method based on Hellinger distance gives the best results. \n\nHowever the originality and significance of this work are weak.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "some interesting results and connections, but also some technical issues; revisions have improved the paper",
            "review": "The authors have updated the paper and clarified some things, and now my impression of the paper has improved. It still feels a little incremental to me, but the potential application areas of these sorts of models are quite large and therefore incremental improvements are not insignificant. This paper suggests some natural follow-up work in exploring Hellinger distance and other variations for these models.\n\n----- original review follows: ------\n\nThis paper discusses loss functions for sequence generation tasks that take into account cost functions that reflect the task-specific evaluation metric. They compare RAML and risk (MRT) formally and empirically, and also test a loss based on Hellinger distance. They compare these to some standard max-margin losses. MRT and the Hellinger distance loss perform best in NMT and summarization experiments. \n\nPros:\n\nThere are some interesting aspects of this paper:\n\n- It is interesting to note that RAML and MRT are (similar to) different directions of KL divergences between the same two distributions. (Caveat: the entropy regularizer, which I discuss in \"Cons\" below.)\n- The new Hellinger-distance-based loss seems promising. \n- The empirical comparison among losses for standard NMT/summarization tasks is a potentially valuable contribution.\n\nCons:\n\nA.\nThe focus/story of the paper need some work. It is unclear what the key contributions are. I think the Hellinger distance loss is potentially the most important contribution, but the authors don't spend much time on that.. it seems that they think the comparison of divergence and max-margin losses is more central. However, I think the authors' conclusion (that the divergence losses are better than the max-margin losses) is not the main story, because RAML is not much better than the max-margin losses. Also, I have some concerns about some of the details of the max-margin losses (listed and discussed below), so I'm not sure how reliable the empirical comparison is. \n\nB.\nAs for the connections and comparison between RAML and MRT: \n\nIt does not seem that MRT corresponds to a divergence of the form given at the start of Sec. 4. There is also an entropy regularizer in Eq. (9). Sec. 4.3 states: \"By comparing the above two methods, we find that both RAML and MRT are minimizing the KL divergence between the model output distribution and the exponentiated payoff distribution, but with different directions of D_KL.\" However, this statement ignores the entropy regularizer in Eq. (9). \n\nMaybe I'm being dense, but I didn't understand where Equation (10) comes from. I understand the equations above it for RAML, but I don't understand the MRT case in Eq. (10). Can you provide more details?\n\nI also don't understand the following sentence: \"It turns out that the hyperparameter \\tau in RAML and \\alpha in MRT have the same effect.\" What does this mean mathematically? Also, does this equivalence also require ignoring the entropy regularizer? As formulated, L_{RAML} necessarily contains a \\tau, but L_{MRT} does not necessarily contain an alpha. It is only when moving to the sample approximation does the alpha become introduced. (MRT does not require this sample approximation; Some older work on MRT developed dynamic programming algorithms to exactly compute the gradients for structured output spaces like sequences, so samples were not used in those cases.) So I think the paper needs to clarify what exactly is meant by the connection between tau and alpha, under what conditions there is a connection between the two, and what exactly is the nature of this connection. If more space is needed for this, many of the details in Sec 3 can be cut or moved to appendices because those are standard and not needed for what follows. \n\nIn the experimental results (Sec. 7), MRT outperforms RAML consistently. The authors discuss the impact of the directionality of the KL divergence, but what about the entropy regularizer? It would be interesting to compare MRT both with and without the entropy regularizer. Without the regularizer, MRT would actually correspond to the KL that the authors are describing in the discussion. As it currently stands, two things are changing between MRT and RAML and we don't know which is responsible for the sizable performance gains. \n\nC. \nThere are several technical issues with the writing (and potentially with the claims/conclusions), most of which are potentially flexible with some corrections and more exposition by the authors:\n\nIs L_{RAML} to be maximized or minimized? Looks like maximized, but clearly both L_{MLE} and L_{MRT} are supposed to be minimized, so the use of L for all of these seems confusing. If different conventions are to be used for each one, it should be explicitly mentioned in each case whether the term is to be maximized or minimized.\n\nAt the end of Sec. 4.1, q' is not defined. I can guess what it is, but it is not entirely clear from the context and should be defined. \n\nIn Equation 6, please use different notation for the y in the denominator (e.g., y') to avoid collision with the y in the numerator and that on the left-hand side. \n\nThe discussion of max-margin losses in Sec. 5 has some things that should be fixed. \n\n1. In Sec. 5, it is unclear why \\Delta is defined to be the difference of two r() functions. Why not just make it -r(y, y^*)? Are there some implied conditions on \\Delta that are not stated explicitly? If \\Delta is assumed to be nonnegative, that should be stated. \n\n2. In Eq. (11), F appears to be a function of y and theta, but in the definition of F, it has no functional arguments. But then further down, F appears to be a function of y only. Please make these consistent.\n\n3. Eq. (11) is not only hard for structured settings; it is also hard in the simplest settings (binary classification with 0-1 loss). This is the motivation for surrogate loss functions in empirical risk minimization for classification settings. The discussion in the paper makes it sound as if these challenges only arise in the structured prediction setting. \n\n4. I'm confused by what the paper is attempting to communicate with Equations 12 and 13. In Eq. 12, y on the left-hand side is not bound to anything, so it is unclear what is being stated exactly. Is it for all y? For any y? In Eq. 13, the \\Delta on the right-hand side is outside the max over y -- is that really what was intended? I thought the max (the slack-rescaled loss-augmented inference step) should take into account \\Delta. Otherwise, it is just doing an argmax over the score function. \n\n5. If the authors are directly optimizing the right-hand side of the inequality in Equation 12 (as would be suggested for the formula for the gradient), then there is no global minimum of the loss. It would go to negative infinity. Typically people use a \"max(0, )\" outside the loss so that the global minimum is 0. \n\n\n\nTypos and minor issues follow:\n\nSec. 1:\n\"the SEARN\" --> \"SEARN\"\n\"the DAGGER\" --> \"DAGGER\"\n\nSec. 2:\n\"genrally\" --> \"generally\"\n\"took evaluation metric into training\" --> \"incorporated the evaluation metric into training\"\n\"consistant\" --> \"consistent\"\n\nSec. 3:\nUse \\exp instead of exp.\n\"a detail explanation\" --> \"a detailed explanation\"\n\nSec. 4.1:\n\"predition\" --> \"prediction\"\n\nSec. 4.2:\n\"only single sample\" --> \"only a single sample\"\n\nSec. 6.1:\n\"less significant than\" --> \"less significantly than\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well written and interesting; experiments could be improved",
            "review": "In this paper the authors distinguish between two families of training objectives for seq2seq models, namely, divergence minimization objectives and max-margin objectives. They primarily focus on the divergence minimization family, and show that the MRT and RAML objectives can be related to minimizing the KL divergence between the model's distribution over outputs and the \"exponentiated payoff distribution,\" with the two objectives differing in terms of the direction of the KL. In addition, the authors propose an objective using the Hellinger distance rather than the KL divergence, and they conduct experiments on machine translation and summarization comparing all the considered objectives.\n\nThe paper is written extremely clearly, and is a pleasure to read. While the discussion of the relationship between RAML and MRT (and MRT and REINFORCE) is interesting and illuminating, many of these insights appear to have been discussed in earlier papers, and the RAML paper itself notes that it differs from REINFORCE style training in terms of the KL direction.\n\nOn the other hand, the idea of minimizing Hellinger distance is I believe novel (though related to the alpha-divergence work cited by the authors in the related work section), and it's nice that training with this loss improves over the other losses. Since the authors' results, however, appear to be somewhat below the state of the art, I think the main question left open by the experimental section is whether training with the Hellinger loss would further improve state of the art models. Even if it would not, it would still be interesting to understand why, and so I think the paper could be strengthened either by outperforming state of the art results or, perhaps through an ablation analysis, showing what aspects of current state of the art models make minimizing the Hellinger loss unnecessary.\n\nIn summary,\n\nPros:\n- well written and interesting\n- a new loss with potential for improvement over other losses\n- fairly thorough experiments\n\nCons:\n- much of the analysis is not new\n- unclear if the proposed loss will improve the state of the art, and if not why \n\nUpdate after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}