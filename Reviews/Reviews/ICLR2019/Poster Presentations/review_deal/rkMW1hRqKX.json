{
    "Decision": {
        "metareview": "This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state-of-the-art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Exciting approach to training sequence-to-sequence models from scratch"
    },
    "Reviews": [
        {
            "title": "Interesting learning algorithms for autoregressive models without MLE pretraining",
            "review": "Quality and Clarity:\nThe writing is good and easy to read, and the idea is clearly demonstrated.\n\nOriginality:\nThe idea of never training over the ground-truth sequence, but training on sampled prefix and an optimized suffix is very novel. The similar idea is also related to imitation learning in other domains such as self-driving car where an oracle can give optimal instruction when exploring a new state. \n\nComments:\nThe paper proposed a very interesting training algorithm for auto-regressive models especially it does not require any MLE pre-training and can directly optimize from the sampling.\n\nHere are some questions:\n(1) The idea should also apply on many “incremental rewards”, for instance, BLEU scores in machine translation, etc. Do you have any comparison? What if the best suffix cannot be found using dynamic programming (when the evaluation metric is not edit-distance, but a sentence-level reward)?\n(2) Can the proposed algorithm be applied in other “bigger” tasks such as neural machine translation?\n(3) Eq. (6) is not very clear. Do you first sample the whole sequence, and then supervise one token for each step? Or the whole suffix?\n(4) Do you have a comparison with the learning efficiency between MLE and OCD? Will it get unstable in the beginning of training as all the samples are wrong.\n\n----------------------------\nMissing Reference:\nDing, Nan, and Radu Soricut. \"Cold-Start Reinforcement Learning with Softmax Policy Gradient.\" Advances in Neural Information Processing Systems. 2017.\n\nThis paper used a very similar idea as the proposed learning method which relies on incremental rewards to find the “optimal” suffix (for instance, edit-distance is a special example). It would be better to have some discussion,\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Exciting approach",
            "review": "The authors propose an alternative approach to training seq2seq models, which addresses concerns about exposure bias and about the typical MLE objective being different from the final evaluation metric. In particular, the authors propose to use a dynamic program to compute the optimal continuations of predicted prefixes (during training) in terms of edit distance to the true output, and then use a per-token cross entropy loss, with a target distribution that is uniform over all optimal next-tokens. The authors conduct a number of experiments, and show that this scheme allows them to attain state-of-the-art performance on end-to-end speech recognition, and that they can moreover do this without needing to pretrain the model with the MLE objective.\n\nThis is a very nice paper; it is generally well written, it gets excellent results, and it contains a comprehensive set of experiments and analysis. It is also quite exciting to see an approach to alleviating exposure bias that does not require pretraining with MLE. Accordingly, my suggestions mainly relate to the presentation and related work:\n\n - It seems a bit strange to argue that the proposed approach doesn't increase the time complexity over MLE. While technically true (as the authors note) if the vocabulary is bigger than the sequence lengths, the difference in (on policy) training time will presumably be felt when dealing with very long sequences, or with cases where the number of labels per time-step is small, like in character-level generation or in seq2seq style sequence labeling.\n\n - I think it's difficult to argue that the proposed approach isn't essentially a modification of imitation learning/learning-to-search algorithms like, say, AggreVaTe or LOLS (Chang et al., ICML 2015). As far as I can tell, the only differences are that cross entropy is used rather than a cost-sensitive classifier, and, perhaps, that the training is done in minibatches (with no aggregation).\n \n - Relatedly, while it is interesting that the loss uses all the optimal completion tokens, it should be noted that there is much work in transition-based parsing that adopts a learning-to-search approach and uses losses that incorporate multiple optimal next-predictions as given by a \"dynamic oracle\"; see Goldberg and Nivre (COLING, 2012) and others.  \n\n - I think it's also worth noting that training approaches like MIXER and others can target arbitrary rewards (and not just those where we can efficiently compute optimal next-steps), and so the proposed approach is a compelling competitor to MIXER-like approaches on problems such as machine translation or image captioning only to the extent that training with edit-distance is useful for such problems. Do you have a sense of whether training with edit-distance does indeed improve performance on such tasks?\n\nPros:\n- well written and interesting\n- good experiments, results, and analysis\n\nCons:\n - perhaps slightly more similar to previous work than is argued\n\n\nUpdate after author response: thanks for your response; I think the revised paper largely addresses my comments and those of the other reviewers, and I continue to hope it is accepted. Here are two small notes on the related work section of the revised paper:\n- In distinguishing OCD from DAgger, you note that the optimal policy is computed rather than provided at training time. In fact, structured prediction applications of SEARN (Daume III et al., 2009, which should also be cited) and DAgger often have this flavor too, such as when using them for sequence labeling (where optimal continuations are calculated based on Hamming distance).\n- Please include a reference to Goldberg and Nivre's (2012) dynamic oracle work.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice twist to seq2seq models",
            "review": "The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation. In particular, a model trained in this way can be biased in the sense that training sequences typically have different sets of prefixes compared to test sequences. As a result, at the prediction time the model does not generalize well and for a given input sequence the decoder constructs a label sequence which reflects the training label sequences rather than the actual target label.\n\nTo address this shortcoming, the authors propose an approach based on edit distances and the implicit use of given label sequences during training. The main idea is to generate a label sequence with respect to the current parameter vector of a conditional probabilistic model (see Eqs. 2 & 3, as well as the objective in Eq. 6) and then based on the edit distance find the best possible completions for any prefix of that model-based label sequence. The training objective is then defined in Eq. (6): to each element in the output sequence the objective assigns the KL-divergence between a conditional distribution of the next element in the label sequence given a label prefix generated using the current model and the exponential family model based on edit distances given by the prefixes and optimal completions after the position of interest in the label sequence. The objective and the corresponding gradient can be computed efficiently using dynamic programming.\n\nIntuitively, the approach tries to find a parameter vector such that the decoder at a particular instance is likely to construct a label sequence with a small edit distance to the target label. As the training objective now considers all possible elements of the vocabulary given a prefix sequence, it is reasonable to expect that it performs better than MLE which only considers target vocabulary elements given target prefix sequences (e.g., compare Eqs. 3 & 6).\n\nThe experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art. I am not an expert in the field and cannot judge the related work objectively but can say that the context for their approach is set appropriately. I would, however, prefer more clarity in the presentation of the approach. This especially applies to the presentation of the approach around Eq. (6). It might not be straightforward for a reader to figure out how the tilde-sequences are obtained. As the objective is non-convex, in order to be able to reproduce the results it would be useful to provide some heuristics for choosing the initial solutions for the parameter vector. In Section 3, please also provide a reference to the appendix so that a reader can understand the conditional probabilistic model.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}