{
    "Decision": {
        "metareview": "All reviewers appreciate the empirical analysis and insights provided in the paper. The paper also reports impressive results on SSL. It will be a good addition to the ICLR program. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting analysis and insights into SWA for semisupervised learning"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper proposes to apply Stochastic Weight Averaging to the semi-supervised learning context. It makes an interesting argument that the semi-supervised MT/Pi models are especially amenable to SWA since they are empirically observed to traverse a large flat region of the weight space during the later stages of training. To speed up training, the authors propose fast-SWA.\n\nSecition 3.2 is a little confusing. \n- If a random direction is, with high probability, not penalized, then why is it so flat along a random direction? Or is this simply an argument for why it is not guaranteed to be penalized, and therefore adversarial rays exist? I think the claim needs to be more precise (though it remains unclear how accurate the claim would be).\n- I also think that there is maybe something special about measuring the SGD-SGD ray at epochs 170/180. It coincides with the regime of training where the signal is dominated by the consistency loss. Is it possible this somehow induces a near-linear path in the parameter space? I would be interested in seeing projections of other epoch’s SGD-SGD (e.g. 170/17x) vectors onto the 170/180 SGD-SGD ray and the extend to which they are co-linear. \n- It is also striking that traversing the SGD-SGD ray causes an error rate so similar to the adversarial ray for the supervised model; can the authors explain this phenomenon? \n- All this being said, I find the diversity argument compelling---though what would happen if we train the model even longer? Does it keep exploring?\n- Overall, I am not sure how comfortable we should be with interpreting the SGD-SGD ray results. It is important that the authors provide a convincing argument for the interpretability of the SGD-SGD ray results, as this appears to be the key to the “large flat region” claim.\n\nI think Mandt’s paper should be cited in-text, since this is what motivates Figure 2d.\n\nIs the benefit of Fast-SWA’s fast convergence (to a competitive/better solution than SWA) unique to semi-supervised learning? Or can it be demonstrated by fully-supervised learning too? Given the focus on the semi-supervised regime, I would prefer if what the authors are proposing is, in some sense, special to the semi-supervised regime.\n\nTable 1 is confusing to read. I just want to see a comparison between with and without using fast-SWA, *with all else kept equal*. Is the intention to compare “Previous Best CNN” and “Ours CNN”? Is this a fair comparison?\n\nPros:\n+ Interesting story\n+ Good empirical performance\nCons:\n- Unclear whether the story is entirely correct\n\nIf the authors can provide a convincing case for the interpretability of the SGD-SGD results, I am happy to raise my score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very thorough analysis but limited novel contribution",
            "review": "OVERVIEW:\nThe paper looks at the problem of self-supervised learning using consistency-enforcing approaches. Their main contributions are two-fold:\n1. Analysis to understand current state-of-the-art methods for self-supervised learning, namely the Mean Teacher model (MT) by Tarvainen and Valpola (2017) and the \\Pi model (Laine and Aila, 2017). They show a theoretical analysis (Sec.3.1) of a simplified version of the \\Pi model and show that it reaches flatter minima leading to good generalization. They show an analysis of the SDG trajectories (Sec. 3.2) that shows how these self-supervised models achieve flatter and lower minima compared to a fully supervised approach. They also provide an intuitive explanation to explore more solutions along the SGD trajectory. Finally, in Sec.3.3, they also discuss how ensembling and weight averaging help get better solutions.\n2. Fast-SWA, which is a tweak to the SWA procedure (Izmailov et al, 2018) that averages models in the weight space along the SGD trajectory with a cyclical learning rate.\nThey show good performance on CIFAR-10 and CIFAR-100 with their proposed Fast-SWA.\n\nPROS:\n1. The paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution. The problem and experiments are very organized and explained very well.\n2. Exhaustive experiments, plots and tables showing very good performance on the standardized benchmark.\n\nCONS:\n1. The novel contribution (as I see it) is in the theoretical analysis of Sec. 3.1 & A.5 and the Fast-SWA procedure. The Fast-SWA is a minor tweak to the regular SWA. The theoretical analysis is the main novelty and it is hidden away in the appendix ! Also, the results seems to be derived on the basis of Avron and Toledo and the authors' contribution relative to that is not clear. Also, what is the difference between the regular \\Pi model and simplified \\Pi model and how big a difference does this make in your theory ?\n2. Can the Fast SWA be used directly say while supervised training of ImageNet ? Or is it applicable only to self-supervised problems ? Comments on the generalizability of this contribution might help increase novelty.\n\nOVERALL:\nI like the thorough analysis and good results of the paper. The novelty being a little weak results in the final rating of 7.5 (rounded up to 8, subject to change depending on other reviewers).",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice read + questions",
            "review": "The paper is nice thread, easy to follow.\n\nThe paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. \n\nOverall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution.\n\nI have following questions to the authors:\n1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100\n2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great.\n3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen?\n4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training?\n5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)?\n6. In section 5.2, page 8 , can authors provide their intuition behind the results: \"We found that the improvement on VAT is not drastic – our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%\" - why did fast-SWA not improve much?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}