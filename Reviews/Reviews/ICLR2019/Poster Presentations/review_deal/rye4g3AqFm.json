{
    "Decision": {
        "metareview": "Dear authors,\n\nThere was some disagreement among reviewers on the significance of your results, in particular because of the limited experimental section.\n\nDespite this issues, which is not minor, your work adds yet another piece of the generalization puzzle. However, I would encourage the authors to make sure they do not oversell their results, either in the title or in their text, for the final version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An interesting addition to the deep learning theory literature"
    },
    "Reviews": [
        {
            "title": "A fresh study to the generalization capabilities of (deep) neural networks, with the help of the PAC-Bayesian learning theory and empirically backed intuitions.",
            "review": "The paper brings a fresh study to the generalization capabilities of (deep) neural networks, with the help of an original use of PAC-Bayesian learning theory and some empirically backed intuitions.\n\nExpressing the prior over the input-output function space generated by the neural network is very interesting. This provides an original analysis compared to the common PAC-Bayesian analysis of neural networks that express the prior over network parameters space. The theoretical study here appears simple (noteworthy, it is based one of the very first PAC-Bayesian theorems of McAllester that is not the most used nowadays), and the study is conducted mainly by empirical observation. Nevertheless, the experiments leading to these observations are cleverly designed, and I think it gives great insights and might open the way to other interesting studies.\n\nOverall, the paper is enjoyable to read. I also appreciate the completeness of the supplementary material. I recommend the paper acceptance, but I would like the authors to consider the concerns I rise below:\n- The paper title is a bit presumptuous. The paper presents a conjunction backed by empirical evidence on some not-so-deep neural networks. Even if I consider it as an important piece of work, it does not provide any definitive answer to the generalization puzzle. \n- Many peer-reviewed publications are cited as arXiv preprints. Please carefully complete the bibliography. Some papers are referenced by the name, title and year only (Smith and Le 2018; Zhang et al, 2017)\n- I recommend adding to the learning curves of Figures 2 and 3 the loss on the training set. \n\nOther minor comments and typos:\n- Intro: Please define \"parameter-function\" map \n- Page 4: Missing parentheses around Mand et al. (2017)\n- SGD has not had time ==> SGD did not have time\n- Please refers to the definition in the supplementary material/information the first time you mention Lempel-Ziv complexity.\n- Please mention that SI stands for Supplementary Information\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting perspective but most relevant experiments are on very tiny networks",
            "review": "This paper propose an interesting perspective to explain the generalization behaviors of large over-parameterized neural networks by saying that the parameter-function map in neural networks are biased towards \"simple\" functions, and through a PAC-Bayes argument, the generalization behavior will be good if the target concept is also \"simple\". I like the perspective of view that combines the \"complexity\" of both the algorithm bias and the target concept in the view of generalization. However, the implementation and presentation of the paper could be improved.\n\nFirst of all, the paper is a bit difficult to follow as some important information is either missing or only available in the appendix. For example, in Section 2, to measure the properties of the parameter-function mapping, a simple boolean neural network is explored. However, it is not clear how the sampling procedure is carried out. There is also a 'training set of 64 examples', and it not obvious to the reader how this training set is used in this sample of neural network parameters.\n\nFollowing that, the paper uses Gaussian Process and Expectation-Propagation to approximately compute P(U). But the description is brief and vague (to non-expert in GP or EP). As one of the main contribution stated in the introduction, it would be better if more details are included.\n\nMoreover, the generalization bound is derived with the assumption that the learning algorithm uniformly sample from the set of all hypothesis that is consistent with a given training set. It is unlikely that this is what SGD is doing. But explicit experiments to verify how close is the real-world behavior to the hypothetical behavior would be helpful.\n\nThe experiment in section 6 that verify the 'complexity' of 'high-probability' functions in the given prior is very interesting. It would be good if some kind of measurements more directly on the real world tasks could be done, which will better support the argument made in the paper.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "not surprising",
            "review": "\nThe authors make a case that deep networks are biased\ntoward fitting data with simple functions.\n\nThe start by examining the priors on classifiers obtained by sampling\nthe weights of a neural network according to different distributions.  They do this\nin two ways.  First, they examine properties of the distribution\non binary-valued functions on seven boolean inputs obtained by\nsampling the weights of a small neural network.  They also empirically compare\nthe labelings obtained by sampling the weights of a network with\nlabelings obtained from a Gaussian process model arising from earlier\nwork.\n\nNext, they analyze the complexity of the functions produced, using\ndifferent measures of the complexity of boolean functions.  A\nfavorite of theirs is something that they call Lempel-Ziv complexity,\nwhich is measured by choosing an arbitrarily ordering of the\ndomain, writing the outputs of the function in that ordering,\nand looking at how well the Lempel-Ziv algorithm compresses this\nsequence.  I am not convinced that this is the most meaningful\nand fundamental measure of the complexity of functions.\n(In the supplementary material, they examine some others.\nThey show plots relating the different measures in the body\nof the paper.  None of the measures is specified in detail in the\nbody of the paper. They provide plots relating these complexity\nmeasures, but they don't demonstrate a very close connection.)\n\nThe authors then evaluate the generalization bound obtained by\napplying a PAC Bayes bound, together with the assumption that\nthe training process produces weights sampled from the distribution\nobtained by conditioning weights chosen according to the random\ninitialization on the event that they fit they fit the training\ndata perfectly.  They do this for small networks and simple datasets.\nThey bounds are loose, but not vacuous, and follow the same order\nof difficulty on a handful of datasets as the true generalization\nerror.\n\nIn all of their experiments, they stop training when the training\naccuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf\nhave found that continuing training past this point further improves test\naccuracy.  The experiments all use architectures that are\nquite dissimilar to what is commonly used in practice, and\nachieve much worse accuracy, so that a reader is concerned\nthat the results differ qualitatively in other respects.\n\nI do not find it surprising that randomly sampling parameters\nof deep networks leads to simple functions.\n\nPapers like the Soudry, et al paper cited in this submission are\ninconsistent with the assumption in the paper that SGD samples\nparameters uniformly.\n\nIt is not clear to me how many hidden layers were used for the\nresults in Table 1 (is it four?).  \n\nI did find it interesting to see exactly how concentrated the\ndistribution of functions obtained in their 7-input experiment\nwas, and also found results on the agreement of the Gaussian process\nmodels with the randomly sampled weight interesting, as far as they\nwent.  Overall, I am not sure that this paper provided enough\nfundamental new insight to be published in ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}