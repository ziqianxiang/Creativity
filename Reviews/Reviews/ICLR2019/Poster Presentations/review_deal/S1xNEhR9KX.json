{
    "Decision": {
        "metareview": "This paper studies an interesting phenomenon related to adversarial training -- that adversarial robustness is quite sensitive to semantically lossless shifts in input data distribution. \n\nStrengths\n- Characterizes a previously unobserved phenomenon in adversarial training, which is quite relevant to ongoing research in the area.\n- Interesting and novel theoretical analysis that motivates the relationship between adversarial robustness and the shape of input distribution.\n\nWeaknesses\n- Reviewers pointed out some shortcomings in experiments, and analysis of causes and remedies to adversarial robustness. The authors agree that given the current state of understanding, these are hard questions to pose good answers for. The result and observations by themselves are interesting and useful for the community.\n\nThe weakness that the paper does not propose a solution for the observed phenomenon remains, but all reviewers agree that the observation in itself is interesting. Therefore, I recommend that the paper be accepted.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting observation and results"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "A nice paper that clarifies the difference between the clean accuracy (accuracy of models on non-perturbed examples) and the robust accuracy (accuracy of models on adversarially perturbed examples) and it shows that changing the marginal distribution of the input data P(x) while preserving its semantic P(y|x) fixed affects the robustness of the model. Therefore, testing the robustness of the model should be performed in a careful manner. Comprehensive experiments were performed to show that changing the distribution of the MINST (smoothing) and CIFAR (saturation) data could lead to a significant difference in robust accuracy while the clean accuracy is almost steady. In addition, a set of experiments were performed in an attempt to search for the criteria required for choosing a proper dataset for testing adversarial attack to measure the robustness. \n\nAlthough Iâ€™m not expert in the field of adversarial attack but the paper is very nice to read and easy to follow (I have not checked the proof of the theorems though). \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "investigation as to the origin of lack of robustness of classifiers to perturbations of the input data",
            "review": "The paper is interesting and topical: robustness to adversarial input presentation (or shifts in training data itself, even those of the nature described by the authors 'semantic-lossless' shifts). Adversarial inputs are investigated under l-inf bounded perturbations, while multiclass classification on images is the target problem considered. The theoretical parts of the paper, assigning lack of adversarial robustness to the shape of the input distribution (Section 2) is the strongest part of the paper, adding some simple and important insights. Unfortunately, the empirical part of the paper is weakened by an over-reliance of (custom perturbations of ) the popular MNIST and CIFAR10 datasets (which are themselves based on larger sets). Furthermore, the basic conclusion as to causes and remedies of lack of robustness is not evident, and it is not evident that it has been sufficiently investigated. Shape yes, differences in perturbable volume not (how does that concur with Section 2?), and inter-class distance also not. Are we to base these conclusions on 2 perturbed datasets? How are readers to synthesize the final conclusion that robustness is a 'complex interaction of tasks and data', other than what they would already expect? In short, a valiant effort, and a good direction, but one that needs more work.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A super interesting paper discussing the impact of data distribution on adversarial robustness of trained neural networks",
            "review": "This paper provides several theoretical and practical insights on the impact of data distribution to adversarial robustness of trained networks. The paper reads well and provides analysis on two datasets MNIST and CIFAR10. I particularly like the result demonstrating that a lossless transformation on the data distribution could significantly impact the robustness of an adversarial trained models. The idea of using smoothness and saturation to bridge the gap between the MNIST and CIFAR10 datasets was also very interesting. One thing that is not clear from the paper is how one could use the findings from this paper and put it into practice. In other words, it would help if the authors could provide some insights on how to improve a model robustness w.r.t the changes in the data distribution. The authors did an attempt toward this in section 5, but that seems to only cover three factors that do not cause the difference in robustness.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}