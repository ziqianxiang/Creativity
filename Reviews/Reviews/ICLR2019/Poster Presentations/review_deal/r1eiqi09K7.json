{
    "Decision": {
        "metareview": "Dear authors,\n\nAll reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at ICLR.\n\nPlease make sure to implement all their comments in the final version.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An interesting analysis and algorithm"
    },
    "Reviews": [
        {
            "title": "Riemannian Adam/Amsgrad on product manifolds with convergence guarantee but not supported well by experiments.",
            "review": "The paper extends Euclidean optimization methods, Adam/Amsgrad, to the Riemannian setting, and provides theoretical convergence analysis which includes the Euclidean versions as a special case. To avoid breaking the sparsity, coordinate-wise updates are performed on product manifolds. \n\nThe empirical performance seems not very good, compared to RSGD which is easier to use.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Riemannian ADAM",
            "review": "I have enjoyed reading this paper. The paper is accessible in most cases and provides a novel optimization technique. Having said this, I have a few concerns here,\n\n\n- I am not sure why the notion of product manifolds is required in developing the technique. To me, all the arguments follow without that. Even if the authors are only interested in manifolds that can be constructed in a product manner (say R^n from R),  the development can be done without explicitly going along that path. Nevertheless I may have missed something so please elaborate why product manifolds. I have to add that in many cases, the underlying Riemannian geometry cannot be derived as a product  space. For example, the SPD manifold cannot be constructed as a product space of lower dimensional geometries. \n\n- I have a feeling that finding the operator \\Pi in many interesting cases is not easy. Given the dependency of the developments on this operator, I am wondering if the method can be used to address problems on other manifolds such as SPD, Grassmannian or Stiefel. Please provide the form of this operator for the aforementioned manifolds and comment on how the method can be used if such an operator is not at our disposal.\n\n- While I appreciate the experiments done in the paper,  common tests (e.g., Frechet means) are not presented in the paper (see my comment below as well).  \n\n- last but not least, the authors missed the work of   Roy et. al., \"Geometry Aware Constrained Optimization Techniques for Deep Learning\", CVPR'18 where RSGC with momentum and Riemannian version of RMSProp are developed. This reference should be considered and compared.\n\n\nAside from the above, please\n\n- define v and \\hat{v} for Eq.(5) \n\n- provide a reference for the claim at l3-p4 (claim about the gradient and Hessian)\n\n- maybe you want to mention that \\alpha -> 0 for |g_t^i| at the bottom of p4\n\n- what does [.] mean in the last step of the algorithm presented in p7\n\n- what is the dimensionality of the Hn in the experiments\n \n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper is well-writen except a few flaws (see below). The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.",
            "review": "This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.\n\nThis paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.\n\nRemarks:\n*) P1, line 2: it particular -> in particular.\n*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.\n*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.\n*) P5, Figure 1: does a loop for the index $i$ missing?\n*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \\to R: \\theta-> , where m is the number of nodes. Otherwise, it is not obvious to see the domain. \n*) P7, last paragraph: Tables 2 and 3 -> Figures 2 and 3.\n*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}