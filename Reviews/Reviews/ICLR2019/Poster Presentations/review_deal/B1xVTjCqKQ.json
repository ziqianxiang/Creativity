{
    "Decision": {
        "metareview": "This paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline. \nReviewers reached a consensus that this is an interesting contribution that advances data-driven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights. We thus recommend acceptance as poster. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Trainable Image Compressed Sensing with solid empirical results"
    },
    "Reviews": [
        {
            "title": "Review: An interesting approach to data-driven compressed sensing",
            "review": "\nThis paper proposes a (CNNs) architecture for encoding and decoding images for compressed sensing. \nIn standard compressed sensing (CS), encoding usually is linear and corresponds to multiplying by a fat matrix that is iid gaussian. The decoding is performed with a recovery algorithm that tries to explain the linear measurements but also promotes sparsity. Standard decoding algorithms include Lasso (i.e. l1 regularization and a MSE constraint) \nor iterative algorithms that promote sparsity by construction. \n\nThis paper instead proposes a joint framework to learn a measurement matrix Phi and a decoder which is another CNN in a data-driven way. The proposed architecture is novel and interesting.  \n\nI particularly liked the theoretical motivation of the used MSE loss by maximizing mutual information. \n\nThe use of parallel convolutions is also neat and can significantly accelerate inference, which can be useful for some applications. \n\nThe empirical performance is very good and matches or outperforms previous state of the art reconstruction algorithms D-AMP and Learned D-Amp. \n\nOn comparisons with prior/concurrent work: The paper is essentially a CNN autoencoder architecture but specifically designed for compressed sensing problems. \nThere is vast literature on CNN autoencoders including (Jiang 2017 and Shi 2017) paper cited by the authors. I think it is fine to not compare against those since they divide the images into small blocks and hence have are a fundamentally different approach. This is fine even if block-reconstruction methods outperform this paper, in my opinion: new ideas should be allowed to be published even if they do not beat SOTA, as long as they have clearly novel ideas. It is important however to discuss these differences as the authors have done in page 2.  \n\nSpecific comments: \n\n1. It would be interesting to see a comparison to D-Amp and LDAmp for different number of measurements or for different SNRs (i.e. when y = Phi x+ noise ). I suspect each method will be better for a different regime?\n\n2. The paper: `The Sparse Recovery Autoencoder' (SRA) by Wu et al. https://arxiv.org/abs/1806.10175\nis related in that it learns both the sensing matrix and a decoder and is also focused on compressed sensing, but for non-image data. The authors should discuss the differences in architecture and training. \n\n3. Building on the SRA paper, it is possible that the learned Phi matrix is used but then reconstruction is done with l1-minimization. How does that perform for the matrices learned with DeepSSRR?\n\n4. Why is Figure 1 going from right to left?\n\n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theory, shakey experiments",
            "review": "Quality & Clarity:\nThis is a nice paper with clear explanations and justifications. The experiments seem a little shakey.\n\nOriginality & Significance:\nI'm personally not familiar enough to say the theoretical work is original, but it is presented as so. However it seems significant. The numerical results do not seem extremely significant, but to be fair I'm not familiar with state of the art nearest neighbor results ie Fig 3.\n\nPros:\nI like that you don't take much for granted. E.g. you justify using convolutional net in 2.1, and answered multiple of my questions before I could type them (e.g. why didn't you include nonlinearities between convolutions, why bother with cascaded convolutions, and what you mean by near-optimal).\n\nCons:\nThe visual comparisons in Figure 4 are difficult to see. DLAMP appears to be over-smoothing but in general it's hard to compare to low-ish resolution noisy-looking textures. I strongly recommend using a test image with a clear texture to illustrate your point (eg the famous natural test image that has on the side a tablecloth with zig-zag lines)\n\nThe horizontal error bars are obfuscated by the lines between markers in Fig 3a.\n\nI don't understand Fig 3a. You are varying M, which is on the Y-axis, and observing epsilon, on the X-axis?\n\nQuestions:\nCan you state what is novel about the discussion in the \"Theoretical Insights\" subsection of 2.1? I guess this is described in your abstract as \"we cast the problem ... by using a maximum likelihood protocol...\" but your contribution could be made more explicit. For example \"We show that by jointly optimizing phi and lambda (sensing and recovery), we are maximizing the lower bound of mutual information between reconstructions (X) and samples (Y)\" (that is my understanding of the section)\n\nWhy don't you use the same M for all methods in the Figure 3 experiments? ie why did you use a different M for numax/random versus deepSSRR/DCN?\n\nWhy do you choose 20-layers for the denoiser? Seems deep...\n\nThe last part of the last sentence of the 2nd paragraph of section 3.1 should be a complete sentence \"though, with more number of parameters\". Does that mean that the DCN has more parameters than the DeepSSRR?\n\nI am willing to change score based on the response\n\n******************\nUpdate after author response:\nThanks for the clear response and Figure 3, and nice paper. My score is updated.\nPS: I still think that the (tiny) error bars are obfuscated because the line connecting them is the same thickness and color.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A data-driven and distributed approach to sparse signal representation and recovery",
            "review": "Authors case the problem of finding informative measurements by using a maximum likelihood formulation and show how a data-driven dimensionality reduction protocol is built for sensing signals using convolutional architectures. A novel parallelization scheme is discussed and analyzed for speeding up the signal recovery process.\n \nPrevious works have been proposed to jointly learn the signal sensing and reconstruction algorithm using convolutional networks. Authors do not consider them as the baseline methods due to the fact that the blocky reconstruction approach is unrealistic such as MRI. However, there is no empirical result to support his conclusion.  In addition, the comparisons to these methods can further convince the readers about the advantage of the proposed method.\n \nIt is not clear where the maximum deviation from isometry in Algorithm 1 is discussed since the MSE is used as a loss function.\n \nAuthors provided theoretical insights for the proposed algorithm. It indicates that the lower-bound of the mutual information is maximized and minimizing the mean squared error is a special case, but it is unclear why this can provide theoretical guarantee for the proposed method. More details are good for the connections between the theory and the proposed algorithm.\n \nOne of the contributions in this paper is the speed, so the results on the speed should be put in the main paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}