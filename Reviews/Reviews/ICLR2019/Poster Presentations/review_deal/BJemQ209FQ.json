{
    "Decision": {
        "metareview": "All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.\n\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": " Important topic, solid contribution"
    },
    "Reviews": [
        {
            "title": "a good RL application paper for dealing with large action and state spaces",
            "review": "This paper developed a curriculum learning method for training an RL agent to navigate a web. It is based on the idea of decomposing an instruction in to multiple sub-instructions, which is equivalent to decompose the original task into multiple easy to solve sub-tasks. The paper is well motivated and easily accessible. The problem tackled in this work is an interesting application of RL dealing with large action and state spaces. It also demonstrates superior performance over the state of the art methods on the same domains\n\nHere are the comments for improving this manuscript:\n  \nThere are a few notations used without definition, for example DOM tree, Potential (in equation (4))\n\nSome justification regarding the the Q value function specified in (1) might be helpful, otherwise it looks very adhoc.\n\nAlthough using both shallow encoding and augmented reward lead to good empirical results, it might be useful to give more insights, for example, sample size limit cause overfitting for deep models?\n\nWhat are the sizes of action state and action spaces?\n\nThe conclusion part is missing.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "solid experiments, needs clarity improvement",
            "review": "UPDATE:\n\nThank you to the authors for a comprehensive response.  I have increased my score based on these changes.  I apologize for the misunderstanding about ArXiV papers and indeed the authors are correct on that point.  Thank you as well for reporting the learning speeds.  As you mentioned, they confirm our intuitions and complete the picture of the algorithm’s behavior.  The addition of pseudo-code does make the paper and algorithm easier to follow.  Thank you for adding it.  The rewritten section 5 is indeed much easier to follow and makes the coordination between the agents clear.  Seeing that the instructor is a fixed policy resolves the game theoretic issue form the original review.\n\n\nSummary:\n\nThe paper proposes a deep reinforcement learning approach to filling out web forms, called QWeb.  In addition to both deep and shallow embeddings of the states, the authors evaluate various methods for improving the learning system, including reward shaping, introducing subgoals, and even a meta-learning algorithm that is used as an instructor.  These variations are tested in several environments and basic QWeb is shown to outperform the baselines and many of the adaptations perform even better than that in more complex domains.\n\nReview:\n\nOverall, the problem the paper considers is important and their results seem significant.  The authors have derived a novel architecture and are the first to tackle the problem of filling in web forms at this scale with an autonomous learning agent rather than one that is taught mostly by demonstration.  \n\nThe related work section is very well written with topical references to recent results and solid differentiations to the new algorithm.  However, I see many references in the paper are not from peer reviewed conferences or journals.  Unless absolutely necessary, such papers should not be cited because they have not been properly peer reviewed.  If the papers cited have actually been in a conference or journal, please add the correct attribution.\n\nThe experiments seem well conducted.  I liked that each new addition to the algorithm was tested incrementally in Figure 7 to give a realistic view of the gains introduced by each change.  I also thought the earlier comparisons to the baselines were well done and I liked that they were done against modern cutting-edge LfD demonstrations.  The only thing I would have liked to seen beyond these results are actual learning curves showing, after X iterations, what percentage of the tasks could be completed.  I suspect that in many domains the baseline LfD techniques are learning much faster since learning from teachers tends to be more targeted and sample efficient.  Learning curves would show us whether or not this is the case. \n\nThe weakest part of the paper was the description of the instructor network and the Meta-training in general.  This portion seemed ill-described and largely speculative, despite the promising results in Figure 7.  In particular, Section 5 is very unclear on how exactly the Meta-Learning works.  Pseudocode is definitely needed in this portion well beyond the quick descriptions in Figure 4 and 5, which I could not understand, despite multiple readings.  I suggest eliminating those figures and providing concrete pseudo—code describing the meta learning and also addressing the following open questions in the text:\n•\tWhy is a rule based randomized policy good to learn from?  How is this different from learning from demonstration in the baselines?\n•\tHow is a “fine grained signal” generated?  What does that mean?  Is it a reward?\n•\tIn Section 5.1, are there two RL agents, an instructor and a learner with different reward functions?  If so, isn’t this becoming game theoretic and is this likely to converge in most scenarios?\n•\tWhat does Q_D^I actually represent?  Why is maximizing these values a good thing?\n\nThere are a few grammatical mistakes in the paper including:\n\nAbstract – simpler environments -> simple environments\nAbstract- with gradually increasing -> with a gradually increasing\nPage 2 – generate unbounded -> generate an unbounded\nPage 7 – correct value -> correct values\nPage 9 – episode length -> episode lengths\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel proposal addressing a complex problem with a large number of components but without a clear analysis of their relevance",
            "review": "The paper propose a framework to deal with large state and action\nspaces with sparse rewards in reinforcement learning. In particular,\nthey propose to use a meta-learner to generate experience to the agent\nand to decompose the learning task into simpler sub-tasks. The authors\ntrain a DQN with a novel architecture to navigate the Web.\nIn addition the authors propose to use several strategies: shallow\nencoding (SE), reward shaping (AR) and curriculum learning (CI/CG). \nIt is shown how the proposed method outperforms state-of-the-art\nsystems on several tasks.\n\nIn the first set of experiments it is clear the improved performance\nof QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not\nable to learn in the social-media-all problem. The authors tested only\none of the possible variants (AR) of the proposed approach with good\nperformance. \n\nIt is not clear in the book-flight-form environment, why the\nQWeb+SE+AR obtained 100% success while the MetaQWeb, which includes\none of main components in this paper, has a lower performance.\n\nThe proposed method uses a large number of components/methods, but it\nis not clear the relevance of each of them. The papers reads like, \"I\nhave a very complex problem to solve so I try all the methods that I\nthink will be useful\". The paper will benefit from an individual\nassessment of the different components.\n\nThe authors should include a section of conclusions and future work.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}