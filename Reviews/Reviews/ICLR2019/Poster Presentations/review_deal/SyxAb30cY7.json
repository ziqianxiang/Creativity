{
    "Decision": {
        "metareview": "This paper provides interesting discussions on the trade-off between model accuracy and robustness to adversarial examples. All reviewers found that both empirical studies and theoretical results are solid. The paper is very well written. The visualization results are very intuitive. I recommend acceptance.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good paper. Accept."
    },
    "Reviews": [
        {
            "title": "interesting findings, however seems to confirm some of the already known behavior in linear classification setup",
            "review": "This paper presents a study of tradeoffs between adversarial and standard accuracy of classifiers. Though it might be expected that training for adversarial robustness always leads to improvement in standard accuracy, however the authors claim that the actual situation is quite subtle. Though adversarial training might help towards increasing standard accuracy in certain data regimes such as data scarcity, but when sufficient data is available there exists a trade-off between the two goals. The tradeoff is demonstrated in a fairly simple setting in which case data consists of two kinds of features - those which are weakly correlated with the output, and those which are strongly correlated. It is shown that adversarial accuracy depends on the feature which exhibit strong correlation, while standard accuracy depends on weakly correlated features.\n\nThough the paper presents some interesting insights. Some of the concerns  are :\n - The paper falls short in answering the tradeoff question under a more general setup. The toy example is very specific with a clear separation between weak and strongly correlated features. It would be interesting to see how similar results can be derived when under more complicated setup with many features with varying extent of correlation.\n - The tradeoff between standard and robustness under linear classification has also been demonstrated in a recent work [1]. In [1], it is also argued that for datasets consisting of large number of labels, when some of the labels are under data-scarce regimes, an adversarial robustness view-point (via l1-regularization) helps in accuracy improvement for those labels. However, for other set of labels for which there is sufficient data available,  l2-regularization is more suited, and adversarial robustness perspective decreases standard accuracy. From this view-point, one could argue that some of the main contributions in the current paper, could be seen as empirical extensions for deep learning setup. It would be instructive to contrast and explore connections between this paper, and the observations in [1].\n[1] Adversarial Extreme Multi-label Classification, https://arxiv.org/abs/1803.01570\n==============post-rebuttal======\nthanks for the feedback, I update my rating of the paper",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "good paper, interesting findings, should be cautious on over-claiming",
            "review": "This paper discusses the hypothesis of the existence of intrinsic tradeoffs between clean accuracy and robust accuracy and corresponding implications. Specifically, it is motivated by the tradeoffs between clean accuracy and robust accuracy of adversarially trained network. The authors constructed a toy example and proved that any classifier cannot be both accurate and robust at the same time. They also showed that regular training cannot make soft-margin SVM robust but adversarial training can. At the end of the paper, they show that input gradients of adversarially trained models are more semantically meaningful than regularly trained models.\n\nThe paper is well written and easy to follow. The toy example is novel and provides a concrete example demonstrating robustness-accuracy tradeoff, which was previously speculated. Demonstrating adversarially trained models has more semantically meaningful gradient is interesting and provides insights to the field. It connects robustness and interpretability nicely.\n\nMy main concern is on the overclaiming of applicability of the \"inherent tradeoff\". The paper demonstrated that the \"inherent tradeoff\" could be a reasonable hypothesis for explaining the difficulty of achieving robust models. I think the authors should emphasize this in the paper so that it does not mislead the reader to think that it is the reason.\n\nOn a related note, Theorem 2.2 shows adversarial training can give robust classifier while standard training cannot. Then the paper says \"adversarial training is necessary to achieve non-trivial adversarial accuracy in this setting\". The word \"necessary\" is misleading, here Thm 2.2 showed that adversarial training works, but it doesn't exclude the possibility that robust classifiers can be achieved by other training methods. \n\nminor comments\n- techinques --> techniques\n- more discussion on the visual difference between the gradients from L2 and L_\\infty adversarially trained networks\n- Figure 5 (c): what does \"w Robust Features\" mean? are these values accuracy after perburtation?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, clear accept",
            "review": "The paper demonstrates the trade-off between accuracy and robustness of a model. The phenomenon is shown in previous works, but this work interestingly proposes a theoretical model that supports the idea. The proving technique can be particularly beneficial to developing theoretical understanding for the phenomenon. Besides, the authors also visualize the gradients and adversarial examples generated from standard and adversarially trained models, which show that these adversarially trained models are more aligned to human perception.\n\nQuality: good, clarity: good, originality: good, significance: good\n\nPros: \n- The paper is fairly well written and the idea is clearly presented\n- To the best of my knowledge (maye I am wrong), this work is the first one that \nprovides theoretical explanation for the tradeoff between accuracy and robustness\n- The visualization results supports their hypothesis that adversarially trained models \npercepts more like human.\n\nSuggestions:\nIt would be interesting to see what kind of real images can fool the models and see whether the robust model made mistakes more like human.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}