{
    "Decision": {
        "metareview": "see my comment to the authors below",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Interesting idea, scope is much narrower than presentation would suggest"
    },
    "Reviews": [
        {
            "title": "Interesting problem and approach; more experimental domains and careful analysis on experiment results would be appreciated.",
            "review": "Summary:\n\nThis paper proposes a policy evaluation and search method assisted by a counterfactual model, in contrast previous work using vanilla (non-causal) models. With “no model mismatch” assumption the policy evaluation estimator is unbiased. Empirically, the paper compares Guided Policy Search with counterfactual model (CF-GPS) with vanilla GPS, model based RL algorithm and show benefit in terms of (empirical) sample complexity.\n\nMain comments:\n\nThis paper studies several interesting problems: 1) policy learning with off-policy data; 2) model based RL and how to use model to help policy learning. By capturing a nice connection between causal models and MDP/POMDP model with off-policy data, this paper can leverage SCMs to help the model guided policy search in POMDP. The combination of those ideas is novel and enjoyable.\n\nOn the negative side, I find I met several confused points as a reader with more RL background and less causal inference background. It would be better if the authors could clarify what is the prior distribution P(u) and posterior distribution P(u|h) exactly means in terms of CF-PE algorithm and MB-PE algorithm. I would also appreciate if a more detailed proof of corollary 1 and 2 are included in the appendix, and a higher level intuition/justification about those two results in main body. Maybe I am missing these points due to my limited background in causal inference, but I think those clarification can definitely be helpful for RL audience without that much knowledge in causal inference.\n\nThe main theoretical result seems to be based on the assumption of no model mismatch, and I guess here how the model is estimated from sample are ignored, unless I missed anything. Thus I assume the main contribution of this paper should be algorithmic and empirical. I expect to see the empirical study in more domains with more informative results about how this CF model get the benefit of sampling from p(u|h) rather than p(u) (as an evidence to support motivation paragraph on page 5). ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting ideas; unclear if assumptions are too strong",
            "review": "Summary: by assuming a correct, strongly factored environment model, improved estimators useful for policy search can be derived by \"counterfactual reasoning\", where data sampled from experience is used to refine initial conditions in the model; this translates into improved estimators of policy values, which improves policy search.\n\nMajor comments:\n\nI enjoyed this paper.  I think that model-based RL deserves more work, and I think that this is a simple, reasonably workable approach with some nice theoretical benefits.  I like the idea of SCMs; I like the idea of counterfactual reasoning; I like the idea of leveraging models in this unique way.\n\nOn the negative side, I felt that the paper makes some rather strong assumptions - specifically, that the agent has access to a perfect model with no mismatch, and that the model decomposes neatly into noise variables plus deterministic functions.  Given such a model, one wonders if there are other techniques, say, from classical planning, that could also be used for some sort of policy search.\n\nI have a few questions about approximations.  First, I see that probabilistic inference is a core element of each algorithm (where p(u|h) must be computed).  For large, complex models, I assume this must be approximate inference.  This leads naturally to questions about accuracy (does approximate inference result in biased estimators? [probably yes]), efficacy (do the inaccuracies inherent in approximate inference outweigh the benefits of using p(u|h) vs. p(u)?) and scalability (how large of a model can we reasonably cope with before degradation is unacceptable, or no better than non-CF algorithms?).  As far as I can tell, none of this was addressed in the paper, although I do not expect every paper to answer every question; this is a first step.\n\nI wish the experiments were a little more varied.  The experimental results really only show marginal improvement in one small task.  While I understand that this is not an empirical paper, neither does it fit strongly into the category of \"theory paper\".  For example, there are no theory results indicating what sort of benefit we might expect from using the methods outlined here, and in the absence of such theory, we might reasonably look to various experiments to demonstrate its effectiveness.\n\nPros:\n+ Integration with SCMs is interesting\n+ Counterfactual variants of algorithms are clearly motivated and interesting\n+ Paper is generally well-written\n\nCons:\n- Assumption that the agent is given a model with no mismatch is very strong\n- Model class (noise variables + deterministic functions) seems potentially restrictive\n- Questions about impact of approximate inference\n- Experiments could have been more varied\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach to relevant problem; nice integration of causal reasoning with RL; experiment setup avoids dealing with some practical challenges",
            "review": "Summary:\nProposes Counterfactual Guided Policy Search (CF-GPS), which uses counterfactual inference from sampled trajectories to improve an approximate simulator that is used for policy evaluation. Counterfactual inference is formalized with structural causal models of the POMDP. The method is evaluated in partially-observed Sokoban problems. The dynamics model is assumed known, and a learned model maps observation histories to a conditional distribution on the starting state. CF-GPS outperforms model-based policy search and a \"GPS-like\" algorithm in these domains. GPS in MDPs is shown to be a particular case of CF-GPS, and a connection is also suggested between stochastic value gradient and CF-GPS.\n\nReview:\nThe work is an interesting approach to a relevant problem. Related literature is covered well, and the paper is well-written in an approachable, conversational style. \n\nThe approach is technically sound and generally presented clearly, with a few missing details. It is mainly a combination of existing tools, but the combination seems to be novel. \n\nThe experiments show that the method is effective for these Sokoban problems. A weakness is that the setting is very \"clean\" in several ways. The dynamics and rewards are assumed known and the problem itself is deterministic, so the only thing being inferred in hindsight is the initial state. This could be done without all of the machinery of CF-GPS. I realize that the CF-GPS approach is domain-agnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties. The issue of inaccurate dynamics models seems especially relevant, and is not addressed by the Sokoban experiment. It's also notable that the agent cannot affect any of the random outcomes in this problem, which I would think would make counterfactual reasoning more difficult.\n\nComments / Questions:\n* Please expand on what \"auto-regressive uniformization\" is and how it ensures that every POMDP can be expressed as an SCM\n* What is the prior p(U) for the experiments? \n* \"lotion-scale\" -> \"location-scale\"\n\nPros:\n* An interesting and well-motivated approach to an important problem\n* Interesting connections to GPS in MDPs\n\nCons:\n* Experimental domain does not \"exercise\" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known\n* Work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper-)parameter settings ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}