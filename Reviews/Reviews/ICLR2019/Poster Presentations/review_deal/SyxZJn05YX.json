{
    "Decision": {
        "metareview": "The paper proposes an interesting idea (using \"reliable\" samples to guide the learning of \"less reliable\" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects.\n\nOn the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less-reliable samples is domain-specific (it makes sense for object detection tasks, but it's unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to \"Feature Intertwiner for Object Detection\" to alleviate such criticisms. \n\nGiven this said, I think this paper is over the acceptance threshold and would be of interest to many researchers. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "decision"
    },
    "Reviews": [
        {
            "title": "performance is good but novelty is limited",
            "review": "This paper aims to facilitate feature learning in NN models by exploiting more from reliable examples. This is very similar to self-paced learning where the model  learns from the easier samples at first and proceeds to learn from difficult and challenging samples. The authors should discuss their difference with self-paced learning. \n\nThe method is positioned as a general one for feature learning. I do not know the reason why the authors only apply for object detection on a very specific dataset. It is expected to see whether the proposed method is also effective for image classification. \n\nMore datasets for evaluation are needed, even only for the object detection application. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting take on the problem of detecting small objects",
            "review": "OVERVIEW:\nThe authors tackle the problem of detecting small/low resolution objects in an image. Their key idea is that detecting bigger objects is an easier task and can be used to guide the detection of smaller objects. This is done using the \"Feature Intertwiner\"  which consists of two branches, one for the larger objects (more reliable set that is also easier to detect) and one for the smaller objects (less reliable set). The second branch contains a make-up layer learned during training (which acts as the guidance from the more reliable set) that helps compensate details needed for detection. The authors define a class buffer that contains representative elements of object features from the reliable set for every category & scale and an intertwiner loss that computes the L2 loss between the features from the less reliable set & the class buffer. They also use an Optimal Transport procedure with a Sinkhorn divergence loss between object features from both sets. The overall loss of the system is now a sum of the detection loss, the intertwiner loss and the optimal transport loss. They evaluate their model on the COCO Object detection challenge showing state-of-the-art performance. They also provide thorough ablation analysis of various design choices. The qualitative result in Fig.1 showing well clustered features for both high & low resolution objects via t-SNE is a nice touch.\n\nCOMMENTS:\nClarity - The paper is well written and easy to follow.\nOriginality & Significance - The paper tackles an important problem and provides a novel solution. \nQuality - The paper is complete in that it tackles an important problem, provides a novel solution and demonstrates via thorough experiments the improvement achieved using their approach. \n\nQUESTIONS:\n1. The Class Buffer seems very restricted in having a single element per object category per scale to represent all features. The advantage of forcing such a representation is tight clustering in the feature space. But, wouldn't a dictionary approach with multiple elements give more flexibility to the model and learn a richer feature representation at the cost of not-so-good clustering ?\n2. Any comment on why you drop performance for couch ? (and baseball bat + bedroll)\n3. In Table 4 of Appendix where you compare with more object detection results, I find it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else. I was expecting you to be the best under that metric due to the explicit design for small objects. They (MaskRCNN, updated results) are also significantly better than the rest under AP_M but worse under AP_L. Can you explain this behavior ? Is the ResNeXt backbone that much better for small objects ?\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a novel approach with the hypothesis that the reliable features can guide the less reliable ones. This approach is applied to the object detection task and show consistent performance improvements.\n\npros)\n(+) This paper is well-written and easy to follow.\n(+) The base idea that divides the learned features into two sets; the reliable feature set and the less reliable one is very interesting and looks novel. Plus, the hypothesis, which is that reliable features can guide the features in the less reliable set is also interesting.\n(+) The performance improvements are quite large.\n(+) Extensive ablative studies are provided to support the proposed method well.\n\ncons)\n(-) The method of obtaining the representative in buffer B is not clearly presented.\n(-) The overall training and inference procedure are not clearly presented. \n(-) Some notations and descriptions are vague and confusing.\n(-) More than two datasets are necessary to show the effectiveness of the methods\n\ncomments)\n- What is the higher level feature map P_m? and How did you choose the higher level feature map at the m-th level in option (b) and (c) in Section 3.3.\n- What is the meaning of the \"past\" features in Section 3.2?\n- It is better to show the exact architecture of the make-up module and the critic module.\n- Can this method apply to the other backbones such as VGG or ResNets without FPN?\n- The sentences at the bottom of p.4 starting with \"Note that only~\" looks ambiguous. \n- f_critic^j may be the j-th element of F_critic, please denote what f_critic^j stands for.\n\nEven if the paper needs to be revised for better readability, I think this paper is above the standard of ICLR because the idea is interesting and novel. Furthermore, the experimental studies are properly designed and well support the main idea. I am leaning toward acceptance, but I would like to see the other reviewers' comments.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}