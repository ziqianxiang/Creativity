{
    "Decision": {
        "metareview": "\nPros:\n- A useful and well-structured dataset which will be of use to the community\n- Well-written and clear (though see Reviewer 2's comment concerning the clarity of the model description section)\n- Good methodology\n\nCons:\n- There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn't be harvested from school texts directly.  Presumably it would've been a lot more work but please address the issue in your rebuttal.\n- Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics-specific models (a couple of which are mentioned in the text).  On the other hand, Reviewer 2 is concerned that the specific choices (e.g. \"thinking steps\") made for the general models are non-standard in seq-2-seq models.  I haven't heard about the thinking step approach but perhaps it's out there somewhere. It would be helpful generally to have more discussion about the reasoning involved in these decisions.\n\nI think this is a useful contribution to the community, well written and thoughtfully constructed.  I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section.  Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen.  Also consider adding additional experiments to allay the concerns of the reviewers.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good contribution; some questions about the evaluation"
    },
    "Reviews": [
        {
            "title": "Interesting dataset, but the evaluation and comparison need to be improved",
            "review": "This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models. It consists of math problems in various categories such as algebra, arithmetic, calculus, etc. The dataset is designed carefully so that it is very unlikely there will be any duplicate between train/test split and the difficulty can be controlled. Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset. The result showed some interesting insights about the evaluated models. The evaluation of mathematical reasoning ability is an interesting perspective. However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough. \n\nThe paper is relatively well-written, although the description of the neural models can be improved. \n\nThe generation process of the dataset is well thought out. The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited. \n\nOne suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks. \n\nMy main concerns are about the evaluation and comparison of standard neural models. The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM\" doesn’t seem to be a standard approach. In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM\"). I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models. \n\nWith some improvements in the evaluation and comparison, I believe this paper will be more complete and much stronger. \n\ntypo:\npage 3: “freefrom inputs and outputs” -> “freeform inputs and outputs”\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An intriguing paper on dataset generation for math problem-solving",
            "review": "Summary: This paper is about models for solving basic math problems. The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type. The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems. The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down. \n\nStrengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models. There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these. The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good.\n\nWeaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks). It would have been useful to compare the general models here with some specific math problem-focused ones as well. Some details weren't clear to me. More in the comments below.\n\nVerdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion.\n \n \nComments:\n\n- One area that could stand to be improved is prior work. I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems. Since this is the core contribution, this should also be the main comparison. For example, EMLNP 2017 paper \"Deep Neural Solver for Math Word Problems\" mentions a size 60K problem dataset. A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse.\n\n- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets? \n\n- The authors divide dataset construction into crowdsourcing and synthetic. This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students. These are solved, and only require very limited validation. They are also categorized by difficulty and area. Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc? \n\n- How do are the difficulty levels synthetically determined?\n\n- When generating the questions, the authors \"first sample the answer\". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected.\n\n- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.\n\n- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3). This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive. But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful. In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this). On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited. \n\n- Really would be good to do real-world tests in a more extensive way. A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?\n\n- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?\n\n- The 1+1+...+1 example is pretty intriguing, and could be a nice \"default\" question!\n\n- Minor typo: in the abstract: \"test spits\" should be \"test splits\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of \"Analysing Mathematical Reasoning Abilities of Neural Models\"",
            "review": "This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems. The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer. \n\nI think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results. I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset. The paper is a reasonable dataset/analysis paper. Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).\n\nI think that the dataset generation process is well-thought-out. There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion. The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks. I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly. Details on how to generate the dataset, however, can be moved into the appendix. I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a \"soft\", secondary metric?\n\nOne other thing I want to see is a test set with multiple different difficulty levels. The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)? Also, is there an option for \"unsolvable\"? For example, the answer being a special \"this is impossible\" character for \"factorise x^2 - 5\" (if your training set does not use \\sqrt, of course).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}