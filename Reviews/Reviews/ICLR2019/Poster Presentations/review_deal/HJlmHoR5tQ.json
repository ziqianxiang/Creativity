{
    "Decision": {
        "metareview": "This paper proposes a regularization for IRL based on empowerment. The paper has some good results, and is generally well-written. The reviewers raised concerns about how the approach was motivated; these concerns have largely been addressed from the reframing of the algorithm from the perspective of regularization. Now, all reviewers agree that the paper is somewhat above the bar for acceptance. Hence, I also recommend accept. There are several changes that the authors are strongly encouraged to incorporate in the final version of the paper (based on discussion between the reviewers):\n- The claim that empowerment acts as a regularizer in the policy update is a fairly complicated interpretation of the effect of the algorithm. It relies on an approximation derived in the appendix that relates the proposed objective with an empowerment regularized IRL formulation. The new framing makes much more sense. However, the one sentence reference to this section of the appendix in the main paper is not appropriate given that it is central to the claims of the paper's contribution. More discussion in the main text should be included.\n- There are still some parts of the implemented algorithm that could introduce bias (using a target network in the shaping term which differs from the theory in Ng et al. 1999), but this concern could be remedied by a code release. The authors are strongly encouraged to link to the code in the final non-blind submission, especially since IRL implementations tend to be quite difficult to get right.\n- The authors said they would change the way they bold their best numbers in their rebuttal. The current paper does not make the promised change, and actually adopts different bolding conventions in different tables which is even more confusing. The numbers should be bolded in a consistent way, bolding the numbers with the best performance up to statistical significance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta review"
    },
    "Reviews": [
        {
            "title": "Good empirical results, but overall lacking in clarity with potentially problematic issues",
            "review": "Summary/Contribution:\nThis paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.\n\nPros:\n    - The approach outperform AIRL by a convincing margin on the crippled ant problem, while obtaining comparable/favorable performance on other benchmarks.\n\nCons:\n    - The justification for using the empowerment maximization framework to learn the shaping parameters is unclear. The formulation introduces a potentially confounding factor by biasing the policy optimization which clouds the experimental picture. \n\nJustification for rating:\nThis paper presents good empirical results, but without a clear identification of the source of improvement. I lean on the side of rejecting unless the authors can better eliminate any potential bias in their formulation (see question below). The justification for combining the empowerment maximization objective is also unclear while being integral to the novelty of the proposed method. \n\nQuestions I could not resolve from my reading:\n    - The \"imitation learning benchmark\" numbers in Table 2 are different from the original AIRL paper. Do the authors have an explanation as to why? Is this only due to a difference in the expert performance?\n    - Can the authors confirm that in the transfer experiments, the policy is optimized with only the transfered reward and no empowerment bonus? Otherwise, can the authors comment on whether the performance benefits could be explained by the additional bonus.\n    - In equation (12), \\Phi is optimized as an (approximate) mutual information, not a value function, so it is not clear why this term approximates the advantage (I suspect this is untrue in EAIRL as V* is recovered at optimality in the AIRL/GAN-GCL formulation). Can the authors comment?\n    - Why is w* unnormalized? Unless I am misunderstanding something, in the definition immediately above it, there is a normalization term Z(s). \n\nOther comments:\n    - \"AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near optimal rewards function\" -> This characterization is strange since on some tasks AIRL(s,a) outperforms or is within one standard deviation of EAIRL (e.g. on Half Cheetah, Ant, Swimmer, Pendulum).\n    -  \"Our experimentation highlights the importance of modeling discriminator/reward functions.. as a function of both state and action\". AIRL(s) is better on both the pointmass and crippled-ant task than AIRL(s,a). Can the authors clarify?\n    - \"Our method leverages .. and therefore learns both reward and policy simultaneously\". Can the authors clarify in what sense the reward and policy is being learned simultaneously in EAIRL where it is not in AIRL?\n    - In all the tables, the authors' approach is bolded as oppose to the best numbers. I would instead prefer that the authors bold the best numbers to avoid confusion.\n\n- Typos:\n    - \"the imitation learning methods were proposed\"\n    - \"quantify an extent to which\" \n    - \"GAIL uses Generative Adversarial Networks formulation\"\n    - \"grantee\"\n    - \"no prior work has reported the practical approach\"\n    - \"but, to\"\n    - \"(see (Fu et al., 2017))\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "AR3: Adversarial Imitation via Variational Inverse Reinforcement Learning",
            "review": "The authors propose empowerment-based adversarial inverse reinforcement learning (EAIRL), an extension of AIRL which uses empowerment (which quantifies the extent that an agent can influence its state, see eq. 3) as a reward-shaping potential to recover more faithful learned reward functions. \n\nEvaluation:     4/5     Experiments are more preliminary but establish the benefit of the approach.\nClarity:        4/5     Well written. Just a few typos (see below minor comments)\nSignificance:   4/5     Effective, well motivated approach. Excellent transfer learning results.\nOriginality:    3.5/5   As the empowerment subroutine is existing work, as is AIRL, combining previous work, but effectively.\n\nRating:         7/10\nConfidence:     3/5     Reviewed this paper in a little less detail than I would prefer, due to time constraints. I will review in more detail and update this and add any additional questions/comments below the minor comments below.\n\nPros:\n- Extension of AIRL which utilizes empowerment to advance the SOE in reward learning\n- Well written, related previous work well explained.\nCons:\n- Experiments more preliminary\n- Combines existing approaches, somewhat incremental\n\nMinor comments: \n- grantee (typo), barely utilized -> not fully realized?, \n\n----\n\nUpdated review:\n\nAfter reviewing the comments and the paper in more detail (whose story has evolved substantially) , I have revised my score slightly lower. While in hindsight I can see that the paper has definitely improved, the story has changed rather dramatically, and appears to be still unfolding: the paper's many new elements require further maturation, and that the utility of empowerment for reward shaping and/or regularization to evolve AIRL (i.e. the old story vs. the new story) still needs further investigation/maturation. If the paper is accepted I'm reasonably confident that the authors will be able to \"finish up\" and address these concerns. \n(typo: eq. 4 omits maximizing argument)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good work, but the important aspects are not discussed",
            "review": "The paper proposes a method for inverse reinforcement learning based on AIRL. It's main contribution is that the shaping function is not learned while training the discriminator, but separately as an approximation of the empowerment (maximum mutual information). This shaping term aims to learn disentangled rewards without being restricted to learning state-only reward functions, which is a major restriction of AIRL.\n\nThe main weakness of the paper is, that it does not justify or motivate the main deviations compared to AIRL. The new objective for updating the policy is especially problematic because it does no longer correspond to the RL objective but includes an additional term that biases the policy towards actions that increase its empowerment. Although both terms of the update can be derived independently from an IRL and Empowerment perspective respectively, optimizing the sum was not derived from a common problem formulation. By combining these objectives, the learned reward function may lead to policies that fail to match the expert demonstration without such bias. This does not imply that the approach is not sound per se, however, simply presenting such update without any discussion is insufficient--especially given that it constitutes the main novelty of the approach. I think the paper would be much stronger if the update was derived from an empowerment-regularized IRL formulation. And even then, the implications of such bias/regularization would need to be properly discussed and evaluated, in particular with respect to the trade-off lambda, which--again--is hardly mentioned in the submission. I'm also not sure if the story of the paper works out; when we simply want to use empowerment as shaping term, why not use two separate policies for computing the empowerment and reward function respectively. Is the bias in the policy update maybe more important than the shaping term in the discriminator update for learning disentangled rewards?\n\nKeeping these issues aside, I actually like the paper. It tackles the main drawback of AIRL and the idea seems quite nice. Having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions. The paper is also well written with few typos. The parts that are discussed are clear and the experimental results seem fine as well (although more experiments on the reward transfer would be nice).\n\nMinor notes:\nI think there is a sign error in the policy update\nTypo in the theorem, grantee should be guarantee\n\nQuestion:\nPlease confirm that the reward transfer was learned with a standard RL formulation. Does the learned policy change, when we use the empowerment objective as well?\n\n\n\nUpdate (22.11)\nI think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. \n\n- The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.\n\n- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.\n\n- The derivation could be a bit more rigorous.\n\nAs the presentation is now much more sound, I slightly increased my rating.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}