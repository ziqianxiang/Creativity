{
    "Decision": {
        "metareview": "Pros\n\n- Thorough analysis on a large number of diverse tasks\n- Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)\n- The comparison can be useful when deciding which representations to use for a given task\n\nCons\n\n- Nothing serious, it is solid and important empirical study\n\nThe reviewers are in consensus.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "A thorough study of contextualized word representations"
    },
    "Reviews": [
        {
            "title": "Nice empirical paper",
            "review": "\nThis is a nice paper that attempts to tease apart some questions about the effectiveness of contextual word embeddings (ELMo, CoVe, and the Transformer LM). The main question is about the value of context in these representations, and in particular how their ability to encode context allows them to also (implicitly) represent linguistic properties of words. What I really like about the paper is the “Edge probing” method it introduces. The idea is to probe the representations using diagnostic classifiers—something that’s already widespread practice—but to focus on the relationship between spans rather than individual words. This is really nice because it enables them to look at more than just tagging problems: the paper looks at syntactic constituency, dependencies, entity labels, and semantic role labeling. I think the combination of an interesting research question and a new method (which will probably be picked up by others working in this area) make this a strong candidate for ICLR. The paper is well-written and experimentally thorough.\n\nNitpick: It would be nice to see some examples of cases where the edge probe is correct, and where it isn’t.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice discussion of what type of information is actually encoded by contextualized word embeddings",
            "review": "This paper provides new insights on what is captured contextualized word embeddings by compiling a set of “edge probing” tasks.  This is not the first paper to attempt this type of analysis, but the results seem pretty thorough and cover a wider range of tasks than some similar previous works.  The findings in this paper are very timely and relevant given the increasing usage of these types of embeddings.  I imagine that the edge probing tasks could be extended towards looking for other linguistic attributes getting encoded in these embeddings.\n\nQuestions & other remarks:\n-The discussion of the tables and graphs in the running text feels a bit condensed and at times unclear about which rows are being referred to.\n-In figures 2 & 3: what are the tinted areas around the lines signifying here? Standard deviation?  Standard error?  Confidence intervals?\n-It seems the orthonormal encoder actually outperforms the full elmo model with the learned weights on the Winograd Schema.  Can the authors comment on this a bit more?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Current work reps capture a surprising amount of structure",
            "review": "I have no major complaints with this work.  It is well presented and easily understandable. I agree with the claim that the largest gains are largely syntactic, but this leads me to wonder about more tail phenomena.   PP attachment is a classic example of a syntactic decision requiring semantics, but one could also imagine doing a CCG supertagging analysis to see how well the model captures specific long-tail phenomena.  Though a very different task Vaswani et al 16, for example, showed how bi-LSTMs were necessary for certain constructions (presumably current models would perform much better and may capture this information already).\n\nAn important caveat of these results is that the evaluation (by necessity) is occurring in English.  Discourse in a pro-drop language would presumably require longer contexts than many of these approaches currently handle.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}