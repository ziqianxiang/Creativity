{
    "Decision": {
        "metareview": "Pros:\n- novel, general idea for hard exploration domains\n- multiple additional tricks\n- ablations, control experiments\n- well-written paper\n- excellent results on Montezuma\n\nCons:\n- low sample efficiency (2B+ frames)\n- unresolved questions (non-episodic intrinsic rewards)\n- could have done better apples-to-apples comparisons to baselines\n\nThe reviewers did not reach consensus on whether to accept or reject the paper. In particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points. However, given that the other three reviewers argue strongly and credibly for acceptance, I think the paper should be accepted.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Promising method but poor evaluation and presentation",
            "review": "My apologies for posting late, I was seriously injured around the reviewer deadline.\n\n---------------------------------\n\nThe authors propose \"random network distillation,\" a method that adds an additional reward based on a proxy for \"exploration\" to the RL task at hand. The method works by including an extra term in the reward during training. The term is calculated as follows. A randomly initialized network is created during rollout generation. Another network is initialized as well, and during rollouts is trained to predict the output of the randomly initialized network applied to the states. The agent then uses a measure of the prediction loss as an intrinsic reward. These rewards are then included as part of the trajectory, and are predicted separately for training purposes.\n\nThe authors find that when you combine these intrinsic rewards with agents trained at extremely large scale (~2 billion frames per training run!) it is possible to perform very well on Montezuma's revenge and other sparse reward tasks.\n\nOverall, the paper has great potential - it presents the first algorithm to solve a challenging sparse reward RL task. However, while the method itself is promising, the weak baselines (in particular, the lack of evidence disentangling the benefits of larger scale / more frames vs the benefits of the proposed method) and unclear presentation make me unable to yet recommend the paper for acceptance.\n\nPositive:\n - The work reaches the state-of-the-art on several sparse reward tasks, most notably Montezumas revenge\n - On Montezumas revenge, the method is able to pass through the first level, and explore the vast majority of rooms.\n - The reward mechanism seems to be novel\n\nNegative:\n - All previous work used more than an order of magnitude more frames in training. From the experiments given, it is impossible to distinguish the impact of RND vs larger scale training\n - The baselines are not very strong: The forward dynamics baseline does significantly worse on Montezumas revenge than the previous results in Ostrovski et al and Bellemare et al, even using more than an order of magnitude more frames.\n - Important experimental details lack adequate descriptions\n - Tables and figures are not written with adequate details\n\nDetails of negative feedback:\n\nMajor:\n-------------\nUnclear baselines and questionable improvement on SOTA:\n\n - Previous work (the neural density functions of Ostrovski et al or the CTS scheme of Bellemare et al.) used significantly fewer (~100 million and ~150 million respectively vs ~2 billion) frames of experience in solving Montezumas Revenge, which makes this method’s benefit somewhat incomparable to previous methods given the sampling regime it operates in.\n - It is important to disentangle the impacts of:\n\n   (1) Using many more (an order of magnitude) frames than previous methods\n   (2) The presented RND bonus method\n\n  and it is impossible to separate these without further extensive experimentation with previous methods. The main claim of the paper is that the RND bonus is a better method for solving hard exploration games; this needs to be shown through a rigorous comparison.\n - The fact that the forward dynamics does worse than vanilla PPO (and the previous results in Ostrovski et al and Bellemare et al) on Montezuma's revenge brings the strength of the used baseline into question\n\n\nOverall, the experimental details are greatly lacking:\n\n - The way that the value function is trained (i.e. the objective function) is never explained in the paper. The value function in PPO is typically (according to the baselines repository) trained at each step to fit (GAE advantage + previous value), but in the paper this is not elaborated on.\n - If this is indeed the case, then the statement that the extrinsic value function fits a stationary distribution on page 5 should be fixed.\n - In Table 4 the $\\lambda$ hyperparameter is listed, but is not described at all in the paper. I am guessing that it is the corresponding GAE hyperparameter, but I am not sure as the GAE method is never written about or cited throughout the paper.\n - The paper is not written in a way that is accessible to people that do not closely follow the line of work on sparse rewards. For example, though it is possible to infer, the paper never explicitly defines the intrinsic reward $i_t$ in the main paper text. The exact mechanism through which the \"forward dynamics\" baseline is never given.\n\n\nTables and figures do not give sufficient detail to know what they are describing:\n\n - Table 5 states that the values given are means, but does not say how many samples each mean was generated from until Table 6. The contents of Table 6 should be in the figure captions; it is important to understand how many samples graphs are generated with\n - Similarly, the way that the shaded regions are calculated should be included up front in the first figure with them in it. At first I believed that the intervals were confidence intervals, but they are actually standard deviations.\n - How are the graph lines calculated? I am not sure, but they look like they have been smoothed out - the captions should indicate this if so. If they are smoothed, are the standard deviations calculated before or after smoothing?\n\nMinor:\n-------------\n - Figure 7 has only 3 random seeds compared. To make comparisons between the RND RNN and CNN policies methods you should use more seeds/samples.\n - On page 2 it is said that previous exploration methods are difficult to scale; a (very short) explanation on why would be appreciated\n - On page 4, it would be good to explain why one would be concerned that episodic rewards can leak information about the task to the agent\n - It would be interesting to plot the RND exploration bonus over time as training iteration progresses; this could give some insight into training dynamics that we cannot see from looking at reward trajectories alone.\n - It would be good to include experimentation around understanding if there is a benefit to using this technique in dense reward tasks.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clear writing, strong results, sensible algorithm, good paper",
            "review": "This paper presents an approach to exploration in RL via random network distillation.\nThe agent generates a random neural network, and adds an \"intrinsic reward\" based on the regression error of this random function.\nThe main evidence for its efficacy comes from evaluation on Atari games, particularly Montezuma's revenge, where it attains state of the art results.\n\nThere are several things to like about this paper:\n\n- The writing is clear and well thought out. \n- The actual algorithm is sensible, simple, intuitive and clearly effective.\n- The results are significant: this is really a \"step change\" compared to previous Montezuma results.\n- This work takes the well-known \"exploration bonus\" approach, combines it with some of the observations of (Osband et al) and simplifies the treatment... so in some ways it's quite standard... but there are several new insights:\n  + Focus on normalization schemes for \"randomized prior function\"\n  + Bootstrapping \"intrinsic reward\" over episode boundaries\n  + Incorporating large-scale policy-based algorithms\n\nTo help improve the paper, I will highlight some potential issues:\n\n- For a paper on exploration, it does not make sense to present results in terms of \"parameter updates\". This should instead be presented in terms of actor/environment steps. This is something that happens consistently across the paper. If you want to show that \"many actors makes it better\" then you can divide this by #actors... so that the curves still functionally look the same. This is an easy thing to change... but I think it's important to do this!\n- Like other \"count-based\" methods, this exploration bonus is not linked to the task. As such, you have to get \"lucky\" that you do the right kind of generalization from the \"random network\". I think that you should mention this issue, potentially in your section 2. That is not to say that this is therefore a bad method, but particularly with reference to (Osband et al 2018) this approach does not address their observation from Section 2.4 of that paper... you don't necessarily get the \"right\" type of generalization from this random network (that has nothing to do with the task). You could then point out that, empirically, using a random convnet seems to do just fine in Atari! ;D\n- The whole section about \"pure exploration\" is somewhat interesting, but you shouldn't assess that performance in terms of \"reward\"... because that is just a peculiarity of these games... we could easily imagine a game where \"pure exploration\" gives a huge negative reward... but that wouldn't mean that it was bad at pure exploration! Therefore, how can you justify the quality of pure exploration by reference to the \"best return\".\n- Although the paper is definitely good, and I've already outlined several truly novel additions from this paper, on another level the actual intellectual contribution of this paper is perhaps not *as* large as it may seem from the Abstract or associated OpenAI publicity/blog posts https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/\n  + This paper is about adding an \"exploration bonus\" to RL rewards (this goes back at least to Kearns+Singh 2002)\n  + The form of this bonus comes from prediction error on a random function\n  + I have some concerns on the process of \"anonymous\" reviews in this \"blog+tweet\" setting\n\nOverall, I like the paper a lot, I think it must be accepted and also it's right at the top of ICLR best papers!\nThe writing is good, the results are good, the algorithm is good and I think it will have impact.\nThe main missing piece is a clear discussion of any of the algorithms potential weaknesses - is this the final solution to exploration? What do you think about the issues of generalization? How would this perform in a linear system? What if the basis functions are not aligned?\nIt's not that the algorithm needs to address all of these things to be a good algorithm, but the paper should try to do a better job about highlighting any potential missing pieces - particularly when the results are so impressive.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Efficient and simple to implement exploration strategy for RL",
            "review": "The paper presents a simple but remarkably efficient exploration strategy obtaining state-of-art results in a well known hard-exploration problem (Montezuma's Revenge). The idea consists of several parts:\n1. The authors suggested distilling a fixed randomly initialized network into another randomly initialized trained network in order to use prediction errors as pseudo-rewards. The authors claim that distillation error is a proxy for visit-counts and experimentally demonstrate this idea on MNIST dataset.\n2. The authors suggested using two separate value heads to evaluate expected rewards and expected pseudo-rewards with different time horizons (discount factors) under the same policy.\n\nThe paper is overall well written and easy to read. As far as I can tell, the use of a distillation error as an exploration reward is novel. Relative efficiency of the method compared to its simplicity should interest most people working in RL.\n\nThe main problem which I see is the presentation of learning curves as a function of training steps rather than acting steps. While I acknowledge that the achievement of state-of-art asymptotic performance is valuable on its own, presenting results as a function of acting steps (rather than parameter update steps) may better show data and exploration efficiency. This would also facilitate comparisons with other RL algorithms which may have different architectures (for example, multiple networks updated at different frequencies).\n\nI liked the idea to use two value heads to evaluate intrinsic and extrinsic values with different discounts. Still, as both heads share a common 'trunk' network, they will inevitably affect each other. For example, scaling the pseudo-rewards by 10 and scaling the pseudo-reward value function by 0.1 to produce the same summed value function may lead to a different training dynamics due to the influence of intrinsic value head onto the extrinsic one. Are the results sensitive to this effect? Also, how are the results sensitive to the scale of pseudo-rewards? What would happen if they were simply multiplied or divided by 10?\n\nAlso, what was the distributed training setting that you used to train your agent? Were the actors running on a single machine or on multiple machines? Was a single trainer running on a single machine training network on batched observations or was training distributed in some way? The reason why I am asking this is that as a distillation error fundamentally depends on its training dynamics, I would not be surprised if the results could be affected by the training setting. For example, if the network was trained in a distributed setting, asynchronous updates could introduce implicit momentum and thus may cause a pseudo reward to oscillate. While I do not think that is a fundamental problem with the work either way, it would be nice to know a few more details for future reproducibility.\n\nOther minor comments:\nFigure 2. It would be nice to see if both x and y axes was plotted in log scale in order to visualize any power-law (if one exists) between samples and MSE.\nFigure 3. I would prefer 'x' axis to be in the number of steps.\nFigure 4. Again, performance between different actor-configurations would be easier to see if x axis was a total number of steps, as it would be easier to see if the curves overlap and the method scales linearly with the number of actors.\n\n",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple yet surprisingly effective take on intrinsic motivation for exploration in sparse reward RL tasks",
            "review": "The algorithm proposed in this paper consists in driving exploration in RL through an intrinsic reward, computed as the prediction error of a neural network whose target is the output of a randomly initialized network (with the state reached by the agent as input). The intuition is that rarely seen states will have a large prediction error, thus encouraging the agent to visit them (until they have been seen often enough that the error goes down). Among potential benefits of this method, compared to previously proposed intrinsic curiosity techniques for RL, are its simplicity and its robustness to environment stochasticity. Extensive experiments on the Atari game Montezuma’s Revenge investigate several variants of this idea (combined with PPO), with the best results significantly outperforming the current state-of-the-art. Other results on five other hard exploration Atari games show competitive performance as well.\n\nThe proposed technique definitely exhibits impressive performance on some tasks, in spite of its simplicity. Despite lacking theoretical grounding, I believe such results should be quite interesting to the RL research & applied community, as a novel and easy way to encourage exploration in sparse rewards tasks. I also really appreciate that the authors have included “negative” results contradicting their expectations, and are sharing their code: this is the kind of openness that in my opinion should be highly encouraged.\n\nThe paper is overall well written and easy to follow, except (from my point of view) section 2.2.2, which I found rather confusing and not very convincing. First, eq. 1 is a bit surprising since one expects the posterior to be in the same family of functions, i.e. of the form f_theta rather than f_theta + f_theta*. After a (very superficial) look at Osband et al (2018) I see that this particular lemma holds for linear functions, and the extension to nonlinear function approximation seems to be essentially based on intuition. Then the sentence “the optimization problem (...) is equivalent to distilling a randomly drawn function from the prior” ignores the sign mismatch (we are actually distilling the opposite of f_theta*, though I agree it can still make sense with a symmetric prior around 0, which is not mentioned). Finally, the reasoning to reach the conclusion “the distillation error could be seen as a quantification of uncertainty in predicting the constant zero function” seems somewhat unconvincing to me, considering the significant differences compared to Osband et al (2018), in particular: sharing weights among models in the ensemble, ignoring the specific regularization term R(theta), and not adding noise to the training data. As a result I find this link rather weak and I would appreciate if this section could be improved (at the very least with a better explanation of its limitations)\n\nAmong the various findings from experiments, one puzzled me in particular: the striking difference between episodic and non episodic intrinsic rewards in Fig. 3. I think this would have deserved a more thorough empirical investigation than the intuitive explanation from 2.3 (e.g. by checking whether the agent trained with non episodic rewards was indeed taking more risks and thus dying more often). What I find particularly surprising is that the beginning of the game should not yield much intrinsic reward relatively fast, since it should be the part the agent sees most often initially. As a result, I would expect that getting zero reward when dying (episodic rewards) should not be much different from getting future (small and discounted) intrinsic rewards, unless maybe early in training. What am I missing here?\n\nI also have some comments regarding a couple of other findings and associated hypotheses:\n- Section 3.3 shows some surprising results when varying discount factors (“This is at odds with the results in Figure 3 where increasing gamma_I did not significantly impact performance”). I wonder however to which extent these may be caused by the difference in the scale of discounted returns: for instance increasing gamma_I from 0.99 to 0.999 will (roughly) multiply V_I by 10, giving it more weight in the sum V = V_E + V_I. A fair comparison would either rescale V_I accordingly , or use a weighted sum and optimize the weights (the hyper-parameters table in the Appendix suggests that weights were actually used, but they are not mentioned in the main text and it is not clear how they were chosen).\n- 3.7 shows an interesting behavior (“dancing with skulls”). The authors hypothetize it may be due to the inherent danger of such behavior. But could it be also (and possibly more) related to the fact the skulls are moving? (which leads to many varied different states, that the predictor network will take time to learn perfectly).\n\nHere are a few more questions for the authors regarding specific details:\n1. In 3.1, “The best return achieved by 4 out 5 runs of this setting was 6,700.” What does this mean?\n2. In 3.5 a downsampling scheme is used to keep the training speed of the predictor network constant when increasing the number of actors. This raises the question of the impact of this training speed on the results, which is not investigated in the current experiments: do hyper-parameters influencing the predictor’s training speed (e.g. downsampling ratio, learning rate) need to be very carefully tuned, or are results robust across a wide range of speeds?\n3. In A.5 there is mention of “a CNN policy with access to only the last 16 most recent frames”: does that mean the number of “frames stacked” (Table 2) was increased from 4 to 16? If so, why? (it is not clear to me what we learn compared to Fig. 4)\n4. Your technique implicitly relies on the assumption that the predictor network’s weights will never be exactly the same as the target network’s (as otherwise nothing will be novel anymore, regardless of the states being visited). Do you foresee potential issues with this, and if yes do you have any idea to solve them? (a short discussion in the paper on this topic would be good as well)\n\nAnd finally some suggestions for small improvements:\n- Please try to find another name than “target” network since it is already widely used in the deep RL literature for something completely different (suggestions: “random”, “distillation”, “feature”, “reference”)\n- In 2.1 (last paragraph) there are various papers cited regarding forward or inverse dynamics, but several of them contain both, while the way they are cited suggests they deal only with one. Just moving “and inverse dynamics” before the full list of citations would fix it.\n- In first paragraph of section 3 please mention that the algorithm is based on PPO\n- In Fig. 3 the x axis seems to be missing a multiplication by 1K (?)\n- At end of 3.2, “having two value heads is necessary for combining reward streams with different characteristics”, please specify what are these characteristics. \n- On p. 7, last paragraph: please (briefly) explain how the “random features” are computed\n- The reference Ostrovski et al appears twice\n- In Alg. 1, “Update reward normalization parameters using it”: the “s” in parameters can be misleading, suggesting that both mean and standard deviation are used for normalization => explicitly saying “Update running standard deviation” would avoid such confusion (or say it on the “Normalize” step below)\n- Alg. 1 is not very clear on how returns and advantages are computed (and the corresponding code is not super easy to read). It also seems to be missing the update of the critic V.\n- Alg. 1 mentions “number of optimization steps” while Table 4 says “Number of optimization epochs”: I guess they are the same, so they should probably have the same name\n- After reading the paper, I felt like one learning was that CNN models worked better than RNN ones. However Table 5 shows that this can vary between games (ex: RND RNN outperforms RND CNN on Gravitar and Solaris) and/or algorithms (ex: PPO RNN outperforms PPO CNN on 3 games). I think the main text should at least point to this table when mentioning the superiority of the CNN.\n- In the “Related work” section there is a very short paragraph about “vectorized value functions”. It seems to be overlooking the whole field of multi-objective reinforcement learning. Maybe you could cite a related survey paper like “A Survey of Multi-Objective Sequential Decision-Making”.\n- The paper’s title and the OpenReview submission name should probably match\n\nUpdate following author and reviewer discussion: I agree with others regarding the weakness of the empirical comparison to pseudo-counts in particular, but still believe that the paper deserves to be accepted due to the fact that (1) some of the results are really good, and (2) this is a simple original idea that has the potential to drive further advances (hopefully addressing the empirical and theoretical limitations of the current work)",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}