{
    "Decision": {
        "metareview": "This manuscript proposes a new algorithm for learning from positive and unlabeled data. The motivation for this work includes cases of selection bias, where the positive label is correlated with observation. The resulting procedure is shown to learn a scoring function that preserves the class-posterior ordering, and can thus be thresholded to obtain a classifier.\n\nThe problem addressed is interesting, and the approach sounds reasonable. The writing seems to be well done, particularly after the rebuttal when the work was better placed in context.\n\nThe reviewers and AC note issues with the evaluation of the proposed method. In particular, the authors do not provide a sufficiently convincing empirical evaluation on real data. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "New technique for positive-unlabeled learning focussing on addressing selection bias.",
            "review": "In this paper, the authors present a new technique to learn from positive and unlabeled data. Specifically they are addressing the issues that arise when the positive and unlabeled data do not come from the same distribution. The way to achieve this is to learn a scoring function which preserves -the order- of the label posteriors. In other words, the authors are not making assumptions and then learning the exact posterior of p(y|...) but rather just a function r(x) with the property that if p(y_i) < p(y_j) then r(x_i) < r(x_j).\n\nI am not super familiar in the area but I didn't see any fundamental flaws. The approach makes sense and although I cannot judge the novelty of this paper, it is a useful tool in the PU learning toolbox addressing an arguably important problem (selection bias). Except for section 5.3, the experiments are not that interesting as they are made up artificially by the authors.\n\nThoughts:\n- In example 1, be specific about what p(y|...) and p(o|...) are.\n- In example 2, I wasn't sure what p(o|...) exactly would be.\n- Assumption 1, the first sentence I understand. The \"if and only if\" part I don't see. Can you clarify?\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Reasonable but somewhat unsurprising approach to an interesting problem",
            "review": "The paper proposes an approach to learning from positive and unlabelled data with a sample selection bias. Specifically, it is assumed that the observed positive instances are not necessarily drawn iid from the true positive distribution: rather, there is some bias as to which positive examples are selected. Under an assumption on the selection probability being proportional to the true probability, it is established that one may equally rank instances based on their probability of being labelled. Two algorithms are proposed for this task.\n\nLearning under sample selection bias is an important and interesting problem. It is also arguably more realistic than the classic PU learning setting. The paper proposes a reasonable solution, which builds on some recent advances in the literature on PU learning.\n\nMy only critique is that the results are somewhat unsurprising in light of existing work on this topic (the idea of constructing unbiased risk estimators), and also on the topic of learning from label loans. Further, I believe some clarifications would better position the contributions of the paper, both in terms of strengths and limitations. More specifically:\n\n- it seems the problem could be cast as a (interesting) special case of learning from instance dependent label noise. The assumption of the selection (i.e., label flip) probability preserving the ordering of the true class probability has a fair amount of precedent in these works; see, e.g.,\n\nBylander '97, Learning probabilistically consistent linear threshold functions\nDu and Cai '15, Modelling class noise with symmetric and asymmetric distributions\nBootkrajang '16, A generalised label noise model for classification in the presence of annotation errors\n\nIt is in light of these works that I do not find Theorem 1 surprising. I note that the sample-selection bias setting could be seen as an interesting special case, but some discussion on the connection seems prudent.\n\n- like in the above works, the proposed approach does not construct an unbiased estimator to the underlying risk. Instead, what is shown in e.g. Theorem 3 is that the Bayes-optimal solution to the risk is sensible. This is of course a minimal desiderata for any learning method, but unlike approaches for the classic PU learning setting, the lack of unbiasedness implies that minimizing over a restricted function class F may result in quite different solutions than if we had access to the true labels. Again, this isn't a limitation unique to this particular work, but I did feel the point could be made a little more explicit.\n\n- also like the above work, there isn't a clear way of estimating P(y = 1). As this is crucial for the final risk estimate, it somewhat restricts the universality of the approach.\n\n- with regards to the two algorithms proposed, both go about estimating the underlying \"noisy\" class probability (i.e., the probability of an instance being selected for labeling), just with different losses. While the logistic or \"LSIF\" loss are certainly valid choices, one could use any number of other similar loss (e.g., the exponential loss from class-probability estimation, or the \"KLIEP\" loss from density ratio estimation). Of course the specific choice of LSIF e.g. can be motivated since it has a closed-form solution, but the basic point is that the two approaches really boil down to changing the underlying loss function. This point could also be clarified.\n\n\nOther comments:\n\n- I believe the Elkan & Noto paper operates in the censoring rather than case-controlled setting.\n\n- there are a few grammatical issues: e.g.., \"Several recent researches\", \"is to find anomaly data\"\n\n- I don't follow how the case-controlled setting is \"more general\" than the censoring setting, as claimed in Sec 2; do you mean it is more practically realistic?\n\n- it is correct to say in 2.1 that one cannot estimate p(y = 1 | x) from only PU data without assumptions. The next sentence states that a typical assumption that is thus made is SCR. However, this also does not guarantee that we can estimate the probability, since estimating p(y = 1) is also not possible without even further assumptions (see e.g. the mutually contaminated distribution work of Scott et al., 2013).\n\n- in Defn 1, it would be clearer to explicate the dependence of all quantities on r.\n\n- it is interesting that one achieves the BEP with the choice of threshold given by (2). But given that p(y = 1) is in general hard to estimate, it seems one could equally cast the problem of estimating p(y = 1) as the problem of choosing a good threshold? (This of course ignores the fact that we ostensibly need p(y = 1) when constructing the risk estimate.)\n\n- restricting attention to scorers with output in [eps, 1 - eps] is a little strange. I assume this is in order to avoid solutions at +- infinity, which is a well-known problem with the logistic loss. It may be more natural to simply state that you operate with the extended real numbers.\n\n- in the proof of Thm 3, I don't see the need to go through an infinite dimensional Lagrangian route. Since one is optimizing over all possible measurable functions, can one not (under suitable regularity conditions on the distribution & loss) simply compute the minimizer point wise for each x? This optimization would be a one-dimensional problem over predictions the domain [eps, 1 - eps]. The \"inner risk\" to be optimized (in the sense of Steinwart '06, \"How to compare different loss functions and their risks\") would I believe be a convex function, admitting exactly the minimizer claimed in the statement of the theorem.\n\n- it is a bit confusing to move from F to \\hat{F} as the function class.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Little novelty, experiments do not offer comparison with related work",
            "review": "The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.\n\nThe proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \\theta such that the number of datapoints x with r(x) > \\theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \\theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \\theta, and to port the original problem to a vanilla binary classification problem.\n\nConcerns:\n- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.\n- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?\n- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for \"unbiased PU learning (PU)\", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.\n- Related work discussion is completely missing apart from one paragraph in the introduction.\n\nMinor:\n- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.\n- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.\n- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.\n- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?\n- Theorem 3: add some intuition and explain tradeoff on \\epsilon\n- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, \"we trained a classifier minimizing (4) and (7) with the model (10)\" is difficult to digest and follow.\n- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}\n- Too many acronyms and abbreviations.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}