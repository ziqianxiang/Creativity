{
    "Decision": {
        "metareview": "The paper extends the results in Yarotsky (2017) from Sobolev spaces to Besov spaces, stating that once the target function lies in certain Besov spaces, there exists some deep neural networks with ReLU activation that approximate the target in the minimax optimal rates. Such adaptive networks can be found by empirical risk minimization, which however is not yet known to be found by SGDs etc. This gap is the key weakness of applying approximation theory to the study of constructive deep neural networks of certain approximation spaces, which lacks algorithmic guarantees. The gap is hoped to be filled in future studies. \n\nDespite the incompleteness of approximation theory, this paper is still a good solid work. Based on fact that the majority of reviewers suggest accept (6,8,6), with some concerns on the clarity, the paper is proposed as probable accept. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Approximation of Besov spaces by Deep ReLU neural networks. "
    },
    "Reviews": [
        {
            "title": "Nice and Relevant Results",
            "review": "Summary:\n========\nThe paper presents rates of convergence for estimating nonparametric functions in Besov\nspaces using deep NNs with ReLu activations. The authors show that deep Relu networks,\nunlike linear smoothers, can achieve minimax optimality. Moreover, they show that in a\nrestricted class of functions called mixed Besov spaces, there is significantly milder\ndependence on dimensionality. Even more interestingly, the Relu network is able to\nadapt to the smoothness of the problem.\n\nWhile I am not too well versed on the background material, my educated guess is that the\nresults are interesting and relevant, and that the analysis is technically sound.\n\n\n\nDetailed Comments:\n==================\n\n\nMy main criticism is that the total rate of convergence (estimation error + approximation\nerror) has not been presented in a transparent way. The estimation error takes the form\nof many similar results in nonparametric statistics, but the approximation error is\ngiven in terms of the parameters of the network, which depends opaquely on the dimension\nand other smoothness parameters. It is not clear which of these terms dominate, and\nconsequently, how the parameters W, L etc. should be chosen so as to balance them.\n\n\nWhile the mixed Besov spaces enables better bounds, the condition appears quite strong.\nIn fact, the lower bound is better than for traditional Holder/Sobolev classes. Can you\nplease comment on how th m-Besov space compares to Holder/Sobolev classes? Also, can\nyou similiarly define mixed Holder/Sobolev spaces where traditional linear smoothers\nmight achieve minimax optimal results?\n\n\nMinor:\n- Defn of Holder class: you can make this hold for integral beta if you define m to be\nthe smallest integer less than beta (e.g. beta=7, m=6). Imo, this is standard in most\ntexts I have seen.\n- The authors claim that the approximation error does not depend on the dimensionality\n  needs clarification, since N clearly depends on the dimension. If I understand\n  correctly, the approximation error is in fact becoming smaller with d for m-Besov\n  spaces (since N is increasing with d), and what the authors meant was that the\n  exponential dependnence on d has now been eliminated. Is this correct?\n\nOther\n- On page 4, what does the curly arrow notation mean?\n- Given the technical nature of the paper, the authors have done a good job with the\n  presentation. However, in some places the discussion is very equation driven. For e.g.\n  in the 2nd half of page 4, it might help to explain many of the quantities presented in\n  plain words.\n\n\n\nConfidence: I am reasonably familiar with the nonparametric regression literature, but\nnot very versed on the deep learning theory literature. I did not read the proofs in\ndetail.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Paper that establishes minimax optimal rates for deep network models over Besov spaces",
            "review": "This paper makes two contributions:\n* First, the authors show that function approximation over Besov spaces for the family of deep ReLU networks of a given architecture provide better approximation rates than linear models with the same number of parameters.\n* Second, for this family and this function class they show minimax optimal sample complexity rates for generalization error incurred by optimizing the empirical squared error loss.\n\nClarity: Very dense; could benefit from considerably more exposition.\n\nOriginality: afaik original. Techniques seem to be inspired by a recent paper by Montanelli and Du (2017).\n\nSignificance: unclear.\n\nPros and cons: \nThis is a theory paper that focuses solely on approximation properties of deep networks. Since there is no discussion of any learning procedure involved, I would suggest that the use of the phrase \"deep learning\" throughout the paper be revised.\n\nThe paper is dense and somewhat inaccessible. Presentation could be improved by adding more exposition and comparisons with existing results.\n\nThe generalization bounds in Section 4 are given for an ideal estimator which is probably impossible to compute.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Are piecewise linear estimators really minimax optimal for piecewise polynomial signals?",
            "review": "This paper describes approximation and estimation error bounds for functions in Besov spaces using estimators corresponding to deep ReLU networks. The general idea of connecting network parameters such as depth, width, and sparsity to classical function spaces is interesting and could lead to novel insights into how and why these networks work and under what settings. The authors carefully define Besov spaces and related literature, and overall the paper is clearly written. \n\nDespite these strengths, I'm left with several questions about the results. The most critical is this: piecewise polynomials are members of the Besov spaces of interest, and ReLU networks produce piecewise linear functions. How can piecewise linear approximations of piecewise polynomial functions lead to minimax optimal rates? The authors' analysis is based on cardinal B-spline approximations, which generally makes sense, but it seems like you would need more terms in a superposition of B-splines of order 2 (piecewise linear) than higher orders to approximate a piecewise polynomial to within a given accuracy. The larger number of terms should lead to worse estimation errors, which is contrary to the main result of the paper. I don't see how to reconcile these ideas. \n\nA second question is about the context of some broad claims, such as that the rates achieved by neural networks cannot be attained by any linear or nonadaptive method. Regarding linear methods, I agree with the author, but I feel like this aspect is given undue emphasis. The key paper cited for rates for linear methods is the Donoho and Johnstone Wavelet Shrinkage paper, in which they clearly show that nonlinear, nonadaptive wavelet shrinkage estimators do indeed achieve minimax rates (within a log factor) for Besov spaces. Given this, how should I interpret claims like \"any linear/non-linear approximator\nwith fixed N -bases does not achieve the approximation error ... in some parameter settings such as 0 < p < 2 < r \"?\nWavelets provide a fixed N-basis and achieve optimal rates for Besov spaces. Is the constraint on p and r a setting in which wavelet optimality breaks down? If not, then I don't think the claim is correct. If so, then it would be helpful to understand how relevant this regime for p and r is to practical settings (as opposed to being an edge case). \n\nThe work on mixed Besov spaces (e.g. tensor product space of 1-d Besov spaces) is a fine result but not surprising.\n\nA minor note: some of the references are strange, like citing a 2015 paper for minimax rates for Besov spaces that have been known for far longer or a 2003 paper that describes interpolation spaces that were beautifully described in DeVore '98. It would be appropriate to cite these earlier sources. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}