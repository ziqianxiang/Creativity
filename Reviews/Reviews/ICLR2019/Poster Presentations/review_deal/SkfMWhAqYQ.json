{
    "Decision": {
        "metareview": "This paper presents an approach that relies on DNNs and bags of features that are fed into them, towards object recognition.  The strength of the papers lie in the strong performance of these simple and interpretable models compared to more complex architectures.  The authors stress on the interpretability of the results that is indeed a strength of this paper.\n\nThere is plenty of discussion between the first reviewer and the authors regarding the novelty of the work as the former point out to several related papers;  however, the authors provide relatively convincing rebuttal of the concerns.\n\nOverall, after the long discussion, there is enough consensus for this paper to be accepted to the conference.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Combing Patch-level CNN and BoF model has been done before, but the paper has the smallest patch",
            "review": "The idea of image classification based on patch-level deep feature in the BoF model has been studied extensively.  \n\n Just list few of them:\n\nWei et al. HCP: A Flexible CNN Framework for Multi-label Image Classification, IEEE TPAMI 2016\nTang et al. Deep Patch Learning for Weakly Supervised Object Classification and Discovery, Pattern Recognition 2017\nTang et al. Deep FisherNet for Object Classification, IEEE TNNLS\nArandjelović et al. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition, CVPR 2016\n\nThe above papers are not cited in this paper.\n\nThere are some unique points. This work does not use RoIPooling layer and has results on ImageNet. But, the previous works use RoIPooling layer to save computations and works on scene understanding images, such as PASCAL. \n\nBesides, the paper uses the smallest patch among all the patch-based deep networks. It is interesting.\n\nI highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper is worth being accepted. The bag-of-words information in the neural network is important for high prediction accuracy. Possibly has high impact in the community and need to be further investigated.",
            "review": "This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features. The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches. The proposed method provides the state-of-the-art prediction accuracy unexpectedly, and several additional experiments show the state-of-the-art neural networks mainly learn without association between information in different patches.\n\nThe proposed algorithm is simple and does not provide completely new idea, but this paper has a clear contribution connecting the previous main idea of feature extraction, bag-of-words, and the prevailing blackbox algorithm, CNN. The results in the paper are worth to be shared in the community and need further investigated.\n\nThe presented experiments look fair and reasonable to show the importance of the independent patch information (without association between them), and the presented experimental results show some state-of-the-art methods also perform with independent patch information. \n\nComparison with attention models is necessary to compare the important patches obtained from conventional networks.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting empirical analysis",
            "review": "This is an experimental paper that investigates how spatial ordering of patches influences the classification performances of CNNs. To do so, the authors design CNNs close to ResNets that almost only consist in a simple cascade of 1x1 convolutions, obtaining relatively small receptive field. It is an interesting read, and I recommend it as a valuable contribution to ICLR, that might lead to nice future works.\n\nI have however several comments and questions, that I would like to be addressed.\n\n1) First I think a reference is missing. Indeed, to my knowledge, it is not the first work to use this kind of techniques. Cf [1]. This does not alterate however the novelty of the approach.\n\n2) « We construct a linear DNN-based BoF » : I do not like this formulation. Here, you assume that you build a ResNet-50 with 1x1 as a representation and have a last final linear layer as a classifier. One could also claim it is a ResNet-48 as a representation followed by 2 layers of 1x1 as a classifier.\n\n3) « our proposed model architecture is simpler » this is very subjective because for instance the FV models are learned in a layer-wise fashion, which makes their learning procedure more interpretable because each layer objective is specified. Furthermore, analyzing these models is now equivalent to analyze a cascade of fully connected layers, which is not simple at all.\n\n4) Again, the interpretability mentioned in Sec. 3  is in term of spatial localization, not mapping. I think it is important to make clear this consideration. Indeed, this work basically leaves the problem of understanding general CNNs to the problem of understanding MLPs.\n\n5) The graphic of the Appendix A is a bit misleading : it seems 13 downsampling are performed whereas it is not the case, because the first element of each group of block is actually only done once.(if I understood correctly)\n\n6) I think the word feature is sometimes mis-used: sometimes it seems it can refer to a patch, sometimes to the code for a patch. (« Surprisingly, feature sizes assmall as 17 × 17 pixels »)\n\nI got also few questions:\nQ1 : I was wondering if you did try manifold learning on the patches ? Do you expect it to work ?\nQ2 : Is there a batch normalization in the FC or a normalization? Did you try to threshold the heat maps before feeding them to the linear layer? I'm wondering indeed if the amplitude of those heatmaps is really key.\nQ3 : do you think it would be easy to exploit the non-overlapping patches for a better parallelization of computations ?\n\nFinally, I find very positive the amount of experiments to test the similarity with standard CNNs. Of course, it’s far from being a formal proof, but I think it is a very nice first step.\n\n[1] Oyallon, Edouard, Eugene Belilovsky, and Sergey Zagoruyko. \"Scaling the scattering transform: Deep hybrid networks.\" International Conference on Computer Vision (ICCV). 2017.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}