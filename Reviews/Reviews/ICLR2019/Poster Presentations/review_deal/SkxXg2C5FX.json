{
    "Decision": {
        "metareview": "This paper presents new generalized methods for representing sentences and measuring their similarities based on word vectors. More specifically, the paper presents Fuzzy Bag-of-Words (FBoW), a generalized approach to composing sentence embeddings by combining word embeddings with different degrees of membership, which generalize more commonly used average or max-pooled vector representations. In addition, the paper presents DynaMax, an unsupervised and non-parametric similarity measure that can dynamically extract and max-pool features from a sentence pair. \n\nPros:\nThe proposed methods are natural generalization of exiting average and max-pooled vectors. The proposed methods are elegant, simple, easy to implement, and demonstrate strong performance on STS tasks.\n\nCons:\nThe paper is solid, no significant con other than that the proposed methods are not groundbreaking innovations per say. \n\nVerdict:\nThe simplicity is what makes the proposed methods elegant. The empirical results are strong. The paper is worthy of acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "The simplicity is what makes the proposed methods elegant. The empirical results are strong."
    },
    "Reviews": [
        {
            "title": "Very polished paper, simple but effective.",
            "review": "This is one of the best papers I reviewed so far this year (ICLR, NIPS, ICML, AISTATS), in terms of both the writing and technical novelty.\n\nWriting: the author provided sufficient context and did comprehensive literature survey, which made the paper easily accessible to a larger audience. And the flow of this paper was very smooth and I personally enjoyed reading it.\n\nNovelty: I wouldn't say this paper proposed a groundbreaking innovation, however, compared to many other submissions that are more obscure rather than inspiring to the readers, this paper presented a very natural extension to something practitioners were already very familiar with: taking an average of word vectors for a sentence and measure by cosine similarity.  Both max pooling and Jaccard distance are not something new, but the author did a great job presenting the idea and proved it's effectiveness through extensive experiments. (disclaimer: I didn't follow the sentence embedding literature recently, and I would count on other reviewers to fact check the claimed novelty of this paper by the authors)\n\nSimplicity: besides the novelty mentioned above, what I enjoyed more about this paper is it's simplicity. Not just because it's easy to understand, but also it's easy to be reproduced by practitioners.\n\nQuibbles: the authors didn't provide error bar / confidence interval to the results presented in experiment session. I'd like to know whether the difference between baselines and proposed methods were significant or not.\n\nMiscellaneous: I have to say the authors provided a very eye-catching name to this paper as well, and the content of the paper didn't disappoint me neither. Well done :)\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and simple idea",
            "review": "This submission presents a simple model for sentence representation based on max-pooling of word vectors. The model is motivated by fuzzy-set theory, providing both a funded pooling scheme and a similarity score between documents. The proposed approach is evaluated on sentence similarity tasks (STS) and achieves very strong performance, comparable to state-of-the-art, computationally demanding methods.\n\nPros:\n+ The problem tackled by this paper is interesting and well motivated. Fast, efficient and non-parametric sentence similarity has tons of important applications (search, indexing, corpus mining).\n+ The proposed solution is elegant and very simple to implement. \n+ When compared to standard sentence representation models, the proposed approach has very good performance, while being very efficient. It only requires a matrix vector product and a dimension-wise max.\n+ The paper is very well written and flows nicely.\n+ Empirical results show significant differences between different word vectors. The simplicity of this approach makes it a good test bed for research on word vectors.\n\nCons:\n- Nothing much, really. \n- Eq. (3) is awkward, as it is a sequence of equalities, which has to be avoided. Moreover, if U is the identity, I don't think that the reader really need this Eq...\n\nI have several questions and remarks that, if answered would make the quality of the presentation better:\n\n* In infersent, the authors reported the performance of a randomly-initialized and max-pooled bi-lstm with fasttext vectors as the input lookup. This can be seen as an extreme case of the presented formalism, where the linear operator U is replaced by a complicated non linear function that is implemented by the random LSTM. Drawing that link, and maybe including this baseline in the results would be good.\n\n* Related to this previous question, several choices for U are discussed in the paper. However, only two are compared in the experiments. It would be interesting to have an experimental comparison of:\n- taking U = W\n- taking U = I\n- taking U as the principal directions of W\n- taking U as a random matrix, and comparing performance for different output dimensions. \n\nOverall, this paper is a very strong baseline paper. The presented model is elegant and efficient. I rate it as an 8 and await other reviews and the author's response.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents (a) a fuzzy bag of word representation and (b) DynaMax, a similarity measure that max pools salient features from sentence pairs.",
            "review": "Strengths:\n- Good coverage of related work\n- Clear presentation of the methods\n- Evaluation using established SemEval datasets\n\n\nWeaknesses:\n1. It is not entirely clear what is the connection between fuzzy bag of words and DynaMax. In principle DynaMax can work with other methods too. This point should be elaborated a bit more.\n2. It is claimed that the this paper shows that max-pooled word vectors are a special case of fuzzy bag of words. This is not correct. The paper shows how to \"convert\" one to the other. \n3. It is also claimed that point 2 above implies that max-pooled vectors should be compared with the fuzzy Jaccard index instead of cosine similarity. There is no proof or substantial justification to support this. \n4. Some relevant work that is missing:\n- De Boom, C., Van Canneyt, S., Demeester, T., Dhoedt, B.: Representation learning for very\nshort texts using weighted word embedding aggregation. Pattern Recognition Letters 80,\n150–156 (2016)\n- Kenter, T., De Rijke, M.: Short text similarity with word embeddings. In: International on\nConference on Information and Knowledge Management. pp. 1411–1420. ACM (2015)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}