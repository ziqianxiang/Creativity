{
    "Decision": {
        "metareview": "This paper generated a lot of discussion (not all of it visible to the authors or the public). \n\nR1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection. \n\nAfter an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Limited technical merit and significance",
            "review": "This paper applies IRL to the cases of multiple tasks/environments and multimodal input features involving natural language (text) and vision (images). It is interesting to see the better performance of their proposed approaches with language-conditioned rewards over language-conditioned policies. The paper is written well.\n\nI view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.\n\nConsidering the use of deep learning that can handle highly complex images and text, the practical significance of this work can be considerably improved by grounding their work in real-world context and/or larger-scale environments/tasks, as opposed to simulated environments in this paper. See, for example,\n\nM. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, Large-scale cost function learning for path planning using deep inverse reinforcement learning, The International Journal of Robotics Research, 2017. \n\nThe authors say that \"The work of MacGlashan et al. (2015) requires an extensively hand-designed, symbolic reward function class, whereas we use generic, differentiable function approximators that can handle arbitrary observations, including raw images.\" What then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?\n\n\n\nMinor issues\n\nPage 2: a comparison to in Section 6 to as an oracle?\nPage 3: What is rho_0?\nPage 7: In order compare against?\nPage 7: and and indicator on?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper, but results are not very convincing.",
            "review": "Paper Summary: This paper studies the inverse reinforcement learning problem for language-based navigation. Given the panorama image as its observation, language embedding as its goal, a deep neural network architecture is proposed to predict the reward function from the input observation and goal. Maximum causal entropy IRL has been adopted to learn such language-conditioned reward function. This paper used the SUNCG environment for experiments and designed two tasks (navigation and pick-and-place) for evaluation.\n\n==\nNovelty & Significance:\nThis paper studies a very interesting topic in reinforcement learning and the problem has potential usage when training robot agent in the real world.\n\n==\nQuality:\nOverall, reviewer feels that the experimental results are not very strong. Some of the points are not clearly presented. \n\nFirstly, is not very clear whether the argument made in the abstract “directly learning a language-conditioned policy leads to a poor performance” is justified or not. Please clarify this point in the rebuttal.\n\nSecondly, Table 1 and Table 2 can only be treated as ablation studies. The “reward-regression” is not a baseline but more about a oracle model. \nIs it possible to compare against some recent work such as Tung et al 2018 or Bahdanau et al 2018? Otherwise, it is not very clear whether the proposed approach is the state-of-the-art or not.\n\nThirdly, using the panorama image as observation seems not a practical choice. Is it possible to provide some ablation studies or discussions on the performance over number of views? \n\nFinally, the architecture design is not well-justified. Why not using pre-trained image classifiers (or DQN encoder) as feature extractor (or finetune the model from pre-trained image classifier)? The actual resolution (32 x 24 x 3) in the paper looks a bit unusual. \n\nOne more thing, the url provided in the paper directs to an empty project page. \n\nIf these concerns can be addressed in the rebuttal, reviewer is happy to re-evaluate (e.g., raise scores) this work.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comparison to Imitation Learning (not just naive BC)!",
            "review": "Summary:\n\nThis paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like \"go to the cup\". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). \n\nThe really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. \n\nIn order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. \n\nExperiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. \n\nComments and Questions:\n\n- The paper is generally well-written and easy to understand. Thanks!\n\n- The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning.\n\n- One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in \"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. \n\n- The following papers are relevant and should be cited and discussed:\n\"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\" by Anderson et al, CVPR 2018.\n\n\"Embodied Question Answering\", Das et al, CVPR 2018.\n\nUpdate:\n------------\nAfter looking at other reviews and author rebuttals to all reviews I am raising my grade. \n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}