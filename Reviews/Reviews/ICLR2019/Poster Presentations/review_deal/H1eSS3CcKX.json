{
    "Decision": {
        "metareview": "This paper proposes a general-purpose continuous relaxation of the output of the sorting operator. This enables end-to-end training to enable more efficient stochastic optimization over the combinatorially large space of permutations.\n\nIn the submitted versions, two of the reviewers had difficulty in understanding the writing. After the rebuttal and the revised version, one of the reviewers is satisfied. I personally went through the paper and found that it could be tricky to read certain parts of the paper. For example, I am personally very familiar with the Placket-Luce model but the writing in Section 2.1 does not do a good job in explaining the model (particularly Eq 1 is not very easy to read, same with Eq. 3 for the key identity used in the paper). \n\nI encourage authors to improve writing and make it a bit more intuitive to read.\n\nOverall, this is a good paper and I recommend to accept it.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Good paper, but writing can be improved."
    },
    "Reviews": [
        {
            "title": "An improvement to relaxed sort operators; some even-harder experiments",
            "review": "This work builds on a sum(top k) identity to derive a pathwise differentiable sampler of 'unimodal row stochastic' matrices. The Plackett-Luce family has a tractable density (an improvement over previous works) and is (as developed here) efficient to sample. \n\n[OpenReview did not save my draft, so I now attempt to recover it from memory.]\n\nQuestions:\n- How much of the improvement is attributable to the lower dimension of the parameterization? (e.g. all Sinkhorn varients have N^2 params; this has N params) Is there any reduction in gradient variance due to using fewer gumbel samples?\n- More details needed on the kNN loss (uniform vs inv distance wt? which one?); and the experiment overall: what k got used in the end?\n- The temperature setting is basically a bias-variance tradeoff (see Fig 5). How non-discrete are the permutation-like matrices ultimately used in the experiments? While the gradients are unbiased for the relaxed sort operator, they are still biased if our final model is a true sort. Would be nice to quantify this difference, or at least mention it.\n\nQuality:\nGood quality; approach is well-founded and more efficient than extant solutions. Fairly detailed summaries of experiments in appendices (except kNN). Neat way to reduce the parameter count from N^2 to N.\n\nI have not thoroughly evaluated the proofs in appendix.\n\nClarity:\nThe approach is presented well, existing techniques are compared in both prose and as baselines. Appendix provides code for maximal clarity. \n\nOriginality:\nFirst approach I've seen that reduces parameter count for permutation matrices like this. And with tractable density. Very neat and original approach.\n\nSignificance:\nMore scalable than existing approaches (e.g: only need N gumbel samples instead of N^2), yields better results.\n\nI look forward to seeing this integrated into future work, as envisioned (e.g. beam search)",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice results ",
            "review": "After responses: I now understand the paper, and I believe it is a good contribution. \n\n================================================\n\nAt a high level, the paper considers how to sort a number of items without explicitly necessarily learning their actual meanings or values. Permutations are discrete combinatorial objects, so the paper proposes a method to perform the optimization via a continuous relaxation. \n\nThis is an important problem to sort items, arising in a variety of applications, particularly when the direct sorting can be more efficient than the two step approach of computing the values and then sorting.\n\nI like both the theoretical parts and the experimental results. In the context of ICLR, the specific theoretical modules comprise some cute results (Theorem 4; use of past works in Lemma 2 and Proposition 5). possibly of independent interest. The connections to the (Gumbel distribution <--> Plackett Luce) results are also nicely used. This Gumbel<-->PL result is well known in the social choice community but perhaps not so much in the ML community, and it is always nice to see more connections drawn between techniques in different communities. The empirical evaluations show quite good results.\n\nHowever, I had a hard time parsing the paper. The paper is written in a manner that may be accessible to readers who are familiar with this (or similar) line of research, but for someone like me who is not, I found it quite hard to understand the arguments (or lack of them) made in the paper connecting various modules. Here are some examples:\n\n- Section 6.1 states \"Each sequence contains n images, and each image corresponds to an integer label. Our goal is to learn to predict the permutation that sorts these labels\". One interpretation of this statement suggests that each row of Fig 3a is a sequence, that each sequence contains n=4 images (e.g., 4 images corresponding to each digit in 2960), and the goal is to sort [2960] to [0269]. However, according to the response of authors to my earlier comment, the goal is to sort [2960,1270,9803] to [1270,2960,9803]. \n\n- I did not understand Section 2.2.\n\n- I would appreciate a more detailed background on the concrete goal before going into the techniques of section 3 and 4.\n\n- I am having a hard time in connecting the experiments in Section 6 with the theory described in earlier sections. And this is so even after my clarifying questions to the authors and their responses. For instance, the authors explained that the experiments in Section 6.1 have \\theta as vacuous and that the function f represents the cross-entropy loss between permutation z and the true permutation matrix. Then where is this true permutation matrix captured as an argument of f in (6)? Is the optimisation/gradients in (7) over s or over the CNN parameters?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical results, but connection to the experimental results is not clear",
            "review": "In many machine learning applications, sorting is an important step such as ranking. However, the sorting operator is not differentiable with respect to its inputs. The main idea of the paper is to introduce a continuous relaxation of the sorting operator in order to construct an end-to-end gradient-based optimization. This relaxation is introduced as \\hat{P}_{sort(s)} (see Equation 4). The paper also introduces a stochastic extension of its method \nusing Placket-Luce distributions and Monte Carlo. Finally, the introduced deterministic and stochastic methods are evaluated experimentally in 3 different applications: 1. sorting handwritten numbers, 2. Quantile regression, and 3. End-to-end differentiable k-Nearest Neighbors.\n\nThe introduction of the differentiable approximation of the sorting operator is interesting and seems novel. However, the paper is not well-written and it is hard to follow the paper especially form Section 4 and on. It is not clear how the theoretical results in Section 3 and 4 are used for the experiments in Section 6. For instance:\n** In page 4, what is \"s\" in the machine learning application?\n** In page 4, in Equation 6, what are theta, s, L and f exactly in our machine learning applications?\n\nRemark: \n** The phrase \"Sorting Networks\" in the title of the paper is confusing. This term typically refers to a network of comparators applied to a set of N wires (See e.g. [1])\n** Page 2 -- Section 2 PRELIMINARIES -- It seems that sort(s) must be [1,4,2,3].\n\n[1] Ajtai M, Komlós J, Szemerédi E. An 0 (n log n) sorting network. InProceedings of the fifteenth annual ACM symposium on Theory of computing 1983 Dec 1 (pp. 1-9). ACM\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}