{
    "Decision": {
        "metareview": "This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works.\n\nR1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3.\n\nBased on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id=ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre-trained VAE model.\n\nThere are still two weaknesses pointed out by R3 that would help improve the paper by addressing them:\n1. The paper does not handle different kinds of missingness beyond missing at random.\n2. VAE model makes the trade-off between computational complexity and accuracy.\nPoint 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "Variational Autoencoder with Arbitrary Conditioning"
    },
    "Reviews": [
        {
            "title": "Solid work on using VAEs for feature imputation",
            "review": "This paper introduces the VAEAC model, inspired by CVAEs, it allows conditioning on any subset\nof the latent features. This provides a model able to achieve good results on image inpainting\nand feature imputation tasks.\n\nThe paper appears to be technically sound, and the experiments are\nthoughtfully designed. The writing is clear and the model is easy to\nunderstand. The closest work to this of the Universal Marginalizer is\ncompared to well, with more compelling examples in the appendix. I\nwould have preferred if more of the experimental results were in the\nmain paper instead of in the appendix especially as the authors state\nthey chose to highlight their better results in the main paper.\n\nWhile not the first model to try to handle modeling data with missing features, it is\nstill a fairly original and elegant formulation.\n\nMinor details:\n\nIn equation (8) should x be x_b?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting paper; missing/weak experiments",
            "review": "The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. The idea is to extend the conditional VAE framework such that the posterior is a function of an arbitrary subset of observed variables. Accordingly, reconstruction loss only penalizes the error in the reconstruction of masked (unobserved) variables. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture.\n\nMy concern about the experimental results on missing data imputation is that strong competition such as Gondra et al’17 and Yoon et al’18 that report better results on UCI than classical approaches are not included. Could you please comment? See also [1,2] for other autoencoding architectures for this task.\n\nWhile the derivation of the method is principled, it assumes that either the mask is known during the training OR one could efficiently sample a distribution of masks to learn arbitrary conditional densities. Given the exponential number of valid masks in a general setting, one only subsamples a small portion during the training. The question is whether the model can generalize well in this regime? The experimental results in this setting is not very encouraging, suggesting the proposed approach is effective only when the limitted mask patterns are known in advance. \n\n[1] Gondara, Lovedeep, and Ke Wang. \"Multiple imputation using deep denoising autoencoders.\" arXiv preprint arXiv:1705.02737 (2017).\n\n[2] Zhang, Hongbao, Pengtao Xie, and Eric Xing. \"Missing Value Imputation Based on Deep Generative Models.\" arXiv preprint arXiv:1808.01684 (2018).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Idea: Train a VAE to maximize the likelihood of a subset of the data while using the other subset for posterior inference. Poorly written paper with some interesting qualitative results.",
            "review": "The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of \"missing\" (to the generative model) features.\n\nNovelty:\nGenerative models have a long history of being used to impute missing data. e.g. http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2011.pdf, https://arxiv.org/pdf/1610.04167.pdf,\nhttps://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F]\nIt is a little difficult to guage what the novelty of this work is.\n\nClarity\nThis is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use \"p\").\n\nExperimental Results\nThe model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot?\nBeyond that:\n(a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?\n(b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?\n\nThe qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned).\n\nTable 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?\n\n\nPriors on b:\nWhat kind of priors on b did you experiment with? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}