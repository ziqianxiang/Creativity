{
    "Decision": {
        "metareview": "The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.  The results are convincing, and the reviewers are convinced.\n\nIt's unfortunate however that the method is only evaluated on simulated data.  Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "solid work, would merit from more experimentation"
    },
    "Reviews": [
        {
            "title": "simple yet effective",
            "review": "The authors present a method on prediction of frames in a video, with the key contribution being that the target prediction is floating, resolved by a minimum on the error of prediction. The authors show the merits of the approach on a synthetic benchmark of object manipulation with a robotic arm. \n\nQuality: this paper appears to contain a lot of work, and in general is of high quality. \n\nClarity: some sections of the paper were harder to digest, but overall the quality of the writing is good and the authors have made efforts to present examples and diagrams where appropriate. Fig 1, especially helps one get a quick understanding of the concept of a `bottleneck` state. \n\nOriginality: To the extent of my knowledge, this work is novel. It proposes a new loss function, which is an interesting direction to explore.\n\nSignificance: I would say this work is significant. There appears to be a significant improvement in the visual quality of predictions. In most cases, the L1 error metric does not show such a huge improvement, but the visual difference is remarkable, so this goes to show that the L1 metric is perhaps not good enough at this point. \n\nOverall, I think this work is significant and I would recommend its acceptance for publication at ICLR. There are some drawbacks, but I don’t think they are major or would justify rejection (see comments below). \n\n\nI’m curious as to why you called the method in section 3.2 the “Generalized minimum”? It feels more like a weighted (or preference weighted) minimum to me and confused me a few times as I was reading the paper (GENerative? GENeralized? what’s general about it?). Just a comment.\n\nWhat results does figure 4 present? Are they only for the grasping sequence? Please specify.  \n\nIn connection with the previous comment, I think the results would be more readable if the match-steps were normalized for each sequence (at least for Figure 4). There would be a clearer mapping between fixT methods and the normalized matching step (e.g., we would expect fix0.75 to achieve a matching step of 0.75 instead of 6 / ? ).\n\nSection 4, Intermediate prediction. The statement “the genmin w(t) preference is bell-shaped” is vague. Do you mean a Gaussian? If so, you should say “a Gaussian centered at T/2 and tuned so that …”\n\nSection 4, Bottleneck discovery frequency. I am not entirely convinced by the measuring of bottleneck states. You say that a distance is computed between the predicted object position and the ground-truth object position. If a model were to output exactly the same frame as given in context, would the distance be zero? If so, doesn’t that mean that a model who predicts a non-bottleneck state before or after the robotic arm moves the pieces is estimated to have a very good bottleneck prediction frequency? I found this part of the paper the hardest to follow and the least convincing. Perhaps some intermediate results could help prospective readers understand better and be convinced of the protocol’s merits.\n\n\n\nTypos:\n\nAppendix E, 2nd paragraph, first sentence: “... generate an bidirectional state” --> “generate A bidirectional state” \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting proposal and well-written method. Experiments section is though poorly structured",
            "review": "Revision\n----------\nThanks for taking the comments on board. I like the paper, before and after, and so do the other reviewers. Some video results might prove more valuable to follow than the tiny figures in the paper and supplementary. Adding notes on limitations is helpful to understand future extensions.\n\n-----------------------------------------------\nInitial Feedback:\n---------------------\nThis is a very exciting proposal that deviates from the typical assumption that all future frames can be predicted with the same certainty. Instead, motivated by the benefits of discovering bottlenecks for hierarchical RL, this work attempts to predict ‘predictable video frames’ – those that can be predicted with certainty, through minimising over all future frames (in forward prediction) or all the sequence (in bidirectional prediction). Additional, the paper tops this with a variational autoencoder to encode uncertainty, even within those predictable frames, as well as a GAN for pixel-level generation of future frames. \n\nThe first few pages of the paper are a joy to read and convincing by default without looking at experimental evidence. I do not work myself in video prediction, but having read in the area I believe the proposal is very novel and could make a significant shift in how prediction is currently perceived. It is a paper that is easy to recommend for publication based on the formulation novelty, topped with VAEs and GANs as/when needed.\n\nBeyond the method’s explanation, I found the experiment section to be poorly structured. The figures are small and difficult to follow – looking at all the figures it felt that “more is actually less”. Many of the evidence required to understand the method are only included in the appendices. However, having spent the time to go back and forth, I believe the experiments to be scientifically sound and convincing.\n\nI would have liked a discussion in the conclusion on the method’s limitation. This reviewer believes that the approach will struggle to deal with cyclic motions. In this case the discovered bottlenecks might not be the most useful to predict, as these will correspond to future frames (not nearby though) that are visually similar to the start (in forward) or to the start/end (in bidirectional) frames. An additional loss to reward difficult-to-predict frames (though certain compared to other times) might be an interesting additional to conquer more realistic (rather than synthetic) video sequences.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good novel contribution",
            "review": "Summary:\nThe paper reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead it is trained to generate frames that happen at any point in the future. The motivation for such approach is that there might be future states that are highly uncertain – and thus, difficult to predict – that might not be useful for other tasks involving video prediction such as planning. The authors derive different loss functions for such Time-Agnostic Prediction (TAP), including extensions to the Variational AutoEncoders (VAE) and Generative Adversarial Networks (GAN) frameworks, and conduct experiments that suggest that the frames predicted by TAP models correspond to ‘subgoal’ states useful for planning.\n\nStrenghts:\n[+] The idea of TAP is novel and intuitively makes sense. \nIt is clear that there are frames in video prediction that might not be interesting/useful yet are difficult to predict, TAP allows to skip such frames.\n[+] The formulation of the TAP losses is clear and well justified. \nThe authors do a good job at showing a first version of a TAP loss, generalizing it to express preferences, and its extension to VAE and GAN models, showing that \n\nWeaknesses:\n[-] The claim that the model discovers meaningful planning subgoals might be overstated. \nThe hierarchical planning evaluation experiment seems like it would clearly favor TAP compared to a fixed model (why would the middle prediction in time of the fixed model correspond to reasonable planning goals?). Furthermore, for certain tasks and environments it seems like the uncertain frames might be the ones that correspond to important subgoals. For example, for the BAIR Push Dataset, usually the harder frames to predict are the arm-object interactions, which probably would correspond to the relevant subgoals.\n\nOverall I believe that the idea in this paper is a meaningful novel contribution. The paper is well-written and the experiments support the fact that TAP might be a better choice for training frame predictors for certain tasks.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}