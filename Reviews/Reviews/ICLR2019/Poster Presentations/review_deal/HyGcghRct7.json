{
    "Decision": {
        "metareview": "This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise-constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion. \nTwo of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity. \nAt the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper.\nThe authors also have significantly improved the clarity of the manuscript throughout the discussion period.\nIt would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. “Deep Component Analysis via Alternating Direction Neural Networks\n” of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning  a feedforward mapping. \n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Accept (Poster)",
        "title": "interesting direction for inverse problems"
    },
    "Reviews": [
        {
            "title": "Interesting method, but limited demonstrations and unclear reason for working",
            "review": "Summary:\nGiven an inverse problem, we want to infer (x) s.t. Ax = y, but in situations where the number of observations are very sparse, and do not enable direct inversion. The paper tackles scenarios where 'x' is of the form of an image. The proposed approach is a learning based one which trains CNNs to infer x given y (actually an initial least square solution x_init is used instead of y).\n\nThe key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. The desired x is then optimized for given the predicted predicted projections.\n\nPros:\n- The proposed approach is interesting and novel - I've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored)\n- The presented results are quantitatively and qualitatively better compared to a direct prediction baseline\n- The paper is generally well written, and interesting to read\n\nCons:\nWhile the method is interesting, it is apriori unclear why this works, and why this has been only explored in context of linear inverse problems if it really does work.\n\n- Regarding limited demonstration: The central idea presented here is is generally applicable to any per-pixel regression task. Given this, I am not sure why this paper only explores it in the particular case of linear inversion and not other general tasks (e.g. depth prediction from a single image). Is there some limitation which would prevent such applications? If yes, a discussion would help. If not, it would be convincing to see such applications.\n\n- Regarding why it works: While learning a single projection maybe more sample efficient, learning all of them s.t. the obtained x is accurate may not be. Given this, I'm not entirely sure why the proposed approach is supposed to work. One hypothesis is that the different learned CNNs that each predict a piecewise projection are implicitly yielding an ensembling effect, and therefore a more fair baseline to compare would be a 'direct-ensemble' where many different (number = number of projections) direct CNNs (with different seeds etc.) are trained, and their predictions ensembled.\n\n\nOverall, while the paper is interesting to read and shows some nice results in a particular domain, it is unclear why the proposed approach should work in general and whether it is simply implicitly similar to an ensemble of predictors.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "novel method for inverse problems",
            "review": "This paper proposes a novel method of solving ill-posed inverse problems and specifically focuses on geophysical imaging and remote sensing where high-res samples are rare and expensive. \nThe motivation is that previous inversion methods are often not stable since the problem is highly under-determined.  To  alleviate these problems, this paper proposes a novel idea: \ninstead of fully reconstructing in the original space, the authors create reconstructions in projected spaces. \nThe projected spaces they use have very low dimensions so the corresponding Lipschitz constant is small. \nThe specific low-dimensional reconstructions they obtain are piecewise constant images on random Delaunay trinagulations. This is theoretically motivated by classical work (Omohundro'89) and has the further advantage that the low-res reconstructions are interpretable. One can visually see how closely they capture the large shapes of the unknown image. \n\nThese low-dimensional reconstructions are subsequently combined in the second stage of the proposed algorithm, to get a high-resolution reconstruction. The important aspect is that the piecewise linear reconstructions are now treated as measurments which however are local in the pixel-space and hence lead to more stable reconstructions. \n\nThe problem of reconstruction from these piecewise constant projections is of independent interest. Improving this second stage of their algorithm, the authors would get a better result overall. For example I would recommend using Deep Image prior as an alternative technique of reconstructing a high-res image from multiple piecewise constant images, but this can be future work. \n\nOverall I like this paper. It contains a truly novel idea for an architecture in solving inverse problems. The two steps can be individually improved but the idea of separation is quite interesting and novel. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear why this should work",
            "review": "This paper describes a novel method for solving inverse problems in imaging.\n\nThe basic idea of this approach is use the following steps:\n1. initialize with nonnegative least squares solution to inverse problem (x0)\n2. compute m different projections of x0\n3. estimate x from the m different projections by solving \"reformuated\" inverse problem using TV regularization.\n\nThe learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a \"good\" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be.\n\nMore specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. \n\nEmpirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem.\n\nThe core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the \"inverse-ness\" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned.\n\nAt the heart of this paper is the idea that for an L-Lipschitz function f : R^k → R the sample complexity\nis O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided.\nThis isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we\nreally get the benefits.\n\nThe authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares.\n\nPractically I'm quite concerned about their method requiring training 130 separate convolutional neural\nnets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is\nit possible that any network at all would be okay? Can we just reconstruct the image from regression\non 130 randomly-initialized convolutional networks? \n\nThe proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}