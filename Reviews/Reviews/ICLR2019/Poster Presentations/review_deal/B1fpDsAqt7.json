{
    "Decision": {
        "metareview": "Important problem (modular & interpretable approaches for VQA and visual reasoning); well-written manuscript, sensible approach. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "very interesting work, but a lot of the details are not clear. ",
            "review": "[Summary]\nThis paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality.\n\n[Strength]\n1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.  This is different from most existing work. \n\n2: By examing different modules, the proposed method is more interpretable compare to canonical methods. \n\n3: The experiment results are good, especially for the counting problem. \n\n[Weakness] \n1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).  \n\n2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). \" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation? From the supplementary, it seems Epsilon means the environment? \n\n3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? \n\n4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. \n\n5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. \n\n6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? \n\n7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposes to combine different task-level modules in a progressive way for the task for VQA. The model achieved state-of-the-art performance.",
            "review": "The paper proposes to learn task-level modules progressively to perform the task of VQA. Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model. The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better. The results are mainly shown on VQA 2.0 set, with a good amount of analysis.\n\n- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization. I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection. I do not have major comments about the paper itself, although I did not check the technical details super carefully.\n\n- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component. \n\n- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)? \n\n- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules. For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships? This can help us not only better understand the models, but also the dataset (VQA) and the task in general. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official review.",
            "review": "Summary:\nThe authors propose a network for VQA incorporating hand-crafted modules and their hierarchy, each of which is a network for a high-level vision task. Some modules may share the same sub-modules at a different level in the module hierarchy. Each module is individually (not end-to-end) trained with a dataset containing a dedicated annotation for their high-level tasks. The proposed model shows comparable scores to the existing models.\n\nPresentation and clarity:\nThe paper is well written and easy to follow and contains reasonable experiments for understanding the proposed method.\n\nOriginality and significance:\nI mainly do not agree that this work generalizes NMN. Instead, I believe that this work is a special case of NMN where the modules and their hierarchy are manually defined based on the authors' intuition. Meanwhile, the proposed network architecture is static, and thus the main idea of having multiple modules in a network is not novel as other approaches using static network architectures such as [A] also facilitate multiple modules for different sub-procedures (e.g., RNN for questions and CNN for image) and sometimes share modules in multiple stages too. The main difference between this and previous works is that the modules in this work deal with high-level tasks chosen by the authors. I am not convinced that designing the modules with high-level tasks is a better choice over designing modules that are less task-specific. Rather, I see more drawbacks as the proposed method requires multiple datasets with diverse task-specific annotation. Also, the modules and their connectivity are less scalable and extendable as they are not learned.\n\nConsidering all the model and dataset complexities, the improvements over black-box models are mostly marginal. The main benefits we get from all these complexities are the interpretability. However, for many modules, the interpretability comes from indirect signals that are often not clear how to interpret for the question answering. On the other hand, the manually designed sub-tasks may cause error propagation in the network as these modules are not directly optimized for the final objective.\n\nSome questions and comments:\nI do not understand why it is necessary to have the image captioning module as it does not directly relate to the question answering. Moreover, the caption itself is generated without conditioning on the question.\n\n[A] Yang, Zichao, et al. \"Stacked attention networks for image question answering.\" CVPR 2016.\n\n\n== After discussion phase\nBased on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}