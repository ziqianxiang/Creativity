{
    "Decision": {
        "metareview": "This paper proposes several improvements for the MAML algorithm that improve its stability and performance.\nStrengths: The improvements are useful for future researchers building upon the MAML algorithm. The results demonstrate a significant improvement over MAML. The authors revised the paper to address concerns about overstatements \nWeaknesses: The paper does not present a major conceptual advance. It would also be very helpful to present a more careful ablation study of the six individual techniques.\nOverall, the significance of the results outweights the weaknesses. However, the authors are strongly encouraged to perform and include a more detailed ablation study in the final paper. I recommend accept.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "A paper with marginal novelty over an established framework.",
            "review": "[Summary]\nThis work presents several enhancements to the established Model-Agnostic Meta-Learning (MAML) framework. Specifically, the paper starts by analyzing the issues in the original implementations of MAML, including instability during training, costly second order derivatives evaluation, missing/shared batch normalization statistics accumulation/bias, and learning rate setting, which causes unstable or slow convergence, and weak generalization. The paper then proposes solutions corresponding to each of these issues, and reports improved performance on benchmark datasets.          \n\nPros\nGood technical enhancements that fix some issues of a popular meta-learning framework\nCons\nLittle conceptual and technical novelty \n\n[Originality]\nThe major problem I found in this work is the lack of conceptual and technical novelty. The paper basically picks up some issues of the well-established MAML framework, and applies some common practices or off-the-shelf technical treatments to fix these drawbacks and improve the training stability, convergence, or generalization, etc. E.g., it seems to me that the most effective enhancement comes from the use of adoption of learning rate setting (LSLR), or variant version of batch normalization (BNWB+BNRS) in Table 1, which have been the standard tricks to improve performance in the deep learning literature. Overall, the conceptual originality is little.         \n\n[Quality]\nThe paper does get most things well executed from the technical point of view. There does not seem any major errors to me. The results reported are also reasonable within the meta-learning context, despite lack of originality.  \n\n[Clarity]\nThe paper is generally well written and I did not have much difficulty to follow. \n\n[Significance]\nThe significance of this work is marginal, given the lack of originality. The technical enhancements presented in the paper, however, may be of interest to people working in this area. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In-depth discussions and improvements on MAML",
            "review": "In the work, the authors improve a simple yet effective meta-learning algorithm called Model Agnostic meta-learning (MAML) from various aspects including training instability, batch normalization etc. The authors firstly point out the issues in MAML training and tackle each of the issue with a practical alternative approach respectfully. The few-shot classification results show convincing evidence.\n\nSome major concerns:\n1. The paper is too specific about improving one algorithm, the scope of the research is quite narrow and I'm afraid that some of the observations and proposed solutions might not generalize into other algorithms;\n2. Section 4, \"Gradient Instability → Multi-Step Loss Optimization.\" I don't see clearly why the multi-step loss would lead to stable gradients. It causes much more gradient paths than the original version. I do see the point of weighting the losses from different step;\n3. The authors should have conducted careful ablation study of each of the issues and solutions. The six ways of proposed improvements may make the the performance boost hard to understand. It would help to see which way of the proposed improvement contribute more than others;\n4. Many of the proposed improvements are essentially utilizing annealing mechanisms to stabilize the training, including 1) anneals the weighting of the losses from different step; 2) anneal the second derivative  to the first derivative;\n5. For the last two improvements about the learning rate, there are dozens of literature on meta-learning learning rate and the proposed approach does not seem to be novel;  \n \nMinors\n1. The reference style is inconsistent across the paper, sometimes it feels quite messy. For example, \"Batch Stochastic Gradient Descent Krizhevsky et al. (2012)\" \"Another notable advancement was the gradient-conditional meta-learner LSTM Ravi & Larochelle (2016)\";\n2. Equation (2) (3) the index b should start from 1, size of B should be 1 to B;\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Improving MAML",
            "review": "Paper summary - This paper provides a bag of sensible tricks for making MAML more stable, faster to learn, and better in final performance.\nQuality - The quality of the work is strong: the results demonstrate that tweaks to MAML produce significant improvements in performance. However, I have some concern that certain portions of the text overclaim (see concerns section below).\nClarity - The paper is reasonably clear, with some exceptions (see concerns section).\nOriginality - The techniques described in the paper range from only mildly novel (e.g. MSL, DA), to very obvious (e.g. CA). Additionally, the paper's contributions amount to tweaks to a previously existing algorithm. \nSignificance - The quality of the results make this a significant contribution in my view.\nPros - Good results on a problem/algorithm of great current interest.\nCons - Only presents (in some cases obvious) tweaks to a previous algorithm; clarity and overclaiming issues in the writeup.\n\nConcerns (please address in author response)\n- The paper says  \"we … propose multiple ways to automate most of the hyperparameter searching required\". I'm not sure that this is true. The only technique that arguably removes a hyperparameter is LSLR. Even in this case, you still have to initialize the inner loop learning rates, so I'm not convinced that even this reduces hyperparameters. Perhaps I've missed something, please clarify.\n- Section 4's paragraph on LSLR seems to say that you have a single alpha for each layer of the network. If this is right, then saying your method has a \"per layer gradient direction\" is very confusing. Each layer's alpha modulates the magnitude of that layer's update vector, but not its direction. The per-layer alphas together modify the direction of the global update vector. Perhaps I've misunderstood; equations describing exactly what LSLR does would be helpful. In any case, this should be clarified in the text.\n\nSuggestions (less essential than the concerns above)\n- The write-up is redundant and carries unnecessary content. The paper would be better shorter (8 pages is not a minimum :)\nSection 1 covers a lot of background on the basics of meta-learning background that could be skipped. Other papers you cite (e.g. the MAML paper cover this). \n    - Section 2 goes into more detail about e.g. matching nets than is necessary. \n    - Section 2 explains MAML, which is then covered in much more detail in Section 3; better to leave out the Section 2 MAML paragraph. \n    - Sections 3 and 4 are very redundant. Combine them for a shorter (i.e., better!) paper.\n- The paper says, \"Furthermore, for each learning rate learned, there will be N instances of that learning rate, one for each step to be taken. By doing this, the parameters are free to learn to decrease the learning rates at each step which may help alleviate overfitting.\" Does this happen empirically? Space could be freed up (see above) to have a figure showing whether or not this happens.\n- The paper says, \"we propose MAML++, an improved meta-learning framework\" -- it's a little too far to call this a new framework. it's still MAML, with improvements.\n\nTypos\n- \"4) increase the system’s computational overheads\" -> overhead\n- \"composed by\" -> composed of\n- \"Santurkar et al. (2018).\", \"Krizhevsky et al. (2012),\",  \"Finn et al. (2017) \" -> misplaced citation parens\n- \"a method that reduce\" -> reduces\n- \"An evaluation ran consisted\" -> evaluation consisted\n- The Loshchlikov and Hutter citation in the bibliography isn't right. It should be \"Sgdr: Stochastic gradient descent with restarts.\" (2016) instead of \"Fixing weight decay regularization in adam\" (2017).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}