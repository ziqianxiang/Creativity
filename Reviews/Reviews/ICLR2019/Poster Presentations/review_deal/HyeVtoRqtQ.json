{
    "Decision": {
        "metareview": "The paper proposes a novel network architecture for sequential learning, called trellis networks, which generalizes truncated RNNs and also links them to temporal convnets. The advantages of both types of nets are used to design trellis networks which appear to outperform state of art on several datasets.  The paper is well-written and the results are convincing.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "An interesting novel approach combining advantages of truncated RNNs and temporal convnets"
    },
    "Reviews": [
        {
            "title": "Review of \"Trellis Networks for Sequence Modeling\"",
            "review": "The authors propose a family of deep architecture (Trellis Networks ) for sequence modelling. Paper is well written and very well connected to existing literature. Furthermore, papers organization allows one to follow easily.  Trellis Networks bridge truncated RNN and temporal convolutional networks. Furthermore, proposed architecture is easy to extend and couple with existing RNN modules e.g. LSTM Trellis networks. Authors support their claims with an extensive empirical evidence. The proposed architecture is better than existing networks.\nAlthough the proposed method has several advantages, I would like to see what makes proposed architecture better than existing methods.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting formulation of a convolutional view of recurrent networks but the real impact of this model has yet to be shown",
            "review": "The authors propose a new type of neural network architecture for sequence modelling : Trellis Networks. A trellis network is a special case of temporal convolutional network with shared weights across time and layers and  with input at each layer. As stated by the authors, this architecture does not seem really interesting. The authors show that there exists  an equivalent Trellis Network to any truncated RNN and therefore that truncated RNN can be represented by temporal convolutional network. This result is not surprising since  truncated RNN can be  unrolled and that their time dependency is bounded.  The construction of the Trellis Network equivalent to a truncated RNN involves sparse weight matrices, therefore using full weight matrices provides a greater expressive power. One can regret that the authors do not explain what kind of modelling power one can gain with full weight matrices. \n\nThe author claim that bridging the gap between recurrent and convolutional neural networks with  Trellis Network allows to benefit from techniques form both kinds of networks. However, most of the techniques are already used with  convolutional networks.  \n\nExperiments are conducted with LSTM trellis network on several sequence modelling tasks : word-level and character-level language modelling, and sequence modelling in images (sequential MNIST, permuted MNIST  and sequential CIFAR-10). Trellis network yield very competitive results compare to recent state of the art models. \n\nThe ablation  study presented in Annex D Table 5 is interesting since it provides some hints on what is really useful in the model. It seems that full weight matrices are not the most interesting aspect (if dense kernel really concerns this aspect) and that the use if the input at every layer has most impact.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper introduces a novel architecture for sequence modeling, called the trellis network. The trellis network is in a sense a combination of RNNs and CNNs. The authors give a constructive proof that the trellis network is a special case of a truncated RNN. It also resembles CNNs since the neurons at higher levels have bigger receptive fields. As a result, techniques from RNN and CNN literature can be conveniently brought in and adapted to trellis network. The proposed method is evaluated on benchmark tasks and shows performance gain over existing methods.\n\nThe paper is well-written and easy to follow. The experimental study is extensive. The reviewer believes that this paper will potentially inspire future research along this direction. However, the novelty of the proposed method compared to the TCN seems limited: only weight sharing and input injection. It would be great to include the performance of the TCN on the PTB dataset, on both word and character levels in Table 1 and 2.\n\nAccording to Theorem 1, to model an M-truncated L-layer RNN, a trellis network needs M + L âˆ’ 1 layers. When M is large, it seems that a trellis network needs to be deep. Although this does not increase to model size due to weight sharing, does it significantly increase computation time, both during training and inference?\n\nThe review might have missed it, but what is the rationale behind the dotted link in Figure 1a, or the dependence of the activation function $f$ on $z_t^{(i)}$? It seems that it is neither motivated by RNNs nor CNNs. From RNN's point of view, as shown in the proof of Theorem 1, $f$ only depends on its first argument. From CNN's point of view, the model still gets the same reception field without using $z_t^{(i)}$.\n\nMinor comments:\nThe authors might want to give the full name of TCN (temporal convolutional networks) and a short introduction in Section 2 or at the beginning of Section 4.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}