{
    "Decision": {
        "metareview": "The reviewers and authors had a productive conversation, leading to an improvement in the paper quality. The strengths of the paper highlighted by reviewers are a novel learning set-up and new loss functions that seem to help in the task of protein contact prediction and protein structural similarity prediction. The reviewers characterize the work as constituting an advance in an exciting application space, as well as containing a new configuration of methods to address the problem.\n\nOverall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Clear accept"
    },
    "Reviews": [
        {
            "title": "Are protein sequence emebeddings learned from structure?",
            "review": "This work learns embeddings for proteins. They use techniques from deep learning on natural langauge that are typically applied to scentences and words, and apply them correspondinly to proteines and amino acids. Thus they learn a vector representation using a bi-directional LSTM for amino acids by training the amino acid equivalent of a language model.\n\nThe authors then multitask 2 models using the embeddings that perform contact prediction (using an mlp and CNN) and structural class similarity model, which appear to perform very well.\n\nTheir SSA - soft symmetric alignment mechanism is neat and gives a single scalar value for a pair of proteins (by comparing their strings of emebedded amino acids by L1 distance), and it is descriptive enough feature for a simple ordinal regression to output a set structural similarity scores via a linear classifier (one for each strength of similarity re. the SCOP classification hierachy). It seems to work well, but I am unable to judge how good this is with respect to more recent work in this field. I would suspect being able to backprop to the embedding LSTMs through the SSA at this point would give much better results. \n\nAuthors only give 2 recent refeneces for protein embedding work [12,13] but should also take a look at this work: Melvin et. al, PLOS Computational Biology, this work uses structural class labels from SCOP to supervise the embedding. Although they do mention profile HMM in 'related work' which was used to create features in that work.\n\nThese authors, as far as I can tell do not \"backprop\" to the amino acid embeddings (and the LSTMs) from the contact or similarty loss. So the bi-LSTM-produced feature vectors, although trained unsupervised from many proteins, are not trained with structural supervision as claimed in the title (they state this in last paragraph of 3.1) and so the embeddings are not related to structural similarity directly. They do, however, seem to produce good features for the tasks they then tackle.\nThey say in the conclusion that the SSA model is fully differentiable, but I don't see where they \"backprop\" through it.\n\nI would say (if this assessment is correct) then the title is very misleading, although the work and  final results look good.\n\n\nupdate: the authors have assured me in comments that the model is trained end to end - changing rating to good..\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising idea but falls short in write-up / evaluation",
            "review": "Thanks for the detailed responses. After reading the author response and the updated paper, I am satisfied on several of my concerns, many of which were due to the writing in the earlier submission. The updated results on various comparisons are also good.  I have updated my score accordingly. Some qualitative analysis of the results would have been nice -- examples of protein pairs where they do well and other methods have difficulty as they don't use the structural similarity info / global sequence info used by this paper. But maybe those can be in a journal submission.\nMy only remaining concern is on the lack of reporting average performance on the test data (which used to be the norm until recently for papers submitted to ML conferences).\n\n\nSummary:\nThis paper proposes an approach for embedding proteins using their amino-acid sequences, with the goal that embeddings of proteins with similar structure are encouraged to be close in the embedded space. A stacked 3-layer Bi-directional LSTM is used for embedding the proteins. The structure information is obtained from the SCOP database which is used in an ordinal regression framework, where the output is the structural similarity and inputs are the embeddings. Along with the ordinal regression, another loss term to incorporate contacts of amino-acid residues is used. Results are shown on structural similarity prediction and secondary structure prediction.\n\nClarity:\n1. The introduction of the paper is not very well written and it takes some time to figure out the exact problem being addressed. Is it learning sequence embeddings, or predicting structure from sequence or searching for similar structures in a database. Defining a clear goal -- input/output of their pipeline is important before describing the applications of the method, such as predicting structural similarity. \n2. Due to the write-up, the method comes across as having too many modeling components without a very clear motivation for why these help the problem at hand. Where is the alignment part?\n3. Why is each sequence embedded as a matrix? What is the motivation for a vector representation at each amino-acid position?\n4. The authors need to explain the particular choice of 3 layers of bi-directional LSTMs. Why three? And why Bi-LSTM and not LSTMs? \n\nQuality:\n1. While the problem being addressed is interesting, the work lacks a clear reasoning behind the choice of modeling components which makes it seem ad-hoc.\n2. Structural similarity is defined using the hierarchy of protein structure classes and the numbers seemed a bit arbitrary to me. Why not have a vector to encode the different aspects of structure? Have they looked at prior work?\n3. How does the pre-trained language model on Pfam sequences help? Why is the output from it concatenated; have other composition functions been considered? \n\nOriginality:\nThe various components of the model are not novel, but the particular framework of putting them together is novel.\n\nResults:\n1. While the authors claim that some prior methods only work with high sequence similarity, their own evaluation only considers pairs of sequences with 95% identity. HHalign for instance, considers sequences with ~20% identity.\n2. Why weren't several train/test splits of the data tried, so that performance can be reported with std. error bars?\n3. Methods against which they compare have not been described properly.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good application paper but evaluation must be strengthened",
            "review": "General comment\n==============\nThe authors describe two loss functions for learning embeddings of protein amino acids based on i) predicting the global structural similarity of two proteins, and ii) predicting amino acid contacts within proteins. As far as I know, these loss functions are novel and the authors show clear improvements when using the learned embeddings in downstream tasks. The paper is well motivated and mostly clearly written. However, the evaluation must be strengthened and some aspects of it clarified. Provided that the authors address my comments below, I think it is a good ICLR application paper.\n\nMajor comments\n=============\n1. The authors should describe how they optimized hyperparameters such as the learning, lambda (loss section 3.3), or the smoothing factor (section 3.4). These should be optimized on an evaluation set, but the authors only mentioned that they split the dataset into training and holdout (test) set (section 4.1).\n\n2. The way the authors present results in table 1 and table 2 is unclear. Both table 1 and table 2 contain results of the structural similarity tasks but with different baselines. ‘SSA w/ contact predictions’ is also undefined and can be interpreted as ‘with’ or ‘without’ contacts predictions. I therefore strongly recommend to show structural similarity results in table 1 and secondary structure results in table 2 and include in both tables i) ‘SSA full’, ‘SSA without contact predictions’, and ‘SSA without language model’.\n\n3. The authors should compare SSA to the current state-of-the art in structure prediction in addition to baseline models.\n\n4. The authors should evaluate how well their method predicts amino acid contact maps.\n\n5. The authors should describe how they were dealing with variable-length protein sequences. Are sequences truncated and embedded to a fixed length? What is the mean and variance in protein sequence lengths in the considered datasets? The authors should point out that their method is limited to fixed length sequences.\n\n6. The authors should briefly describe the training and inference time on a single GPU and CPU. How much memory is required for training with a certain sequence length, e.g. 400 amino acids per sequence? Does the model fit on a single GPU?\n\n7. The authors should discuss limitations of their method, e.g. that it cannot handle variable length sequences and that the memory scales quadratically by the the sequence length.\n\n8. CRF (SSA) (table 3) includes a biLSTM layer between SSA and the CRF. However, the biLSTM can learn a non-linear projection of embeddings learned by SSA such that it is unclear if improvements are due to the embeddings learned by SSA or the biLSTM+CRF architecture. The authors should therefore train a biLSTM+CRF model on one-hot encoded amino-acids and include it as baseline in table 3.\n\n\nMinor comments\n=============\n9. The way the similarity score s’ is computed (section 3.2.1) should be motivated more clearly. Why do the authors compute the score s’ manually instead of predicting it, e.g. using a model that takes the embeddings z of both proteins as input and predicts a single scalar s’? \n\n10. How does ordinal regression (section 3.2.2) perform compared with a softmax layer? Why do the authors compute s’ and then train logistic regression classifiers on s’ to predict the similarity level, instead of predicting the similarity level directly based on the embeddings z?\n\n11. Why do the authors use a distance threshold of 8A (section 3.3)? Is this common practice in the field?\n\n12. Why do the authors use the not product and the absolute difference as features instead of the embeddings z directly? Which activation function is used to predict contact probabilities (sigmoid, softmax, …)?\n\n13. The authors should reference and describe the results presented in table 1 more clearly.\n\n14. Optional: the authors should analyze if learned embeddings are correlated with amino acid and structural properties such as their size, charge, or solvent accessibility. Do embeddings clusters by certain properties? This can be analyzed, e.g., using a tSNE plot. \n\n15. How does TMalign perform when using the maximum or geometric average instead of the arithmetic average of the two scores (section 4.1)",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}