{
    "Decision": {
        "metareview": "This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available. The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used. This setting appears to be pervasive. The reviewers agree that the paper is well written and the proposed bandit optimization-based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "well written, effective and relevant work on blackbox adversarial example generation"
    },
    "Reviews": [
        {
            "title": "good paper, accept",
            "review": "This paper formulates the black-box adversarial attack as a gradient estimation\nproblem, and provide some theoretical analysis to show the optimality of an\nexisting gradient estimation method (Neural Evolution Strategies) for black-box\nattacks.\n\nThis paper also proposes two additional methods to reduce the number of queries\nin black-box attack, by exploiting the spacial and temporal correlations in\ngradients. They consider these techniques as priors to gradients, and a bandit\noptimization based method is proposed to update these priors. \n\nThe ideas used in this paper are not entirely new. For example, the main\ngradient estimation method is the same as NES (Ilyas et al. 2017);\ndata-dependent priors using spatially local similarities was used in Chen et\nal. 2017.  But I have no concern with this and the nice thing of this paper is \nto put these tricks under an unified theoretical framework, which I really \nappreciate.\n\nExperiments on black-box attacks to Inception-v3 model show that the proposed\nbandit based attack can significantly reduces the number of queries (2-4 times\nfewer) when compared with NES. \n\nOverall, the paper is well written and ideas are well presented.\nI have a few concerns:\n\n1) In Figure 2, the authors show that there are strong correlations between the\ngradients of current and previous steps. Such correlation heavily depends on\nthe selection of step size.  Imagine that the step size is sufficiently large,\nsuch that when we arrive at a new point for the next iteration, the\noptimization landscape is sufficiently changed and the new gradient is vastly\ndifferent than the previous one. On the other hand, when using a very small\nstep-size close to 0, gradients between consecutive steps will be almost the\nsame. By changing step-size I can show any degree of correlation.  I am not\nsure if the improvement of Bandit_T comes from a specific selection of\nstep-size. More empirical evidence on this need to be shown - for example, run\nBandit_T and NES with different step sizes and observe the number of queries\nrequired.\n\n2) This paper did not compare with many other recent works which claim to\nreduce query numbers significantly in black-box attack. For example, [1]\nproposes \"random feature grouping\" and use PCA for reducing queries, and [2]\nuses a good gradient estimator with autoencoder. I believe the proposed method\ncan beat them, but the authors should include at least one more baseline to \nconvince the readers that the proposed method is indeed a state-of-the-art.\n\n3) Additionally, the results in this paper are only shown on a single model\n(Inception-v3), and it is hard to compare the results directly with many other\nrecent works. I suggest adding at least two more models for comparison (most\nblack-box attack papers also include MNIST and CIFAR, which should be easy to\nadd quickly). These numbers can be put in appendix.\n\nOverall, this is a great paper, offering good insights on black-box adversarial\nattack and provide some interesting theoretical analysis. However currently it\nis still missing some important experimental results as mentioned above, and\nnot ready to be published as a high quality conference paper. I conditionally\naccept this paper as long as sufficient experiments can be added during the\ndiscussion period.\n\n\n[1] Exploring the Space of Black-box Attacks on Deep Neural Networks, by Arjun\nNitin Bhagoji, Warren He, Bo Li and Dawn Song, https://arxiv.org/abs/1712.09491\n(conference version accepted by ECCV 2018)\n\n[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking\nBlack-box Neural Networks, by Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia\nLiu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng,\nhttps://arxiv.org/abs/1805.11770\n\n==========================================\n\nAfter discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments. The paper looks much better than before. Thus I increased my rating.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, low confidence.",
            "review": "Paper formalizes the gradient estimation problem in a black-box setting, and provs the equivalence of least Squares with NES. It then improves on state of the art by using priors coupled with a bandit optimization technique.\n\nThe paper is well written. The idea of using priors to improve adversarial gradient attacks is an enticing idea. The results seem convincing.\n\nComments:\n- I missed how data dependent prior is factored into the algorithms 1-3. Is it by the choice of d? I suggest a clearer explanation.\n- In fig 4, I was confused that the loss of the methods is increasing. it took me a minute to realize this is the maximized adversarial loss, and thus higher is better. you may want to spell this out for clarity. I typically associate lower loss with better algorithms.\n- I am confused by Fig 4c. If I am comparing g to g*, I do expect a high cosine similarity. cos = 1 is the best. Why is correlation so small? and why is it 0 for NES? You may also want to offer additional insight in the text explaining 4c. \n\nMinor comments:\n- Is table one misplaced?\n- The symbol for \"boundary of set U\" may be confused with a partial derivative symbol\n- first paragraph of 2.4: \"our estimator a sufficiently\". something missing?\n- \"It is the actions g_t (equal to v_t) which...\" refering to g_t as actions is confusing. Although may be technically correct in bandit setting\n- Further explain the need for the projection of algorithm 3, line 7.\n- Fig 4: refer to true gradient as g*\n\nCaveat: Although I am well versed in bandits, I am not familiar with adversarial training and neural network literature. There is a chance I may have misevaluated central concepts of the paper.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A decent paper",
            "review": "UPDATE:\n\nI've read the revised version of this paper, I think the concernings have been clarified.\n\n-------\n\nThis paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.\n\nAlthough I think this paper is a decent paper that deserves an acceptance, there are several concernings:\n\n1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become \"vanishingly small\", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).\n\n2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.\n\n3. In the experimental results, what is the difference between one \"query\" and one \"iteration\"? It looks like in one iteration, the Algorithm 2 queries twice?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}