{
    "Decision": {
        "metareview": "This paper extends the single source H-divergence theory for domain adaptation to the case of multiple domains. Thus, drawing on the known connection between H-divergence and learning the domain classifier for adversarial adaptation, the authors propose a multi-domain adversarial learning algorithm. The approach builds upon the gradient reversal version of adversarial adaptation proposed by Ganin et al 2016. \n\nOverall, multi-domain learning and limiting the worst case performance on any single domain is an interesting problem which has been relatively underexplored. Though this work does not have the highest performance on all datasets across competing methods, as noted by reviewers, it proposes a useful theoretical result which future research may build on. I would encourage the reviewers to compare against and discuss the missing prior work cited by Rev 3. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "Theoretical contribution limiting worst case performance in the multi-domain setting for adversarial based adaptation methods"
    },
    "Reviews": [
        {
            "title": "Good idea, but the results are not particularly convincing",
            "review": "PROS:\n* Original idea of using separate \"discriminator\" paths for unknown classes\n* Thorough theoretical explanation\n* A variety of experiments\n* Very well-written, and clear paper\n\nCONS:\n* The biggest problem for me was the unconvincing results. MNIST-to-MNIST-M has better baselines  (PixelDA performed better on this task for example), Office is not suitable for domain adaptation experiments anymore unless one wants to be in a few-datasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for NN-based domain adaptation); the results on CELL were not convincing, I don't know the dataset but it seems that baseline NN does better than DA most of the times.\n* Comparison with other methods did not take into account a variety of hyperparameters. Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one. What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper has good presentation and sufficient contribution to the field",
            "review": "In this paper, the authors proposed a multi-domain adversarial learning approach, MULANN, to improve the classification accuracy on three datasets-DIGITS, OFFICE and CELL-in the semi-supervised DA setting. It’s contributions include: i) using the H-divergence to bound both the risk across all domains and the worst-domain risk (imbalance on a specific domain); ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state-of-the-art on two standard image benchmarks, and a novel bioimage dataset, CELL. \nIn addition, this paper has a clear logic to explain and prove the problem to be solved, and has ample experimental evidence. Above on, this paper did a meaningful work. But there are some errors of expression, so it should be checked.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting known/unknown label multidomain learning setting, but weak evaluation",
            "review": "Summary:\n\nThe manuscript proposes a multi-domain adversarial learning (MDL) method called MULANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. The authors define a new discrimination task to discriminate, within each domain, labeled samples from unlabeled ones that most likely belong to extra classes (classes with no labeled or unlabeled samples in the domain). They also introduce a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence.\n\nStrengths:\n\n- The idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong to extra classes is interesting.\n\n- A new generalization bound for MDL is introduced.\n\n- The paper was clear, well written, well-motivated and nicely structured.\n\n- The authors perform numerous empirical experiments on several types of problems on various datasets (Digit, OFFICE,CELL) successfully showing how the MULANN can reduce the nasty effects of the adversarial domain discriminator and repulse (a fraction of) unlabeled examples from labeled ones in each domain.\n\nWeaknesses:\n\n- all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context. More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset. Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.\n\n- The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier. Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples (e.g. the classifier may output high entropy for the unlabeled samples of the classes with labeled samples) and they may harm the performance of adaptation.  It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.\n\n- Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL (MULANN handles only the class asymmetry when domains involve distinct sets of classes and it has nothing to do with MDL). hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.\n\n- Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).\n\nThe reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.\n\n[1] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017.\n\n[2] Bousmalis, Konstantinos, et al. \"Domain separation networks.\" Advances in Neural Information Processing Systems. 2016.\n\n[3] Zhao, Han, et al. \"Multiple Source Domain Adaptation with Adversarial Learning.\" Advances in Neural Information Processing Systems. 2018.\n\n[4] Hoffman, Judy, Mehryar Mohri, and Ningshan Zhang. \"Algorithms and Theory for Multiple-Source Adaptation.\"  Advances in Neural Information Processing Systems. 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}