{
    "Decision": {
        "metareview": "* Strengths\n\nThe paper addresses an important topic: how to bound the probability that a given “bad” event occurs for a neural network under some distribution of inputs. This could be relevant, for instance, in autonomous robotics settings where there is some environment model and we would like to bound the probability of an adverse outcome (e.g. for an autonomous aircraft, the time to crash under a given turbulence model). The desired failure probabilities are often low enough that direct Monte Carlo simulation is too expensive. The present work provides some preliminary but meaningful progress towards better methods of estimating such low-probability events, and provides some evidence that the methods can scale up to larger networks. It is well-written and of high technical quality.\n\n* Weaknesses\n\nIn the initial submission, one reviewer was concerned that the term “verification” was misleading, as the methods had no formal guarantees that the estimated probability was correct. The authors proposed to revise the paper to remove reference to verification in the title and the text, and afterwards all reviewers agreed the work should be accepted. The paper also may slightly overstate the generality of the method. For instance, the claim that this can be used to show that adversarial examples do not exist is probably wrong---adversarial examples often occupy a negligibly small portion of the input space. There was also concern that most comparisons were limited to naive Monte Carlo.\n\n* Discussion\n\nWhile there was initial disagreement among reviewers, after the discussion all reviewers agree the paper should be accepted. However, we remind the authors to implement the changes promised during the discussion period.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "well-written paper addressing timely question"
    },
    "Reviews": [
        {
            "title": "Ok to accept after discussion",
            "review": "Verifying the properties of neural networks can be very difficult.  Instead of\nfinding a formal proof for a property that gives a True/False answer, this\npaper proposes to take a sufficiently large number of samples around the input\npoint point and estimate the probability that a violation can be found.  Naive\nMonte-Carlo (MC) sampling is not effective especially when the dimension is\nhigh, so the author proposes to use adaptive multi-level splitting (AMLS) as a\nsampling scheme. This is a good application of AMLS method.\n\nExperiments show that AMLS can make a good estimate (similar quality as naive\nMC with a large number of samples) while using much less samples than MC, on\nboth small and relatively larger models.  Additionally, the authors conduct\nsensitivity analysis and run the proposed algorithm with many different\nparameters (M, N, pho, etc), which is good to see.\n\n\nI have some concerns on this paper:\n\nI have doubts on applying the proposed method to higher dimensional inputs. In\nsection 6.3, the authors show an experiments in this case, but only on a dense\nReLU network with 2 hidden layers, and it is unknown if it works in general.\nHow does the number of required samples increases when the dimension of input\n(x) increases? \n\nFormally, if there exists a violation (counter-example) for a certain property,\nand given a failure probability p, what is the upper bound of number of samples\n(in terms of input dimension, and other factors) required so that the\nprobability we cannot detect this violation with probability less than p?\nWithout such a guarantee, the proposed method is not very useful because we\nhave no idea how confident the sampling based result is. Verification needs\nsomething that is either deterministic, or a probabilistic result with a small\nand bounded failure rate, otherwise it is not really a verification method.\n\nThe experiments of this paper lack comparisons to certified verification\nmethods. There are some scalable property verification methods that can give a\nlower bound on the input perturbation (see [1][2][3]).  These methods can\nguarantee that when epsilon is smaller than a threshold, no violations can be\nfound.  On the other hand, adversarial attacks give an upper bound of input\nperturbation by providing a counter-example (violation). The authors should\ncompare the sampling based method with these lower and upper bounds. For\nexample, what is log(I) for epsilon larger than upper bound?\n\nAdditionally, in section 6.4, the results in Figure 2 also does not look very\npositive - it unlikely to be true that an undefended network is predominantly\nrobust to perturbation of size epsilon = 0.1. Without any adversarial training,\nadversarial examples (or counter-examples for property verification) with L_inf\ndistortion less than 0.1 (at least on some images) should be able to find. It\nis better to conduct strong adversarial attacks after each epoch and see what\nare the epsilons of adversarial examples.\n\nIdeas on further improvement:\n\nThe proposed method can become more useful if it is not a point-wise method.\nIf given a point, current formal verification method can tell if a property is\nhold or not.  However, most formal verification method cannot deal with a input\ndrawn from a distribution randomly (for example, an unseen test example). This\nis the place where we really need a probabilistic verification method. The\nsetting in the current paper is not ideal because a probabilistic estimate of\nviolation of a single point is not very useful, especially without a guarantee\nof failure rates.\n\nFor finding counter-examples for a property, using gradient based methods might\nbe a better way. The authors can consider adding Hamiltonian Monte Carlo to\nthis framework (See [4]).\n\nReferences: \nThere are some papers from the same group of authors, and I merged them to one.\nSome of these papers are very recent, and should be helpful for the authors\nto further improve their work.\n\n[1] \"AI2: Safety and Robustness Certification of Neural Networks with Abstract\nInterpretation\", IEEE S&P 2018 by Timon Gehr, Matthew Mirman, Dana\nDrachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev \n\n(see also \"Differentiable Abstract Interpretation for Provably Robust Neural\nNetworks\", ICML 2018. by Matthew Mirman, Timon Gehr, Martin Vechev.  They also\nhave a new NIPS 2018 paper \"Fast and Effective Robustness Certification\" but is\nnot on arxiv yet)\n\n[2] \"Efficient Neural Network Robustness Certification with General Activation\nFunctions\", NIPS 2018. by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui\nHsieh, Luca Daniel.  \n\n(see also \"Towards Fast Computation of Certified Robustness for ReLU Networks\",\nICML 2018 by Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,\nDuane Boning, Inderjit S. Dhillon, Luca Danie.)\n\n[3] Provable defenses against adversarial examples via the convex outer\nadversarial polytope, NIPS 2018. by Eric Wong, J. Zico Kolter.\n\n(see also \"Scaling provable adversarial defenses\", NIPS 2018 by the same authors)\n\n[4] \"Stochastic gradient hamiltonian monte carlo.\" ICML 2014. by Tianqi Chen,\nEmily Fox, and Carlos Guestrin.\n\n============================================\n\nAfter discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea for quantitatively estimating the robustness of a network. Would like to see more comprehensive large-scale experiments. ",
            "review": "Given a network and input model for generating adversarial examples, this paper presents an idea to quantitatively evaluate the robustness of the network to these adversarial perturbations. Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.  Detailed review below:\n- How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension. \n- Did you experiment with other MH proposal beyond a random walk proposal? Is it possible to measure the diversity of the samples using techniques such as the effective sample size (ESS) from the SMC literature?\n- What is the performance of the proposed method against \"universal adversarial examples\"?\n- The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?\n- Please provide some intuition for this line in Figure 3: \"while the robustness to perturbations of size \u000f = 0:3 actually starts to decrease after around 20 epochs.\"\n- A number of attack and defense strategies have been proposed in the literature. Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very interesting paper with a nice methodological tansfer between rare event estimation and NN verification",
            "review": "This is a paper of the verification of neural networks, i.e. check their robustness, \nand the main contribution here is to tackle it as a statistical problem adressed with \nmulti-level splitting Monte Carlo approach. I found the paper well motivated and original, \nresulting in a publishable piece of research up to a few necessary adjustments. These \nconcern principally notation issues and some potential improvements in the writing. \nLet me list below some main remarks along the text, including also some typos. \n\n* In the introduction, \"the classical approach\" is mentioned but to be the latter is \ninsufficiently covered. Some more detail would be welcome. \n\n* page 2, \"predict the probability\": rather employ \"estimate\" in such context? \n\n* \"linear piecewise\": \"piecewise linear\"? \n\n* what is \"an exact upper bound\"? \n\n* In related work, no reference to previous work on \"statistical\" approaches to NN \nverification. Is it actually the case that this angle has never been explored so far?\n\n* I am not an expert but to me \"the density of adversarial examples\" calls for further \nexplanation. \n\n* From page 3 onwards: I was truly confused by the use of [x] throughought the text \n(e.g. in Equation (4)). x is already present within the indicator, no need to add yet \nanother instance of it. Here and later I suffered from what seems to be like an awkward \nattempts to stress dependency on variables that already appear or should otherwise \nappear in a less convoluted way. \n\n* In Section 4, it took me some time to understand that the considered metrics do not \nrequire actual observations but rather concern coherence properties of the NN per se. \nWhile this follows from the current framework, the paper might benefit from some more \nexplanation in words regarding this important aspect. \n\n* In page 6, what is meant by \"more perceptually similar to the datapoint\"? \n\n* In the discussion: is it really \"a new measure\" that is introduced here? \n\n* In the appendix: the MH acronym should better be introduced, as should the notation \ng(x,|x') if not done elsewhere (in which case a cross-reference would be welcome). \nBesides this, writing \"the last samples\" requires disambiguation (using \"respective\"?). \n\n\n \n\n ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}