{
    "Decision": {
        "metareview": "The reviewers had some concerns regarding clarity and evaluation but in general liked various aspects of the paper. The authors did a good job of addressing the reviewers' concerns so acceptance is recommended.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Poster)",
        "title": "boderline - but leaning to accept"
    },
    "Reviews": [
        {
            "title": "Interesting paper with some clarifications required",
            "review": "The paper proposes a Variational IB based approach to learn action representations directly from video of actions being taken. The basic goal of the work is to disentangle the dynamic parts of the scene in the video from the static parts and only capture those dynamic parts in the representation. Further, a key property of these learned representations is that they contain compositional structure of actions so as to their cumulative effects. The outcome of such a method is better efficiency of the subsequent learning methods while requiring lesser amount of action label videos. \n\nTo achieve this, the authors start with a previously proposed video prediction model  that uses variational information bottleneck to learn minimal action representation. Next, this model is augmented with composability module where in latent samples across frames are composed into a single trajectory and is repeated in a iterative fashion and again the minimal representation for the composed action space is learned using IB based objective. The two objectives are learned in a joint fashion. Finally, they use a simple MLP based bijection to learn the correspondence between actions and their latent representations. Experiments are done on two datasets - reacher and BAIR - and evaluation is reported for action  conditioned video prediction and visual servoing.\n\n- The paper is well written and provides adequate details to understand the flow of the material.\n- The idea of learning disentangled representation is being adopted in many domains and hence this contribution is timely and very interesting to the community.\n- The overall motivation of the paper to emulate how humans learn by looking at other's action is very well taken. Being able to learn from only videos is a nice property especially when the actual real world environment is not accessible.\n- High Performance in terms of error and number of required action labeled videos demonstrates the effectiveness of the approach.\n\nHowever, there are some concerns with the overall novelty and some technical details in the paper:\n- It seems the key contribution of the paper is to add the L_comp part to the already available L_pred part in Denton and Fergus 2018. The trick use to compose the latent variables is not novel and considering that variational IB is also available, the paper lacks overall novelty. A better justification and exposition of novelty in this paper is required.\n- Two simple MLP layers for bijection seems very adhoc. I am not able to see why such a simple bijection would be able to map the disentangled composed action representations to the actual actions. It seems it is working from the experiments but a better  analysis is required on how such a bijection is learned and if there are any specific properties of such bijection such that it will work only in some setting. Will the use of better network improve the learned bijection?\n- While videos are available, Figures in the paper itself are highly unreadable. I understand the small figures in main paper but it should not be an issue to use full pages for the figure on appendix.\n- Finally, it looks like one can learn the composed actions (Right + UP) representation while being not sensitive to static environment. If that is the case, does it work on the environment where except the dynamic part everything else is completely different? For example, it would be interesting to see if a model is trained where the only change in environment is a robot's hand moving in 4 direction while everything else remaining same. Now would this work, if the background scene is completely changed while keeping the same robot arm?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "I like the idea, but concerns with experimental evaluation",
            "review": "The authors propose a way to learn models that predict what will happen next in scenarios where action-labels are not available in abundance. The agents extend previous work by proposing a compositional latent-variable model. Results are shown on BAIR (robot pushing objects) and simulated reacher datasets. The results indicate that it is possible to learn a bijective mapping between the latent variables inferred from a pair of images and the action executed between the observations of the two images. \n\nI like the proposed model and the fact that it is possible to learn a bijection between the latent variables and actions is cute. I have following questions/comments: \n\n(a) The authors have to learn a predictive model from passive data (i.e. without having access to actions). Such models are useful, if for example an agent can observe other agents or internet videos and learn from them. In such scenarios, while it would be possible to learn “a” model using the proposed method, it is unclear how the bijective mapping would be learnt, which would enable the agent to actually use the model to perform a task that it is provided with. \nIn the current setup, the source domain of passive learning and target domain from which action-labelled data is available are the same. In such setups, the scarcity of action-labelled data is not a real concern. When an agent acts, it trivially has access to its own actions. So collecting observation, action trajectories is a completely self-supervised process without requiring any external supervision. \n\n(b)  How is the model of Agrawal 2016 used for visual serving? Does it used the forward model in the feature space of the inverse model or something else? \n\n(c) In the current method, a neural network is used for composition. How much worse would a model perform if we simply compose by adding the feature vectors instead of using a neural network. It seems like a reasonable baseline to me. Also, how critical is including binary indicator for v/z in the compositional model? \n\nOverall, I like the technical contribution of the paper. The authors have a very nice introduction on how humans learn from passive data. However, the experiments make a critical assumption that domains that are used for passive and action-based learning are exactly the same. In such scenarios, action-labeled data is abundantly available. I would love to see some results and/or hear the authors thoughts on how their method can be used to learn by observing a different agent/domain and transfer the model to act in the agent’s current domain. I am inclined to vote for accepting the paper if authors provide a convincing rebuttal. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, needs some clarification in the experimental section and the introduction",
            "review": "PAPER SUMMARY\n-------------\nThis paper proposes an approach to video prediction which autonomously finds an action space encoding differences between subsequent frames. This approach can be used for action-conditioned video prediction and visual servoing. \nUnlike related work, the proposed method is initially trained on video sequences without ground-truth actions. A representation for the action at each time step is inferred in an unsupervised manner. This is achieved by imposing that the representation of this action be as small as possible, while also being composable, i.e. that that several actions can be composed to predict several frames ahead.\nOnce such a representation is found, a bijective mapping to ground truth actions can be found using only few action-annotated samples. Therefore the proposed approach needs much less annotated data than approaches which directly learn a prediction model using actions and images as inputs.\n\nThe approach is evaluated on action-conditioned video prediction and visual servoing. The paper shows that the learned action-space is meaningful in the sense that applying the same action in different initial condition indeed changes the scenes in the same manner, as one would intuitively expect. Furthermore, the paper shows that the approach achieves state of the art results on a action-conditioned video prediction dataset and on a visual servoing task.\n\nPOSITIVE POINTS\n---------------\nThe idea of inferring the action space from unlabelled videos is very interesting and relevant.\n\nThe paper is well written.\n\nThe experimental results are very interesting, it is impressive that the proposed approach manages to learn meaningful actions in an unsupervised manner (see e.g. Figure 3).\n\nNEGATIVE POINTS\n---------------\nIt is not exactly clear to me how the model is trained for the quantitative evaluation. On which sequences is the bijective mapping between inferred actions and true actions learned? Is is a subset of the training set? If yes, how many sequences are used? Or is this mapping directly learned on the test set? This, however, would be an unfair comparison in my opinion, since then the actions would be optimized in order to correctly predict on the tested sequences.\n\nThe abstract and introduction are too vague and general. It only becomes clear in the technical and experimental section what problem is addressed in this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}