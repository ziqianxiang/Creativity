{
    "Decision": {
        "metareview": "The paper proposes a method to perform manifold regularization for semi-supervised learning using GANs. Although the SSL results in the paper are competitive with existing methods, R1 and R3 are concerned about the novelty of the work in the light of recent manifold regularization SSL papers with GANs, a point that the AC agrees with. Given the borderline reviews and limited novelty of the core method, the paper just falls short of the acceptance threshold for ICLR. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Borderline paper: reasonably good SSL results but limited novelty"
    },
    "Reviews": [
        {
            "title": "Interesting Approach with Good Results",
            "review": "Review for MANIFOLD REGULARIZATION WITH GANS FOR SEMISUPERVISED LEARNING\nSummary:\nThe paper proposed to incorporate a manifold regularization penalty to the GAN to adapt to semi-supervised learning. They approximate this penalty empirically by calculating stochastic finite differences of the generator’s latent variables. \nThe paper does a good job of motivating the additional regularization penalty and their approximation to it with a series of experiments and intuitive explanations. The experiment results are very through and overall promising. The paper is presented in a clear manner with only minor issues. \nNovelty/Significance:\nThe authors’ add a manifold regularization penalty to GAN discriminator’s loss function. While this is a simple and seemingly obvious approach, it had to be done by someone. Thus while I don’t think their algorithm is super novel, it is significant and thus novel enough. Additionally, the authors’ use of gradients of the generator as an approximation for the manifold penalty is a clever.\nQuestions/Clarity:\nIt would be helpful to note in the description of Table 3 what is better (higher/lower). Also Table 3 seems to have standard deviations missing in Supervised DCGANs and Improved GAN for 4000 labels. And is there an explanation on why there isn’t an improvement in the FID score of SVHN for 1000 labels?\nWhat is the first line of Table 4? Is it supposed to be combined with the second? If not, then it is missing results. And is the Pi model missing results or can it not be run on too few labels? If it can’t be run, it would be helpful to state this.\nOn page 11, “in Figure A2” the first word needs to be capitalized. \nIn Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there?\nWhat are the differences of the 6 pictures in Figure A7? Iterations?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Borderline: Manifold Regularization with GANS for SEMI-Supervised Learning ",
            "review": "This paper builds upon the assumption that GANs successfully approximate the data manifold, and uses this assumption to regularize semi-supervised learning process.\nThe proposed regularization strategy enforces that a discriminator or a given classifier should be invariant to small perturbations on the data manifold z. It is empirically shown that naively enforcing such a constraint by randomly adding noise to z could lead to under-smoothing or over-smoothing in some cases which can harm the final classification performance. Consequently, the proposed regularization technique takes a step of tunable size in the direction of the manifold gradient, which has the effect of smoothing along the direction of the gradient while ignoring its norm.\n \nExtensive experiments have been conducted, showing that the proposed approach\noutperforms or is comparable with recent state-of-the-art approaches on cifar 10, especially in presence of fewer labelled data points. On SVHN however, the proposed approach fails in comparison with (Kumar et al 2017) but performs better than other approaches.\n\nFurthermore, it has been shown that adding the proposed manifold regularization technique to the training of GAN greatly improves the image quality of generated images (in terms of FID scores and inception scores). Also, by combining the proposed regularizer with a classical supervised classifier (via pre-training a GAN and using it for regularization) decreases classification error by 2 to 3%.\n \nFinally, it has also been shown that after training a GAN using the manifold regularization, the algorithm is able to produce similar images giving a low enough perturbation of the data manifold z.\n \nOverall, this paper is well written and show significant improvements especially for image generation. However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts. The paper would be improved if the following points are taken into account:\n \nA comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).\nHow do the FID/Inception improvements compare to (Mescheder et al 2018)?\nIt would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.\nAlthough there is a clear improvement in FID scores for Cifar10. It would be informative to show the generated images w/ and w/o manifold regularization.\nMore analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.\nIt should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple but interesting idea, but the results are not very significant and some baselines are missing.",
            "review": "The paper tackles the problem of semi-supervised classification using GAN-based models. They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference. The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations. The idea is to use GAN to learn the manifold. The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs. They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \\delta to z directly (||f(z) - f(g(z+\\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small. The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \\bar r(z) and then adding a tunable magnitude of \\bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\\epsilon \\bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.\n\nPros:\n- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.\n- The idea is simple and easy to implement based on a standard GAN.\n- The authors conduct various experiments to show the interaction of the regularization and the generator.\n\nCons:\n- For semi-supervised classification, the paper did not report the best results in other baselines. E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels). The performance of the proposed method is worse than the previous work but they claimed \"state-of-the-art\" results. The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2]. The experimental results are not very convincing because many importance baselines are neglected.\n- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018). \n\nI'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3]. It would be better to compare with them.\n\nReferences:\n[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018\n[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018\n[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}