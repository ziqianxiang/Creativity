{
    "Decision": {
        "metareview": "This was a borderline paper and a very difficult decision to make.\n\nThe paper addresses a potentially interesting problem in approximate POMDP planning, based on simplifying assumptions that perception can be decoupled from action and that a set of sensors exhibits certain conditional independence structure.  As a result, a simple approach can be devised that incorporates a simple greedy perception method within a point-based value iteration scheme.\n\nUnfortunately, the assumptions the paper makes are so strong and seemingly artificial to the extent that they appear reverse engineered to the use of a simple perception heuristic.  In principle, such a simplification might not be a problem if the resulting formulation captured practically important scenarios, but that was not convincingly achieved in this paper---indeed, another major limitation of the paper is its weak motivation.  In more detail, the proposed approach relies on decoupling of perception and action, which is a restrictive assumption that bypasses the core issue of exploration versus exploitation in POMDPS.  As model of active perception, the proposal is simplistic and somewhat artificial; the motivation for the particular cost model (cardinality of the sensor set) is particularly weak---a point that was not convincingly defended in the discussion.  Perhaps the biggest underlying weakness is the experimental evaluation, which is inadequate to support a claim that the proposed methods show meaningful advantages over state-of-the-art approaches in important scenarios.  A reviewer also raised legitimate questions about the strength of the theoretical analysis.\n\nIn the end, the reviewers did not disagree on any substantive technical matter, but nevertheless did disagree in their assessments of the significance of the contribution.  This is clearly a borderline paper, which on the positive side, was competently executed, but on the negative side, is pursuing an artificial scenario that enables a particularly simple algorithmic approach.\n\nDespite the lack of consensus, a difficult decision has to be made nonetheless.  In the end, my judgement is that the paper is not yet strong enough for publication.  I would recommend the authors significantly strengthen the experimental evaluation to cover off at least two of the major shortcomings of the current paper: (1) The true utility of the proposed method needs to be better established against stronger baselines in more realistic scenarios.  (2) The relevance of the restrictive assumptions needs to be more convincingly established by providing concrete, realistic and more challenging case studies where the proposed techniques are still applicable.  The paper would also be improved if the theoretical analysis could be strengthened to better address the criticisms of Reviewer 4.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Borderline paper: strong assumptions enable simplified approximate planning for restricted POMDPs"
    },
    "Reviews": [
        {
            "title": "Algorithmic/theoretical development is sound, but assumptions are questionable.  ",
            "review": "This paper proposes a planning algorithm for a restricted class of POMDPs where the sensing decisions do not have any bearing on the hidden state evolution, or any material cost in terms of reward. A sensing decision consists of querying k out of n sensors which yield independent measurements of the hidden state. In this setting, the authors propose an optimization 2-stage optimization strategy, the first stage tries to find the optimal \"planning\" action in a point-based fashion, whereas the second aims to find the the sensor configuration that reduces the entropy of the post-update belief-state. The key observation is that the entropy minimization step is submodular and can be approximated greedily. This in turn translates to policy approximation bounds via information geometric arguments. \n\nThe positive: the paper is well written (save for some contained parts), the algorithm looks to be generally sound. Altogether the paper makes good points and is an interesting read. \n\nThe negative: \n* first, I think there is some wide-spread misuse of the term \"nearly optimal\". When talking about near-optimality, this usually refers to finding a (controllably) bounded approximation to the optimal policy/value function. However, here this refers to the error relative to the approximate solution produced by the 2-stage procedure of minimizing entropy, then making a planning decision. It is not clear to me to begin with that this approach would produce bounded policies/value functions. As a counter example, consider a state space consisting of two state variables S1, S2 which evolve independently with additive reward  R(S1, a)  + R(S2, a) with  R(S1, a) !=0, while R(S2, a) = 0 for all actions a. Now, there could be a sensing configuration that collapses the uncertainty over S2 completely, but does nothing over S1, and a different one that give some small reduction of uncertainty  over S1 and nothing over S2. The former may outperform the latter to any degree in terms of belief state entropy, but it will not lead to an optimal policy, since that entropy reduction is not value directed. Unless I misunderstand something, in which case the authors should clarify. \n\n* Second,  the particular assumptions in this paper are quite restrictive. This paper generally reads like a solution that was fit to a problem. This really hurts the story of the paper. It would be a vast improvement if the authors could find at least one plausible problem where there's a compelling case for this particular configuration of assumptions and try to evaluate how well they do on that problem relative to some reasonable baseline.\n\nRemarks: \n* The belief state notation used in this paper impacts undue suffering upon the reader. It comes in the form of expressions with multi-level sub/superscipts and accents such as: \"b prime subscript b tilde superscript a superscipt pr comma omega\". This is extremely hard to parse and possibly unnecessary, as b prime subscript b tilde and b tilde are the only configurations of accents and subscripts that appear. These could just be called alpha and beta and the rest is clear from the context. \n* The claim in theorem 4, the argmax is the same under both directions of the KL divergence, is not obvious. It is definitely not true for minimization, otherwise the I-projection and the M-projection would coincide. This should be argued. Alternatively, this point can be alltogether skipped, since Pinsker's bound, which is the only place this is used, does not depend on the direction of KL.\n\nOverall, this paper raises some nice points, but with these problems it is not a clear accept. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Decomposition of observation acquisition and action planning in POMDPs:  Insufficient motivation and results",
            "review": "This work addresses the problem of decomposing the observation acquisition from action planning in POMDPs.  Unfortunately, the paper has two major weaknesses.  First it is hindered by a confusing motivation, and the lack of clarity on the real purpose of the work is a problem throughout. Second the experiments are insufficient given current standards in the literature.\n\n1. Motivation:  The introduction suggests that the main motivation is to reduce the computational cost of treating all observations within planning (“one must establish a trade-off between optimality and tractability”), though later, different reasons are offered (“power, processing capability, and cost constraints”).  It seems that each of these poses different constraints, and depending on which we are most concerned with, a different approximation scheme should be selected. For example, if the real concern is tractability of the POMDP solution, I don’t see why it’s not possible to just acquire all the sensor information, and afterwards decide how to approximate the tracking (this by the way is what most point-based POMDP methods effectively do).  For the proposed AP^2-POMDP, possibly a more reasonable motivation is high cost of observation acquisition; a clean argument would have to be made about the class of problems for which this constraint is crucial, why cardinality of sensor is the right way to articulate this constraint.\n\n2. Experimental results:  The domains selected for the experiments are too simple, given current standards in the literature.  Looking at the 1D and 2D domains is fine to illustrate specific properties of your methods.  But it does not support the claim that the proposed model is more scalable than standard POMDPs.  In such simple domains, why not include results for a point-based method?  They should work in the 1D domain, probably also in the 2D domain.  Also, the setting for the 1-D domain, with a camera in every cell, seems very artificial.  First, if sensors are expensive, why put a camera in every cell?  And if they are not expensive, then why do we need to reason about which sensor to use at each step?  And why just read from k cameras at every step?  These questions point back to the concern regarding what is the real motivation for this work.  For the 2D domain, there are not even quantitative results on cumulative reward.  To be convincing, the results would need to be on substantially more complex domains; there are several POMDP benchmarks that could be considered, e.g. those in the work of Kurniawati et al.\n\nOther comments:\n-\tAssumption 1 states that the observations from sensors are mutually independent give the state and action.  Can you explain why this is reasonable?  Or whether this is a strong assumption (unlikely to be met in practice)?\n-\tSome of the bounds seem like they could be very loose in practice, even (in the worse case) worse than the default bound of (R_max-R_min)/(1-\\gamma). For example in Thm 3, in the case where the L1 distance between the 2 beliefs is 2, this is worse than the default bound.  Did you check what is the bound for the domains in the experiments?  Is it tighter than this?\n-\tA key statement is on p.8: “this added complexity is significantly lower than concatening the combinatorial perception actions with the planning actions”.   It is important to support this statement, ideally with both a precise complexity analysis, and with empirical results showing the lesser performance of standard point-based methods.\n\nMinor comment:\n-\tThe referencing style is broken and should be fixed, in particular proper use of Author (year) in the text.\n-\tThe derivations in the top part of p.4 (Eqn 2-4 & surrounding text) are confusing, given that these apply to a standard POMDP, whereas on the previous page your present the AP^2-POMDP model. It might be better in Sec.2 to first (briefly) introduce POMDPs, with Eqn 2-4, then introduce AP^2-POMDP in Sec.3.\n-\tP.5: “It is worth noting that the objective function does not explicitly depend on perception actions”. This is a confusing statement; V depends on observations through b_t.  The next sentence clarifies this, but it would be better to avoid the confusing statement.\n-\tAlg.2:  Add a reference beside the title (unless you claim it is new).  Maybe Pineau et al. 2003.\n-\tP.7: “can be combined with any sampling and pruning method in other solvers” -> Add references for such sampling & pruning methods.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review for \"Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes\"",
            "review": "Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to the combinatorial nature of such a selection process, it is computationally intractable to integrate the perception decision with the planning decision. \n\nThe author proposes a new form of POMDPs called AP2-POMDP, which takes active perception into account. The AP2-POMDP problem restricts the maximum number of sensors that can be selected by an agent. The agent also faces the planning problem to select the sensors. To prevent such expansion of the action space, the authors propose a greedy strategy for observation selection and obtain a near optimal bound based on submodular optimization.\n\nThe author also proposes a greedy-based scheme for the agent to find an almost optimal active perception action by minimizing the uncertainty of beliefs. The author also uses theories to prove the near-optimal guarantees of this greedy method. The author also proposes a novel perception-aware point-based value iteration to calculate the value function and obtain the policy. The author also operates an interesting simulation experiment, which shows less uncertainty of the robot when taking planning actions when using the proposed solver.\n\nThe contribution is significant in reinforcement community. The writing is in general clear. It can be improved with minor modifications, for example, explaining math equations better in English. \n\nMy main comment for the authors is whether they have considered the scenario where the perception and the planning actions are connected. I agree with the authors that the best strategy for perception is to reduce uncertainty (and indeed, the greedy approach yields a near-optimal performance), given the restricted situation that the perception and planning are two separated processes. Nonetheless, in most real-world applications, the two processes are coupled, and therefore, we face, immediately, the trade-off between exploration and exploitation. I wonder if the authors have considered how they can extend their approach to such scenarios. \n\nA few minor comments:\n\n\t\n(i) The authors should add a legend and perhaps, more explanation in the captions of Figure 5. The colors of the heat-map are confusing. If dark blue and dark red represent lowest and highest frequency, what about other colors? Are there obstacles placed in the grid? If so, are they placed as shown in Figure 3(b)?\n\t\n(ii) What is the effect of k, the maximum number of sensors to be placed? Can the authors provide a figure showing the change of performance with varying k?\n\t\n(iii) It will be more convincing if the author deploys this algorithm to real-world robots and demonstrate its effectiveness. \t\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}