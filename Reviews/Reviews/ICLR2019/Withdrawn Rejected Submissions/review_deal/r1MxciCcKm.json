{
    "Decision": {
        "metareview": "I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG. While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at ICLR.\n\nIn the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. It is important to describe how the parameters are tuned. Given the additional hyper-parameters, one may consider giving all of the algorithms the same budget of hyper-parameter tuning. I also agree with reviewers that the policy gradient baseline seems to underperform typical results. One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper-parameters.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Borderline paper"
    },
    "Reviews": [
        {
            "title": "AR3 Review: Connecting the Dots Between MLE and RL for Sequence Generation",
            "review": "The authors propose a more unified view of disparate methods for training sequence models. Specifically, a multi-term objective L(q,theta) consisting of:\n1) The standard reward maximization objective of policy gradient, E_{p_\\theta}[R], \n2) A weighted (weight alpha) reverse KL divergence of the parametric policy and a non-parameteric policy q, \n3) A weighted (weight beta) entropy term on q, \nIs proposed for sequence training (see equation (1). L can be iteratively optimized by solving for q given p_\\theta, and the \\theta given q (see eq. 2).\n\nThis framework mathematically generalizes softmax-policy gradient (SPG, alpha=1, beta=0), and reward-augmented maximum likelihood (alpha=0, beta=temperature), and also standard entropy regularized policy gradient (alpha=0), among other algorithms.\n\nThe paper is well written, and the approach sensible. However, combining SPG and RAML by introducing their respective regularization terms is a rather straightforward exercise, and so seems quite incremental.\n\nOther major concerns are:\n1) the true utility of the model, and \n2) the integrity of the experiments. \n\nWrt: \n1), While RAML was a significant contribution at the time, it is now well established that RAML generally doesn't perform well at all in practice due to exposure bias (not conditioning on it's own previous predictions during training). Moreover SPG, as the authors point out, was supposed to address the need for ML pre-training, but required much engineering to work. The fact is that REINFORCE-based policy gradient methods are still more effective than these methods, provided they have a good baseline to reduce varince. Which brings me to point \n2) Was MIXER run with a learned baseline and judiciously optimized? Table 1 suggests that MIXER can outpeform ML by only 0.1 Bleu points, and outpeformed by RAML? Something is wrong with your implementation then. Moreover, there are techniques like self-critical sequence training (SCST), which far outpeform MIXER, and we haven't even discussed Actor-Critic baselines...\n\nIn summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Also, a paper on essentially the same approach was submitted and rejected from ICLR 2018(https://openreview.net/pdf?id=H1Nyf7W0Z), although this paper is better written, and puts the method more fully in context with existing work, I think that several of the concerns with that paper apply here as well. \n\nLook forward to the authors' feedback, and additional/corrected results - will certainly update my score if these concerns are addressed. In particular, if this generalization can significantly outpeform existing methods it generalizes with non-degenerate settings, this would overcome the more incremental contribution of combining SPG and RAML.\n\nCurrent Ratings:\n\nEvaluation      2/5: Results are not consistent with previous results (e.g. MIXER results). Stronger baselines such as SCST and AC are not considered.\nClarity         5/5: Clear paper, well written.\nSignificance    3/5: RAML and SPG have not been established as important methods in practice, so combining them is less interesting.\nOriginality     2/5: RAML and SPG are fairly straightforward to combine for experts interested in these methods.\n\nRating          4/10 Okay but not good enough, reject.    \nConfidence      5/5\n\nPros: \n- Generalizes RAML and SPG (and also standard entropy-regularized policy gradient).\n- Well written paper, clean generalization.\nCons:\n- RAML and SPG have not been established as important methods in practice.\n- generalization of RAML and SPG is straightforward,  incremental.\n- Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML)\n- Stronger REINFORCE-based algorthms like SCST, as well as Actor-critic algorithms, have not been compared.\n\nUpdate after author responses:\n--------------------------------------------\n\nAuthors, thank you for your feedback.\n\nWhile it is true that generalizing RAML and SPG into a common framework is not trivial, the presented framework simply augments the dual form of SPG (i.e. REPS [16] in the SPG paper) with a RAML term. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.\n\nWrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. However, the unusally poor performance of MIXER remains unexplained, and falls short even of scheduled sampling (SS), which suggests a lingering major issue. REINFORCE techniques rely on 1) strong baselines, verified by the authors, 2) larger batch sizes to reduce variance, and 3) pre-training to reduce variance and facilitate efficient exploration. If the MLE is undertrained or overtrained (the latter the more likely issue given the plots), then MIXER will perform poorly. Actually, it is now standard practice to pre-train with MLE+SS before RL training, and this is really the (also dynamically weighted objective) baseline that should be compared against. The current REINFORCE results (MIXER or otherwise) really need to be updated (or at the least removed, as they are not captured by the framework, but the comparison to PG methods is important!).\n\nMore generally, I feel that the experiments are not yet comprehensive enough. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is.\n\nOverall I feel that the paper is still in need of substantial maturation before publication, although I have revised my score slightly upward.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting theoretical contribution",
            "review": "The authors provide a common mathematical perspective on several learning algorithms for sequence models. They also introduce a new algorithm that combines several of the existing ones and achieves significant (but small) improvements on a machine translation and a text summarization task.\n\nThe paper is clearly written, giving a good exposition of the unifying formulation.\n\nI believe the paper is quite insightful, and contributes to the community's understanding of the learning algorithms. However, the improvements in their experiments are rather small, and could probably be improved with more experimental work. They do showcase the usefulness of their new formulation, but not very strongly. Thus my recommendation to accept the paper is mostly based on the theoretical content that opens an interesting new perspective.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting unifying perspective, but lacklustre experiments",
            "review": "This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. This enables insightful new interpretations of standard issues in MLE training in terms of exploration for instance.\nBased on this new perspective, a new algorithm is introduced. Its performance is analysed on a machine translation and a text summarisation task.\n\n==> Quality and clarity\nThe paper is overall well-written, although it can be improved upon (see details below). The bibliography for instance does not reference the conference/journals where the articles were published and lists many (>10) published papers as arXiv preprints.\n\nThe ideas are clearly presented, which is crucial in a paper trying to unify different approaches, and the new perspective on exploration is well motivated.\n\n==> Originality and significance\nThe unifying framework is interesting, and helps shed new light on some standard issues in sequence generation.\nOn the other hand, the new algorithm and its analysis seem like a slightly rushed attempt at leveraging the unifying framework. \nThe experiments, in particular, present several issues.\n- For instance, it's clear from Figure 3 that both MLE and RAML are overfitting and would benefit from more dropout (in the literature, 0.3 is commonly used for this type of encoder-decoder architecture). Having access to these experimental results is important, since it would enable the reader to understand whether the benefits of the new approach are subsumed by regularisation or not.\n- Further, the performance of the competing methods seems a bit low. MLE reports 26.44 BLEU, which is a bit surprising considering that: \n   - with beam-search (beam of size 10, not 5, admittedly), Bahdanau et al (2016) get 27.56 BLEU, and this is without dropout.   \n   - with dropout 0.3 (but without beam search), Leblond et al (2018) get 27.4 BLEU.\nMaking a strong case for the benefits of the new algorithm requires more thorough experiments.\n\nOverall, the first half of the paper is interesting and insightful, while the second would benefit from more time. \n\nPros\n- clarity of the ideas that are presented\n- interesting unifying perspective on sequence generation algorithms\n- insightful new interpretations of existing algorithms in terms of exploration\n\nCons\n- the example new algorithm is not very original\n- the associated experiments are incomplete\n\n==> Details\n1. page 2, \"Dayan & Hinton (1997); Levine (2018); Abdolmaleki et al. (2018) study in a probabilistic inference perspective.\" is an incomplete sentence.\n2. at the beginning of section 3.1, policy optimisation is a family of algorithmS\n3. page 7 in the setup of the experiments, \"We use the Adam optimizer for SGD training\" is incorrect since SGD is not a family but a specific algorithm, which is different from Adam.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}