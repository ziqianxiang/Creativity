{
    "Decision": {
        "metareview": "The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Lack of clarity makes it difficult to discern a clear contribution over recent literature"
    },
    "Reviews": [
        {
            "title": "Some good ideas, many issues",
            "review": "The authors prove some theoretical results under the mean field regime and support their conclusions with a small number of experiments. Their central argument is that a correlation curve that leads to sub-exponential correlation convergence (edge of chaos) can still lead to rapid convergence if the rate is e.g. quadratic. They show that this is the case for ReLU and argue that we must ensure not only sub-exponential convergence, but also have a correlation curve that is close to the identity everywhere. They suggest activation functions that attain conditions as laid out in propositions 4/5 as an alternative.\n\nThe paper has many flaws:\n- the value of the theoretical results is unclear\n- the paper contains many statements that are either incorrect or overly sweeping\n- the experimental setup and results are questionnable\n\nTheoretical results:\n**Proposition 1: pretty trivial, not much value in itself\n**Proposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result.\n**Proposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence \"fast\"? Is it \"slow\"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic? Do those activation functions exist? It would be good to compare this result against similar results for other activation functions. For example, do swish / SeLU etc. have a convergence rate that is less than quadratic?\n**Proposition 4: The conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function, let alone how one could generate new activation functions that satisfy these conditions. In fact, for an arbitrary nonlinearity, verifying the conditions of proposition 4 seems harder than verifying f(x) - x \\approx 0 directly. Hence, proposition 4 has little to no value. Further, it is not even clear whether f(x) - x \\approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not.\n**Proposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much.\n**Proposition 6: True, but the fact that we have f(x) - x \\approx 0 for swish when q is small is kind of obvious. When q is small, \\phi_swish(x) \\approx 0.5x, and so swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via propposition 4 to realize this.\n\nPresentation issues:\n- While I understand the point figures 1, 2 and 4b are trying to make, I don't understand what those figures actually depict. They are insufficiently labeled. For example, what does each axis represent?\n- You claim that for ReLU, EOC = {(0,\\sqrt{2})}. But this is not true. By definition 2, EOC is a subset of D_\\phi,var. But {(0,\\sqrt{2})} is not in D_\\phi,var, because it simply leaves all variances unchanged and does not cause them to converge to a single value. You acknowledge this by saying \"For this class of activation functions, we see (Proposition 2) that the variance is unchanged (qal = qa1) on the EOC, so that q does not formally exist in the sense that the limit of qal depends on a. However,this does not impact the analysis of the correlations.\" Section 2 is full of complicated definitions and technical results. If you expect the reader to plow through them all, then you should really stick to those definitions from then on. Declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation. This problem becomes even worse in section 3.2, where it is not clear which definition is actually used for EOC in your main result (prop 4), making prop 4 even harder to parse than it already is.\n\nCorrectness issues:\n- \"In this chaotic regime, it has been observed in Schoenholz et al. (2017) that the correlations converge to some random value c < 1\" Actually, the correlation converges deterministically, so c is not random.\n- \"This means that very close inputs (in terms of correlation) lead to very different outputs. Therefore, in the chaotic phase, the output function of the neural network is non-continuous everywhere.\" Actually, the function computed by a plain tanh network is continuous everywhere. I think you mean something like \"the output can change drastically under small changes to the input\". But this concept is not the same as discontinuity, which has an established formal definition.\n- \"In unreported experiments, we observed that numerical convergence towards 1 for l ≥ 50 on the EOC.\" Covergence of a sequence is a property of the limit of the sequence, and not of the 50th element. This statement makes no sense. Also if you give a subjective interpretation of those experimental results, you should present the actual results first.\n- \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions.\" This statement is very vague and sweeping. Also, one could argue that the fact that ReLU is much more popular and tends to give better results than tanh in practice disproves the statement outright.\n- \"Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions. However, these functions suffer from the vanishing gradient problem during back-propagation\" At the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \\chi_1=1, but \\chi_1 is also the rate of growth of the gradient. Pascanu et al (2013) discussed vanishing gradients in RNNs, which is a different story.\n- \"Other activation functions that have been shown to outperform empirically ReLU such as ELU (Clevert et al. (2016)), SELU (Klambauer et al. (2017)) and Softplus also satisfy the conditions of Proposition 4 (see Supplementary Material for ELU).\" Firstly, SeLU does not satisfy proposition 4. f(x) \\approx x requires \\phi to be close to a linear function in the range where the pre-activations occur. Since SeLU has a kink at 0, it cannot be close to a linear function no matter how small the pre-activations are. Secondly, softplus also doesn't satisfy proposition 4, as \\phi(0) = 0 does not hold. Thirdly, this statement is too sweeping. If ELU / SELU / Softplus \"outperform\" ReLU, why is ReLU still used in practice? At best, those nonlinearities have been shown to outperform in a few scenarios.\n- \"We proved in Section 3.2 that the Tanh activation guarantees better information propagation through the network when initialized on the EOC.\" Prop 4 only applies in the limit as \\sigma_b converges to 0. So you can't claim that you showed tanh as \"better information propagation\" in general.\n- \"However, for deeper networks (L ≥ 40), Tanh is stuck at a very low test accuracy, this is due to the fact that a lot of parameters remain essentially unchanged because the gradient is very small.\" But in figure 6b the accuracy for tanh is decreasing rapidly, so therefore the parameters are not remaining \"essentially unchanged\", as this would also cause the accuracy to remain essentially unchanged. Also, if the parameter changes are too small ... why not increase the learning rate?\n- \"To obtain much richer priors, our results indicate that we need to select not only parameters (σb , σw ) on the EOC but also an activation function satisfying Proposition 4.\" Prop 4 only applies when \\sigma_b is small, so you additionally need to make sure \\sigma_b small.\n- \"In the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value.\" But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success.\n\nExperimental issues:\n- \"We use the Adam optimizer with learning rate lr = 0.001.\" You must tune the learning rate independently for each architecture for an ubiased comparison.\n- In figure 6b, why does tanh start with a high accuracy and end up with a low accuracy? I've never seen a training curve like this ... This suggests something is wrong with your setup.\n- You should run more experiments with a larger variety of activation functions.\n\nMinor comments: \n- \"Therefore, it is easy to see that for any (σb , σw ) such that F is increasing and admits at least one fixed point,wehaveKφ,corr(σb,σw) ≥ qwhereqistheminimalfixedpoint;i.e. q := min{x : F(x) = x}.\" I believe this statement is true, but I also think it requires more justification.\n- At the end of page 3, I think \\epsilon_r should be \\epsilon_q\n\nThere are some good ideas here, but they need to be developed/refined/polished much further before publication. The above (non-exhaustive) list of issues will hopefully be helpful for this.\n\n\n### Addendum ###\nAfter an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \\phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \\phi(x) gets too close to a linear function. (Many other criticisms also remain.)\n\nThe one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid \"structural vanishing gradients\", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from \"regular\" vanishing gradients (as shown by \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" and \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting solid work but few concerns remain",
            "review": "Studying properties of random networks in the infinite width limit, this work suggests guidance for choosing initialization and activation function. \n\nIn my opinion, novel contribution comes for guidance for choosing activation functions and theoretical grounds for superior performance of ```'swish’ activation function. \n\nI have two main concerns :\n\nIn terms of selection on initialization, the findings seem to be mostly discussed already in Schoenholz et al (2017) [1]. In their work, Edge of Chaos is critical line separating different phases and was already shown to have power-law decay rather than exponential decay. As far as I can tell, analysis on EOC on ReLU-like activations are different from Schoenholz et al (2017) [1]. Some of the results for ReLU are already appeared in the literature e.g. Lee et al (2018) [2].\n\nAnother main concern is in the author’s experimental setup. It is hard to draw conclusions when comparison experiments were done with a fixed learning rate.  As we know learning rate is one of the most critical hyperparameter for determining performance and optimal learning rate is often sensitive to architecture choice. Especially for different non-linearity and different depth/width optimal learning rate can change.\n\nPros: \n - Clearly written and easy to understand what authors are trying to say \n - Interesting theoretical support for activation function which recently got attention due to boosting performance in neural networks\n - Nice suggestion of choosing activation function for deep networks (Proposition 4)\n      -- ELU/SELU/Softplus/Swish all satisfy this suggestion\nCons:\n - Novelty may be not strong enough as the standard analysis tool from [1] was mostly used\n - Experimental setup may suffer from some critical flaw \n\nFew comments/questions:\n- P3: Is M_{ReLU} = 2 correct, from ReLU EOC, shouldn’t it be ½?\n- For all the works using activation functions satisfying Proposition 4 (ELU/SELU/Softplus/Swish), the initialization scheme close to EOC? Does this work’s analysis actually explain performance boost over ReLU for these activation functions?\n\n[1] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[2] J. Lee, Y. Bahri, R. Novak, S.S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. 6th International Conference on Learning Representations, 2018.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results; providing some insights on the selection of activation function.  ",
            "review": "Good results; providing some insights on the selection of activation function.  \n\nThis paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.)\nThe two principal results of this paper are \n1. Initializing the network critically on the edge of chaos.  \n2. Identifying some conditions on the activation functions which allow good \"information flow\" through the network.   \n\nThe first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. \n\nThe second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.  \n\n\n\ncons:\n1. I don't think the experimental results are convincing enough for the reasons below:\n    1.1. All experiments are conducted over MNIST with testing accuracy around 96%.  The authors should consider using large datasets (at least Cifar10).\n    1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine.   \n    1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" \n\n\n2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. \n\n3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. \n\n4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically.  \n\n\nIn sum, the paper has some interesting theoretical results but the empirical results are not convincing.  \n\n\nOther comments:\n1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. \n2. Consider replacing \"Proposition 4\" by  \"Theorem\", since it is the main result of the paper.  \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}