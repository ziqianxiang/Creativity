{
    "Decision": {
        "metareview": "The paper provides a novel analysis of the robustness to adversarial attacks in network representation learning. It appears to be a useful contribution for important class of models; however,  the detailed reviews (1 and 2) raise some concerns that may require a bit of further work (though partially addressed in revised version).",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Novel analysis of an important problem, but needs some improvements"
    },
    "Reviews": [
        {
            "title": "Nice first try but needs improvement",
            "review": "The topic of this paper is interesting; however, the significance of the work can be improved.  I recommend that the authors test the vulnerability of node embeddings on various random graph models.  Examples of random graph models include Erdos-Renyi, Stochastic Kronecker Graph, Configuration Model with power-law degree distribution, Barabasi-Albert, Watts-Strogatz, Hyperbolic Graphs, Block Two-level Erdos-Renyi, etc.  That way we can learn what types of networks are more susceptible to attacks on random-walk based node embeddings and perhaps look into why some are more vulnerable than others.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting but somewhat incomplete paper on poisoning graph embeddings.",
            "review": "This paper is a timely work on poisoning random walk based graph embedding methods. In particular, it shows how to derive a surrogate loss function for DeepWalk. Even though the analysis method and the algorithm proposed is somewhat loose, I think this paper do a good contribution towards the adversarial attack problem for important models.\n\nThere are still some room to improve in this paper. One problem is that since the paper proposes a surrogate loss L_{DW3}, it would be natural to analysis its gap from L_{DW1}. In this paper I can only see some empirical results on this issue. Another issue is that the algorithm the paper used is another approximation towards L_{DW3} by an additional sampling method. And the overall strategy can be far away from the true optimal solution for maximizing L_{DW3}. Still there's no analysis on that issue. A potential drawback for the method proposed is that its complexity is O(NE), which can be quite expensive when the graph is big, and # edges is \\Omega(NlogN).\n\nThe experiments in this paper is convincing. It seems that the method proposed is way better than its competitors when removing edges. Is there any further intuition on that? Why the method is not so good when we add edges? Moreover, the black-box attack scenario requires more justification. What is the relative performance gain for A_{DW3} against other attacks in the black-box setting?\n\nOverall, this paper targets on an important issue in machine learning. My main concern is that it leaves too many questions behind their algorithms. Still some effort is required to improve the paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "a novel adversarial attack on node embedding method based on random walks",
            "review": "Brief Summary:\nThe authors present a novel adversarial attack on node embedding method based on random walks. They focus on perturbing the structure of the network. Because the bi-level optimization problem can be highly challenging, they refer to factorize a random walk matrix which is proved equivalent to DeepWalk. The experimental results show that their approach can effectively mislead the node embedding from multi-classification. \n\nQuality:\nThis paper is well-written except for some minor spelling mistakes\n\nClarity:\nThis paper is quite clear and easy to follow.\n\nOriginality:\nThis work follow the proposal Qiu et al.(WSDM'18)'s proof and present a novel approach to calculate the loss when the network changes by A'.\n\nPros:\n1. Detailed proofs presented in appendix\n2. They present 6 questions and answer them with effective experiments.\n3. They present a new way to attack node embedding by factorizing a equivalent matrix.\n\nCons:\n1. I have noticed that ZÃ¼gner et al.(KDD'18) present an adversarial attack method on GCN for graph data. I think it is reachable by the day the authors submitted this paper. This is opposite to the first sentence \"Since this is the first work considering adversarial attacks on node embeddings there are no known\nbaselines\" said in Section 4.\n2. The author present the time analysis of their approach but the efficiency result of their approach is not presented.\n3. To enforce misclassification of the target node t, the author set the candidate flip edges as edges around t. Does this mean only the node's local edges can mislead the target node from downstream tasks? I think the authors should consider more candidate edges but this may lead to larger time complexity.\n4. Figure. 4 tells that low-degree nodes are easier to mis-classify. If the baseline method B_{rnd} randomly select edges to flip among the local area of node t, I think the result should be similar to the proposed approach on low-degree nodes because the flipped edges should be the same. \n\n== I have read the rebuttal. Thanks for the response.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}