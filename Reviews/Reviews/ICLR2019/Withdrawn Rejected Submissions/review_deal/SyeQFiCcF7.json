{
    "Decision": {
        "metareview": "The paper extends capsule networks with a pairwise learning objective and evaluates on small face verification datasets. The authors do a great job describing prior work, but lack clarity when articulating their contribution and proposed method. In addition, some important implementation details, such as hyperparameter selection, are missing causing further confusion as to the final approach. Overall, according to the experiments shown, the approach offers modest improvements over prior work.\n\nThe approach offers an interesting and promising direction. We encourage the authors to revise the manuscript to clarify their approach and contribution and to improve their evaluation by including the relevant metrics and implementation details. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Incremental extension of capsule networks with results on face verification"
    },
    "Reviews": [
        {
            "title": "Limited Contribution, Unclear results",
            "review": "Authors present an adaptation of Capsule Networks for pairwise learning tasks. The pose vectors of final capsule layers for each tower is concatenated and passed through a fully connected layer to calculate the embedding for each tower's input. Then a contrastive loss based on a distance metric is optimized for the embeddings. An interesting regularizer (?) is used which is a dropout based on Capsule activation for connecting last layer capsules to the fully connected layer. \n\nPros:\n\nThe literature review is rich and complete. In general authors explain details of the previous techniques as they use them too which is a good writing technique and improves the readability.\n\nBy utilizing Capsules authors avoid a rigorous preprocessing as it is common with the community. As I understand they do not even use face landmarks to align images.\n\nMeasured by the optimized loss, the proposed method achieves significant improvement upon baseline in the small At&t dataset.\n\nCons:\n\nThe contribution of this work is not on par with ICLR standard for conference papers. Specially since SDropCapsNet (the added dropout) is seems to be auxiliary (gives a slight boost only in LFW without double margin).\n\nThe method used for reporting results is unjustified and not compareable to prior work. For face verification, identification one should report at least an ROC curve based on a threshold on the distance or nearest neighbor identification results which are standards in the literature. Where as they only report the contrastive loss of their model and their own implementation of baselines and Figure 3 which does not clearly show any advantage for CapsNet Siamese networks.\n\n\nQuestion:\nThe architecture description for last layer is vague. In text 512 is mentioned as the input dimmension, 512 is 16*32, Figure 1 shows 9 features per capsule or 3x3 kernels over capsules where it has to be fully connected? Also it says last layer is 128 dimmension where the text implies it should be 20. Could you please explain the role of 20?\n\nIs table 1 the final contrastive loss achievable for each model?\n\nHave you tried just gating the pose parameters of last layer by their activation (multiply to the tanh) rather than using a stochastic dropout?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper has some new ideas about Capsule Network",
            "review": "In this paper, the author extends Capsule Network on the task of face verification to solve the problem of learning from only few examples and speeding the convergence. They propose a Siamese Capsule Network, which extends Capsule Networks to the pairwise learning setting with a feature l2-normalized contrastive loss that maximizes inter-class variance and minimizes intra-class variance. Here is a list of suggestions that will help the authors to improve this paper.\n1.\tThe pairwise learning setting allow learning relationships between whole entity encodings \n2.\tThe ability to learn from little data that can perform few-shot learning where instances from new classes arise during testing\n3.\tWhen a contrastive loss is used that takes face embeddings in the form of encoded capsule pose vectors, speed of converging is lifted.\n4.\tThe description of experiment is too brief to show specific details.\n5.\tThe figure of Siamese Capsule Network Architecture (figure 1) cannot show kernel of author(s)â€™s method, and lack explanation in the paper.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good direction for research on capsules, but results too weak and idea too incremental",
            "review": "This paper presents an extension of Capsule Networks, Siamese Capsule Networks (SCNs), that can be applied to the problem of face verification. Results are reported on the small AT&T dataset and the LFW dataset. \n\nI like the direction that this paper is taking. The original Capsules work has been looking at fairly simple and small scale datasets, and the natural next step for this approach is to start addressing harder datasets, LFW being one of them. Also face verification is a natural problem to look at with Capsules.\n\nHowever, I think this paper currently falls short of what I would expect from an ICLR paper. First, the results are not particularly impressive. Indeed, SCN doesn't outperform AlexNet on LFW (the most interesting dataset in the experiments). Also, I'm personally not particularly compelled by the use of the contrastive loss as the measure of performance, as it is sensitive to the scaling of the particular representation f(x) used to compute distances. Looking at accuracy (as in other face verification papers, such as DeepFace) for instance would have been more appropriate, in my opinion. I'm also worried about how hyper-parameters were selected. There are A LOT of hyper-parameters involved (loss function hyper-parameters, architecture hyper-parameters, optimizer hyper-parameters) and not much is said about how these were chosen. It is mentioned that cross validation was used to select some margin hyper-parameters, but results in Table 1 are also cross-validation results, which makes me wonder whether hyper-parameters were tuned on the performance reported in Table 1 (which of course would be biased).\n\nThe paper is also pretty hard to read. I recognize that there is a lot of complicated literature to cover (e.g. prior work on Capsule Networks has introduced variations on various aspects which are each complicated to describe). But as it currently reads, I can honestly say that I'm not 100% sure what exactly was implemented, i.e. which components of previous Capsule Networks were actually used in the experiments and which weren't. For example, I wasn't able to figure out which routing mechanism was used in this paper. The paper would strongly benefit from more explicitly laying out the exact definition of SCN, perhaps at the expense of enumerating all the other variants of capsules and losses that previous work has used.\n\nFinally, regardless of the clarify of the paper, the novelty in extending Capsule Networks to a siamese architecture is arguably pretty incremental. This wouldn't be too much of a problem if the experimental results were strong, but unfortunately it isn't the case.\n\nIn summary:\n\nPros\n- New extension of Capsule Networks, tackling a more challenging problem than previous work\n\nCons\n- Novelty is incremental\n- Paper lacks clarity and is hard to read\n- Results are underwhelming\n\nFor these reasons, I'm afraid I can't recommend this paper be accepted.\n\nFinally, I've noted the following typos:\n- hinton1985shape => use proper reference\n- within in => within\n- that represent => that represents\n- a Iterated => an Iterated\n- is got => is obtained\n- followed two => followed by two\n- enocded => encoded\n- a a pair => a pair\n- such that to => such as to\n- there 1680 subjects => there are 1680 subjects\n- of varied amount => of the varied amount\n- are used many => are used in many\n- across the paper: lots of in-text references should be in parenthesis\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}