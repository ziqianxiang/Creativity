{
    "Decision": {
        "metareview": "The submission proposes a setting of two agents, one of them probing the other (the latter being the \"demonstrator\"). The probing is done in a way that learns to imitate the expert's behavior, with some curiosity-driven reward that maximizes the chance that the probing agents has the expert do trajectories that the probing agent hasn't seen before.\n\nAll the reviewers found the idea and experiments interesting. The major concern is whether the setup and the environments are too contrived. At least 2 reviewers commented on the fact that the environments/dataset seemed engineered for success of the given method, which is a concern about how this method would generalize to something other than the proposed setup.\n\nI also share the concern with R3 regarding the practicality of the proposed method: it is not obvious to me what problems this would actually be *useful* for, given that the method requires online interaction with an expert agent in order to succeed. The space of such scenarios where we can continuously probe an expert agent many many times for free/cheap is very small and frankly I'm not entirely sure why you would need to do imitation learning in that case at all (if the method was shown to work using only a state, rather than requiring a state/action pair from the expert, then maybe it'd be more useful).\n\nIt's a tough call, but despite the nice results and interesting ideas, I think the method lacks generality and practical utility/significance and thus at this point I cannot recommend acceptance in its current form.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "1) Summary\nThis paper proposes a method for learning an agent by interacting and probing an expert agents behavior. This method is composed of a policy that learns to imitate an expert’s action, and a policy that challenges the expert in order to get it to take multiple possible routes to solve a task. The two policies share a “behavior tracker” that models the expert’s behavior, and communicates it to both policies being learned. The probing policy is optimized using a curiosity-driven reward in order to get the expert take trajectories the probing policy has not seen before. In experiments, the authors perform experiments to show how the learned agent can generalize to unseen configurations in the corresponding environments in which the agents were trained, and also use the proposed technique in a sorting task in which the method generalizes to longer arrays to be sorted.\n\n\n2) Pros:\n+ Neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).\n+ Cool experiments for applicability.\n+ Well written paper and easy to understand.\n\n3 Comments:\n- Equation 1 typo?:\nTo my understanding, in curiosity driven exploration, the exploration is driven based on how well the next state can be predicted by the agent. In equation 1, different time steps are being compared, m^t and m^{t-1}, but the comparison should be between the predicted time step t and real time step t. Can the authors clarify why different time steps are compared in the equation?\n\n- Baseline missing: Random actions from expert\nA simple baseline to compare against could be to simply force the expert to take a few random actions during its trajectory and let the imitator learn from these. Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.\n\n- Baseline missing: Simple RNN policies that communicate hidden states.\nAnother baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states. While optimizing the curiosity reward the hidden states could be used as well. If successful, this baseline can show that we actually need to model the “behavior” with a separate network.\n\n- Ablation study for the importance of fusion:\nThe authors have a “fusion” layer within the imitator and probing policies. An ablation study showing that this layer is actually necessary is missing from the paper.\n\n- Generalizability argument\nThe authors claim that they show a single starting configuration for the agents during training, and different starting configurations during testing. While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment. It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert. A more drastic change of the environment could make for a stronger argument. \n\n\n4) Conclusion:\nOverall, I like the idea of having a policy that tries to figure out the general behavior of a demonstrator by probing it. Having said that, I feel this paper needs to improve in the aspects mentioned above. If the authors present more convincing evidence that successfully address the comments above, I am willing to increase my score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice work, more details and some references to previous work needed",
            "review": "The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator’s policy by carrying out actions eliciting strong changes in the demonstrator’s trajectory. The former is implemented as imitation learning, i.e. policy learning, the latter as curiosity driven RL.\n\nThe authors are encouraged to review some of the related literature on optimal teaching, which also has developed a rich set of approaches to agent modeling, e.g. the work by Patrick Shafto. It may also be relevant to think about the relationship to active learning in IRL. \n\nI am not sure whether I would be able to implement and reproduce the presented work on the basis of the current manuscript including the appendix. It would be very helpful for the community to be able to do so. E.g., details on the the training of the demonstrators, their reward functions, and the behavior tracker. Particularly the \"fusion\" module remains extremely unclear.\n\nOverall, this is a nice paper, despite the fact that the example domains and problems considered are engineered strongly to allow for the proposed algorithm to be useful. Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d. I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society, though. \n\nMinor points:\n“differs from this in two folds”\n“by generate queries”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "\"Interactive Agent Modeling by Learning to Probe\" provides an interesting approach to improve imitation learning",
            "review": "This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent. The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning. The mind of the demonstrating agent is modeled as a latent space representation from a neural net. This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior. The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods. \n\nGeneral comments, in no particular order:\n\n1. The authors should provide more details on how the hand-crafted demonstrator agents were made. I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task? \n\n2. The demonstrated tasks are (gridworld and algorithmic) which are very simple RL taks with low-dimensional (non-visual) state-spaces.  It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco. \n\n3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps. Because the latent vector is modeled as a non-linear function, distances between latent vector representations do not necessarily correspond to similar distances between behavior policies (for example, KL distances between two policy distributions). Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors. \n\n4. The biggest flaw that I see in this method is the practicality of it's use. This method relies on the ability to obtain or gain access to a demonstration agent to learn from. In very simple tasks, such as the one presented here, the authors were able to hard-code their own demonstration agent. However, in harder tasks, this will not be feasible. If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.  In reality, for sufficiently difficult tasks, a human would be the demonstration agent (as is done in most robotics tasks). In practice, imitation learning from a human works well since the learning can be done offline (i.e., post-hoc after a set of demonstrations are collected from the human). However, this task requires the learning to be interactive and thus the demonstrator needs to be present during the learning.  Interactively learning from a human becomes a problem if the learning takes tens of thousands of episodes of training since a human cannot reasonably be expected to be present for that amount of time. Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?\n\n5. My previous comment relates mainly to the application of improved imitation learning. However, I do think this is still very useful in the context of multi-agent reinforcement learning for collaborative and competitive tasks (sections 4.6 and 4.7). I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy. \n\nOverall, I think the paper presents a really nice idea of how to improve modeling of agents. essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations.   This work sounds novel to me from a reinforcement learning perspective, however, I'm not well versed on theory of mind research. \n\n\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning to model other static agents in the environment. Compelling idea but limited evaluation.",
            "review": "The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment. The method builds on imitation learning (behavioural cloning) to model the agent’s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours. Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent’s environment. While learning to model the target agent’s inner state, the RL reward is generated based on the difference of the target agent’s inner state between consecutive time steps.\n\nThe approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines. Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours. It would be highly beneficial to evaluate these aspects. Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness. \n\nOverall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4). \n\nMinor issues:\n- Reward formulations for the baselines as part of the appendix.\n- Same scale for the y-axes across figures\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}