{
    "Decision": {
        "metareview": "This paper proposes an amortized proximal optimization method to adapt optimization hyperparameters. Empirical results on many problems are performed. \n\nReviewers overall find the ideas interesting, however there still are some questions whether strong baselines are used in the experimental comparisons. The reviewers also point that the theoretical results are not useful ones since the assumptions are not satisfied in practice. One of the reviewer increased their score, but the other has maintained that the paper requires more work.\n\nThe presentation of the result is also a bit problematic; the font sizes in the figure are too small to read.\n\nThe paper contains interesting ideas, but it does not make the bar for acceptance in ICLR. Therefore I recommend a reject. I encourage the authors to resubmit this work after improving the presentation and experiments.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea but does not make the bar."
    },
    "Reviews": [
        {
            "title": "Interesting and Novel contribution - Some concerns that need to be answered regarding experiments and theory",
            "review": "Summary:\nThis paper introduces Amortized Proximal Optimization (APO) that optimizes a proximal objective at each optimization step. The optimization hyperparameters are optimized to best minimize the proximal objective. \n\nThe objective is represented using a regularization style parameter lambda and a distance metric D that, depending on its definition, reduces the optimization procedure to Gauss-Newton, General Gauss Newton or Natural Gradient Descent.\n\nThere are two key convergence results which are dependent on the meta-objective being optimized directly which, while not practical, gives some insight into the inner workings of the algorithm. The first result indicates strong convergence when using the Euclidean distance as the distance measure D. The second result shows strong convergence when D is set as the Bregman divergence. \n\nThe algorithm optimizes the base optimizer on a number of domains and shows state-of-the-art results over a grid search of the hyperparameters on the same optimizer.\n\n\nClarity and Quality: The paper is well written. \n\nOriginality: It appears to be a novel application of meta-learning. I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well. Also how does this compare to adaptive hyperparameter training techniques such as population based training?\n\nSignificance:\nOverall it appears to be a novel and interesting contribution. I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques. Also, your convergence results appear to rely on strong convexity of the loss. How is this a reasonable assumption? These are my major concerns. \n\nQuestion: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Update baselines",
            "review": "The paper proposes an approach to adapt hyperparameters online. \nWhen learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures. \nA. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning. Thus, it is hard to say whether the results are applicable in practice. \nB. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate. \nC. Your method involves a hyperparameter to be tuned which affects the shape of the schedule. This hyperparameter itself benefits from (requires?) some scheduling. \n\nIt would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes. Online tuning of  hyperparameters is an important functionality and I hope your paper will make it more straightforward to use it in practice. \n\n\n* Minor notes:\n\nYou mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it. I guess you mean w.r.t. the original RMSprop. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and intuitive idea but needs more clarification",
            "review": "I raised my rating. After the rebuttal.\n\n- the authors address most of my concerns.\n- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.\n- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.\n\n-------------------------------------------------------------\nThis paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters. The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.\n\n1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters. \n- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters. After all, newton method and natural gradients method are not used in experiments.\n- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed. \n\n2. No need to write so much decorated bounds in section 3. The convergence analysis is on Z, not on parameters x and hyper-parameters theta. So, bounds here can not be used to explain empirical observations in Section 5. \n\n3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?\n\n4. Authors have done a good comparison in the context of deep nets.  However,\n- could the authors compare with changing step-size? In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates. Is it better to decay learning rates for toy data sets? It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems. \n- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001\". What are reasons for these?\n- In Section 5.2, it is said lambda is tuned by grid-search. Tuning a good lambda v.s. tuning a good step-size, which one costs more?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}