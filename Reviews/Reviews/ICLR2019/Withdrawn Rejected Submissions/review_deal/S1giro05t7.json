{
    "Decision": {
        "metareview": "The paper proposes methods to deal with estimating classification confidence on unseen data distributions. \n\nThe reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) the authors' new comparison with Guo et al. (2017) asked by Reviewer 2 is not convincing enough.\n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need new ideas to meet the high standard of ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited novelty"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper proposes two ideas for reducing overconfident wrong predictions:\n- Method 1: “G-distillation” of an ensemble with extra unsupervised data\n- Method 2: Novelty Confidence reduction (NCR) using novelty detector\n\nThe paper is well-written and was a pleasure to read. In particular, I really enjoyed reading the introduction and related work. My main concern is that some of the contributions claimed were already shown in previous work (see method 1 below for details), and the novelty feels a bit limited. That said, I like the simplicity of the method and think that the extensive experiments on a variety of datasets and architectures is useful to the community.\n\nMethod 1:\n- The paper claims “Draw attention to a counter-intuitive yet important problem of highly confident wrong predictions when samples are drawn from a unknown distribution that is different than training” as one of the contributions. Note that previous work has already shown that single models are overconfident on unknown classes and ensembles are less overconfident, e.g. see Section 3.5 of the paper: \nSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\nhttps://arxiv.org/pdf/1612.01474.pdf\n- If I understand correctly, the key difference is that the proposed method 1 also uses ensemble prediction on unlabeled data for distillation, which could make the distilled model more robust. Ensembling on unlabeled data for robustness does seem novel to me, however, the text needs to be updated to clarify the novelty.\n\n\nMethod 2:\n- By off-the-shelf, do you mean a pre-trained network released by ODIN? Or did you train ODIN-based novelty detector on your dataset? \n- There was a recent paper that proposed to reduce confidence on novel inputs, which might be worth discussing:\nReliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors\nhttps://arxiv.org/pdf/1807.09289.pdf\n\n\nMinor issues:\n- Figures 3,4 are a bit small and hard to see\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A marginally novel method to estimate classification confidence on novel data distributions; Experimental results need to be more comprehensive and they are not conclusive enough. ",
            "review": "The authors proposed two methods to deal with estimating classification confidence on novel unseen data distributions. The first idea is to use ensemble methods as the base approach that helps in identifying uncertain cases and then using distillation methods to reduce the ensemble into a single model mimicking behavior of the ensemble. They propose a generalization of this idea, that is to also perform distillation on a more generic unsupervised data distribution (than the supervised one that is used in training the ensemble). It is not clear whether this distribution should overlap with the novel distribution as a requirement or not. The second idea is to use a novelty detector classifier and weight the network output by the novelty score. \nMy major concern is that the comparison doesn't seem to be sufficiently comprehensive. The main method that is used to compare against is (Kendall & Gal, 2017), in which the main aim seems to be reducing uncertainty and improving generalization error under i.i.d. assumptions. This is different from the main focus of the paper, which is to better estimate classification confidence on novel data distributions. It seems that other approaches, such as \"Calibration methods\" (Guo et al. 2017) are better aligned with the focus of the paper, and should be considered instead. \nMy other concern is that the novelty seems to be marginal: either extending distillation methods in a very natural form, or weighting the network output using a novelty detector. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "simple regularization reduces overconfidence",
            "review": "This paper introduces two methods of adjusting the overconfidence error for predictions on novel data. The ensemble distillation approach is to penalize the distillation loss on a potentially unlabeled general dataset.  The second approach (NCR) detects the novelty first and reweigh the prediction based on the familiarity to training data. \n\n*stationarity*\nFrom the statistical perspective, the overconfidence of extrapolation can kick in from two sources: a)the epistemic uncertainty.  The point estimation of softmax ignores the uncertainty of prediction at all.  A full Bayesian approach will remedy this though computationally impractical.  b) the generative distribution p(y|x) might not be identical on training data and test data.  To see the difference, if the training sample size goes to infinity, the uncertainty in a) will go to zero, but b) may still exist.  Section 3 assumes the invariant p(y|x) in novel data.  But theoretically, both methods do not require such invariance?\n\nSlightly related here, there can be novel data for classification, and in principle, there can also be novel data for novelty detection?  That will make NCR fail.\n\n*why the distillation helps uncertainty adjustment*\nI am not convinced how the g-distillation works for this task.  In the extreme case if the ensemble model itself is totally wrong for novel data and the unlabeled general data used in training, how can I learn any extra uncertainty information from that noise? To be fair, when the temperature goes high enough, the ensemble will make uniform prediction and then the distillation loss is merely a loss function that enforces uniformity.  If I replace the ensemble softmax by a uniform prior for unlabeled general data, do I achieve the same effect?  That is essentially the same regularization as method 2, except g-distillation is on logit scale.  \n\n*robustness-accuracy tradeoff*\nThe experiments do not reveal too much robustness-efficiency conflict, as the new methods still perform good enough on familiar dataset. Indeed they can be even better than the baseline in E99 loss. Does it suggest the over-confidence is even a concern for familiar data/ iid data?\n\nIn general, the paper is well-written and well-motivated. It would be more interesting to make some theoretical explanation why/when this simple approach works.   I would recommend a weak accept at this point.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}