{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting engineering work, but rather limited novelties",
            "review": "This manuscript introduces a method, called siamese network (SN), to identify duplicate and copied/modified images, which can be used to improve surveillance of the published and in-peer-review literature. The topic is interesting and the authors showcased that their method can be used for a side application of semantic segmentation as well. \nThe paper is a very interesting engineering article. But its novelties and merits seem limited and rather incremental. There are several unclear points in the method, and more importantly it is not clear where the method stands among the various previous works. There are no proper comparison with previous works nor enough ablation studies to support the claims provided in the paper.\n-\tThere are three parallel streams of CNN in the proposed 3-branch network. Are they sharing parameters? If No, why not! It seems like they need to be learning similar set of features sharing parameters is the most feasible approach. There are no discussions on this, nor any analyses in the experiments. Whereas for the previous work on the same topic (Chopra et al. (2005)) has a two-branch network that shares parameters.  \n-\tThe results suggest that SN may not be good enough to be used as a fully autonomous forensic detector, but it can narrow down the pool of images which can be subjected to further human review or interventions. The question here will be how different this method is compared to all previous similar research (some of which introduced in Section 2). There are absolutely no comparisons with any previous/other methods (neither theoretically nor experimentally). Why do we need another method that does almost the same as previous works? If it is better than the previous ones, this should come out from the paper. \n-\tThe results section is very brief and does not include proper comparisons. Even for the semantic segmentation, the authors only compared with U-Net, and their results are just comparable with it, not any better. U-Net now is not even close to the state-of-the-art for semantic segmentation, especially for the datasets used in the paper (for cell and nuclei segmentation). \n-\tOne of the main issues with this line of research is that the dataset is not public and, probably due to legal issues, it may not become public. Therefore, future comparisons and reproducibility of the research may be of concern. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Application of triplet loss to duplicate detection, lack of novelty.",
            "review": "The paper presents an application of deep convolutional networks for the task\nof duplicate image detection.\n\nIt is a pure application paper and the triplet loss very well known in the\ncommunity for retrieval and ranking.\nThe generation of the near-duplicate images using random transformations is\nalso quite standard. However, given the application domain the authors should\nexplain why some parameters have been chosen rather than others. I assume the\nlevel of variability here is not that large and the space can be constrained.\n\nThe parameter alpha in the triplet loss could be omitted as it can be\n\"absorbed\" by the network. Also why didn't the author comment on their choice\nof such loss? The original one has a margin for example.\n\nI am afraid that the paper lacks the necessary rigor in the experimental\nsection. There are no baselines and comparisons to other losses such as the\nsiamese loss, that would seem easier and more appropriate for this task given\nthe binary outcome.\n\nI have to admit that I don't quite understand the motivation of the work,\nto me it seems that standard techniques would be already quite successful here.\nFor example classic feature matching with SIFT, to which I encourage the authors\nto compare to.\n\nTo summarize I think the paper still requires lot of work before being considered\nfor publication.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Application paper w/o research contributions.",
            "review": "This work adresses the problem of finding duplicate or near duplicate images from biomedical publications. The authors propose standard CNN and loss functions and apply it to this field. The evaluation is performed on an in-house dataset and synthetic data.\n\nNovelty:\nThere are no significant new contributions other than the application to this very narrow field. The experimental results don't appear to be very conclusive wrt the usability of this proposed approach.\n\nClarity:\nThe paper is written well and mostly clear. Eq 1 does not explain what the sum over k is about. Eq 2/3 seems to have a typo and should be f^01, f^02.\n\nExperiments:\nIt is not clear what the classes in Sec 4 refer to, and more specifically what the distinction between classes and categories is. It seems that categories are different types of images whereas classes are different transformations of images in the same category. If this is the case it doesn't seem very instructive to compare the accuracy across categories.\n\nFig3 a/b shows a scatter plot, I believe that a ROC curve would be much more instructive here.\n\nIt is not clear what exactly was compared when comparing with U-Net. The authors mention that their models train faster and don't require labelled data, but some more details of what exactly was compared would be helpful. E.g. what exactly was pre-trained and fine-tuned.\n\nOverall this work does not seem suitable for ICLR due to the narrow application, lack of positive results and no scientific contributions beyond what's already widely used in the vision community.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}