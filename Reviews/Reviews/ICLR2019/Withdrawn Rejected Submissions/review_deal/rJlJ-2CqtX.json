{
    "Decision": {
        "metareview": "Strengths:  The paper introduces a novel constrained-optimization method for RL problems.\nA lower-bound constraint can be imposed on the return (cumulative reward), \nwhile optimizing one or more other costs, such as control effort. \nThe method learns multiple \nThe paper is clearly written.  Results are shown on the cart-and-pole, a humanoid, and a realistic Minitaur \nquadruped model.  AC: Being able to learn conditional constraints is an interesting direction.\n\nWeaknesses:  There are often simpler ways to solve the problem of high-amplitude, high-frequency \ncontrols in the setting of robotics.  \nThe paper removes one hyperparameter (lambda) but then introduces another (beta), although beta\nis likely easier to tune. The ideas have some strong connections to existing work in \nsafe reinforcement learning.\nAC: Video results for the humanoid and cart-and-pole examples would have been useful to see.\n\nSummary:   The paper makes progress on ideas that are fairly involved to explore and use \n(perhaps limiting their use in the short term), but that have potential, \ni.e., learning state-dependent Lagrange multipliers for constrained RL. The paper is perfectly fine\ntechnically, and does break some new ground in putting a particular set of pieces together. \nAs articulated by two of the reviewers, from a pragmatic perspective, the results are not \nyet entirely compelling. I do believe that a better understanding of working with constrained RL,\nin ways that are somewhat different than those used in Safe RL work.  \n\nGiven the remaining muted enthusiasm of two of the reviewers, and in the absence of further\ncalibration, the AC leans marginally towards a reject. Current scores: 5,6,7.\nAgain, the paper does have novelty, although it's a pretty intricate setup.\nThe AC would be happy to revisit upon global recalibration.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "some novelty;  muted endorsements; solid writing and results; revisit?"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper proposes a model free reinforcement learning algorithm with constraint on reward, with demonstration on cartpole and quadruped locomotion. \n\nstrength: (1) challenging examples like the quadruped.\n                (2) result seems to indicate the method is effective\n\nThere are several things I would like the authors to clarify:\n(1) In section 3.2, why is solving (4) would give a \"exactly the desired trade-off between reward and cost\"? First of all, how is the desired trade-off defined? And how is (4) solved exactly? If it is solved iteratively, i.e, alternating between the inner min and outer max, then during the inner loop, wouldn't the optimal value for \\lambda be infinity when constrained is violated (which will be the case at the beginning)? And when the constrained is satisfied, wouldn't \\lambda = 0? How do you make sure the constrained will still be satisfied during the outer loop since it will not incurred penalty(\\lambda=0). Even if you have a lower bound on \\lambda, this is introducing additional hyperparameter, while the purpose of the paper is to eliminate hyperparamter?\n(2) In section 3.2, equation 6. This is clearly not a convex combination of Qr-Vr and Qc, since convex combination requires nonnegative coefficients. The subtitle is scale invariance, and I cannot find what is the invariance here (in fact, the word invariance\\invariant only appears once in the paper). By changing the parametrization, you are no longer solving the original problem (equation 4), since in equation (4), the only thing that is related to \\lambda is (Qr-Vr), and in (6), you introduce \\lambda to Qc as well. How is this change justified?\n(3)If I am not mistaken, the constrained can still be violated with your method. While from the result it seems your method outperforms manually selecting weights to do trade off,  I don't get an insight on why this automatic way to do tradeoff is better. And this goes back to \"exactly the desired trade-off between reward and cost\" in point(1), how is this defined?\n(3) The comparison in the cartpole experiment doesn't seem fair at all, since the baseline controller is not optimized for energy, there is no reason why it would be comparable to one that is optimized for energy. And why would a controller \" switch between maximum and minimum actuation is indeed the optimal solution\" after swingup? Maybe it is \"a\" optimal solution, but wouldn't a controller that does nothing is more optimal(assuming there is no disturbance)?\n(4)For Table I, the error column is misleading. If I understand correctly, exceeding the lower bound is not an error (If I am wrong, please clarify it in the paper). And it is interesting that for target=0.3, the energy consumption is actually the lowest.\n(5)Another simple way to impose constrained would be to terminate the episode and give large penalty, it will be interesting to see such comparison.\n\nminor points: \n* is usually used for optimal value, but is used in the paper as a bound.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "value constrained model-free continuous control",
            "review": "This paper uses constrained Markov decision processes to solve a multi-objective problem that aims to find the correct trade-off between cost and return in continuous control. The main technique is Lagrangian relaxation and experiments are focus on cart-pole and locomotion task.\n\nComments:\n\n1) How to solve the constrained problem (8) is unclear. It is prefer to provide detailed description or pseudocode for this step.\n\n2) In equation (8), lambda is a trade-off between cost and return. Optimization on lambda reduces burdensome hyperparameter selection, but a new hyperparameter beta is introduced. How do we choose a proper beta, and will the algorithm be sensitive to beta?\n\n3) The paper only conducts comparison experiments with fixed-alpha baselines. The topic is similar to safe reinforcement learning. Including the comparison with safe reinforcement learning algorithms is more convincing.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks. The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward. This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers). The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.\n\nStrengths:\n+ The paper is generally clear and readable.\n+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.\n\nMajor concern:\n- My biggest concern is that the technical contributions of the paper are not clear at all. The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control). The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)). The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either. Overall, the paper does not make a compelling case for the novelty of the problem or approach.\n\nOther concerns:\n- For the cart-pole task, the paper states that the reward is modified \"to exclude any cost objective\". Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this). I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.\n- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.\n- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent. However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied). \n\nTypos:\n- Pg. 5, Section 3.4: \"...this is would achieve...\"\n- Pg. 6: ...thedse value of 90...\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}