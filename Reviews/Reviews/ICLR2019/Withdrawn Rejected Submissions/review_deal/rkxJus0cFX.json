{
    "Decision": {
        "metareview": "This paper proposed Residual Gradient Compression as a promising approach to reduce the synchronization cost of gradients in a distributed settings. It provides a useful approach that works for a number of models. The reviewers have a consensus that the quality is below acceptance standard due to practicality of experiments and lack of contribution.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "limited contribution"
    },
    "Reviews": [
        {
            "title": "Good analysis and provides empirical value of gradient compression",
            "review": "Paper focuses on Residual Gradient Compression (RGC) as a promising approach to reducing the synchronization cost of gradients in a distributed settings. Prior approaches focus on the theoretical value of good compression rates without looking into the overall cost of the changes. This paper introduces RedSync that builds on the existing approaches by picking the most appropriate ones that reduce the overall cost for gradient reduction without unduly focusing on the compression rate.\nThe paper does this by providing an analysis of the cost of RGC and also the limitations in scaling as the bandwidth required grows with the number of nodes. It also highlights the value of applying different algorithms in this process for compression and the benefits and issues with each.\n\nPros:\n- Useful analysis that will help direct research in this area\n- Shows that this approach works for models that have a high communication to computation ratio\n- Provides a useful approach that works for a number of models\n\nCons:\n- Positive experimental results are on models that are typically not used in practice e.g. AlexNet and VGG16\n- Speedups shown on LSTMs don't see worthwhile to scale, and in practice a model-parallelism approach may scale better\n\nCorrections:\n- Typo in notes for Table 1 last sentence RCG => RGC\n- Typo in first sentence in section 3.2: RedSycn => RedSync\n- Section 3.3, #2 last sentence: maybe overdrafts => overshadows ?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "RedSync should implement a more systematic approach for optimization. ",
            "review": "This paper introduces a set of implementation optimizations for minimizing communication overhead and thereby reducing the training time in distributed settings. The method relies on existing gradient compression and pruning techniques and is tested on synchronous/data-parallel settings. \n\nThe contribution and impact of the paper is unclear. The authors claim implementation innovations that show true performance gains of gradient compression techniques. But again it is unclear what those innovations are and how they can be reused for accelerating training for a new model.\n\nThe authors did perform an extensive set of experiments and while the method works well for some models and batch sizes, it doesn't work well for some other models. What would make the paper much more compelling would be if it came up with ways to systematically explore the relationship between training batch size, model parameter size, communication/computation/decompression ratos, and based on these properties, it can come up with best strategies to accelerate distributed data parallel training for any new model. \n\nThe paper needs to be polished as it has multiple typos. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good implementation optimizations in a important practical problem, but relatively incremental contribution",
            "review": "Quality and clarity:\nThe paper proposes an approach to reduce the communication bandwidth and overhead in distributed deep learning. The approach leverages on previous work (mainly the residual gradient compression (RGC) algorithm), and proposes several implementation optimizations. From what I can read, it is the basic RGC algorithm that is used, but with some clever optimization to improve the performance of it. \n\nThe quality of the paper is good, it is well-written and easy to read. The evaluation of the proposed approach is well done, using several diverse datasets and models, and executed on two different parallel systems. However, the reasons why RGC and qRGC sometimes have better accuracy than SGD needs to be analyzed and explained. \n\nOriginality and significance:\nThe originality of the paper is relatively low (optimization of an existing algorithm) and the contributions are incremental. However, the paper addresses an important practical problem in distributed learning, and thus can have a significant practical impact on how distributed deep learning systems are implemented.\n\nPros:\n* Addresses an important issue. \n* Good performance.\n* Good evaluation on two different systems. \n\nCons:\n* Limited contribution. Although I like implementation papers (very important), I think the contribution is to low for ICLR.\n\nMinor:\n* In general, the figures are hard to read (the main problem is to small text)\n* Compression in the title is slightly misleading, since it's mainly selection that is done (top-0.1% gradients). Although the values are packed in a data structure for transmission, it's not compression in a information theory perspective.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}