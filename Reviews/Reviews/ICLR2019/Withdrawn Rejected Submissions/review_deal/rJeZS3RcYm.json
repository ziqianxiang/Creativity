{
    "Decision": {
        "metareview": "The paper considers a procedure for the generation of adversarial examples under a black box setting. The authors claim simplicity as one of the main selling points, with which reviewers agreed, while also noting that the results were impressive or \"promising\". There were concerns over novelty and some confusion over the contribution compared to Guo et al, which I believe has been clarified.\n\nThe highest confidence reviewer (AnonReviewer2), a researcher with significant expertise in adversarial examples, raised issues of inconsistent threat models (and therefore unfair comparisons regarding query efficiency), missing baselines. A misunderstanding about comparison against a concurrent submission to ICLR 2019 was resolved on the basis that the relevant results are mentioned but not originally presented in the concurrent submission. \n\nWhile I disagree with AnonReviewer2 that results on attacking a particular image from previous work (when run against the Google Cloud Vision API) would be informative, the reviewer has remaining unaddressed concerns about the fairness of comparison (comparing against results reported in previous work rather than re-run in the same setting), and rightly points out that as many variables should be controlled for as possible when making comparisons. Running all methods under the same experimental setting with the same *collection* of query images is therefore appropriate. \n\nThe authors have not responded to AnonReviewer2's updated post-rebuttal review, and with the remaining sticking point of fairness of comparison with respect to query efficiency I must recommend rejection at this point in time, while noting that all reviewers considered the method promising; I thus would expect to see the method successfully published in the near future once issues of the experimental protocol have been solidified.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A method that is appealing for its simplicity, but reviewer concerns regarding fairness of comparison persist."
    },
    "Reviews": [
        {
            "title": "simple algorithm, intriguing message",
            "review": "This paper demonstrates that a simple greedy random search algorithm in DCT space based on score feedback is able to synthesize adversarial examples with quite good query efficiency.  The algorithm is demonstrated on ImageNet  with three common architectures, showing much higher efficiency when sampling from the DCT basis. The algorithm is also shown to outperform state of the art attacks in terms of query count. Finally, a successful attack is demonstrated on Google Cloud Vision.\n\nWhile not particularly heavy on technicalities, this work does make a couple intriguing points, namely that adversarial attacks can potentially be quite easy to perform due to the inherent nature of high-dimensional classification, and that the space is in which the search is perform might be more important than the sophistication of the search itself. I interpret the proposal not so much as a claim to a state-of-the-art algorithm (even though the results are impressive) but as a very reasonable baseline in the evaluation of attack efficiency -- one might even wonder why it has not been common practice thus far to evaluate against such kinds of algorithms by default. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "simple and effective blackbox attack based on random directions",
            "review": "This paper presents a simple and effective black box adversarial attack on sota deep nets for image classification tasks. It is based on randomly picking a low frequency component of the DC Transform.  It is claimed to be most efficient when compared to the sota methods in terms of number of queries required for the attack. It is shown that a median of 600 queries for resnet-50 for imagenet dataset, and 2500 for google cloud vision. Due to its simplicity, it is also claimed that the attack is quite simple to implement in code. The paper presents a detailed analysis of their attack in pixel and DCT space, targeted vs untargeted attack, comparison over different architecture such as Densenet, resnet, and inception.\n\nThough the work is quite important and presents a simple and effective baseline black box attack. My concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of Guo etal 2018, which this paper says is the motivation behind their work. So, it is not clear what is the contribution of this paper, as a similar study seems to have been carried out in that paper as well. The authors do not clearly give the relative comparison wrt Guo etal 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting black-box adversarial attack using DCT basis; performance evaluation on targeted attack is insufficient and threat model is inconsistent (unfair comparison)",
            "review": "This paper proposed a simple query-efficient \"score-based\" black-box attack based on iteratively perturbing an input image with a direction randomly sampled (w/o replacement) from a set of orthonormal bases. In particular, the authors proposed the use of low-frequency parts of DCT (discrete cosine transformation) as in (Guo 2018) to perform this task. Experimental results on ImageNet and three different classification models demonstrate the query efficiency of the proposed method -- able to achieve high attack success rate within fewer query budgets, where the visual distortion has an L2 norm threshold set to be 10. The authors also demonstrate an untargeted score-based black-box attack on Google CloudVision API.\n\nWhile the results seem promising, there are several issues that may potentially weaken the query-efficient claims made in this paper, especially due to the lack of sufficient attack comparisons (on smaller datasets) and inconsistent threat models when compared to existing works. My main concerns are summarized as follows.\n\n1. Unfair comparison due to inconsistent threat models (knowledge known to an attacker): the proposed method (simBA) is a \"score-based\" black-box attack, not a \"decision-based\" black-box attack. The proposed method assumes knowing the prediction likelihood (or prediction score) as the model output when performing black-box attacks, whereas the compared methods in black-box settings, such as Opt Attack and Boundary Attack, are \"decision-based\" attack that assumes only knowing the top-1 prediction label. Therefore, the query count comparison is meaningless and unfair, since these two methods require far less information from the model.\n\nOn the other hand, ZOO/AutoZOOM is a score-based attack. But ZOO can achieve a very low L2 distortion due to its coordinate descent nature. A fair comparison is to set the same L2 distortion for all score-based methods, and compare the median/avg query counts of each image to reach the same L2 distortion. The comparison to Opt-Attack / Boundary attack makes sense only if the proposed method (simBA) can also perform decision-based attack. Nonetheless, the query count to same-distortion comparison argument still holds. The authors should specify whether simBA can apply to the decision-only attack scenario. If so, how to implement and what is the performance?\n\nLastly, the QL attack (Ilyas 2018) can perform both score-based and decision-based attacks. So the authors should make the query comparison (to same L2 distortion) as well. According to a recent report (Table-1, NES column) in https://arxiv.org/pdf/1807.07978.pdf, the QL-attack has a comparable performance in terms of query counts as reported in this paper.\n\n2. More experiments on targeted black-box attacks: While untargeted attacks on Imagenet is a relatively easy task, I was a bit skeptical on the attacking performance of simBA in targeted attacks - since the selection of low-frequency bases directly limits the search space of adversarial examples, as opposed to arbitrary random directions adopted in QL-attack, Boundary-attack, and Opt-attack. It is also not clear how the target label is chosen in the targeted attack experiment.\nI suggest including two more experiments to validate the function of simBA: (i) compare the performance of least-likely targeted attack (ii) show results on smaller datasets such as Cifar-10. As pointed out by the authors, Imagenet has too many image dimensions and make it more vulnerable to attack. Showing attacking results on smaller datasets can properly justify the value of the proposed attack, rather than the benefit from high dimensionality.\n\n\n3. Novelty relative to LFBA (Guo) should be better differentiated: The idea of using DCT is originated from the LFBA paper. Since in that paper the authors also leveraged low-frequency DCT to perform black-box attacks, it is not clear to me what makes the proposed method perform better than the LFBA paper. The novelty and difference between this paper and the LFBA paper should be addressed.\n\n4. The Google Cloud Vision API attack is not too appealing - the tree label is still there and the trees are obviously present in the picture, while I appreciate the effect of removing the original top-3 labels. Can the authors show another set of non-trivial (more surprising) and targeted-attack experiments? Or simply do the same experiment using the same image (men snowing -> dog)  as in the QL-attack.\n\n----\nPost-rebuttal review\n\nI appreciate the authors' efforts in clarifying some of my concerns. However, I am still not convinced the comparison has been made fair. Many numbers from Table 1, such as ZOO, Opt-attack, QL-attack and AutoZOOM seem to be directly adapted from the papers rather than implemented and reproduced based on the same setting as the proposed attack. In particular, given that QL-attack is a published work, one of the state-of-the-art method and its codes has been released, I would really love to see a direct comparison using the same data samples and threat model. I would also like to emphasize that implementing all attacks under the same setting is crucial, since different attack methods may have a different criterion to determine attack successfulness. For example, QL-attack has some pre-defined distortion (L2 or Linfinity) for determining an adversarial example is successful, in addition to a different predicted class.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}