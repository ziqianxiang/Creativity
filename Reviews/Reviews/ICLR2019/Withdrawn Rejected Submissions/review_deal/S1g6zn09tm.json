{
    "Decision": "",
    "Reviews": [
        {
            "title": "Conditional convolutional nets with transformations at latent space. Similar models have been studied before.",
            "review": "This paper proposed a conditional convolutional net for image synthesis. The main novelty, as the authors argued, is to apply transformations at latent space, with the weights of these transformations are also learned. The authors applied this model to a number of view synthesis tasks. \n\nThe paper is complete and in general well-written. The proposed system works reasonably well. I suggest the authors explain how the CTU mappings are learned in the main paper (step 2 of the workflow in section 3), instead of in the supp. This will help readers better understand the proposed CTU.\n\nMy main concern is the authors seem unaware of the many papers that have studied the problem a while ago. The idea of simultaneously learning features and transformation parameters was explored at NIPS 2016 with various names---dynamic filter networks [A], spatial transformer predictors [B], and cross convolutional networks [C]. While the first two learn transformation parameters applied to the image space, cross convolutional networks learn features and transformation parameters in latent space. It’s unclear to me how novel this paper is given these prior methods. They should at least be cited and discussed, and ideally compared with.\n \nMy second concern is the proposed CTU mappings are task-specific, and need to be hand-designed given the data. This limits the applicability of the system. This is maybe why results on mostly on carefully designed synthetic data. It’d be great to see results on real, in-the-wild images, as those NIPS 2016 papers did.\n\n[A] Dynamic Filter Networks. NIPS’16.\n[B] Unsupervised Learning for Physical Interaction through Video Prediction. NIPS’16.\n[C] Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks. NIPS’16.\n\n== post rebuttal ==\n\nThe authors didn't address my concerns. I've lowered my rating to 2.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but crucial parts seem under-explained, despite the paper's length",
            "review": "This paper proposes a network architecture for conditional view synthesis. The task here is: given an image, and a condition variable representing a target viewpoint, render the content of the image under the target viewpoint. The idea is to first run the input image through an encoder, and then use the condition variable to transform this encoding to the target location in the latent space, and then decode the transformed encoding into the target image. This formulation allows the usage of new unsupervised losses in the embedding space. The paper also proposes a new output space for image generation, which splits the normal RGB space into subtasks: value, scaling, and global color balance, which improves performance. This is learned with supervised input-output pairs, along with adversarial and smoothness losses.\n\nMy main problem with this paper is its presentation. The paper uses a half-page for pseudocode that belongs in the appendix, and another half-page (Figure 2) displaying architectures that the method does NOT use. The whole paper is about image generation, and yet the paper does not show a single generated image until the appendix. This needs rearrangement. The text also has problems, which are maybe more critical. \n- The paper says it \"uses\" conditioning information to \"select which collection of weights\" of the CTU get applied to the encoded input. How exactly does this happen? Is there some attention mechanism doing a soft selection of weights? Or are the weight sets defined before any training begins, such that channels 0-31 are for View1, 32-63 are for View2, and so on? That second interpretation seems implied by Figure 1, but I could not find verification for it, even in the appendix. If this second interpretation is correct, then why not simply say that there is an independent CTU per viewpoint? \n- What is the architecture of the CTU? The main text says it is \"a collection of convolutional layers\", but the appendix contradicts this saying \"This CTU is implemented as a convolutional layer\" (i.e., a single layer). The CDU is described as similar to the CTU, so I am unclear about its architecture as well. \n- The appendix introduces (!) a noise vector to the architecture. What is this noise vector doing? Pix2Pix and similar papers show that a noise vector usually goes ignored by the decoder. Furthermore, the usage seems antithetical to the main method, since the paper argues against simple concatenation of conditioning information.\n\nOverall, I do like the paper, and the conditional latent space transformation seems useful, but it is a frustrating read. Critical questions that arise from the method's initial introduction go unanswered all the way past the 30th page.\n\n\n--------\n\nPost rebuttal: I lowered my rating from 5 to 4. While the paper's presentation improved during the rebuttal, I lost confidence in the method's novelty, thanks to prior work brought up by AnonReviewer3.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review ",
            "review": "This paper preposed a conditional generative model for image generation. The proposed contributions include a conditional transformation unit that learns the latent space transformations corresponding to specified target views, a consistency loss term to guide the encoding, a task-divided decoder for generation refinement, and an adaptive discriminator.  Below are the evaluations.\n\nAbout the novelty of the proposed conditional transformation unit. Please be more specific about its novelty and significant difference compared with existing ones, for example, https://arxiv.org/pdf/1807.04812.pdf , https://academic.oup.com/bioinformatics/article/34/17/i603/5093214 , etc.\n\nAbout the consistency loss term, what type of consistency do you claim? Any proof or evaluation of the consistency?\n\nFor the ablation analysis, any result for the model without generation refinement?\n\nThe claim of lightweight and real-time solution needs more evidence. For the experiment, is there any comparison on training time? Although the proposed model is claimed to be a lightweight neural network that is suitable for real-time applications, it seems to me the parameters to train still have much bigger size than baselines. Also all the training are done using GPU. Not many real-time applications have GPU resources.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}