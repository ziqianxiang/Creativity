{
    "Decision": {
        "metareview": "The proposed “input forgetting” problem is interesting, and the reflective likelihood can come to be seen as a natural solution, however the reviewers overall are concerned about the rigor of the paper. Reviewer 2 pointed out a technical flaw and this was addressed, however the reviewers remain unconvinced about the theoretical justification for the approach. One suggestion made by reviewer 1 is to focus on simpler models that can be studied more rigorously. Alternatively, it could be useful to focus on stronger empirical results. The method works in the experiments given, but for example in the imbalanced data experiments, only MLE is compared to as a baseline. I think it would be more convincing to compare against stronger baselines from the literature. If they are orthogonal to the choice of estimator, then it would be even better to show that these baselines + RLL outperforms the baselines + MLE. Alternatively, you mention some challenging tasks like seq2seq, where a convincing demonstration would greatly strengthen the paper. While the paper is not yet ready in its current form, it seems like a promising approach that is worth further exploration.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting problem and approach, but not quite rigorous enough."
    },
    "Reviews": [
        {
            "title": "Interesting ideas that need further refinement",
            "review": "Summary:\n\nThis paper proposes maximizing the “reflective likelihood,” which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term.  The paper derives the reflective likelihood for classification models and unsupervised latent variable models.  Choices for \\alpha are also discussed, and connections are made to ranking losses.  Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling.\n\nPros:\n\nI like how the paper frames the reflective likelihood as a ranking loss.  It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed ‘focus’ the model on the dependent relationship y|x.  Can this be further formalized?  I would be very interested in seeing a derivation of this kind.    \n\nI like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration.\n\nCons:\n\nMy biggest issue with the paper is that I find much of the discussion lacks rigor.  I followed the argument through to Equation 3, but then I became confused when the discussion turned to ‘dependence paths’: “we want our learning procedure to follow the dependence path—the subspace in Θ for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3—the independence path” (p 3).  What are these dependence paths?  Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3?  Surely any conditional model x-->y has a ‘dependence path’ flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure?\n\nMoving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I’m not sure how exactly that final form was settled upon.  There seems to be a connection to maximum entropy methods?  That is,   E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well.  Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy.  As there is a long history of maximum entropy methods / classifiers, I’m surprised there were no mentions or references to this literature.  Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent).  I’m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references.   \n\nThe data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for.  I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood.  As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word ‘calibration’ is never mentioned.  More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus.  It would be great to show that this method provides good calibration directly during optimization and doesn’t need the post-hoc calibration steps that most methods require. \n\nEvaluation:  While the paper has some interesting ideas, they are not well defined, making the paper unready for publication.  Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper’s argument.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "the paper is technically flawed",
            "review": "This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation:  d – p_data; d(y|x) – p_d(y|x); m(y|x) – p_theta(y|x)\n\nmax E_x~d E_y~d(y|x) [ log m (y|x) ]                 \t\t\t\t               (1) \nmax E_x~d { E_y~d(y|x) ) [ log d(y|x) ]}  -  E_y~d(y|x) [ log m (y|x) ]}        (2)\nmax { E_y~d [  log  (y) ]  -  E_y~d  log E_x~d(x|y) [ m (y|x) ]}                        (3)\n\nFirst error is that the “max” in (2) and (3) should be “min”. I will assume this minor error is corrected in the following.\nThe equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term  in (2) does not depend on model.  The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following:\n\n min { E_y~d [  log d (y) ]  -  E_y~d  E_x~d(x|y) [ log m (y|x) ]}                     (3’)\n\nNotice the swap of “E_x” and “log”. By Jensen’s nequality, we have \n\n log E_x~d(x|y)  m (y|x) ]  > E_x~d(x|y) [ log m (y|x)\n -  E_y~d  log E_x~d(x|y)  [ m (y|x) ]    < -  E_y~d  E_x~d(x|y) [ log m (y|x) ]                    \n\nSo, minimizing (3) amounts to minimizing a lower bound of the correct objective (3’). It does not make sense at all.\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "potentially interesting idea, but very confusing in current form",
            "review": "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).\n\nThere is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.\n\n- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.\n\n- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}