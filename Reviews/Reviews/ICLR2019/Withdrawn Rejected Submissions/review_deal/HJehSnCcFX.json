{
    "Decision": {
        "metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review for neural Hawkes particle smoothing paper"
    },
    "Reviews": [
        {
            "title": "This paper proposes an algorithm for missing data problem in continuous time events data (ie, point processes) where both past and future events are helpful. ",
            "review": "This paper tackles a very important and practical problem in event stream planning. The problem is very interesting and the approach taken is standard.\n\nThe presentation of the paper is not clear enough. The notations and definitions and methods are presented in a complicated way. It's difficult to follows.\n\nFrom the contribution point of view the paper looks like to be a combination of several existing and well developed approach: Neural Hawkes Process + particle smoothing + minimum bayes risk + alignment. It's not very surprising to see these elements together. It would have helped if the authors made it clear why each part is chosen and clearly state what is the novelty and contributed of the paper to the field.\n\nThe paper in its current format is not ready for publication. But it's a good paper and can be turned to a good paper for the next venue.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting problem with weak experimental evaluation",
            "review": "The authors propose a particle smoothing approach with an approximate minimum Bayes risk decoder to impute missing events in the Neural Hawkes Process (NHP). The main goal is to address the missing events problem in continuous-time event analysis, which is an important problem in practice. The core idea is within the framework of particle smoothing. \n\nTo formulate the posterior distribution of the missing event, the authors consider both the left-to-right past events and the right-to-left future events. The paper first applies the NHP to capture both the observed and inferred missing events to learn a representation of the past events, and then uses a similar NHP to learn the representation of the observed events from the future. Based on the two representations, it then formulates the intensity function of the missing events and uses the thinning algorithm to sample different particles. Based on the proposed distribution, the paper also considers to decode a single prediction achieving the Minimum Bayes Risk. Experiments on synthetic datasets with 10 different initializations and two real datasets show that the proposed smoothing approach is better than the filtering baseline. \n\nIn general, this paper considers an important problem which is under active research in literature recently. However, there are a few weaknesses of the paper that should be addressed. \n\n1. The proposed technique is tightly connected to NHP, which could limit the applicability of the approach to other temporal point processes. The essential idea is similar to Bi-LSTM to learn the representation from both ends of a sequence of asynchronous temporal events. There are several different ways to represent the inter-event time to feed into the network other than NHP. Can the proposed method also be applied to other processes?\n\n2. Within the particle filtering framework, each particle (hypothesis) is weighted by the likelihood of the sequence of observed events under that hypothesis. It turns out that the integral part of Equation 5 does not have an obvious analytical solution under NHP. Then, we first need a set of samples to approximate the likelihood evaluation. Later, we also need to sample particles. I am not quite convinced the computational efficiency of this approach in real applications of practice. Also, there is no analysis either empirically or analytically about the impact of the accumulative sampling errors on the inference performance. Furthermore, to learn the proposed distribution, the paper applies the REINFORCE algorithm under the proposed distribution q. But REINFORCE is known for large variance issue. Given that we already need lots of samples for the likelihood, it is unclear to me how stable the algorithm could be in practice.\n\n3. The experimental evaluation is weak. For particle filtering and smoothing, it is known that the filtering techniques are candidates for solving the smoothing problem but perform poorly when T is large. That's why it is necessary to develop more sophisticated strategies for good smoothing\nalgorithms. As a result, it is unfair to only compare the smoothing approach with the filtering baseline. \n\nActually, what people really care about is how different techniques can behave in real data to impute realistic missing events. From this perspective, I suggest to use the QQ-plot to evaluate the goodness of fitting on the synthetic dataset. For example, given a sequence of events generated from an independent temporal point process, we can randomly delete events, and then apply different techniques, including Linderman et al. (2017), Shelton et al.(2018), to impute missing events. Finally, we can compare the imputed sequence of events with the groundtruth. \n\nIn addition, sequential monte carlo approach often suffers from skewed particle issue where one particle gradually dominates all the other particles with no diversity. It is unclear how the proposed approach is able to handle this. \n\nOne missing related paper is \"Learning Hawkes Processes from Short Doubly-Censored Event Sequences\"\n\nSection 5.2 can be significantly strengthened if comparing with at least one of these approaches.\n\n4. The paper is fairly written. I had some trouble reading back and forth for understanding Figure 1 since it has long caption that is not self-contained. The annotation of Section 2 is also too heavy to quickly skim through to memorize. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Re: particle smoothing for neural Hawkes Processes",
            "review": "The paper presents an inference method (implicit distribution particle smoothing) for neural Hawkes processes that accounts for latent sequences of events that influence the observed trajectories.\n\nQuality\n+ The paper combines ideas from multiple areas of machine learning to tackle a challenging task of inference in multivariate continuous-time settings.\n- The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.\n\nClarity\n- notation is complex replete with symbols \"@\" and text in math formulas\n- It's not clear what p (\"the data model\") and p_miss (\"the missingness mechanism\") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol.  In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as: p(r|x_u, x_o) = p(r|x_o), r is a missingness vector, x_u is x unobserved, and x_o is x observed, then r corresponds to observation or not, whereas in the manuscript p_miss is on the values themselves, i.e. on the space where z={k_{i,j}@t_{i,j}} resides.  We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown.  If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.\n\nOriginality\n+ the work is rich, complex, original, and uses leading methods from multiple areas of ML.\n\nSignificance\n+ the significance of this work could be high, as it may provide a way to conduct difficult inference in an effective way to produce increasingly flexible modeling of trajectories amidst partial observation.\n- however the exposition (particularly the experiments) does not fully demonstrate this.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}