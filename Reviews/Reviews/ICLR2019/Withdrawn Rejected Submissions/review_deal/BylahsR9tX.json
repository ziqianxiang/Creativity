{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good work with extensive experiments but it seems the novelty is somewhat limited",
            "review": "This paper proposes to accelerate LSTM by using MF as the post-processing compression strategy. Extensive experiments are conducted to show the performance. However, there are some concerns.\n\n-The novelty of this paper is somewhat limited. The MF methods such as SVD and NMF are directly used in LSTM without any analysis. What are the motivations? Some necessary discussions should be added. There are many MF methods (\"Deep Collaborative Embedding for Social Image Understanding \"). Is another one suitable?\n-The research paper should propose a new idea rather than provide many results to report to readers. The current version seems a good technical report rather than a research paper.\n-There are extensive experimental results. How about the running time? It is better to show some results about the running time. Besides, why not implement the method with GPU, which is widely used in the research community?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Detailed experiments and nice work but it seems the idea is not new",
            "review": "This paper focuses on compressing pretrained LSTM models using matrix factorization methods, and have detailed analysis on compressing different parts of LSTM models.\n\nIt seems the idea is very similar to [1]. Both use matrix factorization to compress pretrained RNN models. The method is not exactly the same and the experiments are different tasks. It would be nice if it could be added as a baseline.\n\nThe LSTMP model proposed in [2] also uses matrix factorization as a method to do LSTM model compression and speedup. The difference here is that this paper focuses on compression of a pretrained model. However, when fine-tuning is used, the difference becomes less.  So I think for comparison, another baseline is needed, the LSTM model with projection layer that is trained from scratch. \n\n[1] Prabhavalkar, Rohit, et al. \"On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition.\" arXiv preprint arXiv:1603.08042 (2016).\n[2] Sak, Haşim, Andrew Senior, and Françoise Beaufays. \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling.\" Fifteenth annual conference of the international speech communication association. 2014.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good technical report of applying decomposition methods to weights of LSTM. But lack of originality and significance. ",
            "review": "\n[PROS]\n\n[quality]\n\nThe paper did extensive experimental studies on compressing weights of LSTM with two matrix decomposition methods (SVD and NMF). Their experiments cover three NLP downstream tasks and two benchmark datasets, and performed the decomposition methods on state-of-the-art language models. They explored effects of different methods on different weights of LSTM, and also examined how the results are influenced by different norms (used in the objective function of decomposition method). The experimental studies are very solid and convincing---they can well support their claims. \n\n[clarity]\n\nThe paper is clearly-written and has a very detailed appendix.  \n\n[CONS]\n\n[originality]\n\nOne major weakness of this paper is its lack of originality. The paper directly applied two well-known and popularly used methods---SVD and NMF---to the weights of LSTM, but did not provide any insights on the fitness of these methods. For example, what the non-negativity means in the case of LSTM weights? \n\nMoreover, why LSTM? The way of applying these methods in this paper has nothing to do with LSTM’s unique properties that distinguish it from other neural models. None of the experimental findings seem tied to any of LSTM’s properties. We can always expect a speed-up in runtime but a drop in performance after applying these methods to another neural model. As long as we still maintain certain rank, the performance will not drop much. The effect of using different norms seems only dependent on the properties of the norms and the matrices, but not anything related to the structure of a LSTM. \n\n[significance]\n\nAs mentioned in [quality], the experiments are extensive and results are convincing. But the results are largely predictable as discussed in [originality], and not necessarily tied to LSTM. That being said, this submission is certainly a really high-quality technical report, but does not seem significant enough to be accepted to ICLR. \n\n[questions for authors]\n\nWhy are experiments only performed on CPUs but not GPUs? I understand that the time complexity does not depend on the device, but the actual runtime does---because of massive cores in GPUs. So when people apply large RNNs like ELMO, they would prefer using GPUs. So do methods proposed in this work also provide any edge in that case? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}