{
    "Decision": {
        "metareview": "The reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than BP (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large-scale experiment results to show this is practically relevant. In the AC's opinion, a principled kernel-based approach can be counted as interpretable, and there the AC would support the paper if a) is the only concern. However, c) seems to be a serious concern since the paper doesn't seem to have experiments beyond fashion MNIST (e.g., CIFAR is pretty easy to train these days) and doesn't have experiments with convolutional models. Based on c), the AC decided that the paper is not quite ready for acceptance. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Good paper, some things are oversold",
            "review": "This paper attempts to learn layers of NNs greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. The paper is well-written and was an interesting read, despite being notation heavy. \n\nI think the interpretability claims have some merits but are over-stated. Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically. I have some additional questions about the paper, and I am reserving my recommendation on this paper till the authors answer them. \n\n1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \\tau * norm(weights) ?  What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form? Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here?\n\n\n2) Lemma 4.3 assumes separability (since c should be > a for \\tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. \n\nMinor :\nBelow Def 4.1  \"to a standard normal distribution \" should be \"according to P\".\nSome typos, please proof read e.g. spelling error \"represnetation \". ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical work which could be improved by further experimental work",
            "review": "****Reply to authors' rebuttal****\n\nDear Authors,\n\nThank you very much for all the effort you have put into the rebuttal. Based on the improved theoretical and experimental results, I have decided to increase my score from 5 to 6.\n\nBest wishes,\nRev 1\n\n\n****Original review****\n\n\nThis paper explores integration of kernel machines with neural networks based on replacing the non-linear function represented by each neuron with a function living in some pre-defined RKHS. From the theoretical standpoint, this work is a clear improvement upon the work of Zhang et al. (2017). Authors further propose a layer-wise training algorithm based on optimisation of a particular similarity measure between embeddings based on their class assignments at each layer, which eliminates necessity of gradient-based training. However, the experimental performance of the proposed algorithm is somewhat lacking in comparison, perhaps because the authors focus on kernelised equivalents of MLPs instead of CNNs as Zhang et al.\n\nMy rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training, and absence of experimental comparison with several baselines (see details below). It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability. Apart from the comments below, I would like to ask the authors to discuss relation to the following related papers:\n\n\t1) Kulkarni & Karande, 2017: \"Layer-wise training of deep networks using kernel similarity\" https://arxiv.org/pdf/1703.07115.pdf\n\n\t2) Scardapanea et al., 2017: \"Kafnets: kernel-based non-parametric activation functions for neural networks\" https://arxiv.org/pdf/1707.04035.pdf\n\n\nDetailed comments:\n\nTheory\n\n- (Sec 4.1) Backpropagation (BP) is being criticised: BP is only a particular implementation of gradient calculation. It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms, rather than to obtaining gradients through BP?! Regarding the criticism that BP forces intermediate layers to correct for \"mistakes\" made by layers higher up: it seems your layer-wise algorithm attempts to learn the best possible representation in first layer, and then progresses to the next layer where it tries to correct for the potential error of the first layer and so on. In other words it seems that the errors of layers are propagated from first to last, instead of last to first as in BP, but are still being propagated in a sense. I do not immediately see why propagation forward should be preferable. Can you please further explain this point?\n\n- It is proven in the appendix (Lemma B.3) that under certain conditions stacking additional layers never leads to degradation of training loss. Can you please clarify whether additional layers can be helpful even in the case where previous layers already succeeded in learning the optimal representation?\n\n- (Sec 4.1) Layer-wise vs. network-wise optimality: I find the claim that BP-based learner is not aware of the network-wise optimality confusing. BP explicitly optimises for network-wise optimality and the relative contribution to the network-wise error of each weight is propagated accordingly. I suppose my confusion stems from lack of a clear description of what defines a learner \"aware\" or \"blind\" to network-wise optimality. In general, I am not convinced layer-wise optimality is a useful criterion when what we want to achieve is network-wise optimality. As you show in the appendix, if layer-wise optimality is achieved then it implies network-wise optimality; however, layer-wise optimality is only a sufficient condition and likely not a necessary one (except for the simplified scenario studied in B.3). It is thus not clear to me why layer-wise training would always be preferable to network-wise training (e.g. using BP) especially because its greedy nature might intuitively prevent learning of hierarchical representations which are commonly claimed to be key to the success of neural networks. Can you please clarify?\n\n- (Sec 4.2) I think it would be beneficial to state in the introduction that the \"risk\" is with respect to the hinge loss which is common in the SVM/kernel literature but much less in the deep learning literature and thus could surprise a few people when they reach this point. \nFuther questions:\n\t- From Lemma 4.3, it seems that the derived representation is only optimal with respect to the **upper bound** on the empirical risk (which for \\tau >= 2 will be an upper bound on the population risk). I got slightly confused at this point as my interpretation of the previous text was that the representation is optimal with respect to the population risk itself. Does the upper bound have the same set of optima? Please clarify.\n\n\t- (p.5) There are two assumptions that I find somewhat restrictive. Just before Lemma 4.3 you assume that the number of points in each class must be the same. Can you comment on whether you expect the same representation to be optimal for classification problems with significantly imbalanced number of samples per class? The second assumption is after Lemma 4.4 where you state that the stationary kernel k^{l-1} should attain its infinum for all x, y s.t. || x - y || greater than some threshold. This does not hold for many of the popular kernels like RBF, Matern, or inverse multiquadric. Do you think this assumption can be relaxed?\n\n\t- (p.5) Choice of the dissimilarity measure for G: Can you provide more intuition about why you selected L^1 distance and whether you would expect different results with L^2 or other common metrics?\n\n- (Sec 4.3) Can you please provide more detaild about the relation of the proposed objective (\\hat(R)(F) + \\tau max_j ||f_j||_H) to Lemmas 4.3 and 4.5 where the optimal representation was derived for functions that optimise an upper bound in terms of Gaussian complexity (e.g. is the representation that minimises risk w.r.t. the Gaussian bound also optimal with respect to functions that optimise this objective)?\n\n\nExperiments\n\n- I would appreciate addition of some standard baselines, like MLP combined with dropout or batch normalisation, and optimised with RMSProp (or similar). These would greatly help with assessing competitiveness with current SOTA results.\n\n- It would be nice to see the relative contribution of the two main components of the paper. Specifically, an experiment which would evaluate empirical performance of KNs optimised by some form of gradient descent vs. by your layer-wise training rule would be very insightful.\n\n\nOther\n\n- (p.2, 1st par in Sec 2) [minor] You state \"a kernel machine is a universal function approximator\". I suppose that might be true for a certain class of kernels but not in general?! Please clarify.\n\n- (p.2, 3rd par in Sec 2) [minor] Are you using a particular version of the representer theorem in the representation of f_j^{(i)} as linear combination of feature maps? Please clarify.\n\n- (p.2, end of 1st par in Sec 3) L^{(i)} is defined as sup over X_i. It is not clear to me that this constant is necessarily finite and I suspect it will not be in general (it will for the RBF kernel (and most stationary kernels) used in experiments though). Finiteness of L^{(i)} is necessary for the bound in Eq. (2) to be non-vacuous. Please clarify.\n\n- (p.3, after 1st display in Sec 4.2.1) [minor] Missing dot after \"that we wish to minimise\". Next sentence states \"**the** optimal F\" (emphasis mine) -- I am sorry if I overlooked it, but I did not notice a proof that a solution exists and is unique, and am not familiar enough with the literature to immediately see the answer. Perhaps a footnote clarifying the statement would help.\n\n- (p.4, 1st par in Sec 4) You say \"A generalisation to regression is reserved for future work\". I did not expect that based on the first few pages. On high-level, it seems that generalisation to regression need not be trivial as, for example, the optimal representation derived in Lemma 4.3 and Lemma 4.5 explicitly relies on the classification nature of the problem. Can you comment on expected difficulty of extension to regression? Possibly state in the introduction that only classification is considered in this paper.\n\t- (p.7, 1st par in Sec 6) [related] \"However they did not extend the idea to any **arbitrary** NN\" (emphasis mine). Can you please be more specific here?\n\n- (p.5-6) [minor] Last sentence in Lemmas 4.3 and 4.5 is slightly confusing. Can you rephrase please?\n\n- (p.6) [minor] You say \"the learned decision boundary would generalise better to unseen data\". Can you please clarify the last sentence (e.g. being more precise about the meaning of the word \"simple\" in the same sentence) and provide reference for why this is necessarily the case?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting theoretical analysis of layer-wise training of kernel-based neural networks, concerns about practicality ",
            "review": "Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided.\n\nEvaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up.\n\n- interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier?\n- function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? \n- training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. \n- the intuition of layer-wise optimality: on page 4, the paper states that \"the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training\" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this?\n- the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets?\n- vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this?\n- some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}