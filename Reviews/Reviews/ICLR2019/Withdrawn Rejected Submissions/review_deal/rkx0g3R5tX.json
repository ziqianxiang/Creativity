{
    "Decision": {
        "metareview": "All reviewers agree that the paper is not quite ready for publication.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "not above threshold"
    },
    "Reviews": [
        {
            "title": "Interesting idea, need more clarification and detail, not sure if language modeling is good application",
            "review": "The mutually exclusive assumption of traditional softmax can be biased in case negative samples are not explicitly defined. This paper presents Cooperative Importance Sampling towards resolving this problem. The authors experimentally verify the effectiveness of the proposed approach using different tasks including applying matrix factorization in recommender system, language modeling tasks and a task on synthetic data.\n\nI like this interesting idea, and I agree with the authors that softmax does exist certain problem especially when negative samples are not well defined. I appreciate the motivation of this work from the PU learning setting. It would be interested to show more results in PU learning setting using some synthetic data. I am interested to see the benefit of this extension of softmax with respect to different amount of labeled positive samples.\n\nHowever, I am not completely convinced that the proposed method would be a necessary choice for language modeling tasks.\n--To me, the proposed method has close connection to 2-gram language model. \n--But for language tasks, and other sequential input, we typically make prediction based on representation of very large context. Letâ€™s say, we would like to make prediction for time step t given the context of word_{1:t} based on some recurrent model, do you think the proposed softmax can generally bring sizable improvement with respect to traditional choices. And how?\n\nBy the way, I think the proposed method would also be applicable in the soft-label setting.\n\nFor the experiments part, maybe put more details and discussions to the supplementary material.\nA few concrete questions.\n-- In some tables and settings, you only look at prec@1, why? I expect the proposed approach would work better in prec@K.\n-- Can you provide more concrete analysis fortable 6? Why proposed methods does not work well for syntactic. \n-- Describe a little bit details about MF techniques and hyper-parameters you used. \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "missing critical details in formulation and evaluation",
            "review": "This paper proposed PMES to relax the exclusive outcome assumption in softmax loss. The proposed methods is motivated from PU settings. The paper demonstrate its empirical metrit in improving word2vec type of embedding models. \n\n- on experiment: \n-- word2vec the window size = 1 but typically a longer window is used for NS. this might not reflect the correct baseline performance. is the window defined after removing rare words? what's the number of NS used? how stop words are taken care of? \n-- would be good to elaborate how CIS in word similarity task were better than full softmax. Not sure what;s the difference between the standard Negative sample objective. Can you provide some quantitative measure?  \n-- what is the evaluation dataset for the analogy task? \n\n-- MF task: the results/metrics suggests this is a implicit [not explicit (rating based)] task but not clearly defined. Better to provide - embedding dimensions, datasets positive/negative definition and overall statistics (# users, movies, sparsity, etc), how the precision@K are calculated, how to get a positive label from rating based dataset (movielens and netflix), how this compares to the plain PU/implicit-matrix factorization baseline. How train/test are created in this task?\n\n\n- on problem formulation:\nin general, it is difficult to parse the technical contribution clearly from the current paper. \n-- in 3.3., the prob. distribution is not the standard def of multi-variate bernoulli distribution.\n-- (6) first defined the support set but not clear the exact definition. what is the underlying distribution and what is the support for a sington means?\n-- it is better to contrast against the ns approximation in word2vec paper and clarify the difference in term of the mathematical terms. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, writing needs a lot of improvement",
            "review": "This paper presents Partially Mutual Exclusive Softmax (PMES), a relaxation of the full softmax that is commonly used for multi-class data. PMES is designed for positive-unlabeled learning, e.g., language modeling, recommender systems (implicit feedback), where we only get to observe positive examples. The basic idea behind PMES is that rather than considering all the non-positive examples as negative in a regular full softmax, it instead only considers a \"relevant\" subset of negatives. Since we actually don't know which of the negatives are more relevant, the authors propose to incorporate a discriminator which attempts to rate each negative by how hard it is to distinguish it from positives, and weight them by the predicted score from the discriminator when computing the normalizing constant for the multinomial probability. The motivation is that the negatives with higher weights are the ones that are closer to the decision boundary, hence will provide more informative gradient comparing to the negatives that are further away from the decision boundary. On both real-world and synthetic data, the authors demonstrate the PMES improves over some other negative sampling strategies used in the literature. \n\nOverall the idea of PMES is interesting and the solution makes intuitive sense. However, the writing of the paper at the current stage is rather subpar, to the extend that makes me decide to vote for rejection. In details:\n \n1. The motivation of PMES from the perspective of mutual exclusivity is quite confusing. First of all, it is not clear to me what exactly the authors mean by claiming categorical distribution assumes mutual exclusivity -- does it mean given a context word, only one word can be generated from it? Some further explanation will definitely help. Further more, no matter what mutual exclusive means in this context, I can hardly see that PSME being fundamentally different given it's still a categorical distribution (albeit over a subset).\n\nThe way I see PMES from a positive-unlabeled perspective seems much more straight-forward -- in PU learning, how to interpret negatives is the most crucial part. Naively doing full softmax or uniform negative sampling carry the assumption that all the negatives are equal, which is clearly not the right assumption for language modeling and recommender systems. Hence we want to weight negatives differently (see Liang et al., Modeling user exposure in recommendation, 2016 for a similar treatment for RecSys setting). From an optimization perspective, it is observed that for negative sampling, the gradient can easily saturate if the negative examples are not \"hard\" enough. Hence it is important to sample negatives more selectively -- which is equivalent to weighting them differently based on their relevance. A similar approach has also been explored in RecSys setting (Rendle, Improving pairwise learning for item recommendation from implicit feedback, 2014). Both of these perspectives seem to offer more clear motivation than the mutual exclusivity argument currently presented in the paper.\n\nThat being said, I like the idea of incorporating a discriminator, which is something not explored in the previous work.  \n\n2. The rigor in the writing can be improved. In details:\n\n* Section 3.3, \"Multivariate Bernoulli\" -> what is presented here is clearly not multivariate Bernoulli\n\n* Section 3.3, the conditional independence argument in \"Intuition\" section seems no difference from what word2vec (or similar models) assumes. The entire \"Intuition\" section is quite hand-wavy.\n\n* Section 3.3, Equation 4, 5, it seems that i and j are referred both as binary Bernoulli random variables and categorical random variables. The notation here about i and j can be made more clear. Overall, there are ambiguously defined notations throughout the paper. \n\n* Section 4, the details about the baselines are quite lacking. It is worth including a short description for each one of them. For example, is PopS based on popularity or some attenuated version of it? As demonstrated from word2vec, a attenuated version of the unigram (raised to certain power < 1) works better than both uniform random, as well as plain unigram. Hence, it is important to make the description clear. In addition, the details about matrix factorization experiments are also rather lacking. \n\n3. On a related note, the connection to GAN seems forced. As mentioned in the paper, the discriminator here is more on the \"cooperative\" rather than the \"adversarial\" side. \n\nMinor:\n\n1. There are some minor grammatical errors throughout. \n\n2. Below equation 3, \"\\sigma is the sigmoid function\" seems out of the context.\n\n3. Matt Mohaney -> Matt Mahoney ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}