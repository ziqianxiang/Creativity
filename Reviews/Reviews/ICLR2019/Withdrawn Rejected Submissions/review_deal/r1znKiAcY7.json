{
    "Decision": {
        "metareview": "A new regularized graph CNN approach is proposed for semi-supervised learning on graphs.  The conventional Graph CNN is concatenated with a Transposed Network, which is used to supplement the supervised loss w.r.t. the labeled part of the graph with an unsupervised loss that serves as a regularizer measuring reconstruction errors of features. While this extension performs well and was found to be interesting in general by the reviewers,  the novelty of the approach (adding a reconstruction loss),  the completeness of the experimental evaluation, and the presentation quality have also been questioned consistently. The paper has improved during the course of the review, but overall the AC evaluates that paper is not upto ICLR-2019 standards in its current form.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "interesting extension; but too preliminary"
    },
    "Reviews": [
        {
            "title": "presentation could be significantly improved, details are missing, validation is not compelling ",
            "review": "This paper proposes to regularize the training of graph convolutional neural networks by adding a reconstruction loss to the supervised loss. Results are reported on citation benchmarks and compared for increasing number of labeled data.\n\nThe presentation of the paper could be significantly improved. Details of the proposed model are missing and the effects of the proposed regularization w.r.t. other regularizations are not analyzed.\n\nMy main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.\n\nDetails / references of the transposed convolution operation are missing (see e.g. https://ieeexplore.ieee.org/document/7742951). It is not clear what the role of the transposed convolution is in that case. It seems that the encoder does not change the nodes nor the edges of the graph, only the features, and the filters of the transposed convolution are learnt. If the operation is analogous to the transposed convolution on images, then given that the number of nodes in the graph does not change in the encoder layers (no graph coarsening operations are applied), then learning an additional convolution should be analogous (see e.g. https://arxiv.org/pdf/1603.07285.pdf Figure 4.3.). Could the authors comment on that?\n\nDetails on the pooling operation performed after the transposed convolution are missing (see e.g. https://arxiv.org/pdf/1805.00165.pdf, https://arxiv.org/pdf/1606.09375.pdf). Does the pooling operation coarsen the graph? if so, how is it then upsampled to match the input graph?\n\nFigure X in section 2.1. does no exist.\n\nSupervised loss in section 2.2.1 seems to disregard the sum over the nodes which have labels.\n\n\\hat A is not defined when it is introduced (in section 2.2.2), it appears later in section 2.3.\n\nSection 2.2.2 suggests that additional regularization (such as L2) is still required (note that the introduction outlines the proposed loss as a combination of reconstruction loss and supervised loss). An ablation study using either one of both regularizers should be performed to better understand their impact. Note that hyper-parameters chosen give higher weight to L2 regularizer.\n\nSection 3 introduces a bunch of definitions to presumably compare GCN against SRGCN, but those measures of influence are not reported for any model.\n\nExperimental validation raises some concerns. It is not clear whether standard splits for the reported datasets are used. It is not clear whether hyper-parameter tuning has been performed for baselines. Authors state \"the parameters of GCN and GAT and SRGCN are the same following (Kipf et al; Velickovic et al.)\". Note that SRGCN probably has additional parameters, due to the decoder stacked on top of the GCN. Reporting the number of parameters that each model has would provide more insights. Results are not reported following standards of running the models N times and providing mean and std. Moreover, there are no results using the full training set.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting extension to GCNs, somehwat lacking a comprehensive evaluation",
            "review": "I appreciate the author response and additional effort to provide comparison with MoNet. I have raised my rating by 1 point. It should be noted that the edits to the revision are quite substantial and more in line of a journal revision. My understanding is that only moderate changes to the initial submission are acceptable.\n\n-----------------------------------------------\n\nThe paper introduces a new regularization approach for graph convolutional networks. A transposed GCN is appended to a regular GCN, resulting in a trainable, graph specific regularization term modelled as an additional neural network.\n\nExperiments demonstrate performance en par with previous work in the case where sufficient labelled data is available. The SRGCNs seem to shine when only few labelled data is available (few shot setting).\n\nThe method is appealing as the regularization adapts to the underlying graph structure, unlike structure-agnostic regularization such as L1.\n\nUnclear why the results are not compared to MoNet (Monti et al. 2017) which seems to be the current state-of-the-art for semi-supervised classification of graph nodes.\n\nOverall, well written paper with an interesting extension to GCN. The paper is lacking a comprehensive evaluation and comparison to latest work on graph neural networks. The results in the few shot setting are compelling.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Idea is reasonable; work is preliminary",
            "review": "Edited: I raised the score by 1 point after the authors revised the paper significantly.\n\n--------------------------------------------\n\nThis paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.\n\nThe approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.\n\n1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.\n\n2. In Section 2.2 the authors write \"... framework is shown in Figure X\" without even showing the figure.\n\n3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.\n\n4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: \"vertices which satisfies\", \"makes W be affected\", \"the some strong baseline methods\", \"a set research papers\", and \"in align with\". The authors are suggested to do a thorough proofreading.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}