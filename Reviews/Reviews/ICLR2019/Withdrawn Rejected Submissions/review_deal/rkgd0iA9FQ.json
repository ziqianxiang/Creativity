{
    "Decision": {
        "metareview": "The reviewers and ACs acknowledge that the paper has a solid theoretical contribution  because it give a convergence (to critical points) of the ADAM and RMSprop algorithms, and also shows that NAG can be tuned to match or outperform SGD in test errors. However, reviewers and the AC also note that potential improvements for the paper a) the exposition/notations can be improved; b) better comparison to the prior work could be made; c) the theoretical and empirical parts of the paper are somewhat disconnected; d) the proof has an error (that is fixed by the authors with additional assumptions.) Therefore, the paper is not quite ready for publications right now but the AC encourages the authors to submit revisions to other top ML venues. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Nice idea , not so good presentation  ",
            "review": "Summary:\nThis paper present a convergence analysis of the popular methods RMSProp and ADAM in the case of smooth non-convex functions. In particular it was shown that the above adaptive gradient algorithms are guaranteed to reach critical points for smooth non-convex objectives and bounds on the running time are provided. An empirical investigation is also presented with main focus on the comparison of the adaptive gradient methods and the Nesterov accelerated gradient algorithm (NAG).  \n\nComments:\nAlthough the results are promising, I found the reading (mainly because of the not defined notation) of this paper really hard. \nIn terms of presentation, the motivation in introduction is fine, but the following section named \"Notations and Pseudocodes\" is confusing and has many undefined notations which makes the paper very hard to read. It gives the impression that the section was added the last minute. For example what is fundtion \"g\" in the definition 1? What is support(v) and the diag(v) in the definition 2. the diag(v) is more obvious to me but then why at page 18 the diag(v)at the top of the page is bold (are these two things different)?\nIn the presentation of RMSProp what the $g_t^2$ means? Please have a look to last year's ICLR paper [Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar. \"On the convergence of adam and beyond.\" (2018).] for a more appropriate introduction of the notation.\n\nIn the introduction the authors refer to NAG as a stochastic variant of the Nesterov's acceleration and they informally present the algorithm in the end of the first paragraph. There the update rule includes stochastic gradients \\nable f_i(.) while in the formal presentation in the update rule there is \\nabla f(x) which is the full gradient of the objective function of the original problem. I expect this difference is somehow justified from the mentioning in the algorithm of the possibly noisy oracle but this is never mention in the main text.\n\nIf the above statements in terms of presentation, are ignored the convergence results and numerical experiments are interesting. \nHowever, the numerical evaluation does not correspond to the theoretical results. It is a comparison of NAG ,ADAM and RMSPROP with interesting conclusions  that can be beneficial for practitioners that they use these methods.\n\nSome missing references:\nOn Adam methods:\n1) Chen, Xiangyi, et al. \"On the convergence of a class of adam-type algorithms for non-convex optimization.\" arXiv preprint arXiv:1808.02941 (2018).\n2) Zhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\nOn momentum (heavy ball) methods:\n3) Loizou, Nicolas, and Peter Richt√°rik. \"Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods.\" arXiv preprint arXiv:1712.09677 (2017).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "There may exist an error on the proof of Stochastic RMSProp.",
            "review": "There may exist an error on the proof of Theorem 3.1 in appendix. For the first equation in page 13, the authors want to estimate lower-bound of the term $E<\\nabla{f}(xt),V_t^{-0/5}*gt>$. The second inequality $>$ may be wrong. Please check it carefully.  (Hints: both the index sets { i | \\nabla{f}(xt))_{i}*gt_{i} <0 } and { i | \\nabla{f}(xt))_{i}*gt_{i} >0 } depend on the random variable $gt$. Hence, the expectation and summation cannot be exchanged in the second inequality.)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Missing a very relevant reference that discusses the exact same issue",
            "review": "*Summary:\nThis paper analyzes the convergence of ADAM and RMSProp to stationary points\nin the non convex setting.\nIn the second part the authors experimantally compare the performance of these methods to Nesterov's Accelerated method.\n\n\n\n*Comments:\n\n-The paper does not tell a coherent story and the two parts of the paper are somewhat unrelated.\n\n-The authors claim that they are the first to analyze adaptive methods in the non-convex setting, yet this was recently done in \n[Xiaoyu Li, Francesco Orabona; On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes]\nThe authors should cite this paper and compare their results to it.\n\n-The above paper of [Li and Orabona] demonstrates a nice benefit of AdaGrad in the non-convex setting. Concretely they show that in the noisless setting adaptive methods give a faster rate of $O(1/T)$ compared to the standard rate of $O(1/\\sqrt{T})$ of SGD.\n\nUnfortunately, the results of the current paper do not illustrate the benefit of adaptive methods over SGD, since the authors provide similar rates to SGD or even worse rates in some situations.\nI think that in light of [Li and Orabona] one should expect a $O(1/T)$ rate also for ADAM and RMSProp.\n\n\n-The experimental part is not so related to the first part. And the experimental phenomena is only demonstrated for the MNIST dataset, which is not satisfying. \n\n\n*Summary:\nThe main contribution of this paper is to provide rates for approaching stationary points.\nThis is done for ADAM and RMSProp, two adaptive training methods.\nThe authors do not mention a very relevant reference, [Li and Orabona].\nAlso, the authors do not show if ADAM and RMSProp have any benefit compared to SGD in the non-convex setting, which is a bit disappointing. Especially since [Li and Orabona] do demonstrate the benefit of AdaGrad in their paper.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}