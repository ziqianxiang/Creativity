{
    "Decision": {
        "metareview": "The paper proposes a new method for training generative models by minimizing general f-divergences. The main technical idea is to optimize f-divergence between joint distributions which is rightly observed to be the upper bound of the f-divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution.  The basic ideas in this work are not completely novel but are put together in a new way. \n\nHowever, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach. The only quantitive results are in table 2, which is only a simple Gaussian example. It essential to have more substantial empirical results for supporting the new algorithm. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Rejection: interest idea but empirical results are too week "
    },
    "Reviews": [
        {
            "title": "Nice trick to utilize an arbitrary f-divergence as the objective for training generative models.",
            "review": "The paper proposes a method for training generative models with general f-divergences between the model and empirical distribution (the VAE and GAN objectives are captured as specific instantiations). The trick that leads to computational tractability involves utilizing a latent variable and optimizing the f-divergence between joint distributions which is an upper bound to the (desired) f-divergence between the marginal distributions. Distribution support issues are handled by convolving the data space with a blurring function. Empirical results on image datasets demonstrate that the additional training flexibility results in qualitatively different learned models. Specifically, optimizing the reverse KL (and/or Jensen-Shannon divergence) leads to sharper images, perhaps at the loss of some variance in the data distribution.\n\nI liked the simplicity of the core idea, and appreciated the exposition being generally easy to follow. The application of upper bounds to general f-divergences for training generative models is novel as far as I know. My two issues are with the practicality of the implementation and evaluation methodology, both potentially affecting the significance of the work. Regarding practicality, it appears the training details are specific to each experiment. This begs the question of how sensitive the results are to these settings. Regarding the methodology, it would have been nice to see the method of Nowozin et al. (2016) applied in all experiments since this is a direct competitor to the proposed method. Moreover, the subjective nature of the results in the real dataset experiments makes it difficult to judge what the increased flexibility in training really provides - although I do note that the authors make this same point in the paper. Finally, given that model training is the paper's focus, an explicit discussion of computational cost was missed.\n\nEven with these issues, however, I believe the paper makes a contribution to the important problem of fitting expressive generative models. In my opinion, a more rigorous and thorough experimental exploration would increase the value, but the paper demonstrates that training with alternative f-divergences is feasible.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper with weak experiments",
            "review": "This paper proposed a novel variational upper bound for f-divergence, one example of which is the famous evidence lower bound for max-loglikelihood learning. The second contribution might be the spread f-divergence for distributions having different supports. Even though theoretically sound, I believe that the presented experimental results are not strong enough to support the effectiveness of the proposed techniques. Detailed comments are listed below.\n\n1) Notations are confusing, especially in Section 3 when introducing the SPREAD f -DIVERGENCE.\n2) I cannot find on arXiv the reference “D. Barber, M. Zhang, R. Habib, and T. Bird. Spread divergences. arXiv preprint, 2018.” So I am not sure whether you can take credit from the “spread f-divergence” or not.\n3) Important analysis/experiments on several key points are missing, for example, (i) how to specify the variance of the spread divergence in practice? (ii) how to estimate log p(y)? What is the influence?\n4) In the paragraph before Sec 4.2, how the sigma of the spread divergence is annealed?\n5) Despite the toy experiment in Sec 4.4, what are the advantages of the proposed f-divergence upper bound over the Fenchel-conjugate f-divergence lower bound? The current experimental results barely show any advantage.\n\nMinors:\n1) Page 6, under Figure 3. The statements of “KL (moment matching)” and “reverse KL (mode seeking)” are not consistent with what’s stated in Sec 2.2 (the paragraph under Eq (3)). \n2) “RKL” and “JS” are not defined. Forward KL and standard KL are both used in the paper.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The research direction itself is interesting, but further experimental validation is needed",
            "review": "\\clarity & quality\nThe paper is easy to follow and self-contained. \nHowever, the motivation for minimizing the upper bound is not so clear for me. \nAs far as I understood from the paper, changing the objective function to the upper bound of f-divergence have two merits compared to the existing methods. One is that by using the reverse KL, we can obtain sharper outputs, and the second one is that the optimization process will be stable compared to that of the lower bound. \nIn the introduction, the author just mentioned that \"the f-divergence is generally computationally intractable for such complex models. The main contribution of our paper is the introduction of an upper bound on the f-divergence.\"\nFor me, this seems that the author just introduced the new fancy objective. I think the motivation to introduce the new objective function should be stated clearly.\n\n\\originality & significance\nAlthough the upper bound of the f-divergence is the trivial extension, the idea to optimize the upper bound for the latent model seems new and interesting.\n\nHowever, it is hard to evaluate the usefulness of the proposed method from the current experiments.\nIt seems that there are two merits about the proposed method as above.\nThe only evidence that the learning tends to be stable is the Fig.8 in the appendix, but this is just the fitting of univariate Gaussian to a mixture of Gaussians, thus it is too weak as the evidence.\nAbout the sharp output, there are already many methods to overcome the blurred output of the usual VAE. No comparison is done in the paper.\nSo I cannot tell whether the proposed objective is really useful to learn the deep generative models.\nI think further experimental results are needed to validate the proposed method.\n\n\\Question\nIn page 4,  the variance of the p(y|x) and p_\\theta(y|z) are set to be the same. What is the intuition behind this trick? \nSince this p(y|x) is used as the estimator for the log p(y) as the smoothed delta function whose Gaussian window width (the variance), and the Gaussian window width is crucial to this kind of estimator, I know why the author used this trick.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}