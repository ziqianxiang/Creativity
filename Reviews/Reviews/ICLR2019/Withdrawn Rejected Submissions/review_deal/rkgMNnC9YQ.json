{
    "Decision": {
        "metareview": "The paper proposes an approach to define an \"interpretable representation\",\nin particular for the case of patient condition monitoring. Reviewers point\nto several concerns, including even the definition of explainability and\nlimited significance. The authors tried to address the concerns but reviewers\nthink the paper is not ready for acceptance. I concur with them in rejecting it.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting problem and hypothesis, inconclusive analyses and experiments",
            "review": "This paper is motivated in an interesting application, namely \"explainable representations\" of patient physiology, phrased as a more general problem of patient condition monitoring. Explainability is formulated as a communication problem in line with classical expert systems (http://people.dbmi.columbia.edu/~ehs7001/Buchanan-Shortliffe-1984/MYCIN%20Book.htm). \nInformation theoretical concepts are applied, and performance is quantified within the minimum description length (MDL) concept.\n\nQuality & clarity \nWhile the patient dynamics representation problem and the communication theoretical framing is interesting , the analyses and experiments are not state of the art. \nWhile the writing overall  is clear and the motivation well-written, there are many issues with the modeling and experimental work.\nThe choice of MDL over more  probabilistic approaches  (as e.g. Hsu et al 2017 for sequences) could have been better motivated. The attention mechanism could have been better explained (attention of whom and to what?) and also the prior (\\beta). How is the prior established - e.g. in the MIMIC case study   \nThe experimental work is carried out within a open source data set - not allowing the possibility of testing explanations against experts/users. \n\nOriginality \nThe main originality is in the problem formulation. \n\nSignificance \nThe importance of this work is limited as the case is not clearly defined. How are the representations to be used and what type of users is it intended to serve (expert/patients etc) \n\nPros and cons\n+ interesting problem\n\n-modeling could be better motivated\n-experimental platform is limited for interpretability studies\n\n== \nHsu, W.N., Zhang, Y. and Glass, J., 2017. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in neural information processing systems (pp. 1878-1889).\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, incomplete work.",
            "review": "Summary:\nThe authors propose a framework for training an external observer that tries explain the behavior of a prediction function using the minimal description principle. They extend this idea by considering how a human with domain knowledge might have different expectations for the observer. The authors test this framework on a multi-variate time series medical data (MIMIC-III) to show that, under their formulation, the external observer can learn interpretable embeddings.\n\nPros:\n- Interesting approach: Trying to develop an external observer based on information theoretic perspective. \n- Considering the domain knowledge of the human subject can potentially be an important element when we want to use interpretable models in practice.\n\nIssues:\n(1) In 2.4: So between M and M^O, which one is a member of M_reg? \n(2) On a related note to issue (1): In 2.4.1, \"Clearly, for each i, (M(X))_i | M^O(X) follows also a Gaussian distribution: First of all, I'm not sure if that expression is supposed to be  p(M(X)_i | M^O(X)) or if that was intended. But either way, I don't understand why that would follow a normal distribution. Can you clarify this along with issue (1)?\n(3) In 2.4.2: The rationale behind using attention & compactness to estimate the complexity of M^O is weak. Can you elaborate this in the future version?\n(4) What do each figure in Figure 4 represent?\n(5) More of a philosophical question: the authors train M and M^O together, but it seems more appropriate to train an external observer separately. If we are going to propose a framework to train an agent that tries to explain a black box function, then training the black-box function together with the observer can be seen as cheating. It can potentially make the job of the observer easier by training the black box function to be easily explainable. It would have been okay if this was discussed in the paper, but I can't find such discussion.\n(6) The experiments can be made much stronger by applying this approach to a specific prediction task such as mortality prediction. The current auto-encoding task doesn't seem very interesting to apply interpretation.\n(7) Most importantly: I like the idea very much, but the paper clearly needs more work. There are broken citations and typos everywhere. I strongly suggest polishing this paper as it could be an important work in the model interpretability field.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A Definition for Interpretability based on MDL Principle",
            "review": "This paper proposes a definition for interpretability which is indeed the same as model simplicity using the MDL principle. It has several issues:\n\n1) Interpretability is not the same as simplicity or number of model parameters. For example, an MLP is thought to be more interpretable than an RNN with the same number of parameters.\n\n2) The definition of explainability in Eq. (5) is flawed. It should not have the second term L(M(X)|M^o, X) which is the goodness of M^o's fit. You should estimate M^o using that equation and then report L(M^o|M^p) as the complexity of the best estimate of the model (subject to e.g. linear class). Mixing accuracy of estimation of a model and its simplicity does not give you a valid explainability score. \n\n3) In Section 2.4.2, the softmax operator will shrink the large negative coefficients to almost zero (reduces the degrees of freedom of a vector by 1). Thus, using softmax will result in loss of information. In the linear observer case, I am not sure why the authors cannot come up with a simple solution without any transformation.\n\n4) Several references in the text are missing which hinders understanding of the paper.",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}