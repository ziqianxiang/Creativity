{
    "Decision": {
        "metareview": "This paper proposes Direct Sparse Optimization (DSO)-NAS to obtain neural architectures on specific problems at a reasonable computational cost. Regularization by sparsity is a neat idea, but similar idea has been discussed by many pruning papers. \"model pruning formulation for neural architecture search based on sparse optimization\" is claimed to be the main contribution, but it's debatable if such contribution is strong: worse accuracy, more computation, more #parameters than Mnas (less search time, but also worse search quality). The effect of each proposed technique is appropriately evaluated. However, the reviewers are concerned that the proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy. There's also some concerns about the search space of the proposed method. It is debatable about claim that \"the first NAS algorithm to perform direct search on ImageNet\" and \"the first method to perform direct search without block structure sharing\". Given the acceptance rate of ICLR should be <30%, I would say this paper is good but not outstanding. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "borderline"
    },
    "Reviews": [
        {
            "title": "Official Review",
            "review": "Summary:\nThis paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.\n\nThe main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph. All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a). The DAG’s edges can be pruned via a sparsity regularization term. The optimization objective of DSO-NAS is thus:\n\nAccuracy + L2-regularization(W) + L1-sparsity(\\lambda),\n\nwhere W is the shared weights and \\lambda specifies which edges in the DAG are used.\n\nThere are 3 phases of optimization:\n1. All edges are activated and the shared weights W are trained using normal SGD. Note that this step does not involve \\lambda.\n2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018).\n3. The best architecture is selected and retrained from scratch.\n\nThis procedure works for all architectures and objectives. However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts.\n\nTheir experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet).\n\nStrengths:\n1. Regularization by sparsity is a neat idea.\n\n2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet. Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive.\n\n3. Incorporating architecture costs into the search objective is nice. However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper.\n\nWeaknesses:\n1. Some experimental details are missing. I’m going to list them here:\n- Was the auxiliary tower used during the training of the shared weights W?\n\n- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?\n\n- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?\n\n- In Section 3.3, it is written that “The sparse regularization of \\lambda induces great difficulties in optimization”. This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.\n\n2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison.\n\n3. The paper has some grammatical errors. I obviously missed many, but here are the one I found:\n\n- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?\n\n- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”\n\nReferences.\n[1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf\n\n[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "If we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising",
            "review": "\n- Summary\nThis paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem. Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network. The proposed method is evaluated on CIFAR-10 and ImageNet dataset.\n\n- Pros\n  - The proposed method shows competitive or better performance than existing neural architecture search methods.\n  - The experiments are conducted thoroughly in the CIFAR-10 and ImageNet. The selection of the datasets is appropriate. Also, the selection of the methods to be compared is appropriate.\n  - The effect of each proposed technique is appropriately evaluated.\n\n- Cons\n  - The search space of the proposed method, such as the number of operations in the convolution block, is limited.\n  - The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.\n  - The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.\n\nOverall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The authors present an architecture search method where connections are removed with sparse regularization. It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.\n\nThere are a few grammatical/spelling errors that need ironing out.\n\ne.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc.\n\nA few (roughly chronological comments).\n\n- Pioneering work is not necessarily equivalent to \"using all the GPUs\"\n\n- There are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!\n\n- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?\n\n-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.\n\n- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?\n\n- You should add DARTS 1st order to table 1. \n\n- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?\n\n- The ablation study is good, and the results are impressive.\n\nI propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time. However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.\n\n------------\nUPDATE: Score changed based on author resposne\n------------\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}