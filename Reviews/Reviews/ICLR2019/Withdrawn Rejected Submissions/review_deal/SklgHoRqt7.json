{
    "Decision": {
        "metareview": "While there was some support for the ideas presented, the majority of reviewers did not think this paper was ready for publication at ICLR. In particular the experiments need more work, including the protocol for validation, and attention to overfitting.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not ready for publication at ICLR"
    },
    "Reviews": [
        {
            "title": "good initiative, not mature enough",
            "review": "The authors propose to optimize a black-box (validation/test) metric by learning to re-weight the training examples. The weights are calculated from a linear model on an auto-encoder-computed embedding, and the parameters of the linear model is found by an Gaussian-Process-Regression-(UCB)-guided global optimization procedure. Experimental results demonstrate that the learnt weights outputs uniform weights.\n\nThe paper is well-written with clear motivations. Nevertheless, the paper is not mature due to the following reasons:\n\n(1) The alternatives are not carefully compared/discussed for the key components of the proposed framework.\n   (1a) Why need the linear model? What if the weights are not calculated from the linear model parameters, but optimized by GPUCB directly?\n   (1b) Why need GPUCB? What if we just do random search or standard simulated annealing for global optimization?\n   (1c) Following (1a) and (1b), what if the weights are optimized directly through random search?\n   (1d) What if, in the case of MNIST, class weights are used instead of example weights\n   (1e) How sensitive is the proposed framework in terms of hyperparameters like p and q, and perhaps other GP parameters?\n\n(2) No comparison on the standard-but-challenging metrics like F1. The authors state in the experiments that it is not the focus of the work, but I do believe that the comparison is meaningful to help understand whether the proposed framework is close to the state-of-the-art in those standard metrics. Otherwise the baseline (uniform weights) is arguably just too weak.\n\n(3) Is the framework just overfitting the validation data set by reusing it to evaluate multiple times? Are there overfitting behavior observed during the reuses?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting problem. I think more discussion and experiments to understand better potential overfitting issues are needed here.  I'm intrigued that the approach works as well as it does on the examples given, so there may be something interesting here.",
            "review": "Pros:\n- Addresses several interesting and important problems all at once: covariate shift, concept drift, mismatch between training loss and test loss.\n- Fairly simple and elegant solution.\n- Multiple examples of the method working.\n- Clearly written\n\nCons:\n- Examples don't feel like full-fledged machine learning examples, where you  tune your learning algorithm on the validation set, as well as the example weights (their approach).  \n- Needs discussion of potential overfitting issues (see comments below).\n\nComments:\n- I think one area that is underexplored in this paper is overfitting.  One issue is overfitting the validation data by having more complicated weight functions. For example, in Figure 3(b), as the embedding dimension goes beyond 14, it seems like the error metric gets worse.  Would be interesting to see a plot of the validation error metric alongside this test error metric.  Also, what happens when we use even larger embedding dimensions -- that should clarify whether this is an overfitting situation, or just a random chance fluctuation.\n- In machine learning contexts, it's standard to try many different ML methods (or at least network architectures) with various hyperparameter settings and regularization methods, yet this isn't discussed at all in the paper.  Would you use the search for alpha as an inner loop in your model search and hyperparameter selection process?  I'd expect there could be additional issues with overfitting the validation set as you used more complicated models.  I think not discussing or investigating this makes the examples feel a little bit more like toys.  I think tuning your learning algorithm settings on a validation set is pretty intrinsic to machine learning approaches.\n- Relatedly, you say \"we impose no regularization on the model parameters\"... does this include things like early stopping, dropout, or other things that are used to prevent overfitting?  This seems just part of the \"no hyperparameter tuning\" setting of the paper.  \n- In the introdution you say \"MOEW . . . reshapes the total loss function to better match the testing metric.  This is similar to the idea of basis expansion, where we approximate the metric function using a linear combination of per example loss functions\".  You make a similar statement in the conclusion. This is an interesting idea, but it doesn't seem to represent what you're doing. This explanation suggests that you are fitting \\alpha's so that the objective function value approximates the validation metric for each theta.  But that's not what you're doing, right?\n- In 3.2, you say that c \"is a constant that normalizes the weights over a (batch from) the training set T\".  Why would you renormalize per batch?  This seems to potentially negate the effect of the reweighting, especially for small batches.\n- It might have been interesting to see if there was any significant differences between hinge loss and cross-entropy loss for the binary case.\n- In the MNIST experiment, am I correctly understanding that the difference between the 300 uniform weighted models you tried was the random initialization of the weights? Was there a lot of variation in performance among these trials?  Folk wisdom makes me think there would not be large performance differences.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper motivates using a nonlinear function based on an autoencoded representation to derive training sample weights to optimize any test metric. It seems like a significant contribution, particularly for analyzing datasets where training and validation data are known to be drawn from different distributions.",
            "review": "This proposal learns training data weights that optimize any given test metric.\nThey do this by learning both a weighting function and a classifier. They \niteratively train an ensemble of K {weighting_k,classifier} pairs, and\nselecting the pair that presents the best best metric-of-interest value over\nthe test set (over all iterations) as the final output.\n\nThe paper provides a large set of references that I found useful, and is\nclearly written.\n\nEach training iteration of their MOEW algorithm first optimizes classifiers \nover all training examples, for each of the K sample weightings.  Each\nweighting gives rise to one converged classifier, whose metric-of-interest is\nevaluated on the validation set.  Given K sets of weighting functions and their\nmetric-of-interest values, new parameters for the weighting functions are\ngenerated.\n\nThe weighting function choice is a simple function, based on a linear transform\nof autoencoder features, a normalization factor, and factor to account for\ndiffering label frequencies in training and test data.  The low-dimensional\nauto-encoding of {data,label} pairs is trained once, using the training data.\nThe weighting function parameters are a linear transformation matrix. They\nalso have a sigmoid non-linearity in their weighting function.\n\nIf I understand correctly, their approach seems to generalize the importance\nsampling methods that they reference by using a nonlinear combination of autoencoder\nfeatures.  They reference importance sampling methods that, for example, use \nGaussian kernel basis functions to model training/test example densities.\nWhile they do provide extensive references to previous approaches, I would have\nenjoyed a clearer explanation of main differences between their weighting\nfunction and ones that have previously been used in importance sampling.\n\nTheir main contribution seems to be the procedure (Alg. 2) used to updated the\nweighting function parameters.  They first fit a model that predicts\nmetric-of-interest values given all previous weighting function parameters used\nin the algorithm.  Then they use this model to generate the next set of\nweighting function parameters.  Their method adapts a Gaussian Process\nUpper-Confidence Bound to batch-wise processing.  Within-batch and between\nbatch exploration of the weighting function parameters is controlled by two\nparameters, which they held fixed at values corresponding to +/-1 for a normal\ndistribution.\n\nOnly the validation set need be iid with the test set, so their method seems\nquite general.\n\nTheir MNIST results use very simple networks to show that learning weighting\nfunctions has the most benefit when classifier networks are severely\nunder-parameterized.  Their wine price example addresses the choice of\nembedding dimension, and they found their error metric decreased from 52% with\nuniform weighting, to around 46% as they approached ~10 dimensions in the\nautoencoded {data,label} representation of the training data.  They then use a\nsmall crime dataset with a complicated test metric measuring *fairness*, based\non dividing the dataset into 4 quantiles based on white population. Using\nMOEW they could improve the fairness metric with little effect on the accuracy\nmetric.  They could also preset thresholds to achieve very good fairness on the\ntraining data and use MOEW to maximize the accuracy metric, which resulting\nin improving both fairness and accuracy compared to uniform sample weighting.\n\nFor spam blocking and web page quality, I would like to see a little more \ninterpretation of their results. They again show improvements using MOEW to\nprovide weights for training data.  This time, Table 2 presents a comparison\nwith importance sampling, but the methodology they used for importance sampling\nis not well described.  I'd like to understand where this improvement came\nfrom. What model/procedure was used for importance weighting? Gaussian rbfs?  \nDo the authors attribute the dramatic improvement for MOEW for Spam Blocking \nsimply to having a more flexible sample distribution function?  Or is it mainly\ndue to their adapting to the test metric?  Then for Web Page Quality, would it\nbe correct to conclude that the old and new web pages (training vs\nvalidation/test) are actually fairly similary distributed?\n\nWhat values of B and K are reasonable values in Alg. 1?  When applied to larger\nproblems, where do the authors feel the bottlenecks in Algs 1 and 2 will lie?\nDo the authors find that retuning the classifier for different weighted samples\nto take a lot of time?\n\nPros:\n - tests on several datasets\n - seems fairly generally applicable.\n \nCons:\n - To reproduce, I guess I'd need to adapt existing GP-UCB code from python or\n   C++, and I'm not sure how easy this would be. Releasing code to reproduce\n   the results might be nice.\n - Better understanding of differences wrt. typical Importance Sampling methods\n   would nice.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}