{
    "Decision": {
        "metareview": "The authors propose an algorithm for generating adversarial examples for ASR systems treating them as black boxes. \n\nStrengths\n- One of the early works to demonstrate black box attacks on ASR system that recognize phrases instead of isolated words.\n\nWeaknesses\n- The approach assumes that the logits are available, which may not be realistic for most ASR systems when they are used in practice -- typically only the final transcription is available.\n- Although the technique is applied to continuous speech, algorithmic improvements over prior work of Alzanot et al. is minimal.\n- Evaluation is weak. For example, cross correlation cannot completely capture the adversarial nature of a generated audio sample. \n- The authors use a genetic algorithm for generating new set of examples which are pruned and mutated. Itâ€™s not clear what guarantees exist that the algorithm will eventually succeed. \n\nThe reviewers agree that the presented work puts forth an interesting research direction. But given the deficiencies of the current submission as pointed out by the reviewers, the recommendation is to reject the paper.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Limited novelty compared to previous works"
    },
    "Reviews": [
        {
            "title": "Evaluation is weak",
            "review": "This paper proposes a black-box attack on multi-word ASR systems.  Most work on black-box attacks have focused on tasks in vision. This work adds to the literature on attac\nks on speech systems. The key novelties are the handling of a loss function over multiple decodings as well as the use of novel genetic algorithms to generate the adversari\nal examples.\n\nA weakness of this paper is that they do not compare to the closely related Alzantot et al. work. While the latter is focused on single word settings and is thus solving an\n easier problem, what would happen if the Alzantot et al. method was applied to each\n\n\nWhile the idea is interesting but incremental, the evaluation of the approach is weak.\n\n1. Insted of choosing random pairs of words as target phrases, it would be interesting to pick phrases that are likely to occur in English and to ask how success rate varie\ns as a function of the initial phrase and target phrase.\n\n2. To confirm that the resulting adversarial examples are similar to audio samples in the original dataset, the authors should do user studies. This is a key component in e\nvaluating the efficacy of such attacks. The cross correlation is useful but does not get at perceptual similarity.\n\n3. Table 1 is not useful since either the datasets are different or information is not given on the specific white box attacks.\n\n4. Does increasing the iterations lead to a higher success rate as claimed at end of page 7?\n\n\nAbstract:\n1. This sentence is misleading : \"Current work..are known\" given the Alzantot et al. work focuses on black-box attacks.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Targeted adversarial examples for black box audio systems",
            "review": "In \"Targeted adversarial examples for black box audio systems\" the authors look at an adversarial problem in neural nets for audio processing. There is quite a lot of recent interest in adversarial problems in machine learning. That work is mostly on the image side, and so this work is very topical. The problem is to modify an audio signal without changing how it sounds to the human ear, so that it is interpreted as the attacker wishes by the neural network. In the black box approach, the weights of the neural network are not known by the attacker. The attacker however must be able to present modified audio and learn the network's interpretation as often as the attacker wants. This work is very exciting and topical, and of interest to the ICLR community.\n\nThe authors demonstrate a proof of concept using the recent DeepSpeech model, and they connect very well with recent literature on adversarial networks.\n\nThe particular algorithm the authors propose is based on genetic algorithms. I thought that this was a weak part of the paper, because genetic algorithms are quite ad hoc and have few theoretical guarantees when compared to SMC, MCMC, nested sampling or herding, which all do basically the same thing as genetic algorithms. This can lead to loose ends, such as the \"momentum mutation\" introduced by the authors in 2.2, wherein probability of mutation increases as the population fails to adapt. It is true that momentum mutation would avoid local maxima, but it would also take the solution away from global maxima through a sort of \"sampling noise\" (the global maxima is a point at which the population also \"fails to adapt\", as there's no more adaptation to be done). It's unclear if this is a problem, but things like annealed importance sampling also deal with the same problem (or effective sample size of SMC), and they have theory to back them up.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper is not well-positioned against the existing literature on black-box attack. Its empirical evaluation is somewhat sloppy.",
            "review": "PAPER SUMMARY:\n\nThis paper introduces a biologically motivated black-box attack algorithm. \nThe target model in this case is DNN applied to the ASR context (automatic speech recognition system). \n\nNOVELTY & SIGNIFICANCE:\n\nThe proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.\n\nThis however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.\n\nIt is also unclear how this mutation component improves over the existing work (more on this in the sections below).\n\nAnother issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:\n\nChen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.\nZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. \nIn Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM\n\nCheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.\nQuery-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457\n\nWhile these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.\n\nTECHNICAL SOUNDNESS:\n\nI find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.\n\nCLARITY:\n\nThe paper is clearly written.\n\nEMPIRICAL RESULTS:\n\nI do not understand this statement:\n\n\"That 35% of random attacks were successful in this respect highlights the fact that black box\nadversarial attacks are definitely possible and highly effective at the same time\"\n\nWhy is 35% successful attack rate a positive result? The result tends to suggest that this is an attack with low success rate. \n\nThe 2nd paragraph in 3.2 seems to give a vague explanation: \"the vast majority of failure cases are only a few edit distances away from the target. \n\nThis suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity\".\n\nGiven the above statement, I do not see why the authors didn't actually \"run the algorithm for a few more iterations\" to verify it ...\n\nI am also curious why is the success rate of the proposed method is significantly lower than that of the existing system -- I assume \"single word black box\" is the work of (Alzantot et al., 2018).\n\nI find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?\n\nREVIEW SUMMARY:\n\nThe paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}