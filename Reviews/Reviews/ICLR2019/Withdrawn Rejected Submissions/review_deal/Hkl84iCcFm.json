{
    "Decision": {
        "metareview": "The paper uses dynamical systems theory to evaluate feed-forward neural networks.  The theory is used to compute the optimal depth of resnets.  An interesting approach, and a good initiative.\n\nAt the same time, the approach seems not to be thought through well enough, and the work needs another level of maturation before publication.  The application that is realised is too immature, and the corresponding contributions are not significant in their current form.  All reviewers agree on rejection of the paper.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "novel yet unripe"
    },
    "Reviews": [
        {
            "title": "Technical contribution and readability needs to be improved given the  niche subject  ",
            "review": "In this paper the authors analyse the role of residuals in the performance of residual networks (res-nets) with shared layer weights and propose a method to adapt the depth of such networks during training. They exploit  the fact that res-nets identical blocks / shared layer weights are discrete time dynamical systems to conduct their analysis. Their main contribution seems to be an empirical evaluation of the role of transient dynamics in the performance of such res-nets. Experiments are done on a toy dataset on concentric circles and MNIST data.\n\nI find the scope of the paper very narrow (res-nets with shared weights) even though there seem to be quite some other papers addressing this problem.\n\nClarity and quality of writing. It seems to me that the paper could  be much better written, the authors present long trains of thoughts and sometimes unsubstantiated arguments and at times there seems to be no clear storyline. In particular I would strongly  suggest a rewrite of  Sections 1, 2.1, and 4.  The storyline often very sketchy and is littered with hypothetical claims or less relevant information that can distracts and confuse the reader. I find that the hypotheses are not well formulated,   the claimed contributions of the paper don't seem to be significant enough and they should also be better connected to the experimental sections. Sometimes there are some odd choices of concepts/words such as \"softmax algorithm\" and \"classification behaviour.\"\n\nTechnical quality and contribution. The main technical contribution of the paper seems to be the observation that  that the solution of an  ODE if the integral of the RHS an that one can derive an ODE for the residuals only. I don't find  these results significant/relevant enough to justify the publication of this paper.  I expected a better written and more informative Section 2.1: some approximations of rates of convergence, a bit more about basins of attraction. I am also a bit skeptical about the claim that the analysed network us a prey-predator model. \n\nExperimental section. I find the description of experiments in Sec 4 very hard to read. The metrics are not clearly defined (not sure visualisations serve the purpose either), and the performed experiments are not well motivated and explained, for example in Section 4/P2 while I think I understand what the authors want to show (path of relevant neurons), I find the purpose of the whole experiment not very relevant.  I better analysis of the number of fixed points for classification tasks should be added, comparison of resulting features to other methods such as simple MLPs with same last layer could help . More relevant datasets should be be added e.g. CIFARxxx., ImageNet.\n\nOverall: I find that this paper needs to be improved both in terms of readability and technical contribution to ready for publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Residual Networks Classify Inputs Based On Their Neural Transient Dynamics\"",
            "review": "This paper aims to view the computations performed by residual networks through the lens of transient dynamical systems. A rough intuition is provided, followed by experiments in a toy concentric circle example and on MNIST. Finally, a method to determine the appropriate depth of ResNets is proposed, though experiments are only performed in the toy concentric circle experiment. \n\nThe approach of attempting to interpret feedforward networks through a dynamical systems perspective is an interesting and worthwhile one. However, this paper suffers from a number of flaws, and is clearly not ready for publication. \n\nThe clarity of this paper can be significantly improved. In general, the text is confusing, and as currently written, it is difficult to understand the central narrative of the manuscript. The review of literature in the introduction is relatively complete, though again, the presentation makes this section difficult to understand. \n\nScientifically, it is clear that further experiments on less toy datasets and settings will be required. While MNIST is useful for prototyping, experiments on datasets such as CIFAR (or ideally ImageNet) will be necessary to evaluate whether the observations made hold in more realistic settings. Moreover, the primary claim: that ResNets sum the residuals across layers is by definition true and by design. The scientific contribution of this statement is therefore questionable.\n\nIn addition, the case analyzed in the majority of the paper -- weight sharing across all layers -- is an unusual one, and as Figure 3 shows, clearly changes the network dynamics. The use of sigmoid activation functions is also an unusual one given that ResNets are generally trained with ReLU activations. \n\nFinally, the proposed method for determining the optimal depth for ResNets is an interesting idea, and worth further examination. However, the paper currently evaluates this only on an an extremely toy dataset of concentric circles. Evaluation on more realistic datasets (comparatively) with appropriate baselines will be required to determine whether this method is in fact helpful. \n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction, but questionable significance of contributions ",
            "review": "This paper presents the following main insight (quoting the authors): Resnets classify input patterns based on the sum of the transient dynamics of the residuals in different layers.\nThe authors formulate this insight mathematically, and experimentally demonstrate its validity on a toy binary classification problem. They also show that the behaviour of a Residual network trained on MNIST is in line with the claims. Finally, a method to adapt the depth of a Residual network during training is proposed and applied to the toy classification problem.\n\nThe paper is generally of good quality and easy to understand. My only complaint: the introduction (including related work) is too long and I think it will be unclear to a general reader before reading section 2, where the terms used in the paper are explained and clarified. I think it will be an improvement to leave out detailed discussion of related work for a separate section, and focus on clarifying what the paper is about.\n\nOverall, while the paper is related to an interesting and potentially fruitful perspective of neural networks (as dynamical systems), in my view the contributions are not significant at this stage. That the sum of transients determines network outputs is almost by design, and can be shown without a dynamical systems perspective. Using the paper’s notation, one can sum over the equations for all the layers to obtain this.\n\nx(1) = x(0) + y(1)\n…\nx(T) = x(T-1) + y(T)\n————————————————————————\nx(T) = x(0) + sum(y(t))\n\nSince the classification is performed using x(T), it is clear that the sum of transients is what causes the change in representation and that y(T) can be the same or not for different class inputs.\n\nBased on my understanding, I don’t find the findings to be significantly novel or surprising. In particular, I don’t see any concrete benefits of employing a dynamical systems perspective here. Nevertheless, the experimental analysis is interesting, and I’m hopeful that this direction will lead to more insights in the future.\n\nThe final contribution is a new method to learn the depth of Residual networks during training, but this is insufficiently explored (only tested on the toy dataset) so its practical significance can not be evaluated at this stage.\n\n\nMinor notes:\n- Note that many citations are textual when they should be parenthetical.\n- The reference “No & Liao (2016)” has incorrect author name.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}