{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper suggests that \"attention shift\" is a key property behind failure of adversarial attacks to transfer. The authors propose an attention-invariant attack method by using a Gaussian filter on the gradient step before applying it. This method is shown to circumvent several defenses.\n\nComments: The method is interesting and evidently effective. However, the reason behind so-called \"attention shift\" being behind these defenses is hard to see---more likely, I suspect that the attack is exploiting a property similar to those used in [1]. Furthermore, the paper uses neural networks' shift invariance heavily in the proofs, which has shown to be not true [2,3]. The paper also has many typos and some broken references, which affect the readability of the work.\n\nMy main comment is that this work requires (a) more substantiation of the claim that attention shift is the phenomenon at play when it comes to lack of transferability, (b) improvement to the writing, and (c) more motivation behind the choice of mitigation mechanism.\n\nSmaller comments:\n- page 2 paragraph 3 line 3 “composed of an legitimate”: a instead of an\n- page 3 paragraph 1 line 3 “another line of works that perform attacks” work instead of works and performs instead of perform\n- page 3 paragraph 1 line 6 missing reference\n- page 3 paragraph 2 line 3,4 missing reference\n- page 4 paragraph 2 in section 3.2, and last sentence of pg 4: meaning is very unclear, needs rewriting\n- page 7 paragraph 3 in section 4.3 line 2 “It should by noted“: be rather than by\n\n[1] https://arxiv.org/pdf/1707.07397.pdf\n[2] https://arxiv.org/pdf/1711.09115.pdf\n[3] https://arxiv.org/pdf/1712.02779.pdf",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The motivation of the method is unclear",
            "review": "This paper argues the expansion of the attentive regions (attention shift) as a mechanism of several defense models, then proposes an attention-invariant attack method by convolving attack gradients with the Gaussian filter. The technique demonstrates its effectiveness against various defense models.\n\nThe main concerning to me is that the motivation of using shifting operation to solve the attention \"shift\" is entirely unclear.  As shown in Figure 1, the expansion of the attentive regions seem to be fairly large, so it is difficult to say if shifting the image by several pixels (or convolving the gradients by a 15x15 kernel) can mitigate the effects of attention \"shift\". The true mechanism of this attack could be very different in my opinion.\n\nSecondly, as shown in Figure 2, the patterns of adversarial perturbations of the proposed method seem to be reasonably easier to be detected, or more perceptible to humans (at least to me). I wonder if the authors could training a detection network to distinguish if an image is attacked by the proposed method and report the results.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple and elegant CNN adversarial attack. Some typos",
            "review": "*Summary:* The present paper propose a new way of overcoming state of the art defences against adversarial attacks on CNN. This is done efficiently by noticing that many defences agains adversarial attacks have a an attention shift, and exploiting it efficiently via CNN invariance to translations. The authors shows that the proposed method can fool many state of the art defences.\n\n*Clarity:* The paper is well written and easy to follow. To the best of my knowledge the maths are correct. However the shift mechanism is still not clearly understood. See answer to R1.\n\n*Originality and Significance:*  to the best of my knowledge the proposed method is new and the paper looks complete enough to be fully reproducible. The experiments show that on a wide selection of dataset, many state of the art method are significantly sensitive to the proposed attack. If the paper is accepted ot would be really nice to provide a link to the code used. As the experiments showed that the proposed attacks fools many state of the art defences  and the method is new I believe this paper has a good significance/impact.\n\n*Questions:*\n   - Have you tried to fool the defences with other invariants that CNN are also known to be invariant too? eg rotations or diffeomorphism? Would it be possible to obtain the same computational 'trick' as in eq 9?\n   - I would see the continuous version of the method as integrating over all the possible translation T_{uv} over a measure mu(u, v) (instead of T_{ij}, w_{ij}) is that correct?\n   - Have you tried something else than the gaussian kernel for the convolution? Do you have a proof / explaination / experimental evidence that other kernels such as a uniform / laplacian are less performant than a Gaussian kernel? (point 1) page 5 is convincing but point 2) is irrelevant in my opinion).\n   - Could you propose new defences agains your attack?\n\nTypos:\nsome references (eg p 3) are broken (ie ??)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}