{
    "Decision": {
        "metareview": "This manuscript proposes an extension of convolution operations for manifold-valued data. The primary contributions include the development and description of the approach and implementation and evaluation on real data.\n\nThe reviewers and AC expressed concern about the clarity of the presentation, particularly for a general ICLR audience. Though the contributions are primarily conceptual/theoretical, reviewers expressed concern about the breadth and quality of the presented experimental results. Some additional concerns related to missing proofs and details were addressed in the rebuttal.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Interesting idea, however, more explanations needed. ",
            "review": "This paper proposes a method to use weighted Frechet Mean (wFM) for the operation on Manifold valued data for CNN. The novel point is to view wFM as a convolutional layer. Overall, this paper is mathematically well written, however, how each theory improves CNN and the model used in experiments are not clear enough. \n\nPros\n+ The use of wFM instead of a convolutional layer is an interesting idea. \n+ This paper is mathematically well written. \n\nCos \n- It is hard to understand how each theory presented in this paper helps to improve CNN. For example, the invariance to group operation. Some experimental results would help to understand the advantage of the group invariance.\n\n- It is also unclear why the authors constructed the invariant last layer although the inputs of the last layer are invariant under group operations. \n\n- In the introduction section, the authors raised the omnidirectional camera, diffusion magnetic resonance imaging, elastography as examples of manifold-valued data. However, experiments are limited to standard video sequences. \n\n- It is unclear how to obtain the weights {w_i} of wFM by backpropagation. \n\n- Since the contribution of this paper is to to use wFM instead of a convolutional layer, it is more interesting to visualize the weights {w_i}. \n\n- More explanation needs for the model used for experiments. Especially in dimensional reduction experiments, I could not understand how each subspace is obtained and averaged. If each frame is a subspace, by averaging frames, the reconstruction would be blurred. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "missing important related works, careless writing and insufficient evaluation",
            "review": "This paper introduces a generalization of convolution operations to manifold-valued data using  the computation of the weighted Frechet mean (wFM). Without applying any non-linear units and pooling layers, the paper proposes to merely stack suching generalized convolutional layers to construct a deep network for the data residing on Riemannian manifolds. The evaluations on video classification and reconstruction tasks show the advantages of the introduced network over some baselines. \n\nThe paper’s major contribution is using wFM to generalize the traditional convolution for manifold-valued data.  Accordingly, the critical technical contribution is to estimate the wFM. The paper suggests a inductive/recursive way for the wFM approximation. To the best of my knowledge, there already exist some inductive/recursive wFM estimation methods like [1,2]. Unfortunately, the authors seem to overlook them and do not discuss them at all. Accordingly, I think the paper missed some important related works for sufficient study.\n\n[1] Y. Lim and M. Pa ́lfia. Weighted inductive means .LAA, 453:59–83, 2014.\n[2] Hesamoddin Salehian et al., An efficient recursive estimator of the Fre ́chet mean on a hypersphere with applications to Medical Image Analysis, Mathematical Foundations of Computational Anatomy. 2015.\n\nDue to the inconsistent fonts, chaotic layout it is really hard for the readers to follow the content of the paper. I feel like the paper seems to be completed in the last minute. This brings another critic problem, it is not easy to reimplement the wFM layers, which is the core contribution of the paper. For instance, the paper claims that they used intrinsic Riemannian metric when using wFM to convolve the manifold-valued data. I guess it is involved in \\Gamma_X^Y (Eq.2) which is explained as the shortest curve from X to Y. Anyway it fails to describe what kind of intrinsic metric they used for the specific manifold-valued data like SPD matrices and linear subspace. In addition, is it \\Gamma_{M_{n-1}}^{X_n} rather than \\Gamma_{M_{n-1}}^{X_M} for Eq.(2)? \n\nAnother problem is the evaluations are far from sufficient. For video classification, the paper only uses the moving MNIST, which is not a challenging dataset while there are plenty of large scale video datasets. In addition, the paper is expected to compare SOTA video classification methods. To learn the advantage of the proposed network over some related manifold networks like Huang et al., 2016, Huang & Van Gool 2017, it is necessary to evaluate them in the experiments.  For video reconstruction, using 1000 frame color sample of video is also not sufficient to study the effectiveness of the proposed ManifoldNet. Furthemore, it is also expected to compare more SOTA auto-encoder based reconstruction models like VAE [3], AAE [4]  and WAE [5].\n\n[3] Kingma et al., Auto-encoding variational bayes, 2013\n[4] Makhzani et al., Adversarial autoencoders, 2015\n[5] Wasserstein auto-encoders, 2017\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Too many things bother me to recommend acceptance",
            "review": "Brief summary:\nThe paper considers a generalization of convolutional neural networks (CNNs) to data residing on Riemannian manifolds. The idea is to replace convolutions with weighted averages, which are implemented intrinsically on the manifold. It is shown that this operator is equivariant to isometric group actions. A related approach for dimensionality reduction is also proposed, but I think this only applies to Euclidean data, so I am a bit confused about that part. Empirical performance is reported on toy data with weak baselines.\n\nGood points about the paper:\n+ It is a relevant point that the intrinsic average in manifolds is a way to generalized convolutions to the Riemannian domain.\n+ The paper is generally fairly easy to read.\n\nConcerns with the paper:\n- A key point of CNNs is that you learn filters that are small, i.e. only have non-zero weight assigned to a few neighboring pixels. As far as I can tell, the here proposed \"filters\" would be one weight per data point. \n\n- I am concerned about the stacking of multiple \"convolution\" (wFM) layers: since each layer computes the average of a set of points, then doesn't stacking multiple \"convolution\" (wFM) layers on top of each other correspond to computing the average of a set of averages? And can this not be computed by a single average? In other words, is a cascade of \"convolution\" (wFM) layers equivalent to a single layer? Seems like a complicated way of doing shallow learning unless I misunderstand.\n\n- In section 2.2 it is argued that the weighted average (wFM) is a contraction mapping. While I think the proof is correct, I am concerned about the prerequisite assumption that that distance between a set of points X and Y is the *maximal* distance between points in the two sets. Usually, one would define this distance as the *minimal* distance (akin to the Hausdorff distance). It seems that under this more reasonable choice of distance, the proof no longer holds\n\n- Large parts of section 2.1 is devoted to an efficient algorithm for computing weighted averages on manifolds.  Here the text is written such as to indicate that this is a novel contribution of the present paper, even if these results are readily available in the literature. I strongly encourage a re-writing to emphasize that this is a repetition of previous knowledge.\n\n- Proposition 5 does not appear to come with a proof.\n\n- Section 3.2 introduce a new dimensionality reducing layer based on the Grassmann average construction for subspace learning. I was quite confused when reading this. From what I can tell this layer is only applicable when the input data is Euclidean, and as such appears to be unrelated to the rest of the paper.  \n\n- At times the paper is rather sloppy written, e.g. fonts are way too small in figures, the dot(.) notation is not defined (e.g. in def. 7), and the citation style is very difficult to read (please use \\citet and \\citep instead of \\cite).\n\nOther comments:\n*) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seem quite strong. It would be good to comment on this in the paper.\n\n*) It is not clear to me that the weighted average is a particularly good way to generalize convolution. Yes, I agree, it is *one\n way to generalize, but why should I pick this one in particular?\n\n*) In the experiment in sec. 3.1 the manifold comes from a particular way to extract features from data. In a deep learning context, we would usually learn such features instead of manually designing them. This seem like a general issue, that most often manifold come into play through a manual feature-design, which seem to be at odds with the end-to-end learning mindset. It might be good to have examples where this is not the case.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}