{
    "Decision": {
        "metareview": "This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning. All three reviewers seemed happy to see an attempt to unify these two closely related relation-learning problems. However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence-pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they're tested on.\n\nI'm happy to accept the authors' arguments that sentence-to-vector models have practical value, and I'm not placing too much weight on the reviewer's comments about the choice to use that modeling framework. I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously.\n\nI'm mostly concerned, though, about the lack of an impactful positive contribution: I'd have hoped for a paper of this kind to offer a  a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Interesting framing, but limited contributions"
    },
    "Reviews": [
        {
            "title": "SRL composition functions for Sentence Embedding Tasks",
            "review": "This work proposes a view on tasks requiring pairs of sentence representations from the perspective of statistical relational learning (SRL), where there exist a multitude of composition functions for pairs of entity vectors. The authors systematically categorise different types of composition functions and apply them in tasks for testing sentence representations, with two sentence representation pretraining tasks.\n\nStrengths:\n- proposition of a unifying view onto two mostly separate strands of research\n- principled way of thinking about the requirements to a relational vector composition model\n- systematicity - several composition functions are categorised and systematically compared\n- breadth: comparison of a large variety of composition functions on multiple tasks with two separate sentence embedding pretraining tasks\n- good practice: significance testing of results. \n\nWeaknesses:\n- empirical differences are marginal, but authors claim to “significantly improve the state of the art […]” in the abstract, which I do not see as justified. The major source of variation appears to be the pretraining task chosen, not the composition function.\n- The chosen scope is limited: here sentence representations cannot depend on one another, whereas this is commonplace in practice, e.g. via per-token attention on the other sequence, or in conditional LSTMs.\n- the observations on expressiveness of composition functions in SRL are not new. It would have been interesting to see particular ways in which these observations differ from SRL when lifted over to the new context of sentence representations, rather than entity pairs.\n- The proposed method boils down to combining relatively simple components in a straightforward manner, little innovation in terms of methodology.\n- related work mostly discusses word-level representations, whereas the paper is about sentence-level representations and SRL.\n- particular formulations and claims are in several points unclear, vague, imprecise or too broad — see other comments below.\n\n\nOther comments:\n- imprecise: “reasoning over its input embeddings“. Can this be made more specific? \n- Section 2 unclear: “additive and weak“ interaction. What does that mean?\n- Section 2,  bullet point 3: imprecise, and motivation unclear. Perhaps: formulate in terms of computational complexity?\n- Broad claim - be more specific: “The ComplEx model yields state of the art performance on knowledge base completion“ — other models have been proposed, many of which outperform ComplEx on specific datasets, so the claim in its full generality cannot hold.\n- strong wording in abstract: “we prove that textual relational models implicitly use compositions from baseline SRL models”. In my personal view this is not strong enough a theoretical result to “prove” it.\n- speculative/unclear: “expressive compositions can also be helpful to improve interpretability and evaluation of sentence embeddings by providing sentence level analogies”\n- speculative and vague: “From our previous analysis, we believe this composition function favours the use of contextual/lexical similarity rather than high-level reasoning and can penalise representations based on more semantic aspects. This bias could harm research since semantic representation in an important next step for sentence embedding”\n- Section 4.2: initially unclear goal of this section\n- Four of the Transfer evaluation tasks only use one embedding - h1. While it is interesting to have results also on these tasks, these are somewhat unrelated to the main topic of the paper\n- some more details on significance tests would be useful. Normality assumption? Number of re-runs?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A new learning representation for compositions of text embeddings is proposed and evaluated on two NLP tasks.",
            "review": "The paper describes an new representation for compositions of text embeddings using ideas from statistical relational learning.\n\nThe work is based on the premise that existing simple compositions are not very effective for relational problems in NLP such as semantic similarity, entailment etc. Therefore, the authors propose to use more complex compositions of embeddings to learn richer representations that can be useful several NLP problems that need to relate embeddings. The main idea is to develop compositions for language models based on SRL methods that learn relationships between entities such as ReSCAL and IETrans.\n\nCompositions based on deep semantic meaning in language is a significant problem. The proposed ideas seem to be quite general for NLP compositions. However, some of the listed contributions were not so clear to me. For example, in section 2, it seems like the results were already known that some of the compositions are weak (or maybe the way it is written needs to be changed a little). Regarding the novelty, w.r.t the compositions, it does use existing SRL work but in a different context of NLP problems, this makes novelty a bit weak.\n\nRegarding the experiments, several NLP datasets are used for evaluation across 2 tasks, showing that the method can generalize well.  However, the improvement over existing methods is marginal. Are there tradeoffs w.r.t training time etc. since the compositions are more complex? There is a sentence about it but a little vague. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "What does the SRL lens contribute?",
            "review": "This paper presents a view of sentence-level prediction tasks as statistical relation learning problems. In particular the paper argues that composition functions used in recent SRL techniques developed for entity-to-entity relationship detection can be applied to sentence-level relation prediction tasks. \n\nSuppose there is a prediction training task defined over pairs of sentences (x1, x2). This task requires some function 'f' that composes the sentence representations h1 and h2 into a single representation which is then used to \nmake the relation prediction i.e., we have a model g(f(h1, h2)) that is used to predict some relation between R(x1, x2).  This paper aims to show that with a better 'f' we can hope for a better result in transfer tasks (in addition to doing better on the training task). \n\nThe paper argues that this setting, at a high level, is similar to the composition function used in entity-entity relation prediction. There have been many such methods in the recent past (e.g., TransE, ComplEx, RESCAL). This paper asks whether these composition functions can work well for sentence-level tasks.\n\nThe paper then presents experiments which compare the performance of different composition functions against a basic composition function used in InferSent.\n\nStrengths of the paper:\n\n1. I like the main question of what can we learn from SRL. This seeks to bridge some independent research threads.\n2. The evaluation considers a range of composition functions used in SRL and applies them to the sentence tasks. \n3. Points out that some of the composition functions used in existing models are not particularly strong.\n\nIssues:\n\nI like the starting point for this paper very much and agree that the existing composition functions for sentence relations are rather weak. However, I am struggling to see if there is (i) a convincing conceptual argument for why SRL view of compositions is necessarily the answer for sentence level tasks, or (ii) a convincing empirical case for the same.  Some details on these points:\n\n1) The parallels between entity-entity relations and sentence-sentence relations seems a bit of a stretch to me. There is always some level of abstraction at which two problems might look similar, which can be advantageous for repurposing solutions. However, in this case I think the SRL view of the world hides the complexities in sentence-sentence relation tasks (e.g. aligning relevant pieces of information, requiring more complex composition functions to derive meaning etc.). \n\n2) I am not sure what knowledge we are getting from an SRL view of the problem that is not already known already to the communities that work on sentence embedding. The minimum requirements laid out can be met easily by existing methods for sentence representations. For instance that we need to allow for asymmetric relations (entailment order) is very well known. As the authors themselves point out there are solutions for this problem.  \n\n3) The empirical results don't appear convincing. The average gain for any particular method over InferSent is 0.3 in macro average. There is no single SRL based composition method that works consistently clear gains across most tasks. \n\nHere are some suggestions that I think will improve the paper (or at least help me buy the motivation): \n\n1. One question that might be useful to make a conceptual argument is how much work should be done in 'f' and should it change for the different type of target tasks.\n\nIf the idea is to transfer h for single sentence target task, then a powerful 'f' can render h1 and h2 to be simple enough, such that bulk of the work in extracting task related information might be done by 'f' itself. Therefore, transferred h may not be as powerful as it could have been with a less powerful 'f'.\n\nIf the idea is to transfer f(h1, h2) for sentence-pair target tasks, then a powerful 'f' might be a good thing. \n\n2) Another useful discussion would be to discuss why more powerful alignment based sentence representations are not being considered at least for comparison purposes. \n\nThe paper wants to go from a simple 'f' (i.e. concat(h1,h2), h1-h2) to some other choices for 'f' that are known functions from SRL. \n\nThere are several sentence-level representation functions such as ESIM [Chen et al., 2016] which uses a combined representation of premise and hypothesis sentences using soft alignment to specifically address the issues in comparing sentences. A similar representation is computed in BiDAF [Seo et al., 2017] in the context of matching question representation with sentence representations. \n\nTo summarize, I really like the basic starting point for the paper and would love to see a more compelling presentation of the conceptual argument and a stronger empirical comparison.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}