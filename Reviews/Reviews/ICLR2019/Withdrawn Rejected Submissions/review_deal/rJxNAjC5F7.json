{
    "Decision": {
        "metareview": "The paper proposes learning a hash function that maps high dimensional data to binary codes, and uses multi-index hashing for efficient retrieval. The paper discusses similar results to \"Similarity estimation techniques from rounding algorithms, M Charikar, 2002\" without citing this paper. The proposed learning idea is also similar to \"Binary Reconstructive Embedding, B. Kulis, T. Darrell, NIPS'09\" without citation. Please study the learning to hash literature and discuss the similarities and differences with your approach.\n\nDue to missing citations and lack of novelty, I believe the paper does not pass the bar for acceptance at ICLR.\n\n\nPS: PQ and its better variants (optimized PQ and cartesian k-means) are from a different family of quantization techniques as pointed out by R3 and multi-index hashing is not directly applicable to such techniques. Regardless, I am also surprised that your technique just using hamming distance is able to outperform PQ using lookup table distance.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "The paper needs improvement"
    },
    "Reviews": [
        {
            "title": "Interesting intuition but still far from a real-world solution",
            "review": "This paper is about learning to hash. The basic idea is motivated by the intuition: given points z_i and z_j on the hypersphere, the angle between the two points is arccos(z_i \\dot z_j), while the probability that a random bit differs between them is arccos(z_i \\dot z_j)/\\pi. This leads to a nice formulation of learning Hamming Distance Target (HDT), although the optimization procedure requires every input has a similar neighbor in the batch.\n\nThe minor issue of this paper is that the writing should be polished. There are numerous typos in paper citing (e.g., Norouzi et al in the 3rd page is missing the reference; Figure 3.2 in the 7th page should be Figure 3; and a number of small typos). But I believe these issues could be fixed easily.\n\nThe major issue is how we should evaluate a learning to hash paper with nice intuition but not convincing results. Below are my concerns of the proposed approach.\n\n1. Learning to hash (including the HDT in this paper) and product quantization (PQ) are not based on the same scenario,  so it is unfair to claim hashing method outperforms PQ.\n\nMost learning to hash methods requires two things in the following:\na) the query samples\nb) similar/dissimilar samples (or we can call them neighbors and non-neighbor) to the query\n\nPQ does not require a) and b). As a result, in PQ based systems, a query can be compared with codewords using Euclidean distance, without mapping to a hash code. This is important especially for novel queries, because if the system does not see similar samples during training, it will probably fail to map such samples to good hash codes. \n\nSuch advantage of PQ (or other related quantization methods) is important for real-world systems, however, not obvious in a controlled experiment setting. As shown in the paper, HDT assumes the queries will be similar in the training and testing stages, and benefits from this restricted setting. But I believe such assumption may not hold in real systems. \n\n2. It is not clear to me that how scalable the proposed method is.\n\nI hope section 1.2 can give analysis on both **space** and time complexity of Algorithm 2. It will be more intuitive to show how many ms it will take to search a billion scale dataset. Currently I am not convinced how scalable the proposed algorithm is. \n\n3. Implementation details\nIn page 5, it is not clear how the hyper parameters \\lamda, \\lamda_w and p_0 are selected and how sensitive the performance is. I am also interested in the comparison with [Johnson Dooze Jegou 2017] “Billion-scale similarity search with GPUs”.\n\n4. Missing literature\nI think one important recent paper is “Multiscale quantization for fast similarity search” NIP 2017\n\n\nTo summarize, I like the idea of this paper but I feel there are still gap between the current draft and real working system. I wish the submission could be improved in the future.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid idea in the learning to hash area, needs further development",
            "review": "Summary: This paper contributes to the area of learning to hash. The goal is to take high-dimensional vectors in R^n resulting from an embedding and map them to binary codewords with the goal of similar vectors being mapped to close codewords (in Hamming distance). The authors introduce a loss function for this problem that's based on angles between points on the hypersphere, relying on the intuition that angles corresponds to the number of times needed to cross to the other side of the hypersphere in each coordinate. This is approximately the Hamming distance under a simple quantization scheme. The loss function itself forces similar points together and dissimilar points apart by matching the Hamming distance to the binomial CDF of the angle quantization. They also suggest a batching scheme that enforces the presence of both similar and dissimilar matches. To confirm the utility of this loss function, the authors empirically verify similarity on ImageNet and SIFT.\n\nStrengths: The main idea, to match up angles between points on the hypersphere and Hamming distance is pretty clever. The loss function itself seems generally useful.\n\nWeaknesses: First, I thought the paper was pretty difficult to understand without a lot of background from previous papers. For the most part the authors don't actually state what the input/output/goals are, leaving it implied from the context, which is tough for the reader. The overall organization isn't great. The paper doesn't contain any theory even for simplified or toy cases (which actually seems potentially tractable here); there is only simple intuition. I think that is fine, but then the empirical results should be extensive, and unfortunately they are not. \n\nVerdict: I think this work contains a great main idea and could become quite a good paper in the future, but the work required to illustrate and demonstrate the idea is not fully there yet. \n\n\nComments and Questions:\n\n- Why do you actually need the embedded points y to be on the unit hypersphere? You could compute distances between points at different radii. The results probably shouldn't change much.\n\n- There's at least a few other papers that use a similar idea, for example \nGong et al \"Angular Quantization-based Binary Codes for Fast Similarity Search\" at NIPS 2012. Would be good to discuss the differences.\n\n- The experimental section seems very limited for an empirical paper. There's at least a few confusing details, noted below:\n\n- The experimental results for ImageNet comparing against other models are directly taken from those reported by Lu et al. That's fine, but it does mean that it's hard to make comparisons against *any* other paper than the Lu paper. For example, if the selected ImageNet classes are different, then the results of the comparison may well be different. I checked the HashNet paper (Cao et al. 2017), and it papers that their own reported numbers for ImageNet are better than those of the Lu et al paper. That is, I see 0.5059 0.6306 0.6835 for 16/32/64 bit codewords vs Lu's result of 0.442 0.606 0.684, which is quoted in this paper. What's causing this difference? It would probably be a bit less convenient but ultimately better if the results for comparison were reproduced by the authors, and possibly on a different class split compared to the single Lu paper.\n\n- The comparison against PQ should also consider more recent works of the same flavor as PQ, which themselves outperform PQ. For example, \"Cartesian k-means\" by Norouzi and Fleet, or \"Approximate search with quantized sparse representations\" by Jain et al. These papers also use the SIFT dataset for their experimental result, so it would be great to compare against them.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Is it fair to compare the proposed algorithm to existing hashing algorithms and PQ",
            "review": "This paper proposed a new hashing algorithm with a new loss function. A multi-indexing scheme is adopted for search.  There is one key issue: in general hashing is not good at multi-indexing search for vector-based search in the Euclidean distance or Cosine similarity. The advantage of hashing is reducing the code size and thus memory cost, but it is still not as good as quantization=based approach. \n\nHere are comments about the experiments.\n(1) Table 1: do other algorithms also use multi-indexing or simply linear scan?\n(2)  Figure 4: HDT-E is better than PQ. It is not understandable. Something important is missing. How is the search conducted for PQ? Is multi-indexing used? It is also strange to compare the recall in terms of #(distance comparisons). \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}