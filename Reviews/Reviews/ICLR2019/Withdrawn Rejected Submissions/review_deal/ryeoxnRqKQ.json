{
    "Decision": {
        "metareview": "Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.\n\nI am recommending rejecting this submission for multiple reasons.\n\nGiven that this is a \"black box\" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative-free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder-Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative-free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative-free optimization.\n\nThe method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.\n\nFinally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "the only favorable review does not make a convincing argument to accept the paper"
    },
    "Reviews": [
        {
            "title": "original",
            "review": "In this paper, authors propose a \"universal\" Gaussian balck-box adversarial attack.\nOriginal and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR.\nTo the best of my knowledge, the study is technically sound.\nIt fairly accounts for recent literature in the field.\nExperiments are convincing.\nOne thing I am not so convinced about is the naming of the evaluation curve as \"a new ROC curve\". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK",
            "review": "Summary: In this paper the authors discuss a black-box method to learn\nadversarial inputs to DNNs which are \"close\" to some nominal example\nbut nevertheless get misclassified. The algorithm essentially tries to\nlearn the mean of a joint Gaussian distribution over image\nperturbations so that the perturbed image has high likelihood of being\nmisclassified. The method takes the form of zero-th order gradient\nupdates on an objective measuring to what degree the perturbed example\nis misclassified. The authors test their method against 10 recent DNN\ndefense mechanisms, which showed higher attack-success rates than\nother methods. Additionally the authors looked at transferrability of\nthe learned adversarial examples.\n\nFeedback: As noted before, this paper shares many similarities with\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)\n\nand the authors have responded to those similarities in two follow-ups. I have reviewed these results and their \nmethod does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, \nmainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. \n\nI appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we \nalso see that many of the percentages in Figure 1 converge  to 1. On the one hand this suggests that all defense methods \nare in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \n\"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good evaluation but important prior work was missed which substantially reduces novelty and makes a major rewrite necessary",
            "review": "In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.\n\nAs confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:\n\n* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.\n* Different motivation/derivation of NES.\n* Concept of adversarial distributions.\n* Regression network for good initialization.\n* Introduction of accuracy-iterations plots.\n\nMy main concerns are as follows:\n* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement “However, existing black-box attacks are weaker than their white-box counterparts” is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].\n* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.\n* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.\n\nHence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.\n\n[1] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” (https://arxiv.org/abs/1804.08598) \n[2] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)\n[3] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (https://arxiv.org/abs/1805.09190)\n[4] Athalye et al. (2017) “Synthesizing Robust Adversarial Examples” (https://arxiv.org/pdf/1707.07397.pdf)\n[5] Madry et al. {2017) “Towards Deep Learning Models Resistant to Adversarial Attacks” (https://arxiv.org/pdf/1706.06083.pdf)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}