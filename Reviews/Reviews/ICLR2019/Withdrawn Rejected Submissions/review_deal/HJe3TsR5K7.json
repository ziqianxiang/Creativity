{
    "Decision": {
        "metareview": "This paper proposes a new image to image translation technique, presenting a theoretical extension of Wasserstein GANs to the bidirectional mapping case. \n\nAlthough the work presents promise, the extent of miscommunication and errors of the original presentation was too great to confidently conclude about the contribution of this work. \n\nThe authors have already included extensive edits and comments in response to the reviews to improve the clarity of method, experiments and statement of contribution. We encourage the authors to further incorporate the suggestions and seek to clarify points of confusion from other reviewers and submit a revised version to a future conference.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Shows promise but requires improvements to presentation to make contribution clear"
    },
    "Reviews": [
        {
            "title": "Interesting approach with poor presentation",
            "review": "This paper studies the joint distribution matching problem where given data samples in two different domains, one is interested in learning a bi-directional mapping between unpaired data elements in these domains. The paper proposes a joint Wasserstein auto-encoder (JWAE) to solve this problem. The paper shows that under the decomposable cost metric and deterministic decoding maps, the optimization problem associated with the JWAE formulation can be reduced to a tractable optimization problem. The paper also establishes a generalization bound for the JWAE formulation. Finally, the paper conducts an experimental evaluation of the proposed solution with the help of a video-to-video synthesis problem and show improved performance as compared to the existing results in the literature.\n\nOverall, the reviewer finds that the paper considers an important problem and proposes some interesting ideas to tackle the problem. However, in its current form, there is a large scope for improvement in the presentation of the paper. The paper is full of errors/typos which make it an extremely difficult read (see my comments below). That said the paper fairs quite well as compared to other existing methods. Since the reviewer is not very much familiar with this field, the reviewer leaves it to the other reviewers to decide the significance of these results.\n\nPros: \n\n1) The paper aims to provide a theoretical treatment of the joint distribution matching problem which has many interesting applications, including image-to-image translation and video-to-video synthesis. \n\n2) The proposed method in the paper had good empirical performance on the real world datasets.\n\nCons:\n\nThe paper is very poorly written with many typos and (possibly) mistakes. Some of the comments in this direction are as follows.\n\n1) The paper does not formally define the underlying problem before diving into the details of the proposed solution. The authors only informally talk about the problem in the introduction. Given that the ICLR has a wide audience, it would have been nice if the authors have made the presentation of the paper self-contained.\n\n2) In the same vein, the paper talks about many important quantities without introducing them first. E.g., what are $E_{A}(f^*)$, $E_B^g(f^*)$ etc. in the statement of Theorem 2? These quantities are first defined inside the proofs in the supplementary material!\n\n3) Some of the notation in the paper is also very confusing. For example, cross-domain mapping have two different sets of notations. $(E1oG2, E2oG)$ in Sec. 4.2 and $(G2oE1, G1oE2)$ in Section 5. It should be latter. Similarly, Sec. 4.2 refers to $E1oG1$ and $E2oG2$ as auto-encoders, which should be $G1oE1$ and $G2oE2$, respectively. In Sec. 3, the authors refer to $N$ and $M$ as the number of samples in the $X$ and $Y$ domain, respectively. This is then reversed in Theorem 2 and 4. These are only a small list of large number of such typos. Also, what is the notion defined in the last line of Sec. 3?\n\n4) It is not clear to me why the sets $Q_1$ and $Q_2$ in Theorem 1 are define in their current forms. In particular, it is not clear why the equality hold in Eq. (17) in the proof of Theorem 1. \n\n5) One line in the proof of Lemma 1 says, \"Specifically, we choose its equality, then we have\". Could the authors elaborate on this?\n\n6) Eq. (24) should be inequality?\n\n7) Given that the authors write a regularized problem in (4). Does that mean now sets $Q_1$ and $Q_2$ are different from how they are defined in the statement of Theorem 1?\n\n#########################\n\nPost rebuttal: The authors have addressed most of my concerns regarding the poor presentation of the earlier version. I have updated my score.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "some issues prevent me from recommending an acceptance",
            "review": "This paper proposes a joint Wasserstein Auto-Encoder (JWAE) method to solve the problem of joint distribution matching. Instead of Ô¨Ånding a coupling, the paper seeks a decoupling to make the primal problem of Wasserstein distance tractable. The decoupled version of joint Wasserstein distance is used for empirical reconstruction losses of within-domain Auto-Encodings and cyclic mappings. In addition, two GAN divergences are used to learn the cross-domain mappings such that the generated distributions are close to the real distribution, and another GAN divergence is imposed to align the latent distributions generated by two Auto-Encoders. Later, the paper applies the proposed model on the interpolation based video-to-video synthesis problem. \n\nAs far as I understand, the paper can be thought of revisiting the Cycle-Consistent Adversarial Networks (CycleGAN) from the joint Wasserstein Auto-Encoder point of view. In other words, it essentially extends the CycleGAN using additional within-domain auto-encoding reconstruction losses and the latent code alignment loss. Accordingly, the proposed model can be naturally applied to image-to-image translation. I have no idea why the paper merely applies it to interpolation based video-to-video translation. In addition, as the paper tries to apply the relaxed optimal Wasserstein distance to Auto-Encoder and cycle consistency losses, why not apply such Wasserstein distance to the distribution divergence as well. To study the generative power of the proposed generative model using the relaxed Wasserstein distance, it is quite necessary to evaluate the use of exit Wasserstein distance based VAE (e.g., Wasserstein AE) and GAN (e.g., Wasserstein GAN and spectral normalized Wasserstein GAN) losses. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a straightforward extension to WAE",
            "review": "The whole model can be simplified by this: using auto-encoders for X and Y's reconstruction, then use Triple GAN loss to match the joint distribution of (X, Y).  However, the deterministic model with GAN loss looks problematic to me.\n\nquestions:\n\n1. Although the authors showed strong evidence in their experiment part, they still failed to compare models with Bicycle-GAN, i.e., how Bicycle GAN performs on these two dataset?\n\n2. missing some comparison: why use simplified Triple-GAN loss (i.e. without two regularization terms)  instead of Triangle-GAN, which is addressed to be better? I think the authors need to discuss about this. Also, the authors need to use MMD and other methods mentioned in the original WAE paper.\n\n3. In table 1, without triple-GAN loss, the whole model is deterministic, but the authors can still show the FID score for the generalization ability, which is better than all other cycle-GAN based models, why is that possible? Is this equivalent to claim that auto-encoder has the ability to generate realistic images just by sampling z? \n(If I understand the experiment correctly, the author's synthesized images is generated by $y_hat = G_2(E_1(X))$, no sampling z required)\n\n4. Can the authors show the generalization ability of JWAE? For example, with input X, we can have different correct corresponding Ys, just like Bicycle-GAN did.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}