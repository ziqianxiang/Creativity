{
    "Decision": {
        "metareview": "The reviewers raise an important issue about the parameters in the proposed gradient in Theorem 1. There could be different parameters for each policy in the gradient (though some parameter sharing could be possible), and computing this gradient would be prohibitive. The solution is to just use the most recent parameters, but then the gradients become off-policy again without motivation for why this is acceptable. This approximation needs to be better justified. \n\nAs an additional point, there are other off-policy policy gradient methods, than just DPG. The authors could consider comparing to these strategies (which can use replay buffers) and explain why the proposed strategy provides benefits beyond these. What is inadequate about these methods? Further motivation is needed for the proposed strategy. This is additionally true because the proposed strategy requires entire sampled trajectories for a fixed policy (to make the policy gradient sound, with weighting dpi_n(s)), whereas DPG and other off-policy AC methods do not need that.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Issues with the proposed gradient"
    },
    "Reviews": [
        {
            "title": "Seems a trivial extension of TRPO",
            "review": "The paper tries to bring together the replay buffer and on-policy method. However, the reviewer found major flaws in such a method.\n\n- Such replay buffers are used for storing simulations from several policies at the same time, which are then utilised in the method, built upon generalised value and advantage functions, accommodating data from these policies.\n\nIf the experience the policy is learning from is not generated by the same policy, that is off-policy learning. \n\nIn the experiment part, the replay buffer size is often very tiny, e.g., 3 or 5. The reviewer believes there may be something wrong in the experiment setting. Or if the reviewer understood it incorrectly, please clarify the reason behind such a tiny replay buffer.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interessting work but some open questions remain",
            "review": "The authors introduce a off-policy method for TRPO by suggesting to use replay buffers to store trajectories and sample from them during training. To do this they extend the definition of the Q function to multiple policies where the Q_pi bar is then the expectation over the several policies. They propose the same for the value function and consequently the advantage function. \nIn my opinion this is some interesting work, but there are some details that are not clear to me, so i have several questions.\n\n1. Why is it necessary to define these generalized notions of the Q, Value and Advantage functions? You motivate this by the fact the samples stored in the replay buffer will be generated by different policies, i.e. by differently parametrized policies at a certain time step. But this also holds almost all algorithms using replay buffers. Could you plese explain this part further?\n\n2. In eq. (26) you introduce the parameter alpha as a sort of Lagrange multiplier to turn the unconstrained optimization problem defined by TRPO into a constrained one. This is was also proposed early by Schulman et al. in Proximal Policy Optimization. Yet, it is not cited or referenced. In the discussion of the experimental results go further into this. Please explain this part in more detail.\n\n3. Another point of your work is the learnable diagonal covariance matrix. How can you be sure that the improvements you show are due to the replay buffers and not due to learning these? Or learning covariance in combination with the penalty term alpha?\n\n4. Can you provide comparative results for PPO? PPO outperforms DDP and TRPO on most tasks so it would be interessting to see\n\n5. How many trajectory samples do you store in the replay buffers? Can you provide results where you use your method but without any replay buffers, i.e. by using the last batch of data points?\n\nMinor Suggestions:\n- The references for the figures in the Experiments part are off. In fig. 1 you cite Todorov et al. for Mujoco but not TRPO and ACKTR, the same in fig. 2. Then in fig. 3 you cite DDPG also with Todorov et al.\n- Some parts of the text is a bit unorganized. In section 2.1 you introduce AC algorithms and on the next page you give the definitions for all components but you don't say anything about how the interact. Also, the definition of the expected return was not \"invented\" by Schulman et al, and neither were Advantages, Q-, and Value functions. Maybe add a second or third reference.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting Generalization of G/V/advantage function but some clarifications are needed. ",
            "review": "In this paper, the authors present how to integrate replay buffer and on-policy trust region policy optimization (TRPO) by generalizing Q/V/advantage function and then empirically show the proposed method outperforms TRPO/DDPG.\n\nThe generalization of advantage function is quite interesting and is well written. One minor issue is that d^{\\pi_n} (s) is confusing since it appears after ds. \n\nThe theory in Section 3.1 makes sense. However, due to the limitation in Theorem 1 that $\\theta$ is the joint parameters, applying Theorem 1 can be difficult. In Eq (25), what is the $\\theta$ here? And what does $\\nabla_\\theta \\pi_n$ mean? Does $\\pi_n$ uses $\\theta$ for computation? One of the problems of using replay buffers in on-policy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue. Further explanation on Eq (25) might help. If the distributions of states are assumed to match, then the joint distribution of states and actions may mismatch so additional importance sampling might help, as suggested in [1] Eq (3). \n\nAnother problem is on the barrier function. In Eq (26), if we only evaluate $\\rho_b(\\theta)$ (or its gradient w.r.t. $\\theta$) at the point $\\theta_old$, it doesn't differ with or without the barrier function. So in order to show the barrier function helps, we must evaluate $\\rho_b(\\theta)$ (or its gradient) at a point $\\theta \\neq \\theta_old$. As far as I know, the underlying optimizer, K-FAC, just evaluates the objective's (i.e., $\\rho_b$) gradients at $\\theta_old$. Both Conjugate Gradient (CG), which TRPO uses, and K-FAC are trying to solve $F^{-1} g$ where $g$ is the gradient of the objective at the current point. \n\nThe experiments show significant improvement over TRPO/DDPG. However, some experiments are also expected.\n1. How is the proposed algorithm compared to PPO or Trust PCL? \n2. How does the barrier function help? More importantly, what's the comparison of the barrier function to [1] Eq (5)? \n\nThe proposed algorithm seems more like a variant of ACKTR instead of TRPO since line search is missing in the proposed algorithm and the underlying optimizer is K-FAC instead of CG.\n\nRef:\n[1]: Proximal Policy Optimization Algorithms, by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}