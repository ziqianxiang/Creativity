{
    "Decision": {
        "metareview": "Several visualizations are shown in this paper but it is unclear if they are novel.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "All reviewers agree that paper is not strong enough"
    },
    "Reviews": [
        {
            "title": "ideas seems a common practice in various prior visualization tasks",
            "review": "Paper presented a new and simple method to visualize the embedding space geometry rather than standard t-SNE or PCA. The key is to carefully select items to be visualized/embed and interpretable dimensions. A few case study and user study were conducted to show the benefit of the proposed approach. \n\n- on algebraic formulae (AF): it would be good to clarify the def of AF explicitly. Rules/extention/axes are not very clear and mathematically consistent in section 3. Would projection idea be applied to arbitrary AFs?\n\n- while the idea being simple, I am not quite confident about the novelty. For example for the de-bias application, Bolukbasi et al. had already did the same plot along the he-she axis. Similar plots on the polysemous word embedding can be found in Arora et al., 2017, etc. \n\n- The user study with n=10 are typically less reliable for any p-value evaluation. \n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "The idea of analyzing embedding spaces in a non-parametric (example-based) way is well-motivated. However, the main technical contribution of this paper is otherwise not clear - the methodology section covers a very broad set of techniques but doesn't provide a clear picture of what is novel; furthermore, it makes a strong assumption about linear structure in the embedding space that may not hold. (It's worth noting that t-SNE does not make this assumption.)\n\nThe visualization strategies presented don't appear to be particularly novel. In particular, projection onto a linear subspace defined by particular attributes was done in the original word2vec and GloVe papers for the analogy task. There's also a lot of other literature on interpreting deeper models using locally-linear predictors, see for example LIME (Ribeiro et al. 2016) or TCAV (Kim at el. 2018).\n\nEvaluations are exclusively qualitative, which is disappointing because there are quantitative ways of evaluating a projection - for example, how well do the reduced dimensions predict a particular attribute relative to the entire vector. Five-axis polar plots can pack in more information than a 2-dimensional plot in some ways, but quickly become cluttered. The authors might consider using heatmaps or bar plots, as are commonly used elsewhere in the literature (e.g. for visualizing activation maps or attention vectors).\n\nUser study is hard to evaluate. What were the specific formulae used in the comparison? Did subjects just see a list of nearest-neighbors, or did they see the 2D projection? If the latter, I'd imagine it would be easy to tell which was the t-SNE plot, since most researchers are familiar with how these look.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting but not clear how useful",
            "review": "To the best of my understanding the paper proposes some methodological ideas for visualizing and analyzing representations. \nThe paper is unclear mainly because it is a bit difficult to pinpoint the contribution and its audience. What would help me better understand and potentially raise my rating is an analysis of a classical model on a known dataset as a case study and some interesting findings would help make it more exciting and give the readers more incentives to try this out. Like train an AlexNet and VGG imagenet model and show that the embeddings are better aligned with the wordnet taxonomy in one of the other. This should be possible with their approach if i understand it correctly. \n\npros:\n- visualization and analysis is a very exciting and important topic in machine learning\n- this is clearly useful if it worked\ncons:\n- not sure what the contribution claim for the paper is since these types of plots existed already in the literature (is it a popularization claim ?)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}