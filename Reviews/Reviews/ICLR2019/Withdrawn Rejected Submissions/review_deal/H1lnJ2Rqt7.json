{
    "Decision": {
        "metareview": "I would like to commend the authors on their work engaging with the reviewers and for working to improve training time. However, there is not enough support among the reviewers to accept this submission. The reviewers raised several important points about the paper, but I believe there are a few other issues not adequately highlighted in the reviews that prevent this work from being accepted:\n\n1. [premises] It has not been adequately established that \"large batch training often times leads to degradation in accuracy\" inherently which is an important premise of this work. Reports from the literature can largely be explained by other things in the experimental protocol. Even the framing of this issue has become confused since, although it may be possible to achieve the same accuracy at any batch size with careful tuning, this might require using (at worst) the same number of steps as the smaller batch size in some cases and thus result in little to no speedup. For example see https://arxiv.org/abs/1705.08741 and recent work in https://arxiv.org/abs/1811.03600 for more information. Even Keskar et al. reported that data augmentation eliminated the solution quality difference between their larger batch size and their smaller batch size experiments which indicates that even if noisiness from small batches serving to regularize training other regularization techniques can serve just as well.\n\n2. [baseline strength] The appropriate baseline is standard minibatch SGD w/momentum (or ADAM or whatever) algorithm with extremely careful tuning of *all* of the hyperparameters. None of the popular learning rate heuristics will always work and other optimization parameters need to be tuned as well. If learning rate decay is used, it should also be tuned especially if one is trying to measure a speedup. The submission does not provide a sufficiently convincing baseline.\n\n3. [measurement protocol] The protocol for measuring a speedup is not convincing without more information on how the baselines were tuned to achieve the same accuracy in the fewest steps. Approximating the protocols in https://arxiv.org/abs/1811.03600 would be one alternative.\n\nAdditionally there are a variety of framing of issues around hyperparameter tuning, but, because they are easier to fix, they are not as salient for the decision. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "not enough support from reviewers to accept this paper"
    },
    "Reviews": [
        {
            "title": "Clearly an unfinished paper",
            "review": "The authors propose using information from the Hessian to grow the batch size as the training progresses. It is well-known that larger batch sizes can be used for later stages of optimization (ie, https://arxiv.org/abs/1711.00489, https://arxiv.org/abs/1706.05699), but they are missing motivation as to why use Hessian information for this.\n\nFurthermore, the description of the algorithm is lacking detail and is essentially unreproducible in current form.\n\nThe main description of their method is Algorithm 1 box, which suggests to grow batch size when \"eigenvalue\" is much smaller than previous eigenvalue. Is that the top eigenvalue? How is it estimated? Why is that the criterion? Note that for stochastic least squares problem one benefits from later batch sizes in later stages of optimization even though Hessian doesn't change.\n\nIn section Section 4.3 they start talking a bit about computing Hessian, referring to non-existent figure 6 for details of block approximation.\n\nAuthors mention that Hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for ImageNet experiments?).\n\nNote that a single row of Hessian (hence full Hessian) can be computed in all major frameworks by differentiating an element of the gradient. IE, in PyTorch https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7, and also eigenspectrum of Hessian can be approximated -- https://github.com/noahgolmant/pytorch-hessian-eigenthings",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting work, but the theoretical part is not strong enough",
            "review": "This paper studies the large batch size training of neural networks, and incorporates adversarial training and second-order information to improve the efficiency and effectiveness of the proposed algorithm. In particular, the authors use second-order information to automatically generate the step size and batch size in each iteration, and apply adversarial training as a regularization method to improve the test performance. Finally, the authors demonstrate their algorithm and compare it with the baseline algorithms on a wide range of datasets. This paper is clearly written and has the following strength:\n\n1.\tThis paper proposes an adaptive method for SGD training, which proves its convergence for strongly convex optimization.\n2.\tThis paper incorporates the adversarial training and robust optimization into the adaptive SGD training, and shows that this combination significantly improves the test performance.\n3.\tThe authors perform experiments on different datasets, which show that the proposed method enjoys less training time and higher accuracy when using large batch size.\n\nHowever, this paper also has the following weakness:\n\n1.\tThe theoretical analysis is somewhat trivial, and the assumption on the objective function is rather strong, which is not consistent with the nonconvex loss functions that are widely applied in training neural networks.\n2.\tTheorem 1 provides convergence rate of SGD on strongly convex objective functions. However, the authors do not carefully characterize the learning rate to ensure that the loss function achieve \\epsilon-accuracy. Moreover, in order to make the last term in (5) be smaller than \\epsilon, the learning rate \\eta_0 should be in the order of O(\\epsilon), which is no longer a tuning-free parameter.\n3.\tThe authors mention that the proposed algorithm converges faster than basic SGD, but it is not clearly demonstrated from Theorem 1.\n4.\tI am confused about how to determine the number of iterations for different algorithm as shown in Table 1s and 2? Do you stop each algorithm when they attain the same training error on the training dataset?\n5.\tIt is also confused that the number of iterations for ABS and ABSA are relatively larger than that of BL (Tables 1 and 2), but the training time of BL is longer than those of ABS and ABSA as reported in Table 3?\n6.\tSome minor flaws. In (9) it should be \\|\\nabla L(\\Theta)\\|^2; in Lemma 3, the expectation on the left side should be taken conditioned on \\theta_t.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising numerical results, but lacks clear description and explanation of the algorithm",
            "review": "Based on my understanding, this paper describes a novel approach for addressing the large batch training problem. The authors propose increasing the batch size based on reductions in the largest eigenvalue of the Hessian. This is combined with adversarial training using the fast gradient sign method to reduce the total number of iterations required for training and improve generalization performance. Unfortunately, although the numerical results seem quite promising, the algorithm and its explanation and details are not described clearly in the paper, which makes me lean towards rejection. I describe this more fully below:\n\n1. Description of the Algorithm\n\nThe description of the algorithm in Section 3.1 is simply not clear, and lacks clear exposition motivating why the algorithm ought to work. To add to this confusion, there appear to be some inconsistencies between the (brief) description of the method and the description given in the Main Contributions and Limitations section in the Introduction. \n\nAs an example, in Section 3.1, the approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance <= 10^-2), but this should be elaborated on in Section 3.1. In fact, the limitations should not be discussed until a clear description of the algorithm is given.\n\nThe introduction makes this even more confusing by claiming the second order information is computed by “backpropagating the Hessian operator”. This seems to imply that the 3rd derivative information is computed for second-order information. Later in the Introduction, the authors claim to use Hessian matvecs to perform the power iteration. I believe that the authors mean that the Hessian-vector product is obtained by differentiating the product g’v (a scalar quantity). \n\nIn addition, it was not described how the learning rate is changed in the algorithm. Later in the experiments, none of the additional hyperparameters in the procedure are given, such as the duration factor, kappa, the hyperparameters in the adversarial training, and more. This all ought to be included for completeness.\n\n2. Questions about Details of the Algorithm\n\nIf it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? In addition, if the intention was to compute the largest eigenvalue of the Hessian, one must be wary that the power iteration/Lanczos method computes the eigenvalue with largest magnitude (the absolute value of lambda), which may mean that it’s possible that the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima. This needs to be addressed.\n\nSecondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian. This is certainly not true for all optimization problems. Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region? This intuition and explanation needs to be expanded on (and argued for) in order for the algorithm to make any conceptual sense.\n\nLastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? How is the duration factor tuned?\n\n3. Inconsequential Theoretical Results\n\nThe authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2] and does not utilize any of the algorithmic mechanisms described in the paper. Hence, the theoretical result is not novel, does not provide any additional insight on the algorithm, and could be applied to any adaptive/changing batch size SG algorithm. In my opinion, this ought to be removed. (Assumption 2 is also mentioned in the main paper, but is only described in the Appendix.)\n\n4. Additional Considerations\n\nThe paper is missing much work done by Nocedal’s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3].\n\nOther relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. \n\nLastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. It may make sense to plot test loss/error against epochs and batch size against iterations for clarity.\n\nTypos/Grammatical Errors:\n- Page 2: Should not state “(We refer to this method as ABS)”, easier to include by including (ABS) after Adaptive Batch Size in the beginning of the bullet point.\n- Page 6: Section 4: “information” not “informatino”\n- Page 6: Section 4: “the” not “teh”\n- Page 7: Section 4.1: “confirms” not “confirming”\n- Page 7: Section 4.1: no “a” in “a very consistent performance”\n\nSummary:\n\nOverall, although the paper presents some promising numerical results, it lacks a detailed description and explanation of the algorithm to be worthy of publication. It leaves many aspects of the algorithm open to the reader’s interpretation, and I do not believe I could reproduce the results with the information provided. The manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.\n\nReferences:\n[1] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for machine learning.\" arXiv preprint arXiv:1802.05374(2018).\n[2] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. \"Optimization methods for large-scale machine learning.\" SIAM Review 60.2 (2018): 223-311.\n[3] Byrd, Richard H., et al. \"Sample size selection in optimization methods for machine learning.\" Mathematical programming134.1 (2012): 127-155.\n[4] Chaudhari, Pratik, et al. \"Entropy-sgd: Biasing gradient descent into wide valleys.\" arXiv preprint arXiv:1611.01838(2016).\n[5] Jastrzębski, Stanisław, et al. \"DNN's Sharpest Directions Along the SGD Trajectory.\" arXiv preprint arXiv:1807.05031(2018).\n[6] Sagun, Levent, et al. \"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks.\" arXiv preprint arXiv:1706.04454 (2017).\n[7] Zhu, Zhanxing, et al. \"The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent.\" arXiv preprint arXiv:1803.00195 (2018).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}