{
    "Decision": {
        "metareview": "All three reviewers agree that the research question—should pretrained embeddings be used in code understanding tasks—is a reasonable one. However, there were some early issues with the way in which the paper reported results (involving both metrics and baselines). After some discussion with the reviewers, it seems that the paper now presents a clear picture of the results, but that these results are not sufficiently strong to warrant acceptance. \n\nI'm wary to turn down a paper over what are basically negative results, but for results like this to be useful to the community, they'd have to come from a very thorough experiment, and they'd have to be accompanied by a frank and detailed discussion. Neither of the two more confident authors are convinced that this paper meets that bar.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Reasonable experiments, but limited contributions"
    },
    "Reviews": [
        {
            "title": "Incremental empirical study",
            "review": "The paper presents some experiments using pre-trained code embeddings on the task of predicting a method name from the code of method body. The paper is well written and the motivations and the design of empirical study are clear.\n\nThe empirical results of validation loss in Figure 1 is reporting the behaviour of random initialization of embedding. From the plots of 10 projects we may derive a couple of claim: (i) the validation loss of random initialization after 10 epochs may increase and get unstable, (ii) random initialization after 5-10 epochs may reach the same loss as pre-trained embeddings. The working assumption is that pre-trained embedding should speed-up the learning process. The empirical results show that it is not just a matter of reducing the training time but also of performance. The discussion is neglecting to comment this behaviour that looks not compliant with the working assumptions. \n\nMinor comment. The reference [Allamanis et al., 2016] is pointing to arxiv.org despite the fact that the work is published as Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and important research questions, unconvincing results",
            "review": "This paper sets to understand whether pretraining word embeddings for\nprogramming language code by using NLP-like language models has an\nimpact on extreme code summarization task (i.e., generate/predict the\nname of a function based on its body).\n\nI think the paper asks some important questions, however the execution\nof the research and the results presented are not convincing.\n\nI think the area is relevant and the research questions are worth\npursuing; however the work as it is presented in the paper needs\nimprovement to be accepted for publication.\n\nPros:\n* The study of language models for programming language code\n* Pretraining is performed for 3 different languages (C, Java, Python) - target task is in Python\n\nCons:\n* Strange claims of speedup and performance improvement\n* Inconclusive results\n\nSome suggestions for improvement:\n\n* The section on language models pretraining is very sparse, more\n  details are needed.\n\n* The claims of speedup and improvement are strange. Speedup refers to\n  the training speed, I suppose. The performance of the downstream\n  task is never discussed. Only the validation loss is shown and all\n  the performance \"improvement\" is discussed on these graphs, which I\n  found strange. Also, the graphs have their y-axes starting at\n  non-zero values. I personally prefer graphs that start at zero and\n  if there is a need to \"zoom-in\" find a way to \"zoom-in\" to the part\n  of the graph that is important.\n\n* In general the paper writing and reporting on the experiments sounds\n  ad-hoc and not well thought-out.\n\n* I don't agree with many of the explanations in the paper. For\n  example (page 6), it's not true that the extreme summarization task\n  does not require much of the syntactic information (there are\n  submission at the current ICLR'19 that show exactly the opposite,\n  encoding based on syntactic information is useful). The model\n  studied in the paper does NOT use any syntactic information, it\n  treats the code like a sequence of tokens.\n\n* The last question in Section 6 is not a Yes/No question, the answer\n  is phrased as a Yes/No question.\n\nI encourage the authors to pursue the research questions, however in a\nmore systematic and with better methodology.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Are pre-trained code embeddings more effective than pre-trained natural language embeddings?",
            "review": "THE EFFECTIVENESS OF PRE-TRAINED CODE EMBEDDINGS\n\nSummary:\n\nThis work shows how pre-training word vectors using corpuses of code leads to representations that are more suitable than randomly initialized (and trained) representations for function/method name prediction (here called extreme summarization). This paper applies a standard language model to several collected corpuses of code written in different programming languages in order to pretrain the embeddings. It then uses a standard model for the extreme summarization task that takes pre-trained language model embeddings. This leads to speedups in training, improvements in validation loss, and less overfitting. I worry that these results didn't really need much proving given that we have already seen the exact same methods work with natural languages. It would actually be more surprising if they didn't work for programming languages, which suggests that the real question is whether code embeddings are actually more effective than natural language embeddings for this problem given that the authors show syntax of the code is far less important than the semantics of the words in the vocabulary. It is not clear that embeddings pre-trained with much more data on natural languages wouldn't work just as well.\n\nPros:\n\nThis paper takes the time to clearly explain the effectiveness of pre-training words vectors in a new setting. It is easy to follow and understand thanks to the clear organization and exposition.\n\nSpeedup and validation loss improvements are demonstrated for a variety of programming languages despite the final evaluation being only in Java, which is quite surprising. \n\nThe authors discover that overlap in actual programming language syntax is less important than overlap in semantic representation. \n\nBecause the models are out-of-the-box, it is easy to focus on the actual contributions of this paper related to pre-training.\n\nDoes seem to clearly demonstrate that pre-trained embeddings of some form should be used for this extreme summarization task.\n\n\nCons:\n\nIt is not clear how big the collected corpus is. This is important because the work that this paper cites on pre-trained embeddings (Mikolov et al 2013, Pennington et al 2014, McCann et al 2017, Peters et al 2018) typically use fairly large datasets for pre-training. All of these results might be watered down by insufficient pre-training data for the language model when in fact the results could be much stronger with more data. It would be nice to show the effects of pre-training dataset size as is done in the aforementioned previous works. Without this comparison, it is hard to tell whether the paper sufficiently explores this idea.\n\nThe models are both standard, out-of-the-box models. There is no novelty on the modeling side of this paper.\n\nThe pre-training methods are also not novel. They are methods that have already been shown to work applied in a slightly different setting.\n\nIt is not clear that the setting is actually different enough to require this pre-training. Comparing to randomly initialized embeddings is fine, but I would also like to see a comparison to other pre-trained embeddings like GloVe, GloVe+CoVe, or ELMo (Pennington et al 2014, McCann et al 2017, Peters et al 2018). Since the authors find that it is the semantics of the words that matter more than the syntax of any particular programming language, then perhaps it would actually be better to use pre-trained embeddings that tap into much larger amounts of data. At the very least, it seems it would make sense to perhaps supplement a standard pre-trained embedding with those suggested by the authors since so many of the words in the code must be English words. If this is too farfetch'd, then I would suggest that the authors provide some statistics showing why GloVe, GloVe+CoVe, and ELMo are not appropriate starting points for comparison, but the overlap from the pre-training corpuses is already so low that it seems supplementing with standard pre-trained embeddings should only help.\n\nThe evaluation dataset detailed in Allamanis et al 2016 uses two metrics: an F1 metric and an exact match metric. This paper only compares on validation loss. What's more, it reports everything in relative terms so that the raw improvement is masked until Figure 1 makes it somewhat possible to deduce. The problem here is that we don't know how a 0.0-0.5 raw improvement in validation loss translates to the metrics established for the dataset by Allamanis et al 2016. If those are no longer the standard metrics, the authors should explain how validation loss came to supplant the original metrics proposed by Allamanis et al 2016.\n\nWhat's more, there is no context for how well models typically do on this evaluation task. Without any comparisons it is impossible to tell whether any of the experiments are using models in a reasonable realm of performance on this task.\n\nOverall:\n\nAll these effects have already been shown for pre-trained embeddings in the past, and the experiments involve running standard methods on newly collected datasets. This means there is no novelty in the pre-training method or the extreme summarization method. Little is known about the newly collected datasets, it is not clear how to interpret the relative improvements in validation loss compared to the original metrics of Allamanis et al 2016, and the paper lacks necessary comparisons to othef pre-trained embeddings, so though the overall claim that pre-trained embeddings should be used for this task seems to hold up, it is not clear that this is a complete argument for the method chosen by the authors.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}