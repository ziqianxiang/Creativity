{
    "Decision": {
        "metareview": "This paper proposes to unroll power iterations within a Slow-Feature-Analysis learning objective in order to obtain a fully differentiable slow feature learning system. Experiments on several datasets are reported. \n\nThis is a borderline submissions, with reviewers torn between acceptance and rejection. They were generally positive about the clarity and simplicity of the presentation, whereas they raised concerns about the relative lack of novelty (especially related to the recent SpIN model), as well as the current limitations of the approach on large-scale problems. Reviewers also found authors to be responsive and diligent during the rebuttal phase. The AC agrees with this assessment, and therefore recommends rejection at this time, encouraging the authors to resubmit to the next conference cycle after addressing the above points. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting approach but somewhat limited analysis"
    },
    "Reviews": [
        {
            "title": "A natural place to incorporate deep learning",
            "review": "The authors state a clear summary of their contribution to Slow Feature Analysis (SFA) in Section 5: \"The key idea for gradient-based SFA is that a whitening layer can be applied subsequently to any differentiable architecture (such as deep neural networks) to enforce outputs that approximately obey the SFA constraints while still being a differentiable architecture.” As a result, the proposed method can replace the previous SFA pipeline of [fixed, non-linear feature extraction + learned linear feature extraction] with simply end-to-end [learned, non-linear feature extraction]. The resulting method is thus more flexible and expressive and less hand-crafted than traditional SFA approaches. The paper shows experiments that validate that the method can learn meaningful representations in practice.\n\nThe writing is difficult to follow, unclear in several places, and has grammatical mistakes. As a result, it is more challenging to understand the proposed method, its motivation, and its precise place among related literature. For example, the authors discuss SpIN, and from that discussion, it seems that SpIN are quite similar; SpIN was submitted to arXiv 3-4 months before the ICLR deadline and thus is not concurrent as the authors claim (on my understanding, from reading the ICLR and other ML conference guidelines).\n\nThe approach does seem somewhat incremental, though I would be open to changing my mind on author response. On one view, this method can be seen as simply replacing SFA’s fixed, non-linear feature extraction with learned non-linear feature extraction, with a trick to make it work. Deep learning consistently improves over fixed non-linear feature extractors, so it is not surprising a surprising place to incorporate neural networks.\n\nBased on the method, this paper could go either way, but given my concerns on its novelty and writing quality/clarity, I lean slightly towards reject.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Gradient-based SFA",
            "review": "In this paper the authors present a differentiable objective for slow feature analysis, to facilitate end-to-end training.   I am not clear on the novelty of this formulation, as it appears to have been proposed in a similar form in previous works (e.g., A maximum-likelihood interpretation for slow feature analysis by Turner and Sahani - Eq., (2)) and can probably be considered straightforward.  Nevertheless, the approximate whitening layer and the way it is used is a smart approach for this problem.  The experiments are interesting and shed light on the properties of the method.  In summary, the paper may lack technical novelty in some respect, but the experiments are convincing in terms of proof-of-concept, and the approach is smart.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Neural implementation of approximate SFA with a whitening layer using power iterations.",
            "review": "Summary\nThe manuscript proposes to use power iterations in an approximate \"whitening layer\" to optimize the slowness objective of SFA in a very general setting. A set of experiments illustrates that this way of doing nonlinear SFA is meaningful.\n\nQuality\nAlthough the idea is pretty straight forward and the paper shows qualitative results on a number of datasets, the relative merit of the approach is empirically not well characterized.\n\nClarity\nThe manuscript is in general well written and the technical content is well accessible. However the description of the whitening layer implementation needs some more details.\n\nOriginality\nThe idea of using a whitening layer together with the slowness objective has not been explored before. There is a second ICLR 2019 submission (Pfau et al.) with a very similar idea, though.\n\nEmpirical Evaluation\nThe approximate whitening should lead to a trade-off between whitening and slowness optimization. I miss an experiment illustrating that trade-off. Also the comparison to nonlinear SFA using expansion or kernelization of hierarchical SFA is empirically not properly characterized. In the end, if one takes the slowness objective seriously, one would use the method yielding slower results.\n\nSignificance\nThe manuscript introduces a way of running nonlinear SFA with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.\n\nReproducibility\nThe data is either synthetic or publicly available. The Keras implementation of the PowerWhitening layer as well as the entire neural network along with its optimization schedule is not shared. Hence, there should be some effort involved to reproduce the experiments.\n\nPros and Cons\n1+) The idea of an approximate whitening layer is conceptually simple and clear.\n2-) The description of the practical implementation of the power iteration is slightly imprecise.\n3-) The algorithm scales badly in the number of output dimensions. This scaling is bad in a computational sense and also in a statistical sense.\n\nDetails\na) Section 6.1: Why do you need to add the noise term? What is the statistical meaning of this added noise?\nb) Section 6.1: the solutions if comparable -> the solutions is comparable\nc) References: Shaham -> ICLR 2018 paper\nd) References: nyström -> Nyström\ne) The name for the algorithm \"Power SFA\" is a little bit bold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}