{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good results, but method unconvincing",
            "review": "Summary:\nThis paper presents an end-to-end learnable deep network for action recognition that estimates \"optical flow\" within the neural network layers. The \"flow\" is quoted as it may not be the optical flow in traditional sense over image frames, but could be the \"flow\" over the intermediate channels of a layer from the two frames.  The paper further extends this idea to compute the flow of flow. For the flow computation, the paper uses the standard TV-L1 scheme, but its steps are being done within the network and thus could be back propagated. Finally, the entire framework is trained on a loss defined over action classification. Experiments are presented on Kinetics and HMDB datasets, and show good performance.\n\nStrengths:  I think the main strength of this paper is its competitive performances in Table 9 against RGB only methods. There are also various analysis presented on several architectural choices for the fusion with the flow layer.\n\nWeaknesses: \n1) I think the main weakness of this paper is its lack of any significant novelty, or inadequate coverage of state of the art in deep learning based flow estimation. For example, there is prior work such as FlowNet or FlowNet2.0 that could compute optical flow in an end-to-end manner. This paper does not cite these prior works and in the context of which the novelty as claimed in the paper is not substantial.\n\n2) Another important weakness is the lack of any convincing argument on why it is a good idea to compute/apply an optical flow algorithm on the intermediate feature maps of a network? How is the fundamental assumption of flow -- the brightness constancy -- applicable to such intermediate layers? \n\n3) Further, while the idea of proposing the deep variant of flow was to avoid the computational expense of an otherwise out-of-the-box flow algorithm, the paper ultimately has to resort to several heuristic workarounds such as low-resolution of the inputs, or reducing the iterations of flow optimization, etc. to make the model practical. The flow estimation of the feature maps also require trimming down the number of channels in a layer. \n\n4) It is surprising to see that even after such heuristic workarounds to compute an approximate flow, the final performance of the model is compelling; which needs more analysis to understand where precisely is the proposed scheme gaining in performance even with these approximations in comparison to an otherwise accurate external optical flow scheme?\n\n5) Finally, given that the paper proposes to optimize the model against the action classification loss, it is unclear to me if the proposed flow layers are in fact learning anything related to flow, or why should they learn flow, or for that matter, if the proposed iterative scheme is even necessary? If learning flow is important, shouldn't there be some intermediate objective that ensures that the flow layers do learn flow, using a suitable loss? For example, as in the FlowNet2.0 paper. \n\nMinor comments: \n1) The section 3.1 on optical flow methods could be improved to be more intuitive. For example, it is unclear what \\rho_c is capturing, and Eq.(2) seems to be just |I_2-I_1|.  What precisely are the roles of \\lambda, \\theta etc? How is the second term \\lambda |I_1-I_2| useful in (7) ? Isn't it a constant?\n\n2) More details and discussions need to be added to the comparisons against state of the art in Table 9. Why are the HMDB results missing from the middle column? \n\nOverall, I think the paper has some interesting results, however the method is unconvincing/unclear why it should work nor its novelty commendable.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach, but novel contributions seem to be few and poorly justified over Sun et al's work",
            "review": "The paper proposes a convolutional layer, inspired by optical flow calculation, to calculate the flow between feature maps. Given two feature maps (t, t+1) at layer L in the network, the flow layer calculates the flow between these\ntemporally-consecutive feature maps using TV-L1 optical flow algorithm. The parameters of the flow algorithm are learnt as opposed to being hand selected. This representation does not require pre-computation of flow, and thus is more\ncomputationally efficient compared to approaches that fuse two streams (e.g. 2SCNN). The paper is well-written. The motivation is strong, there is enough experimental evidence in the paper that this works. The idea of calculating the\nshift (i.e the flow) of the feature maps that are optimised for action recognition is interesting. On the other end, I have several reservations/criticisms to the presentation and conclusions of the paper:\n\n1. The paper dismisses other efforts that make similar assumptions as irrelevant in the related work. Sun et al (2018) also only use RGB input, propose a feature representation layer inspired by flow calculations and perform impressively on HMDB outperforming this paper’s results. Dismissing this work in the comparison, and only comparing to Fan et al, seems slightly misleading experimentally. The architectural formulation of Sun et al is slightly different in that flow of features is computed at multiple depths of the RGB network--this seems to be a more expressive approach than the proposed one. I understand that the authors add iterative optimisation, but the contribution in the paper seems to claim similar contribution to Sun et al in proposing the flow layer.\n\n2. The fact that these feature maps are only optimised for the flow (and not to perform recognition separately) seems slightly strange. In the current architecture there’s an assumption that the RGB features are already optimised for the problem, if I understood correctly. I had expected an architecture that optimises the RGB features separately from optimising the flow. It is not clear how the network is trained, is the RGB network first trained and then the flow representation layer? Or is it trained in a single pass backpropagating gradients through the flow layer?\n\n3. I found the structure of the ablation study to be very difficult to follow. Every bold title can be understood solely but is difficult to link to what’s before/after. The usage of a variety of dataset flavours: mini-Kinetics, lowRes-HMDB, HMDB, pre-training, etc makes it difficult to follow the argument.\n\n4. The motivation for flow-of-flow whilst interesting seems to suffer from the same flaw as computing flow-of-flow using TV-L1: the method makes the brightness constancy assumption which will only hold when computing flow between features of constant velocity. The use of an intermediate convolutional layer between the stacked flow layers (flow-conv-flow) achieves\nsuperior results to the flow layer alone, there is no theoretical motivation or explanation for why this would work/solve the brightness-constancy assumption issues in computing flow-of-flow.\n\n5. Flow-of-flow which is pushed from the abstract of the paper as novel, fails to produce valuable results towards the end. Given this is a primary contribution, it is not clear what conclusions to make about this.\n\n6. It has been shown experimentally that calculating flow after the third resNet block produces the best results, but it is not clear why this is the case. The number 3 seems to be magical and little explanation is given.\n\nMinor comments:\n\n1. Fig 1 appears to be missing the additional convolutional layer remapping from 2C' channels back to C channels described in the text\n2. Feature maps are mapped to [0, 255] help numerical stability, but no explanation for why this improves numerical stability is given.\n3. Providing standard deviation metrics for runtime performance in table 9 would give more confidence in the method's superior runtime performance\n4. I Would like to see a study of how changing the number of input channel to the flow layer C' affects performance.\n5. The name MiniKinetics has already been taking and using it to refer to a different subset of the Kinetics dataset is confusing for readers who might mistakenly compare methods based on these results.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Need more experiments to justify FoF and comparison to previous work ",
            "review": "This paper proposes a \"representation flow layer\" for action recognition, to learn hidden motion representation in a CNN end-to-end. The authors also come up with a concept termed \"flow-of-flow\". The paper is interesting, but I have several concerns.\n\n1. The concept of 'flow-of-flow' needs more justification. As shown in Table 5, flow-of-flow performs much worse than a single flow layer. After adding a conv layer in between, the performance gets better. I assume this is because the estimated flow are noisy, directly compute flow on them again making the result worse. It is the same as applying TVL1 twice. Adding the extra conv layer can help smoothing the feature map. So how about justify another configuration \"flow-conv\"? I just want to see if 'flow-of-flow' makes sense. \n\nActually, as pointed out by authors, flow-of-flow usually performs worse due to inconsistent optical flow results. I think the reason it gets a little bit higher accuracy here is because over-fitting. \n\nSo one more experiment of only \"flow-conv\", will help making the paper solid. \n\n2. The caption of figure 5 need more clarification. Is it TVL1 twice and FoF? or simply TVL1 and flow? Because subfig (b) and (c) do not align well. I am assuming this is computing the flow twice. \n\n3. I have several concerns for Table 9. \n\n(1) The authors do not provide results on HMDB dataset (2nd column). Is it because the performance is inferior to R(2+1)D or simply have no time before the deadline? This result is important, so please complete the table. \n\n(2) I don't see obvious advantage of the proposed approach over R(2+1)D and S3D. Especially for S3D, the performance is similar but S3D is faster. So the contribution of the paper is limited.  \n\n(3) If we compare the proposed approach to I3D, I3D is still better. Besides, there are several recent work reporting better performance on these two datasets. For example, ARTNet (Appearance-and-Relation Networks for Video Classification, CVPR18) report a 78.7 score on Kinetics. \n\n(4) The authors report their performance for 2D CNN and 2+1D CNN, but what about 3D CNN? It is also important to show. \n\n4. Since another goal of this paper is to improve efficiency, so I want to see a comparison, at least a discussion, to two previous literature. These two literature are the first work to propose to learn optical flow inside a CNN for action recognition. In these two literature, the authors incorporate FlowNet-like architectures to compute optical flow and feed them as input to the temporal stream. Because the FlowNet-like architecture can be quite shallow, their efficiency (fps) is very high, and the performance is on par with state-of-the-art.\n\n(1) Yi Zhu, et al, Hidden Two-Stream Convolutional Networks for Action Recognition\n(2) Laura Sevilla-Lara, et al, On the Integration of Optical Flow and Action Recognition\n\n\nIn conclusion, \n\n(1) In terms of novelty, the flow layer is adapted from TVNet, and the FoF concept need more clarification. \n(2) In terms of performance, there is no obvious advantage over previous work like S3D and R(2+1)D. And the experiments right now seem not complete. \n\nHence, I recommend a initial rating of 5. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}