{
    "Decision": {
        "metareview": "The paper proposes a new framework for out-of-distribution detection, based on variational inference and a prior Dirichlet distribution.\n\nThe reviewers and AC note the following potential weaknesses: (1) arguable and not well justified choices of parameters and (2) the performance degradation under many classes (e.g., CIFAR-100).\n\nFor (2), the authors mentioned that this is because \"there are more than 20% of misclassified test images\". But, AC rather views it as a limitation of the proposed approach. The out-of-detection detection problem is a one or two classification task, independent of how many classes exist in the neural classifier.\n\nIn overall, the proposed idea is interesting and makes sense but AC decided that the authors need more significant works to publish the work.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Arguable choices of parameters and the performance degradation issue"
    },
    "Reviews": [
        {
            "title": "Bayesian reasoning about DNN outcome",
            "review": "Summary\n=========\nThe paper describes a probabilistic approach to quantifying uncertainty in DNN classification tasks.\nTo this end, the author formulate a DNN with a probabilistic output layer that outputs a multinomial over the\npossible classes and is equipped with a Dirichlet prior distribution.\nThey show that their approach outperforms other SOTA methods in the task of out-of-distribution detection.\n\nReview\n=========\nOverall, I find the idea compelling to treat the network outputs as samples from a probability distribution and\nconsequently reason about network uncertainty by analyzing it.\nAs the authors tackle a discrete classification problem, it is natural to view training outcomes as samples from\na multinomial distribution that is then equipped with its conjugate prior, a Dirichlet.\n\nHowever, the model definition needs clarification. In the classical NN setting, I find it misleading\nto speak of output distributions (here called p(x)). As the authors point out, NNs are deterministic function approximators\nand thus produce deterministic output, i.e. rather a function f(x) that is not necessarily a distribution (although can be interpreted as a probability).\nOne could then go on to define a latent multinomial distribution over classes p(z|phi) instead that is parameterized by a NN, i.e. phi = f_theta(x).\nThe prior on p(phi) would then be a Dirichlet and consequently the posterior is Dirichlet as well.\nThe prior distribution should not be dependent on data x (as is defined below Eq. 1).\n\nThe whole model description does not always follow the usual nomenclature, which made it at times hard for me to grasp the idea.\nFor instance, the space that is modeled by the Dirichlet is called a simplex. The generative procedure, i.e. how does data y constructed from data x and the probabilistic procedure, is missing.\nThe inference procedure of minimizing the KL between approximation and posterior is just briefly described and could be a hurdle to understand, how the approach works when someone is unfamiliar with variational inference.\nThis includes a proper definition of prior, likelihood and resulting posterior (e.g. with a full derivation in an appendix).\n\nAlthough the authors stress the importance of the approach to clip the Dirichlet parameters, I am still a bit confused on what the implications of this step are.\nAs I understood it, they clip parameters to a value of one as soon as they are greater than one.\nThis would always degrade an informative distribution to a uniform distribution on the simplex, regardless whether the parameters favor a dense or sparse multinomial.\nI find this an odd behavior and would suggest, the authors comment on what they mean with an \"appropriate prior\". Usually, the parameters of the prior are fixed (e.g. with values lower one if one expects a sparse multinomial).\nThe prior then gets updated through the data/likelihood (here, a parameterized NN) into the posterior.\n\nClipping would also lead to the KL term in Eq. 3 to be 0 often times, as the Dir(z|\\alpha_c) often degrades to Dir(z|U).\n\nThe experiments are useful to demonstrate the application and usefulness of the approach. \nOutcome in table 3 could maybe be better depicted using bar charts, results from table 4 can be reported as text only, which would free up space for a more thorough model definition.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not well motivated.",
            "review": "This paper proposes a new framework for out-of-distribution detection, based on variational inference and a prior Dirichlet distribution. The Dirichlet distribution is presented, and the way it is used withing the method is discussed (i.e. clipping, scaling, etc). Experiments on several datasets and comparison with the state of the art is reported and extensively discussed.\n\nThe motivation of the proposed approach is clear, and I agree with the attempt to regularize the network output. The choice of the Dirichlet distribution is quite natural, as each of its samples are the prior weights of a multinomial distribution. However, some other choices are less clear (clipping, scaling, decision, and the use of a non-informative prior). The overall inference procedure appears to be advantageous in the many experiments that the authors report (several datasets, and several baselines).\n\nThe first thought that came to my mind, is that out-of-distribution detection is to classification what outlier detection is to regression. Therefore, relevant and recent work on the topic deserves to be cited, for instance:\nS. Lathuiliere, P. Mesejo, X. Alameda-Pineda and R. Horaud, DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model, In ECCV, 2018.\n\nOne thing that I found quite strange at first sight is the choice of clipping the parameters of the Dirichlet distribution. It is said that this is done in order to choose an appropriate prior distribution. However, the choice is not very well motivated, because what \"appropriate\" means is not discussed. So why do you clip to 1? What would happen if some of the alpha's go over 1? Is it a numerical problem, a modeling problem, a convergence issue?\n\nI would also like the authors to comment on the use of a non-informative Dirichlet distribution within the KL-divergence. The KL divergence measures the deviation between the approximate a posteriori distribution and the true one. If one selects the non-informative Dirichlet distribution, this is not only a brutal approximation of the true posterior, but most importantly a distribution that does not depend on x, and that therefore cannot be truly called posterior.\n\nIt is also strange to take a decision based on the maximum alpha. On the contrary, the smallest alpha should be taken, since it is the one that concentrates more probability mass to the associated corner in the simplex.\n\nRegarding the scaling function, it is difficult to grasp its motivation and effects. It is annouced that the aim of the smoothing function is to smooth the concentration parameters alpha. But in what sense? Why do they need to be smoothed? Is this done to avoid numerical/convergence problems? Is this a key part of the model? The same stands, by the way, for the form of the input perturbation.\n\nThe experiments are plenty, and I appreciated the sanity check done after introducing the datasets. However, I did not manage to understand why some methods appear in some tables and not in other (for example \"Semantic\"). I also have the feeling that the authors could have chosen CIFAR-100 in Table 2, since most of the metrics reported are quite high (the problems are not much challenging).\n\nRegarding the parameter eta, I would say that its discussion right after Table 3 is not long enough. Specially, given the high sensitivity of this parameter, as reported in the Table of Figure 4. What is the overall interpretation of this sensitivity?\n\nFrom a quantitative perspective, the results are impressive, since the propose methods systematically outperforms the baselines (at least the ones reported). However, since these baselines are not the same in all experiments, CIFAR-100 is not used, and the discussion of the results could be richer, I conclude that the experimental section is not too strong.\n\nIn addition to all my comments, I would say that the authors chose to exceed the standard limit of 8 pages. Even if this is allowed, the extra pages should be justified. I am affraid that there are many choices not well motivated, and that the discussion of the results is not informative enough. I am therefore not inclined to accept the paper as it is.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "new method approximating the distribution of classification probability",
            "review": "This paper provides a new method that approximates the confidence distribution of classification probability, which is useful for novelty detection. The variational inference with Dirichlet family is a natural choice.\n\nThough it is principally insightful to introduce the “higher-order” uncertainty, I do see the fundamental difference from the previous research on out-of-distribution detection (Liang, Lee, etc.). They are aimed at the same level of uncertainty.  Consider a binary classier, the only possible distribution of output y is Bernoulli- a mixture of Bernoulli is still Bernoulli.   \n\nIn ODIN paper,  the detector contains both the measurement of the extent to which the largest unnormalized output of the neural network deviates from the remaining outputs (U1 in their notation) and another measurement of the extent to which the remaining smaller outputs deviate from each other (U2 in their notation).  In this paper, the entropy term has the same flavor as U2 part?\n\nI am a little bit concerned with the VI approach, which introduces extra uncertainty.  I do not understand why there is another balancing factor eta in equation 6, which makes it no longer a valid elbo. Is the ultimate goal to estimate the exact posterior distribution of p(z) through VI, or purely some weak regularization that enforces uniformity?  Could you take advantage of some recent development on VI diagnostics and quantify how good the variational approximation is?\n\nIn general, the paper is clear and well-motivated, but I find the notation sometimes confusing and inconsistent. For example, the dependency on x and D is included somewhere but depressed somewhere else.  alpha_0 appears in equation 4 but it is defined in equation 7. \n\nI am impressed by the experiment result that the new method almost always dominates best known methods, previously published in ICLR 2018. But I am not fully convinced why it works theoretically.  I would recommend a weak accept.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}