{
    "Decision": "",
    "Reviews": [
        {
            "title": "interesting idea but result is not significant",
            "review": "This paper considers the problem of computing preimages of convolutional network outputs. The main technology is a change of basis: expressing an arrangement of d generic hyperplanes in \\R^d in the basis givens by their rays (section 3). The paper focuses on arrangements coming from circulant matrices (section 4), and found that the expression of such arrangements in this basis is regular, and for a particular example in \\R^3, showed that they could completely understand the ReLu map (section 5). \n\nThe problem is definitely interesting, and very difficult. However, the contribution is minimal. Firstly, the reduction to circulant matrices is a huge step and needs serious justification. The paper justified its restriction to circulant network via this claim: (Section 4):\n\\begin{quote}\nWe will exploit the fact that convolutional\nmatrices are in most respects asymptotically equivalent to those of circulant matrices where each new row is a one element cyclic shift of the previous.\n\\end{quote}\nThis needs to be made precise to be useful. In what respect? What does asymptotically equivalent mean? Without a serious justification here, analyzing circulant matrix is too narrow a scope. \n\nNest codes: the punchline seems to be that the network contracts data, which is not new. The results are from the analysis of a *single* example in 3 variables - how far does this extend? \n\nIn short, this paper has very little concrete mathematical or computational contribution. \n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting work, but not significant enough",
            "review": "This paper studies the geometry of convolutional networks with ReLU activation functions. It provides some insights into the way neural networks process data. However, the overall quality of this paper is relatively low. \n\nPros:\n\nThis paper introduces a method to compute the preimages of a neural network layer using dual bases defined by the affine mappings of the network. The authors also relate the number of layers in convolutional neural network to the contraction of data. Some interesting phenomena are illustrated using examples. Although not very rigorous, the conclusion of this paper gives some insights on the geometry of convolutional neural networks.\n\nCons:\n\nThe paper is not written very clearly. First of all, the analysis of the paper is based on the assumption that each layer of the neural network has exactly d (the dimension of the input) nodes. However it seems that this assumption is not mentioned anywhere in this paper. Some of the notations are misleading. For example, in Equation (20) d is redefined as a parameter of an affine transform. There are also grammar errors and missing punctuation which I suggest the authors should carefully fix. \n\nThe contribution of this paper is not very significant. No results in this paper can be formulated into rigorous theorem. As previously mentioned, the proposed method to compute preimages only works when all the layers have same amount of nodes, which is not the case in practice. Most of the conclusions the authors make are vague and lack impact. The authors do not convincingly demonstrate how the proposed preimage calculation method can be applied to practically useful network structures. The claim that nested cones efficiently contract input data is not very convincing either, and looks not very relevant to the preimage calculation part of the paper. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Difficult to read, and I am unsure about relevance",
            "review": "Summary:\nThe paper is concerned with finding the preimage of of the features extracted by a series of convolution layers with ReLU activations. It is argued that these can be described through a series of nested polyhedral cones. This seem like a reasonable observation, but I am unsure why it is a particularly interesting observation. In the end, what does this tell me about the behavior of convolution layers?\n\nI found the paper very difficult to read; not so much because of the topic, but due the writing. Too often I simply do not understand individual sentences, and I am left to guess the intended meaning. In such scenarios, I cannot recommend acceptance.\n\nDetails (I am not including all my confusions here, but just a selection to keep review time manageable):\n*) References to figures appear to be incorrect. I think the authors accidentally refer to section numbers instead of figure numbers, but I am constantly left to guess which figure I should look at.\n\n*) Sentences often miss periods, and words are capitalized within sentences.\n\n*) It is chosen not to take subsampling or max pooling into account during the analysis. I can understand skipping max pooling, but why skip subsampling? Isn't that just a linear projection, which wouldn't affect the analysis?\n\n*) On page 2: \"Cones associated with successive layers are then in general partly nested inside their predecessor.\" I don't quite understand what is meant by \"partly nested\"; I get the intuition that two cones can intersect, but if the wording is supposed to mean something more specific, then I would like a more specific definition of \"partly nested\".\n\n*) On page 2: \"These hyperplanes divides the input space into a maximum of 2^d number of different cells with the maximum attained if all hyperplanes cut through the input space which we take as the non negative orthant of the d-dimensional Euclidean space R_+^d.\" I am not sure I understand this sentence, e.g. how can a hyperplane not cut through the input space? Should I read this as the input space is the non negative orthant? Why is that the input space? Is this based on the assumption of non negative pixel intensities? If so, why not include a maximal intensity such that the input space is a box?\n\n*) On  page 3: i is used as a general index as well as the negative half space index (i vs i_k). This is a bit confusing, so it might be good to rename one of the indices.\n\n*) On page 3, what does it mean for the set e_i to be \"complete\"? Do you mean that span({e_i}) = R^d ?\n\n*) It would be good to be more explicit about the term \"dual basis\" -- dual to what?\n\n*) On page 4: \"We now define the positive direction of the vector e_i as that associated with the negative side of the plane Î _i\" (an example of a sentence missing a period, btw). What is the negative side of a plane?\n\n*) On page 4: \"We will exploit the fact that convolutional matrices are in most respects asymptotically equivalent to those of circulant matrices where each new row is a one element cyclic shift of the previous.\" (previous sentence miss a period, btw) and \"...this approximation will be negligible.\" I strongly miss either an explanation of this or a citation. As it stands, these are claims not facts.\n\n*) On page 5: \"The domain of the input that is mapped to output domain is just quite trivial planar manifolds.\" I don't understand this sentence.\n\n*) On page 8: \"The convolutional part of a deep network can therefore be seen as a component in a metric learning system where the purpose of the training will be to create domains in input space associated with different classes that are mapped to separate low dimensional outputs as a preprocessing for the final fully connected layers that will make possible efficient separation of classes.\" I don't understand how this conclusion is related to the analysis of the paper. I do not disagree with the quoted statement, but I miss how this is related to the analysis regarding nested cones.\n\n*) On page 9: Just below the figure caption appears an \"as\" that appear to have not relation to any other text in the paper.\n\n*) Citation 13: The authors name is misspelled, it should be \"Roweis\".\n",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}