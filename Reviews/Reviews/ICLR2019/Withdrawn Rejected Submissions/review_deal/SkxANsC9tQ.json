{
    "Decision": {
        "metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review for Learning Graph Representations paper"
    },
    "Reviews": [
        {
            "title": "Relative entropy for evaluating hierarchical clustering",
            "review": "This paper introduces a relative entropy metric for evaluating hierarchical clustering. The paper is not well-written and its contributions are not clear to me. The abstracts and introduction promising for a novel hierarchical clustering algorithm, but I can only understand that they are using an agglomerative algorithm with the inter-cluster similarity of Eq. 10.\nThey show that their similarity metric reduces to the previous work by setting \\pi = p or uniform. However, in the experiments, they only use these two cases, which does not support the importance of using relative entropy metric.  I guess picking the best \\pi is an important part of this approach which has been left out. \nThe authors violate the blindness of the paper by including a link to their github page, for which an anonymous repository should have been used.\nIt also worth noting that nowadays the graph representation term is often used for graph embedding, which makes the title very misleading. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clearly written but the overall quality is not high enough.",
            "review": "This paper proposes a new formulation of hierarchical clustering for graphs.\n\nQuality:\nThe proposed formulation has not been analyzed in detail and its advantage is not clear compared to existing approaches.\nIn addition, there are some existing measures for hierarchical clustering, for example, the dendrogram purity[1].\nIt would be interesting to analyze the relationship between the proposed method and such existing measures. \n[1] Heller, K. A. and Ghahramani, Z.: Bayesian Hierarchical Clustering, ICML2005\n\nClarity:\nThis paper is clearly written and easy to read.\nThe proposed criteria is carefully derived and well explained.\nI feel that the title of the paper is not appropriate as it says the paper is about graph representation learning while a graph (representation) is already given as an input in the setting discussed in this paper. \"Learning hierarchical representation ...\" would be better.\n\nOriginality:\nThe originality is not high as most of theoretical discussion is based on the existing work and the resulting hierarchical clustering algorithm is a straightforward extension of the average linkage method.\nOf course, it is quite interesting if a minor change makes a big difference in clustering performance (theoretically and/or empirically), but such result is not given.\n\nSignificance:\nSignificance of the contribution is not high as the advantage of the proposed formulation is not clear.\nOne of interesting questions is: how about higher order relationships between nodes?\nThe proposed method takes up to second order relationships between nodes, that is, edges into account.\nSince the proposed formulation can naturally include higher order relationships, it would be interesting to analyze such relationships in hierarchical clustering.\n\nPros:\n- The paper is clearly written.\n- The proposed formulation of hierarchical clustering is interesting.\nCons:\n- The advantage of the proposed formulation is not presented.\n- Experiments are not thorough.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning Graph Representations by Dendrograms",
            "review": "This paper suggests a new metric for assessing the quality of hierarchical clustering. Dasgupta recently suggested the first such metric with interesting properties. This has encouraged a number of recent works about designing algorithms that work well for the metric and other similar metrics. This paper suggests a new metric for evaluating hierarchical clustering of given graph data. \n\nHere are the main comments about the paper:\n- I am not convinced about the advantages of the new metric over the previously suggested metrics by Dasgupta and Cohen-Addad et al.\n    - Theoretical analysis shows properties of the new metric that are similar to that of Dasgupta (since the metric itself has similarities). However, the advantage of the new metric is not very clear. \n    - Experimental analysis just shows that the new metric is different from Dasgupta’s but again there is no evidence to suggest why the new metric may be better. \n\n- In the abstract it is mentioned that “The best representation of the graph for this metric in turn yields a novel hierarchical clustering algorithm.” I do not understand this. Which novel algorithm is being referred to? \n\n- Again, it is mentioned in the abstract that “Experiments on both real and synthetic data illustrate the efficiency of the approach”. What efficiency is being referred to here and what is the approach? What I see is that known clustering algorithms are used to compare the new metric with the previous one by Dasgupta.\n\nOverall, I think more work is needed in this paper. There are some non-trivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.\n\nOther comments:\n- Section 5: NP-hardness has not been shown and it is just mentioned that the authors believe that the problem is NP-hard just as the problem associated with the cost function of Dasgupta et al.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}