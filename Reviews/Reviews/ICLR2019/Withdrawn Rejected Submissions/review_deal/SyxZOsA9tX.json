{
    "Decision": {
        "metareview": "The paper proposes to use Anderson Mixing to accelerate value iteration and DQN.  The idea is interesting, with some theoretical and empirical support.  However, reviewers feel that the contribution is somewhat limited, and certain parts (e.g., the DP view) can be further developed to strengthen the technical contribution.  Furthermore, one reviewer points out that the empirical results are not very strong, where the improvements on 3 Atari games are not very substantial.  Overall, while the paper is interesting and does have the potential, it seems too preliminary to be published in its current form.\n\nMinor comments:\n1. The paper is partially motivated by the claim given at the beginning of section 3: \"Based on the observation that full policy evaluation accelerates convergence, ...\"  Can a reference be given?\n\n2. Another way to look at Anderson Mixing is the standard linear value function approximation framework, where the previous K value functions serve as basis functions.  See Mahadevan & Maggioni (JMLR'07), Parr et al. (ICML'08) and Konidaris et al. (AAAI'11) for a few examples of constructing basis functions; the approach here seems to provide another way to automatically construct basic functions.  A discussion would be helpful.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea, but contributions need to be strengthened"
    },
    "Reviews": [
        {
            "title": "review",
            "review": "This is a very well-written paper which proposed a way to accelerate the value-iteration of MDP. The method is the so-called \"Anderson-Mixing\" method. It replaces the policy evaluation step by solving a smaller linear equation: find a linear combination of a few historical values to represent the value of the current policy. The paper also presents a very nice explanation of why such a modification of VI accelerates VI. The paper also extends the method to DQN and shows a very nice acceleration. The experiments are convincing and interesting.\n\nI only have two concerns: \n\n1) In section 4, the convergence proof is shown but the contraction is only gamma. This is the same as the original VI. Of course, this is the worst case best bound. Is it possible to show a result that the modified-VI is always better than the original VI?\n\n2) In section 4, the dependence on k has not been studied. But k actually critically affects the time complexity. Is it possible to obtain convergence proof depending on k?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Extension to the Approximate DP case needed",
            "review": "This paper introduces the \"Anderson mixing\" ideas from the broader literature on general fixed-point problems to the specific problem of finding the fixed-point to the Bellman optimality equations for a Markov Decision Processes. The general idea is to summarizes the history of previous iterates (value functions in this case) by finding of convex combination which also minimizes the residuals. The authors provide a solution for when an iterate is no longer representable by a convex combination of the recent history by simply bypassing the interpolation step and replacing it with a usual value iteration step. Using the intuition developed in the MDP case, they then adapt their DP algorithms to the learning case by substituting exact (tabular) value functions with deep function approximators. Experimental results are presented in 3 games from the ALE environment.\n\nThe jump from the DP formulation to the learning case is rather abrupt, and lacks sufficient motivation. The way the paper is currently structured is 50-50: 50% of the contribution is the DP view of the proposed method while the remaining half comes from the deep formulation (and experiments). I think that I would have preferred to see the entire paper being dedicated to the DP point of view, followed by a more principled Approximate DP analysis in the simpler linear case. Dedicating the remaining of the paper to the deep formulation almost feels like a missed opportunity to fully developing the theory initiated in the first section. But then of course the price to pay would be a paper which would be less aligned with the \"representation learning\" aspect of the conference. My main concern is that extending this technique to the deep setting mare involve some serious interference with other mechanisms already at play. It is very difficult to explain if the observed improvement come from the underlying DP basis or as a secondary effect of architectural and algorithmic considerations. \n\nTo my knowledge, this is the first attempt at using Anderson mixing in the MDP framework. However, I would appreciate if the authors could survey previous attempts (if any) by other authors, or more generally existing results in the literature on non-linear fixed-point methods.  You may find relevant work by consulting the recent Zhang, O’Donoghue and Boyd paper (2018). \n\n# Detailed comments\n\n> Puterman 2014\n\nThe 2014 edition is likely to be a re-print of the 1994 which is commonly cited. I would double-check to see if there is any difference in the content between the 2014 and 1994 edition. If not (and just a re-print) I would cite the 1994 edition which is more widely recognized. \n\n> Citations for VI and PI\nYou should cite Bellman 1957 and Howard 1961 (not Puterman). For exact references, see bibliographical remarks in Puterman. \n\n> Citation for Modified policy iteration\n\nPlease cite original paper(s) by Puterman and Brumelle ~1978. See bibliographical remarks in Puterman 1994 (or 2014) for the origins of MPI. \n\n>  via the Neumann expansion\n\ntruncated\n\n> computationally inefficient for complex decision problems\n\nCompared to what? More efficient than full PI for sure\n\n> Page 2, notation for $\\Gamma_\\pi$ vs $\\Gamma$\n\nI suggest using a different notation for the (linear) policy evaluation operator vs the Bellman optimality one. The subscript \"_\\pi$ is easy to miss. \n\n> converges much faster with K\n\nDefine K\n\n> In most cases, we can\n\nIn reinforcement learning, we can\n\n> value iteration can be finished\n\nFinished ? \n\n> value iteration can be finished by estimating Γ(v) through sampling.\n\nWe are no longer in the realm of DP, but more stochastic approximation methods. This isn't quite VI anymore. I would be more careful when jumping from one setting to the other.\n\n> provided the sampling estimations are accurate enough\n\nThe approach described so far does not involve any sampling. \n\n> This modification is based on the observation that the recent successive policies do not\n\nSo far, the mixing equations (3) and (4) only describe the evaluation case. You haven't mentioned yet how you plan to combine this into a more general control algorithm where successive (changing) policies are generated.\n\n> the solution can be written explicitly as\n\nPlease cite where this comes from (or provide proof inline or appendix)\n\n> while PI is similar to Newton’s method\n\nCite Puterman and Brumelle for the original work on showing the connection between PI and Newton's method. \n\n> except that the tangent line is replaced with a secant line.\n\nPlease explain this intuition: how you obtain this geometric interpretation.\nAlso, the secant method being an analogue to quasi-Newton methods, and policy iteration being Newton's method, there is an opportunity to better develop and explain those parallels.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Accelerated Value Iteration via Anderson Mixing",
            "review": "This paper seems like a nice idea, but I'm not sure if it's ready for publication. It seems that the main contribution of this paper is the DA2Q algorithm, since the A2VI algorithm is a straightforward application of AA to VI. However the numerical examples are very weak, only 3 games are tested, and the results are not that strong. Furthermore in Figure 3 with the results it's not clear what the 'Time' axis is.\n\nSmaller comments:\n\nIt seems like this recent paper should be cited:\nhttps://arxiv.org/abs/1808.03971\nit includes value iteration as an example, both in theory and in practice.\n\nI think that lemma 1 is a direct consequence of the fact that PI has finite convergence (this is easily seen since there are finite policies and it converges). \n\nIn the contraction for PI what is K?\n\nWith the constraints as specified after equation 5 it is no longer Anderson acceleration. The convex combination constraint is just the standard alpha >= 0 constraint.\n\nRejection step seems very onerous, how often does it occur in practice?\n\nNote that a simple application of AA to VI would not have the problem that it needs to \"Jump out of the subspace\".\n\nDA2Q algorithm as printed is very complicated, can it be simplified somehow? Just focusing on the novel steps would help.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}