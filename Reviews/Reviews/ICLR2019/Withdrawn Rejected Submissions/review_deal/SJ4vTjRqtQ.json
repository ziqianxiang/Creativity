{
    "Decision": {
        "metareview": "\npros:\n- Good quantitative results showing clear improvement over other model-based methods in sample efficiency and computational cost (though see Reviewer 2's concerns about the need for more experiments on computational cost).\n- Cool qualitative results showing discovery of BFS and DFS\n- Potentially novel approach (see cons)\n\ncons:\n- Lack of clarity especially concerning equation (1).  Both Reviewers 1 and 3 were unsure of the rationale for this equation which lies at the heart of the method.  It looks to me like a combination of surprise and value but the motivation is not clear.  There are a number of other such places pointed out by the reviewers where model choices were made that seem ad hoc or not well motivated.\n- In general it's hard to understand which factors are important in driving the results you report.  As Reviewer 3 points out, more ablation studies and analysis would help here.  Providing more motivation, explanation and analysis would help the reader understand better the reasons for the performance of the model.\n\nThe results are nice and the method is intriguing.  I think this potentially a very nice paper and if you can address the above concerns but isn't quite up to the acceptance bar for ICLR this year.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Potentially very nice paper with clarity issues"
    },
    "Reviews": [
        {
            "title": "Nice results, but weakened by a mysterious inner objective and lack of novelty",
            "review": "This paper proposes a new architecture for model-based deep RL, in which an “inner agent” (IA) takes several planning steps to inform an “outer agent” (OA) which actually acts in the world. The main contributions are to propose a new objective for the IA, and to allow the IA to “undo” its imagined actions. Overall I think this could be a great paper, but it needs further justification of some of the architectural choices and more rigorous analysis/experiments before it will be ready for acceptance.\n\nPros:\n- Nice demonstration of improved data efficiency over existing model-based methods.\n- Substantial improvement over other model-based methods in terms of computational cost.\n- Interesting qualitative analysis showing discovery of DFS and BFS-like search procedures.\n\nCons:\n- Limited novelty over existing methods.\n- It is unclear what in the model contributes to improved performance.\n\nQuality\n---------\n\nThe results in the paper seem impressive in terms of sample complexity, but I think there needs to be further exploration of the source of the results. I strongly suggest including in a revision a number of ablation experiments to tease these details apart---for example, what do the results look like if the IA uses the same objective as the OA? Does the agent achieve worse performance if it has to restart its imaginations from the root of the tree each time, as is more analogous to MCTS and other previous model-based approaches?\n\nAdditionally, there are a few places in the paper where unjustified statements are made. For example, in Section 5.3, the paper states that “we hypothesize that focusing, by repeatedly visiting the same state, the IA ensures that the POI is remembered in its hidden state such that the OA can act accordingly, given this information”. This seems very speculative. It would certainly be very interesting if true, but there needs to be something more than just intuition to back up this hypothesis. I recommend including some probe experiments (e.g., force the IA to take a sequence of such actions, or not, and see what the result on the behavior of the OA is) or removing speculations such as this (or moving it to the appendix).\n\nThe literature review is missing some related work, particularly from the realm of model-based continuous control. [1-3] are a few references to start with; these papers take a different approach in that they don’t use tree search but they are still worthwhile discussing. I think a reference to [4] is also missing, which takes a related approach to learning the decisions needed to perform MCTS.\n\nClarity\n--------\n\nOverall, the paper is well-written and I understand what was done and how the architecture works. However, I had a hard time understanding the choice of the inner objective (Equation 1). The paper states that this equation defines the “value of information” and defines it as the product of the KL from the OA’s hidden state prior to the transition to after the transition, multiplied by the Q value estimated by the OA and the action probability of the IA. This is very mysterious to me. Why is this a good objective? Why does the KL term of the hidden state of the OA have anything to do with the value of information? Given that the difference in objective of the IA is one of the main contributions of the paper, this choice needs to be justified, explained, and examined. As mentioned above, it would be best if a revision could include some ablation experiments where the choice of this objective is more closely examined.\n\nMore broadly, as mentioned above, it is unclear to me what part of the framework results in improved performance. Is it the ability to “undo” actions (rather than starting over from the root or exhaustively performing BFS), or is it the KL-based reward given to the IA? The paper does not provide any insight into this question, making it unclear what are the key points I should take away. \n\nMinor:\n- The colors in the caption of Figure 5 do not match the colors in the figure.\n- The colors in Figure 7 are very dark and it is hard to make out what is actually happening in the figure.\n\nOriginality\n-------------\n\nThe objective of the inner agent (Equation 1) appears novel, though as discussed above it is unclear to me what exactly it means and what its implications are. The idea of constructing an imagination tree state-by-state is not particularly novel, and was previously explored by Pascanu et al. (2017). I think this paper deserves more discussion and comparison than it is given in the present work (in particular, compare Figure 3 of the present paper and Figure 2 of Pascanu et al.). In general, the main idea in both papers is the same: have an agent learn to take internal planning steps and construct a planning tree that then informs the final action in the world. The biggest differences from Pascanu et al. are that the present work uses a separate objective for the inner agent, and allows taking a step backwards and returning to the previous state (whereas Pascanu et al. only allowed imagining from the current imagined state or from the root). So, the overall the paper has some new ideas, but is not highly novel compared to previous work. I see the two biggest original contributions as being: (1) the separate objective in the inner agent and (2) the ability for the agent to restart its imagination from the previous imagined state.\n\nSignificance\n----------------\n\nThe results reported by the paper are significant in that they do show dramatic improvement in sample complexity over existing model-free methods, as well as improvement in computational cost over existing model-based methods. However, as discussed above, it is hard for me to know what conclusions I should draw from the paper in terms of what aspects of the approach drive this performance. Thus, I think the lack of clarity in this respect limits the significance of the paper.\n\n[1] Finn, C., & Levine, S. (2017). Deep visual foresight for planning robot motion. In Proceedings of the International Conference on Robotics and Automation (ICRA 2017).\n[2] Srinivas, A., Jabri, A., Abbeel, P., Levine, S., & Finn, C. (2018). Universal planning networks. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018).\n[3] Henaff, M., Whitney, W., & LeCun, Y. (2018). Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177\n[4] Guez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., … Silver, D. (2018). Learning to search with MCTSnets. In Proceedings of the 35th International Conference on Machine Learning (ICML 2018).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting proposal of flexible model-based planning ",
            "review": "I think the ideas proposed in this paper are interesting. The paper is quite clearly written and the authors have provided a thorough reviews of related works and stated how the current work is different. I think this work has some significance for model-based reinforcement learning, as it provided us with a new adaptive way to rollout the simulation. I see the the work as a nice extension/improvement of the I2A (Weber et. al. 2017) and the ATreeC/TreeQN work (Fraquhar et. al. 2017). As the authors pointed out, the L2P agent can adaptive rollout different trajectories by choosing to move back to the root (start state) or move one step backward in the tree (regret last planned action). This is different from ATreeC/TreeQN where the whole tree is expanded in BFS way, and from I2A where rollouts are linear for each possible actions at current state.\nI have a bit doubt about the experimental results though. The levels used to evaluate seems quite simple, and I wonder the baseline model-free agents are not properly tuned or are not trained long enough to be fair. I have a list of questions detailed below:\n\n1. The IA is trained with utility that is a measure of \"value of information\" provided to the OA. I think this is a cool idea. Though I think the readers could understand better the intuition better if the authors can expand the explanation further. any reference on the idea? Why it has the form of Q^ * D_KL, for example why not Q^ + D_KL? Has the authors try to only set the utility as Q^ or as D_KL only as controls?\n\n2. One key part of this model is that during IA's unroll, the agent will choose z* from (z^p, z^c, z^r} (previous, current, root states), and then choose an action to unroll from z*. I wonder if this can be even further extended. For example, one possibility is that the agent can have z* set to any z in the tree that has already expanded. Or, another possibility is that the agent can have z* set to any z along the path from current node to the root (i.e. regret k-steps).  Also, would it be possible to have a dynamic planning steps? These suggestions may be practically hard to work properly, but may worth discuss.\n\n3. \"Push is similar to Sokoban used by Weber et. al. (2017) with comparably difficulty\". I cannot quite be convinced by this statement. Any quantification or evidence to support this sentence? To me, Sokoban seems to be much harder, as the agent need to solve the whole level to get score and can get stuck if making a single bad decision, while the Push seems much more tolerable (a lot of boxes, the obstacles is softly defined.) So stating that L2P learn Push in an order of magnitude less steps in Push compared to I2A learn Sokoban seems a chicken to egg comparison to me.\n\n4. Is it possible to run I2A as a baseline in the two environment you tried?\n\n5. I don't quite understand why DQN-{Deep, Wide} perform badly in the Gridworld environment. Checking the learning curves, one can see they actually converged to lower score than when the models started (from close to -1 down to -1.3). Can the authors comment more on why this is the case? The authors mentioned 'the agents learn only to navigate around the map for 25-50 steps before an episode ends'. I could not digest this sentence and would hope to understand better. To me, this gridworld level is quite trivial, the agent decide which goal is closest to the agent, and then move forward to that one and then onto next goal sequentially. I would like to understand better why this is a good level to test model-based RL and why model-free RL should have a hard time.\n\n6. a few possible typos:\n(1) formula 5, 3rd equation, should it be:\n      z*_{tau+1} = z_tau + z'' (double prime instead of single prime)?\n\n(2) The last sentence of the paragraph after equation (5)\n     z_{tau+1}^r = z_{tau=0}  (tau+1 instead of tau) \n\n(3) the color indication in Figure 5 caption is wrong. (while the description is fine in the main text)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting paper",
            "review": "Quality: This paper proposes learning to plan approach that can learn to search with an inner agent; conditioned on the output of the inner agent (IA) the outer agent starts to learn a reactive policy in the environments. The inner agent, different from other searching agent, learns to decide what searching pattern to choose. The presented method shows better computation efficiency than competitive baselines. The main applications are on \"box pushing\" game and grid-world navigation.   \n\nclarity: The paper is well-written.\noriginality: The paper is original. \nsignificance: This paper shows a promising method to combine traditional search method with machine learning techniques and therefore boost sample-efficiency of RL method. \n\ncons:\n1. The dynamics model used to plan is given and fully observable. That means a pure Monte-Carlo tree search can achieve very high accuracy.  In the figure 6, AtreeC can also have good performance after 4e7 steps, even better than the proposed method. I am wondering what would happen if 4e7 steps were applied to the proposed method.\n2.  One argument from the paper is that their method is computationally efficient. However, this should be presented in a more realistic test environment. In the push and gridworld environment, 84 steps of planning wouldn't be too bad. So a demonstration of the effectiveness from the proposed method on a visually complex game would be great.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}