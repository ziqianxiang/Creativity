{
    "Decision": {
        "metareview": "This paper proposes a new method for combining previous state representation learning methods and compares to end-to-end learning without without separately learning a state representation. The topic is important, and the authors have made an extensive effort to address the reviewer's concerns, particularly regarding clarity, related work, and accuracy of the drawn conclusions. The reviewers found that the main weakness of the paper was the experiments not being sufficiently convincing that the proposed approach is better than the alternatives. Hence, it does not currently meet the bar for publication.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "This paper on SRL provides some interesting results, but it methods should be better motivated, and its conclusions be made more precise..",
            "review": "This paper discusses State Representation Learning for RL from camera images. Specifically, it proposes to use a state representation consisting of 2 (or 3) parts that are trained separately on different aspects of the relevant state: reward prediction, image reconstruction and (inverse) model learning. The paper is easy to read, and seems technically sound. However, the conclusions do not directly follow from the results, so should be made more precise. The contribution is minor, and the reasoning behind it could be better motivated. \n\nThe most important point of critique is that the conclusion that the split representation is the best is at best premature. The presented results indicate that SRL is useful (Table 1), and that auto-encoding alone is often not enough. Other than that, the different approaches tested all work well in different tasks. The discussion of the results reflects this, but the introduction and conclusion suggest otherwise.\n\nThe same problem also occurs for the conclusion about the robustness of SRL approaches. In the main text, no results are presented that warrant such a conclusion. The appendix includes some tests in this direction, but conclusions should not be based on material that is only available in the appendix. Furthermore, even the tests in the appendix are not comprehensive enough to to warrant the conclusion as written.\n\nThe second point is the motivation of the split approach: it seems in direct contradiction with the \"disentangled\" and \"compact\" demands the authors pose. Because the parts of the state that are needed for multiple different prediction tasks (reconstruction, inverse model, etc.) need to be in the final\nstate representation multiple times. Due to the shared feature extractor, the contradictory objectives (and hence the need for tuning of the weights in the cost function) are still a potential problem.\n\nMinor points:\n\n- The choice for these tasks is not motivated well. Please indicate why these tasks are chosen. It seems the robot arm task is very similar to the navigation task, due to robot arm's end effector being position controlled directly. Why is it worthwhile to study this task separately?\n\n- The GTC metric is not very well established (yet). Please provide some extra information on how it is calculated. This should also include some discussion on why this metric allows judging sufficiency and disentangledness. How would rotating the measurement frame of the ground-truth influence the results?\n\n- Why are the robotics priors not in Table 1?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear approach and contribution",
            "review": "This paper aims at comparing end-to-end learning vs separately learning a state representation and subsequently a controller.\n\nWhile this would be a relevant and important topic, the paper does not currently present consistent evidence to support this hypothesis.\n\nIn particular:\n- The approach proposed in the approach is not explained in sufficient details. After reading Sec.4 I have only a very vague and high-level idea of how the proposed approach might work. In Figure.2, what is I_t? What is the model that you are training? How are you learning this model? how do you define L_inverse?\n- The cited literature about state representation learning is absolutely incomplete. Papers like Lange et al. , Wahlström et al. and Finn et al. and citations herewithin.\n- From the experimental results, it is difficult to say anything definitive about the proposed hypothesis. 1) There are multiple end-to-end approaches in the literature, with significant differences in performance. which one are you using? (it seem A2C and PPO, but to which label do they correspond in the tables?) 2) How do you tune the weights of the reward function proposed? This seems an important design choice, but it is not much discussed. 3) In the table reported (e.g., Table 1) it does not seem to me that SRL consistently outperforms other approaches. Even for the arm tasks, Random features seem to outperform the proposed approach (and indeed all the methods except the ground truth). What is going on there?\n\nOverall¸ the paper would benefit from a clearer and more detailed text, and from improved experiments and comparisons.\n\nMinor comments:\n- It is unclear to me what \"goal-based robotic tasks\" means. How do you define a task without a goal?\n- An important and missing characteristic of a suitable state representation should be the generalization. In fact, a good representation would ideally allow the agent to generalize to some degree. \n- It seems very odd to me that the \"action should be implicitly encoded into the state representation\" could you elaborate of the motivation for this and the effects?\n\nReferences:\n- Autonomous reinforcement learning on raw visual input data in a real-world application\nS Lange, M Riedmiller, A Voigtlander\nNeural Networks (IJCNN), The 2012 International Joint Conference on, 1-8\n- From pixels to torques: Policy learning with deep dynamical models\nN Wahlström, TB Schön, MP Deisenroth\narXiv preprint arXiv:1502.02251\n-  Deep Visual Foresight for Planning Robot Motion\nChelsea Finn, Sergey Levine\nInternational Conference on Robotics and Automation (ICRA), 2017 ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting experiment results, unfortunately lacking in terms of new insights.",
            "review": "The paper is easy to read and the presentation is clear, and I really appreciate this.\n\nThe authors address the very important topic of feature extraction and state representation learning. New results in this area are always valuable and welcome. However, my feeling is that the paper falls short in terms of making sufficient new contributions for an ICLR paper. \n\n1. The authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking. These are standard and well-known techniques in machine learning. The key contribution one looks for is in terms of new insights on why and when each approach works. The paper fails to provide much insight in this regard. Take this simple scenario: Suppose my input image is actually generated by a linear map plus gaussian noise on the true states. Then I can simply use a PCA as my \"auto encoder\" and happily learn a high quality state representation close to the ground truth. We know why this works. In the real task, the image is a complex non-linear transformation of the true states. What insights do I gain from this work in terms of how I should tackle this?\n\n2. Section 3 states some desirable characteristics in constructing a state representation. These are well-known and fundamental aspects of machine learning -- applicable to almost all models that we want to learn. In this sense, I do not find the section very informative.\n\n3. The empirical results (say, Table 1) seem too noisy to interpret (other than that using the ground truth provides the best performance). It almost seems to suggest that one should simply use random features (as done in the \"extreme learning machine\" approach). Again, not much insight to draw from this.\n\n4. Last comment. Suppose I have a new robotic goal-directed task and my inputs are camera images. Does this work tell me something that I don't already know in terms of learning new feature representation that is highly suitable for my task?\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}