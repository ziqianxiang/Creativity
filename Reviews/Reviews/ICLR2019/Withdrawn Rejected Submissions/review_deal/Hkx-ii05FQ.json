{
    "Decision": {
        "metareview": "The paper investigates a variant of the \"cross-entropy method\" (CME) for heuristic combinatorial optimization, based on stochastically improving a search distribution via policy optimization in a surrogate objective.\n\nUnfortunately, the reviewers unanimously recommended rejection, noting that the significance of the contribution over CME remains far from clear and insufficiently supported by the given evidence.  The experimental evaluation was unconvincing to all of the reviewers, particularly since only one artificial problem (clique finding) was considered in the paper (with an additional problem, k-medoid clustering, briefly and incompletely considered in the appendix).  Several additional concerns were raised about the experimental evaluation, which triggered lengthy author responses but really need to be properly handled in the paper itself:\n\n- The sensitivity of performance to the optimization algorithm is a concern and requires more detailed understanding so that reasonable choices can be made in practice.\n\n- The independence assumption between search components is an extreme simplification that limits the appeal and applicability of the proposed approach.  Even after author response, it remains unconvincing that an independent search distribution over subcomponents can be effective in challenging combinatorial spaces.  Concrete evidence on challenging problems would be a more effective evidence than discussion.\n\n- The comparisons omitted any tailored algorithms for the specific problems.  Even if the authors insist on only comparing to more \"general purpose\" methods, there is a large space of evolutionary and Bayesian optimization strategies that have been neglected from the comparison.  A justification is needed for such an omission (if indeed it is even justifiable).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Variation of \"cross-entropy method\", not considered sufficient for acceptance"
    },
    "Reviews": [
        {
            "title": "Interesting paper with similar prior work and unconvincing experimental evaluation",
            "review": "The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure. The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task.\n\nThe proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper. The proposed sampling distributions assumes independence between the random variables over which the authors optimize â€” I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution. Can the authors elaborate on this? However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions. The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach but unconvincing experiments and motivation.",
            "review": "The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process). They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution. They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3.\n\nI think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method. The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult. The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer. I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.\n\nAdditionally, I find the motivation for caring about local optimality unconvincing. I take exception that people care more about local optimality than the actual objective. From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself). This also holds for k-means, which is usually run multiple times with different starting conditions.\n\nSome comments:\n- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample. e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)\n- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).\n- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)\n- I would also encourage the authors to come up with a more descriptive name for the approach.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Cakewalk is too similar to the cross-entropy method to warrant acceptance",
            "review": "## Summary ##\n\nThe authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem.\n\n\n## Assessment ##\n\nI don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. \n\n I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.\n\nThey both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n\n\n## Specific Comments and Questions ##\n\n1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.\n2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$.\n3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this?\n4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently.\n5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. \n6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}