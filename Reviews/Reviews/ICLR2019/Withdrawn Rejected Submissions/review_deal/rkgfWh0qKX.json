{
    "Decision": {
        "metareview": "This paper adapts language models (LMs), recurrent models trained on large corpus to produce the next word in English, to two commonsense reasoning tasks: the Winograd schema challenge and commonsense knowledge extraction. For the former, the language model score itself is used to obtain substantial gains over existing approaches for this challenging task, while a slightly more involved training procedure adapts the LMs to commonsense extraction. The reviewers appreciated the simplicity of the changes to existing LMs and the impressive results (especially on the WSC). \n\nThe reviewers point out the following potential weaknesses: (1) clarity issues in the writing and the presentation, (2) a lack of novelty in the proposed approach, given a number of recent work has shown the ability of language models to perform commonsense reasoning, and (3) critical methodological issues in the evaluation that raise questions about the significance of the results. A lack of response from the authors meant that there was no further discussion needed, and the reviewers encourage the authors to take the feedback to improve further versions of the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Clarity and Evaluation Issues"
    },
    "Reviews": [
        {
            "title": "some interesting results, but could use more rigor and empirical exploration",
            "review": "This paper evaluates language models for tasks that involve \"commonsense knowledge\" such as the Winograd Schema Challenge (WSC), Pronoun Disambiguation Problems (PDP), and commonsense knowledge base completion (KBC). \n\nPros:\n\nThe approach is relatively simple in that it boils down to just applying language models. \n\nThe results outperform prior work, in some cases by pretty large margins. \n\nThe language models are quite large and it appears that this is the first time that large-scale language models have been applied seriously to the Winograd Schema Challenge (rather than, say, to the NLI version of it in GLUE, to which it is hard to compare these results). \n\nSome of the additional and ablation experiments are interesting. \n\n\nCons:\n\nWhile this paper has some nice results, there are some aspects of it that concern me, specifically related to hyperparameter tuning and experimental rigor:\n\nThere are three methods given for using an LM to make a prediction: full, full-normalized, and partial. For PDP, full (or perhaps full-normalized?) works best, while for WSC, partial works best. The differences among methods, at least for WSC, are quite large: from 2% to 10% based on Figure 3. I don't see a numerical comparison for PDP, so I'm not sure how these methods compare on it. Since the datasets are so small, there is no train/dev/test split, so how were these decisions made? They seem to be oracle decisions. This is concerning to me, as there is not much explanation given for why one method is better than another method. \n\nMy guess is that the reason why partial works better than full for WSC is because the WSC sentences were constructed such that the words up to and including the ambiguous pronoun were written such that it would be difficult to identify the antecedent of the pronoun. The rest of the sentence would be needed to identify the antecedent. I'll assume for this discussion that the sentence can be divided into three parts x, y, and z, where x is the part before the pronoun, y is the phrase that replaces the pronoun, and z is the part after the pronoun. Then p(z|xy), which is partial scoring, corresponds to p(xyz)/p(xy), which can be viewed as \"discounting\" or \"normalizing for\" the probability of putting y in place of the pronoun given the context x. For WSC, I think one of the goals in writing the instances is to make the \"true\" p(xy) approximately equal for both values of y. The language model will not naturally have this be the case (i.e., that p(xy) is the same for both antecedents), so dividing by p(xy) causes the resulting partial score to account for the natural differences in p(xy) for different antecedents. This could be explored empirically. For example, the authors could compute p(xy) for both alternatives for all PDP and WSC instances and see if the difference (|p(xy_1) - p(xy_2)|, where y_1 and y_2 are the two alternatives) is systematically different between WSC and PDP. Or one could see if p(xy) is greater for the antecedent that is closer to the pronoun position or if it is triggered by some other effects. It could be the case that the PDP instances are not as carefully controlled as the WSC instances and therefore some of the PDP instances may exhibit the situation where the prediction can be made partially based on p(xy). The paper does not give an explanation for why full scoring works better for PDP and chalks it up to noise from the small size of PDP, but I wonder if there could be a good reason for the difference.\n\nThe results on KBC are positive, but not super convincing. The method involves fine-tuning pretrained LMs on the KBC training data, the same training data used by prior work. The new result is better than prior work (compared to the \"Factorized\", the finetuned LM is 2.1% better on the full test set, and 0.3% better on the novelty-based test set), but also uses a lot more unlabeled data than the prior work (if I understand the prior work correctly). It would be more impressive if the LM could use far fewer than the 100K examples for fine-tuning. Also, when discussing that task, the paper says: \"During evaluation, a threshold is used to classify low-perplexity and high-perlexity instances as fact and non-fact.\" How was this threshold chosen?\n\nI also have a concern about the framing of the overall significance of the results. While the results show roughly a 9% absolute improvement on WSC, the accuracies are still far from human performance on the WSC task. The accuracy for the best pretrained ensemble of LMs in this paper is 61.5%, and when training on WSC-oriented training data, it goes up to nearly 64%. But humans get at least 92% on this task. This doesn't mean that the results shouldn't be taken seriously, but it does suggest that we still have a long way to go and that language models may only be learning a fraction of what is needed to solve this task. This, along with my concerns about the experimental rigor expressed above, limits the potential impact of the paper.\n\n\nMinor issues/questions:\n\nIn Sec. 3.1: Why refer to the full scoring strategy as \"naive\"? Is there some non-empirical reason to choose partial over full?\n\nThe use of SQuAD for language modeling data was surprising to me. Why SQuAD? It's only 536 articles from Wikipedia. Why not use all of Wikipedia? Or, if you're concerned about some of the overly-specific language in more domain-specific Wikipedia articles, then you could restrict the dataset to be the 100K most frequently-visited Wikipedia articles or something like that. \n\nI think it would be helpful to give an example from PDP-60.\n\nSec. 5.1: How is F_1(n) defined?  I also don't see how a perfect score is 1.0, but maybe it's because I don't understand how F_1(n) is defined.\n\nSec. 6.1: Why would t range from 1 to n for full scoring? Positions before k are unchanged, right? So q_1 through q_{k-1} would be the same for both, right?\n\nIn the final example in Figure 2, I don't understand why \"yelled at\" is the keyword, rather than \"upset\". Who determined the special keywords?\n\nI was confused about the keyword detection/retrieval evaluation. How are multi-word keywords handled, like the final example in Figure 2? The caption of Table 5 mentions \"retrieving top-2 tokens\". But after getting the top 2 tokens, how is the evaluation done?\n\nSec. 6.3 says: \"This normalization indeed fixes full scoring in 9 out of 10 tested LMs on PDP-60.\" Are those results reported somewhere in the paper? Was that normalization used for the results in Table 2?\n\nSec. 6.3 says: \"On WSC-273, the observation is again confirmed as partial scoring, which ignores c [the candidate] altogether, strongly outperforms the other two scorings in all cases\" -- What is meant by \"which ignores c altogether\"?  c is still being conditioned on and it must not be ignored or else partial scoring would be meaningless (because c is the only part that differs between the two options). \n\n\nTypos and minor issues:\n\nBe consistent about \"common sense\" vs. \"commonsense\".\n\nBe consistent about \"Deepnet\" vs. \"DeepNet\" (Tables 2-3).\n\nSec. 1:\n\"even best\" --> \"even the best\"\n\"such as Winograd\" --> \"such as the Winograd\"\n\"a few hundreds\" --> \"a few hundred\"\n\"this type of questions\" --> \"this type of question\"\n\"does not present\" --> \"is not present\"\n\"non-facts tuples\" --> \"non-fact tuples\"\n\nSec. 2:\n\"solving Winograd\" --> \"solving the Winograd\"\n\"Store Cloze\" --> \"Story Cloze\"\n\"constructed by human\" --> \"constructed by humans\"\n\nSec. 4:\nWhat is \"LM-1-Billion\"?\nWhy SQuAD?\n\"Another test set in included\" --> \"Another test set is included\"\n\nSec. 5.2:\nCheck margin in loss_new\n\n\"high-perlexity\" --> \"high-perplexity\"\n\nSec. 6:\nFigure 2 caption: \"keyword appear\" --> \"keyword appears\"\n\nSec. 6.2:\n\"for correct answer\" --> \"for the correct answer\"\n\nAppendix A:\n\"acitvation\" --> \"activation\"\nAppendix B:\nFigure 4 caption: \"is of\" --> \"is\"\nThe right part of Figure 4 has some odd spacing and hyphenation.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Two somewhat disconnected small contributions",
            "review": "This paper uses a language model for scoring of question answer candidates in the Winograd schema dataset, as well as introduces a heuristic for scoring common-sense knowledge triples.\n\nQuality:\nPros: The paper shows improvements over previous papers for two tasks related to common-sense knowledge. They both mainly utilise simple language models, which is impressive. The second one uses an additional supervised collaborative filtering-style model. The authors further perform a detailed error analysis and ablation study.\nCons: The paper isn't very well-written. It contains quite a few spelling mistakes and is unclear in places. The Winograd Scheme Challenge isn't a very interesting dataset and isn't widely used. In fact, this is evidenced by the fact that most cited papers on that datasets are preprints and technical reports.\n\nClarity:\nThe paper is confusing in places. It should really be introduced in the abstract what is meant by \"common sense\". Details of the language model are missing. It is only clear towards the end of the introduction that the paper explores two loosely-related tasks using language models.\n\nOriginality:\nPros: The suggested model outperforms others on two datasets.\nCons: The suggested models are novel in themselves. As the authors also acknowledge, using language models for scoring candidates is a simple baseline in multiple-choice QA and merely hasn't been tested for the Winograd schema dataset.\n\nSignificance:\nOther researchers within the common-sense reasoning community might cite this paper. The significance of this paper to a larger representation learning audience is rather small.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Studying whether LM encode common-sense information. Novelty, clarity and methodology concerns",
            "review": "This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.\n\nThis paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. \n\nThe paper also suffers from methodological problems:\n-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. \n-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.\n\nFinally, the paper suffers clarity issues. \n-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). \n-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).\n-- The authors do not address hyper-parameter tuning. \n-- What is the gold standard for the \"special word retrieved\" data? how is it computed?\n\n\nOther comments: \n-- Page 2: \"In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning.\": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.\n-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. \n-- Page 5: how is F1 defined?\n\nMinor comments: \n-- Page 2: \" ... despite the small training data size (100K instances).\": 100K is typically not considered a small training set (for most tasks at least)\n-- Page 5: \"... most of the constituent documents ...\": was this validated in any way? how?\n-- The word \"extremely\" is used throughout the paper without justification in most cases.\n\n\nTypos and such:\npage 1: \"... a relevant knowledge to the above Winograd Schema example, **does** not present ... \": should be \"is\"\npage 5: \"In the previous sections, we ***show*** ...\": showed\npage 7: \"For example, with the ***test*** ...\": \"test instance\"\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}