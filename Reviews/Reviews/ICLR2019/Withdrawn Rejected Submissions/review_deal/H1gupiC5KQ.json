{
    "Decision": {
        "metareview": "The paper suggests using an ensemble of Q functions for Q-learning. This idea is related to bootstrapped DQN and more recent work on distributional RL and quantile regression in RL. Given the similarity, a comparison against these approaches (or a subset of those) is necessary. The experiments are limited to very simple environment (e.g. swing-up and cart-pole). The paper in its current form does not pass the bar for acceptance at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "The paper can be improved"
    },
    "Reviews": [
        {
            "title": "Not enough novelty",
            "review": "This paper proposes a cute idea as suggesting ensembles of Q-function approximations rather than a singular DQN. \n\nHowever, at the core of it, this boils down to previously studied methods in the literature, one of which also is not cited here: \n\n@inproceedings{osband2016deep,\n  title={Deep exploration via bootstrapped DQN},\n  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},\n  booktitle={Advances in neural information processing systems},\n  pages={4026--4034},\n  year={2016}\n}\n\nExperiments provided in this paper compares with only the weak baseline of single DQN, however, it fails to compare other similar ideas in the literature such as the above paper. Hence, this paper lacks enough novelty for publication, and it is not clear from the experiments that the specific method proposed in this paper is better than others in the SOTA. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea while the experiments are not enough.",
            "review": "This paper proposes the deep reinforcement learning with ensembles of Q-functions. Its main idea is updating multiple Q-functions, instead of one, with independently sampled experience replay memory, then take the action selected by the ensemble. Experimental results demonstrate that the proposed method can achieve better performance than non-ensemble one under the same training steps, and the decision space can also be stabilized.\n\nThis paper is well-written. The main ideas and claims are clearly expressed. Using ensembles of Q-function can naturally reduce the variance of decisions, so it can speed up the training procedure for certain tasks. This idea is simple and works well. The main contribution is it provides a way to reduce the number of interactions with the environment. My main concern about the paper is the time cost. Since the method requires updating multiple Q-functions, it may cost much more time for each RL time step, so I’m not sure whether the ensemble method can outperform the non-ensemble one within the same time period. This problem is important for practical usage. However, the authors didn’t show these results in the paper.\n\nMinor things:\n+The main idea is described too sketchily. I think more examples, such as in section 8.1, should be put in the main text.\n+Page6 Line2, duplicated ‘the’.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review report",
            "review": "This paper introduces an ensemble version of Deep RL by bagging Q-function approximation estimates. In the experiments, the performance of the proposed work is compared to the baseline, single DQN. In spite of the contribution, this paper has a critical issue. \n\nIt has been extensively studied in the literature that ensemble DQN could lead to better performance than a single DQN. See the seminal work by Osband et al. (2016). The authors did not cite this paper, not to say a long list of recent works who have cited this seminal work. This indicates that the authors fail to conduct a serious literature review. In addition, more comprehensive experiments are required to compare the proposed work with the state-of-the-art ensemble DQN methods.\n\nOsband et al. (2016), Deep Exploration via Bootstrapped DQN. NIPS.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}