{
    "Decision": {
        "metareview": "AR1 is concerned about the poor organisation of this paper.  AR2 is concerned about the similarity between TRL and TR. The authors show some empirical results to support their intuition, however, no theoretical guarantees are provided regarding TRL superiority.  Moreover,  experiments for the Taskonomy dataset as well as on RNN have not been demonstrated, thus AR2 did not increase his/her score.  AR3 is the most critical and finds the clarity and explanations not ready for publication.\n\nAC agrees with the reviewers in that the proposed idea has some merits, e.g. the reduction in the number of parameters seem a good point of this idea. However, reviewer urges the authors to seek non-trivial theoretical analysis for this method. Otherwise, it indeed is just an intelligent application paper and, as such, it cannot be accepted to ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Numerous concerns."
    },
    "Reviews": [
        {
            "title": "Poorly organized, poorly motivated paper.",
            "review": "Summary: The authors propose tensor ring nets for multi-task learning\n\nCons: This is a poorly organized paper and poorly motivated. \nThis paper discusses relevant mathematics with no motivation in section 2, while the  prior work is in section 4. Seems backward.\n\nPlease reference the first papers to employ tensor decompositions for imaging.\n\nM. A. O. Vasilescu, D. Terzopoulos, \"Multilinear Analysis of Image Ensembles: TensorFaces,\"  Proc. 7th European Conference on Computer Vision (ECCV'02), Copenhagen, Denmark, May, 2002, in Computer Vision -- ECCV 2002, Lecture Notes in Computer Science, Vol. 2350, A. Heyden et al. (Eds.), Springer-Verlag, Berlin, 2002, 447-460. \n\n M. A. O. Vasilescu, D. Terzopoulos, \"Multilinear Subspace Analysis for Image Ensembles,'' Proc. Computer Vision and Pattern Recognition Conf. (CVPR '03), Vol.2, Madison, WI, June, 2003, 93-99. \n\nM.A.O. Vasilescu, \"Multilinear Projection for Face Recognition via Canonical Decomposition \",  In Proc. Face and Gesture Conf. (FG'11), 476-483.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple idea with interesting results",
            "review": "The novelty and experiments are somewhat limited. Thus I am lowering my score.\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe authors proposed a variant of tensor ring formulation for multi-task learning. They achieved that by sharing some of the TT cores for learning \"common task\" while learning individual TT cores for each separate tasks.\n\nPros:\n1) Overall nice but simple extension of TT/ TR framework\n2) Nice set of experiments which have shown improvement over standard TT/ TR framework for MTL.\n\nCons (and suggestions):\n1) As to my knowledge TT/ TR have not been used for MTL before, I wonder if someone wanted to the proposed method is the only way to achieve it, so in that sense, it's a very \"simple\" extension.\n2) Though authors called something called \"TRL\", I think it is just an indexing scheme so essentially the same idea of TR.\n3) I wonder why authors suddenly mentioned about convolution in the end of section 3.1., looks very out of the place discussion.\n4) I suggest in Section 3.2., make the shareable cores not adjacent in Eq. (4) as they claimed.\n5) The experiments are somewhat \"````````simplistic\" and I believe the power of this sharing should have experimented on Taskonomy data (https://arxiv.org/pdf/1804.08328.pdf). Right now, the experimental setup is very much simplistic, which is one of the main points the authors should address.\n6) Can the authors comment on the number of parameters used?\n7) I wonder if the author can show some RNN/ LSTM experiment because some of the datasets used like OMLIGLOT/ MNIST are too simple to count as an experiment. Challenge will be to see the performance in challenging MTL. \n8) I believe the authors should comment on the choice of c and the location of the shareable cores.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Tensor-based soft-sharing MTL, upgraded with TR-decomposition. Maybe practically useful to enhance. But not clearly enough written or evaluated.",
            "review": "Summary:  This paper studies deep multi-task learning. Prior papers have studied various knowledge sharing approaches for deep multi-task learning including hard and soft sharing schemes. And some soft sharing schemes have used tensor decompositions (including TT, and Tucker). This paper fuses this line of work with the recently proposed Tensor-Ring decomposition in order to obtain Tensor Ring (TR)-based soft sharing for multi-task learning. The results show some improvement over prior deep MTL methods based on other tensor factorisation methods.\n\nStrengths:\n+ Nice extension of existing line of work tensor-factorisation based MTL.\n+ More flexibility for controlling shared/unshared portions of weights compared to DMTRL. \n+ Improves on previous methods results.\n+ Experiments evaluate how MTL methods relate with various amounts of training data on each task.\n\nWeaknesses:\n- Novelty/significance is limited. \n- Writing. Many things are not clearly and intuitively explained. Some claims are not adequately justified. \n- Introduces more hyper parameters to tune.\n- Results may rely on hyper parameter tuning. \n\nComments:\n1. Novelty.  Existing studies already established the template of different tensor factorisation methods (TT, Tucker) being possible to plug into deep networks for different kinds of soft-sharing MTL. Meanwhile, TR decomposition is taken off the shelf; (and as it’s been applied for compression before, this is not the first time TR decomposition has been used in a CNN context either). Therefore this is an A+B paper and a high bar should be met for the additional analysis, insight, or performance improvements that should provided.\n2. Lots of writing issues:\n2.1 Many things are not explained transparently enough at best (or major over-claim at worst). For example: \n2.1.1 Paper claims the benefit that each task can have its own I/O dimensionality. However if TR-decomp is “circularly connected” TT-decomp (Fig 1), then this seems not to happen automatically. So it should be unpacked more clearly how this is achieved. \n2.1.2 Paper claims favourable ability to use more private cores than TT, where only one core is private. However circular TT would also seem to have one private core by default (the core with a task axis). So I suspect something else is going on, but this is completely unclear and should be explained more transparently. Furthermore it should be justified if whatever modifications do enable these properties are definitely a unique property of TR-decomp, or could also be applied to TT-decomp. \n2.1.3 Statement “TRMTL generalizes to allow the layer-wise weight to be represented by a relatively lager number of latent cores” unclear: generalises what? larger number of cores than what? Than TT? The previous presentation suggests TT and TR should have same number of cores.  \n2.1.4 Statements like “TR enjoys the property of circular dimensional permutation invariance” are made without any explanation about what is the implication of this for neural networks and multi-task learning.  \n2.2 Many claims are inaccurate or not adequately backed up by theory or experiment. EG: (i) Paper claims to include DMTRL as a special case. But it only subsumes DMTRL-TT, not DMTRL-Tucker. Because TR-decomp does not include Tucker-decomp as an exact special case.  (ii) Sentences “TR-ranks are usually smaller than TT-ranks” are assertions without verification. \n2.3 Sentences are taken verbatim from other papers, plagiarism. For example: “TR model is more flexible than TT, because TR-ranks can be equally distributed in the cores, but TT-ranks have a relatively fixed pattern”  is verbatim from Zhao’16 TR-decomp paper. \n3. Hyperparameters: This paper apparently gains some practical benefit due to the notion of shared/unshared cores. However, this also introduces  additional hyper parameters (E.g., each layers private proportion “c”) to tune besides the ranks. Unlike the rank that can be pre-estimated by reconstruction error, this one seems to require tuning by cross-validation. This is not scalable. \n4. Hyperparameters+Tuning: Hyperparameters Private proportion, “sharing pattern”, IO dimension seem to be tuned by accuracy.( “We test different sharing patterns and report the ones with the best accuracies”). This is even less scalable, and additional tuning makes it unsurprising it surpasses other models performance.\n5. Insight & Analysis. All the core selection & public/private core selection are treated as black box optimisation. No insight is given about what turns out to be useful to share or not, and how consistent this is, etc.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}