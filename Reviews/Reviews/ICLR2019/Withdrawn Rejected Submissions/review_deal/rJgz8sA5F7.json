{
    "Decision": {
        "metareview": "This work is effectively an extension of progressive nets, where the task ID is not given at test time. There were several concerns about novelty of this work and the evaluation being insufficient. There was a reasonable back and forth between the reviewers and authors, and the reviewers are all aligned with the idea that this work would need a substantial rewrite in order to be accepted at ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Not good enough",
            "review": "The work proposes a structure that mimics progressive nets. Maybe the main difference from progressive nets is that backwards connection from the new features to the old features in layer 2 are not 0 out. This could cause interference, however is solved by using the task ID to not evaluate those new features when going back to a previous task. I think this is a technical detail, that does not provide any explicit advantage or disadvantage over progressive nets. \n\nEmploying GANs/VAE to predict task id also can be seen as not an ideal choice. In particular the GAN network will suffer from catastrophic forgetting, which is solved (if I understood correctly) by training the GAN with data from all tasks. Which makes one wonder, if we can affort to access data from all tasks to learn the GAN then why not the classification model too !?\n\nI think an alternative might be something like the Forget Me Not Process published and used in the original work with EWC.\n\nUnfortunately due to presence of these previous works, lack of more thorough comparison with other existing approaches, the work should not be accepted to ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "missing references and comparisons, unclear evaluation",
            "review": "Summary\nThis paper proposes an extension of Progressive Networks [Rusu et al. NIPS 2016] (unfortunately, not cited) where the task id is not given at test time. This is inferred by a battery of classifiers trained on data produced by generative models trained on task specific data.\nThe authors argue strong connections to similar mechanisms in the human brain and demonstrate this method on a stream of 2 or 3 vision tasks. However, the interpretation of these results is dubious.\n\nNovelty: given prior work on Progressive Nets and other methods using generative models for continual learning, novelty is limited.\n\nRelevance: the motivation and aim of this work is certainly relevant for ICLR.\n\nClarity: the paper is overall clear, although it needs a bit of rewriting to improve fluency (see for instance sec. 3.4.1).\n\nReferences: the authors should definitely cite Progressive Networks and their extension \"Progress and Compress\" (Schwarz ICML 2018), as their approach is an extension of the former with the only difference that the task id is inferred at test time by using a battery of binary classifiers.\n\nEmpirical validation: The empirical validation is limited because of:\na) lack of comparison to Progressive Nets, \nb) lack of simple baselines (e.g., how about replacing H-Net with an inference process like task_id = argmin_i=1..T loss(C-Net, task = i) ),\nc) unclear interpretation of the provided results (how can the accuracy on MNIST be 100%? are the authors reporting training accuracy?)\nd) very limited number of tasks considered (up to 3)\n\nGeneral comments\nMajor drawbacks of the proposed approach are: 1) training on new tasks can never improve performance on past tasks (unlike other methods like GEM (Lopez-Paz et al. NIPS 2017), 2) the number of parameters grow linearly with the number of tasks (an issue addressed by the Progress and Compress paper above), and 3) the overall approach is not efficient as it requires lots of data from each task in order to train the generative models.\nFinally, I think all the connections and inspiration from how the human brain works should be toned down.  Statements like \"the C-Net corresponds to the human cortex...\" should be at the very least rephrased appropriately.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough novelty and experiments are not very informative.",
            "review": "The key idea of this paper is to expand the network for training on new tasks which is termed as C-Net, and train an additional generative model which is used for predicting task id (which is called H-Net), and use the task id for selecting weights from the C-Net.\n\nPros:\n1. It is relatively easy to understand the paper. \n2. The originality of this paper lies in the usage of generative model to predict task id (H-Net). To my knowledge has not been proposed before.\n3. In contrast to previous works in multi-task learning, which assumes task id is available both during training and inference, this work tries to remove the need of task id during inference, which makes it closer to the general definition of continual learning.\n\nCons:\n1. Expanding the network for new tasks is not a novel contribution of this paper, it has already been proposed in previous works on multi-task learning. Doing expansion on all of the layers does not qualify for a major contribution in my opinion.\n2. The experimental comparison is not very fair in my opinion, \n     a. Comparing accuracy of C-Net to other methods is not very useful. Because this methods expands the network for every new task, while other methods (EWC, LwF) has limited to no expansion in the network. Given that the single network result is far from state of the art (table 3), I suppose model size could contribute to the accuracy boost.\n     b. It is not explicitly stated in the paper whether the output neurons are shared between tasks or an individual set of output neurons are used for different tasks, but from the rest of the paper I suppose disjoint neurons are used. Then the comparison between EWC and this work is not fair because EWC shares the output neurons among tasks.\nThis is not to blame this paper for not making fair comparison, since given different assumptions between methods (availability of task id, shared output neurons etc.), it is usually difficult to fairly compare between continual learning methods.  This problem is raised in another submission https://openreview.net/forum?id=ByGVui0ctm. The point here is that the accuracy of C-Net is not a good measure of how good this method is.\n3. I disagree with the point that MNIST and SVHN are similar, they have very different distributions and are very easy to tell apart with a model. One concern is that the generative H-Net may fail to work once the distributions of the tasks overlap to some extent. e.g. cifar10 vs cifar100.\n\nAs a conclusion, the key contribution of this work is using generative model to determine task id which removes the need for task id during inference. It is relatively insufficient for publication on ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}