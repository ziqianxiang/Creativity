{
    "Decision": {
        "metareview": "This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. \n\nThese concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting direction but needs more work"
    },
    "Reviews": [
        {
            "title": "An interesting proposal lacking justification.",
            "review": "In this paper the authors introduce a new quantity, the nonlinearity coefficient, and argue that its value at initialization is a useful predictor of test time performance for neural networks. The authors conduct a wide range of experiments over many different network architectures and activation functions to corroborate these results. The authors then extend their method to compute the local nonlinearity of activation functions instead.\n\nI am a bit torn on this paper. I appreciate the direction that the authors have chosen to pursue. The topic of identifying parameters that are predictive of trainability is certainly interesting and has the potential to be quite impactful. Moreover, the breadth of the experiments conducted by the authors is novel and significant. Finally, I find the the overall manner in which the authors have chosen to present their data refreshingly transparent. Together, this leads me to believe that the quantity proposed by the authors might be useful to researchers.\n\nHaving said that, I am concerned by the author’s exposition of the nonlinearity coefficient itself. Fundamentally, my concern stems from the fact that it seems a lot of relatively ad-hoc decisions were made in the construction of the nonlinearity coefficient and an insufficiently good job was done to compare it to other measures of nonlinearity.\n\nSpecifically, it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function. Moreover, I feel as though there is already a well defined notion of nonlinearity at a point that could be constructed by reference to the Hessian (or generally by the approximation error induced by truncating the Taylor series after the linear term). I would like to see some comparison between these two methods. \n\nThis is made more troubling given that the correlation found by the authors is present but does not seem especially strong. For example, in fig. 2A it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value. Prior work (for example, [1] from last years ICLR) has shown strong correlations between the Frobenius norm of the Jacobian and test error (see fig. 5 and fig. 6). Since the definition of the nonlinearity coefficient seems somewhat ad-hoc I would love to see a comparison between it and just looking at the Jacobian norm in terms of predicting test accuracy.\n\n[1] - SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY\nRoman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid contribution",
            "review": "This paper proposes a metric to measure the \"nonlinearity\" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.\n\nApart from a few problems I think this paper is well written and thorough. The contribution is solid, although not earth shattering given previous work on such metrics.  There seems to be a basic error in some of the early math, although I don't think this will qualitatively affect the results in any significant way.\n\n\n-----------------\nDetailed comments by section:\n------------------\n\nSection 3:\n\nIt seems like a 1/sqrt(d) factor is missing from these Q_i(S_x x(i)) and Q_j(S_x f(x,j)) formulas.  As far as I can tell this doesn't affect Def 1 because you seemed to use the correct formula there. \n\nHowever, the rewritten version with the traces doesn't seem to be correct. There should be a d_in factor in the denominator (inside the square root). This error seems unrelated to the other one.  Assuming I'm correct and that this is an error, does this affect your results in the various figures?  And what is the actual final definition of NLC that you used?\n\nIn general, it's annoying for the reader to verify that all of these forms are equivalent.  And it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as I can tell).  I would suggest making this section more rigorous and writing out everything carefully. And you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. \n\nThe use of the Q and S symbols feels superfluous and counterproductive. Standard notation with expectations and squares wouldn't take much more space and would be a lot clearer.\n\n\nSection 4:\n\n\"we plot the relative diameter of the linearly approximable regions of the network as defined in section 3\": but you don't seem to define \"relative diameter\" there. As far as I can tell it's only defined in Appendix E, and this is only mentioned in the caption of figure 1.  It's impossible to interpret this result without knowing precisely what \"relative diameter\" is.  If you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the NLC estimates.\n\nIn Figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap?  I guess the Figure 3 results suggest the latter possibility, which is surprising to me. \n\n\nWhat does it mean to have a \"very biased output\".  What does that inequality mean intuitively?  Should there be an absolute value on the RHS?  It would be much easier to parse it if it were written in plain notation without these S and Q symbols.\n\n\nSection 6:\n\n\"metric also an\" -> \"metric also has an\"\n\nCan you generate a failure case for \"correlation information\" that doesn't involve Batch Norm layers?  I don't think the authors of those works meant for their results to deal with that.\n\nNote that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Does the nonlinearity coefficient measure nonlinearity?",
            "review": "I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \\lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\\lambda^2 + n - 1) (\\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \\lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. \nConversely, it’s possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldn’t necessarily result in good generalization, which seems to agree with the experimental observation. \nNow given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC?\nThe experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}