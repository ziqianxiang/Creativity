{
    "Decision": {
        "metareview": "A focused contribution that is clearly presented. That being said, the task of low-resource named entity recognition is fairly narrow and it is hard to tell how significant the empirical results are. The paper could be much stronger if it evaluated on a second task (and third task). Right now it is unclear whether the technique would generalize to other tasks.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Nicely and clear written paper with innovative aspects. The results look strong but could increase confidence in results via reporting of variance / stat sig; Paper is a bit narrow as applied new method to one task only.",
            "review": "The authors propose a new architecture Dual Adversarial Transfer Network for addressing low-resource NER. They achieve a new SOTA on low resource language. The authors compare a base-line with two alternatives based on variants of GANs.  \n\nThe results go beyond SOTA for low resource NER which seems a solid contribution. The paper is well and clearly written and I would be able to replicate the experiments. I wonder if this is a new architecture that works well for one task or if it could be applied to other tasks too. This would strengthen the approach and paper quite a bit. Could the method for instance be applied to other labeling tasks: POS tagging, morphological features. This would increase the potential impact substantially.  \n  \nIn low resource scenarios often methods work that do stop working at a some point with more resources. For this methods the boundary when this effect occurs would be interesting to explore; Figure 2 goes into this direction which is a quite nice study but the boundary is not  explored further; by using for instance the English NER data. Additionally, the performance on the English data set would indicate what the method could perform in comparison to current SOTA for normal resource setting - you could use some of the low resources in addition. The English data set was used but only to exploit it for the transfer learning. \nTable 2 is a good overview on SOTA. I really wonder about the variance of the results of the system which can be depending on the network quite large. Why not running repetition test, this  would enable the authors to report variance and statistically significance between the baseline and their other systems. \nI wonder also how more standard exploitation of additional data would do such as Bert, ELMO or older methods such as up-training - this would help to get a more complete picture and strength the paper further. \n\nThe paper could be stronger by applying the method to other task too as stated the authors - this is a ‘new architectures’ (for NER) which triggers the question and does it generalize to other tasks? In the conclusion there is even the claim as a statement! that this can be generalized to other NLP task without actually trying. I think, this can not be claimed in the conclusions without pursuing this in some other task and I suggest to tune this down.   \n\nOverall:\nNicely and clear written paper containing innovative elements. The results look strong too me but due to the lack of variance and stat sig., I am not sure if they are really super strong. The paper could be stronger by applying the method to other task too as stated ‘new architectures’ which triggers the question if the method generalizes to other tasks?   \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach for cross-domain / cross-lingual NER with solid empirical results; limited technical novelty",
            "review": "<Summary>\nAuthors propose the new “DATNet” for the NER task, which extends the base neural model for NER (Bi-LSTM+CRF sequence model with input represented with CharCNN-word embeddings) with the following two main components: (1) GRAD: a language (or resource) adversarial discriminator with adaptive weights that regularize source-target data imbalance, and (2) additional adversarial training approaches that perturb input samples in the embeddings space.\n\nThe paper reports big improvement over their baseline approaches without having to rely on other auxiliary or hand-crafted features. The experiment is performed for various low-resource scenarios (varying training data size). \n\n<Comments>\n- While the idea of applying “dual adversarial” approaches is new in the context of NER, the technical novelty of each component is limited. GRAD, for example, is rather a minor modification of Language Adversarial Discriminator (Kim et al., 2017) with a scalar weight parameter on loss. The empirical superiority of the proposed method (GRAD) over normal AD approaches cannot be claimed either -- for this authors need to report quantitative results of the (Base + AT + F/P-transfer with AD -- or e.g. \\alpha fixed at 0.5 or at some other rate), which I believe is missing in all figures and tables. Visualization of resulting feature distribution (Figure 3) is interesting to look at, but that alone does not suffice. There is no technical novelty in applying adversarial training either, except that it was used in the context of NER.\n\n\n- Authors use both source and target data to train their “base model” as well (“... we exploit all the source data and target data ...“), e.g. presumably by merging the source and target dataset as well as their embedding matrices, etc. It is important that authors report if there ever is a negative transfer case (e.g. a base model trained with just target data may outperform models trained with source+target data) at varying resource scenarios -- especially at sufficiently-resourced cases.\nIf by any chance their “base model” refers to in-domain training / in-domain testing results on target, the aforementioned baseline (naive merge of source and target data) is obviously an important baseline to report. I suggest that authors provide these details or clarify. (The confusion comes mainly because some of the SOTA results the authors quote are in-domain training / in-domain evaluation results on respective languages, and some are cross-domain results -- yet they are all under “cross-lingual/domain” columns in Table 2).\n\n- It would be interesting to report the learned optimal \\alpha value for each different setting (at varying r or training size) to see if the authors’ intuition is met. On a related note, from the manuscript alone, it is not entirely clear if \\alpha is a learnable parameter or a tunable hyper parameter -- by context I believe it is a model parameter. If they are tuned, authors need to report these values.\n\n<Nit> \n- Section 3.2.3, “... (GRAD) to enable adaptive weights for [each sample]” → I think it reads better with [each resource] or [each source and target], unless you meant \\alpha_i for each sentence.\n- Section 3.2.5, “... recently, adversarial samples are [wisely] incorporated” → [widely]\n- Fonts for figures could be bigger. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting model, thorough exposition and analysis, but partly flawed experiment setup",
            "review": "The paper introduces a novel architecture for low-resource named entity tagging: a dual adversarial transfer network, in which fusion between high- and low-resource, or high- and low-noise data is achieved via also novel resource-adversarial discriminator.\n\nThe model is interesting, novel, clearly exposed in sufficient detail, and warrants publication as such. The idea to unify representation differences and data imbalance under one model is noteworthy.\n\nI find that the description of related work, especially in the introduction, does not credit past contributions sufficiently. For one, large parallel corpora do exist for many languages, albeit some of them may not be sufficiently ample in named entities to facilitate cross-lingual NER. Yet, for the fortunate ones, such corpora do make for rather reasonable NER taggers via multi-source projection (cf. Enghoff et al., W-NUT 2018). Absent is the prominent work by Mayhew et al. (2017) in cross-lingual NER, as well as Pan et al. (2017) who engage with evaluation in 282 languages.\n\nThis unfair account of related work would not trouble me as much if it weren't coupled with an experiment in \"low-resource\" NER that features---Spanish and Dutch as target languages. Firstly, these languages are rich in resources, after all, they featured in CoNLL 2003, for one. Secondly, they are closely related to English as the source language, and any simulated low-resource scenario that features both the injection of target-language data *and* a very closely related source language is simply *not* representative of any true low-resource scenario.\n\nThis experiment setup troubles me, especially in light of real and synthetic NER data available to test the setup for true low-resource languages: from silver data by Al-Rfou et al. (2015) or Pan et al. (2017), via Mayhew et al. (2017) or Cotterrell and Duh (2017) who test on 10-15 gold datasets, etc., real low-resource NER data that is multilingual can be found. Any paper that in 2018 claims to do low-resource NER and then simulates a setup with Dutch and Spanish is poor scholarship in my submission, regardless of the clever model.\n\nI do let the clever model upvote my review, but not beyond borderline.\n\nMinor:\n- the use of \"lingual\" as noun is rather off-putting, at least to me",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}