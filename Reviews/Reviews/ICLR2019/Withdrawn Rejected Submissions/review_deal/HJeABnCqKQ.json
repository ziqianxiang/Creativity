{
    "Decision": {
        "metareview": "The paper proposes an extension to reinforcement learning with self-imitation (SIL)[Oh et al. 2018]. It is based on the idea of leveraging previously encountered high-reward trajectories for reward shaping. This shaping is learned automatically using an adversarial setup, similar to GAIL [Ho & Ermon, 2016]. The paper clearly presents the proposed approach and relation to previous work. Empirical evaluation shows strong performance on a 2D point mass problem designed to examine the algorithms behavior. Of particular note are the insightful visualizations in Figure 2 and 3 which shed light on the algorithm's learning behavior. Empirical results on the Mujoco domain show that the proposed approach is particularly strong under delayed-reward (20 steps) and noisy-observation settings.\n\nThe reviewers and AC note the following potential weaknesses: The paper presents an empirical validation showing improvements over PPO, in particular in Mujoco tasks with delayed rewards and with noisy observations. However, given the close relation to SIL, a direct comparison with the latter algorithm seems more appropriate. Reviewers 2 and 3 pointed out that the empirical validation of SIL was more extensive, including results on a wide range of Atari games. The authors provided results on several hard-exploration Atari games in the rebuttal period, but the results of the comparison to SIL were inconclusive. Given that the main contribution of the paper is empirical, the reviewers and the AC consider the contribution incremental.\n\nThe reviewers noted that the proposed method was presented with little theoretical justification, which limits the contribution of the paper. During the rebuttal phase, the authors sketched a theoretical argument in their rebuttal, but noted that they are not able to provide a guarantee that trajectories in the replay buffer constitute an unbiased sample from the optimal policy, and that policy gradient methods in general are not guaranteed to converge to a globally optimal policy. The AC notes that conceptual insights can also be provided by motivating algorithmic or modeling choices, or through detailed analysis of the obtained results with the goal to further understanding of the observed behavior. Any such form of developing further insights would strengthen the contribution of the submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "combination of self-imitation and GAIL - needs more thorough development of conceptual insights"
    },
    "Reviews": [
        {
            "title": "No comparison/evaluation on discrete action tasks (i.e. ATARI games)",
            "review": "[Paper Summary]:\nThis paper proposes a regularization technique for existing RL algorithms by encouraging them to learn to reproduce the best past trajectories which obtained higher reward than that of current policy. The proposed method in the paper has the same high-level idea as \"Self-imitation learning\" [Oh et.al. ICML 2018] with a different objective. Instead of performing imitation learning to distill the knowledge from past best trajectories, this proposes to use inverse reinforcement learning via GAIL objective [Ho and Ermano, 2016]. The best k trajectories from past experience are stored to train a discriminator which is then used to augment the external reward function with a discriminator reward.\n\n[Paper Strengths]:\nThe paper combines ideas from GAIL and self-imitation learning to propose a method that leverages past best trajectories via inverse-RL. This combination allows one to interpret self-imitation of best trajectories as a mechanism for \"reward shaping\" where learned discriminator shapes the environmental reward using past experiences. This is an exciting perspective and needs further discussion.\n\n[Paper Weaknesses and Clarifications]:\n=> This paper is very closely related to self-imitation learning [Oh et.al.], however, there is no theoretical justification provided (unlike [Oh et. al.]) whether the policy learned by optimizing Equation-11 is in anyway related to the optimal policy -- which was the case as shown in [Oh et.al.]. That being said, this is not a requirement for a paper to show theoretical justification as long as the paper justifies given approach with ample empirical evidence.\n=> The main comparison point for the proposed approach, \"GASIL\", is \"SIL\" [Oh et.al.]. This paper provides a good comparison on continuous control tasks on Mujoco where \"GASIL\" performs slightly better than \"SIL\" in 3 out of 6 environments. However, \"SIL\" [Oh et. al.] showed extensive experiments on all Atari Games + Mujoco tasks. Since the proposed approach is mainly empirically motivated, the experiments should at least show a comparison on all the environments of the closely-related prior work. It would be much more convincing to see a bar chart across all 48 Atari Games showing relative improvement of \"GASIL\" over \"SIL\", as shown in Figure-4 of [Oh et. al.].\n=> Other concerns:\n    - The paper mentions on multiple occasions that the proposed method would handle delayed and \"sparse\" reward. However, it is not clear how can past best trajectories help with \"sparse\" rewards (\"delayed-dense-rewards\" seems alright, but they are not the same as \"sparse\"!). For instance, suppose the agent gets only terminal-reward in a maze. In such a case, the agent would need to rely on some form of exploration bonus (count-based, curiosity etc.) to reach the sparse-goal even once.\n    - What prevents the learned policy from over-fitting to the local minima of the \"locally\" best trajectories seen so far?\n\n[Final Recommendation]:\nI request the authors to address the comments raised above. The paper has good potential, but sufficient empirical evidence is needed to justify the proposed technique. If the results on all Atari games can be included and shown to improve over \"SIL\", I would update my final rating.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good natural algorithm but significance and understanding of when to apply is unclear.",
            "review": "Summary:\n\nThis paper proposes a self-imitation learning technique which modifies GAIL such that top-k trajectories with high reward found by the agent are kept in a buffer (and updated as learning goes on) such that the discriminator tries to distinguish between trajectories generated by the generator and those in the buffer while the generator tries to fool the discriminator by trying to imitate trajectories present in the buffer. \n\nExperiments are shown on two domains: 1. a simple 2D domain where the agent must avoid orange circles (negative reward) and touch green and blue circles which yield positive reward and 2. on MuJoCo against PPO and variants of PPO as baselines where it is shown that GASIL performs better (even under increased stochastic noise in the dynamics.)\n\n\nComments:\n\n- Generally well-written and easy to understand. Thanks!!\n\n- Intuitive algorithm and good experiments with ablation studies on MuJoCo.  \n\n- My main concern is that the paper while offering a good self-imitation algorithm fails to really shine light on when/why this is expected to work. Especially in the following natural areas:\n\na. Why is it that performance decreases as buffer size B increases?\nb. Why doesn't the policy get stuck imitating the first few good trajectories? the conjecture offered is that policy gradient strongly encourages greedy myopic behavior while GASIL does not. Wouldn't one expect GASIL to suffer more?\nc. Does GASIL work better on rich observation spaces (e.g. Atari games) as well?\n\nWithout good answers (theoretical or empirical) to the above questions it is a bit hard to assess how significant of an improvement GASIL actually is and what is the prescription for using this over non-GAIL style self-imitation learning algorithms?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "combination of GAIL and self-imitation learning, but not convincing",
            "review": "This paper presents an incremental extension to the Self-imitation paper by Oh, Junhyuk, et al. The previous paper combined self-imitation learning with actor-critic methods, and this paper directly integrates the idea into the generative adversarial imitation learning framework.\n\nI think the idea is interesting, but there remains some issues very unclear to me. In the algorithms, when updating the good trajectory buffer, it is said \"We define ‘good trajectories’ as any trajectories whose the discounted sum of rewards are higher than that of the policy\". What does \"that of the policy\" mean? How do you know the reward of the policy?\n\nSecond, without defining good trajectories, I don't think Algorithm 1 would work. Algorithm ` 1 misses the part of how to update buffer B. After introducing their own algorithm, the author did not provide much solid proof or analysis for why this self-imitation learning works.\n\nIn the experiment section, the author implemented GASIL for various applications and presented reasonable results and compared them with other methods. Nevertheless, without theoretical proof, it is hardly convincing that the results could be consistently reproduced instead of being merely accidental for some applications.\n\nUpdate:\nThe rebuttal resolves some of my concerns. However, I still think the contribution is incremental. The current version looks too heuristic, more theoretical analysis or inspirations need to be added.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}