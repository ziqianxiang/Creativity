{
    "Decision": "",
    "Reviews": [
        {
            "title": "Exploration using Distributional RL and UCB",
            "review": "Summary:\n========\nThe paper presents an RL method to manage exploration-exploitation trade-offs via UCB\ntechniques. The main idea appears to be that quantile regression can be used to construct\ntighter upper confidence bounds which give rise to better performance. The authors\ndemonstrate their method on a synthetic multi-armed bandit problem and the Atari games.\n\nI don't think the methodological and technical contributions are significant enough to\nwarrant acceptance. Moreover, the presentation of the paper needs to be improved.\n\n\n\nDetailed Comments\n=================\n\nI see two main issues with the proposed method.\n1. First, the fact that the authors are attempting to recover the entire pdfs of the\nreward distributions - the sample complexity for estimating pdfs (or quantilies) is\nsignificantly more than what is required to recover the optimal arm. In that sense, the\nproposed method runs counter to theoretical work in the MAB literature.\n2. Once you estimate the quantiles/pdfs, there really is no reason to use the\n \"mean + constant * std\" form for the UCB anymore - you can directly use the pdf to\n discard a low probability region.\n\n\nMany design choices are made in a very ad hoc manner with only (if any) speculative\njustification. Some examples,\n- Many of the statements in the para starting \"In the case of UCB type ...\"\n- The quantity \\sigma^2_+ and in particular, the use of the median and not the mean\n\n\nOther:\n- Page 1: In the RL setting, an arm corresponds to a state-action pair ... : this\n  statement needs justification. The naive way to treat n RL problem as a MAB problem is\n  to treat all sequences of actions as an arm.\n- If you are repeating algorithms for other work (e.g. Algorithm 1), include the citation\n  there.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting results, but somewhat lacking in novelty",
            "review": "The authors propose a method to use the distribution learned by Quantile Regression (QR) DQN (Dabney et al.) for exploration. This is accomplished by computing an estimate of the variance from the QR value function and using this as an exploration bonus for generating actions, in place of the usual epsilon-greedy strategy.\n\nThe novelty of this method is somewhat low and a few parts of the algorithm and their uses are unclear. From the text of the paper it seems like the authors are using this exploration bonus for action selection and the update “remains the same”. However in Algorithm 3 the update for the value function has been modified to include this bonus in the Bellman update (line 2).\n\nIt is also somewhat unclear that this is the right approach to take for exploration, and the authors do not discuss this fact. QR-DQN provides an update for which the learned distribution will converge towards the inherent distribution of values due to the noise in the environment. However, there is no guarantee (e.g. with high probability) that the intermediate values will upper bound the true value. Further, it is possible that this leads to an agent that becomes “addicted to noise”. IE since this should converge to the true noise of the system an agent may continue to revisit states that are “uncertain” only due to irreducible noise from the environment.\n\nFinally, the authors provide a somewhat odd presentation of UCB-style algorithms. There is an odd distinction made between the “optimism in the face of uncertainty” approach and the optimistic setting of Sutton et al. These algorithms differ, but are fundamentally related via the optimism line of reasoning. Further, approaches based on optimism in this way go back to Lai and Robbins (1985) and are not necessarily based on Hoeffdings Inequality, but rather the more general idea of a high-probability bound on the value. Finally, the authors also give great weight to asymmetry in their introduction of the variance, however this overlooks the frequent use of so-called “one sided” bounds.\n\nOverall, this approach does have some interesting results, however it is a relatively simple modification of QR-DQN, and it is unclear that it is more generally applicable. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The reviewer does not understand why equation 2 solves the exploration-exploitation tradeoff.",
            "review": "This paper proposes new algorithms (QUCB and QUCB+) to handle the exploration exploitation tradeoff in Multi-Armed Bandits and more generally in Reinforcement Learning. The first idea of the paper is to use an Upper Confidence Bound based on the variance estimation rather than an exploration factor based on a concentration inequality such as Hoeffding inequality. The variance estimation is done using Quantile Regression. The second idea of the paper is to adapt the proposed algorithm, QUCB, to the case of asymmetrical distributions. \\sigma+ is defined as the standard deviation of the quantiles higher than the median,. \\sigma+ isused in place of \\sigma in QUCB. Synthetic experiments are done for MAB, and Atari game are used to test DQN-UCB+ for RL.\n\nMajor concern:\n\nThe authors claim that the main advantage of QUCB is that no counter is needed to evaluate the Upper Confidence Bound, and therefore that QUCB can be used in RL where the number of state-action pairs is too high to be stored. Indeed, QUCB does not use counts on the number of times each action has been selected. However, the upper confidence bound that is used in QUCB (see eq 2, line 5 of algorithm 2) is not optimistic in face of uncertainty but reckless: the arm, that is selected, is the arm with the highest mean plus standard deviation.  The reviewer understands that to obtain an accurate estimate of means, the arms with high variance need to be sampled more than the arms with low variance, as in UCB-V.  The reviewer does not understand why equation 2 solves the exploration-exploitation tradeoff. No theoretical analysis of the algorithm is given. No convincing arguments are provided. At a minimum, \n1/ the authors have to explain why it could work. May be based on Chebyshev's inequality or Follow the Perturbed Leader ?\n2/ QUCB has to be compared with UCB (Auer et al 2002) in the experiments.\n3/ The variance and/or the distribution of arms have to be different in the experiment. Notably it could be interesting to launch an experiment where the best arm has a low variance while the worst arm has an high variance.\n\n\nMinor concerns:\n\nTheorem 1 is not used in QUCB, so I suggest removing it.\nq_j is not defined and not initialized in algorithm 1 and 3. \nc_t is not defined in algorithm 3. \nThe right reference for UCB is Finite Time Analysis of the Multi-Armed Bandit Problem, P. Auer, N. Cesa-Bianchi, P. Fischer, 2002.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}