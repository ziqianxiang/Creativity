{
    "Decision": {
        "metareview": "There reviewers unanimously recommend rejecting this paper and, although I believe the submission is close to something that should be accepted, I concur with their recommendation.\n\nThis paper should be improved and published elsewhere, but the improvements needed are too extensive to justify accepting it in this conference. I believe the authors are studying a very promising algorithm and it is irrelevant that the algorithm is a relatively obvious one. Ideally, the contribution would be a clear experimental investigation of the utility of this algorithm in realistic conditions. Unfortunately, the existing experiments are not quite there.\n\nI agree with reviewer 2 that the method is not particularly novel. However, I disagree that this is a problem, so it was not a factor in my decision. Novelty can be overrated and it would be fine if the experiments were sufficiently insightful and comprehensive.\n\nI believe experiments that train for a single epoch on the reduced dataset are absolutely essential in order to understand the potential usefulness of the algorithm. Although it would of course be better, I do not think it is necessary to find datasets traditionally trained in a single pass. You can do single epoch training on other datasets even though it will likely degrade the final validation error reached. This is the type of small scale experiment the paper should include, additional apples-to-apples baselines just need to be added. Also, there are many large language modeling datasets where it is reasonable to make only a single pass over the training set. The goal should be to simulate, as closely as is possible, the sort of conditions that would actually justify using the algorithm in practice.\n\nAnother issue with the experimental protocol is that, when claiming a potential speedup, one must tune the baseline to get a particular result in the fewest steps. Most baselines get tuned to produce the best final validation error given a fixed number of steps. But when studying training speed, we should fix a competitive goal error rate and then tune for speed. Careful attention to these experimental protocol issues would be important.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting task, but clear consensus among reviewers to reject this paper based on limited experiments"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "General:\nThe paper proposed an algorithm named Select Via Proxy(SVP), which can be used for data sampling. The idea is simple and straightforward: 1) use a proxy model to get decision boundary 2) train the large target model on the data points close to the decision boundary.\n\nStrength:\n1. Roughly this is a well-written paper. The main idea is quite clear to me.\n2. Empirical validation of the experiments looks good. The results show that SVP help reduce the training time with ResNet. The author(s) also showed the influence of different quantifying uncertainty methods.\n\nPossible Improvements:\n1. In Related Work, several previous works were mentioned. Although the author(s) claimed that SVP can be combined with them, it's better to show the performance of SVP compared with them. This would show the significance of the work.\n2. In the experiments, I was hoping to see how well SVP works on ImageNet. The problem is that: For ResNet152 and ResNet164, they are relatively too deep on such small data sets. Since the dimension of the data points(images) is not high, SVP can easily catch a reasonable decision boundary with a smaller model. I am almost sure ResNet20 is good enough to do this. I am more concerned about the situation where the capacity of the model is challenged by the size of the dataset. e.g. The data sets of autonomous driving are usually extremely large and even very deep models cannot be fully trained on that. \n3. The data points close to the decision boundary can be considered as tough data points, whose features might be hard to be caught by the model. If training model only on these data points, the trained model may just memorize tough data points and not learn the other data points from the data set. One solution is that, while training on tough data points, the model should also be trained on a small portion of well-learned data points. I don't think training only on the points close to the decision boundary is enough and was more expecting to see some discussion about this in the paper.\n\nConclusion:\nMy two biggest concerns are: 1) The algorithm is not tested on large data seta 2) The algorithm is not tested with the models of limited capacity. As a conclusion, I tend to vote for rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to data selection, but needs comparative experiments",
            "review": "# Summary\nThe paper presents a method for identifying and selecting the most informative subset of the training dataset in order to reduce training time while maintaining test accuracy. The method consists of training a proxy model that is smaller and has been trained for fewer epochs, and which can optionally be ensembled. Experiments show promising results, indicating that some datasets can be reduced to half the size without impacting model performance.\n\n# Quality\nThe paper appears sound and of good quality. Background literature is cited and the proposed method is discussed in sufficient detail.\n\nI would, however, like to see some additional comparative experiments. All experiments are constructed to show that the method can indeed achieve accuracy comparable to the full model but with a smaller training set. I would like to see how it compares to existing strategies -- are there any reason to pick this method over existing ones?\nSince the last sentence in section 2 states that the proposed method is orthogonal to previous subsampling techniques, and therefore can be combined with any of them, it would be interesting to see how SVP compares to these and whether a combination of, say, SVP and importance sampling will in fact achieve better performance than the importance sampling on its own.\nAdditionally, given the model's high resemblance to active learning, it would be interesting to see it compared to some prominent active learning methods.\n\n# Clarity\nThe paper reads quite well. I particularly like the paragraph headlines, which makes it easy to get an overview of the paper.\n\nThe figures are generally nice and readable, except for figure 3, which I don't understand. Maybe I am missing it, but I can't find an explanation for what the rows and columns indicate, and the labels themselves should also be increased in size.\n\n# Originality\nI do not find the paper particularly novel. To me, the proposed method seems to be a variant of active learning, not orthogonal to this as it is claimed in section 2. The choice of surrogate model and uncertainty metric might be new, but the method itself boils down to uncertainty sampling, a well-known strategy in active learning.\nHowever, I am happy to change my mind if the authors can explain to me exactly how their method differs from active learning.\n\n# Significance\nWhile techniques for speeding up training without sacrificing performance are, of course, always interesting, I find the proposed method to be rather incremental and not significant enough for ICLR. It would be better suited as a workshop paper.\n\n# Other notes\nIn the last paragraph of section 1, you write that \"Our proposed framework is robust to the choice of proxy model architecture.\" I am not sure what you mean by this. Do you mean that one can choose any model as the proxy (which is clearly correct) or do you mean that the method is \"proxy agnostic\" in the sense that any proxy model will work better than no proxy? If the latter is the case, I would like some arguments for this. Also, if the method is indeed proxy agnostic, it should be possible to remove the proxy completely and select the data in some other way.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "meaningful work, but lacks more supporting evidences",
            "review": "This paper studies a very simple and intuitive method to boost the training speed of deep neural networks. The authors first train some light weighted proxy models, using these models to rank the data according to its uncertainty, and then pick the most uncertain subset to train the final model. Experiments on CIFAR10/SVHN/Amazon Review Polarity demonstrates the effectiveness.\n\nIn general, I think the authors did a decent job in showing that such a simple idea could surprisingly work well to boost NN training. I believe it will inspire future works on speeding up NN training. However, to form a solid ICLR publication, plenty of future works need to be done.\n\n1)\tI will not be fully convinced if an idea aiming to speed up, is only verified on small scale dataset (e.g., CIFAR10). It will be much better if there are large scale experiments conducted such as on ImageNet and WMT neural machine translation. \n\n2)\tPlease well position some related works. First, it would be more interesting and informative if some baselines in section 2 (especially those in “Optimization and Importance Sampling’), are compared with. Second, there are important related works omitted such as L2T [1], which also talks/shows the possibly of using partial training data to achieve speed up.\n\n3)\tSome writing issues: it would be better to *clearly* demonstrate the final accuracy of different models (i.e. ResNet 164 trained on whole data and selected subset), such as putting them into a table, but not merely showing them vaguely in the curves and text. I’m also note sure about the meaning of `epoch’ in Table 1: does it mean how many epochs the proxy model is trained? If so, I can hardly get the intuition of why smaller epochs works better. I noted a conjecture raised by the authors in the last sentence of paragraph “comparing different proxies”. However, I cannot catch the exact meaning. \n\n[1] Fan, Y., Tian, F., Qin, T., Li, X. Y., & Liu, T. Y. Learning to Teach. ICLR 2018\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}