{
    "Decision": {
        "metareview": "The paper is a premature submission that needs significant improvement in terms of conceptual, theoretical, and empirical aspects.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "A technical report rather than a research paper",
            "review": "General:\nIn general, this looks like a technical report rather than a research paper to me. Most parts of the paper are about the empirical analysis of adaptive algorithms and hyper-gradient methods. The contribution of the paper itself is not sufficient to be accepted.\n\nPossible Improvements:\n1. The study of such optimization problem should consider incorporating mathematics analysis with necessary proof. e.g. show the convergence rate under specific constraints. Even the paper is based on others' work, the author(s) could have extended their work by giving stronger theory analysis or experiment results.\n2. Since this is an experimental-based paper, besides CIFAR10 and MNIST data sets, the result would be more convincing if the experiments were also done on ImageNet(probably should also try deeper neural networks).\n3. The sensitivity study is interesting but the experiment results are not very meaningful to me. It would be better if the author(s) gave a more detailed analysis.\n4. The paper could be more consistent. i.e. emphasize the contribution of your own work and be more logical. I might miss something, but I feel quite confused about what is the main idea after reading the paper. \n\nConclusion:\nI believe the paper has not reached the standard of ICLR. Although we need such paper to provide analysis towards existing methods, the paper itself is not strong enough.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "incremental empirical contribution",
            "review": "Clarity: Below average\n- The introduction would be easier to follow if you named Baydin's approach and your own approach, because in the 2-4 bullet points you say \"this online scheme\", and \"the learning rate schedule\", without being perfectly clear what you are talking about\n- The last sentence of the introduction is meant to clearly state your hypothesis, so I was expecting \"emphasize the value of *\", i.e. either adaptive or non-adaptive methods, rather than just general 'tuning', which is self-apparently important.\n\nQuality: Below average\nThis is a purely empirical study that does not go too deep. It is not quite a review paper, but only compares previous methods.\n\nPros:\nI especially appreciate the sensitivity analysis, ie Fig 6. If only all ML papers had something like this to suggest the difficulty of setting hyperparameters for their proposed methods.\n\nCons:\n- You should use mathematics to describe what you are talking about with adaptive stepsize in Sec 2.1. \"these methods multiply the gradient with a matrix\". Just giving one equation would be extremely helpful.\n- If I understand correctly, you are interpreting the inverse-Hessian as used in Newton's method and other non-diagonal 'gradient conditioners' as types of stepsize. This is definitely interesting, but again it would be very simple to see what you are saying with an equation instead of starting with the phrase \"stepsize\" which is generally understood to be a scalar multiple on the gradient.\n- I'm surprised you jump right into experiements after your background settings. It's apparent that this paper fundamentally relies on the Wilson (2017) hypergradient paper. Your paper should be more self-contained: 'hypergradient' is not even defined in this paper, is it?...\n\nEspecially:\nHow do you know that if you change the model architecture, data, and loss, that a similar result will occur? I imagine that it heavily relies on the data and model-- in other words, that the sensitivity is dependent on \"how an algorithm reacts to a certain data/loss/model landscape\". I'm trying to say that I'm not convinced these results generalize to any other situation than the one presented here (so does it really say anything about the different stepsize selection rules?)\n\nRandom side note:\nSince your appendix is only a few lines, you could consider succinctly listing learning rates with set notation, for example {1e-n,5e-n : -5<n<1}.",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An emperical study on several methods for adjusting learning rate ",
            "review": "The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). \n\nThough it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. \n  \nOn page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?\n\nThe URL in References looks out of bound. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}