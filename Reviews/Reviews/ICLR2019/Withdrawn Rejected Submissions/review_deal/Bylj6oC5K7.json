{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting approach, not sure how significant.",
            "review": "The paper studies how logit regularization techniques can affect adversarial robustness. The authors study the recently proposed technique of adversarial logit pairing (ALP) and attempt to understand the different effects it has on the resulting model and how each of them can affect robustness. They argue that a significant part of ALP's success stems from regularizing the logits and that alternative logit regularization methods can lead to similar robustness. Finally, they propose an alternative to ALP that explicitly disentangles logit similarity and logit regularization to train models that are claimed to be more robust than state-of-the-art models.\n\nBefore continuing with my review, I would like to note that the effectiveness of ALP in increasing robustness is still being debated. The robustness of ALP models has not been verified by a third party. The models released by the authors were shortly found to vulnerable to simply running a standard attack for more steps (https://arxiv.org/abs/1807.10272). The ALP authors claim that this is due to releasing a smaller model for a competition but they still have not released another model that is claimed to be robust. Nevertheless, I believe that this situation should not affect how the current paper is reviewed. The idea of regularizing the logits is natural and precisely understanding its effect on the model is a valuable research direction.\n\nOverall I like the idea of the paper. I agree that the effect of logit regularization methods on the model is unclear and attempting to dissect these techniques is a very important research direction. On the other hand, this is a very tricky area to perform such experiments. Accurately measuring the robustness of a model is a challenging task. It is therefore hard to understand if an increase in adversarial accuracy is due to a true increase in the robustness of a model or due to the attack being used performing sub-optimally when the setting is modified.\n\nMost of the results of the paper revolve around rather fine-grained differences in the adversarial accuracy of <5% (e.g. Figure 1, Table 1). Such small differences might in fact be artifacts of the PGD attack used performing sub-optimally. Moreover, for some of the results, it is clear that they are indeed an artifact of the attacks performing sub-optimally. For instance the left of Figure 2, label smoothing is reported to achieve high accuracy again PGD yet in Section 5.3 it is reported that more steps can reduce the accuracy to <15%. It is thus clear that the claim about label smoothing increasing robustness is misleading.\n\nI believe it is essential that the authors validate theirs results, at the very least Table 1, by evaluating the models on different attacks.\nGiven that the models regularizes the logits, evaluating an attack optimizing the logits directly (e.g. the CW attack, https://arxiv.org/abs/1608.04644) is essential. Moreover, given that the gradients of the loss appear to not be informative, an attack based on finite differences is also crucial here (see SPSA attack of https://arxiv.org/abs/1802.05666).\n\nMoreover, I find the reasoning behind why should logit regularization techniques increase robustness very limited. The change in logit distributions (Figure 1,2) are rather marginal and do not suggest fundamental differences of the underlying models.\n\nI believe that additional experiments and justifications of the results are needed before the paper is considered for acceptance. I thus recommend rejection at this time.\n\n[UPDATE after reading the public discussion]: It appears the someone already suggested running SPSA as an attack. I really appreciate the authors evaluating the attack and reporting their findings. Indeed it seems that the reported robustness from logit regularization does not stand up to an SPSA attack. As a result, I think it is fairly clear that logit regularization has a limited effect on the robustness of the model and is only making PGD attacks less effective. I update my score to a clear reject.\n\nMinor comments to authors:\n-- I would suggest remove the sentence close to the end of the intro about ALP being perhaps the most robust defense. As I outlined above, the robustness of ALP has not been convincingly verified yet. Similarly for the text above equation 2. Perhaps most importantly, the last paragraph before Section 4 where you state that \"...ALP _works_...\" in a very definitive tone.\n-- Paragraph after (5): the logits of the other classes are not guaranteed to increase. Some of them might decrease in the process of making the loss as large as possible.\n-- I like equation (10) where you attempt to decompose that effect of similarity and regularization. It would be interesting to understand the interplay between lambda and beta.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice observation and result on CIAFR10. Would like to see how it performs on CIAFR100.",
            "review": "This paper studies adversarial training of robust classification models. It starts off by analyzing ALP method, and hypothesizes that smaller logits may bring more robustness. Based on this observation, the authors experiment with several logit shrinking method including label smoothing and mixup. The authors further propose a variant loss (Eq. 10) of traditional ALP. Experiments on CIFAR10 demonstrate the observations and the proposed method LRM.\n\n0. My overall evaluation: This paper is well-motivated and the result is good. But it lacks novelty. And more experiments will definitely make it stronger. \n\n1. The Eq. (2) slightly differs from Kannan's ALP paper. They use half clean and half adversarial images in a batch, and here, you use both clean and adversarial images in a batch. Personally I think using both may lead to more robust models. Also, in Table 1, how do you implement ALP? Setting \\alpha = 0? Then this is basically adding one regularization term on top of Madry's loss function.\n\n2. When you expand the square and get Eq. 10, this can also illustrates that: 1) minimize logits 2). minimize a distance measure between clean and adversarial images. Also, when it comes to Eq. 10, some parenthesis typos?\n\n3. For Table 1, I would like to see PGD20 (iterations) + 2 (step size in pixels), PGD100 + 2 and PGD200 + 2. Also, I am interested in seeing CW loss which is based on logit margin. My best guess is that PGD, ALP, LRM may not go down for large iterations, because they all depend on adversarial examples (e.g., PGD7) during training. But I am not sure if the gain will be still there. Also, I think label smoothing may go down significantly.\n\n4. I would like to see results using the \"wide\" model in [madry17] paper for ALP and LRM. I think results from large-capacity models are more convincing.\n\n5. I would like to see results on CIFAR100, which is a harder dataset, 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for justification nowadays (maybe enough one year ago). It is possible that ALP or logit regularization-based method work only on simple dataset and simple decision boundary (10 classes). Since ImageNet is,  to some extent, computationally impossible for schools, I want to see the justification results on CIFAR100.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Unconvincing",
            "review": "The authors examine the effect of logit regularization techniques on the robustness of neural network models to adversarial examples, under the threat model of an L-infinity attacker. Logit pairing is a recently proposed defense technique, which adds an L-2 loss penalizing the difference of the logits on the correct class between the clean and adversarial example. The authors of this paper argue that logit pairing effect tends to make the distribution of clean logits smaller and adversarial logits larger. Thus this can be thought of as regularizing the logits (similar to weight decay) and the authors argue that some other recently proposed measures of defense (like label smoothing, paired example data augmentation etc) can also be understood through the lens of logit regularization. From this insight, the authors propose a new defense: by separating the difference of logit terms from the original ALP loss using (a-b)^2 = a^2 + b^2 - 2ab, they can now vary the effect of logit regularization by having different multipliers on the logit norms, and claim to have more robust models on CIFAR-10.\n\nMy main concern with this paper is that it is basing it's arguments on defenses that are well known to cause gradient masking/obfuscation. See this paper https://arxiv.org/abs/1807.10272 for an illustration of how ALP just makes the optimization landscape harder, but it is not inherently more robust. Increasing the number of PGD steps with random restarts reduces model accuracy from the claimed 27.9% to 0.6% which is worse than a PGD trained model (which gets 1.5%) on Imagenet. Therefore, one can conclude that the ALP defense is broken and I suspect that the gain from the logit regularization method that the current authors report will not stand up to scrutiny - in particular the authors only report numbers for 20 step PGD and show better numbers compared to PGD, but it is likely to be broken on increasing the number of iterations. Given the large number of questionable defense papers being accepted to deep learning conferences which are then immediately broken following publication, I think it would serve the community to reject such papers, thereby reducing the noise in this research area.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}