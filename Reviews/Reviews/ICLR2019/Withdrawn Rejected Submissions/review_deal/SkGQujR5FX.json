{
    "Decision": {
        "metareview": "The paper needs more revisions and better presentation of empirical study.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Good paper but needs more revisions"
    },
    "Reviews": [
        {
            "title": "Incremental improvement to ASGD approaches at mid-size scaling",
            "review": "Paper offers an improvement to existing approaches using momentum with SGD for asynchronous training across a distributed worker pool. The key value in the proposal seems to be that it works \"out-of-the-box\" and requires no new parameters to be tuned, while delivering similar final accuracy as other distributed methods.\n\nThe authors begin with an explanation of ASGD training, why it doesn't scale - worker lags that lead to gap in parameter that gradients are computed on (worker parameters) vs parameters applied (master parameters). It also discusses the kind of momentum approaches that are in use today and how it helps and hurts. \n\nThe new proposal in this paper is DANA that builds on Nesterov Momentum to reduce the lag between these two sets of parameters by predicting the parameters that should be used for computing gradients at each worker.\n\nPros:\n- A key issue with most optimization methods is the number of hyperparameters to tune. DANA is \"out-of-the-box\" in that it doesn't introduce any new hyperparameters thus making it easy to scale the training of any model.\n\nCons:\n- The sweetspot for DANA seems to be between 8-24 workers. In practice these days it is pretty easy to run synchronous SGD for these sizes with a setup of 8 GPUs per machine with a few machines. The tuning of learning rate as a hyperparameter is required anyway, and keeping training synchronous doesn't really change that. The only issue is if one often changes number of workers for training, which isn't typical.\n- ASGD is useful for a larger number of workers as it is harder to train with SSGD for those because of the additional synchronization overhead. That is one area though where DANA starts to have worse behavior than other ASGD approaches.\n\nComments:\n- Paper assumes block-random scheduling for simulation, however in practice it is quite common to have a few workers that are consistently slower. How does this kind of bias effect their methods?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In general, a good paper addressing an important issue with distributed deep learning training, i.e., the gradient staleness vs parallel performance (speedup).",
            "review": "The paper addresses an important problem in distributed training of deep learning models, i.e., the gradient staleness vs the parallel performance. Keeping the gradient up-to-date in distributed training is important in order to achieve a low test error and high accuracy, but that comes at a cost: the overhead of more communication and synchronization. Asynchronous methods to update the gradient have been proposed, but they usually suffer from staleness, i.e., the communication latency between the master and the slaves impacts the accuracy and training time since the accumulated gradient already is \"old\" in relation to the model parameters when distributed to the slaves. \n\nThe paper proposes an approach to estimate the future model parameters at the slaves using Bengio-Nesterov momentum, thus reducing the effects of the communication latency (the gap) between the master and the slaves when collecting and distributing the gradient. The novelty is mainly on the application and implementation side of the spectrum, and not so much theoretical novelty. The contribution is relatively incremental, but important and clear. Reducing training times with maintained accuracy is an important practical problem, and we need all kinds of measures to address that.\n\nThe evaluation seems solid and the results are very promising. The comparison is done with relevant \"competitors\" (e.g., both synchronous and asynchronous approaches for distributed training). However, since the goal of distributed learning is improved execution performance, I would have liked to see more performance numbers. \n\nMinor:\n* Page 7, top paragraph. It's written that Table 2 shows that DANA easily scales to 32 workers. That information is not shown in Table 2... You don'r show any execution time / speedup numbers at all for ImageNet input. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but evaluation constraints are limiting",
            "review": "# overview\nIn this work, Nesterov Accelerated gradient based updates are applied in a distributed fashion to scale SGD based training to multiple nodes without the introduction of further hyperparameters or having to adapt the learning rate schedule from that of single node training on the same data.\n\nEvaluation is carried out on image classification workloads using ResNet model variants across CIFAR10,100, and ImageNet datasets, utilizing from 8-32 nodes. In contrasting test error relative to single node performance, the authors find their method degrades less than other synchronous and asynchronous SGD based approaches as node count increases.\n\nOverall, this work is presented in a fairly clear and logical manner, and the writing is easy to follow.  However the approach described appears to be contingent on very specific worker communication patterns and timing which seem unrealistic for real-world settings (namely that each worker sends exactly one update per N sized block received).  Extrapolating from the curvature of the results shown it doesn't appear that DANA would continue to outperform other methods like ASGD once the worker count scales beyond the 32 node limit evaluated.\n\n# pros\n* no additional hyperparameter tuning required\n* should be easy to drop into existing asynchronous SGD implementations, just need to modify the worker side.\n* does appear to scale slightly better from an accuracy perspective in 16-32 node counts\n\n# cons\n* Biggest criticism is the assumption of block random or round-robin worker update scheduling. Presuming each worker will update master exactly once to determine future parameter position is far from realistic on real hardware (varying capacity, performance, system loads, dealing with stragglers) and should probably be considered a synchronous not asynchronous update.\n* only evaluated on image classification tasks on cifar10, cifar100, imagenet on resnet-20 and resnet-50. Would have been better to evaluate on a more varied set of tasks/models/datasets\n\n# other comments\n* Figure 2 baseline performance reported is a bit misleading/confusing since it was only evaluated on a single worker. Would suggest restricting to a single point rather than some extrapolated line that seems to indicate being run on multiple-workers.\n* Figure 3 should should also show multi-node speedups for the other methods compared for completeness. \n* Section 5.2 should report on percentage scaling efficiency rather than using speedup as it doesn't normalize for worker count.  For instance 16x could be interpreted as good or poor if it was achieved using 16 vs 160 nodes.\n* Section 5.2 there's a small typo: GPUs -> GPU\n* Consider https://arxiv.org/abs/1705.07176 in related work?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}