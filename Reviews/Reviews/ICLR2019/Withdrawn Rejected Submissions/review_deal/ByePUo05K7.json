{
    "Decision": {
        "metareview": "This paper claims to demonstrate that CNNs, unlike human vision, do not have a bias towards reliance on shape for object recognition. Both AnonReviewer1 and AnonReviewer2 point to fundamental flaws in the paper's argument, which the rebuttal fails to resolve. (AnonReviewer1's criticisms are unfortunately conflated with AnonReviewer1's reluctance to view neuroscience or biological vision as an appropriate topic for ICLR; nonetheless AnonReviewer1's technical criticism stands).\n\nThese observations are:\n\nAnonReviewer2:\n\n\"Authors have carefully designed a set of experiments which shows CNNs will [overfit] to non-shape features that they added to training images. However, this outcome is not surprising.\"\n\nAnonReviewer1:\n\n\"The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias\"\n\n\"The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented.\"\n\nThe AC agrees with both of these observations. CNN behavior is partially a product of the training regime. To examine the scientific question of whether CNNs have similar biases as human vision, the training regimes should be similar. Conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias.\n\nThis paper appears fundamentally flawed in its approach. The results are not informative about differences between human vision and CNNs, nor are they surprising to machine learning practitioners.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview: fundamentally flawed approach"
    },
    "Reviews": [
        {
            "title": "This paper addresses an important issue but fails to propose a solution",
            "review": "Humans leverage shape information to recognize objects. Shape prior information helps human object recognition ability to generalize well to different scenarios. This paper aims to highlight the fact that CNNs will not necessarily learn to recognize objects based on their shape. Authors modified training images by changing a value of a pixel where its location is correlated with object category or by adding noise-like (additive or Salt-and-pepper) masks to training images. Parameters of such noise-like masks are correlated with object category. In other words if one learns noise parameters or location of altered pixel for each object category, they can categorize all images in the training set. This paper shows that CNNs will overfeat to these noise based features and fail to correctly classify images at test time when these noise based features are changed or not added to the test images.  \n\nDataset bias is a very important factor in designing a dataset (Torralba et al,. 2011). Consider the case where we have a dataset of birds and cats. The task is image classification. All birds' images have the same background which is different than cats' background. As a result the network that is trained on these images will learn to categorize training images based on their background. Because extracting object based features such as shape of a bird and bird's texture is more difficult than extracting background features which is the same for all training images. \n\nAuthors have carefully designed a set of experiments which shows CNNs will overfeat to non-shape features that they added to training images. However, this outcome is not surprising. Similar to dataset design example, if you add a noise pattern correlated with object categories to training images, you are adding a significant bias to your dataset. As a result networks that are trained on this dataset will overfeat to these noise patterns. Because it is easier to extract these noise parameters  than to extract object based features which are different for each image due to different viewpoints or illumination and so on. \n\nThis paper would have been a stronger paper if authors had suggested mechanisms or solutions which could have reduced dataset bias or geared CNNs towards extracting shape like features.  \n\nAntonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '11).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Shape-bias or shortcut-bias and catastrophic forgetting?",
            "review": "The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision – they don’t encode shape-bias like human vision. Towards this goal, the authors modified the training data with ‘shortcut’ features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task.\n\nInvestigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. \n\nThe paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting.\n\nNonetheless, there are significant drawbacks in the presented work:\n\n1.\tThe experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Let’s make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.)\n\nThe best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. \n\n2.\tDue to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so:\n\na.\tWe tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on ‘nopix’ case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed.\n\nDetails: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: https://file.io/qiziAK. The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-à-vis the results in the paper. The ‘same’ setting didn’t achieve 100% either though it got close - achieving 98.4%. \n\nSummary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality.\n\nI encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including ‘shape’-features and submit to a future conference.\n\nRevision: Updated my rating to acknowledge that the reproducibility issue is addressed.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "clever experiments with interesting implications",
            "review": "This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. \n\nThe hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs.\n\nIn a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize \"shape\" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images).\n\nAdditional analysis involving visualization methods to further explain why shape features were ignored would have been a plus– with bonus points for providing a heuristic to determine the \"shapelessness\" of a convolution kernel.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}