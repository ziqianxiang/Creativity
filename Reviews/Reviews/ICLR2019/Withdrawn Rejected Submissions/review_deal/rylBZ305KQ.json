{
    "Decision": "",
    "Reviews": [
        {
            "title": "Little awareness to previous attempts to dynamic language models",
            "review": "This paper aims to improve sequential prediction using LSTM by incorporating\nanother latent variables {z_t} that follow distinct dynamics rather than LSTM\nitself. \nExperimental results on comparing with naive LSTM and dynamic word embedding\napproaches confirmed that the proposed method works better than these \nbaselines.\n\nBasically the proposed method is sensible, but this paper has two clear\ndrawbacks: (1) it is not compared with similar previous attempts; and (2)\nthe proposed architecture is not guaranteed to represent \"global\" contextual\nbehavior rather than local contextual one.\n\n(1) There have been numerous attempts in sequential modeling, especially\nlanguage models, to incorporate a global and contextual factor into prediction.\nEarly attempts used latent dirichlet allocation combined with n-gram models;\nafterwords, LSTM became prevalent, and in fact there exists a famous baseline\nof Latent LSTM allocation [1]. Therefore, it is minimally required for this\npaper to compare the proposed model with latent LSTM.\n\n(2) Latent LSTM above has a clear advantage that there is an explicit mechanism\nof latent topic distribution that governs a specific sequence (global variable).\nHowever, the proposed method in this paper, contextual behavior is determined\nonly locally, and might be quite prone to overfit to exceptional observations,\nin this case, words. Of course, it might handle a global context by chance,\nbut the architecture does not guarantee it. Therefore, such a potential\ndisadvantage should be experimentally investigated by a comparison with a\nsuitable baselines noted above.\n\nI agree with the authors that the contexual behavior should be continuous\nand flexible, such as pursued in this paper. Thus a challenge is how to\nsuitably incorporate a global context into such a representation.\nFinally, I would like to know what kind of function g is learned through this\nexperiment. Investigating into the strengths and weaknesses of the learned\nfunction g is also beneficial for a more flexible language models in the \nfuture.\n\n[1] Zaheer, M., Ahmed, A., & Smola, A. J. (2017). Latent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence Data. In International Conference on Machine Learning (pp. 3967-3976).\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A straightforward extension of recurrent neural network language model with stochastic hidden variables. A simplified version of stochastic RNN.",
            "review": "This paper presents a straightforward extension of recurrent neural network language model (RNNLM) by considering the stochastic hidden variables rather than deterministic hidden variables in traditional RNNLM. The motivation is to capture the shift of meaning in language over time based on this stochastic modeling. This RNN model is developed with twofold considerations. One is to represent the latent transition while the other is to perform the reconstruction. Two separate RNNs were run to fulfill the whole model. The key of this paper is the regularization term in Eq. (3) which constrains the transition of two neighboring hidden states.  This paper considers the global latent variables at each time step as well as the transition function over different time steps. The transition of latent variables in two neighboring steps is modeled by a Gaussian distribution with the mean driven by latent variable in former step which is similar to what dynamic topic model was doing.\n\nPros: A simplified version of stochastic RNN which adopts a Gaussian distribution for state transition with neural network based mean parameter. Idea is interesting. Solution is meaningful.\n\nCons: \n1. The stochastic or variational recurrent neural networks have been published before. Citations to these previous works are missing in this paper.\n2. A variant of stochastic RNN with a number of assumptions and simplifications. Two RNNs were used and separately trained. The transition probability from z_{t-1} to z_t is independent of observation x. This is odd.\n3. In variational neural network, there are an encoder and a decoder for recognition and generation. This work is missing such a structure. It is difficult to explain how latent variable is obtained and how new word is predicted.\n4. Although the experimental results show improvement, the justification is still not enough. The comparison with other variational RNN (e.g. the work entitled \"Sequential Neural Models with Stochastic Layers\") is missing. The visualization of latent information is required.\n3. Typo: The sentence in the second line of Conclusion \".. conditioned and ...\" is incomplete.\n\nRemarks:\n1. Deriving a hybrid criterion and joint optimization over the parameters (for encoder and decoder) is suggested. \n2. The interpretation about how the training data are related to the stochastic transition is required.\n3. Suggest to figure out the details of Eq. (3) instead of simply citing Krishnan et al. (2017).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited novelty on the modeling side + lack of motivation on the relevance of the overall problem",
            "review": "The paper proposes to model shifts in language use over time with a recurrent neural network architecture augmented with stochastic variables. Stochastic variables are sampled at the beginning of each time segment and are supposed to capture variability introduced by the time shift. The proposed architecture is mainly compared to approaches that model evolution of “word embeddings” through time and the authors seem, throughout the paper, to contrast their approach to these streams of works. Experimental results show the proposed model architecture outperform RNNs augmented with dynamic word embedding models at test time, in language modeling settings.\n\nThe first drawback of the paper is the limited novelty of the architecture. These types of architectures are well-known and have already been used in a variety of other tasks (Fraccaro et al. https://arxiv.org/pdf/1605.07571.pdf, Serban et al. (https://arxiv.org/pdf/1612.00377.pdf) , etc...). Therefore, the contribution of this paper is to apply this architecture to model shifts in language over time.\n\nFurthermore, it is not clear to me how the proposed model can be used to track evolution of semantic similarities between words over time as well as detecting topical changes. Estimating dynamic word embeddings has the positive aspect of enhancing interpretability by modeling the evolution of emergent “topics” across time. This seems to me one of interesting aspects in modeling time shift in language and, for this reason, it has been previously extensively studied. How would that be possible under the proposed approach ? How to extract the evolution of the semantic field of a specific term across time under the proposed model ? In this aspect, contrasting the current architecture with models based on dynamic word-embeddings harm the paper.\n\nFollowing up on these last considerations, the relevance of the problem tackled in this paper seems a bit weak/unclear to me. Why would it be important to form model of language that are robust across time-spans ? Couldn’t we argue that the same models could be “retrained” on new data as it comes in which would lead to study (language) models that can continually learn in an efficient fashion ?\n\nIn summary, the main points of concern are:\n- the limited novelty on the modeling side\n- the lack of proper motivation on the importance of the addressed problem \n\nMinor remarks:\n\n- Yao et al. (2018) “use alternate optimization that breaks the flow of gradient through time” , unclear as their method can be optimized using stochastic gradient descent, as the Yao et al. point out. Maybe one drawback is parameter efficiency.\n\n- Learning a transition function in the latent space has also been proposed in Fraccaro et al. (https://arxiv.org/pdf/1605.07571.pdf), and should be cited.\n\n- What would these stochastic variables capture ? Why it is a good idea for the current problem ?\n\n- Page 4: DiffDtime -> DiffTime\n\n- How do you compute PPL in your setting ? Is it the upper bound coming from the ELBO ? It would be crucial to see evolution of the KL divergence during time, as a hint of how much information is encoded into the latent variables.\n\n- “recursive inference”, e.g. either finetuning q for a particular x or at test time is related to Salimans et al. (https://arxiv.org/pdf/1410.6460.pdf) and therefore should be cited.\n\n- I cannot really see that “DRLM-F significantly improves long-term performances on NYT”. From the graph, I can see a gap of ~1,1.5 perplexity point from 126 to ~124.5 which doesn’t seem striking to me.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}