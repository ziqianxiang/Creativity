{
    "Decision": "",
    "Reviews": [
        {
            "title": " ",
            "review": "The paper proposes to resolve the issue about a variational auto-encoder ignoring the latent variables. The paper presents a variational auto-encoder with repeated likelihoods, which results in a 1+m factor in front of the log-likelihood term. This paper justifies this model by defining a model as a combination of a variational auto-encoder and a stochastic auto-encoder.\n\nThis paper tries to re-interpret the VAE models with weighting factors such as beta-VAE as a model of repeated likelihoods/views. From the Bayesian modelling perspective, it is a bit problematic as the same observed variables appear multiple times in the model. The model assumes two independent observation of the sam latent variable, however, it is actually given the same observation twice. This introduces a bias in posterior. On the other hand, a weighting factor in front of likelihood is not new. This trick has been used in multi-view learning or imblanced classification as a practical solution to balance the views or the classes.\n\nThe derivations in Section 2 is hard to follow. It is unclear how to Equation 7 is derived from Equation 6.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviews",
            "review": "This paper aims to address the problem that the LSTM decoder of a language model might be too strong to ignore the information from the latent variable. This is a well-known problem and Bowman et al.(2016) proposed to use KL-annealing to help relieve this issue. The proposed solution in this work is to add a stochastic autoencoder to the original VAE model. The experimental results to some extent confirm the advantage of the model. \n\nMy major concerns are \n1. The scope of the model is relative small. The problem to be solved is not the key problem in language modeling. And the KL-annealing trick is a good and cheap solution. Although the performance is relative better, the model is way to complicated than KL-annealing. \n2. The proposed method is somewhat contrived and not well motivated. What is the motivation behind Equation 2? And the assumption of the model, i.e. Eq. 5 and 6, seems too strong. \n\nBased on the above concerns, I will suggest a rejection for the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea on better utilizing latent representations in generative models",
            "review": "This paper presents AutoGen, which combines a generative variational autoencoder (VAE) with a high-fidelity reconstruction model based on autoencoder. The motivation behind AutoGen is to address a common problem when training VAEs with powerful encoders (e.g., autoregressive models) that the model simply ignores the latent representation and does not associate latent representation with the generated data in a meaningful way. A common strategy to (partially) solve this problem is by introducing KL annealing, gradually turning up the KL term in the standard ELBO, which unfortunately means the model is no longer optimizing a valid lower bound on the log-likelihood of the data. AutoGen provides an alternative solution by adding a reconstructing term to the standard ELBO for VAE to enforce the latent representation striking a balance between generation and reconstruction -- the objective still remains a valid lower bound (albeit not on the data log-likelihood) and has close connection to the standard ELBO. It also provides alternative interpretations for some other similar techniques, e.g., beta-VAE and KL-annealing. Experimental results and survey studies demonstrate that AutoGen is able to leverage latent representation more effectively when comparing with VAE without annealing, has better reconstruction overall, but at the same time lose some ability to generate good samples from prior -- this is not too surprising considering the model objective balances between generation and reconstruction.   \n\nOverall the paper is clearly written with some minor issues listed below. The paper presents a simple but reasonable adjustment to the standard VAE training and yields an objective that is intuitive and connects nicely to some other similar techniques that scale the KL term. I have a few concerns about the paper, however, that I hope the authors could better address:\n\n1. My major concern is that VAE after all is a generative model and its ability to sample from prior is an important property. VAE with annealing, admittedly unprincipled, addresses this issue better than AutoGen, especially on shorter generated sentences. There might be cases where reconstruction is important, but the paper did not demonstrate that (this relates to the point 3 below). In the current state, even though the paper presents a simple and intuitive adjustment to the VAE training objective, it hasn't convinced me that if I want a generative model of language I would try to use AutoGen rather than VAE with annealing. \n\n2. As mentioned in the paper, VAE with annealing is an unprincipled approach. It would be interesting to see if AutoGen compares favorably over some other principled approaches along the line of better utilizing the latent representation, e.g., Krishnan et al. On the challenges of learning with inference networks on sparse, high-dimensional data, 2018.\n\n3. I can understand that because of the objective of AutoGen, it does not make much sense comparing held-out likelihood or ELBO between VAE and AutoGen. However, currently the paper is lacking in terms of quantitative evaluation. An interesting experiment would be to use the latent representation z from both AutoGen and VAE (with/without annealing) for some downstream tasks and see if better reconstruction in this case helps. This would also demonstrate the importance of incorporating a reconstruction term in the objective. \n\nMinor: \n\n1. Above equation (5): What exactly do you mean by “symmetric”?\n\n2. Above equation (9): instead of just 2 -> instead of just 1? since m represents the number of reconstructions\n\n3. Also above equation (9): it is worth more elaboration on how to generalize to m reconstructions: it is not L_AutoGen = L_VAE + m * L_SAE (which is what the text seems to suggest), rather it's L_SAE = \\int_z p(x|z)^m p(z|x) dz. \n\n4. Section 3.2, since the model \"finds different reconstructions every time from the same input sentence\", how robust is the reconstruction to the sampling variations? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}