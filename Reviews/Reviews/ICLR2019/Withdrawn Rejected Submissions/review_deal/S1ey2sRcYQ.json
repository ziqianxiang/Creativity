{
    "Decision": {
        "metareview": "The paper presents a novel gradient estimator for optimizing VAEs with discrete latents, that is based on using a Direct Loss Minimization approach (as initially developed for structured prediction) on top of the Gumble-max trick. This is an interesting and original alternative to the use of REINFORCE or Gumble Softmax. The approach is mathematically well detailed, but exposition could be easier to follow if it used a more standard notation. After clarifications by the authors, reviewers agreed that the main theorerm is correct. The proposed method is shown empirically to converge faster than Gumbel-softmax, REBAR, and RELAX baselines in number of epochs. However, as questioned by one reviewer, the proposed method appears to require many more forward passes (evaluations) of the decoder for each example.Â Authors replied by highlighting that an argmax can be more computationally efficient than softmax (in cases when the discrete latent space is structured), and also clarified in the paper their use of an essential computational approximation they make for discrete product spaces. These are important aspects that affect computational complexity. But they do not address the question raised about using significantly more decoder evaluations for each example. A fair comparison for sampling based gradient estimation methods should rest on actual number of decoder evaluations and on resulting timing. The paper currently does not sufficiently discuss the computational complexity of the proposed estimator against alternatives, nor take this essential aspect into account in the empirical comparisons it reports. \nWe encourage the authors to refocus the paper and fully develop and showcase a use case where the approach could yield a clear a computational advantage, like the structured encoder setting they mentioned in the rebuttal.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Original approach, but with unclear computational benefit "
    },
    "Reviews": [
        {
            "title": "Worthwhile and interesting paper, but exposition could use some work (rating maintained after author feedback).",
            "review": "This paper proposes combining the Gumbel-max trick and \"direct loss optimization\" for variance reduction in VAEs with discrete latent variables. This is a natural combination (in hindsight), since the Gumbel-max trick turns sampling into non-differentiable optimization, and direct loss optimization provides a way to optimize the expected value of a non-differentiable loss. The paper is well-written for the most part and is backed by good experimental results. However it like some of the mathematical details and some of the exposition could be greatly improved.\n\nI think there are several mistakes in the reasoning presented in the proof of Theorem 1 (see detailed comments below). Theorem 1 in the current paper seems to me to be a special case of Theorem 1 in (Song 2016), where the expectation over data is replaced by an expectation over the Gumbel variable gamma. If I've understood correctly, it seems like it would be more correct and concise to simply cite that paper with some explanatory comments.\n\nThe word \"direct\" occurs quite a lot in the paper. It sometimes seemed misplaced. For example for \"The direct differentiation of the resulting expectation\" in the introduction, in what sense is the differentiation direct, and what would non-direct differentiation be?\n\nIn section 3, that's not the meaning of the term \"exponential family\".\n\nThe re-use of theta and psi as both model parameters and the log probability density / distribution is unnecessarily confusing.\n\nA small point, but in \"the challenge in generative learning is to reparameterize and optimize (2)\", the authors assume that q has analytic expression for the second KL term in (1). That's often the case but definitely not always. Also, even if this KL term has an analytic expression, it is not always better to use it (see Duvenaud \"Sticking the landing...\").\n\nIn (3), the usual notation is P(x = i) where x is the random variable and i is its possible value, whereas in (3) the random variable z^{\\phi + \\gamma} appears on the right of the equals sign.\n\nThe first paragraph of section 4.1 and (4) and (5) are just a simple application of the law of total expectation, and it would be simpler and clearer to state that.\n\n\"gradient of the decoder\" should be \"gradient of the decoder log probability\" (or log prob density depending on preference). Similarly with \"the decoder is a smooth function\". The decoder is a conditional probability distribution (at least according to my understanding of conventional usage).\n\nIn the first equation in the proof of Theorem 1, it seems as though the authors are using the standard change of variables formula for integrals. However the new variable \\hat{\\gamma} depends on \\hat{z} through \\theta, so I don't see how it's valid to ignore the max in the way the present paper does. One way to see that something is wrong is the fact that the integrand on LHS has \\hat{z} as a bound variable only, whereas the integrand on RHS has \\hat{z} as both a bound variable (inside the max) and a free variable (since \\theta depends on \\hat{z}, though strangely that is not written in the equation). What is the value of \\hat{z} used for \\theta on RHS?\n\nThere's a missing [] after \\partial_\\epsilon in the third line of the paragraph starting \"We turn to prove Equation (8)\".\n\nIn the same line, I don't see why the two expectations are equal. It seems to me that the differentiation w.r.t. epsilon ignores the fact that changing epsilon occasionally changes z^{\\epsilon \\theta + \\phi_v + \\gamma} in a discontinuous way. The term being differentiated has both a continuous-in-epsilon component and a piecewise-constant-in-epsilon component, and the latter appears to have been ignored. While the gradient of a piecewise constant function is zero almost everywhere, the occasional large changes (which could be thought of as delta functions) still can make a large contribution to the overall expression once we take the expectation. To look at it another way, if the reasoning here is correct, why can't the same argument be used on the RHS of (8), first to take the derivative inside the expectation and subsequently to compute the derivative as zero, since the inner term is a piecewise constant function of v? Yet clearly the RHS of (8) is not always zero.\n\nAround \"However when we approach the limit, the variance of the estimate increases...\", I think it would be extremely helpful to explain that for small epsilon, we occasionally obtain a large gradient (and otherwise zero), while for large epsilon we often obtain a moderate non-zero gradient. That gives some insight into the effect of epsilon, and why the variance is larger for small epsilon.\n\nAny reason not to plot the bias in right Figure 1, which is ostensibly about the bias-variance trade-off?\n\nI didn't follow the meaning of the diagram or caption for left Figure 1.\n\nIn (11), I wasn't sure whether S included the supervised examples or not (i.e. whether S_1 was disjoint from or a subset of S). If disjoint, shouldn't the KL term be included, or the expectation-over-gamma term be changed to use ground truth z? I guess I was unclear on the form of loss used for the supervised data, and unclear on the motivation for this choice.\n\nIn the last sentence of section 5.1, should \"chain rule\" be \"variance reparameterization trick\"?\n\nIn section 5.2, it would be helpful to mention what mean field means in terms of the variational distribution q (namely q(z | x) = \\prod_i q(z_i | x) ). Also, the term \"mean field\" is not conventionally used for general distributions (such as the decoder here) as far as I'm aware, only for variational distributions. \"Conditionally independent\" might be clearer.\n\nWhat does \"for which we can approximate z^{...} efficiently\" refer to?\n\nIn section 6.1, what is the annealing rate? Also, the minimal epsilon is set to 0.1. Is epsilon changed as training progresses according to some schedule?\n\n\"The main advantage of our framework is that it seamlessly integrates semi-supervised learning\" seems like an overstatement. Wouldn't semi-supervised learning be relatively straightforward to incorporate into any form of VAE? And why not just use log p(x, z) for updating the decoder parameters and log q(z | x) for updating the encoder parameters?\n\nHow many labeled examples were used for the CelebA semi-supervised learning?\n\nSome bibliography typos. For example, no capitalization throughout (e.g. \"gumbel\" instead of \"Gumbel\"). Also lots of arxiv preprints cited when published papers exist (e.g. Jang 2017 should be ICLR 2017 not arxiv preprint).\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A significant contribution",
            "review": "The authors propose a method to apply the reparametrization trick when the random variables of interest are discrete. Their technique is based on a formulation of the objective function in terms of Gumbel-Max operators. They propose a derivation of the gradient in terms of an auxiliary variable \\epsilon, such that the resulting gradient estimate is biased but the bias is reduced as \\epsilon approaches zero, at the cost of increasing variance. Experiments are performed with VAE including discrete latent variable models. The authors show how their method converges faster than other baselines formed by estimators of the gradient given by the REBAR, RELAX and Gumbel-soft-max methods. In experiments with semi-supervised VAEs, their method outperforms the Gumbel softmax method in terms of accuracy and objective function.\n\nQuality:\n\nThe theoretical derivations seem rigorous and the experiments performed clearly indicate that the proposed method can outperform existing baselines.\n\nClarity:\n\nThe paper is clearly written and easy to read. I found that the network architecture shown in the left of Figure 1 a bit confusing and needs to be explained more clearly.\n\nSignificance:\n\nThe experimental results clearly show that the proposed method can outperform existing baselines and that the proposed contribution is significant.\n\nNovelty:\n\nThe proposed method is novel up to my knowledge. This is the first time I have seen the proposed theoretical derivations, which are significantly different from previous approaches.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An principled approach with weak empirical results",
            "review": "This work proposes a new (biased) gradient estimator to learn a discrete auto-encoders. Similarly to the gumbel-softmax estimator this paper proposes to use the gumbel-max trick and the reparametrization trick but instead of relaxing the argmax by a softmax, the authors derive a formula for the gradient based on direct loss optimization to compute the gradient through the argmax.\n\nPros:\n- The approach is well motivated and the proof of theorem 1 which gives the formula of the new gradient estimator seems correct.\n\nCons:\n- The principle downside of the proposed approach is that it requires to compute the value of the objective for several values of z, which makes it more computationally expensive than gumbel-softmax. Could the author compare the different estimators in terms of running time instead of epoch for example in fig2. it seems like RELAX would perform similarly or better in terms of wall-clock time.\n\n- [1] also proposed an estimator that requires evaluating the objective for different values of z, and showed that it is unbiased and optimal (lowest variance). I think the authors should mention this related work and how their approach differs. I also think the author should compare their work to [1].\n\n- Since both gumbel-softmax and the proposed approach are biased, could the authors give some intuitions on why they believe their approach is better.\n\n- I believe the expectation of the right-hand side of equation (9) can be computed in closed form by using a formula similar to eq (4) and (5), which replace the expectation by a sum over the possible values of z. This will lead to a gradient estimator with no variance, can the author comment on this ?\n\n- I think the bias induced by the mean-field approximation of the decoder should be investigated more thoroughly. Could the authors plot the gap as a function of n for example ? What happens if we also increase the number of category ? (there is a typo in this section it should be k^n instead of n^k) ? Can they compare to gumbel-softmax, is there a threshold at which gumbel-softmax becomes better ?\n\n- It's not clear on what setting is the variance plotted in fig 1. is computed ? Is it computed on the discrete VAE experiment ? if so how many latent variables and category ? Could the bias also be provided ? Could it be compared to gumbel-softmax with varying temperature ?\n\n- The experiments are a bit toyish, it's not clear what happens when the task are more complex, the architecture for the encoder and decoder are deeper or the latent space is bigger. In particular the authors only consider linear encoder and decoder when comparing the ELBO of different methods.\n\n- In the semi-supervised settings what happens if we don't set the perturbed level to the true label ?\n\nConclusion:\nThe experiments are quite toyish and the approach is more computationally expensive than gumbel-softmax. More experiments should be done to clearly show the advantage of this method compared to gumbel-softmax.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}