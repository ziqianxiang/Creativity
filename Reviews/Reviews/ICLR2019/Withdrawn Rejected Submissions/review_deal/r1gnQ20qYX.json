{
    "Decision": {
        "metareview": "This paper presents an approach that combines rule lists with prototype-based neural models to learn accurate models that are also interpretable (both due to rules and the prototypes). This combination is quite novel, the reviewers and the AC are unaware of prior work that has combined them, and find it potentially impactful. The experiments on the healthcare application were appreciated, and it is clear that the proposed approach produces accurate models, with much fewer rules than existing rule learning approaches.\n\nThe reviewers and AC note the following potential weaknesses: (1) there are substantial presentation issues, including the details of the approach, (2) unclear what the differences are from existing approaches, in particular, the benefits, and (3) The evaluation lacked in several important aspects, including user study on interpretability, and choice of benchmarks.\n\nThe authors provided a revision to their paper that addresses some of the presentation issues in notation, and incorporates some of the evaluation considerations as appendices into the paper. However, the reviewer scores are unchanged since most of the presentation and evaluation concerns remain, requiring significant modifications to be addressed.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Presentation and Evaluation concerns remain"
    },
    "Reviews": [
        {
            "title": "Interesting idea but needs significant improvement in terms of presentation and design of method and experiments",
            "review": "Summary: \nThis paper presents a new interpretable prediction framework which combines rule based learning, prototype learning, and NNs. The method is particularly applicable to longitudinal data. While the idea of bringing together rules, prototypes, and NNs is definitely novel, the method itself has some unclear design choices. Furthermore, the experiments seem pretty rudimentary and the presentation can be significantly improved. \n\nDetailed Comments: \n1. In Section 2, the authors seem to define rule list as a set of independent if-then rules. Please note that rule lists have an \"else if\" clause which creates a dependency between the rules. Please refer to \"Interpretable decision sets\" by Lakkaraju et. al. for understanding the differences between rule lists and rule sets. \n2. Section 3.1 is quite confusing. It would be good to give an intuition as to how the various pieces are being combined and in why it makes sense to combine them in this way. The data reweighting process seems a bit adhoc to me. What other choices for reweighting were considered?\n3. I would strongly encourage the authors to carry out at least a simple user study before claiming that the proposed method is more interpretable than existing rule lists. Adding both prototypes and rules, in fact, adds to the cognitive burden of an end user - it would be interesting to see when and how having both prototypes and rules will help an end user. \n\nPros:\n1. First approach to combine NNs, rule learning, prototype learning\n2. Provides an interpretable method for predictions on longitudinal medical data\n3. Experimental results seem to suggest that the proposed approach is resulting in accurate and interpretable models.\n\nCons:\n1. The various pieces in the method (rule learning, prototype, NNs, data reweighting) seem to be somewhat haphazardly connected. Section 3.1 does not give me a good idea about how the different pieces are resulting in an accurate and interpretable model\n2. The paper makes claims such as \"Experimental results also show the resulting interpretation\nof PEARL is simpler than the standard rule learning.\" without actually doing any significant user studies. Furthermore, any other synthetic data experiments which could demonstrate the various facets of accuracy-interpretability tradeoffs are missing\n3. The presentation of the paper is quite unclear. See detailed comments above. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interpretability insufficiently defined",
            "review": "This paper aims at tackling the lack of interpretability of deep learning models, which is especially problematic in a healthcare setting --the focus of this research paper. Specifically, the authors propose Prototype lEArning via Rule Lists (PEARL), which combines rule learning and prototype learning to achieve more accurate classification and better predictive power than either method independently and which the authors claim makes the task of interpretability simpler.  \nThe authors present an interesting and novel architecture in PEARL. Combining the two approaches of rule lists and prototype learning. However, my main concern with the paper and with the architecture in general is the lack of clarity upfront regarding what the authors perceive as the criteria for interpretability. This seems to be one of the chief aims of the paper, however, the authors don’t reach this point until Section 4 of the paper. Given that this is one of the main strengths of the paper as proposed by the authors, this needs to be given more prominence and also needs to be made more explicit what the authors mean by this. The authors define interpretability as measured by the number of rules and number of protoypes identified by a particular model, without, providing an argument, justification, or a citation of previous work which justifies these criterion. Especially since this is one of the main points of the paper, this needs to be better argued and the authors should either elaborate on this point, or restrain on making claims that these models are more interpretable.\nThe model architecture of Section 3.1 was quite obscure both from the intuitive and implementation level. It’s not clear how the different modules (prototype learning, rule lists) link together in practice, nor how these come together to create an interpretable model.\nGenerally, the paper is quite poorly structured and there were several grammatical errors which made the paper quite hard to follow. Although the problems articulated are important, the paper did not do sufficient justice to addressing these problems. \n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Needs more thorough evaluation, more justification of design choices, and improvement in presentation clarity",
            "review": "\n\nReview Summary\n--------------\nThe paper presents a combination of rule lists, prototypes, and deep representation learning to fit classifiers that are said to be simultaneously \"accurate\" and \"interpretable\". While the topic is interesting and the direction seems novel, I don't think the work is quite polished or competitive enough to be accepted without significant revision. The major issues include non-competitive evaluation of what \"interpretability\" means, ROC AUC numbers that are indistinguishable from standard deep learning (RCNN) pipelines that use many fewer parameters, and many unjustified choices inside the method itself. The paper itself could also benefit from revision to improve flow and introduce technical ideas to be more accessible to readers.\n\n\nPaper Summary\n-------------\nThe paper presents a new method called \"PEARL\" (Prototype Learning via Rule Lists), which produces a rule list, a set of prototypes, and a deep feed-forward neural network that can embed any input data into a low-dimensional feature space. The primary intended application is classifying subjects into a finite set of possible disorders given longitudinal electronic health records with categorical features observed at T irregular time intervals. \n\nThe paper suggests learning a representation for each subject's data by feeding the EHR time series into a recurrent convolutional NN. The input data is a 2 x T array, with one row representing observed data and second row giving time delay between successive observations. The vector output of an initial convolutional RNN is then fed into a highway network to produce a final vector denoted \"h\". \n\nGiven an encoder to produce feature vectors, and a fixed rule list learned from data itself, the paper suggests obtaining a prototype for each rule by computing the average vector of all data that matches the given rule. The quality of these prototypes and related neural networks (for computing features and predicting labels from features) is then assessed via their loss function in Eq. 1: a weighted combination of how well the prototypes match the learned embeddings (distance to closest prototype) and how well the classifier predicts labels.  The core idea is that the embedding is learned to classify well while creating a latent space that looks like the prototypes of the rule list.\n\nAfter training an embedding and NN classifier on a fixed rule list, it seems the data is reweighted according to some heuristic procedure to obtain better properties, then a new rule list is trained and the process repeats again. (I admit the reweight procedure's purpose was never clear to me).\n\nExperiments are done on a proprietary heart failure EHR dataset and on a subset of MIMIC data. \n\nStrengths\n---------\n* Seems original: I'm unaware of any other method connecting rule lists AND prototypes AND NNs\n* Neat applications to healthcare\n\nLimitations\n-----------\n* Interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rule-lists shown is an apples-to-apples comparison\n* Prototypes themselves never evaluated \n* Many design choices inside method not justified with experiments -- why highway networks + RCNNs?\n\nMajor Issues with Method\n------------------------\n\n## M1: Not clear that AUC difference between PEARL and baselines is significant\n\nThe major issue is that the presented approach does not seem significantly different in predictive performance than the baseline Recurrent CNN. Comparing ROC AUC, we have PEARL's 0.688 to RCNN'S 0.682 with stddev of 0.009 on the proprietary heart failure dataset, and PEARL's 0.769 to RCNN's 0.766 with stddev of 0.009. When AUCs match this closely, I struggle to believe one model is definitively better, especially given that the RCNN has 2x *fewer* parameters (8.4k to 18.4k). \n\nIf the counterargument is that the resulting \"deep model\" is not \"interpretable\", one should at least compare to a post-processing step where the decision boundary of the RCNN is the reference to which a rule list or decision tree is trained.\n\n## M2: Interpretability evaluation not clear.\n\nIsn't the maximum number of rules set in advance? \n\nAdditionally, prototypes are a key part of this work, but the learned prototypes are not evaluated at all in any figure (except to track avg. distance from prototype while training). If prototypes are so central to this work, I would like to see a formal evaluation of whether the learned prototypes are indeed better (in terms of distance, or inspection of values by an expert, or something else) than alternatives like Li et al.\n\n## M3: Missing a good synthetic/small dataset experiment\n\nNeither of the presented data tasks is particularly easy to understand for non-experts. I'd suggest creating an additional experiment where the audience of ML readers is likely to easily grasp whether a set of rule lists is \"good\" for the problem at hand... maybe create your own synthetic task or a UCI dataset or something, or even use the stop-and-frisk crime dataset from the Angelino et al. 2018 paper. Then you can compare against just a few relevant baselines (rule lists only or prototypes only). I think a better illustrative experiment will help readers grasp differences between methods. \n\n## M4: How crucial is feature selection?\n\nIn each iteration, Algo. 1 performs feature selection before learning rules. Are any other baselines (trees, rule lists) allowed feature selection before the classifier is learned? What would happen to PEARL without feature selection? What method is used for selection? (A search of the document only has 'feature selection' occur once, in the Alg. itself, so it seems explanation is missing).\n\n## M5: Why are multiple algorithm iterations needed?\n\nWon't steps 3 and 4 of Alg. 1 result in the same rules every time? It's not clear then why on subsequent iterations the algorithm would improve. Perhaps it's just the reweighting of data that causes these steps to change?\n\nMinor issues\n------------\n\n## Loss function notation confusing\n\nDoesn't the rule list classifier s_R take the data itself X? Not the learned embedding h(X)? Please fix or clarify Eq. 1. I think you might clarify notation by just writing yhat(h(X)) if you mean the predicted label of some example as done by your NNs. Using \"R\" makes folks think the rule list is involved.\n\n## Not clear why per-example reweighting is required\n\nNone of the experiments assess why per-example reweighting (lines 6-9 of Algo. 1) is required. Readers would like to see a comparison of performance with and without this step.\n\n## Not clear or justified when \"averaged\" prototypes are acceptable\n\nAre your \"averaged\" prototypes guaranteed to satisfy the rule they represent? Is taking the average of vectors that match a rule always guaranteed to also match the rule? I don't think this is necessarily true. Consider a rule that says \"if x[0] == 0 or x[1] == 0, then ___\".  Suppose the only matching vectors are x_A = [0 1] and x_B = [1 0]. The average vector is [0.5 0.5] which doesn't work.\n\n## Several different measures of distance used without careful justification \n\nWhy use two different distances -- Euclidean distance to assess distance to prototypes for prototype assignment, and then cosine similarity when deciding which examples to upweight or downweight? Why not just use Euclidean distance for both (appropriately transformed to a similarity)?\n\nComments on Presentation\n------------------------\nOverall I think every section of the paper needs significant revision to improve a reader's ability to understand main ideas. Notation could be introduced slowly (explain purpose and dimension of every variable), assumptions could be clearly stated (e.g. each individual rule can have ANDs but not ORs), and design choices justified. You might try the test of giving the paper to a colleague and having them explain back the ideas of each section to you... currently I do not believe this version passes this test.\n\nThe introduction claims that \"clinicians are often unwilling to accept algorithm recommendations without clarity as to the underlying reasoning\", but I would be careful in blindly asserting this without evidence. For a nice argument about avoiding blind assumptions about what doctor's will and won't accept, see Lipton's 2017 paper \"The Doctor Just Won't Accept That\" (https://arxiv.org/abs/1711.08037)\n\nAdditionally, the authors should clarify more precisely what definition of interpretability is needed for their applications. Is it simplicity? Is it conceptual alignment with known medical facts? Is it the ability to transparently list the rules in plain English?\n\nLine-by-line details\n--------------------\n\n## Sec. 2\n\nWhen introducing p_j, should clarify this this is one prototype vector of many.\n\nWhen defining p_j = f_j(X), can you clarify what dimensionality p_j has? Is it always the same size as each example's data vector x_i?\n\n\n## Sec. 3\n\nFig. 2: I don't find this figure very easy-to-understand. It's clear that after embedding raw features to a new space, the learned rules are *different*, but it's not clear they are *better*.  None of the illustrated rules perfectly segments the different colors, for example. I guess the point is all the red dots are within one rule? But they aren't alone (there are blue and orange dots too), so it's still not clear this would be a better classifier.\n\nFor EHR datasets, are you assuming that events are always categorical? And that outcomes \"y\" are always discrete (one-of-L) variables? Or could y be real-valued?\n\nEq. 1: You should make notation clearly indicate which terms depend on \\theta. Currently it seems that nothing is a function of \\theta.\n\nEq. 1: Do you also find the prototype set P that minimizes this objective? Or is there another way to obtain P given parameters \\theta? This is confusing just from reading the eqn.\n\nWhat size is the learned representation h(X)? Is it a vector?\n\nEq. 6: Do you really need a \"network\" to compute the distance to each of the K prototypes? Can't you just compute these distances directly?\n\n## Sec 4\n\n\"Mac OS 1.4\" : Do you mean Mac OS version 10.4? Not clear this is relevant.\n\n4.3 Case Study: How do I read these rules? Is this rule applied only if ALL conditions are true? or if any individual one is true (\"or\")? This is unclear.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}