{
    "Decision": {
        "metareview": "This paper was reviewed by three experts (I assure the authors R3 is indeed familiar with RL and this area). Initially, the reviews were mixed with several concerns raised. After the author response, R2 and R3 recommend rejecting the paper, and R1 is unwilling to defend/champion/support it (not visible to the authors). The AC agrees with the concerns raised (in particular by R2) and finds no basis for overruling this recommendation. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Simple and intuitive idea but evaluation seems to be quite lacking",
            "review": "This paper considers the assumption implicit in hindsight experience replay (HER), namely that we have access to a mapping from states to goals. Rather than satisfying this requirement by defining goals as states, which involves great redundancy, the paper proposes a natural language goal representation. Concretely, for every state a teacher is used to provide a natural language description of the goal achieved in that state, which can be used to directly relabel the goal so the episode can be used as a positive experience.   \n\nStrengths:\n- the proposed idea is simple and intuitively appealing, and shows much better results than the DQN baseline.\n\nWeaknesses:\n- In the VizDoom task, the goal specification is already in (templated) language. Given that this is the case, and the mapping from states to goals can be extracted from the environment anyway, it seems like the method that is applied really just reduces to a vanilla implementation of HER. There seems to be little novelty in this. From reading the introduction and method, I expected the ACTRCE approach to be applied to a task where the goal was not originally specified in language, perhaps by collecting language from human teachers. This would be a much more interesting experiment, addressing the question of whether human feedback in natural language can help the agent learn more quickly.\n- Even leaving aside the previous concern, it seems very difficult to put this work in the context of previous work on the same tasks. For example, it is not clear why there are no comparisons to the previous work on instruction following in VizDoom, as the setting appears to be exactly like Chaplot et al. 2017. It would seem like a natural comparison would be to take the model from Chaplot (leaving the task and architecture etc unchanged) and train it using ACTRCE. Is there any reason why this can’t be done? There is already so much existing work in this space, it seems quite unusual that the proposed new method is not compared to any existing work on an existing task.\n\n\nSummary:\nThis is a simple and intuitively appealing idea, but I find the evaluation to be quite lacking because the tasks already use a language specification (such that ACTRCE seems to be vanilla HER in application) and there are no comparisons to previous work. These two concerns seem quite substantial to me and make it difficult to recommend acceptance. \n\nSmaller issues:\n- ACTRCE - possibly the most tortured acronym in recent memory! How should it be pronounced?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, very interesting analysis and insights",
            "review": "This submission presents a method to improve the sample-efficiency of instruction-following models by leveraging the Hindsight Experience Replay framework with natural language goals. \n\nHere are  my comments/questions:\n- The paper is well written and easy to follow, it introduces a simple idea which achieves very good results.\n- In addition to improving the performance as compared to the baselines, the authors perform a wide variety of experiments such as analysis of language representations, visualization of embeddings, etc. which lead several insightful results such as ability of sentence embeddings to generalize to unseen lexicon, ability of the model to perform well with just 1% advice.\n- It is important to note that as compared to the baselines, the proposed method requires access to the set of goals and extra information about which goal was reached in each episode.\n- In Table 1, how many frames were DQN and ACTRCE trained for? I am wondering why the MT performance for DQN is so low. Did the DQN have Gated-Attention?\n- The composition task is very interesting, did the agent receive intermediate rewards for completing a part of the instruction in this task?\n- Some implementation details questions:\n\t- In Appendix D Training details, what do you mean by 'chosen from the range {1000, 10000, 10000}'?\n\t- In Appendix D Training details,  it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic (A3C), where is A3C used in the experiments?",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "not clear; surprising DQN results; toy environments",
            "review": "Paper Summary: \nThe idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. \n\nPaper Strengths:\nUnfortunately, there is not many positive points about the paper except that it explores an interesting direction. \n\nPaper Weaknesses: \n\nI vote for rejection of the paper due to the following issues:\n\n- It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all.\n\n- The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal.\n\n- According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well?\n\n- The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in \"On Evaluation of Embodied Navigation Agents\".\n\n- Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work:\nInteractive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018\n\n- The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics.\n\n- What is the difference between this method and providing a large negative reward at a non-target object?\n\n- The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. \n\n- It seems the same environment is used for train and test.\n\n------------------------\nPost rebuttal comments:\n\nMost of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:\n\n- I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic.\n\n- The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given.\n\n- The writing is still confusing. For instance, it is mentioned that \"Concretely, for each state s ∈ S, we define T as a teacher that gives an advice T(s)\", while that is not true since later it is mentioned that \"the teacher give advice based solely on the terminal state\". These statements are contradictory, and it is not trivial at all to provide an advice for each state.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}