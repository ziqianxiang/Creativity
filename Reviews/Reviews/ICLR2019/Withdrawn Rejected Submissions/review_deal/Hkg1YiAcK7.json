{
    "Decision": {
        "metareview": "The paper proposes a learning by teaching (LBT) framework to train an implicit generative model via an explicit one. It is shown experimentally, that the framework can help to avoid mode collapse. The reviewers commonly raised the question why this is the case, which was answered in the rebuttal by pointing to the differences between the KL- and the JS-divergence and by showing a toy problem for which the JS-divergence has local minima while the KL-divergence has not. However, it still remains unclear why this should be generally and for explicit models with insufficient capacity the case, and if the model will be scalable to larger settings, therefore the paper can not be accepted in the current form.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Intersting paper that would profit from a better understanding of the underlying mechanism "
    },
    "Reviews": [
        {
            "title": "Well-written,  interesting experiments, doubts about scalability",
            "review": "The authors present a novel architecture of  an implicit unsupervised learning architectures using\na teacher student approach.  In particular the main advantage to me seems to be the mode-collapse property,  an important drawback in standard\nGAN approaches.\n\nThe paper is written very well and is easy to follow. The methodology is presented in a clear way and the experiments make sense given the research question.  I particular like that the authors define clear metrics to evaluate success, which is often the weak point in unsupervised learning problems. \n\nI believe the work is interesting, but the results still preliminary and  possibly limited by  scalability.  As the authors put it \n\n\"The main bottleneck of LBT is how to efficiently solve the bi-level optimization problem. On one\nhand, each update of LBT could be slower than that of the existing methods because the computational\ncost of the unrolling technique grows linearly with respect to the unrolling steps.\"\n\nOn the other hand, I appreciate the honesty in discussing possible scalability constraints.\n\nI was a bit surprised that the method the authors propose seems to work well in the  \"Intra-mode KL divergence\".  My expectation was  that the main advantage of your method is capturing the global, holistic shape of the distribution\nof the data, whereas classical methods would, because of mode collapse, only capture specific  sub-spaces.  Therefore, i would expect these classical methods to perform better in intra-mode KL divergence,  which is a metric to measure local\n, not global, approximation quality.\n\nTypos: \n-  In practise (Introduction) -> in practice\n- 3.1 accent -> ascend\n- Conclusion: on one hand / other hand is used for two opposite ways of thinking",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Avoiding Mode Collapse?",
            "review": "This paper presents an learning by teaching (LBT) framework to train implicit generative models. Instead of using discriminator as in GANs, LBT adopts an explicit likelihood estimator as the student, which is formulated as a bilevel optimization problem: \n1) maximize the log-likelihood of the generated samples; \n2) maximize the log-likelihood evaluated on the training data. \nThe authors argue that LBT avoids the mode collapse problem intrinsically as the missing modes will be penalized by the second objective. I have some concerns on this.  Why teaching an explicit likelihood can help learn an implicit one? \n\nSuppose the explicit likelihood estimator is a single Gaussian, but the real distribution has multiple modes, fitting such the generated data and the training data on this likelihood will not help to avoid missing modes. \n\nFrom the empirical results, it is clear that LBT-GAN is better than LBT. From the objective in (8), it seems the true reason is  the P_E and D together representing a mixture model, which may fit the training data better. \n\nIn Figure 2.(b), the Intra-mode KL divergence of LBT-GAN seems to be unstable during the training, is this caused by the joint training of discriminator with the estimator. Can you discuss this?\n\nIn Table 1, the authors just copied the results of VEEGAN. Indeed, in our implementation, DCGAN and VEEGAN can be much better than the reported one. The authors have not tried the effort to tune the results of baselines. \n\nRecently, the Least square GAN has been purposed to address the mode collapse as well. I suggested the authors should empirically compare with it as well.\n\nGenerally, the paper is well-written. The idea is interesting, however, the motivation, analysis and empirical results are not convincing enough to fully support their claim. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good cocktail of ideas, limited analysis",
            "review": "This work introduces a framework for learning implicit models that is robust to mode collapse. It consists in learning an explicit model of the implicit model through maximum likelihood while the later is used to teach the explicit model to better match the data distribution. The resulting bi-level optimization is carried out with truncated unrolled stochastic gradient descent.\n\n# Quality\n\nThe method combines an interesting set of ideas. It is validated on some reasonable experiments. \n\nHowever after reading the paper, I remain with too many unanswered questions:\n- Why should the method avoid mode collapse? Experiments clearly show that it indeed is resilient to mode collapse, but I have would have been curious in seeing some more discussion regarding this point. What is the exact mechanism that solves the issue?\n- What is the effect of K? Is mode collapse solved only because of the unrolled gradients?\n- What is the effect of M? How does the method behave for M=1, as usually done in GANs?\n- What if the explicit model has not enough capacity?\n- The original Unrolled GAN paper presents better results for the ring problem. Why are results worse in the experiments?\n\nMore fundamentally what is the main benefit of this approach with respect to models that can be trained straight with maximum likelihood? (e.g., flow-based neural generative models; and as required for the explicit model) Is it only to produce generative models that are fast (because they are implicit)? Why not training only the explicit model directly on the data?\n\n# Clarity\n\nThe paper is in general well-written, although some elements could be removed to actually help with the presentation.\n- The development around influence functions could be removed, as the method ends up instead making use of truncated unrolled gradients.\n- The theoretical analysis is straightforward and could be compressed in a single paragraph to motivate the method.\n\n# Originality\n\nThe method makes use of several ideas that have been floating around and proposed in different papers. As far as I know, the combination proposed in this work is original.\n\n# Significance\n\nResults show clear resistance to mode collapse, which is an improvement for implicit models. However, other types of generative models generally do not suffer from this issue. Significance is therefore limited.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}