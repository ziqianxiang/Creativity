{
    "Decision": {
        "metareview": "The paper proposes a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling.\n\nThe reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) preliminary experimental results.\n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Limited novelty and non-impressive experimental results"
    },
    "Reviews": [
        {
            "title": "Overall score 6",
            "review": "1. This paper presents a novel approach for training efficient binary networks with both binary weights and binary activations and verifies the capability of the proposed method on benchmark. In addition, related bach normalization and pooling layers are also improved.\n\n2. How long do we need for training BNNs using the proposed method?\n\n3. The paper only compares their performance with only one related work (Hubara et al., 2016), which is somewhat insufficient.\n\n4. Another only concern is that whether the proposed method can be well applied on large scale datasets such as ImageNet.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The investigated problem is interesting, but methods used to handle binary networks are not impressive",
            "review": "To reduce the deep neural networks' reliance on memory and high power consumption, the paper proposed a kind of probabilistic neural networks with both binary hidden node and binary weights. The paper presents a probabilistic way to binarize the activation and weight values. Also, it proposed a random version of batch-normalization and max-pooling.\n\nThe binarization of hidden node is achieved through stochastic sampling according to the sign of the stochastic pre-activation values. The weight binarization is analogously done by sampling from a binary distribution. There is no too much new here, and is a standard way to obtain binary values probabilistically. \n\nThe paper said in the introduction that the binary model will be trained with the re-parameterization trick, through either propagating the distributions or the samples from concrete distribution. But I am still not very clear how this training process is done, especially for the training of weight parameters.\n\nOverall, the problem investigated in this paper is very interesting and is of practical importance, the experimental results are preliminary but encouraging. But all the techniques used in this paper to binarize neural networks are standard, and no too much new here.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A vaguely described idea without enough improvements",
            "review": "## Summary\n\nThis work presents a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling. By sampling from the weight distribution an ensemble of Binary Neural Networks could further improve the performance. In the experimental section, the authors compare proposed PBNet with Binarized NN (Hubara et al., 2016) in two image datasets (MNIST and CIFAR10).\n\nIn general, the paper was written in poor quality and without enough details. The idea behind the paper is not novel. Stochastic binarization and the (local) reparametrization trick were used to training binary (quantized) neural networks in previous works. The empirical results are not significant. \n\n## Detail comments\n\nIssues with the training algorithm of stochastic neural network\nThe authors did not give details of the training method and vaguely mentioned that the variational optimization framework (Staines & Barber, 2012). I do not understand equation 1. Since B is binary, the left part of equation 2 is a combination optimization problem. If B is sampled during the training, the gradient would suffer from high variance.\n\nIssues with propagating distributions throughout the network\nEquation 3 is based on the assumption of that the activations are random variables from Bernoulli distribution. In equation 4, the activations of the current layer become random variables from Gaussian distribution. How the activations to further propagate?  \nIssues with ternary Neural Networks in section 2.4\nFor a ternary NN, the weight will be from a multinomial distribution, I think it will break the assumption used by equation 3.\n\nIssues with empirical evidences\nSince the activations are sampled in PBNET-S, a more appropriate baseline should be BNN with stochastic binarization (Hubara et al., 2016) which achieved 89.85% accuracy on CIFAR-10. It means that the proposed methods did not show any significant improvements. By the way BNN with stochastic binarization (Hubara et al., 2016) can also allow for ensemble predictions to improve performance.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}