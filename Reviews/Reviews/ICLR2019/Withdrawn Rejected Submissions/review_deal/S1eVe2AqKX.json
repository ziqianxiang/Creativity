{
    "Decision": {
        "metareview": "All reviewers rate the paper as below threshold. While the authors responded to an earlier request for clarification, there is no rebuttal to the actual reviews. Thus, there is no basis by which the paper can be accepted.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview: no rebuttal"
    },
    "Reviews": [
        {
            "title": "Simple Idea, Good Results But Novelty? Detailed Analysis?",
            "review": "The paper proposes a simple idea to calibrate probabilities outputted by a CNN model to adapt easily to environments where class distributions change with space and time (and are often skewed). The paper shows that such a simple approach is sufficient to get good accuracies without requiring any costly retraining or transfer learning. Thereby proving to give benefits in terms of resource consumption and at the same time giving better results than the state of the art.\n\nHowever, \nA] The proposed calibration doesn't take any CNN specific details into consideration, rather it is a general calibration method which was also proposed in Saerens et. al, 2002 (cited in the paper). It is unclear why the paper specifically talks about CNN.\nB] The proposed Class Skew Detector is a simple method. Change-point detection is a well-studied area. The paper lacks a literature review in this area and a reasoning of why the proposed approach is preferred. Also, an independent analysis of how the class skew detector behaves in the face of rapidly changing class skews versus slow changing class skews is warranted here. Particularly, given that the paper proposes to use this approaches in mobile which may work in both rapid and slow changing class skews.\nC] The Class Skew Detector is dependent on the base model. Thus, it is also likely that the empirical distribution estimated is biased and yet the final accuracies reported are much higher than the base model accuracies. There is something interesting happening here. An analysis of the robustness of the proposed approach in the face of noisy class skew detection could potentially make this paper a stronger work.\nD] The analysis in the paper has largely focused on pre-trained models. However, another analysis that could have been useful here is, varying the quality of the classifier (e.g. classifier trained on skewed training data vs. balanced training data) and measuring how the quality of the classifier correlates with the final performance. Maybe even attempt to answer the question \"which classifiers are likely to work with this approach?\" In fact, this analysis can be either done in a general context of any classifier or just CNN's and identifying whether certain properties of CNN help in getting better performance.\n\nThe paper lacks novelty and at the same time, it is not quite compensating that with a detailed analysis of the work. The problem is interesting and I like the work because the approach is simple and the results look good. I think with a stronger focus on more detailed analysis, this can be a good submission to an applied conference like MobiCom etc.\n\nBy the way, the paper is riddled with several spelling errors - \n\"filed\" -> \"field\", page 1, second paragraph, last line\n\"complimentary\" -> \"complementary\", page 2, section 2, paragraph 1, last line\n\"epoches\" -> \"epochs\", page 2, section 2, transfer learning, second paragraph, second last line\n\"CNNs does not use\" -> \"CNNs do not use\", page 3, section 3, intuition, first paragraph, first line\n\"formular\" -> \"formula\", page 4, above equation 4\nEquation 4 has a typo in the denominator, P_t(i) should be P_t(j), same with Equation 5\n\"obstained\" -> \"obtained\", page 7, second paragraph, first line\n\"adaptation\" is almost everywhere spelled as \"adaption\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "No technical contribution, Heuristic solution",
            "review": "This paper proposed a way to detect a skew in the distribution of classes in a stream of images and reweight the class priors accordingly, to estimate the final posterior probabilities of present classes. This probability re-calibration is referred to as the probability layer. A simple algorithm is proposed to detect the class distribution skew. The proposed benefit of this method is that they do not require fine-tuning any network parameters using newly skewed data. \n\nOverall the method is quite simple and heuristic. The technical contribution - i) updating class priors online ii) detecting class skews, is marginal. \n\nThe evaluation is performed on a contrived setting of skewed imagenet images. I would have liked to see some evaluation on video stream data where the skews are more natural. \n\nIn real scenarios, the class specific appearances P_{X|Y}(x|i) as well as class distributions P_Y(i) change online. The method seems incapable to handle such problems.  In these situations, there is no simple fix, and one needs to resort to transfer.\n ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The idea proposed in this paper is to improve classification accuracy by making use of the context.\nE.g. on the north pole we will see polar bears but no penguins, on Antartica we have no polar bears but many penguins.\nHence, if we apply our imagenet-like classifier in the wild, we can improve accuracy by taking into account changes in the prior distribution.\n\nThe paper proposes a way to rescale the probabilities to do exactly this and reports improved results on modified versions of \n CIFAR 10 and imagenet with artificial class skew. To achieve this, an additional trick is introduced where the re-scaling is only used when the model is not very certain of its prediction. And additional motivation for this work is that less compute resources are needed if the problem is simplified by utilizing class skew. \n\nThe core idea of the paper is interesting. However, I am not able to understand what exactly is done and I am 100% confident I cannot re-implement it. The authors already improved upon this in our interactions prior to the review deadline. \nAn additional issue is that the paper does not have a good baseline. \nI would not like to dismiss the approach based on its simplicity. An elegant solution is always preferred. However, all the tasks are quite artificial and this limits the \"impact\" of this work. If an \"natural\" application/evaluation where this approach would be possible, it would strengthen the paper greatly. \n\nFor the reasons above I recommend rejection of the manuscript in the current state but I am confident that many of these issues can be resolved easily and if this is done I will update the review.\n\nMissing information\n----------------------------\n- The original manuscript had a lot of information missing, but much of it has since been provided by the authors.\n- In the static class skew experiment, were two passes over the data needed? Or was the Pt(i) pre-set? Would it also be possible to give details about LR, optimizer, LR schedule, batch size, .... for the transfer learning experiments. This would enhance reproducibility. \n- For the imagenet experiments how was Pt(i) set in the if I assume correctly, static setting.\n\nPossible additional baselines:\n-----------------------------------------\n\nWe could make a simpler rescaling by changing the prior distribution and assuming everything else remains constant.\nWhile this is a simplifying assumption, it is very easy to implement and should take only a couple of minutes to run. \nP(i|x)=1/P(X)*P(X|i)*P(i)\nPt(i|x)=P(i|x)*Pt(i)/P(i)\n\nOne could also introduce another baseline where only the most probably classes are considered. Since this approach is clearly sub-optimal since it guarantees some mis-predictions it should serve as a lower bound on the performance that is to be expected. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}