{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good insights on unifying ideas under GVFs; but questions on algorithmic improvements.",
            "review": "The authors analyze various RL algorithms under the purview of Generalized Value Functions (Sutton et al. 2011). Specifically, the successor feature vector, the policy-gradient theorem, the option-value function, and the gradients for manager/worker in FeUdal Networks are all represented crisply in terms of GVFs. The detailed proofs in the Appendix and the mathematical rigor are appreciated.\n\nA major motivation for unifying ideas under GVFs is to facilitate development of new algorithms, but I find the paper slightly less convincing on this front. With respect to different sections:\n\n•\t[GVF for PG]: Rewriting the policy-gradient theorem with 2 GVFs, the authors propose to improve on the baseline algorithm with bootstrapping on both the critic and the actor-gradients. Although the results on a small domain look interesting, I would have enjoyed some discussion on the scalability aspects and integration/comparison with more recent actor-critic algorithms. \n\n•\t[GVF for Options]: The authors claim that “The GVF view highlights the fact that the option-value function depends .... and a long-term signal summarizing the performance of other options”. I believe that the definition of the option-value defined in Bacon et al. (2017) makes this dependence quite clear – it has an expectation over the option-value of all options. Therefore, I’m unable to appreciate the importance of writing option-value in GVFs and/or deriving an algorithmic improvement out of that. \n\n•\t[GVF for FuN]: Using the GVF view, the authors propose to use a different prediction variable (v) for the policy – i.e. instead of difference in state representation at (t+c) and (t) as used in FuN, they use a discounted (by gamma_hat) sum of these differences. Is this interpretation correct? If yes, could you provide some intuition for how this amounts to an algorithmic improvement over FuN? The Atari results definitely don’t show the improvement empirically.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting paper",
            "review": "The authors present an interesting analysis of generalized value functions (GVFs) and demonstrate that several RL algorithms including policy gradient, successor features, options, etc. are special cases of GVFs. The paper is reasonably well-written, and empirical studies demonstrate the benefits of using the GVF formulation to tweak existing algorithms. \n\nPros:\n1. Very interesting analysis of the various algorithms and unification into the single GVF framework. \n2. Especially ike the fact that these connections help discover flaws in existing algorithms (like the mode-collapse issue in FuN)\n\nCons:\n1. The options experiments (and comparison to FuN) are done on simple Atari games, which do not benefit as much from hierarchical policies. It would be good to perform empirical studies on more suitable environments (like 3D mazes / Montezuma's revenge).\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "GVFs for Knowledge",
            "review": "This paper advocates general value functions (gvfs) as a unifying and binding technology for building AI systems, and points out how existing ideas like successor representations, policy gradient, and options can be well-thought of as gvfs. The paper uses these connections to develop new ideas for policy gradient and feudal networks, illustrating these ideas with experiments in Atari and a gridworld environment.\n\nI am leaning to reject this paper. On the one hand I agree with the authors on the importance of gvfs and their potential is spread across numerous papers making it difficult, currently to make these connections. On the other hand many of the insights here are simple and in many cases well-known, but perhaps not yet formally written down. The current paper is not particularly well organized and the experiments are a bit confusing, and not particularly convincing in the case of the Atari results. All taken together, the paper needs a bit more work to achieve the ambitious goals it sets for itself. It seems another round of editing, tightening, and improving the results could indeed make this a very solid paper.\n\nExplanation for rating. This paper is trying to achieve something fairly difficult: elevate the importance of gvfs as a unifying concept for AI systems. This is needed because the Horde and Nexting papers did not provide or attempt to spell out all the connections to other things that can be well-thought of as a collection of gvfs, while later work that used these ideas, like the UNREAL paper, did a poor job of assigning credit and clearly discussing how the gvf idea was key. To do this well the writing of the paper must proceed very carefully, and this submission falls a little flat here. For example the abstract states “In this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents.“ This was the whole objective of the Horde paper, so one has to explain what is missing without sounding redundant. The introduction has similar problematic sentences of this form: “In this paper, we highlight the fact that value functions can in fact be used as the main building block for knowledge representation in RL agents.“  To do this well the paper has to be very precise, and sometimes it fails to do so. For example: “Sutton et al (2011)…illustrates in fact the successful large-scale learning of GVFs about a diversity of signals and at many different time scales, in parallel, from a single stream of data.“. This is not correct, scale was demonstrated in the Nexting paper, not the Horde paper. Furthermore, the related work section is strange because it talks about two architectures that use the gvf formalism. These seem to be examples supporting your argument, not things to contrast against. In fact, it is not clear what the related work section of such a paper should be given that this paper is primarily a review (with unifications that should always be the outcome of an extensive and insightful review) and position paper.\n\nIn the end I am not sure this paper has a clear identify because of the somewhat distracting focus on new algorithms and experiments trying to illustrate their utility—but the experiments don’t make a serious attempt at illustrating a contribution as discussed below. This could all be addressed with a change in the pitch and tone of the paper, but the authors should ask themselves “what are we adding beyond what can be found in the work of Sutton et al (2011), Modayil et al (2013), White (2015), and Sutton’s numerous writings on Predictive approaches to AI?”\n\nThe connection between nexting (and thus gvfs) and Successor features is well known, while theorem 1 follows directly from the original policy gradient theorem. This is not to say these connections are not interesting and should not be talked about, but they are true by construction and that is indeed why using gvfs for knowledge is such a good idea. There must be a way to discuss these things without pitching the resulting new algs as the main contribution of the paper. It seems the paper is trying to do too many things at once. If the new algs are interesting write a paper about just them and provide clearer evidence of their utility rather than hiding it inside a large narrative about gvfs for AI.\n\nThe experiments are difficult to understand. Section 4.1 appears to describe an experiment, but in the end is describing an algorithm described in Fig 1. I think the alg requires rollouts which is not ideal, but this is mentioned in passing. There is an algorithm block in the paper but it is never referenced: I can’t quite see how it connects to experiment #1. Section 4.3 finally describes the experiment (after the the main figure, which does not indicate the task or what the baselines are). What is being learned here: the 4 rooms problem was originally designed for option learning. Are you learning policies for each room, it is never stated. What is the behavior policy here? I don’t understand the y-axis of these graphs it says “value function” but the caption mentions L1 norm. I assume up is good but I have no context to understand if this is good final performance and fast learning. A well explained baseline would help. It is really great that you show all the parameter combinations and their performance, good job.\n\nThe second experiment compares against Kondas AC calling it a a policy gradient method for larger or continuous environments. This was not explained further, and it was evaluated on a small discrete domain. I cannot tell if this experiment was fair, even consulting the plots in the appendix. It raises several questions:  How did you decide the ranges of the meta parameters to sweep? The AC baseline’s performance gets better and better with increasing alphas. What about even larger alphas?  Did that make the two approaches tie.  I don’t understand why the baseline totally fails in the larger gridworld. Is this expected or perhaps it is harder to get the baseline working on the larger domain? Because the paper does not comment on the outcome, the reader is left to wonder.\n\nThe results of the 3rd experiment don’t paint a clear picture either. Sometimes the new approach helps a small amount, sometimes it hurts—basically the same overall. Perhaps this—atari—was not a good domain to illustrate the merit of this idea.\n\nOverall the paper is too rushed, and too unpolished. I look forward to the author respond so that I can refine both my understanding and assessment of this work. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}