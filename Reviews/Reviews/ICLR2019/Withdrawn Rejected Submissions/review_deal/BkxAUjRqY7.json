{
    "Decision": {
        "metareview": "The paper proposes an information theoretic quantity to measure the performance of transferred representations with an operational appeal, easier computation, and empirical validation. \n\nThe relation of the proposed measure to test accuracy is not considered. The operational meaning holds exactly only in the special case of linear fine tuning layers. The paper seems to import heavily from previous works. \n\nReviewers found it difficult to understand whether the proposed method makes sense, that the computation of relevant quantities might be difficult in general, and that the comparison with mutual information was not clear. The revision addresses these points, adding experiments and explanations. Yet, none of the reviewers gives the paper a rating beyond marginally above acceptance threshold. \n\nAll reviewers found the paper interesting and relevant, but none of them found the paper particularly strong. This is a borderline case of a sound and promising paper, which nonetheless seems to be missing a clear selling point. \n\nI would suggest that developing the program laid out in the conclusions could make the contributions more convincing, in particular the development of more scalable algorithms and the application of the proposed measure to the design of hierarchies for transfer learning. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Proposes measure for transferability with various selling points, but just ok "
    },
    "Reviews": [
        {
            "title": "Essential concepts and quantities undefined",
            "review": "The authors propose an information theoretic metric to determine a priori if a representation learned from a source task can be useful when learning another task. This is a timely topic and being able to compute such as metric between tasks upfront would be of great interest.\n\nWhile the motivation and contributions of the paper are clearly stated, a number of questions remain. Central concepts like error exponent or maximal HGR correlation are mentioned in passing. The former is important to understand whether (1) makes sense and the latter is a fundamental quantity for this work. The surprising fact is that HGR is not discussed at all, nor is there any intuition provided of what this quantity represents.\n\nWhile the proposed H-score is attractive when a source task has to be selected, as the denominator of the task transferability itself does not need to be computed, it is not straightforward to compute this quantity in general. The authors suggest to use the alternative conditional expectation (ACE) algorithm in order to optimize (2). ACE is not discussed in any detail (e.g., how accurate is this procedure; what are the choices and/or trade-offs if any?) and (2) is not justified. Overall, I felt section 4.2 was relatively inaccessible, but important as it indicates whether the proposed metric is of any use in practice.\n\nFinally, I did not understand the argument that says the H-score is to be prepared over the mutual information. To me the mutual information is still the golden standard. It was also not clear why the proposed approach would not straightforwardly apply to non classification problems as suggested in the future work.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "new measure of transferability ",
            "review": "The paper proposes the H score H(f), a quantity that measure the goodness of feature f(x) for predicting some target y. This heavily builds on Makur et al. (2015) who introduce information vectors, the error exponent, and the DTM matrix. The paper connects H(f) with these quantities to justify the proposal (e.g., H(f) is proportional to the error exponent (Theorem 1)). The actual transferability is measured by the ratio between H(f) and H(f_opt) where the latter can be computed using the approach of Makur et al. \n\nThe question of how to determine the relevance of a source task for a target task without learning is an important one with lots of previous work. The proposed method seems to be novel and brings many interesting ideas in Makur et al. with empirical validation. One comment is that the paper imports heavily from Makur et al. but does not make the imported definitions and results as clear as they can be. I am still unsure of what exactly the error exponent is: its definition should probably be defined in the main paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "In this paper, the authors considered source domain selection problem in transfer learning.",
            "review": "\nsummary:\nIn this paper, the authors considered a source domain selection problem in transfer learning. Given a feature representation function f, the H-score is defined as the normalized correlation between the output f(X) and the label Y. The transferability is then measured by the ratio of H-score on the target domain and the optimal one. The authors introduced the information-theoretic and statistical meaning of the H-score. Validation of H-score was confirmed by numerical experimenters with image data. \n\n\ncomments:\nApplication of H-score to transfer learning interesting. Numerical experiments using relatively large dataset were convincing to show the validity of the proposed method. The following is some minor comments. \n\n* Some notations and terms in section 2.1 were a bit hard to understand. e.g. what does \"the error exponent corresponding to f(x)\" mean? In particular, \"corresponding to f(x)\" was not clear for me. \n\n* The authors showed some relationship between H-score and error of the statistical test. It would be nice to show a more direct relation between H-score and test accuracy in transfer learning. Typically, the risk in the target domain (T) is bounded above by the risk in the source domain (S) plus some dispersion between T and S as shown in the following paper: \nShen, et al., Wasserstein Distance Guided Representation Learning for Domain Adaptation, AAAI (2018). \n\n* In the higher order transfer of numerical experiments, the concatenated features are employed. Showing a theoretical justification of such a concatenation would be nice. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}