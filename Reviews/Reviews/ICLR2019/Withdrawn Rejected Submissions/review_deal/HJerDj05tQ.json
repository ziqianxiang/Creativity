{
    "Decision": {
        "metareview": "The paper describes a constrained optimization strategy for optimizing on an intersection of two manifolds.  Unfortunately, the paper suffers from generally weak presentation quality, with the technical exposition seriously criticized by two out of the three reviewers.  (The single positive review is too short and devoid of content to be taken seriously.  Even there, concerns are expressed.) This paper requires substantial improvement before it could be considered for publication.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Paper requires further refinement"
    },
    "Reviews": [
        {
            "title": "a novel algorithm on optimization on multiple manifolds",
            "review": "The paper proposes a novel algorithm for optimization on multiple manifolds. The moving direction fuses gradient information from each manifolds via correlation. More importantly, the convergence is guaranteed.\n\nHowever, the empirical results seems not very good compared to SGD.\n\nMy concerns: \n\n1) How you ensure each step is descent? \n\n2) How is the performance of the proposed algorithm compared to the ADMM which is well-suited for this problem.\n\nThe presentation needs to be improved.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The problems addressed in the paper are interesting and crucial from both practical and theoretical perspectives. However, there are various major mathematical, conceptual and algorithmic problems with this paper.",
            "review": "\nThe title is misleading, since only two particular manifolds are studied in this work. In addition, the proposed methods cannot be applied to a larger or a general class of manifolds. Therefore, you should update the title.\n\nThere are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. In addition, some of the proposed problem definitions are incorrect, as explained below:\n\nYou should be more precise about the definition of the manifold you consider in this paper. For example, in equation (1), please define your manifold of interest more precisely checking some standard textbooks.\n\nPlease define intersection of manifolds, what do you mean by which intersection of which type of manifolds?\n\nIn the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.\n\nIn the contribution (2): It is not clear why and how the proposed method can be applied to optimization on manifolds with momentum (what do you mean by use of momentum here?), and regularization (what do you mean by regularization?). There are many problems with this claim, but you can simply consider that applying momentum and regularization will affect the geometry of loss landscape.\n\nDefinition of retraction is not precise, please fix it.\nWhat is L in equation (2)?\n\nPlease define neighborhood in U_x in Lemma 2.1.\n\nWhat is || ||in Lemma 2.1?\n\nAs you also noticed on page 3, an intersection of manifolds may not be a manifold. Then, your proposed first problem (1) fails. Therefore, you should completely change your claims on your problem definitions and contributions.\n\nWhat do you mean by “We add a drift which contains information from the other manifold to the original gradient descent on manifold”? What is “the information from the manifold”? In equations (3) and (4), you just apply optimization on manifolds individually. \n\nHow do you compute/determine a_k^(1) and a_k^(2)? How do they affect the theoretical and experimental results?\nIn your claim “From the construction of bk, we can see that the smaller the correlation between gradf(xk) and hk is, the smaller effect the information from M2 brings”, it is not clear how “the information from M2” affects? First, again, what is “the information”? Second, b_k^(1) and b_k^(2) are computed for individual manifolds separately. Then, how “the information” make an effect?\n\n In Theorem 2.2, what do you mean by “then xk convergence to a local minimizer“?\n\nWhat is <,> in Theorem 2.2?\n\nWhat is ^ in Theorem 2.3?\n\nWhat is v in proof 6?\n\nWhat is an engine value?\n\nWhat does P (1) xk gradf(yk) denote in computation of h_k? For example, gradf(yk) is a vector on tangent space of the second manifold at yk. Then, how do you project orthogonally this projected vector to the tangent space of the first manifold at xk? \n\nThey may be completely different geometries, and such an “orthogonal projection” may not exist in general. Then, how do you compute and calculate that projection?\n\nAll the theoretical results given in the paper are not about convergence of parameters on a manifold at the intersection or product of manifolds but for an individual manifold. For example, x and y belong to manifolds M1 and M2, and convergence results is about x. How are they related to parameters at the intersection or product of manifolds?\n\nThe statements regarding batch normalization are confusing and also sound incorrect:\n\nDo you apply batch normalization on weights on BN(w)?\n\nPlease explain what you mean by “BN(w) has same image space on G(1, n) and St(n, 1)“. There are not such results in the papers Cho & Lee (2017); Huang et al. (2017) you cited for these results.\n\nWhat do you mean by “applying optimization on manifold to batch normalization problem”?\n\nIn your statement “However, the property of these two manifold implies that we can actually optimize on the intersection of two manifolds”. Please explain how does this property imply this result more precisely?\n\nPlease define “Grassmann manifold G(1, n)“ more precisely. In your notation, together with explanation of the notation for St(n,p), G(1,n) is like a set of 1xn dimensional row vectors, while St(n,1) is an nx1 dimensional column vector, Then, their intersection is an empty set and your proposal for optimization on a vector on their intersection is wrong. \n\nNotation and definitions used in (9) are wrong and confusing. Please check and revise them.\n\nIn the whole paper, the problem, method, solutions, theorems, and contributions are proposed for optimization using parameters which belong to intersection of some manifolds. Then, suddenly, you start considering optimization on product manifolds, and give the results for that;\n\nWhat does the statement “Then we apply Algorithm 1 to update parameters, which means we optimize on a product manifold” mean?\n\nWhat do “G(1, k1) × · · · G(1, kn)” and “St(k1, 1) × · · · St(kn, 1)” denote?\n\nDon’t you perform optimization on intersection of manifolds? Why do you ignore your original problem and methods, and consider this problem? \n\nIn addition, how do you use your Algorithm 1 for optimization on product manifolds? Optimization on intersection on manifolds and product manifolds are completely different problems. If they are same or related to each in particular cases in your specific definitions, then you should provide these definitions more precisely.\n\nWhat do you mean by optimization on product manifold of weights of all layers? If you compute a product manifold for spaces of all layers, then you simply perform a shallow optimization on a huge matrix containing millions of dimensions according to this definition. First, how do you do that? Second, how can you train a large network using this approach?\n\nIn the experiments, please first give variance of errors. These results are statistically insignificant.\n\nWhich problem is solved to perform these experiments is not also clear (see above).\n\nThe results reported in the paper are also not good, may be due to the mathematical and algorithmic  problems and errors mentioned above. Please clarify them, and provide additional results, especially using other datasets (small scale mnist and large scale imagenet), and networks (mlp, vgg, resnet etc.)\n\nRelated work is also incomplete, such that many traditional and recent work on optimization on multiple manifolds are omitted. ",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. Therefore, I donot think this paper can be published at this stage.",
            "review": "This paper considers an optimization problem defined on the intersection of multiple manifolds and the intersection is not a manifold. An optimization algorithm is proposed and its convergence analysis is given. An experiment of neural network with batch normalizatiion is used to demonstrate the performance of the algorithm.\n\nThe problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. In addition, the convergence analysis is not complete. See more details below. Therefore, I donot think this paper can be published at this stage.\n\n*) P2, Section 2.1, line 2: The statement \"A manifold is a subspace of R^n$ is not true in general.\n*) P2, Section 2.1, line 7: The statement \"manifold is not a linear space\" is not true in general. A manifold can be a linear space, such as the vector space R^n.\n*) P2, Section 2.1, below (2): The statement \"Riemannian gradient is the orthogonal projection of gradient \\nabla f(x) ...\" is not true in general.\n*) P3, (3) and (4): what is the definition of $h_k^{(1)}$ and $h_k^{(2)}$. Are they arbitrary or the ones given on Page 4?\n*) P4, Theorem 2.3: the iterates {x_k} converges in the sense that \\|gradf(x_k)\\| goes to 0. Does {x_k} go to the intersection of the two manifolds \\mathcal{M}_1 and \\mathcal{M}_2? To complete the proofs, the author may need to show that \\|gradf(y_k)\\| goes to 0 and {x_k} and {y_k} have the same limit.\n*) P5, the grassmann manifold with p = 1: G(1, n), is called projective space, and the Stiefel manifold with p = 1: St(n, 1) is called the unit sphere.\n*) P6, the discussion of the intersection of G(1, n) and St(n, 1) does not make sense to me. G(1, n) is a quotient manifold, which is not a submanifold of R^n. Given a quotient manifold, the typical way in optimization framework is to choose representation of the quotient manifold. Fortunately, the projective space has a global orthogonal section, which is the unit sphere. In other words, G(1, n) is diffemorphisic to the unit sphere St(n, 1), and even can be isometric if appropriate Riemannian metrics are used on G(1, n) and St(n, 1). Therefore, I don't understand the notion of the intersection of G(1, n) and St(n, 1).\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}