{
    "Decision": {
        "metareview": "The paper proposes a new optimization approach for neural nets where, instead\nof a fixed learning rate (often hard to tune), there is one learning rate per\nunit, randomly sampled from a distribution. Reviewers think the idea is\nnovel, original and simple. Overall, reviewers found the experiments\nunconvincing enough in practice. I found the paper really borderline,\nand decided to side with the reviewers in rejecting the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting idea, does not seem to work consistently, limited theoretical explanation",
            "review": "\nThis work proposes an optimization method called All Learning Rate\nAt Once (Alrao) for hyper-parameter tuning in neural networks.\nInstead of using a fixed learning rate, Alrao assigns the learning\nrate for each neuron by randomly sampling from a log-uniform\ndistribution while training neural networks. The neurons with\nproper learning rate will be well trained, which makes the whole\nnetwork eventually converge. The proposed method achieves\nperformance close to the SGD with well-tuned learning rate on the\nexperiments of image classification and text prediction.\n\n\n#Pros:\n\n-- The use of randomly sampled learning rate for deep learning\nmodels is novel and easy to implement. It can become a good\napproximation of using SGD with the optimal learning rate.\n\n-- The paper is well-written and easy to follow. The proposed\nmethod is illustrated in a clear way.\n\n-- The experiments are solid, and the performance on three\ndifferent architectures are shown for comparison. According to the\nexperiments, the proposed method is not sensitive to the\nhyper-parameter \\eta_{min} and \\eta_{max}.\n\n#Cons:\n\n-- The authors have not given any theoretical convergence analysis\non the proposed method.\n\n-- Out of all four experiments, the proposed method only\noutperforms Adam once, which does not look like strong support.\n\n-- Alrao achieves good performance with SGD, but not with Adam.\nAlso, there are no experimental results on Alrao with other\noptimization methods.\n\n#Detailed comments:\n\n(1) I understand that Alrao will be more efficient compared to\napplying SGD with different learning rate, but will it be more\nefficient compared to Adam? No clear clarification or experimental\nresults have been shown in the paper.\n\n(2) The units with proper learning rate could learn well and\nconstruct good subnetworks. I am wondering if the units with \"bad\"\n(too small or too large) learning rate might give a bad influence\non the convergence or performance of the whole network.\n\n(3) The experimental setting is not clear, such as, how the input\nnormalized, how data augmentation is used in the training phase,\nand what are the depth, width and other settings for all three\narchitectures.\n\n(4) The explanation on the influence of using random learning rate\nin the final layer is not clear to me.\n\n(5) Several small comments regarding writing:\n    (a) Is the final classifier layer denoted as $C_{\\theta^c}$ or  $C_{\\theta^{cl}}$ in the third paragraph of \"Definitions and notations\"?\n    (b) In algorithm 1, what is the stop criteria for the do while? The \"Convergence ?\" in the while condition is confusing.\n    (c) Is the learning curve in Figure 2 from one run or is it the average of all runs? Are the results consistent for each run? How about the learning curves for VGG19 and LSTM, do they have similar learning curves with the two architectures in Figure 2?\n    (d) For Figure 3, it will be easier to compare the performance on the training and test set, if the color bars for the two figures share the same range.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "not convinced it's worth it",
            "review": "POST REBUTTAL: I think the paper is decent, there are some significant downsides to the method but it could constitute a first step towards a more mature learning-rate-free method. However, in its current state the paper is left with some gaping holes in its experiment section. The authors tried to add experiments on Imagenet, but these experiments apparently didn't finish before the end of the rebuttal period. For that reason, the paper probably should not be accepted for publication (even if the authors manage to finish running these experiments, we would not have a chance to review these results). \n\n------\nOriginal review\n------\n\nIn this paper, the authors present a method for training deep networks with randomly sampled feature-wise learning rates, removing the need for fixed learning rates and their tuning. The method is shown to perform comparatively to SGD with a learning rate roughly optimized with regards to validation performance. The method applies to the most popular types of deep learning architectures, which includes fully connected layers, convolutional layers and recurrent cells. \n\nQuality: The paper is of a decent quality in general, I noticed no glaring omissions while reading the paper. However, I do worry that the method provides little gain for a lot of work. It is becoming more and more easy to tune the learning rate of deep learning models with strategies such as early stopping, and this method comes at a high cost for models with a big final layer. \n\nClarity: The paper is well written, but the reader is often (too often?) sent to the Appendix, which is itself ordered in a strange way (e.g., the first reference to the Appendix in the paper refers to Appendix F?). If some sections of the Appendix are not needed, I would remove them. \n\nOriginality: The work is original in the approach, i.e. randomization as a way to get rid of learning rates is a novel method. However, there was one work presented last year at NIPS which concerns itself with the same problem, which is getting rid of learning rates:\n\n“Training Deep Networks without Learning Rates Through Coin Betting” by Francesco Orabona and Tatiana Tommasi, NIPS, 2017.\n\nThey don’t compare on the same methods and the same datasets, but I think the authors should be aware of this work and perhaps compare themselves with it. The work takes a very different approach to solve the problem so I don’t think it’s an issue for this paper. \n\nSignificance: I think the work is important, in that it adds another tool to solve the learning rate problem. I would not say it is likely to have a very high impact, because it involves a lot of work, for little benefit. Furthermore, the cost of reproducing multiple times the last layer of the network will be prohibitive in many cases for NLP. \n\nThe method feels ad-hoc in many respects, and there are no guarantees that it would work any better than Adam does on pathological cases. Perhaps some mathematical analysis on simpler problems would help make the contributions stronger. \n\nThe authors state that the learning rate range has little impact on performance, yet it still has enough impact to justify tuning it for different models and datasets (on CIFAR it is 10^-5 to 10^1, on Pennbank it is 10^-3 to 10^2). I would tend to agree that the alrao method is more robust to the choice of learning rate than plain SGD, however the fact of the matter is that there are still parameters to tune. \n\nFigure 5. also seems to suggest that the range is important, although the models were not trained until the end, so it is not clear.\n\nSome additional comments:\n\nNitpicking: In Section 2, most sub-sections (or paragraphs titles?) have the name of the method in them. That’s redundant. Instead of “Alrao principle”, “Alrao update”, etc., just write “Principle.”, “Update.”.\n\nIs there a justification for using the same learning rate for all weights in an LSTM unit? \n\nI believe there is a mistake in Equation 2. The denominator should be log(\\eta_{max}) - log(\\eta_{min})\n\n[second paragraph on page 4.] Once again nitpicking for the sake of clarity: “For each classifier Cθ cl j, we set a learning rate log eta_j = …” this reads as if the learning rate would be set to log eta_j, but you probably mean you will set the learning rate to eta_j = exp(...).\n\nFigure 5b in the appendix does not specify which curve has which learning rate interval. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting method, but more experimental results are needed",
            "review": "In this paper the authors propose a method called “All learning rates at once” (Alrao) which aims to save the time needed to tune learning rate for DNN models testing. The method sets individual learning rate to each feature in each layer of a network using the values sampled from truncated log-uniform distribution. The only cost of the method is the creation of several branches of the classifier layer. Each of the branches is trained with a predefined learning rate value, and the final predictions are obtained by model averaging. In the presented experiments Alrao demonstrates performance comparable to SGD with optimal learning rate and more stable results compared to Adam. The authors indicate limitations of Alrao caused by the overhead in the final layer which complicates the application of the method for models with large classifier layer.\n\nOverall, the paper is written clearly and organized well. However, Equation (2) needs to be corrected. The denominator in the normalizing constant of log-uniform distribution should be \\log\\eta_{max} - \\log\\eta_{min}.\n\nMy main concern is related to the experimental evaluation of the method. I find the experimental evidence for the effectiveness of Alrao insufficient. As the authors propose to employ the method to quickly evaluate models and select best models to further training it would be beneficial to have more results in order to ensure that the method is reliable in this setting. Other demonstrations which would show possibly that the method enhances performance of architecture search methods may emphasize significance of the proposed method. Also, more experiments comparing Alrao against sampling learning rates per weight are needed. Given the current results, it is still unclear whether the proposed method performs better. Finally, I recommend to include comments explaining how much more time is needed in practice to train model with Alrao compared to SGD training.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}