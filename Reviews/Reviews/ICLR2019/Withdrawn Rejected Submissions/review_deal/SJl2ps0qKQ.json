{
    "Decision": {
        "metareview": "+ an interesting task -- learning to decompose questions without supervision\n\n- reviewers are not convinced by evaluation. Initially evaluated on MetaQA only, later relation classification on WebQuestions has been added.  It is not really clear that the approach is indeed beneficial on WebQuestion relation classification (no analysis / ablations) and MetaQA is not a very standard dataset.\n\n-  Reviewers have concerns about comparison to previous work / the lack of state-of-the-art baselines. Some of these issues have been addressed though (e.g., discussion of Iyyer et al. 2016)\n\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "interesting directions / results are not very convincing"
    },
    "Reviews": [
        {
            "title": "Good paper, but need more related work discussions",
            "review": "Summary: the paper is interested in parsing compound questions for querying on knowledge graph, e.g. MetaQA by Zhang et al. (2017). The paper proposes to have two modules, one that segments the question into partitions (up to three) and the other that looks at each segment to get the relation. The relations are merged to obtain a single KG path, which is queried to obtain the answer. Since the segmentation is a non-differentiable process, the paper uses reinforcement learning to propagate gradient to the segmentation model. The segmentation is a process of classifying each word for which partition it should be tied to. Answering is a process of classifying the partition into one of the possible relation edges. The model shows expected results in a synthetic arithmetic dataset, and obtains the state of the art in MetaQA, improving nearly 5% over the baseline. The model especially does much better on 3-hop questions, with nearly 20% improvement.\n\nStrengths: the paper is well-written. The model is simple yet effective and is a novel contribution to compound question answering on KG. Especially, the improvement on 3-hop category is nearly 20%, which is substantial and quite impressive. \n\nWeaknesses: My biggest concern is the lack of discussions on its relevance to  (Iyyer et al., 2016), which also proposed to decompose question into simpler ones for WIkiTableQuestions. Also, I think it would be good to mention Semantic Role Labeling as related literature, which is about tagging each word with its role in the sentence. The partition index can be somewhat considered as a “role” in the sentence.\n\nQuestions:\n1. How do you obtain x^(k)? Is it the last state of the LSTM?\n2. Why did you have to augment “NO_OP” relation in the MetaQA dataset?\n3. Why +1 reward has lower variance than probabilistic reward? Explanation or citation would be needed.\n4. What if two partitions need to share a word? The current setup necessitates that a word participates in only one partition. Wouldn’t this be problematic?\n5. I am a bit confused about how the simple question answering module is trained. Is it directly trained by the gold relation label?\n\nTypos and Suggestions:\n- Second paragraph of 2.1: in stead -> instead\n- Third paragraph of 2.1: research. -> research\n- c_t + h_t: would be good to explicitly mention that the circled plus sign is concatenation.\n- Last paragraph on page 4: “leave to be”?\n- Second last paragraph of 4.1: he -> The\n- Second paragraph of 4.2: “if exists a proper meaning”?\n- First paragraph of page 7: be either assume -> either assume\n- Last paragraph of Section 5: generalizing -> generalize\n- I think you should not put acknowledgment in a double-blind submission.\n\nM Iyyer, W Yih, MW Chang. Answering complicated question intents expressed in decomposed question sequences. 2016 (https://arxiv.org/abs/1611.01242)\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea. Lacking technical details and error analysis.",
            "review": "This paper proposes a knowledge-based QA system that learns to decompose compound questions into simple ones. The decomposition is modeled by assigning each token in the input question to one of the partitions and receiving reward signal based on the final gold answer. The model achieves the state-of-the-art performance on the MetaQA dataset. \n\nMy main complaint about the paper is its lack of technical details and analysis of empirical results. Parts of the paper seem quite unclear, for example:\n\nIn the last paragraph of Section 3.1, it says “We do not assume that nay question should be divided into exactly three parts. … See section 4 for case study.” Does this mean that the model can have <=3 partitions, but not more? How is this number decided?\n\nSection 3.2 describes the simple-question answer. From Eq (4), it seems that the answerer only uses the current partition, is that the case? Moreover, how is the gold relation r obtained?\n\nIt would be nice to add more explanation to the caption of Figure 4 to make it self-contained.\n\nThe case study section (4.3) only contains a single example. It would be very helpful to include more examples of question partitions (there is enough space). Error analysis would also be helpful to understand, for example, why the proposed model is worse than VRN (Zhang et al. 2017) on 1- and 2-hop questions.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Lack of comparison with previous state-of-the-art methods over more widely used benchmarks",
            "review": "This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.\n\nThe proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.\n\nI like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between \"compound questions\" instead of \"multi-hop questions\". In my opinion, \"multi-hop questions\" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.\n\nMy only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:\n\n(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).\n\n(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include \"Search-based Neural Structured Learning for Sequential Question Answering\" and \"ComplexWebQuestions\". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}