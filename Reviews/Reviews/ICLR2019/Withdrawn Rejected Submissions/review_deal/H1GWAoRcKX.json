{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not convinced by the idea of using fewer frames to obtain the same representation for longer frames",
            "review": "Summary: The paper proposed an idea to distill from a full video classification model a small model that only receives smaller number of frames. \nQuality: \n- The paper needs to be carefully proofread. For example, \"classification less\" -> \"classification loss\".\n- I am not fully convinced by the proposed idea. If only a partial number of frames are observed, how could it achieve the same representation in theory? The proposed method can provide sub-optimal solution but the variance of the accuracy reduction might be huge.\n- The paper claims the teacher can be any video models and the paper uses a recurrent model as the teacher. However, there are models not recurrent, such as the I3D model which essentially models a small number of frames and then aggregate them together. So for those models, it is hard to see the value of the proposed method.\n\nClarity: \n- The idea is easy to catch but clarity of the technical part can be improved.\n- How is the small model used? Do you directly use partial videos to do classification or still need to aggregate the small models?\n\nOriginality: \n- I am not aware of existing works trying to distill a long-frame model to short-frame models.\n\nSignificance: \n- The contribution is incremental and the results are not significant. The uniform-k baseline already achieved a decent result while the proposed method added around 3% by increasing the system complication dramatically.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " Review for \"A Teacher Student Network For Faster Video Classification\"",
            "review": "The authors present a teacher-student network to solve video classification problem, aiming at reducing computational cost. Specifically, the authors proposed two training algorithms, serial and parallel training. They showed the proposed approach can reduce inference time up to 30% on YouTube-8M dataset, taking a step forward to on-device classification.\n\nRelated work:\nThe YouTube-8M dataset has been updated this year, and the team hosted the 2nd video classification challenge and workshop at ECCV'18. As they limited the model size up to 1GB this year, many participants actually applied similar idea (distillation). They have posted a summary paper and all submitted papers, so I recommend the authors to refer those papers and summarize in the related work section.\n\nJ. Lee, A. Natsev, W. Reade, R. Sukthankar, G. Toderici. The 2nd YouTube-8M Large-Scale Video Understanding Challenge, ECCV workshop 2018.\n\nSee the list of accepted papers in https://research.google.com/youtube8m/workshop2018/\n\nThe authors motivated the work from high computational cost with RNN/LSTM-based models, claiming that they are the state-of-the-art. However, there are other computationally cheaper but still powerful approaches, such as temporal convolution or learnable pooling. It will be better to introduce these other lines of work to solve video classification problem.\n\nExperiment:\nThe authors conducted experiment mostly on YouTube-8M dataset, but did not specify which version they used. Given the dataset stats, it seems like their very first version with 8.2M videos. According to their update note (https://research.google.com/youtube8m/download.html), this version is most noisy, so they recommend to use more recent version. Also, it will be beneficial to use 2017 or 2018 version to compare against Kaggle challenge and corresponding workshop participants (CVPR'17 and ECCV'18, respectively) directly. As the reported score is based on the first version, it is not comparable to the state-of-the-art models in both workshops. (The first-place team achieved ~0.85 and 0.87+ each year, while this paper reports 0.806 as the best score. It is not directly comparable, as the training and eval set are not the same.)\n\nOther than this, the experiment was well designed, and it is also good to report both GAP and MAP. In video classification problem, examples are usually highly skewed among classes, so it is useful to evaluate with MAP as well, in order to verify if it performs well for rare classes as well as common ones. In Table 2, MAP score shows bigger gap than GAP, so it will be interesting to explain this phenomenon in more details.\n\nThe teacher model tends to be weaker, compared against the best performers at ECCV'18 participants. To fully take advantage of knowledge distillation, it makes more sense to make the teacher with ensembles of multiple models to maximize its performance. At the workshop, participants had to make the final model size less than 1GB, so it will be interesting to compare the model size (~number of parameters) as well.\n\nOverall, the idea presented in this paper would have been more interesting if it was submitted before ECCV'18. Knowledge distillation for video classification is no longer a novel idea unfortunately, so I encourage the authors to catch up with the recent work published through YouTube-8M workshop at ECCV, and propose more distinguishable work compared against to them.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not enough novelty and missing key references",
            "review": "This paper proposes a knowledge distillation based framework to train an action recognition model with fewer video frames as input. I don’t think the paper is novel enough as a number of knowledge distillation works exist which are closely related; some of them are on training efficient networks for action recognition.\n\nPros:\n\n•\tIt is interesting to see how compact deep neural network can be trained with knowledge distillation.\n\nCons:\n\n•\tMy main concern is the limited novelty of the work. The knowledge distillation approach proposed in this work (a combination of feature alignment and prediction KL loss) is a standard one and has been applied to a variety of vision problems.\n•\tIn particular, the parallel training has been used in Y. Zhang, T. Xiang, T. Hospedales and H. Lu, \"Deep Mutual Learning\", CVPR'2018. In fact, that papers shows that if the teacher and student, or peers, teach each other, rather than the teacher-to-student one-way traffic, it is more effective. \n•\tFor action recognition, this paper: B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang, “Real-time action recognition with enhanced motion vector cnns,” in CVPR, 2016, proposed something very similar: instead of teacher network having more frames as input, in their work, the teacher network has access to the more expensive optimal flow modality, whilst the student network uses the motion vector as a by-product of video compression (hence free). Apart from this difference, the formulation is very similar. This paper should certainly be cited. \n•\tApart from the novelty, the experiment is also limited in that only one dataset is used. The hierarchical RNN model may be effective on the YouTube-8M dataset, but other models such as the two-stream model from Simonyan and Zisserman and I3D from Carreira and Zisserman are popular on other datasets such as Kinetics. It would be useful to see some additional experiments on another benchmark with a different network architecture. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}