{
    "Decision": {
        "metareview": "The paper introduces a new dataset that contains multiple landings from the X plane simulator, and each includes readings from multiple sensors for aircraft landing. The  paper also trains a set of self-supervised methods presented in previous works in order to learn sensory representations, and evaluates the learnt representations in terms of disentanglement and re-purposing to a discriminative task. \n Though the evaluations presented are interesting, they are not convincingly useful, as noted by the reviewers. Overall, it is not clear why this dataset is particularly well suited for representation learning. Furthermore, it is difficult to evaluate representation learning methods without relating them to an end-task, e.g., that of landing the aircraft. \nThe paper writing would also benefit from restructuring and improving on English expressions. In particular, the conclusion section contains half-finished sentences.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "introducing a non conclusively useful dataset"
    },
    "Reviews": [
        {
            "title": "a new dataset and evaluation framework for learning representations for landing an airplane",
            "review": "Overview and contributions:\nThe authors present a newly collected dataset and evaluation framework for learning representations for landing an airplane. The dataset is collected from the X-Plane simulation environment and consists of 8011 landings, each landing consists of time series data from 1090 sensors. Their evaluation metric is a combination of disentanglement score, regression tasks, and failure classification. The authors test a combination of baseline models from basic autoencoders to dynamic actions-aware encoders. The writing is generally clear but I have doubts about the proposed evaluation metrics, experiments, and significance of the dataset (details below).\n\nStrengths:\n1. The task seems to be novel and complex. The authors have done a good job of collecting the dataset and ensuring that the data is clean and comprehensive.\n2. The authors have performed a comprehensive job of evaluating the combinations of baseline models for their proposed task.\n\nWeaknesses:\n1. Table 4 on evaluation results, while comprehensive, lacks some explanation. The issue with MSE is that it is hard to interpret what these values mean. Specifically, how difficult is this task? How well can a human perform on this task? How well are the baselines doing relative to human-level performance, and is there room for improvement? The answers to these questions are important towards whether this new dataset will be a strong benchmark for representation learning.\n2. There is less novelty in terms of the models presented for evaluation since they are composed of existing models. What are some state-of-the-art models for similar tasks, and do they constitute fair comparison?\n\nQuestions to authors:\n1. Refer to weakness points 1 and 2.\n2. What biases do you think might exist in the dataset during the collection process? How might these biases affect what the models learn, and how can they be mitigated?\n3. How do you ensure that all sensors are active at all times and that all sensors provide useful information for predicting the label? Are there cases where the multisensor data is noisy in certain modalities or missing in other modalities? If so, what are some models that can remain robustness to noisy or missing modalities?\n4. Why do you think disentangled representations will help? Sure, they have been generally shown to help learn more interpretable representations, and help in flexible generation from disentangled factors. But in terms of discriminative or generative performance on your newly proposed dataset, does learning disentangled representations help? What are some models that can learn effectively learn such disentangled representations?\n\nPresentation improvements, typos, edits, style, missing references:\nSection 3, line 7, 'with with a frequency' -> 'with'\nI would suggest referring to some recent work on multimodal temporal fusion, such as \"Memory Fusion Network for Multi-view Sequential Learning. Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, Louis-Philippe Morency, AAAI 2018\"\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "main contribution: contribution of multi-modal dataset, evaluation code for learning representation tasks and results on dataset",
            "review": "The paper looks into contribution of data set for multi-modal learning using X-Flight simulator in various settings. The authors also contribute code for evaluation of the learning representation tasks and present the results for the data using various setups from autoencoders to dynamics model, using sensor only data and combining image and sensor data, and predicting various timesteps.\n\nImprovements\nMultimodal datasets have been made available previously in Image, video, text combinations, where the outcome was clear (for e.g learning caption etc.), however, in this dataset, the task is more challenging (for e.g predicting the various sensor readings or landing outcome). The paper would benefit from \n- adding clarification on the Learning tasks, as some of the descriptions/settings and result discussion need more explanation. An e.g predicting the timesteps ahead can be meaning different things, depending on when the start time is, sampling rate and the time to land. \n- measure of the scale where only MSE is mentioned for the tasks in the results\n- why the time with lower latent dimensions was same as with higher\n- the explanation for some of the measure being out of whack for some settings is attributed to challenges with the data set and e.g. is provided for images with nighttime landing. A quantitative number around such cases/for the e.g. in the training data, and test data would be good\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially impactful work, but lack clarity",
            "review": "The work releases a large-scale multimodal dataset recorded from the X-Plane simulation, as a benchmark dataset to compare various representation learning algorithms for reinforcement learning. The authors also proposed an evaluation framework based on some simple supervised learning tasks and disentanglement scores. The authors then implemented and compared several representation learning algorithms using this dataset and evaluation framework. \n\npros:\n1.  Releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly;\n2. The authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores;\n3. The authors implemented an extended list of representation learning algorithms and compared them on the dataset;\n\ncons:\n1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework. The authors spent almost half of the space explaining different existing representation learning algorithms. A more convincing story would be to find a few well-established representation learning algorithms to corroborate on the reliability of the dataset and the evaluation metrics;\n2. More details should be put into describing the dataset. It is not clear why this dataset is particularly suited for evaluating representation learning in the context of reinforcement learning. Do the authors have insight on the difficulty of the task? While having multi-modality is appreciated, it might worth thinking a separate dataset focusing on a single modality, e.g., image;\n3.  Given that the authors designed the dataset for evaluating representation learning for reinforcement learning, it is worth evaluating these algorithms on solving the main task using some standard RL techniques on top of the learned representations.\n4. Table 4 is difficult to parse. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}