{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper proposes a new way to utilize the discriminator in adversarial learning. The method seems reasonable and effective, but the paper writing needs significant improvement.",
            "review": "Pros:\n1) The paper utilizes the concept of adversarial learning for eliminating unwanted information from the original input and treats domain adaptation and noise removal as two cases. Such a viewpoint is appreciated.\n\n2) A new way of utilizing the discriminator S to update the encoder---by forcing the features to be orthogonal to the classifier weights---is proposed. The method seems more stable.\n\nCons:\n1) The paper needs significant rewriting. First of all, the motivations are not clear. The introduction simply ignores the problem to be resolved. The abstract (as well as Section 2) looks like a plain description of the technical details instead of highlighting the motivations and contributions. The related work should include discussions on both noisy removal and domain adaptation.\n\n2) The main difference between the proposed method to existing adversarial learning methods is in how to use the information form the discriminator S. To give a more meaningful comparison, I would suggest the authors having a section detailedly comparing the math formulations to support the claim. I would also suggest the authors looking into other models/problems where adversarial learning is being used and see if the proposed idea can be applied---this will make the proposed method widely applicable and have more impact. \n\n3) Section 4 needs improvement.\n3-1) From Fig 3, it seems all the compared methods already perform pretty good. Data from each speaker are already clustered very well.\n3-2) It would be great to have tables summarizing the essential statistics of datasets being used.\n3-3) For Table 2, more compared baselines are needed.\n\nOther comments:\n1) The authors mentioned that Yu's 2017 method is not applicable to the experiments. Then how do they still implement it in Table 1?\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Confusing writing and limited novelty",
            "review": "The authors propose a training procedure to avoid models relying on \"subsidiary information\" while being optimized for a certain task, through a cosine similarity based adversarial process. The authors propose to use this measure to force orthogonality between the features extracted and the weights associated with a subsidiary task.\n\nWhile the idea of removing information not related to the main task of a learning system in order to better generalize is an interesting direction, the novelty of this paper is limited, and the current version contains several flaws. \n\nFirst of all, the paper is very confusingly written. A proper problem formulation, with a more formal definition of what \"subsidiary information\" is, is necessary to walk the reader through the paper. For instance, the introduction is extremely scarce of details (paradoxically, the abstract has more), and there is a logic gap between the description of Generative Adversarial Networks (GANs) and the introduction of the cosine similarity method. The authors write that it \"differs from existing GANs in that it trains the discriminative model by using the cosine similarity-based adversarial process\"; this is a confusing statement, as the method proposed by the authors is not a generative model. At most, this is what makes the proposed method differ from Ganin et al. (ICML2015).\n\nA proper description of the training procedure is missing. How subsidiary information is eliminated is clarified by Figure 2, but it is not clear how and when E and M are trained to perform the primary task. Does this happen only before the iterative procedure? Observing Figure 2, it seems that E and M are never trained to perform the primary task of interest while running the iterative procedure. This seems wrong, because in the described in Figure 2.b the weights of the encoder E are updated and thus also the feature space on which M was trained is changed. Also, what is the stop condition of the iterative procedure? How can one determine if the subsidiary information has been eliminated or not from the codes? Summarizing, the overall training procedure needs to be better detailed. Perhaps with the aid of an Algorithm box to detail the procedure step-by-step? \n\nThis statement of related works confused this reviewer: \"in particular, our proposed CAN framework is designed to work without target samples\". Do the authors mean that in the speaker recognition experiment the target is not used? Because it certainly is in the unsupervised domain adaptation experiments, by definition of the problem formulation. If the target is not used in the speaker recognition experiment, a suggestion to the authors is to read something from the literature about \"domain generalization\".\n\nConcerning unsupervised domain adaptation, the related work section is not complete, the authors only cite Ganin et al. (ICML2015) and Tzeng et al. (CVPR2017), but the literature is not limited to these two works. The authors should include all the relevant works and provide the connections between the proposed method and the related method to let the reader fully understand the contributions. This also makes Table 2 incomplete, as the authors only compare their method against ADDA. It should include at least more recent, effective approaches, which perform substantially better on the faced tasks.\n\nFurthermore, since the proposed approach in unsupervised domain adaptation is very related to Ganin et al. (ICML2015), where the domain information is eliminated from the features through adversarial training, the two methods should be compared in a fairer fashion. In particular, they should be compared using the same network architectures and possibly with the same settings (Leaky ReLU, L2 normalization). This experiment would make the reader better understand and appreciate the pros (and, eventually, cons) of using cosine similarity instead of vanilla adversarial training. At present, the effectiveness of using cosine similarity is not clear.\n\nDue to the aforementioned issues with the current version, this reviewer does not recommend this work for acceptance to ICLR 2019.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A borderline paper. ",
            "review": "The authors proposed a deep framework that incorporating adversarial training process for eliminating the subsidiary information from the input data. In particular, to remove the subsidiary context, a main task neural network M and a subsidiary task neural networks are trained in an alternative way. With that, the authors present intuitive results on some real-world tasks, such as domain adaptations, and reducing channel information. In general, the paper is well-organized and clearly represented. Despite this, there lacks some insightful analysis of the proposed approach, and the empirical studies should be enriched. In particular:\n(1) the authors claimed that \"the performance of S degrades, and the performance of M increases, consequently. \", which is arbitrary and not well motivated. As the subsidiary information is removed from E, why the performance of S will degrade? \n(2) for the proposed objective function, more discussion should be provided regarding the intuition. The loss function is minimized over the cosine similarity between the E and W_i^{sub} for each class i. It is unclear whether useful information is also eliminated from the embedding E, especially some subsidiary is either relevant or correlated with the main task space.\n(3) why L2 normalization can increase the efficiency of the proposed framework?\n(4) I would expect the authors could provide more empirical studies over more benchmark datasets and comparing with more baselines. It will largely convince the readers with such experimental results. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}