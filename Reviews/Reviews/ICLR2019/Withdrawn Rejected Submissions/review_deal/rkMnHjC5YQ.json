{
    "Decision": {
        "title": "meta-review",
        "metareview": "The reviewers seem to reach a consensus that the contribution of the paper is somewhat incremental give the prior work of Goel et al and that a main drawback of the paper is that it's not clear the similar technique can be applied to multiple **convolutional filters**. The authors mentioned in the response that some of the techniques can be heuristically applied to multiple layers, but the AC is skeptical about it because, with multiple layers and multiple convolutional filters, one has to deal with the permutation invariance caused by the multiple convolutional filters. (It's unclear to the AC how one could have a meaningful setting with multiple layers but a single convolution filters.) ",
        "recommendation": "Reject",
        "confidence": "5: The area chair is absolutely certain"
    },
    "Reviews": [
        {
            "title": "Interesting theoretical study of One-hidden-layer Conv Nets",
            "review": "This paper studies the theoretical learning of one-hidden-layer convolutional neural nets. The main result is a learning algorithm and provable guarantees using the algorithm.  This result extends previous analysis to handle the learning of the output layer weights, and holds for symmetric input distributions with identity covariance matrix.\n\nAt a high level, the proof works by using the non-overlapping part of the filter to reduce the problem to matrix factorization. \nThe reduced problem corresponds to learning a rank-one matrix, from which one can learn the output layer weight vector approximately. Given the output weight vector, then the hidden layer weight is learnt using the Convotron algorithm from previous analysis. I think that the technical contribution is interesting.\n\nWeakness: Given the existing work (Goel et al. 2018), I am concerned that the current work is a bit incremental. Secondly, it is unclear if the technical insight has any applications or not. How does the proposed algorithm work on real world data? Even some simple comparisons to other algorithms on a few datasets would provide insight.\n\nQuestion: Where does Assumption 3.2 arise in the proof? Is it necessary (for the proof)?\n\nOther issues: A few typos you may need to fix (e.g. the S notation in Thm 3.1, first sentence in Sec 4.3).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This is a theoretical paper investigating learning a one-hidden-layer CNN with overlap ",
            "review": "I believe the authors need to give more intuition on the importance of such a study, and how it can lead to improvement in real life application.\nThe work seems interesting but is limited and as the authors mentioned it might be a good start for further investigation. However, what I really wanted to see was a simple comparison on a dataset like MNIST with conventional CNN being trained via SGD, for example.\nAlso, there are some small typos you may need to fix, e.g \"will be play\" -> \"will be playing\".",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "review",
            "review": "This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations. The algorithm works on any symmetric input data. The techniques in this paper combines two previous approaches: 1. the algorithm Convotron for learning a single convolutional filter (while the second layer has fixed weight) on any symmetric input distributions; 2. non-convex optimization for low rank matrix factorization.\n\nThe main observation in the paper is that if the overlap in the convolutions is not large (in the sense that each location of the convolution has at least one input coordinate that is not used in any other locations), then the weight that corresponds to the non-overlapping part and the weights in the second layer can be computed by a matrix factorization step (the paper gives a way to estimate a gradient that is similar to the gradient for a linear neural network, and then the problem is very similar to a rank-1 matrix factorization). After this step, we know the second layer and the algorithm can generalize the previous Convotron algorithm to learn the full convolutional filter.\n\nThis is an interesting observation that allows the algorithm to learn a two-layer neural network. On the other hand this two layer neural network is still a bit limited as there is still only one convolutional filter, and in particular there is only one local and global optimum (up to scaling the two layers). The observation also limited how much the patches can overlap which was not a problem in the original convotron algorithm. \n\nOverall I feel the paper is interesting but a bit incremental.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}