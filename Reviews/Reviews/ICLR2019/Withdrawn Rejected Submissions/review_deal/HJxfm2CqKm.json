{
    "Decision": {
        "metareview": "This paper provides further insight into using RL for active learning, particularly by formulating AL as an MDP and then using RL methods for that MDP. Though the paper has a few insights, it does not sufficiently place itself amongst the many other similar strategies using an MDP formulation. I recommend better highlighting what is novel in this work (e.g., more focus on the reward function, if that is key). Additionally, avoid general statements like “To this end, we formalize the annotation process as a Markov decision process”, which suggests that this is part of the contribution, but as highlighted by reviewers, has been a standard approach. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "More focus is needed on what is novel in this work"
    },
    "Reviews": [
        {
            "title": "An intuitive combination of reinforcement learning and active learning.",
            "review": "Summary:\nThis paper presents an RL approach to active learning that is generic across ML model being learned, and across dataset being used. The paper formulates the standard active learning problem as an MDP with the objective of minimizing the number of annotated labels required to meet a pre-specified prediction quality. \n\nThe MDP state proposed by this paper is the current performance score on each sample in a hold-out set. The actions are specified by selecting a datapoint from the set of all un-annotated datapoints. The action feature vector consists of the current performance score of the model on the datapoint, and the average distance of that datapoint from every datapoint in the labeled set and every datapoint in the unlabeled set.\n\nReview:\nI do not recommend this paper for publication in ICLR because I believe:\n1) the work is too incremental\n2) the comparison to baseline and competing methods is incomplete\n3) some design decisions of the proposed method are not well motivated.\n\nI appreciated the clarity of the writting, and the paper organization. I also believe that the proposed method is quite intuitive, and is a good addition to the field. Finally, I appreciate that sufficient experimental details are available within the paper to be able to easily reproduce the results.\n\nDetails:\nMy points (1) and (2) are highly related, so I will discuss both simultaneously. I find that this paper makes only incremental forward progress from the Pang 2018 paper and the Konyushkova 2017 paper. The methodology here looks very similar to the SingleRL method, which Pang 2018 notes can be considered a special case of Konyushkova 2017's method. I think that the work in this paper would be sufficient to stand on its own if it performed a convincing comparison to SingleRL and/or MLP-GAL from Pang 2018. I recognize that this paper references why no such comparison currently exists, but I think this comparison would be extremely valuable to the paper.\n\nA further comment on my point (2), I do not find the comparisons to baseline methods to be entirely convincing. Of note, only the average performance for each method is reported. I'm curious of the variance---and more specifically the standard error and number of independent runs---of each of the reported results. On many of the datasets, the performance difference between the proposed method and uncertainty sampling is quite small in table 1.\n\nA final comment on point (2): I would have liked to see more exploration of different models. I think table 2 is quite informative, showing notable differences between simple baseline AL methods. I would have liked to see table 2 with more classifiers and with more competing AL methods. Because logistic regression is a simple model, the differences between AL methods may be more subtle. Perhaps a more complex model (say a single hidden layer NN) would show more notable differences.\n\nFor point (3), I would have liked to see either an exploration of other design decisions or an explanation of given design decisions. For instance, why only use 30 hold-out samples for the state? I imagine the proposed method would be fairly sensitive to this choice.  Another unexplained design decision was using a maximum budget of 100 datapoints. Table 2 shows some extremely interesting interactions with this budget in its comparison between LogReg-100 and LogReg-200, and further explanation would have been useful. Finally, I would have liked to see some motivation for choice of stopping condition. Using the stopping condition of 98% of maximum performance may have some biasing effect of each method, and it would helpful to have some motivation behind this choice.\n\nQuestions:\n - Why did uncertainty sampling have such limited benefits on LogReg-200 in table 2? This was a surprising result to me, as uncertainty sampling consistently outperformed most other methods.\n - Why is there a disparity between the results for the SVM in table 2 and the discussion in the first paragraph of section 4.3?\n - How does choice of final performance metric affect all methods? Choosing final performance to be 98% of maximum performance could have a major effect on each method. Because the proposed method is non-myopic, I would expect that it performs well when this value is large but would perform poorly with a smaller percentage of maximum performance.\n - Is the proposed method sensitive to number of samples used to compute the state?\n - What does figure 1 show? Are the same 30 samples used for all three subfigures? Perhaps this would more interpretable if, instead of showing the predicted class, this figure showed the prediction error.\n\nMinor nitpicks (did not influence decision):\n - The datasets are 1-based indexed sometimes and 0-based indexed sometimes, even with disparities within a single paragraph.\n - Figure 1 appears a long time before it is discussed, which made it difficult to understand what was going on.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A Reinforcement Learning approach to Active Learning",
            "review": "The authors suggest to model active learning (AL) as a Markov Decision Process to try to learn the best possible AL strategy across related domains. \n\nThe paper is well-written and structured -- although the background section could be expanded. Sec 3 presents the method in a clear and straightforward manner. \n\nMy main concern with regards to the paper is novelty. The authors mention two main contributions, the first one being to defined the AL objective to minimize the number of annotations required to achieve a given prediction quality, instead of maximizing performance given an annotation budget. There has been AL approaches from that perspective in the past (e.g., https://arxiv.org/pdf/1510.02847.pdf). \n\nThe second contribution has to do with a procedure to learn the AL strategy using data from different domains (with available labels). Again, the literature in transfer learning in Reinforcement Learning is extensive and should be discussed. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "OK paper. Well written, but weak novelty.",
            "review": "Summary: This paper studies the recently problem of learning active learning (LAL). It sets up a MDP where the the state is determined by the labeled, unlabelled datasets and classifier, the acton is to query a point, the reward is linked to classifier test set performance improvement and the transition is to update the base classifier. Recent Q-learning algorithms are used to perform the optimisation. The results show that it outperforms some classic handcrafted AL algorithms and some prior LAL algorithms. A feature of this paper is that the method is relatively simple compared to some prior LAL methods, and also that it learns policies that can transfer successfully across diverse heterogenous datasets.\n\nStrengths:\n+ Good results. \n+ Nice that it works well while being simpler and faster than prior transferrable method MLP-GAL.\n+ Generally well written.\n+ Fig 4 is interesting.\n\nWeaknesses:\n- Novelty/originality is rather incremental. \n- Experiments are still on toy datasets.\n\nSpecifics:\n1. Novelty: The concept of formulating AL as a MDP for optimisation is now a standard idea. The optimisers used are recent off-the-shelf Q-learners. The result is that this method is similar to a non-myopic extension of LAL (Konyushkova’17) but several papers already did non-myopic AL. In particular it’s very similar to the SingleRL method in (Pang’18). The only differences are smallish design parameters like: slightly different reward function definition, use Q-learning instead of policy-gradient optimiser, and slightly different state featurisation. The improved sample/speed-efficiency vs SingleRL is likely relatively automatic due to use of recent Q-learning optimisers, rather than vanilla PG optimiser of SingleRL. Not clear that benefit comes from something uniquely contributed here. Other limitations of various prior LAL work, such as binary classifier only, are not alleviated here.\n2. Experiments: The experiments are on toy datasets. Particularly given the small novelty, then evaluation should be much more. For example: 1. How well does it work when transferred to a relatively less toy dataset such as CIFAR. 2. To what extent can it transfer across classifiers rather than only across datasets? \n3. The state representation as a sorted list of scores is rather unintuitive. Is there any intuition on what smart decisions the model could be using this to make?\n4. The featurisations used are not very standard: Like the classifier state sorted score list, and the action featurisation (instance score, instance distance to class, instance distance to unlabelled). It would be good to evaluate this featurisation with a supervised active learner (like LAL), in order to disambiguate whether the good performance comes from these feature choices, or from the recent RL algorithms used to optimise. Similarly for the choice of reward function.\n5. How does the proposed method deal with a suite of training datasets for AL that are of greatly varying difficulty. A relatively very easy dataset needing << 100 examples to reach threshold would generate few AL training examples due to early stopping. A very hard dataset might use all 100 examples. Does it mean that easy datasets contribute less to training than hard ones? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper describes the use of reinforcement learning to learn active learning strategies. This paper attempts to increase the scope of the learning of active learning strategies to transfer across very different datasets.",
            "review": "This paper presents a laudable attempt to generalize the learning of active learning strategies to learn general strategies that apply across many different datasets that have variables of different, not pre-determined, types, and apply the learned active learning strategies to datasets that are different from what they have been learned with. The paper is written quite clearly and is clear in its discussion of what its advance is beyond the current state of the art.\n\nUnfortunately, the motivation of the details of the algorithm and the experiment analysis leave the paper short of what is needed to truly assess the value of this area of work and; therefore, short of what is needed for publication in ICLR. The most notable shortcoming is on page 4, at the bottom, where the actions are described. Among the components of the actions are statistics related to the dataset---the average distance from the chosen point to all the labeled data, and the average distance from the chosen point to all the unlabeled data. The authors do not provide a motivation for the use of these particular statistics. Additionally, the authors did not explore any other statistics. I should think that statistics relevant to the sparsity of the data (e.g., how well they cluster). Additionally, what distance measure is being used? A variety of distance metrics should be explored, such as d-separation for continuous variables and Hamming distance for discrete variables, should be tested, as they intuitively seem likely to affect the results. Additionally, many values are chosen for the experiments without motivation and without testing a variety of values (e.g., 30 for the size of the dataset used to calculate the reward, 1000 RL iterations, and others).\n\nIn the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster? That would help in understanding why the new algorithm performs as it does relative to the baseline.\n\nOne relatively minor point: The authors state on page 3, \"For example, the probability that the classifier assigns to a datapoint suits this purpose because most classifiers estimate this value.\" This is a bit misleading---only generative classifiers would do this, not discriminative classifiers.\n\nPros:\n1. Very clear writing.\n2. Good motivation for the general problem.\n3. Precise description of algorithm.\n\nCons:\n1. Poor motivation for the particular algorithm implementation---features used in the actions, parameter values chosen.\n2. Lack of experiments with different choices for features and parameter values.\n3. Lack of assessment of the dataset characteristics and how they relate to algorithm performance.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}