{
    "Decision": {
        "metareview": "The paper revisits the traditional bias-variance trade-off for the case\nof large capacity neural networks. Reviewers requested several clarifications\non the experimental setting and underlying results. Authors provided some,\nbut it was deemed not enough for the paper to be strong enough to be accepted.\nReviewers discussed among themselved but think that given the paper is mostly\nexperimental, it needs more experimental evidence to be acceptable.\nOverall, I found the paper borderline but concur with the reviewers to reject\nit in its current form.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting paper with some experiments and preliminary results but requires more work ",
            "review": "This paper studies variance-bias tradeoff as a function of depth and width of a neural network. Experiments suggest that variance may decrease as a function width and increase as a function of depth. Some analytical results are presented why this may the case for width and why the necessary assumptions for the depth are violated.\n\nMain comment on experiments: if I am correct the step size for optimization is chosen in a data-dependent way for each size of the network. This is a subtle point since it leads to a data-dependent hypothesis set. In other words, in this experiments for each width we study variance of neural nets that can be found in fixed number of iterations by a step size that is chosen in data-dependent way. It may be the case that as width grows the step size decreases faster and hence hypothesis set shrinks and we observe decreasing variance. This makes the results of experiments with width not so surprising or interesting.\n\nFurther comments on experiments: it probably worth pointing out that results for depth are what we would expect from theory in general.\n\nMore on experiments: it would be also interesting to see how variance behaves as a function of width for depth other than 1.\n\nOn assumptions: it is not really clear why assumptions in 5.2 hold for wide shallow networks at least in some cases. Paper provides some references to prior work but it would be great to give more details. Furthermore, some statements seems to be contradicting: sentence before 5.2.1 seems to say that assumption (a) should hold for deep nets while sentence at the end of page 8 seems to say the opposite.\n\nOverall: I think this paper presents an interesting avenue of research but due to aforementioned points is not ready for publication.  \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Report on paper 1248",
            "review": "The paper offers a different and surprising view on the bias-variance decomposition. The paper shows, by a means of experimental studies and a simplified theoretical analysis, that variance decreases with the model complexity (in terms of the width of neural nets) , which is opposite to the traditional bias-variance trade-off.\n\nWhile the conclusion is surprising, it is somewhat consistent with my own observation. However, there are potential confounding factors in such an experimental study that needs to be controlled for. One of these factors is the stability of the training algorithm being used. The variance term (and the bias) depends on the distribution p(theta|S) of the model parameters given data S. This would be the posterior distribution in Bayesian settings, but the paper considers the frequentist framework so this distribution encodes all the uncertainty due to initialisation, sampling and the nature of SGD optimizer being used. The paper accounts for the first two, but how about the stability of the optimiser? If the authors used a different optimizer for training, what would the variance behave then? A comment/discussion along this line would be interesting.\n\nIt is said in Section 3.1 that different random seeds are used for estimating both the outer and inter expectation in Eq. 5. Should the bootstrap be used instead for the outer expectation as this is w.r.t. the data? Another point that isn't clear to me is how the true conditional mean y_bar(x) = E(y|x)  is computed in real-data experiments, as this quantity is typically unknown. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper needs to show some serious understanding on statistical machine learning",
            "review": "This paper suggests to rethink about the bias-variance tradeoff from statistical machine learning in the context of neural networks. Based on some empirical observations, the main claims in this work are that (1) it is not always the case that the variance will increase when we use bigger neural network models (particularly, by increasing the network width); (2) the variance should be decomposed into two parts: one part accounts for the variance caused by random initialization of network parameters/optimization and the other part is caused by \"sampling of the training set\".\n\nFor the first claim is based the empirical observation that increasing the number of hidden units did not cause the incrase of variance (as in figure 1). However, to my understanding, it only means increasing the number of hidden units is probably not a good way to increase the network capacity. In other words, this cannot be used as an evidence that the bias-variance tradeoff is not valid in neural network learning.\n\nFor the second claim, I don't like the way that they decompose the variance into two parts. To be clear, the classical bias-variance tradeoff doesn't consider the optimization error as an issue. For a more generic view of machine learning errors, please refer to \"The Tradeoffs of Large Scale Learning\" (Bottou and Bousquet, 2008). In addition, if the proposed framework wants to include the optimization error, it should also cover some other errors caused by optimization, for example, early stopping and the choice of a optimization algorithm.\n\nBesides these high-level issues, I also found the technical parts of this paper is really hard to understand. For example,\n\n- what is exactly the definition of $p(\\theta|S)$? The closely related case I can think about is in the Baysian setting, where we want to give a prior distribution of model (parameter). But, clearly, this is not the case here. \n- similar question to the \"frequentist risk\", in the definition of frequentist risk, model parameter $\\theta$ should be fixed and the only expectation we need to compute is over data $S$\n- in Eq. (5), I think I need more technical detail to understand this decomposition.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}