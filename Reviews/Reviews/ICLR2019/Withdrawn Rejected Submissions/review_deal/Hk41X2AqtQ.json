{
    "Decision": {
        "metareview": "Strengths: Interesting work on using latent variables for generating long text sequences.\nThe paper shows convincing results on perplexity, N-gram based and human qualitative evaluation.\n\nWeaknesses: More extensive comparisons with hierarchical VAEs and the approach in Serban et. al in terms of language generation quality and perplexity would have been helpful. Another point of reference for which additional comparisons were desired was: \"A Hierarchical Latent Structure for Variational Conversation Modeling\" by Park et al. Some additional substantive experiments were added during the discussion period.\n\nContention: Authors differentiated their work from Park et al. and the reviewer bringing this work up ended up upgrading their score to a 7. The other reviewers kept their scores at 5.\n\nConsensus: The positive reviewer raised their score to a 7 through the author rebuttal and discussion period.  One negative reviewer was not responsive, but the other reviewer giving a 5 asserts that they maintain their position. The AC recommends rejection. Situating this work with respect to other prior work and properly comparing with it seems to be the contentious issue. Authors are encouraged to revise and re-submit elsewhere.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting latent variable model, borderline paper due to experimental execution and novelty"
    },
    "Reviews": [
        {
            "title": "Nice experiments, but limited novelty",
            "review": "This paper proposed a hierarchical generative model for generating long text. The authors use a hierarchical LSTM decoder to first generate sentence-level representations; then based on the representation of each sentence, a word-level LSTM decoder is utilized to generate a sequence of words in this sentence. In addition, they use multiple layers of latent variables to address the posterior collapse issue.\nThe paper studies an important problem and the authors performed extensive experiments.\n\n\nMy major concern is about the novelty of this paper.\nHierarchical LSTM for generating long txt has been widely studied. For example, in the following works:\nLi, Jiwei, Minh-Thang Luong, and Dan Jurafsky. \"A hierarchical neural autoencoder for paragraphs and documents.\" arXiv preprint arXiv:1506.01057 (2015).\nHierarchical LSTM for Sign Language Translation, AAAI, 2018.\n\nPlacing hierarchical latent variables in VAE  is also investigated before.\nFor example, in \nZhao, Shengjia, Jiaming Song, and Stefano Ermon. \"Infovae: Information maximizing variational autoencoders.\" arXiv preprint arXiv:1706.02262 (2017).  With some adaption from image domain to text domain\nSerban, Iulian Vlad, et al. \"A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues.\" AAAI. 2017.\n\nThe author combines this two ideas together, which is incremental in terms of novelty.\n\n\nIn the writing of section 3.2, the authors should clearly cite the previous works on hierarchical LSTM and acknowledge that this is not the contributions of this paper. Under the current writing, for unfamiliarized readers, it sounds like this is proposed by the authors of this paper, which is not the case.\n\nThe notations of this paper is confusing, which hinders its readbility.\nFor example, in equation 5, the distribution is parameterized by theta.\nIn equation 6, p(x|z) is also parametrized by theta.\n\nIn the experiments, I'd like to see a comparison with the following works.\nI suggest the authors to compare with the following works.\n\nFan, Angela, Mike Lewis, and Yann Dauphin. \"Hierarchical Neural Story Generation.\" ACL (2018).\n\nGhosh, S., Vinyals, O., Strope, B., Roy, S., Dean, T., & Heck, L. (2016). Contextual LSTM: A Step towards Hierarchical Language Modeling.\n\nZhao, Shengjia, Jiaming Song, and Stefano Ermon. \"Infovae: Information maximizing variational autoencoders.\" arXiv preprint arXiv:1706.02262 (2017).  With some adaption from image domain to text domain",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice samples but lack of comparison to existing hierarchical VAEs",
            "review": "This paper proposes using a hierarchical VAE to text generation to solve the two problems of long text generation and mode collapse where diversity in generated text is lost.\n\nThe paper does this by decoding the latent variable into sentence level latent codes that are then decoded into each sentence. The paper shows convincing results on perplexity, N-gram based and human qualitative evaluation.\n\nThe paper is well written though some parts are confusing. For example, equation 4 refers to q as the prior distribution but this seems like it's the posterior distribution as it is described just below equation 5. p(z_1|z_2) is also not well defined. It would be clearer to specify the full algorithm in the paper.\n\nThe work also mentions that words are generated for each sentence until the _END token is generated. Is this token always generated? What happens to a sentence if that token is not generated?\n\nThe novelty of this paper is questionable given the significant amount of existing work in hierarchical VAEs. It's also unclear why a more direct comparison can't be made with Serban et. al in terms of language generation quality and perplexity. If a downstream model is only able to make use of one latent variable, can't multiple variables simply be averaged?\n\nIt's also unclear how this work is novel with regards to the works below.\n\nHierarchical Variational Autoencoders for Music\nRoberts, et. al\nNIPS 2017 creativity workshop\nThis seems to have a similar hierarchical structure where there is an initial 16 step decoder that decodes the latent code for the lower level note level LSTMs to use during generation.\n\nUnsupervised Learning of Disentangled and Interpretable Representations from Sequential Data\nHsu, et. al\nNIPS 2017\nThis proposes a factorized hierarchical variational autoencoder which also has a double latent variable hierarchical structure, one that is conditional on the other.\n\nMinor comments\n- Typo in page 3 under Hierarchical structures in NLP: characters \"from\" a word\n- Typo above section 4.3: hierarhical\n\n=== After rebuttal ===\nThanks for the response.\n\nI believe that Reviewer2's criticism about the similarity to Park et. al isn't sufficiently addressed by the authors. Even if the hierarchical structure is different it's unclear whether this alternative structure is superior to Park et. al. There appears to be no evidence that the latent variables contain more global information relative to VHCR (Park et. al). These claims aren't tested and the results in the paper aren't comparable since the authors don't evaluate on the same datasets as Park et. al.\n\nIn general, I think the claims of a superior hierarchical structure to models such as the factorized hierarchical VAE paper needed to be tested to show evidence of a more powerful representation for hier-VAE.\n\nI will keep my score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a hierarchical variational autoencoder for modeling paragraphs. The model creates two levels of latent variables; one level of sentence-level latent variables and another single global latent. This avoids posterior collapse issues and the authors show convincing results on a few different applications to two datasets.\n\nOverall, it is an impressive result to be able to convincingly model paragraphs with a useful global latent variable. Apart from some issues with confusing/incomplete notation (see below), my main criticism is that the authors fail to compare their approach to \"A Hierarchical Latent Structure for Variational Conversation Modeling\" by Park et al. As far as I can tell, the approaches are extremely similar, except that Park et al. may not learn the prior parameters and also use a hierarchical RNN encoder rather than a CNN (which may be irrelevant). They also are primarily interested in dialog generation, so the lower-level of their hierarchy models utterances in a conversation rather than sentences in general, but I don't see this as a major difference. I'd encourage the authors to compare to this and potentially use it as a baseline. More generally, it would have been nice to see more ablation experiments (e.g. convolutional vs. LSTM encoder). Finally, I know that space is tight, but other papers on global-latent-variable models tend to include more demonstrations that teh global variable is capturing meaningful information, e.g. with attribute vector arithmetic. The authors could include results of manipulating review sentiment via attribute vector arithmetic, for example.\n\n\nSpecific comments:\n\n- \"The Kullback-Leibler (KL) divergence term ... which can be written in closed-form (Kingma & Welling, 2013), encourages the approximate posterior distribution qÏ†(z|x) to be close to the multivariate Gaussian prior p(z).\" The prior is not always taken to be a multivariate Gaussian. You should add a sentence stating that the VAE prior is often taken to be a diagonal-covariance Gaussian for convenience.\n- 3.2 has a few things which are unclear. In the second paragraph, you define z as the sampled latent code which is fed through an MLP \"to obtain the starting state of the sentence-level LSTM decoder\". But then LSTM^{sent} appears to be fed z at every timestep. LSTM^{sent} is also not defined - am I to assume that its arguments are the previous state and current input, so that z is the input at every timestep? Also, you write \"where h^s_0 is a vector of zeros\" which makes it sound like the starting state of the sentence-level LSTM decoder is a vector of zeros, not the output of the MLP which takes z as input. In contrast, LSTM^{word} takes three arguments as input. Which are the \"state\" and which are the \"input\" to the LSTM?\n- I don't see any description of your CNN encoder (only the LSTM decoder in section 3.2, 3.3 only covers the hierarchy of latent variables, not the CNN architecture). What is its structure? Figure 1 shows a CNN encoder generating lower-level sentence embeddings and a high-level global embedding. How are those computed? It is briefly mentioned in 4.1 under \"Datasets\" but this seems insufficient.\n- p_\\theta(x | z) is defined as the generating distribution, but also as a joint distribution of z_1 and z_2. Unless I am missing something I think you are overloading the notation for p_\\theta.\n- I don't think enough information is given about the AAE and ARAE baselines. Are they the same as the flat-VAE, except with the KL term replaced by the an adversarial divergence between the prior and approximate posterior?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}