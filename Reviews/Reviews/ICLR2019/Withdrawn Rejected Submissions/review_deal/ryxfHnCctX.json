{
    "Decision": "",
    "Reviews": [
        {
            "title": "A promising idea but it needs more comprehensive evaluation",
            "review": "This paper studies how to compress a deep neural network more effectively. It proposes to a new framework that prunes a binarized neural network, which has been studied before. The novelty is that this paper further prune network weights on the filter-level. Experiments are done on CIFAR-10 and ImageNet with different well-known architectures NiN, VGG-11, and ResNet-18. Results suggest that the method can further reduce network weights by 20%-40% while maintaining a similar performance (at most 2% decrease). \n\nSince the paper points out a disconnection between network compression and computation acceleration, there should be a table that shows the measurement of memory footprint and run-time efficiency before and after the filter-level pruning. \n\nHow expensive is training the subsidiary network comparing to pre-training the main network? Since training is done in a layer-wise fashion, is training limited by the total number of layers (with weights) in the network? This is a practically important question as I believe it is not too difficult to run ResNet-18 in real-time on mobile devices. So I am curious how hard it is to train deeper networks such as ResNet-50, or ResNet-101. \n\nRegarding Section 4.1.1 and Figure 3, I am confused about the observation “The relatively smaller learning rate (10^-4) will converge with lower accuracy and higher pruning number; however, the larger learning rate (10^-3) leads to the opposite result”. I couldn’t find the evidence that supports “higher pruning number” in Figure 3. I couldn’t find the definition of PNR. Is it the same as PFR (Pruned Filter Ratio)? If so, that contracts my understanding that PFR can’t be controlled. However, it appears that PNR is a controlled variable from Figure 3. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}