{
    "Decision": {
        "metareview": "Some expert reviewers have raised novelty issues, that the authors have addressed in detail. Still, these expert reviewers are not entirely convinced. If this were a journal, I would recommend a major revision or reject-and-resubmit in order to allow the authors to anticipate the reviewers' concerns in the body of the paper and get some fresh reviews. I compliment the authors on the diligence they have put into the rebuttal stage, and look forward to reading the next version of the work. I will note that the bounds by Bartlett, Foster, and Telgarsky (and then the PAC-Bayes versions by Neyshabur et al.) are numerically vacuous empirically, and so whether those bounds or these bounds for RNNs explain generalization is up for debate.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Rewrite needed"
    },
    "Reviews": [
        {
            "title": "Marginal improvement over existing bounds; Incomplete comparison with existing works",
            "review": "This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points:\n\n1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.\n\n2. Vacuous bounds in the regime \\beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \\beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. \n\n3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] \n\n4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the \"model complexity\" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. \n\n\n[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86.1 (1998): 63-79.\n[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" Advances in Neural Information Processing Systems. 1996.\n[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in Neural Information Processing Systems. 2017.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective. ",
            "review": "The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective. Compared to the previous result (Zhang et al., 2018) for RNNs, this paper refines the generalization bounds for vanilla RNNs in all cases and fills the blank for RNN variants like MGU and LSTM. Specifically, in the work of Zhang et al. (2018), the complexity term quadratically depends on the layer (or say, current sequence length, denoted by t in original paper), making it less instructive. This paper improves it to at most linear dependence and can achieve at logarithmic dependence in some cases, which should be accredited. \n\nThe key step in the proof is Lemma 2. In Lemma 2, the spectral norms of weight matrices and the number of weight parameters are decoupled. With Lemma 2, it is natural to construct a epsilon-net for RNNs and then upper bound the empirical Rademacher complexity by Dudleyâ€™s entropy integral, since such methodology is not so novel. Bartlett, et al. (2017) developed this technique to analyze the generalization bound for neural networks in a margin-based multiclass classification. However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification. I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "limited technical novelty",
            "review": "\nThe authors provide new generalization bounds for recurrent neural networks.\nTheir main result is a new bound for vanilla RNNs, but they also have\nbounds for gated RNNs.\n\nThey claim that their vanilla bound improves on an earlier\nbound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.\nThe main result of the submission is incomparable in strength with the earlier result,\nbecause this submission assumes that the activation functions in the hidden\nlayers are bounded, where the earlier paper did not.  Part of the difference in the results\n(roughly speaking, the \"min\" in the bound) can be traced to this difference in the assumptions. \n (This paper uses this assumption in the second-to-last line of the proof of Lemma 6.)\n\nI think that the root cause of the remaining difference is that this paper,\nat its core, adapts the more traditional analysis, used in Haussler's\n1992 InfComp paper.  New analyses, like from the Bartlett, et al\nNIPS'17 paper, strove for a weak dependence in the number of parameters,\nbut this proof technique appears to lead to a worse dependence on the\ndepth.  I think that, if you unwind the network, to view the function\nfrom the first t positions of the input to output number t as a\ndepth t network, and apply Haussler's bound, you will get a qualitatively\nsimilar result (in particular with bounds that scale polynomially with\nd and t).  I think that Haussler's proof technique can be adapted to\ntake advantage of the weight sharing between layers in the unrolled\nnetwork.  \n\nIt is somewhat interesting to note that the traditional bounds have\na better dependence on depth, with correspondingly better dependence\non the length of the output sequence of the RNN.\n\nI also do not see that substantial new insight is gained through the\nanalysis that incorporates gating.\n\nI do not see much technical novelty in this paper.\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}