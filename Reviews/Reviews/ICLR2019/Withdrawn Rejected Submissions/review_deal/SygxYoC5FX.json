{
    "Decision": {
        "metareview": "AR2 is concerned about the marginal novelty, weak experiments and very high complexity of the algorithm. AR3 is concerned about lack of theoretical analysis and parameter setting. AR4 is concerned that the proposed method is useful in very\nrestricted settings and the paper is incremental.\n\nUnfortunately, with strong critique from reviewers regarding the novelty, complexity, poor presentation and restricted setting, this draft cannot be accepted by ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Novelty, complexity and poor presentation are all of concern."
    },
    "Reviews": [
        {
            "title": "An increment of GraphSAGE with restricted applications",
            "review": "This paper modifies the GraphSAGE on unsupervised inductive node embedding.\nThe authors propose to use the bi-attention architecture to sample\ninteresting nodes (instead of the uniform sampler in GraphSAGE), and to use a\nglobal embedding bias matrix in the local aggregating functions. The method\nshowed improvements over GraphSAGE and other baselines on unsupervised\ngraph embeddings.\n\nThe proposition makes sense and the performance improvements are expected.\n\nA major comment, however, is that that the proposed method is useful in very\nrestricted settings, and it is not clear how to generalize to\nother applications which GraphSAGE can be applied on.\nThe overall technical contribution is incremental and\nmay not have enough novelty to be published in ICLR.\n\nThe technical representation is very poor, unorganized and not self-contained.\nThe paper cannot pass the threshold merely based on the way it is presented.\n\nIn the algorithms, please give the output besides the input. After the\nalgorithms, please remark on the computational and memory cost.\n\nIn algorithm 1, what is this function BIATT()? \nAfter algorithm 1, please describe this function as well as SAGE(). \n\nIn the beginning of section 3, please describe the meaning of the\nglobal bias matrix. In algorithm 1, if B is zero-initialized, why\ndoes one need it as input?\n\nSome of the equations are poor formatted (e.g. reduce_sum in page 5).\nPlease try to use rigorous mathematical formulations instead of \"pseudo equations\".\nFor example, re-write \"One_hot(i)\". In section 3.2, explain A_{gg}, etc.\nuse $\\langle \\rangle$ instead of $\\alpha$.\n\nThere are many typos.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall quality is not high.",
            "review": "This paper proposes a new representation learning method for graphs.\n\nQuality:\nThe quality of the paper is not high due to vague presentation of the proposed method (see clarity).\nMoreover, there is no theoretical analysis and empirical evaluation is not thorough (see significance).\n\nClarity:\nThis paper is not clearly written and many parts are unclear.\n- In Introduction, what are \"the first issue\" and \"the second issue\"?\n- There are many grammatical mistakes (such as missing articles and the third-person singular -s) and mistakes of mathematical notations.\n- Too many symbols are not mathematically defined and it is hard to understand the paper. The current version is not appropriate for publication.\n\nOriginality:\nThe proposed method is a minor extension of the existing method GraphSAGE. Hence the originality is not high.\n\nSignificance:\n- There is no theoretical analysis of the proposed method. Hence the significance is not high.\n  In particular, the advantage of the proposed method compared to the existing approach (GraphSAGE) should be theoretically analyzed.\n- How to set parameters in practice? The performance of the proposed method will be greatly affected by parameter setting.\n  In experiments, the sensitivity of the proposed method with respect to parameter changes should be analyzed.  \n\nPros:\n- The relevant problem is studied.\nCons:\n- Presentation is not good.\n- Theoretical analysis is not given.\n- Empirical analysis is not thorough.\n\nOther comments:\n- P.3, L.5 in Sec.3: \"G = {V, E, X}\" should be \"G = (V, E, X)\"\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novelty of the paper seems to be marginal",
            "review": "This paper studied learning unsupervised inductive node embeddings with an attention mechanism. For each positive edge, multiple different sets of neighborhoods are sampled for both the source and target nodes, and the similarity between the neighborhood are used as the attention functions. Experimental results prove the effectiveness of the proposed approach over GraphSAGE on a few networks. \n\nStrength:\n- learning unsupervised inductive node embeddings is an important problem\n- the proposed method seems to work\n\nWeakness:\n- the novelty of the proposed method seems to be very marginal\n- the experiments are quite weak\n- the complexity of the algorithm seems to be very high\n\nDetails:\n- the complexity of the algorithm seems to be very high seem for each pair of nodes, multiple sets of neighborhoods must be sampled for each node.\n- there are also other approaches for inductive unsupervised node embeddings, for example, the varitional graph autoencoder method (Kipf et al. 2017).\n- I am wondering how the proposed method performs comparing with the methods of only selecting the nodes which form triangles with the given positive edges. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}