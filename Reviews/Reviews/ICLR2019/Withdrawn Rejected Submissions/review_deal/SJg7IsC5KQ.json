{
    "Decision": {
        "metareview": "The reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. The AC thus proposes \"revise and sesubmit\".",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Revise and resubmit"
    },
    "Reviews": [
        {
            "title": "Interesting theoretical analysis for batch normalization ",
            "review": "This paper provides a theoretical analysis for batch normalization with gradient descent (GDBN) under a simplified scenario, i.e., solving an ordinary least squares problem. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem. Some practical experiments are carried out to justify their theoretical insights. The paper is in general easy to follow. \n\nPros:\nThis paper provides some insights for BN using the simplified model.\n1. It shows that the optimal convergence rate of BN can be faster than vanilla GD.\n\n2. It shows that GDBN doesn't diverge even if the learning rate for trainable parameters is very large. \n\nCons:\n1. In the main theorem, when the learning rate for the rescaling parameter is less than or equal to 1, the algorithm is only proved to converge to a stationary point for OLS problem rather a global optimal. \n\n2. To show convergence to the global optimal, the learning rate needs to be sufficiently small. But it is not specified how small it is. \n\nOverall, I think this paper provides some preliminary analysis for BN, which should shed some lights for understanding BN. However, the model under analysis is very simplified and the theoretical results are still preliminary.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but unsure analogies hold",
            "review": "The paper presents an analysis of the batch normalization idea on a simple OLS problem. The analysis is interesting as presented but several key questions remain, as described below. It is unclear that these questions are answered to the point where the insight gained can be considered transferable to BN in large Neural Network models.  \n\n- The reason why the auxiliary variable 'a' is included in the formulation (7) is unclear. The whole reason for using BN is to rescale intermediate outputs to have an expectation of zero and variance of one. The authors claim that BN produces \"order 1\" output and so 'a' is needed. Can you please explain his better?\n\n- The scaling proposition 3.2 is claimed to be important, but the authors don't provide a clear explanation of why that is so. Two different settings of algorithms are presented where the iterates should roughly be in the same order if input parameters of the formulation or the algorithm are scaled in a specific way. It is unclear how this leads to the claimed insight that the BN algorithm is yielded to be insensitive to input parameters of step length etc. due to this proposition. also, where is the proof of this proposition? I couldn't find it in the appendix, and I apologize in advance if that's an oversight on my part.\n\n- The 'u' referred to in eqn (14) is the optimal solution to the original OLS problem, so has form H^{-1} g for some g that depends on input parameters. Doesn't this simplify the expression in (!4)? Does this lead to some intuition on how the condition number of H^* relates to H? Does this operation knock off the highest or lowest eigenvalue of H to impact the condition number?  \n \n- Additionally, it is bad notation to use two-letter function names in a mathematical description, such as BN(z). This gets confusing very fast in theorems and proofs, though the CS community seems to be comfortable with this convention.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Relation to prior work is very unclear",
            "review": "The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.\n\n1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.\n\n2) Properties of the minimizer\nThe authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.\n\n3) Scaling property\nI find this section confusing. Specifically,\na) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?\nb) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?\nc) The idea of “restarting” is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don’t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this “property” seem to be used to simply rescale the a and w parameters.\nd) The authors claim that “the scaling law (Proposition 3.2) should play a significant role” to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?\n\n4) Convergence rate\nIt seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,\na) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn’t a constant step size also yield convergence in that case?\nb) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.\n\n5) Saddles for neural nets\nThe authors claim they “have not encountered convergence to saddles” for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?\n\n6) Extension of the analysis to deep neural networks\nThe analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?\n\n7) Experiments\nHow would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}