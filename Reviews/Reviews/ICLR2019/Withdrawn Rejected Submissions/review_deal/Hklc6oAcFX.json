{
    "Decision": {
        "metareview": "This manuscript proposes a technique for co-manifold learning that exploits smoothness jointly over the rows and columns of the data. This is an important topic worth further study in the community.\n\nThe reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the clarity of the presentation. Further improvement of the clarity -- particularly clarification of the learning goals, combined with additional convincing experiments would significantly strengthen this submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "lack of novelty",
            "review": "The manuscript proposes a co-manifold learning approach for missing data.  The problem is important, but the method is lack of novelty. \nPros: important problem setting, Good experimental results.\nCons: the method is lack of novelty.\n\nIn detail, the method just simply combines a loss for competing missing values, which is not new,  and Laplacian losses for rows and columns, which are also not new. I don't see much novelty in the model.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A joint method for filling missing values and emdedding rows & columns, but not convincing",
            "review": "This paper presents a joint learning method for filling missing value and bi-clustering. The method extends (Chi et al. 2017), using a penalized matrix approximation. The proposed method is tested on three data sets, where two are synthetic and one small real-world data matrix. The presented method is claimed to be better than two classical approaches Nonlinear PCA and Diffusion Maps.\n\n1) Filling missing values is not new. Even co-clustering with missing values also exists. It is insufficient to defeat two methods which are older than ten years. More extensive comparison is needed but lacking here. Why not first use a dedicated method such as MICE or collaborative filtering, and then run embedding method on rows and columns?\n\n2) The purpose of the learning is unclear. The title does not give any hint about the learning goal. The objective function reads like filling missing values. The subsequent text claims that minimizing such a objective can achieve biclustering. However, in the experiment, the comparison is done via visualization and normal clustering (k-means).\n\n3) The empirical results are not convincing. Two data sets are synthetic. The only real-world data set is very small. Why k-means was used? How to choose k in k-means?\n\n4) The choice Omega function after Proposition 1 needs to be elaborated. A function curve plot could also help.\n\n5) What is Omega' in Eq. 2?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Sufficient Novelty but Significant Clarity Issues",
            "review": "Review for CO-MANIFOLD LEARNING WITH MISSING DATA\nSummary:\nThis paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction.\nThe overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. \nNovelty/Significance:\nThe overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn’t have a well established solution.\nQuestions/Clarity:\nSmooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn’t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? \n “Replacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties” – The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function’s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2.\nProposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1.\nThe authors write: “Missing values can sabotage efforts to learn the low dimensional manifold underlying the data. … As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.” However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue.\nWhy do the author’s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound.\nEquation 5 is not clear that it is the first order taylor approximation. Omega’ is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ?\n “first-order Taylor approximation of a differentiable concave function provides a tight bound on the function” – Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won’t be anything close to tight.\nThe authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn’t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. \nIt is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is.\nFigure 4 appears before it is mentioned and is displayed as part of the previous section.\nFor the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise.\nHow big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}