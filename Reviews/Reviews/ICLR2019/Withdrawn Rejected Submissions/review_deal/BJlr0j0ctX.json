{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice motivation and impressive empirical result. More experiments needed.",
            "review": "This paper studies training adversarially robust models without generating adversarial examples online during training. This way, the training time can be significantly reduced. Firstly, the authors draw inspiration from linear (approximation of) classifier and work out the epsilon_L formula. Based on this, the authors proposes three ways of making the model more robust. And specifically, the authors focus on label smoothing and logit squeezing, because both of them have an effect of reducing the logit gap and shrinking the logit. Coupled with Gaussian Noise trick, both methods work pretty well. On MNIST, it come, to some extent, close to PGD7 trained models. On CIAFR10, using aggressive smoothing or squeezing hyper-parameters, it surpasses PGD7 trained models.\n\n1. On Table 3, I would like to see PGD-100 steps and PGD-200 steps for xent loss and CW loss.\n\n2. From Table 2,3,4, it seems like the accuracy varies a lot when the hyper-parameter changes. Is there a good rule of thumb on how to set up a good hyper-parameters in case of a new dataset? \n\n3. Have you checked if the accuracy goes down as the perturbation budget epsilon goes up, say 2, 4, 6, 8, 10, 12, using PGD20 or stronger attacks?\n\n4. I would like to see results on CIFAR100, which is a harder dataset, containing 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for empirical justification nowadays (maybe it is enough one year ago). I don't know how the accuracy becomes under aggressive \\alpha and \\beta settings for 100-way classification. Also, evaluate them using PGD20, PGD100, PGD200 in order to make rigorous claims.\n\n5. It seems that Gaussian trick is the key to good performance. Have you tried other form of random noise? Uniform or Laplace?\n\n####### post-rebuttal\n\nI have read the authors' reply, and other relevant comments in this page.  I am satisfied with the rebuttal. The weak point of this paper is 1) building blocks are all existing techniques so the overall method looks like tricks; 2) lack of explanation and understanding of why it works.  But I think this is a good empirical paper. It is true that nowadays there are many papers on this topic coming up, sometimes making contradictory claims (by using different experiment setup), but I am convinced by the results in the paper and in the rebuttal. Examining and running the released code would be better but that would take me too much time and effort.  I think the results are interesting and impressive. It tells the community something new. An important future study would be understanding why it works, and if (or how) it may implicitly inherently relate to adversarial training or regularization or other training methods for defense. Also, extending to ImageNet1000 would be very important. Given the CIFAR100 results, I am worried that smoothing or squeezing alone may not be readily working.  \n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting results on CIFAR-10, but how conclusive are they?",
            "review": "The paper addresses a highly relevant problem in Adversarial Machine Learning: devising alternatives to adversarial training that scale to high-dimensional datasets.\n\nThe proposed approach is a combination of known techniques: label smoothing, logit squeezing and Gaussian data augmentation. While those techniques had previously been studied in isolation, the authors argue that the combination thereof \"saves the day\", i.e. yielding high accuracy on clean data samples while exhibiting robustness against adversarial inputs.\n\nThe authors evaluate their approach on the MNIST and CIFAR-10 datasets. They compare against the adversarially trained and publicly available models by Madry et al (ICLR 2018). The results are mixed: on MNIST, Madry's model clearly outperforms the proposed approach in terms of adversarial robustness. On CIFAR-10, for a certain combination of logit squeezing and Gaussian data augmentation, the authors report results suggesting that their approach outperforms Madry's model both in terms of accuracy on clean samples and robustness against adversarial samples (white- and black-box). Specifically, the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 achieves 90.5% accuracy on clean test samples (compared to Madry's 87.3%), 49.7% under white-box attacks (compared to 45.8%), and 67.0% under black-box attacks (compared to 64.2%).\n\nOverall, a lot of the material in this manuscript didn't strike me as particularly deep or original. For instance, I am not convinced by the analysis \"What does adversarial training do?\" (Section 1) which is based on a linearity assumption, \nthe importance of which isn't properly discussed.\n\nThe proposed alternative to adversarial training seems a bit unprincipled: the building blocks had been there before, but it appears the combination with specific parameter choices does the trick. It is a bit disconcerting that the approach doesn't perform that well for MNIST. A key selling point of the proposed approach is its computational efficiency; from that regard it would have been nice to see results on large-scale datasets like ImageNet. \n\nHaving said all this, if the findings on CIFAR-10 indeed hold true, then I think this is an important result that is worth publishing at ICLR. It would indicate important directions for future research besides the mainstream work on adversarial training.\n\nOne concern that I have though is whether the CIFAR-10 models trained by the authors can withstand a wider range of attacks. To put it into perspective: Madry et al - in their CIFAR-10 challenge (https://github.com/MadryLab/cifar10_challenge) - report 44.71% accuracy under white-box attacks as the worst-case result *under any epsilon-bounded attack* that researchers have tried since this model was published about a year ago. The 49.7% accuracy for the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 is the worst-case result under only 4 different attacks. One has to question: what would the worst-case result for this model be in one year from now if the community had white-box access to it?\n\nIs there any way the authors could make their models publicly available during the review period?\n\nOne specific question: Madry's 46.8% accuracy under the CW attack was obtained for 30-step PGD. Table 3 states that only 20-step PGD was performed. Can the authors shed more light on what exact setup they used for the CW attack and what - if not reported in Table 3 - was the result under a 30-step attack?\n\nA minor comment:\n- Figure 2 (a) vs Footnote 1: was 20- or 40-step PGD being used?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Will not stand up to scrutiny",
            "review": "This paper proposes using label smoothing and logit squeezing together with Gaussian noise injection at training time as a replacement for adversarial training. The paper builds on the flawed premise that label smoothing and logit squeezing can lead to more robustness - it simply masks the gradients so that the optimization landscape is harder. Also I am unconvinced that simply adding Gaussian noise together with these tricks can lead to any robustness against a worst case adversary - Gaussian noise augmentation is known to not lead to increased robustness (see e.g. [1]) while logit squeezing is also known to not lead to increased robustness (see e.g. [2]). So I cannot fathom why the two combined suddenly start working?\n\nAlso a glance at their experimental evaluation immediately uncovers the flaw that they only used 20 step PGD attack to evaluate the robustness of their model on CIFAR-10. Label smoothing and logit squeezing result in gradient masking and a more difficult terrain to optimize over, so increasing the number of attack steps and doing many random restarts will reveal that their claimed model is not as robust as the authors think.\n\n[1] https://arxiv.org/pdf/1711.08478.pdf\n[2] https://arxiv.org/pdf/1807.10272.pdf",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}