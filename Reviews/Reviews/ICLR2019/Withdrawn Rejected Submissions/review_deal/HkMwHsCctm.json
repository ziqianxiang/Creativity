{
    "Decision": {
        "metareview": "The strength of the paper is that it designs an LP-based algorithm for training neural networks with runtime exponential in the number of parameters and linear in the size of the datasets and the algorithm works for worst-case datasets. As reviewer 2 and reviewer 3 pointed out, the cons include a) it's not clear why the algorithm provides any theoretical insights on how to design in the future polynomial time algorithm --- it seems that the algorithms are inherently exponential time and b) it's not clear whether the algorithm is practically at all relevant. The AC also noted that brute-force search algorithm is also exponential in # parameters and linear in size of datasets, and the authors agreed with it. This leaves the main contribution of the paper be that it works for the worst datasets. \bHowever, theoretically speaking, it's not clear this should be counted as a feature for algorithm design because we cannot go beyond the intractability without making assumptions on the data and in the AC's opinion, the big open question is how to make additional assumptions on the data (instead of removing them.) In summary, the drawback b) makes this a purely theoretical paper and the theoretical significance of the paper is unclear due to a). Therefore based on a), the AC decided to recommend reject, although the AC suggested the authors to re-submit to other top theory or ML theory conferences which may better evaluate the theoretical significance of the paper. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "exponential complexity; practical relevance unclear",
            "review": "This work reformulates the neural network training as an LP with size that is exponential in the size of the architecture and data dimension, and polynomial in the size of the data set. They further analyze generalization properties. It extends previous works on 1-hidden-layer neural-nets (say, In Arora et al. (2018)). \n\nPros: Establish new time complexity (to my knowledge) of general neural-nets. \n\nCons: It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners.\n    My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit. \n\nOther questions:\n--The authors mentioned “that is exponential in the size of the architecture (and data dimension)  and polynomial in the size of the data set;” and \"this is the best one can hope for due to the NP-Hardness of the problem \". \na)\tThe time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter? \nb)\tThe NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial. The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful. \n\n-- Novelty in technical parts: The idea of tree-width graph was introduced in Bienstock and Muñoz (2018). The main theorem 3.1 is based on explicit construction for Theorem 2.5, and Theorem 2.5 is an immediate generalization of a theorem in Bienstock and Muñoz (2018) as mentioned in the paper. Thus, this paper looks like an easy extension of Bienstock and Muñoz (2018) --intuitively, minimizing polynomials by LP seems to be closely related to solving neural-nets problems by LP. Could the authors explain more on the technical novelty? \n\nUpdate after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns.\n    The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement.\n     I just realize that my concern on the practical relevance is largely due to the title \"Principled Deep Neural Network Training through Linear Programming\". It sounds like it can provide a better \"training\" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like \"look, here is a new method that can change training\", but \"hey, check some new theoretical progress, it may lead to future progress\". I strongly suggest changing the title to something like \"Reformulating DNN as a uniform LP\" or \"polynomial time algorithm in the input dimension\", which reflects the theoretical nature. \n    That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably.  In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studies the problem of proper learning of deep neural network. In particular, the focus is on doing\napproximate empirical risk minimization over the class of neural networks of a fixed architecture. The main \nresult of the paper is that approximate ERM can be formulated as an LP problem that is of size exponential in the\nnetwork parameters and the input dimensionality. The paper uses a framework of Bienstock and Munoz that shows how to \nwrite a binary optimization problem as a linear problem with size dependent on the treewidth of an appropriate graph\nassociated with the optimization problem. In order to apply the framework, the authors first discretize the parameter\nspace appropriately and then apply analyze the treewidth of the discretized space. The authors also provide treewidth\nanalysis of specific architectures including fully connected networks, and CNNs with various activations.\n\nMost of the technical work in the paper involves analyzing the treewidth of the resulting discretized problem. The nice \nfeature of the result is that it holds for worst case data sets, and hence, the exponential dependence on various\nparameters is unavoidable. On the other hand, it is unclear to me as to how these ideas might eventually lead to \npractical algorithms or shed light on current training practices in the deep learning community. For instance, it would\nbe very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs\nthat depend exponentially only in the depth, as opposed to the input dimensionality.  \n\nI also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid work that could discuss the implications for the ICLR community better",
            "review": "This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem \nusing a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. \n\nIn my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on \na recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. \n\nThe authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as \n\nBrandon Amos, Lei Xu, J. Zico Kolter:\nInput Convex Neural Networks. \nICML 2017: 146-155\n\nGiven the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}