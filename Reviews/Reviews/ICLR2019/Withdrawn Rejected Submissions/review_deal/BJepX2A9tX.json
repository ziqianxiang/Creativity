{
    "Decision": "",
    "Reviews": [
        {
            "title": "Okay paper with limited novelty and lacking experimental evidence for main supposed advantages of proposed method",
            "review": "Disclaimer: I've already reviewed an earlier version of this manuscript for NIPS 2018.\n\n# Summary\n\nIn the context of image classification, the paper proposes a convolutional neural network architecture with rotation-equivariant feature maps that are eventually made rotation-invariant by using the magnitude of the 2D discrete Fourier transform (DFT). Classification experiments are conducted on three different datasets.\n\nThe problem of rotation-invariant image classification addressed by the paper is important, since structures in images may appear at arbitrary orientations in many applications (e.g. microscopy).\n\n\n# Novelty\n\nThe general ideas of the paper are sound, however seem to be of rather minor novelty.\nThe paper claims two main novelties: (1) conic convolutions and (2) using the DFT magnitude for rotation invariant classification in the context of CNNs.\n- While (1) seems novel, the paper doesn't convince me that conic convolutions would be useful in practice. While they are more memory efficient, they achieve this by actually computing fewer features (each conic region is only processed by a particular orientation of a convolution kernel). Hence, they should (theoretically) be less powerful than group convolutions (i.e. G-CNN; the whole image is processed by a particular orientation of a convolution kernel). Furthermore, there are no experiments that demonstrate the advantages of the lower memory footprint of conic convolutions.\n- Novelty (2) is only because it hasn't been used in the context of CNNS, but there is no technical novelty here. Using the DFT magnitude for rotational invariance is an orthogonal contribution that can also be applied to G-CNN, which the paper also evaluates (this is good).\n\n\n# Experiments\n\n- Rotation-equivariant CNNs are expected to perform better than standard CNNs when only limited training data is available. However, this is not thoroughly evaluated in the paper, which only does a very coarse comparison (trained with N=50 or N=100 images). Since this is the main advantage of of CNNs with built-in rotation-equivariance, I expect a more thorough evaluation showing the results for several different training data sizes.\n- The savings in terms of computational and especially storage efficiency of CFNet are not really evaluated, only briefly mentioned in the very short section 3.4. Again, this needs to be expanded since computational/memory efficient is a supposed main advantage of conic convolutions over group convolutions.\n\n- In section 4.2: Why are image rotations used for data augmentation? (The whole point of the compared classification methods (expect for \"CNN\") is to be rotation-invariant.) It would be interesting to show the results with and without image rotation augmentation.\n- G-CNN+DFT is missing for the budding yeast cell classification experiment (section 4.3). As mentioned before, I suspect it would perform well or even better than CFNet. Also, why is Worrall et al. (2017) not compared to in sections 4.2 and 4.3.? (It was the best method in for rotation-MNIST.)\n\n\n# Clarity\n\nAlthough the writing is grammatically well done, I found it difficult to follow the explanation of the proposed method. In particular, the mathematics often add to my confusion instead of clearing it up. Given that the proposed concepts are not actually that complicated, I feel the paper makes heavy use of *mathiness* (\"the use of mathematics that obfuscates or impresses rather than clarifies\" [1]).\n\n[1]: Zachary C. Lipton, Jacob Steinhardt. \"Troubling Trends in Machine Learning Scholarship.\", https://arxiv.org/abs/1807.03341\n\n\n# Missing explanations\n\n- Standard pooling will not preserve rotation-equivariance (RE). While section 3.2 mentions this, it doesn't really explain how pooling is changed to preserve RE. Furthermore, it is also not explained why a deep network based on conic convolutions remains RE after several downsampling and conic convolution layers. I feel there's a problem when the conic regions become tiny after several downsampling operations. Fig. 2 shows that fewer conic regions are used then, limiting the equivariance to 90 degree rotations. This seems like a conceptual limitation.\n- The paper says that the conic convolution layer uses a \"filter size of three pixels\", but fails to mention that this means there are currently strong interpolation artifacts, especially for finer degrees of rotation (the paper only rotates at most 8 times). Section 4 only briefly mentions that this would be alleviated by using steerable filters.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting idea of some novelty, difficult to read if you're not an expert",
            "review": "The authors propose (1)  a new method for constructing rotation equivariant networks using 'conic' convolution, and then furthermore propose (2) to integrate the magnitude response of the 2D-discrete Fourier transform into a transition layer between convolutional and fully connected layers to get rotational invariance.  \n\nThe conceptual advantage over earlier approaches to (1) such as group convolution is that one operates only over the spatial domain., making it more intuitive (or so the authors claim) and also computationally more efficient.\n\nThe advantage of (2) is that invariance is achieved without throwing away useful information (such as earlier approaches did which took e.g. average or maximum. \n\nThe practical advantage of (1) and (2) is that the experiments suggests that it works better than earlier approaches. \n\nPRO\n- Both ideas are reasonable and the experiments suggest that they indeed improve noticeably on state of the art\n\nCON\n- Given the earlier work (especially the group convolution CNN) the developments here are not that surprising\n- The paper, while well-written in the sense of organization, grammar etc. was still quite a hard read for me; if one is not an expert on invariance/equivariance etc. then one has a hard time. Combining this paper with the Cohen/Welling paper I managed to understand most things in the end, and admittedly I am not an expert (something went wrong with the assignment process here...) but still, I feel things can be improved here. \n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A reasonable solution to achieve rotation invariant representation, but experimental and analytic parts should be enhanced. ",
            "review": "Authors provide a rotation invariant neural network via combining conic convolution and 2D-DFT.\nThe proposed conic convolution is a natural extension of quadrant convolution, which modulates directional convolution results with a direction-related mask. \nThe magnitude response of a 2D-DFT is applied to the output of the conic convolution, which further achieves a rotation invariant transformation to the rotation invariant features. \n\nComments:\n1. Both the conic convolution and 2D-DFT module are rotation invariant transformation, which one is more important for the model? I think the G-CNN + DFT is a good baseline to evaluate the importance of the conic convolution part, and authors should take the variant “the Conic Conv without DFT” into consideration, such that the contribution of the DFT module can be shown quantitatively. \n2. Compared with H-Net, the proposed CFNet is slightly worse on the testing error shown in Table 1. Could authors analyze the potential reason for this phenomenon? Compared with H-net, what is the advantage of the proposed method?\n3. In Figure 3(c, d), the learning curves of the proposed CFNet are not as stable as those of its baselines, especially in the initial training phase. What is the reason? And how the configure the hyperparameters of the model?\n\nMinors:\n- Section 3.1 is a little redundant in my opinion. Many simple and well-known background/properties are shown, which can be compressed.\n- Line 4 of Figure 1’s caption, 90 degrees -> 180 degrees?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}