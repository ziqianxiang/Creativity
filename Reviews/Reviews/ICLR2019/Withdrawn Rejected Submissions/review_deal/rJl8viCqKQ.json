{
    "Decision": {
        "metareview": "The paper proposes improvements on the area of neural network inference with homomorphically encrypted data. Existing applications typically have high computational cost, and this paper provides some solutions to these problems. Some of the improvements are due to better \"engineering\" (the use of the faster SAEL 3.2.1 over CryptoNet). The idea of using pre-trained AlexNet features is new, but pretty standard practice. The presentation has been greatly improved in the updated version, however the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade-off), what network modules should be used and how should they be represented? To summarize, the problem considered is important, however, as pointed out by the reviewers, both the empirical and the theoretical results appear to be incremental with respect to the existing literature.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Incremental contribution"
    },
    "Reviews": [
        {
            "title": "Incremental improvements or poor motivation. Writing needs improvement. ",
            "review": "This paper proposes several improvements to cryptonet proposed in Dowlin et al, 2016. The contributions include: \n1. Better implementation to improve speed and throughput. \n2. Modified architecture (LoLa) to reduce latency. \n3. Using features from a deep network rather than raw input. \nContribution 1 is better engineering with modern libraries and more efficient parallelism. This has limited academic novelty. Contribution 2 seems interesting, but is poorly explained. What is the cause of this improvement? What are some guidelines of latency-bandwidth trade-off in homomorphic deep networks? Contribution 3 seems to eliminate the need for remote classification service itself. Users need classification services because they do not have the computation resources, or do not have enough data to train classifiers successfully. If the users need to generate good representations such that classification becomes linearly separable, then why don’t they just train the last linear layer themselves? Is there any computation or statistical reason to use any remote service?\n\nThe biggest problem with the paper is that it is hard to read and has several writing issues:\n1. It is not self contained. I had to refer to Dowlin et al, 2016 to understand what the authors are referring to. \n2. Notations are used but not defined. For example, I couldn’t find any definition several symbols in section 3.\n3. The narration is too long and detailed: the authors report a laundry list of the things they did, details that should go into the appendix. It’s hard to find what the main contributions are. \n\n\n------------ Response to rebuttals\n\nThe writing of the introduction has been greatly improved. \n\nThe authors suggested a practical scenario of using high level features. I am still somewhat skeptical. To do this the users need to map raw input to good representations. It seems that there are two ways this can work: users and service providers agree on publicly available representations, or the service provider is willing to share everything except the top layer. Both assumptions seem rather restrictive. Nonetheless even with practically useful scenarios, it is standard practice to use good features/representations whenever available, so not really an academic contribution. I am evaluating this paper only by the Lola contribution. \n\nThe presentation of LoLa can still be improved, but I see the main ideas. I think the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade-off), what network modules can he choose and how to represent them? I think this type of general discussion can improve the significance and usefulness of the proposed approach. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Low latency version of CryptoNet",
            "review": "This paper proposes a low latency data representation / network architecture to operate on encrypted data. This work is based on CryptoNet, but uses a different data representation. How to apply machine learning on confidential data is important in many practical applications.\n\nI recommend to give a brief review of HE before Section 3, to be more friendly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Potential to advance the area, but the paper should be improved",
            "review": "The paper proposes improvements on the area of neural network inference with homomorphically encrypted data, where a major drawback in current applications is the high computational cost.\n\nThe paper makes several minor contributions and, in my opinion, it fails to provide an unified message to the reader. The arguably independent contributions are:\n1. the use of the faster SAEL 3.2.1 over CryptoNet — not really an innovation per se\n2. flexible representation of message space — the main contribution\n3. a much more complex network than CryptoNet for an experiment on CIFAR10 — minor contribution/application of 2\n4. using pre-trained model for embedding images before encryption — minor contribution\n\nI think the authors should refocus this work on point 2 and 3 only. 1 could simply be a note in the paper, as not a a real contribution and 4 may be even excluded as it goes toward a different direction.\n\nI will mainly focus on the central contribution of the paper — the data representation — which in my opinion has the potential to progress this area further. Unfortunately, the current quality of presentation is suboptimal. The choices on architecture and intermediate representations of LoLa in Section 4 are hard to follow, and it is not clear to me when the authors are making a choice or satisfying a constraint. That is, for example when we aim to vs. we have to use one representation in place of another? Since this is the main contribution, I suggest the author to help the presentation with diagrams and formulae in this Section. Each component should be presented in modular fashion. In fact, this is what the authors did in Section 3. Yet, towards reuse of these concepts for machine learning applications, the authors could present their improvements as applied to *neural network layers*. E.g. responding to answers such as: what are the most efficient data representation in input and output if we want to compute a convolutional layer? It would be also interesting to break down cost requirements for each intermediate computation, to highlight bottlenecks and compare alternative choices.\n\nIncidentally, LoLa-Conv improvements appear to be simply due to the use of a conv vs. dense layer as input.\n\nSection 6 explains the use of a pre-trained AlexNet to embed data for learning a linear model (HE-aware) for Caltech 101. In the context of application of HE, the idea is novel to my knowledge, although it is a rather common practice in deep learning. The number reported in this section have no baseline to compare with and therefore it’s hard to evaluate their significance.\n\nA missing reference is [A].\n\nMinors\n* section 4: “this saves a processing step which saves\"\n* typos in section 6: “a networks”, “in medical imagine\"\n* footnote 4: “convinient\"\n\n[A] TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}