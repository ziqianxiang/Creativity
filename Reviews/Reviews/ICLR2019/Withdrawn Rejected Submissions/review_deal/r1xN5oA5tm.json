{
    "Decision": {
        "metareview": "All reviewers agree in their assessment that this paper does not meet the bar for ICLR. The area chair commends the authors for their detailed responses.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Cool Idea, More Evidence Needed",
            "review": "This paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams (phrases). Experiments on WMT14 show a slight advantage over token-based attention.\n\nThe model is elegant and presented very clearly. I really liked the motivation too. \n\nHaving said that, I am not sold on the claim that phrasal attention actually helps, for two reasons:\n\n1) The comparison to previous results is weak. There are more datasets, models, and hyperparameter settings that need to be tested.\n\n2) Phrasal attention essentially adds an additional convolution layer, i.e. it adds parameters and complexity to the proposed model over the baseline. This needs to be controlled by, for example, adding another transformer block to the baseline model. The question that such an experiment would answer is \"does phrasal attention help more than an extra transformer layer?\" In my view, it is a more interesting question than \"does phrasal attention help more than nothing?\"\n\nAlso related to concern (2), I think that the authors should check whether the relative improvement from phrasal attention grows/shrinks as a function of the encoder's depth. It could be that deep enough encoders (e.g. 10 layers) already contain some latent representation of phrases, and that this approach mainly benefits shallower architectures (e.g. 2 layers).\n\n===  MINOR POINTS ===\nIf I understand the math behind 3.1.2 correctly, you're first applying a 1x1 conv to K, and then an nx1 conv. Since there's no non-linearity in the middle, isn't this equivalent to the first method? The only difference seems to be that you're assuming the low-rank decomposition fo the bilinear term at a different point (and thus get a different number of parameters, unless d_k = d_q).\n\nHave you tried dividing by sqrt(d_k * n) in 3.1.1 too?\n\nWhile the overall model is well explained, I found 3.3 harder to parse.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. ",
            "review": "Phrase-Based Attention\n\nPaper Summary:\n\nNeural translation attention computes latent alignments which pairs input/target positions. Phrase-based systems used to align pairs of spans (n-grams) rather than individual positions, this work explores neural architectures to align spans. It does so by compositing attention and convolution operations. It reports empirical results that compares n-gram to uni-gram attention.\n\nReview:\n\nThis paper reads well. It provides appropriate context. The equations are correct. It lacks a few references I mentioned below. The main weaknesses of the work lies in its motivation and in the \nempirical results.\n\nThis work motivation ignores an important aspect of neural MT: the vectors that attention compares (“queries” and “keys”) do not summarizes a single token/unigram. These vectors aggregate information across nearby positions (convolutional tokens, Ghering et al 2017), all previous tokens (recurrent models, Suskever et al 2014) or the whole source sentence (transformer, Vaswani et al 2017). Moreover multiple layers of attention are composed in modern decoders, comparing vectors which integrates information from both source and target. These vectors cannot be considered as the representation of a single unigram from the source or from the target.\n\nThe key-value convolution method 3.1.1 is not different from (Ghering et al 2017) which alternates computing convolution and attention multiple times. The query as kernel is a contribution of this work, it is highly related to the concurrent submission to ICLR on dynamic convolution “Pay Less Attention with Lightweight and Dynamic Convolutions”. This other work however reports better empirical results over the same benchmark.\n\nOn empirical results, it seems that the table does not include recent results from work on weighted transformer (Ahmed et al 2017) or relative attention (Shaw et al 2018). Also a 0.1 BLEU improvement over Vaswani et al seems brittle, is your result averaged over multiple runs, could the base transformer be better with as many parameters/updates as your model?\n\nReview Summary:\n\nThis work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. The most interesting contribution is the query as kernel approach: however the concurrent submission “Pay Less Attention with Lightweight and Dynamic Convolutions” obtains better empirical results with a similar idea.\n\nMissing references:\n\nKarim Ahmed, Nitish Shirish Keskar, and Richard Socher. 2017. Weighted transformer network for machine translation. arxiv, 1711.02132.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Potentially useful extension of the Transformer model, but needs more solid experiments.",
            "review": "The authors propose to include phrases (contiguous n-grams of wordpieces) in both the self-attention and encoder-decoder attention modules of the Transformer model (Vaswani et al., 2017). In standard multi-head attention, the logits of the attention distribution of each head is computed as the dot-product between query and key representations, which are position-specific. In the phrase-based attention proposed here, a convolution is first computed over the query, key, value sequences before logits are computed as before (a few variants of this scheme are explored). Results show an improvement of up to 1.3 BLEU points compared to the baseline Transformer model. However, the lack of a controlled experiment sheds substantial doubt on the efficiacy of the model (see below).\n\nContributions\n-------------------\nProposes a simple way to incorporate n-grams in the Transformer model. The implementation is straightforward and should be fully replicable in an afternoon.\n\nHaving an inductive bias towards modeling of longer phrases seems intuitively useful, in particular when using subword representations, where subword units are often ambiguous. This is also motivated by the fact that prior work has shown that subword regularization, where sampling different subword segmentations during training can be useful.\n\nImprovements in BLEU scores are quite strong.\n\nIssues\n---------\nThe experiments do not control for parameter count. The phrasal attention model adds significant number of parameters (e.g., \"interleaved attention\" corresponds to 3x the number of 1D convolution parameters in the attention layer). It is well established that more parameters correspond to increased BLEU scores (e.g., the 2x parameter count in the \"big\" Transformer setting from Vaswani et al. (2017) results in over 1 BLEU point improvement). This needs to be fixed!\n\nThe model is a very modest extension of the original Transformer model and so its value to the community beyond improved numbers is somewhat questionable.\n\nWhile an explicit inductive bias for phrases seem plausible, it may be that this can already be fully captured by multi-head attention. With positional encodings, two heads can easily attend to adjacent positions which gives the model the same capacity as the convolutional phrase model. The result in the paper that trigrams do not add anything on top of bigrams signals to me that the model is already implicitly capturing phrase-level aspects in the multi-head attention. I would urge the authors to verify this by looking at gradient information (https://arxiv.org/abs/1312.6034).\n\nThere are several unsubstantiated claims: \"Without specific attention to phrases, a particular attention function has to depend entirely on the token-level softmax scores of a phrase for phrasal alignment, which is not robust and reliable, thus making it more difficult to learn the mappings.\" - The attention is positional, but not necessarily token-based. The model has capacity to represent phrases in subsequent layers. WIth h heads , a position in the k-th layer can in principle represent h^k grams (each slot in layer 2 can represent a h-gram and so on).\n\nThe differences in training setup compared to Vaswani et al. (2017) needs to be explicit (\"most of the training settings\" is too handwavy). Please list any differences.\n\nThe notation is somewhat cumbersome and could use some polishing. For example, the input and output symbols both range over indices in [1,n]. The multi-head attention formulas also do not match the ones from Vaswani et al. (2017) fully. Please ensure consistency and readability of symbols and formulas.\n\nThe model inspection would be much improved by variance analysis. For example, the numbers in table 3 would be more useful if accompanied by variance across training runs. The particular allocation could well be an effect of random initialization. I could also see other reasons for this particular allocation than phrases being more useful in intermediate layers (e.g., positional encodings in the first layer is a strong bias towards token-to-token attention, it could be that the magnitude of convolved vectors is larger than the batch-normalized unigram encodings, so that logits are larger.\n\nQuestions\n--------------\nIn \"query-as-kernel convolution\", it is unclear whether you map Q[t, :] into n x d_q x d_k convolution kernel parameters, or if each element of the window around Q[t] of width n is mapped to a convolution kernel parameter. Also what is the exact form of the transformation. Do you transform the d_q dimensional vectors in Q to a d_q x d_k matrix? Is this done by mapping to a d_q * d_k dimensional vector which is then rearranged into the convolution kernel matrix?\n\nDoes the model tend to choose one particular n-gram type for a particular position, or will it select different n-gram types for the same position?\n\n\"The selection of which n-gram to assign to how many heads is arbitrary\" - How is this arbitrary? This seems a rather strong inductive bias?\n\n\"However, the homogeneity restriction may limit the model to learn interactions between different n-gram types\" - How is the case? It seems rather that the limitation is that the model cannot dynamically allocate heads to the most relevant n-gram type?\n\nI do not understand equation 14. Do you mean I_dec = I_cross = (...)?\n\n\"Phrase-to-phrase mapping helps model local agreement, e.g., between an adjective and a noun (in terms of gender, number and case) or between subject and verb (in terms of person and number).\" Is this actually verified with experiments / model inspection?\n\n\"This is especially necessary when the target language is morphologically rich, like German, whose words are usually compounded with sub-words expressing different meanings and grammatical structures\" This claim should be verified, e.g. by comparing to English-French as well as model inspection.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}