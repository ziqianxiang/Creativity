{
    "Decision": {
        "metareview": "I tend to agree with reviewers. This is a bit more of an applied type of work and does not lead to new insights in learning representations. \nLack of technical novelty\nDataset too small",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Lack of technical novelty"
    },
    "Reviews": [
        {
            "title": "small technical contribution",
            "review": "The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring. The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation. In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives. The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset. \n\nIn sum, the paper has a very good application but not good enough as a research paper. Some of the problems are listed as follows:\n1.\tLack of technical novelty.  It seems to me just a combination of several mature techniques. I do not see much insight into the problem. For example, if the rule-based concept extractor can already extract concepts very well, the “problem retrieval” should be solved by searching with the concepts as queries. Why should we use embedding to compare the similarity? Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap. \n2.\tData size is too small, and the baselines are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models. \nSome clarity issues. For example, Page 6. “is pre-trained on a pure set of negative samples”— what is the objective function? How to train on only negative samples?\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Proposes a method for mathematical problem embedding but the contribution is not strong",
            "review": "This paper proposes a method for mathematical problem embedding, which firstly decomposes problems into concepts by an abstraction step and then trains a skip-gram model to learn concept embedding. A problem can be represented as the average concept (corresponding to those in the problem) embeddings. To handle the imbalanced dataset, a negative pre-training method is proposed to decrease false and false positives. Experimental results show that the proposed method works much better than baselines in similar problem detection, on an undergraduate probability data set. \nStrong points:\n(1)\tThe idea of decomposing problems into concepts is interesting and also makes sense. \n(2)\tThe training method for imbalanced datasets is impressive. \nConcerns or suggestions:\n1.\tThe main idea of using contents to represent a problem is quite simple and straightforward. The contribution of this paper seems more on the training method for imbalanced data sets. But there are no comparisons between the proposed training method and previous related works. Actually, imbalance data sets are common in machine learning problems and there are many related works. The comparisons are also absent in experiments.\n2.\tThe experimental data set is too small, with only 635 problems. It is difficult to judge the performance of the proposed model based on so small data set. \n3.\tThe proposed method, which decomposes a problem into multiple concepts, looks general for many problem settings. For example, representing a movie or news article by tags or topics. In this way, the proposed method can be tested in a broader domain and on larger datasets.\n4.\tFor the final purpose, comparing problem similarity, I am wondering what the result will be if we train a supervised model based problem-problem similarity labels?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of \"Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring\"",
            "review": "This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring. The proposed method performs much better than baseline sentence embedding methods. Another contribution is on using negative pre-training to deal with an imbalanced training dataset. \n\nTo me this paper is just not good enough - the method essentially i) use \"a professor and two teaching assistants\" to build a \"rule-based concept extractor\" for problems, then ii) map problems into this \"concept space\" and simply treat them as words. There are several problems with this approach. \n\nFirst, doing so does not touch the core of the proposed application. For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper. Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.\n\nSecond, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words. I am not sure how this will generalize to a larger number of problem spanning many different domains.\n\nI also had a hard time going through the paper - there aren't many details. Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the \"rule-based concept extractor\", which is the key technical innovation.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}