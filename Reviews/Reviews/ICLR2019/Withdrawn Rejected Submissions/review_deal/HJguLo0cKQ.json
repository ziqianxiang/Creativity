{
    "Decision": {
        "metareview": "The work brings little novelty compared to existing literature. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Possibly significant result but requires more experimental analysis",
            "review": "This paper presents a new adversarial training defense whereby an ensemble of models is trained against both benign and adversarial examples. The authors demonstrate on the CIFAR-10 dataset that the ensemble has improved robustness against a wide variety of white-box and transfer-based black-box attacks compared to other adversarial training techniques. The results appear significant but would greatly benefit from more thorough experiments.\n\nPros:\n- Conceptually simple and intuitive.\n- Thorough baselines and attack methods.\n\nCons:\n- Limited novelty.\n- Needs more experimental validation against other datasets (e.g. ImageNet) and models (e.g. Inception-v3).\n- Table 1 shows that the clean accuracy of adversarially trained models is significantly worse, which suggests that some aspect of training was done improperly.\n\n-----------------------------\n\nI would like to clarify regarding the listed cons:\n\n- Limited novelty: Adversarial training has been well-established as a viable defense against adversarial examples, as well as training a single model against an ensemble of adversarial examples crafted on different networks (Tramer et al. https://arxiv.org/pdf/1705.07204.pdf). While this work is sufficiently different from prior methods, its novelty is insignificant.\n- More experimental validation: Due to the limited novelty, it is crucial that the authors validate their result more thoroughly to eliminate any doubt on applicability, especially against more challenging datasets such as ImageNet. While the experiments on CIFAR-10 are certainly sufficient, it is dangerous, particularly in works on defenses against adversarial examples, to restrict only to a relatively simple dataset.\n- Tramer et al. (https://arxiv.org/pdf/1705.07204.pdf) publicly released their ensemble adversarially trained Inception-v3 model that has the same top-1 and top-5 clean accuracy on ImageNet as the base model. This serves as evidence that it is certainly possible to adversarially train a model without compromise to clean accuracy, especially on a simpler dataset such as CIFAR-10.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple but effective variant on training ensemble of independently adversarially trained models",
            "review": "The paper proposes to train an ensemble of models jointly, where the coupling lies in that at each time step, a set of examples that are adversarial for the ensemble itself is incorporated in the learning.\n\nThe experiments are thorough and compare multiple types of attacks, although they are all based on gradients (while the paper does mention recent attacks that do not rely on gradients so much). The results are rather convincing and show a clear difference between the proposed method and independently training the models of the ensemble (even if each one is training with examples adversarial to itself).\n\nThe paper is clear and well-written.\n\nPros:\n- The superior performance of the proposed method\n- The method is simple and thus could have a practical impact\n- Clear and thorough analysis\n\nCons:\n- Only gradient based attacks (which are somewhat criticized in the introduction) \n- Novelty may be a bit limited: this is a rather small variation on existing stuff (but it works rather well)\n\nRemarks:\n- Fig 2c could use the same line styles and order as Fig 2a/2b\n- \"a gap 7 accuracy\"?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Empirical Study of Robustness of (small) Ensembles of Neural Nets",
            "review": "Summary. The paper considers the robustness of neural nets against adversarial attacks. More precisely, the authors experimentally investigate the robustness of ensembles of neural nets. They empirically show that adversarially trained ensembles of 2 neural nets are more robust than ensembles of 2 adversarially trained neural nets.\n\nPros.\n* Robustness of neural nets is a challenging problem of interest for ICLR\n* The paper is easy to read\n* Experimental results compare different algorithms for 2 neural nets\n\nCons.\n* The study is experimental\n* It is limited to gradient-based attacks\n* It is limited to ensembles of size 2\n* The Ensemble2Adv is a single NN model and not an ensemble model. \n\nEvaluation.\nThe problem is significant and the use of ensemble methods for robustness against adversarial attacks is a promising line of research. The experimental study in this paper opens new lines of research in this direction. But, in my opinion, the paper is not ready for publication at ICLR. Detailed comments follow but the study is limited to k=2; the main finding is limited to the comparison between bagging two adversarially trained neural nets (SeparateEnsemble2Adv) and learning adversarially the average of two neural nets (Ensemble2adv). In my opinion, Ensemble2adv is a single model of double size and not an ensemble model thus somehow contradicting the main claim of the paper.\n\nDetailed comments.\n* Introduction, end of ยง2, it is said that non-gradient based attacks are still effective. But in the sequel you only consider gradient-based attacks and never discussed this question.\n* Introduction, contributions, it should be made clear at the beginning of the paper that you will consider ensembles of size 2 and only gradient-based attacks.\n* Section 2. The momentum-based attack should be cited and could be considered. \"Boosting adversarial attacks with momentum, Dong et al, CVPR18\"\n* Section 3, ยง2, the discussion on ensemble methods is unprecise. Ensemble methods have different objectives. For instance, Bagging-like methods  aim at reducing the generalization error while others as Boosting aim at augmenting the capacity of individual models.\n* Section 3. Here is my main concern on this paper. The classical method would be bagging of neural nets with different initializations. The neural nets could be adversarially trained. This would lead to the so-called SeparateEnsemble2Adv. Here, the authors consider another method. Their method can be viewed as k(=2) copies of the same neural network with different initializations and an additional layer computing the average of the k(=2) outputs. Then adversarially learn the obtained model which leads to the so-called Ensemble2Adv algorithm. This algorithm is not an ensemble method as such. In my opinion for k=2, it is equivalent to doubling the size of a neural net, adding averaging of the outputs, and adversarially training the obtained neural net.\n* Note recent advances in ensemble NNs with papers such as Averaging weights leads to wider optima ..., Izmailov et al, UAI18; Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Garipov et al, arXiv:1802.10026\n* Section 4.1. Here comes the limitation k=2. The case k=4 is considered in table 1 but is not discussed elsewhere in the paper.\n* Section 4.1. I am not convinced by DoubleAdv. It is one way of doubling the size of a neural net but I am not convinced that this is the more efficient. As said before, in my opinion, Ensemble2Adv is another way for doubling the size. And many more should exist.\n* Section 5. In my opinion, the main comparisons should concern SeparateEnsemble2Adv and Ensemble2Adv. Also other methods doubling the size should be considered. \n* Section 5. For k greater than 2, SeparateEnsemblekAdv should be the better method because the adversarial learning phase could be easily parallelized.\n* I am not convinced by the discussion in Section 6.\n* Typos. and -> an l-13, p5; IFGSM5, l-19 p6; then l-6 p7; to due l-6 p9\n* Biblio. Please give complete references\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}