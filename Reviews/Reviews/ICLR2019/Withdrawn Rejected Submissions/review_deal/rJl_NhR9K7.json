{
    "Decision": {
        "metareview": "The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA). The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details. The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior. Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high-dimensional observations. Therefore, it is difficult to evaluate the significance of ISA-VAE. The authors are encouraged to carefully revise their paper to address these concerns. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "VAE with ISA prior"
    },
    "Reviews": [
        {
            "title": "The paper was not clearly written and failed to provide enough details.",
            "review": "The paper used the family of $L^p$-nested distributions as the prior for the code vector of VAE and demonstrate a higher MIG. The idea is adopted from independent component analysis that uses rotationally asymmetric distributions. The approach is a sort of general framework that can be combined with existing VAE models by replacing the prior. However, I think the paper can be much improved in terms of clarity and completeness.\n\n1. The authors used MIG throughout section 4. But I have no idea what it is. Does a better MIG necessarily imply a good reconstruction? I am not sure if we can quantify the model performance by the mere MIG, and suggest the authors provide results of image generations as other GAN or VAE papers do. \n2. Is the \"interpretation\" important for high dimensional code $z$? If yes, can the authors show an example of interpretable $z$?\n3. I had difficulty reading Section 4, since the authors didn't give many technical details; I don't know what the encoder, the decoder, and the specific prior are. \n4. The authors should have provided a detailed explanation of what the figures are doing and explain what the figures show. I was unable to understand the contribution without explanations.\n5. Can the authors compare the proposed prior with VampPrior [1]?\n\nThe paper should have been written more clearly before submission.\n[1] Tomczak, Jakub M., and Max Welling. \"VAE with a VampPrior.\" arXiv preprint arXiv:1705.07120 (2017).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Several Interesting New Priors Are Proposed For The Latent Variables in VAE",
            "review": "The authors point out several issues in current VAE approaches, including the rotational symmetric Gaussian prior commonly used. A new perspective on the tradeoff between reconstruction and orthogonalization is provided for VAE, beta-VAE, and beta-TCVAE. By introducing several non rotational-invariant priors, the latent variables' dimensions are more interpretable and disentangled. Competitive quantitative experiment results and promising qualitative results are provided. Overall, I think this paper has proposed some new ideas for the VAE models, which is quite important and should be considered for publication.\n\nHere I have some suggestions and I think the authors should be able to resolve these issues in a revision before the final submission:\n1) The authors should describe how the new priors proposed work with the \"reparameterization trick\". \n2) The authors should at least provide the necessary implementation details in the appendix, the current manuscript doesn't seem to contain enough information on the models' details.\n3) The description on the experiments and results should be more clear, currently some aspects of the figures may not be easily understood and need some imagination. \n4) There are some minor mistakes in both the text and the equations, and there are also some inconsistency in the notations.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall method is not presented",
            "review": "This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results.\n\nI understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described.\nP1) What is the shape of the posterior distribution?\nP2) How does the reparametrization trick work in your case?\nP3) How can one choose the layout of the subspaces, or this is also learned?\n\nMoreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. \n\nWhen discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect.\n\nIn addition, there are other (secondary) questions that require an answer.\nS1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces?\nS2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right?\nS3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript?\nS4) Which parameters of this family are learned, and which of them are set in advance?\nS5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this?\nS6) How is the Lp layout chosen?\nS7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE?\nS8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed.\n\nFinally, there are a number of minor corrections to be made.\nAbstract: latenT\nEquation (3) missig a sum over j\nFigure 1 has no caption\nIn (8), should be f(z) and not x.\nBefore (10), I understand you mean Lp-nested\nI did not find any reference to Figure 3\nIn 4.1, the standard prior and the proposed prior should be referred to with different notations.\n\nFor all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}