{
    "Decision": "",
    "Reviews": [
        {
            "title": "Novel model on zero-shot VQA, however, a lot of details is not clear in the paper. ",
            "review": "[Summary]\nThis paper study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. The authors tackle the problem in two steps, 1) learning a task conditional visual classifier based on unsupervised task discovery, and 2) transferring and adapting the task conditional visual classifier to visual question answering model. With these two steps, the VQA model is formulated as p(a|v(I,q), eta_(q)) where v(I,q) is the visual feature with attention and eta_(q) is the task specification vector. During the pre-training stage, the embedding of task description can be obtained from two sources, word embedding of the name of the answer set, or caption with <blank> which replace the answers which encoded by a GRU. The authors verify the proposed method by re-construct the VQA 2.0 dataset, which the out-of-vocabulary answer appears during pre-training. The experiment shows that the proposed method is better than the baseline methods. \n\n[Strength]\n1. The proposed model is the first work on zero-shot VQA model which can handle out-of-the-vocabulary answers. \n\n2. Experiment results show the proposed method is effective on the proposed splits. \n\n[Weakness]\n1. There are several parts of the paper is super unclear, can the authors answer my following questions? \n\n- What is the model for each module proposed in the paper? There is no model figure or description at all, which make the proposed method is hard to understand or replicate the results. \n\n- Page 4, weakly supervised task regression, what is the new indirect loss, and how to calculate it, could the authors explain more? \n\n- Page 5, wordnet, how to select which node of the WordNet hierarchy to use as the task specification name t_s? \n\n- Page 6, visual description, how to get the visual description? coco caption annotation or generated by some image captioning model? \n\n- Page 6, how to combine these two linguistic knowledge source? \n\n2. Page 5, footnote mention that the ambiguity of the reference is usually resolved by attention model. what if the attention model attends the wrong parts? \n\n3. Given the WordNet answer set W_ts, how to select the answer from this set? will this differ from the visual description?\n\n4. It seems the dataset split is delicately constructed for the proposed method, will the proposed method apply on normal VQA split? \n\n5. What is the performance with standard VQA 2.0 split, such as train on train and test on val. the novel answers could be obtained by enlarging the VQA answer number. Although the novel answer maybe only just a small portion of the test set, it will be worth checking the performance on this split and compare with other methods. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Approach to “zero-shot” VQA which decouples task prediction from answer prediction for generation; requires additional and more careful experimental evaluation.",
            "review": "The paper looks at the task of extending VQA to a novel answer space, for which there are no corresponding Questions-Image-Answer Triples available at training time, but separate Text and Image classification information exists for these answers. The paper proposes a model which decouples task prediction from answer prediction to allow the model to generalize to novel answers.\n\nStrength:\n1.\tInteresting direction of extending supervised VQA to a novel answer space.\n2.\tModel successfully allows transfer from different task data.\n3.\tOverall novel model which aims to decouple task prediction from answer prediction.\n\nWeaknesses:\n4.\tRelated work: \n4.1.\tThe paper mentions the zero-shot VQA work from Teney & Hengel, however, given that is one of the most related works, I expect a more thorough discussion of the similarity in dataset and method. Is it possible to run the approach from Teney & Hengel on the dataset of the authors or run the authors approach on the zero-shot answer split?\n4.2.\tThe paper is related to zero-shot recognition, which is an established field in computer vision, however this line is not discussed sufficiently, see e.g. Xian et al. for a discussion on the topic.\n4.3.\tWhile the paper discusses the setting of Agrawal et al. (2018), it misses to discuss how this works related to the GVQA model proposed by Agrawal et al, which has a several similar aspects to the model proposed in this work (although it is overall different).\n5.\tEvaluation\n5.1.\tThe paper only evaluates on the novel classes, but it is unclear what happens if the model encounters known “classes”, i.e. has to answer with known answers from the training set. I think it is critical to also evaluate this scenario. For image classification this is sometimes referred to as “Generalized zero shot”, see e.g. Xian et al. CVPR 2017, or for just evaluating including known classes as distractors see Rohrbach CVPR 2010.\n5.2.\tIt also would be good to include a baseline/ablation which does not look at the question, i.e. without task prediction.\n5.3.\tThe paper evaluation consists of a plot which shows the performance over training iterations. Instead the paper should pick the iteration on a validation set and report results on a test set as a single number, best in a table, which allows clear comparison for future work.\n5.4.\tThe paper uses a “test-validation” set which includes the novel answers, which are also in the test set. However, as the validation set is part of model optimization, i.e. training of the model, any validation set, including “test-validation” should not include novel answers included in the test set. Instead I suggest the dataset should split a “test-validation” which does not overlap with test nor with the training set, to do proper validation without conflict of the test set.\n\n\nConclusion: Overall a great direction and interesting approach but requires more careful experimental setup and evaluation and discussion of related work for acceptance.\n\n\nReferences: \nXian, Y., Schiele, B., & Akata, Z. (CVPR 2017). Zero-shot learning-the good, the bad and the ugly. \nAnd/or the TPAMI version Zero-shot learning - A comprehensive evaluation of the good, the bad and the ugly; 2018\n\nRohrbach, M., Stark, M., Szarvas, G., Gurevych, I., & Schiele, B. (CVPR 2010). What helps where â?? and why? Semantic relatedness for knowledge transfer.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting Work",
            "review": "==========\nStrengths:\n==========\n\n- Out-of-vocabulary VQA is an interesting and timely task. \n\n- The presented approach leverages region annotations (Visual Genome attributes and objects) and descriptions (COCO Captions) to learn a task-conditioned visual classifier. This seems fairly intuitive and an efficient reuse of existing data to learn visual groundings for out-of-vocabulary words. \n\n- Overall the qualitative and quantitative results support the proposed method's effectiveness. \n\n- I really appreciate the extended results in the supplement.\n\n- The paper is generally written well with clear descriptions and useful figures. Thanks!\n\n- Thank you for using multiple random seeds and reporting variance. \n\n==========\nConcerns:\n==========\n\n[A] How is the initial answer space of the task-conditioned classifier set? The dataset construction paragraph is somewhat vague about what a 'pretrained visual word' is. Is it the 3000 objects and 1000 attributes from Visual Genome referenced in the pretraining paragraph?\n\n[B] The task construction for visual descriptions could be clearer. From my understanding, you select a caption and randomly remove a word, replacing it with the <blank> symbol. This word becomes the answer and the RNN encoded blanked-caption is the task. How is the word selected? \n\n[C] Minor point: I think the VQA score listed in 5.1 is incomplete. If I understand correctly, the calculation should be run over all 10 choose 9 sets of answers and the mean reported.\n\n[D] Small request: Could the authors add a list of answers in train and test in the supplement? Aside from academic curiosity, I would also like to get a sense for how many of these answer refer to novel concepts / entities / relationships as opposed to being synonyms for existing ones in train. Any attempt to quantify this would also be appreciated!\n\n==========\nOverview:\n==========\n\nI'm overall positive on this paper. I think the problem is interesting and the approach leverages existing source of visual grounding in a exciting way. The results in the main paper and supplement paint a convincing picture of the method's efficacy. There are a couple of things that I would like more clarity on in the submission.\n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}