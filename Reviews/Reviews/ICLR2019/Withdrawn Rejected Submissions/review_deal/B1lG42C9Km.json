{
    "Decision": {
        "metareview": "The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Interesting paper, possibly some confusion on some causal modelling (especially Section 2.1)",
            "review": "The paper introduces a new intrinsic reward for MARL, representing the causal influence of an agent’s action on another agent counterfactually. The authors show this causal influence reward is related to maximising the mutual information between the agents’ actions. The behaviour of agents using this reward is tested in a set of social dilemmas, where it leads to increased cooperation and communication protocols, especially if given an explicit communication channel. As opposed to related work, the authors also equip the agents with an internal Model of Other Agents that predicts the actions of other agents and simulates counterfactuals. This allows the method to run in a decentralized fashion and without access to other agents’ reward functions.\n\nThe paper proposes a very interesting approach. I’m not a MARL expert, so I focused more on the the causal aspects. The paper seems generally well-organized and well-written, although I’m a bit confused about the some of the causal modelling decisions and assumptions. This confusion and  some potential errors, which I describe in detail below, are the reason for my borderline decision, despite liking the paper otherwise. \n \nFirst, I’m a bit confused about the utility of the Section 2.1 model (Figure 1), mostly because of the temporal and multiple agents aspects that seem to be dealt with (“more”) correctly in the MOA model. Specifically in Figure 1, one would need to assume that there is only one agent A influencing agent B at the same time (and agent B does not influence anything else). For example, there is no other agent C which actions also influence agent B, and no agent D that is influenced by agent B, otherwise the backdoor-criterion would not work, unless you add also the action of agent C to the conditioning set (or its state). Importantly, adding the actions of all agents, also a potential agent D that is downstream of B would be incorrect. So in this model there is some kind of same time interaction and there seems to be the need for a causal graph that is known a priori. These problems should disappear if one assumes that only the time t-1 actions can influence the time t actions, as in the MOA model. I assume the idea of the Figure 1 model was to show a relationship with mutual information, but for me specifically it was quite confusing. \n\nI was much less confused by the MOA causal graph represented in Figure 4, although I suspect there are quite some interactions missing (for example s_t^A causes u_t^A similarly to the green background? s_t causes s_{t+1} (which btw in this case should probably be split in two nodes, one s_{t+1} and one s_{t+1}^B?). Possibly one could also add the previous time step for agent B (with u_{t+1}^B influenced by u_t^B I would assume?). As far as I can see there is no need to condition on a_t^B in this case to see the influence of a_t^A on a_{t+1}^B, u_t^A and s_t^A should be enough?\n\nMinor details:\nIs there possibly a log missing in Eq. 2?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "INTRINSIC SOCIAL MOTIVATION VIA CAUSAL INFLUENCE IN MULTI-AGENT RL\n\nMain Idea: The authors consider adding a reward term to standard MARL which is the mutual information between its actions and the actions of others. They show that adding this intrinsic social motivation can lead to increased cooperation in several social dilemmas. \n\nStrong Points:\n-\tThis paper is a novel extension of ideas from single agent RL to multi agent RL, there are clear benefits from doing reward shaping in the right way to make deep RL work better.\n-\tThe paper focuses on cooperative environments which is an underfocused area in RL right now\n \nWeak Points:\n-\tThere is missing discussion of a lot of literature. The causal influence term can be thought of as a form of reward shaping. There is little discussion on the (large) literature on reward shaping to get MARL to exhibit good behavior. \n-\tThe results feel quite thin. Related to the point above: the theory of different types of reward shaping (e.g. optimistic Q-learning, prosociality, etc…) are well understood. It is not clear to me under what conditions the authors’ proposed augmentation to the reward function will lead to better or worse outcomes. The experiments in this paper are quite simple and only span a small set of environments so it would be good to have at least some formal theory.\n-\tSocial dilemmas don’t seem like the best application. The authors define the social dilemma as: “For each individual agent, ‘defecting’ i.e. non-cooperative behavior has the highest payoff.” With the intrinsic motivation, agents learn to cooperate. This is good, however, if we’re thinking about situations where agents aren’t trained together and have their own rewards (the authors’ example: “autonomous vehicles are likely to be produced by a wide variety of organizations and institutions with mixed motivations”) then won’t these agents be exploited by rational agents? Other solutions to this problem (e.g. recent papers on tit-for-tat by Lerer & Peysakhovich or LOLA by Foerster et al. construct agents where defectors get explicitly punished and so don’t want to try exploiting). Is there something I am missing here? Do the agents learn to punish non-cooperators (if no, isn’t it rational at that point to just not cooperate and won’t self-driving cars trained via this method get exploited by others)? \n-\tRelate to the point(s) above: a better environment for application here seems to be coordination games/”Stag Hunt” games where it is known that MARL converges to poor equilibria and many other methods e.g. optimistic Q-learning or prosociality have been invented to make things work better. Perhaps the method proposed here will work better than these (and it has the appealing property that it does not require the ability to observe the other agents' rewards as e.g. prosociality does)\n-\tThis paper contains some quite grandiose language connecting the proposed reward shaping to “how humans learn” (example: It may also have correlates in human cognition; experiments show that newborn infants are sensitive to correspondences between their own actions and the actions of other people, and use this to coordinate their behavior with others) it’s unclear to me that humans experience extra reward for their actions having high mutual information (and/or causal information with others). While it’s fine to argue some of these points at a high level I would suggest scrubbing the text of the gratuitous references to this.\n\nNits:\n“Crawford & Sobel (1982) find that once agents’ interests diverge by a finite amount, no communication is to be expected.” – this is an awkward phrasing of the Crawford and Sobel result (it can be read as “if interests diverge by any epsilon there can be no communication”). The CS result is that information revealed in communication (in equilibrium) is proportional to amount of common interest. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of Paper \"Intrinsic Social Motivation via Causal Influence in Multi-Agent RL\"",
            "review": "This paper proposes an approach to model social influence in a scenario-independent manner by instantiating the concept of intrinsic motivation and combine various human abilities as part of a reinforcement learning function in order to improve the agent's operation in social dilemma scenarios. \n\nAgents are operationalised as convolutional neural network, linear layers and LSTM. Using these base mechanisms, different abilities (communication, models of other agents (MOA)), their causal influence is inferred based on counterfactual actions. The architecture is explored across two different sequential social dilemmas. \n\nThe architecture is described in sufficient detail, with particular focus on the isolation of causal influence for communication and MOA influence. The experimental evaluation is described in sufficient detail, given the low complexity of the scenarios. While the agents with communicative ability and MOA show superior performance, a few results warrant clarification.\n\nFigure 6a) highlights the performance of influencers in contrast to a visible actions baseline. This specific scenarios shows the necessity to run experiments for larger number of runs, since it appears that action observations may actually outperform influencer performance beyond 3 steps. Please clarify what is happening in this specific case, and secondly, justify your choice of steps used in the experimental evaluation. \n\nAnother results that requires clarification is Figure 6f), which is not sufficiently discussed in the text, yet provides interesting patterns between the MOA baseline performance decaying abruptly at around 3 steps, with the influence MOA variant only peaking after that. Please clarify the observation. Also, could you draw conclusions or directions for a combination of the different approaches to maximise the performance (more generally, beyond this specific observation)? \n\nA valuable discussion is the exemplification of specific agent behaviour on Page 7. While it clarifies the signalling of resources in this specific case, it also shows shortcomings of the model's realism. How would the model perform if agents had limited resources and would die upon depletion (e.g. the de facto altruistic influencer in this scenario - since it only performs two distinct actions)? The extent of generalisability should be considered in the discussion. \n\nIn general, the paper motivates and discusses the underlying work in great detail and is written in an accessible manner (minor comment: the acronym LSTM is not explicitly introduced). The quality of presentation is good. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}