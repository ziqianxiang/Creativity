{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice idea, clean exposition, partly thin experiment",
            "review": "The paper proposes to inspect cross-lingual word embeddings as graphs, and suggests that high modularity of these graphs represents groupings by languages, and subsequently yields poor performing NLP models if they are based on such embeddings.\n\nThe idea is simple, clean, and nicely exposed. The experiment clearly suggests that inspecting modularity of graphs into which cross-lingual embeddings are cast makes a lot of sense: As per Figure 3, for example, modularity and performance are strongly inversely correlated. Modularity is further shown to be superior to alternative metrics in section 5.\n\nThe paper does leave me wanting for more cross-lingual embeddings to be analyzed in the experiment, as the list of four approaches does not chart the space of related work that nicely, e.g., no methods that feature parallel corpora as learning signal are included. For an updated version, I would be happy to see a nicer sample of embeddings construction methods.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Should be better supported both theoretically and empirically",
            "review": "This paper presents a new evaluation method for cross-lingual word embeddings. The paper argues that the quality of cross-lingual word embeddings is correlated with their modularity (i.e. whether words in one language tend to appear next to words of the same language alone), proposes a new evaluation metric based on this idea, and explores the relationship between this metric and downstream performance in two tasks.\n\nWhile I think that the core idea of the paper is original and has some interest, I would say that the paper is not strong enough to get accepted. The topic of the paper seems rather narrow to me and, in addition to that, I think that the proposed method should be better supported both theoretically and empirically.\n\nA few comments:\n\n- It is not clear to me what the use case of the proposed method is. In particular, the proposed evaluation seems valid when applied to different cross-lingual mapping methods over the exact same embeddings, but its validity beyond that is not obvious nor tested empirically in the experiments. Is the proposed measure comparable across languages? What about different embedding learning methods and/or hyperparameters (e.g. CBOW vs skip-gram)? In other words, can the proposed method be used to predict downstream performance for any given set of cross-lingual embeddings (possibly using entirely different learning algorithms), or is it only valid to rank different cross-lingual mapping methods as applied to the exact same embeddings?\n\n- I assume that language typology can play an important role here, but this is not discussed at all in the paper. In particular, I think that the core idea of the paper can be somewhat questionable for languages that diverge morphologically. For instance, Spanish distinguishes between \"lento\" (masculine singular), \"lenta\" (feminine singular), \"lentos\" (masculine plural) and \"lentas\" (feminine plural), whereas English has a single word (\"slow\") for all of them. Should \"lento\" be closer to \"slow\" than to \"lenta\", \"lentos\" and \"lentas\"? That's what your proposal would favor, but it is not obvious at all to me! And this is probably the simplest case, as this phenomenon would be accentuated for morphologically rich languages.\n\n- Your experiments only compare 4 cross-lingual embedding mapping methods. This seems too little to me and, to make things worse, the compared methods are strongly connected. In particular, Artetxe et al. (AAAI 2018) show that MSE, CCA and MSE+Orth can be seen as variants of the same general framework, and the unsupervised method of Conneau et al. (ICLR 2018) also uses MSE+Orth in its iterative refinement. I think that you could (and should) compare more cross-lingual mapping methods and, more importantly, you should include other cross-lingual embedding methods that are not based on mappings (e.g. bilbowa and bivec).\n\n- The proposed evaluation method is compared to QVEC-CCA and cosine similarity between translation pairs. This looks like a very unusual choice to me, as most papers in the topic evaluate on bilingual lexicon induction and, to a less extent, cross-lingual word similarity. I think that you should definitely compare your proposed method to word translation accuracy, and including cross-lingual word similarity would also be good.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple idea, off-the-shelf comparison, discussion incomplete",
            "review": "The authors introduce the very simple idea of correlating graph modularity of cross-lingual embedding nearest neighbour graphs with downstream performance on cross-language document classification. Pros and cons: \n\na) Pro: The idea of checking whether nearest neighbors are overwhelmingly in-language, is very intuitive, and it is surprising that no one did this before.\nb) Pro: The empirical analysis of their method is relatively thorough. \nc) Pro: The authors run experiments on actual low-resource languages (bonus points!). \nd) Con: Cross-language document classification was introduced as a way to evaluate cross-lingual embeddings in Klementiev et al. (2012), but is a very crude way of evaluating cross-lingual embeddings; see Upadhyay et al. (2016) or Ruder et al. (2017) for discussion and alternatives. \ne) Con: In their comparison, the authors simply ran off-the-shelf methods. This is not very informative. For example, MSE+Orth uses unit length normalization, projecting both sets of embeddings onto the same sphere. This is likely to decrease modularity. It is, however, unclear whether that is why MSE+Orth works best in your experiments. Similarly, MUSE uses GANs and Procrustes refinement; it would be more apples-to-apples to compare again MUSE using the same dictionary seeds you used for CCA and MSE (rather than the GAN-induced seeds). \nf) Con: Several important details were omitted. E.g.: What dictionary seeds were used? What hyper-parameters did you use? In MUSE, for example, performance is very sensitive to hyper-parameters such as drop-out rate and the number of epochs. \ng) Con: The authors do not discuss the effect of random initialization; see Artetxe et al. (2018) or Søgaard et al. (2018). Specifically, MUSE is known to be very unstable, and the above authors report performance over 10 randomly initialised runs. \nh) Con: The authors do not investigate what causes modularity. In GANs, for example, this could be a result of mode collapse. In the case of MSE, why do the authors think modularity is lower than MSE+Orth (assuming this holds in the normalized case)?\ni) Con: The authors do not discuss the linearity assumption implicit in all these methods. Nakashole et al. (2018) argues that often (ideal) mappings are not linear. How would this effect these methods?\n\nIn sum, I think modularity is probably an important concept in diagnosis, but I think the discussion is incomplete, ignoring the inductive biases of the embedding algorithms and the impact of hyper-parameters. While the empirical analysis was thorough, the main experiments were on a somewhat uninformative task (topic classification). I was also a bit under-whelmed by the off-the-shelf comparison of existing methods. \n\nQuestions and minor comments: \n\nThe authors say: “we focus on mapping-based approaches because of their applicability to low-resource languages by not requiring large bilingual dictionaries or parallel corpora”. An important observation in Ruder et al. (2017), however, is that mapping-based approaches can be equivalent to joint and pseudo-mixed corpus approaches.\n\nIn §2.2., you mention QVEC as a method for measuring consistency, monolingually. I guess coherence can also be measured by analogy, relatedness, or computing the distances (in embedding space) of edges in a knowledge base, say WikiData? This would be possible across languages. \n\nThe problem with mismatched senses is simply a side-effect of one-embedding-per-word-form, right? This is not mentioned explicitly. \n\nThe idea of checking whether nearest neighbors are overwhelmingly in-language, is intuitive, and it is surprising that no one did this before. However, I find statements such as “Existing approaches do not reliably detect this problem” a bit misleading. I feel it would be more fair to talk about the contribution here being a “simple observation” rather than a new approach. On a similar note, modularity is a very simple concept, so, again, it’s a little misleading to say the authors “ apply concepts from network science”. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}