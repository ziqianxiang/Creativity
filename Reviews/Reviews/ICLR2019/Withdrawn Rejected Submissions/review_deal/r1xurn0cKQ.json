{
    "Decision": {
        "metareview": "This is a difficult decision, as the reviewers are quite polarized on this paper, and did not come to a consensus through discussion. The positive elements of the paper are that the method itself is a novel and interesting approach, and that the performance is clearly state of the art. While impressive, the fact that a relatively simple task module trained on the features from Zhu et al. can match the performance of GAZSL suggests that it is difficult to compare these methods in an apples-to-apples way without using consistent features. There are two ways to deal with this: train the baseline methods using the features of Zhu, or train correction networks using less powerful features from other baselines.\n\nReviewer 3 pointed this out, and asked for such a comparison. The defense given by the authors is that they use the same features as the current SOTA baselines, and therefore their comparison is sound. I agree to an extent, however it should be relatively simple to either elevate other baselines, or compare correction networks with different features. Otherwise, most of the rows in Table 1 should be ignored. Running correction networks in different features in an ablation study would also demonstrate that the gains are consistent.\n\nI think the authors should run these experiments, and if the results hold then there will be no doubt in my mind that this will be a worthy contribution. However, in their absence, I can’t say with certainty how effective the proposed method really is.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Novel approach, but needs stronger comparisons."
    },
    "Reviews": [
        {
            "title": "Official Review",
            "review": "Summary: This paper proposes a “meta-learning” approach for zero-shot learning. There is a Task Module that works in a conventional zero-shot way: Training to predict a class prototype using the auxiliary/text data description of that task. The new part is the added Correction Module that inputs both the target/zero-shot task description, the training task description, and the current prediction of the task module, and then outputs a correction vector that is added to the output of the task-module to produce the final output. The resulting system achieves state of the art results on zero-shot fine-grained classification (CUB and NAB).\n\nAssessment: Overall this might be a good idea worthy of publication at some point. But despite the good results, the current realisation is not well analysed about exactly how and why it works, with no insight being provided; and leaves some doubt about the validity of the comparative experiments. The writing is also very rushed. It is not ICLR standard yet.\n\nStrengths:\n+ Interesting idea overall.\n+ Good results.\nWeaknesses:\n- Poor clarity. \n- Some experimental evaluation questions. \n- Poor analysis.\n\nComments:\n1. The correction module inputs the full set of training features T_s (Alg1-L13). However the training dataset is fixed, therefore this input is effectively a constant. So its not clear how a constant input can possibly be useful. \n1.1 Possibly this has something to do with the episodic training, but this is exactly the kind of thing that should be analysed and explained, but is not discussed at all.\n2. The paper is sold as a meta-learning paper, but it’s not clearly explained what is the “meta” part of the algorithm.\n3. Its not explained anywhere how exactly the T_s, T_s^u, etc are fed into the correction network. Is it average pooling? It seems that simple average pooling is unlikely to be adequate given the large number (150) of classes in CUB.\n4. There are no experimental details such as hyper parameters, network architecture, etc.\n5. Based on the ablation study (Tab 2), the baseline task network without correction network already achieves state of the art results. Conceptually the task-network alone is a very standard “regression” based approach to ZSL of the type that people tried almost 10 years ago. So what is the explanation for why its so good? This makes the comparison to all the competitors in Tab1 suspect. If there is some reason (E.g., better image feature extractor or pre/post-processing) that makes the ultra simple baseline there already outperform SotA, then you have to ask how all the prior methods would perform if they were run with the same tweak. \n6. Overall no insight provided about what kind of corrections are made, when they are useful, etc. This is important to provide insight about how/why correcting outputs can work.\n7. There is nothing particularly unique about this setup for ZSL. It could equally be applied to correct outputs in the case of few-shot learning (CF: Prototype Networks). It would be more convincing if it was applied to both settings and analysed better for both.\n8. The writing is very rushed. There are lots of writing and editorial errors. To name a few: P4 Extra “Task module is trained to minimise.” P4 “\\mu_u” Is repeated. Citation style “Mohamed Elhoseiny & Elgammal” is wrong, check the bibtex.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but many flaws.  ",
            "review": "This paper presents an interesting idea by formulating the problem of zero-shot learning in a meta-learning framework.  Specifically, the proposed model consists of two components: the task module and the correction module, where the former module learns to map the text description of a class to the sample mean and the latter one updates the predictions for unseen classes.   \n\nThe presentation of this paper is very poor.  Proposed meta-framework has some flaws. And, the experiments are not persuasive enough to demonstrate the significance of the proposed framework.  \n\nThe proposed zero-shot classifier is based on the nearest centroid.   Authors formulate the learning problem as mapping the text description of each class to sample mean of the data of the class.   Within a meat-training instance, the training performance is based on L2 distance between the mapped mean and the sample mean of each class.   This setup is wired.  This because, no matter how many data (x, y pairs) we get, the proposed method only makes the prediction based on the pre-calculated mean.  In other words, the \"number of samples\" in a meta-training dataset becomes the number of unique classes appears in training.    For instance, if we have 10 classes in the $D_\\mathcal{S}$, and10000 samples per class,  the proposed setup will consider the meta training only consist of 10 data points.   \n\nIn addition, the proposed method heavily rely on the feature extractor of the image.  The classification performance could be poor if two the mean of different classes close to each other.    Even they are not, the proposed framework cannot provide sample-level generalization.  \n\nAnother confusion I have is why the training of the task module is not based on a fixed correction module?   \n\nThe experiments also have many problems.  Authors need to clearly state how they construct meta-training, validation and testing instance.   Since the proposed framework is a meta-framework, authors need to report their performance in different meta-train/test splits.  The conventional split of CUB and NAB is only considered as a single split.  How well the proposed framework generalizes to other meta splits?     How well the proposed method performance to a generalized zero-shot setting?  \n\nThere are many typos.  Auhtors definitely need to improve their writing and the layout of the paper. \n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Original approach with strong results, but lacks many details",
            "review": "=== Post-rebuttal update ===\n\nThe authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.\n\nHence, I've increased my score for this paper.\n\n=== Pre-rebuttal review ===\nThis paper presents a meta-learning approach to zero-shot learning. The idea is to train a correction module which is trained to produce a correction to the output of a previously trained task module. The hypothesis is that the correction should depend on the nature of the training data of the task module, and so the correction module receives as input a representation of the training data of the task module. An episodic approach is then used for training the correction module, whereby many different task modules are trained on various subsets of the total training data, the rest being used as unseen data for the correction module.\n\nThe proposed idea is original and the results are strong. Generally, I'd be inclined to see this paper published.\n\nHowever right now, the paper lacks A LOT of details on how the experiments were run. I would like to see these answered in the rebuttal, before I consider raising my rating for this paper:\n- What are the architectures used for M_T and M_C?\n- What distance functions was used for training?\n- What optimizer was used for training?\n- How was convergence established in the inner and outer while loops of algorithm 1?\n- Text mentions that before evaluation, M_T is trained on all data in D_S. How is this done exactly (e.g. how is convergence assessed)?\n- How is the T_S computed exactly?\n- How expensive is it to run Algorithm 1 (i.e. to train the correction module)? Since a new task module M_T needs to be trained for each subset S^s, it seems like it might be expensive to run... if not, why?\n\nI would also strongly suggest the authors release their code if this paper ends up being published.\n\nIn summary:\n\nPros\n- Claims SOTA results on two good benchmarks for zero-shot learning\n- Approach is original\n\nCons\n- Paper lacks a lot of methodological and experimental details\n\nSome minor details:\n\n- \"We found the task module performance improves slightly when the output of the task module is feed into a classifier with a single hidden layer that is also trained to classify samples from the task model’s training dataset.\" => I don't understand what this means. Isn't the output of the task module already trained to classify samples from its training dataset? So why is this additional single hidden layer needed?\n- Typos:\n  - on few shot learn => on few shot learning\n  - but needs not => but need not\n  - image image classification => image classification\n  - the the compatibility => the compatibility\n  - psuedo => pseudo\n  - \"The task module is trained to minimize\" => that reads like an unfinished sentence\n  - \\hat{\\mu}_U \\hat{\\mu}_U => \\hat{\\mu}_U \n  - inputted => input\n  - FOr => For\n  - it's inputs => its inputs\n  - otherhand => other hand",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}