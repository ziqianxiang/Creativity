{
    "Decision": {
        "metareview": "The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen-Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a \"low-variance, bias-free algorithm\" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN's parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented \"most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this.\"). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias. \nOn December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera-ready version.  However if one of the \"most important idea\" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term. \n\nThe approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over-claiming and insufficient justification and in-depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera-ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Novel approach with promising results for generative modeling, but with incorrect claims and insufficiently analysed heuristic shortcuts"
    },
    "Reviews": [
        {
            "title": "Original idea, clear presentation",
            "review": "*Summary*\nA clear an interresting presentation on learning sequences distributions. It achieve this objective by replacing the discriminator with a \"mediator\", a mixture between the training distribution and the target distribution which is estimated via maximum likelihood.\n\n*Pros*\n- Original idea for modelling distribution of sequence data\n- Theoretical convergence in the Jensen Shanon divergence sense\n- Promising experiments\n\n*Cons*\n- No major cons to the best of my knowledge\n\n*Typos*\n- It would be very nice to have black and white / color blind friendly graphs\n- Eq 10 too long\n- Introduce J_m & J_g in  sentence\n- Coma at the end of Eq 5, and maybe align Generator and Discriminator in some position (e.g. at the semi colon).\n- missing dot at Eq 8.\n\n*Question*\n- How would you ensure reproducibility (e.g. link to some code?)\n- Is there any hope to obtain consistency (convergence) wrt other metrics?",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting and promising method for generative modelling of sequence data without policy gradient",
            "review": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. \n\nOverall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings.\n\nThe paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. \n\nPros:\n- Generative modeling of sequence data still in its infancy\n- Potentially lower variance than policy gradient approaches\n- Experiments are promising\n\nCons:\n- Lots of grammatical errors and odd formulations\n\nQuestions:\n- Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem?\n- Figure 2: how do (b) and (c) relate to each other?\n\nRemarks, small typos and odd formulations:\n- \"for measuring M_\\/phi\": what does measuring mean in this context?\n- What does small m refer to? Algorithm 1 says the total number of steps  but it is also used in the main text as an index for J and \\pi (for mediator?)\n- Equation block 8: J_m has not been defined yet\n- \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context\n- \"if the training being perfect\"\n- \"tend to get stuck in some sub-optimals\"\n- the learned distribution \"collapseS\"\n- \"since  the data distribution is, thus ...\"\n- \"that measures a\" -> \"that estimates a ...\"?\n- \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology\n- \"is well ensured\"\n- \"with the cost of diversity\" -> \"at the cost of diversity\"?\n- \"has theoretical guarantee\"\n- in the references: \"ALIAS PARTH GOYAL\" (all caps)\n- \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3)\n- \"CoT theoretically guarantees the training effectiveness\": what does that mean?\n- Figure 3: \"epochs\" -> \"Epochs\"\n- Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise\n- \"wide-ranged\"\n- Equation 10 is too long and equation number is not properly formatted\n- Figures hard to read in black & white\n- Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "nice idea",
            "review": "Pros:\nThis paper is easy to follow. The idea is nice in three folds. \n1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence.  \n2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite  goals of G and D in GANs. \n3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts.  \n \nCons:\nSome details are missing in the experiments. \n1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference?\n2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu?\n\n[A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017).\n\nMisc:\n1. How will the number of samples (i.e. batch size) affect CoT ?\n2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}