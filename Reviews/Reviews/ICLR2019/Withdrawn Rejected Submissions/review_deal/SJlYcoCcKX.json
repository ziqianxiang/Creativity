{
    "Decision": "",
    "Reviews": [
        {
            "title": "A neuron manifold-based knowledge distillation method ",
            "review": "The work introduces a knowledge distillation method using the proposed neuron manifold concept. The neuron manifold is interesting and intuitive, and the work also provided a way to implement the concept with various structures.  But, as pointed out by the other reviewers, the improvement is relatively simple, and the experiment is not convincible enough. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Plagiarism",
            "review": "Section 3.1 and 3.2 have been copied practically verbatim from https://arxiv.org/abs/1711.02613, which has not been cited.\n\nThis is plagiarism, and the paper should be immediately rejected. ",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "this work is of very poor quality",
            "review": "Basically, in this paper, the authors proposed a knowledge distilling method in which neural manifold is taken as the transferred knowledge. However, this work is so badly shaped so that readers cannot follow the true contribution of this work. \n\nStrengths:\n- This work is probably the first to distill the structured manifold knowledge from a teacher network to a student network. \n\nWeakness:\n- The proposed method seems to only plug the Moving Least Squares Projection (MLSP) method proposed in (Sober et al. 2017) into the KD framework. Besides, such plug-in is not challenging at all. I cannot see any contribution from the authors. \n-  The experiments are totally not persuasive. \n    - The results on the MNIST dataset seem to be comparable with KD.\n    - The results on the CIFAR10 and CIFAR100 dataset (Figure 3) prove that KD is quite effective, and only NMT combined with KD shows superiority.\n    - There are more baselines than the compared in this work. Even the FSP method mentioned by the authors in the related work should be compared, which transfers the Gram matrix modelling the structure information either.\n- poor presentation\n   -  The  grammatical errors and typos are almost everywhere. I just pick several of them to show in the following.\n       - In Abstract, portable platform -> portable platforms\n       - In Introduction, Based on the idea of (Bucilua, Geoffrey et al.) originally introduced ... -> Based on the idea proposed by Bucilua, Geoffrey et al. originally introduced ... \n       - In Related Works, layer wise consideration, however, Jacobian -> layerwise consideration; however, Jacobian\n   -  The notations and the equations are very confusing.\n       - T is used to denote a teacher network, but meanwhile it also denotes a temperature parameter.\n       - What do you mean by saying p_i = p(f_i) = f_iï¼ŸWhat is the meaning of p and q in Equation (6)? \n       - How do you exactly define the function \\theta in Equation (5) and (6)?\n   - The empirical descriptions are misleading. \n       - The legend in Figure 3 includes a method called Hint, but the authors never mentioned it before.\n       - The authors used ResNet in the Section 5.3, but in Table 2 they say the student network is Alexnet.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}