{
    "Decision": {
        "metareview": "The paper has some nice ideas for efficient exploration, but reviewers think more work is needed before it is ready for publication.  In particular, the paper should have an improved discussion of state-of-the-art work on exploration, compare the difference and benefits of the proposed approach, and then conduct proper experiments to validate the claims.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Nice ideas, but need a better comparison to previous work"
    },
    "Reviews": [
        {
            "title": "An interesting form of \"imitation learning\".",
            "review": "This paper considered the idea of accelerating sampling process by exploring uncertainty of rewards. The authors claimed more efficient sampling by building a reference policy and an exploration policy. Algorithm was tested on grids and Atari games.\n\nThe authors proposed eDQN, which is more like exploring the uncertainty of Q values instead of rewards. The reviewer is also expecting to see the convergence guarantee of eDQN.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice paper",
            "review": "The authors develop a new algorithm for reinforcement learning based on adding another agent rewarded by both the extrinsic environment reward and the TD-errors of the original agent, and use the state-action pairs visited by the added agent as data for the original.\n\nThis co-evolutionary process could be more broadly described as a student agent that learns from the trajectories of a teacher agent, which visits trajectories with high reward and/or high student TD-error.\n\nThe algorithm is not proved to converge to the optimal Q.\nAlgorithm (1) by not using epsilon-greedy on Q_hat\nhas an initialization-based counter-example in the tabular bandit case\ne.g.\n  MDP being a bandit with two arms with rewards 100 and 1000 respectively\n  Q_hat that initially is X for first arm and Y for second arm, with X > 100 > Y\nThis could be solved by, for example, adopting epsilon greedy.\n\nPrioritized Experience Replay (Schaul et al. https://arxiv.org/pdf/1511.05952.pdf), which suggests using the TD-error to change the data distribution for Q-learning, should be also a baseline to evaluate against.\n[Speculative: It feels like a way to make prioritized experience replay that instead of being based on time-steps (state, action, reward, new state) is based on trajectories, and this is done by doubling the number of model parameters.]\n\nOn quality:\nThe evaluation needs different experience replay baselines.\n\nOn clarity/naming:\nHere 'uncertainty in reward space' is used to refer to TD-error (temporal difference), I found that confusing.\nHere 'intrinsic motivation' is used but the 'intrinsic motivation' proposed depends on already having an externally defined reward.\n\nPros:\n+ \"Prioritized Experience Replay for Trajectories with Learning\"\nCons:\n- Not evaluated against experience replay methods.\n- No plots showing number of gradient descent steps (as the proposed method has double gradient descent updates than the baselines)\n- No proof of correctness (nor regret bounds).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A well-intentioned piece of work... but the understanding of prior work / the exploration problem is lacking",
            "review": "This paper suggests an exploration driven by uncertainty in the reward space.\nIn this way, the agent receives a bonus based on its squared error in reward estimation.\nThe resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.\n\nThere are several things to like about this paper:\n- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).\n- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.\n- The algorithm does appear to outperform the basic DQN baseline on their experiments.\n\nUnfortunately, there are several places where the paper falls down:\n- The authors wrongly present prior work on efficient exploration as \"exploration in state space\" ... rather than \"reward space\"... in fact prior work on provably-efficient exploration is dominated by \"exploration in value space\"... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to \"counts\" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853\n\n- So this algorithm falls into a pretty common trope of algorithms of \"exploration bonus\" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)\n\n- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around \"randomized value functions\"\n\n- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.\n\nOverall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.\nIt could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests \"exploration by uncertainty in value space\"... e.g. \"deep exploration via randomized value functions\"",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}