{
    "Decision": {
        "metareview": "This work presents an interesting take on how to combine basic functions to lead to better activation functions. While the experiments in the paper show that the approach works well compared to the baselines that are used as reference, reviewers note that a more adequate assessment of the contribution would require comparing to stronger baselines or switching to tasks where the chosen baselines are indeed performing well. Authors are encouraged to follow the many suggestions of reviewers to strengthen their work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting ideas that requires to more adequate baselines"
    },
    "Reviews": [
        {
            "title": "A nice idea but weak empirical results",
            "review": "* Update:\nThanks for you answer and clarification. While the Morph-net appears novel, the authors only report result for image classification task and don't achieve as good performance as standard convolutional baselines. Given the current empirical evaluation, I find hard to assess how significant is the contribution. I would encourage the authors to either compare on a task where dense networks achieve state-of-art performances or extend their approach to 2D inputs.\n\n\n* Review\n\nThis paper introduces Morph-Net, a new architecture that intertwine morphological operator such as dilation/erosion with linear layer. Authors first show than Morph-Net are universal approximator. Morph-Net can be expressed as a sum of multi-order hinge functions which can approximate any continuous function. They then validate empirically the Morph-Net on  MNIST, FashionMNIST,  CIFAR10 and CIFAR100 datasets. In particular, authors investigate a 3 layers  fully-connected Morph-Net and shows that it can outperform its Tanh/Relu/Maxout counterparts.\n\nThe paper is a nice read also some specific point could be clarify. For instance it is not clear how the structuring elements of the dilation/erosion are learned? Are the learned simply through backpropagation? Also, it is not clear to me how Morph-Net differs from the previously proposed morphological neurons? \n\nEmpirical evaluation of Morph-Net could be improved as well. In particular, authors focus on image classification task. While they show that Morph-Net can outperform other fully connected architecture, the results on CIFAR10/100 seems low compared to convolutional network. It raises the question of the advantages of Morph-Net over convolutional neural networks ?  Authors also limit their exploration to  3-layer networks. Why donâ€™t you explore deeper network for both baseline and Morph-Net?  Finally, if I am not mistaken, authors use the same set of hyperparameters for the baselines/Morph-Net? It is not clear to me if the hyperparameters are optimal for all the approach? They might give an unfair advantage to one of the baseline or Morph-Net?\n\nOverall, this paper present a nice idea. Showing the Morph-Net is an universal approximator is a nice result. However, the empirical evaluation could be improved. It is not clear to me at this point if Morph-Net brings a benefit compare to convolutional net for image classification task.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The proposed idea is to replace standard nonlinear activation function with an erosion/dilation operation. The authors report encouraging results but the baseline networks are not state-of-the-art. ",
            "review": "This paper proposes to replace the standard RELU/tanh units with a combination of dilation and erosion operations, arguing for the observation that the new operator creates more hyper-planes and therefore have more expressive power.\n\nThe paper is interesting and there are encouraging results which show a couple of percentage improvements over relu/tanh units.  This paper is also clearly written and easy to understand. However there are two issues:\n1. It is somewhat unclear from the paper what  is the main novelty here (compared to existing morpho neurons), is it the learning of the structuring element s? is it the combination of the dilation+erosion operations?\n2. The second issue is that presumably due to the fact that Conv layers are not used, the accuracy on cifar-10 and cifar-100 are significantly lower than state-of-the-art. It would make the paper extremely strong if the improvement translated to CNNs which are performing near the state-of-the-art. What happens if relu units in CNNs were swapped out for the proposed dilation/erosion operators?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea for using morphological operators but too preliminary",
            "review": "The authors introduce Morph-Net, a single layer neural network where\nthe mapping is performed using morphological dilation and erosion.\nI was expecting something applied to convolutional networks as such operators\nare very popular in image processing, so the naming is a bit misleading.\n\nIt is shown that the proposed network can approximate any smooth function, \nassuming a sufficiently large number of hidden neurons, that is a nice result.\n\nClarity should be improved, for example it is mentioned that the structuring\nelement is learned but never clearly explained how and what difficulties it poses.\nIn the main text it is written that alpha is {-1, 1}, which would result in a\ncombinatorial search, but never explained how it is learned in practice.\nThis is shown only in the appendix but it is not clear to me that using a binarization\nwith the weights is not prone to degenerate solutions and/or to learn at all\nif proper initialization is not used.\nDid the authors experiment with smooth versions or other form of binarization with\nstraight-through estimator or sampling?\n\nIn the proof for theorem 1 it is not clear if the convergence of the proposed\nnetwork is faster or slower than that of a classic single layer network.\n\nThe main result of the paper is that the structuring element can be learned,\nbut there is no discussion on what it is learned. Also, there is no comparison\non related approaches that try to learn the structuring element in an end-to-end\nfashion such as [1].\n\nExperiments lack a more thorough comparison with state-of-the-art and at least\nan ablation study to show that the proposed approach is effective and has merit.\nFor example, what is the relative contribution of using dilation and erosion\njointly versus either one of them.\nWhat is the comparison with a winner-take-all unit over groups of neurons\nsuch as max-pooling?\n\nIt seems that extending the work to multiple layers should be trivial but it is\nnot reported and is left to future investigations. This hints at issues with\nthe optimization and should be discussed, is it related to the binarization\nmentioned above?\n\nOverall the idea is interesting but the way the structuring element is learned\nshould be discussed in more details and exemplified visually. Experiments need\nto be improved and overall applicability is uncertain at this stage.\n\n=======\n[1] Masci et al., A Learning Framework for Morphological Operators Using Counter--Harmonic Mean.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}