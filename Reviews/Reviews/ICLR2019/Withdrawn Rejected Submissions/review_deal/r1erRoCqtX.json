{
    "Decision": {
        "metareview": "Following the unanimous vote of the four submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not ready for publication ICLR"
    },
    "Reviews": [
        {
            "title": "Requires further clarification and empirical justification",
            "review": "The paper presents a method for improving the convergence rate of Stochastic Gradient Descent for learning embeddings by grouping similar training samples together. The basic idea is that gradients computed on a batch of highly associated samples encode related information in a single update that independent samples might take multiple updates to capture. These structured minibatches are constructed by independently combining subsets of positive examples called “microbatches”. Two methods are presented for constructing these microbaches; first by grouping positive examples by shared context (called “basic” microbatches), second by applying Locality Sensitive Hashing to further partition the microbatches into groups that are more likely to contain similar examples.\n\nThree datasets are used for experimental analysis: a synthetic dataset generated using the stochastic block model, and two large scale recommendation datasets. The presented algorithms are compared to a baseline of independently sampled minibatches using the cosine gap and precision for the top k predictions. The authors show the measured cosine gaps over the course of training as well as the gains in training performance for several sets of hyperparameters.\n\nThe motivation and basic intuition behind the work is clearly presented in the introductory section. The theoretical justification for the structured minibatches is reasonably convincing and invites empirical verification.\n\nGeneral concerns:\nAny method for improving the performance of an optimization process via additional preprocessing must show that the additional overhead incurred from preprocessing the data (in this case, organizing the minibatches) does not negate the achieved improvement in convergence time. This work presents no evidence that this is the case. I expected to see 1) time complexity analysis of each new algorithm proposed for preprocessing and 2) experimental results showing that the overall computation time, including the proposed preprocessing steps, was reduced by this method. Neither of these things are present in this work.\n\nFurthermore, the measured “training gains” are, to my knowledge, not clearly defined. I assume that the authors are using the number of epochs or iterations before convergence as their measure of training performance, but this should be stated explicitly rather than implicitly.\n\nFinally, the experimental results presented do not seem to entirely support the authors’ conclusions. Figures 2, 3, and 4, as well as several of the figures in the appendix, show some parameter settings for which the gains over the baseline are quite limited. This makes me suspect that perhaps the coordinated minibatches aren’t the only variable affecting performance.\n\nI have organized my remaining minor concerns and requests for clarification by section, detailed below.\n\nSection 1\n- In the last paragraph, the acronym SGNS is mentioned before being defined. You should either state the full name of the method (with citation) or omit the mention altogether.\n\nSection 2\n- I would like a few sentences of additional clarification on what “focus” entities vs. “context” entities are in the more general case. I am familiar with what they mean in the context of Skip Gram, but I think more discussion on how this generalizes is necessary here. Same goes for what kappa (“association strength”) means, especially considering that this concept isn’t really present (to my understanding) in Skip Gram.\n- Grammar correction:\n“The negative examples provide an antigravity effect that prevents all embeddings to collapse into the same vector”\n“to collapse” -> “from collapsing”\n\nSection 3\n- Maybe this is just me, but I find the mu-beta notation for the microbatch distributions rather odd. Why not just use a single symbol?\n- I would like a bit more clarification on the proof for lemma 3.1, specifically on the last sentence, “the product of these events …”; that statement did not follow obviously to me.\n\nSection 3.1\n- Remove the period and colon near kappas at the end of paragraph 3. It’s visually confusing with the dot index notation right next to them.\n\nSection 4\n- Typo: “We selects a row vector …” -> “We select a row vector …”\n\nSection 5\n- I don’t understand what Figure 1 is trying to demonstrate. It doesn’t do anything (as far as I can tell) to defend the authors’ claim that COO provides a higher expected increase in cosine similarity than IND.\n\nSection 6\n- All figures in this section should have axis labels. The captions don’t sufficiently explain what they are.\n\nSection 6.2\n- How is kappa computed for the recommendations datasets? This isn’t obvious at all.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "limited novelty and experimental results",
            "review": "This paper discussed a non-uniform sampling strategy to construct minibatches in SGD for the task of learning embeddings for object associations. An example throughout the paper is learning embeddings for a set F of focus entities and set C of context entities. In general, for focus update, the algorithm draws for each minibatch certain amount of positive samples (i,j), i \\in F and j \\in C. Then for each positive pair, we select certain amount of negative samples (i,j’) for j’ \\in some uniformly randomly selected subset C’. The same algorithm is implemented for context update, and the training alternates between the two. The authors choose similar positive object in one minibatch since it’s more efficient. Therefore, LSH hashing is used to point similar items to similar keys. Two similarity measures are used here, Jaccard similarity and cosine similarity. Some experiments are demonstrated on synthetic data and two real datasets to show the effectiveness of their method.\n\nConcerns:\n1.\tEvery piece of the method has been well studied, and the combination of them proposed in this paper does not seem very novel.\n2.\tAlgorithm 4, which is the hashing for Jaccard similarity, seems wrong. Only using iid exponentials cannot make collision probability equal Jaccard similarity.\n3.\tLittle experiments on real datasets. No comparison with other non-uniform minibatch construction methods (there should be some).\n4.\tNo quantitative analysis. \n5.\tStructure of the paper could be improved. For example, it’s better to put section 4 and 6 together.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Lacks comprehensive results on real-world data sets, writing does not seem revised",
            "review": "This paper proposes using structured mini-batches to speed up learning of embeddings. However, it lacks sufficient context with previous work and bench-marking to prove the speed-up. Furthermore, it is difficult to read due to its lack of revision. Sentences are wordy and do not always have sufficient detail. \n\nArgument issues and questions:\n- Since the main claim is a speed-up in training, the authors should support with robust experimentation. Only a synthetic and small test are conducted. \n- Not being an expert in this subject, it was difficult to follow some of the ideas of the paper. They were presented without clear explanation of why they supported the conclusion. For example,  I do not understand Figure 4. It seems COO and IND change places. It is not always clear how the figures support the argument. \n- What impact does the size of the micro-batch have on the speed-up? \n- How does this approach compare to other embedding approaches in terms of speed? There are no benchmarks other than IND. \n\nFormatting issues:\n- On page one, the sentence \"We make a novel case here for the antithesis of coordinated arrangements,\nwhere corresponding associations are much more likely to be included in the same minibatch\" seems contradictory. It reads that you are arguing for \"the antithesis of coordinated arrangements, namely independent arrangements\" when you mean \"the antithesis of independent arrangements, namely coordinated arrangements.\"\n- The figures in this paper are all very small with minuscule text and legends. Only after zooming in 200% were they legible. Figure 3, 4, 5, 6, and 7 have no axis labels. It is sometimes clear from the caption what the axes are, but it is hard to follow. \n- Often references are cited in the text without being set off with parentheses or grammatical support. For example at the top of page three: \"One-sided updates were introduced with alternating minimization Csiszar & Tusnády (1984) and for our purposes they facilitate coordinated arrangements and allow more precise matching of corresponding sets of negative examples to positive ones.\" This interrupts the sentence making it hard to read. \n\n\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Experimentally weak with the results not justifying the increased computation. No comparison to other methods doing non-uniform sampling and mini-batch selection. ",
            "review": "###### Post-Revision ########################\nThank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. \n\nUnfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. \n###### Post-Revision ########################\n\nThis paper considers arranging the examples into mini-batches so as to accelerate the training of metric embeddings. The\n- The paper doesn't have sufficient experimental evidence to convince me that the proposed method is useful. There is no comparison against baselines. The paper is not clearly written or well organized. Detailed comments below:\n- For example, when introducing focus and context entities, it would be helpful to give examples of this to make it clearer. \n- In section 3, please clarify that after drawing both positive and negative examples, what is the size of the minibatch for which the gradient is calculated? \n- How do you choose the size of the microbatches? If the microbatch size is too small, then the effect of associating examples is small. \n- In the line, \"Instead, we use LSH modules that are available at the start of training and are only a coarse proxy of the target similarity\" Why are you not iteratively refining the LSH modules as the training progresses? Won't this lead to an improvement in the performance? \n- In the line \"The coarse embedding can come from a weaker (and cheaper to train) model or from a partially-trained model. In our experiments we use a lower dimension SGNS model.\" Could you please clarify what is the additioanal computational complexity of the method? This involves additional computational cost? It doesn't seem to me that the results justify this increased computation. Please justify this. \n- In Lemma 3.2, the term s_i is undefined\n- \"In early training, basic coordinated microbatches with few or no LSH refinements may be most effective. As training progresses we may need to apply more LSH refinements. Eventually, we may hit a regime where IND arrangements dominate.\" This explanation is vague and has no theoretical or empirical evidence supporting it. Please clarify this. \n- Please fix the size of the axes and the legend in all the figures. \n- For figure 1, how is the step-size chosen? What is the dimensionality of the examples?\n- From figure 3, it is not clear that the proposed methods lead to significant gains over the independently sampling the examples? Are there any savings in the wall clock time for the proposed methods? Why is there no comparison against other methods that have proposed non-uniform sampling of examples for SGD (like Zhang, 2017)? Are the hyper-parameters chosen in a principled way for these experiments? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}