{
    "Decision": {
        "metareview": "This paper describes the development of a large-scale continuous visual speech recognition (lipreading) system, including an audiovisual processing pipeline that is used to extract stabilized videos of lips and corresponding phone sequences from YouTube videos, a deep network architecture trained with CTC loss that maps video sequences to sequences of distributions over phones, and an FST-based decoder that produces word sequences from the phone score sequences. A performance evaluation shows that the proposed system outperforms other models described in the literature, as well as professional lipreaders. A number of ablation experiments compare the performance of the proposed architecture to the previously proposed LipNet and \"Watch, Attend, and Spell\" architectures, explore the performance differences caused by using phone- or character-based CTC models, and some variations on the proposed architecture. This paper was extremely controversial and received a robust discussion between the authors and reviewers, with the primary point of contention being the suitability of the paper for ICLR. All reviewers agree that the quality of the work in the paper is excellent and that the reported results are impressive, but there was strong disagreement on whether or not this was sufficient for an ICLR paper. One reviewer thought so, while the other two reviewers argued that this is insufficient, and that to appear in ICLR the paper either (1) should have focused more on the preparation of the dataset, included public release of the data so other researchers could build on the work, and put forth the V2P model as a (very) strong baseline for the task; or (2) done a more in-depth exploration of the representation learning aspects of the work by comparing phoneme and viseme units and providing more (admittedly costly) ablation experiments to shed more light on what aspects of the V2P architecture lead to the reported improvements in performance. The AC finds the arguments of the two negative reviewers to be persuasive. It is quite clear at this point that many supervised classification tasks (even structured classification tasks like lipreading) can be effectively tackled by a combination of a sufficiently flexible learning architecture and collection of a massive, annotated dataset, and the modeling techniques used in this paper are not new, per se, even if their application to lipreading is. Moreover, if the dataset is not publicly available, it is impossible for anyone else to build on this work. The paper, as it currently stands, would be appropriate in a more applications-oriented venue.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Excellent engineering work, but it's hard to see how others can build on it"
    },
    "Reviews": [
        {
            "title": "nice data collection but no technical contribution",
            "review": "The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos.\n\nThe review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task.\n\nThe collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.\n\nThe numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings.\n\nThe result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq.\n\nIt is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data.\n\nHere are some minor details:\n\np.6.\n\nnote that there must be a blank between the 'e' characters to avoid collapsing ...\n--> this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order.\n\nTo explain why modeling characters with CTC is problematic, ...\n--> this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Engineering Marvel",
            "review": "The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR.\n\nThe authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions:\n\n- the authors argue for \"phonemes and ctc\", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary.\n- why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own \"homonym\" argument applies, too, and \"mop\" (or \"mom\") and \"pop\" should be mapped to the same \"viseme\" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier.\n- how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system?\n- LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference?\n- is the data going to be available?\n- what is a \"production-level speech decoder\"? how come your model \"is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques\" if Google does essentially the same (\"in production\")?\n- in Section 1, you say that \"by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts\". in Section 5, you demonstrate the \"generalization power of our V2P approach\"and find that it \"is able to generalize well\" - please clarify\n- \"speech impaired patients\" often have non-canonical articulation, the proposed system may not work well for them\n- it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. \n- finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review of the paper",
            "review": "This is a good paper. First of all, it presents a large-scale corpus for visual speech recognition. Second, it demonstrates a visual speech recognition system based on open-vocabulary that gives the state-of-the-art recognition accuracy.  The paper is very well written and all the technical details are clearly laid out.  I, for one, would like to thank the authors for this meticulous work to the community.   This is by far the largest dataset and the most impressive performance for VSR I have even seen in the ASR/VSR community.  I enjoyed reading this paper.  \n\nI extend this review based on the replies.  One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise.  First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community.  (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies.  I do hope the dataset will be made public.  This is a major reason I gave a high score.)  Second,  the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there,  the impressive performance itself is an impact to the field.  This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}