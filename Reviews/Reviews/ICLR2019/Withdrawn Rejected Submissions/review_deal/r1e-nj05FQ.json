{
    "Decision": "",
    "Reviews": [
        {
            "title": "A decent contribution but requires more focus.",
            "review": "The paper considers cooperative multiagent learning tasks with identical individual agents. To solve the dilemma between individual and social optimality, an additive internal reward function is evolved on a slower timescale than policy learning.  This slower task was carried out within the so-called PBT multiagent training environment using genetic algorithms. \n\nThe paper relies heaviiy on evolutionary-related motivation and analogy, and hints at lessons that can be learned regarding the evolution of cooperation in wider contexts. As an RL  person, I must admit that I found this perspective too central and somewhat distractive of the essential algorithmic idea. As mentioned, the latter is to add an internal reward function, learned using the social reward as a fitness signal. \n\nThe proposed scheme makes a number of specific choices of architecture and algorithms along the way, which make it hard to give full credibility to conclusions found regarding the effectiveness of different components. \n\nSome details of the implementation are not clear, such as the reference to stop-gradient after equation (3). Later on that page an LSTM network is mentioned, for which I did not find details. The appendix could give a more detailed description of the implementation.\n\nOverall I found the presentation somewhat confusing, and not focused enough from the algorithmic learning perspective.\n    \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of Paper \"Evolving intrinsic motivations for altruistic behavior\"",
            "review": "The paper describes an approach to develop a learning mechanism that shows the emergence of altruistic behaviour in public goods games across longer time scales. The essential idea is to combine principles of evolution that favour long-term behaviour with learning that focuses on the short term. \n\nTo achieve this, individual social preferences are modelled as intrinsic motivations and combined with deep reinforcement learning to develop individualised reward structures. This approach is then combined in a training setup that evolves varying policies, from which agents are sampled based on scenario performance and subjected to further selection. Finally, individuals are then matched based on assortative matching, with the baseline configuration being randomised matching. \n\nThe paper is well developed and systematically introduces the approach, along with an experimental evaluation and discussion. Moreover, the paper contributes to a challenging problem - the explanation of cooperative behaviour (more below). \n\nThe quality of the writing is good, apart from some odd expressions on Page 7. The figures are of good quality and sufficiently legible. However, the time series would benefit from better contrast in order to unambiguously identify those when reading the printed paper (black & white).\n\nSome questions/comments:\n\nOne specific question relates to the calculation of cooperativeness in the Harvest scenario. In the current operationalisation the difference between the agent's return and the mean return value is the cooperativeness ranking, confusing poor performance and cooperative behaviour. Could you think of a better metric to disambiguate this interpretation?\n\nGiven the comprehensive evaluation of retrospective and prospective metrics, can you provide generalisable insights as to when you deem the use of prospective reward networks as useful, since they (mostly) perform worse than retrospective ones?\n\nOverall, the paper tackles one of the most challenging problems of all - the explanation of cooperative behaviour. The combination of short-term learning and long-term evolutionary factors is a valuable exploration, which is we illustrated in this work. One more general question remains: can you really explain the basis of cooperative behaviour? The closest you get is the final discussion of the emerging weights, and extracting one of the insights that suggests reward if others are rewarded. While it is intuitively as well as scientifically accessible (e.g., as affective empathy), how do you substantiate that claim simply based on the weights?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The purpose and conclusion are unclear.  The validity of intrinsic reward is questionable.",
            "review": "The purpose and the conclusion of the paper are unclear.  Introduction emphasizes how cooperation emerges in nature.  The last sentence of the second paragraph states \"the goal\", but is this the goal of this paper or some others?  Does the paper tries to develop a model of multi-agent reinforcement learning (MARL) to provide theoretical or empirical underpinnings to how cooperation emerges in nature?  Is the paper simply motivated by nature and trying to develop a method of MARL that performs well?\n\nThe authors state that they apply evolution rater than hand-crafting intrinsic motivation, but it appears that intrinsic motivation is hand-crafted in the paper.  Specifically, in Section 2.2, intrinsic reward is defined through hand-crafted features of the other agents.\n\nFurthermore, those hand-crafted features are based on recently received reward or expected future reward of those agents.  The intrinsic reward for an agent is essentially the reward of the other agents.  Then it is no surprising that the cooperation emerges, because each agent seeks to maximize the total reward of all agents.\n\nIf the agents have access to the reward of each other, and the purpose of the paper is to maximize the overall performance, why not just let each agent simply maximize the total reward?  If the purpose of the paper is to provide some underpinnings to the emergence of cooperation in nature, how this definition of intrinsic reward provide any underpinnings?\n\nIn addition, technical contributions of the paper are rather limited.  What are proposed include (a) two time scales for learning and evolution, (b) assortative matchmaking that let players of a similar kind play each other, and (c) shared reward network.  The novelty of each is limited.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}