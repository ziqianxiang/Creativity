{
    "Decision": {
        "metareview": "Granger Causality is a beautiful operational definition of causality, that reduces causal modeling to the past-to-future predictive strength. The combination of classical granger causality with deep learning is very well motivated as a research problem. As such the continuation of the effort in this paper is strongly encouraged. However, the review process did uncover possible flaws in some of the main, original results of this paper. The reviewers also expressed concerns that the experiments were unconvincing due to very small data sizes. The paper will benefit from a revision and resubmission to another venue, and is not ready for acceptance at ICLR-2019.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "unconvincing experiments; original theorem statement incorrect"
    },
    "Reviews": [
        {
            "title": "An interesting approach ; some concerns regarding assumptions and experiments",
            "review": "The paper proposes an approach to learn nonlinear causal relationship from time series data that is based on empirical risk minimization regularized by mutual information.  The mutual information at the minimizer of the objective function  is used as causal measure.\nThe paper is well written and the proposed method well motivate and intuitive. \n\nHowever I am concerned by the assumption that the lagged variables X_{t-1}^{(j)} follow a diagonal gaussian distribution. This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.\n\nAnother key concern concerns scalability.  The authors mention gene regulatory networks , neuroscience etc as key applications. Yet the experiments considered in the paper are limited to very few time series. For instance the simulation experiments use  N=30,  which is much smaller than the number of time series usually involved say in gene regulatory network data.  The real data experiments use N= 6 or N=2. This is way to small. \n\nThe real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.  How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A easy-to-follow paper on nonlinear Granger causality which requires some further clarification",
            "review": "This paper aims to estimate time-delayed, nonlinear causal influences from time series under the causal sufficiency assumption. It is easy to follow and contains a lot of empirical results. Thanks for the results, but I have several questions.\n\nFirst, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0. In order to correctly estimate causal relations from data, both cases must be considered.\n\nSecond, the conclusion of Theorem 2 seems to be flawed. Let me try to make it clear with the following example. Suppose x^1_{t-2} directly causes x^2_{t-1} and that x^2_{t-1} directly causes x^3_{t}, without a direct influence from x^1_{t-2}  to x^3_{t}. Then when minimizing (2), we have the following results step by step:\n1) The noise standard deviation in x^2_{t-1}, denoted by \\eta_2, may be non-zero. This is because we minimize a tradeoff of the prediction error (the first term in (2)) and a function of the reciprocal of the noise standard deviation \\eta_2 (the second term in (2)), not only the prediction error.\n2) If \\eta_2 is non-zero, then x^1_{t-2} will be useful for the purpose of predicting x^3_{t}. (Note that if \\eta_2 is zero, then x^1_{t-2} is not useful for predicting x^3_{t).) From the d-separation perspective, this is because x^1_{t-2} and x^3_{t} are not d-separated by x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, although they are d-separated by x^2_{t-1}. Then the causal Markov condition tells use that x^1_{t-2} and x^3_{t} are not independent conditional on x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, which means that x^1_{t-2} is useful for predicting x^3_{t}.\n3) Given that x^1_{t-2} is useful for predicting x^3_{t}, when (2) is minimized, \\eta_1 will not go to infinity, resulting in a non-zero W_{13), which *mistakenly* tells us that X^{1}_{t-1} directly structurally causes x^{(3)}_t.\n\nThis illustrates that the conclusion of Theorem 2 may be wrong.  I believe this is because the proof of Theorem 2 is flawed in lines 5-6 on Page 16. It does not seem sensible to drop X^{j}_{t-1} + \\eta_X \\cdot \\epsilon_X and attain a smaller value of the cost function at the same time. Please carefully check it, especially the argument given in lines 10-13.\n\nThird, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993). Such methods are directly applicable to time-delayed causal relations by further considering the constraint that effects temporally follow the causes. \n\nFourth, please make it clear that the proposed method aims to estimate \"causality-in-mean\" because of the formulation in terms of regression. For instance, if x^j_{t-1} influences only the variance of x^i_{t}, but not its mean, then the proposed method may not detect such a causal influence, although the constraint-based methods can.\n\nAny response would be highly appreciated.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach",
            "review": "In the manuscript entitled \"Neural Causal Discovery with Learnable Input Noise\" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).  The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (\"learnable noise risk\") with a flexible functional approximation (neural network).  Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.  The simulation and real data experiments are interesting and seem well applied.\n\nA concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.  In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.  Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.  In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.  Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).  ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}