{
    "Decision": "",
    "Reviews": [
        {
            "title": "An Interesting Comparative Study",
            "review": "This work compares different approaches to few-shots/semi-supervised learning. In particular, the work compares Siamese networks, virtual adversarial training (VAT) and generative adversarial networks (GAN). The authors use MNIST to perform various experiments that shed light on the performance of the compared approaches as the amount of labeled/unlabeled data varies as well as when there is a class distribution mismatch between labeled and unlabeled data. The results seem to suggest that with very little labeled and no additional unlabeled data Siamese networks perform the best, but when additional unlabeled data is available, VAT outperforms GAN in the regime with low amount of labeled. GAN overtakes VAT with moderate amount of labeled data. GANs also seem to suffer less with an increasing amount of class mismatch between labeled and unlabeled data.\n\nSignificance:\nI think that the paper provides a warranted empirical study about recent approaches to learning with little or weakly labeled data, which is of general relevance in the field\n\nPros:\nThe paper is well-written and easy to follow\nNice overview of recent approaches to few-shots/semi-supervised learning.\nDetailed empirical analysis covering a range of training data regimes\n\nCons:\nOnly one benchmark is used\nResults may also vary for different network architectures or application domains\n\nI in general found the paper interesting; however as mentioned, I find it hard to gauge the scope of the results presented here given that they are only provided for one benchmark and one network architecture.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper compares three methods with little data empirically. ",
            "review": "This paper compares three methods with little data empirically. \n\nThough it may be interesting to study the affects of variants of SSL methods, the paper brings much limited contributions. First, the paper does not bring any technique breakthrough. Second, the empirical study is not systemical and solid. Third, the founding of empirical studies are not convincing, since the data sets selected may be biased. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper compares a few approaches for training a convolutional neural network on a dataset with relatively few labeled examples. Specifically, they compare siamese networks (traditionally used for one-shot learning) and two semi-supervised learning techniques: VAT and a GAN. They experiment with varying amounts of labeled and unlabeled data and class mismatch.\n\nOverall I think the idea of comparing approaches for dealing with little data is important, since many problems of interest have limited available labeled data. However, this paper really falls short. First, the experiments are only conducted on MNIST, which is an incredibly easy dataset. The accuracy is already good on MNIST in the very-limited-data regime, and in general many insights on MNIST completely fail to hold up on more difficult datasets. Second, the scope of the paper is pretty limited - it only basically considers two approaches (one-shot learning and SSL) and one-shot learning is only considered for one of the experiments. It only compares a couple of techniques, and only in a couple of settings. It only briefly mentions transfer learning. A more comprehensive treatment of the question of \"what should I do when I have limited data\" should consider many more datasets and many more approaches with many more factors of variation. Finally, various experimental details are not particularly clear. Some specific comments are below. As-is, the paper yields very few novel and useful insights, and simply does not warrant publication. \n\nSpecific comments:\n\n- \"Using generative models it is possible to create additional labeled or unlabeled samples and leverage information from these samples.\" This is not quite true in general. Kingma et al. (2014) actually use a generative model to learn a feature representation, not to generate new data. It has also been argued in \"Good Semi-supervised Learning that Requires a Bad GAN\" that GAN-based SSL works best when the GAN generates poor samples, which is more nuanced than just \"the GAN is generating more labeled data.\"\n\n- \"[Consistency regularization] relies on leveraging information from non realistic samples.\" Not quite. It perturbs the input or the network itself and enforces that the output should not change. These perturbations may not be realistic (e.g. look at the perturbations used in practice in Miyato et al. 2018), and they may not occur on the samples themselves (e.g. using dropout).\n\n- \"Many problems are too specific and similar datasets are not available. Therefore the generalization of this approach is somehow limited.\" I think if you are not going to investigate transfer learning, you need to back up this statement. You could do this by trying to transfer between a bunch of different datasets with varying levels of subjective similarity and showing that for certain datasets transfer learning doesn't work. For example, transferring between MNIST and SVHN may work but CIFAR-10 and SVHN may not work (but who knows).\n\n- I am somewhat surprised the authors chose the Siamese Network approach over Matching Networks, which seem to be a more common (and quite simple) baseline for one-shot classification which has largely supplanted the Siamese Network approach.\n\n- \"An empirical investigation Oliver et al. (2018) has shown, that virtual adversarial training (VAT) Miyato et al. (2018) achieved the best results...\" I believe one of the main points of Oliver et al. was that the different SSL methods are difficult to compare in general and that statements like \"VAT achieved the best results\" will depend on the network, task, unlabeled vs. labeled mismatch, etc.\n\n- The paper is currently over 8 pages which is unnecessary. For example, equations (1), (2), and (3) could easily be made into a single line.\n\n- Many citations are listed using the author's first names (e.g. \"Ian et al. (2014).\" or \"Diederik and Jimmy (2014)\". Please fix.\n\n- \"Furthermore there is not used any unlabeled external data, except for the GAN method, where the creation of fake samples by the generator G is part of the training procedure.\" So, are you saying that you did use unlabeled data to train the GAN? Or did you just use the labeled dataset to train the GAN? If you use unlabeled data for the GAN I don't think it's a fair comparison. If you did not use the unlabeled data I don't think it makes sense to treat the GAN's generated samples as \"unlabeled data\"; it is \"generated data\" which is a distinct concept.\n\n- The experiments need to be repeated with multiple trials to give a mean and standard deviation across runs. The differences in accuracy are too small in many settings to make any subtantiative judgement.\n\n- The Y axis of Figure 2, right, is \"Number of unlabeled elements per category\". I think you mean \"Number of labeled elements per category\" (to match the left figure).\n\n- In Figure 3, are you averaging across varying numbers of unlabeled (left) and labeled (right) data? If so, I don't see why this is useful since the gain in accuracy clearly depends on both variables which is clearly shown in Figure 2. If not, I'm not understanding what the figure is showing so I might suggest some clarification.\n\n- 3.2 and 3.3 only consider semi-supervised learning algorithms, since the problems don' really fall into the framework of one-shot classification. It's a bit odd to talk about one-shot classification when you only apply the Siamese network in only one experiment - and it's not even a one-shot classification experiment, really; it's just a classification experiment with limited data.\n\n- Looking at the lines in Figure 4, it looks like there are many possible percentages of class mismatch. It's a bit hard to estimate, but it looks like maybe 10? points on each line. However, if there are up to 3 classes which can be used as mismatched classes, I think there are only 4 possible percentages of mismatch - corresponding to 0, 1, 2, and 3 mismatched classes. Why are there so many x-values plotted for these curves?\n\n- What hyperparameter values did you use? How did you do hyperparameter selection? In the limited-data regime, it's difficult to do principled model selection because you don't have a large enough validation set to estimate model performance on.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}