{
    "Decision": "",
    "Reviews": [
        {
            "title": "Needs more work and clarity",
            "review": "This paper addresses an interesting task of clarification question generation by proposing a GAN-based approach. It mainly builds on the ideas of Rao & Daum´e III (2018) to understand the usefulness of generated questions via a utility function that acts as the discriminator while a simple seq-to-seq model is used to generate questions in the generator module. The proposed GAN model is inspired by the sequence GAN model of Yu et al. (2017) with simple variations such as using MIXER (Ranzato et al., 2015) as the generator and not using a CNN-based discriminator. Experiments were conducted on two datasets, and the obtained results were mixed and not conclusive. Overall, due to the lack of novelty and unconvincing results, I feel the paper needs more work before it is ready for publication. My detailed comments are below:\n\n- \"As a running example, we will use the Amazon setting: ...\" --> The example in Figure 1 is only referred once and not even in Table 3 to show the related model predictions. I would suggest to truly consider it as a running example to clarify the training and testing procedure better. Also, an example of the StackExchange dataset would be helpful.\n\n- The utility function (Section 2.3) seems to be simple. Did you evaluate the effectiveness of this function solely in predicting the usefulness of a question? How would a binary classifier work instead? I also wonder why simple seq-to-seq models were used as question/answer generators while there exist a lot of work that already outperform these models for similar text generation tasks. \n\n- \"In our model, the answer is an latent variable: we do not actually use it anywhere except to train the discriminator. Because of this, we train our discriminator using (context, true question, generated answer) triples as positive instances and (context, generated question, generated answer) triples as the negative instances.\" --> This part is not clear. Did you use generated answers or the true answers as part of the positive instances? Please clarify across the paper when you used generated answer/question and when you used true answer/question.\n\n- \"Unlike the question generator, the parameters of the answer generator are kept fixed during the adversarial training\" --> please explain why.\n\n- I like that the experiments were carried out on multiple datasets. What are the lengths of the contexts, questions, and answers for both datasets on average number of words? What are the impacts of the length restrictions of 100, 20, and 20 you set for context, question, answer on the evaluation results? How did you come up with these numbers? I would suggest to include an analysis of impacts of variable lengths of context, question, answer on the model performance.  \n\n- It's not clear how the Lucene system was built with human generated questions. Please clarify.\n\n- Table 2 shows mixed results, what should we conclude from this? \n\n- How many crowdworkers were used for human judgements? What was the inter-annotator agreement? How did you convert the human answers into the numeric scores of Table 2? Without these information, it is not possible to judge the utility of the human evaluation. StackExchange results could have been annotated via other crowdsourcing venues e.g. upwork.   \n\n- The related work should be better compared and contrasted with the proposed work, especially the main contributions of the paper should be clearly highlighted. \n\n- Table 3 is not referred in text. I would suggest to include the name of the products also for better context. The human evaluation scores look very subjective, hence, the inter-annotator agreement is an essential factor. \n\n- There are a lot of grammatical mistakes and inconsistencies across the paper that need to be corrected.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very interesting idea with weak evaluation",
            "review": "In the paper, the authors try to improve the generation of clarification questions using reinforcement learning against a discriminator, creating a GAN-like setting. They train two sequence-to-sequence models that i) generate questions from context, and ii) answers from (context, question) pairs. They also train a discriminator model on (question, answer, context) triples.\n\nI believe the task, and the setup created by the authors is very interesting and novel, and the paper is very well written. They show that the additional training against the discriminator leads to a (very small) increase in diversity and question specificity.\n\nOn the negative side:\n  - It is not clear to me that the discriminator acts as a utility function as they claim, at least in the way they define utility. It is only trained to distinguish real questions about a context from random ones.\n  - The results presented in Table 1 and 2 show only very small differences to the other approaches. I wonder why that is, and how much the model actually changes in the reinforcement learning tuning step.\n  - The automated metrics do not seem suitable for the task, since they can only measure how close a generated example is to some gold example. \n  - The only significant improvement is on specificity, with the much more important goal of creating useful questions not achieved. I am actually not sure if increased utility (i.e. identifying missing information and asking about it) can be achieved with a setup like this.\n\nBut despite these weaknesses I still think this is a very interesting contribution.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach with bad evaluation setup",
            "review": "The paper proposes a new approach for the problem of clarification question generation. It is based on training question-answer generator model jointly with a utility function (or discriminator) using a GAN-like objective. The system is compared to a variety of baselines and shows to generate slightly more diverse answers than competing baselines, but is otherwise quite comparable. \n\nThe paper is overall well presented and introduces an interesting approach that combines SeqGANs with MIXER training and a self critical baseline. The authors also took care to establish reasonable baselines given the novelty of the task. The evaluations are carried out on a very artificial task setup, however, that is overall not very usefull for evaluating clarification questions. Therefore, I believe that this paper needs a completely different evaluation setup and I can unfortunately not recommend it for acceptance without that.\n\nDetailed comments:\nIt is unclear to me whether it is really possible to evaluate a model trained in such a setup, because it is impossible to say what is a useful clarification question without establishing an information need first. Just asking random questions about a product, which is the best we can hope to learn, is not a very interesting task. Clarification is usually a means to an end goal, but in this paper it is established as the final goal, which doesn't make too much sense to me. This leads to the rather artificial treatment of \"Utility\", which is impossible to define without a clear down-stream task. The paper relabels generating human-like questions as the utility to optimize towards in order to ultimately fool a discriminator. I don't see how this can be viewed as defining utility. So I strongly suggest to evaluate the approach on a task that might actually require asking clarification questions, in which case utility is naturally defined. GAN training could still be used to make the generated questions more diverse.\n\n\nStrengths:\n- clearly written and well presented\n- learning to generate clarification questions is an important topic\n- interesting combination of SeqGANs, MIXER and self-critical baseline for policy gradient updates\n- a range of good baselines for this novel task setup\n\n\nWeaknesses:\n- minor: automatic evaluations are kind of useless here and the datasets are rather artificial for this task\n- major: generating clarification questions cannot be the end goal in and of itself (see above explanation)\n\n\nOther comments:\n- section pretraining, paragraph question generator: I do not understand the reference to answer generator in this paragraph. I think something got mixed up in this section.\n- needs some proof reading: some spelling mistakes (eg: p3 thier->their), missing spaces (e.g., p.4 \"model§2.1\"), \n\n\nQuestions:\nWhy is specificity such an important aspect if we mainly care about usefulness? In other words, is usefulness not capturing specificity to a certain degree?\n\nThe goal of this paper is to train clarification questions, so I do not really understand why also synthetic answers are being generated? Why not just training the system on (context, question) tuples? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}