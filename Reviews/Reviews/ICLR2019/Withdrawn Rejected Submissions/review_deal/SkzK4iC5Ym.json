{
    "Decision": {
        "metareview": "The paper introduces a modification of batch normalization technique. In contrast to the original batch normalization that normalizes minibatch examples using their mean and standard deviation, this modification uses weighted average \nof mean and standard deviation from the current and all previous minibatches. The authors then provide some theoretical justification for the superiority of their variant of BatchNorm.\n\nUnfortunately, the empirical demonstration of the improved performance seems not sufficient and thus fairly unconvincing. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A sound idea but no sufficient evidence of its effectiveness"
    },
    "Reviews": [
        {
            "title": "interesting idea, empirical results are not convincing",
            "review": "The paper introduces a modification of batch normalization technique. Original\nbatch normalization normalizes minibatch examples using their mean and standard deviation. \nThe proposed version of batch normalization, called diminishing batch normalization, normalized \nexamples in the current minibatch using  mean and standard deviations that are weighted average \nof mean and standard deviation from the current and all previous minibatches. The authors prove convergence of \nbatch gradient descent with diminished batch normalization. Also, the authors show empirically\nthat Adagrad optimization with diminishing batch normalization can find a better local minimum than\nAdagrad optimization with original batch normalization.\n\nThe idea of diminishing normalization is very sound. However I was not convinced that it gives empirical advantage.\nThe paper says that Table 1 shows \"the best result obtained from each choice of \\alpha^m \". Probably the numbers in \nthis table were obtained using some particular choice of the number of epochs. Unfortunately I didn't find in the \npaper any details about the choice of the number of epochs. If we choose the number of epochs that minimize validation\nloss, then according to Figures 4(a) and 3(a), if \\alpha^m=1 then the validation loss is minimized around epoch 55\nand corresponding test error should be less than 2.2%. But the corresponding top left entry in Table 1 has error 2.7%. \n\nAdditional technical remarks:\n1. The abstract says \"we also show the sufficient and necessary conditions for the step sizes and diminishing weights to ensure the convergence\". I didn't find necessary conditions in the paper.\n\n2. The authors claim that they are not aware of any prior analysis of batch normalization. The papers at https://arxiv.org/abs/1805.11604 and \nhttps://arxiv.org/abs/1806.02375 , published initially in 5-6/2018, provide interesting theoretical insights on batch normalization.\n\n3. Sentence after equation (2): change from D_1 to D.\n\n4. Usually batch normalization is applied before non-linear activation. According to equations 3-6, the paper applies \nbatch normalization after the nonlinear activation. My understanding is that convergence proof relies on the former architecture. Does section 5 use the former or the latter architecture? \n\n5. I am not sure that fully connected neural network is an efficient architecture for MNIST dataset. I would like to \nto see experiments with CNN and MNIST. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A momentum based approach for batch normalization with asymptotic convergence analysis",
            "review": "The authors propose a momentum based approach for batch normalization and provide an asymptotic convergence analysis of the objective in terms of the first order criterion. To my understanding, the main effort in the analysis is to show that the sequences of interest are Cauchy. Some numerical results are reported to demonstrate that the proposed variant of BN slightly outperforms BN with careful adjustment of some hyper parameter. The proposed approach is incremental, and the theoretical results are somewhat weak.\n\nThe most important issue is that the zero gradient of the objective function does not imply that it attains an (even local) minimum point. As for the 2-layer case, the objective function can be nonconvex in terms of the weight parameters with stationary points being saddle points, it is crucial to understand whether an iterative algorithm (GD or SGD) converges to a minimum point rather a saddle point. Thus, the first order criterion alone is not enough for this purpose, which is why extensive studies are carried out for nonconvex optimization (e.g., using both first and second order criteria for convergence [1]) and considering the specific structure of neural nets [2].\n\nThe analysis is somewhat confusing. The authors assume that the objective of interest have stationary points (\\theta*, \\lambda*), and also show that the sequence of the norm of gradient convergence to zero, with the \\lambda^(m) converges to \\bar{\\lambda}. What is the relationship between \\lambda* and \\bar{\\lambda}? It is not clear whether they are the same point or not. Moreover, since there is no converge of the parameter, it is not clear what the convergence for the \\lambda imply here, as we also discussed above that the zero gradient itself may mean nothing.\n\nIn addition, the writing need improvements. Some statements are not accurate. For example, on page 3, after equation (2), the authors state “The deep network …”, though they mentioned it is for a 2-layer net. Also, more explicit explanation and definitions are necessary for notations. For example, it is clearer to define explicitly the parameters with \\bar (e.g., for \\lambda) as the limit point. \n\n[1] Ge et al. Escaping from saddle points—online stochastic gradient for tensor decomposition.\n[2] Li and Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review: Diminishing Batch normalization. ",
            "review": "\nIn this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed “generalized” BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. \n\nI have the following three main comments about the paper. \n1)\tOnly deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. \n2)\tBecause the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. \n3)\tOnly one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that “the technique presented can be extended to more layers with additional notation”. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. \n4)\tThe authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \\lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. \n5)\tAssumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. \n6)\tFollow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}