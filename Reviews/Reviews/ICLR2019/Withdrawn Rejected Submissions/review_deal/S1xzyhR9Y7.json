{
    "Decision": {
        "metareview": "This paper offers a new method for sentence representation learning, fitting loosely into the multi-view learning framework, with fairly strong results. The paper is clearly borderline, with one reviewer arguing for acceptance and another arguing for rejection. While it is a tough decision, I have to argue for rejection in this case.\n\nThere was a robust discussion and the authors revised the paper, so none of the remaining technical issues strike me as fatal. My primary concern is simply that the reviewers could not reach a consensus in favor of the paper. In particular, two reviewers expressed concerns that this paper makes too small an advance in NLP to be of interest to non-NLP researchers. I think it should be possible to broaden the scope of the paper and resubmit it to another general ML venue, and (as one reviewer suggested explicitly), this paper may have a better chance at an NLP-specific venue.\n\nWhile neither of these factors was crucial in the decision, I'd encourage the authors (i) to put more effort into comparing properly with the Subramanian and Radford baselines, and (ii) to clarify the points about the human brain. For the second point: While none of the claims about the brain are false *or misleading*, as far as I know, the authors do not make a convincing case that the claims about the brain are actually relevant to the work being done here.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Small but reasonable novel contribution"
    },
    "Reviews": [
        {
            "title": "Impressive results and interesting model",
            "review": "This paper is about a multi-view framework for learning sentence representations. Two objective functions (a generative one and a discriminative one) are proposed that make use of two encoders, one of them is based on an RNN and the other on a linear projection of averaged word embeddings. Each of these objective functions has a multi-view framework where their respective objective functions are in part based on making sure their is some relationship between the two different encoders. This multi-view framework is shown to be helpful over having independent encoders in their ablation study.\n\nThe authors evaluate on the SentEval benchmark (a collection of tasks where a shallow neural network is learned and the sentence embeddings are kept fixed) and a collection of STS tasks (where the cosine between two vectors is used to estimate their semantic similarity).\n\nThe results are impressive. A closer examination of them though leaves me with some questions and thoughts.\n\nRegarding the SentEval numbers, I would like to know the dimensionality of your models in Table 5. I am somewhat unclear of how the final embeddings were produced, it seems that you concatenate mean, max, min, and last_h from the RNN encoder and then mean, max, min from the projection encoder. Is that correct? That would make your feature vector 7*1024 dimensions, which is a bit bigger than most of what you compare to (some of these methods are 4096 dims). With this type of evaluations, larger feature vectors do help performance, thought I am certain that you would have nice performance even if your dimension was reduced (this is from looking at the ablation). I think making the dimensions more explicit and clarifying in the text how the final feature vector was created would be helpful for readers.\n\nAnother thing to consider in the evaluation, is that a paper recently pointed out that max pooling in a certain way in SentEval can artificially inflate results for some of these datasets. I noticed that max-pooling is used in your experiments. This paper also shows how big of an effect larger feature vectors have on performance: https://openreview.net/forum?id=BkgPajAcY7. I'd like to know if your results are affected by this max-pooling operation as is the case for several well-known papers in this area.\n\nI also noticed that a lot of the best SentEval numbers came from using the book review dataset. This makes a lot of sense in that a lot of these are based on sentiment and is something that was in part used by (Radford et al. 2017) to obtain strong performance on these tasks. A similar thing happens with the STS data and the news domain as noted in this paper.\n\nI noticed you did the principal component removal trick for InferSent, but it did not have a large effect on performance. How big of an effect did if have with your methods? I'm glad you included it in InferSent, but I'd like to see this as well in the ablation.\n\nOverall I do think this paper has value for the community. It shows how strong results can be obtained using just raw text and using less parameters and training faster than other recent approaches. I do think a lot of the gains here are due to clever design choices in their experiment (for instance using different types of raw data which help more on certain tasks, removing the first principal component, etc.) but putting everything together to get very competitive results with across all these tasks with an interesting approach and an accompanying analysis is a nice contribution.\n\nMinor comment: The paper was tough to understand in parts due to symbols/abbreviations not being defined or motivated clearly. It'd be nice if the authors could define the symbols/abbreviations that are in the tables in the captions. An example of this would be WR in Table 4. The left-most column in Table 6 could also be clearer (I know it is in the text, but I was confused about what f1 and f2 represent etc. in my first pass). This also occurs in the text as well like when g is introduced in Section 2.2.\n\nPROS:\n- Interesting and novel model combining RNNs and word-averaging\n- I find the multiview framework to be a nice contribution, having the models tied in this way also improves performance.\n- Model is fast to train and requires only raw text\n- Competitive results with SOA on many datasets - both those requiring a trained classifier using the fixed embedding and STS tasks.\n\nCONS\n- Some of their gains are due to choice of dataset for training or removing the first principal component - advantages that other comparable models may or may not have. Not really a con though, more of an observation. I would like to see an ablation to see the effect of removing the first principal component.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "contributions for ICLR community are unclear; after revisions and discussion, I am more positive",
            "review": "After reading through the authors' comments and rereading parts of the submission, I have become a little more positive about this paper. \n\nI am still unsure about the contributions to the ICLR community. The authors merely state \"We believe our contributions to ICLR community are clear and valuable\" without backing up this claim with an argument. \n\nBut in the rest of the authors' comment, they make some good points. I think those points should be made more prominently in the paper itself. I would suggest that the authors describe their approach as using different, complementary encoders of the input sentence and consensus maximization. If they wish to describe this as multi-view learning, that's fine, but I think using the term \"consensus maximization\" (or something more descriptive like that) in prominent places would be helpful. \n\nIf the approach is applicable beyond sentence embedding learning, then it would behoove the authors to describe the approach in a general way so that readers will see how they can apply it to their own tasks. As currently written, the paper is very much focused on sentence embedding learning, which causes me to think that the paper is more appropriate for an NLP venue. But it is true that ICLR publishes papers that are application-specific, so I can't consider this to be a deal-breaker for the paper.\n\nI raised my score to a 6.\n\n--------------------------------- original review follows: ----------------------------------\n\nThis paper describes experiments in learning sentence embeddings from unlabeled text. The paper compares a few different compositional architectures and training objectives. The story of the paper focuses on the training of multiple architectures jointly for a single sentence, then ensembling those architectures at test time to represent sentences. One architecture is an RNN and the other is a word averaging model, and the idea is that these two architectures capture different \"views\" of the sentence. \n\nPros:\n\nAs a general-purpose method to get sentence embeddings without using any resources other than unannotated text documents, this approach has strong results, including solid performance on the SentEval tasks and relatively-low training times. \n\nIt was also nice to see how the results depend on the domain of the training data. Review data definitely helps on the several sentiment-related tasks, which provides further evidence of a worrisome aspect of SentEval. \n\nCons:\n\nOverall, the paper feels incremental and is likely a better fit for an NLP conference. What are the generalizable contributions to the ICLR community? Given the known differences between RNNs and word averaging models for sentences (especially on the SentEval tasks, which, as the authors note, was discussed by Hill et al.), it's entirely unsurprising that combining the two would be a good idea. But even if this were not the case, the ubiquity of ensembles outperforming single models in deep learning also makes it unsurprising that combining these two kinds of model architectures would be beneficial. So I'm just not sure if there is a significant contribution beyond the NLP results. These results seem solid (though a bit incremental), but if the primary contribution is empirical, then the paper would be a better fit for an NLP venue. \n\nIn addition, I'm not sure if \"multi-view\" is an appropriate description of the approach. In Sec. 1, we find the sentence \"Compared to earlier work on multi-view learning (de Sa, 1993; Dhillon et al., 2011) that takes data from various sources or splits data into disjoint populations, our framework processes the exact same input data in two distinctive ways.\"  Therefore, maybe it's not quite accurate to describe this approach using the term \"multi-view learning\"? I think it would make more sense to use a different term rather than stretch the definition for a well-known one. \n\nI kept expecting the paper to present results when combining the generative and discriminative objectives, but as far as I can tell, this was never done. What would happen if one were to use multitask learning and just optimize the sum of the two losses? \n\nI'd suggest citing and comparing to the results from \"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\" by Subramanian et al. (ICLR 2018) and the Byte mLSTM from \"Learning to generate reviews and discovering sentiment\" by Radford et al. \n\nI'm not sure how excited we should get about not using any annotations or structured resources for learning sentence embeddings. The authors do not motivate this goal. \n\n\nBelow are more specific comments/questions:\n\nSec. 2.2 contains the sentence \"Ideally, the inverse of h should be easy to compute so that during testing we can set g = h^-1.\" At this point in the paper, it is not clear what g is going to be applied to at test time, since presumably the following sentence is not going to be available at test time, right? I think it would be good to discuss how the model is going to be used at test time before discussing the inverse of h. \n\nSec. 2.3: \nIn Eq. (3), why does the denominator sum always start at 1 no matter what i and j are? That is, why would the denominator always sum over the first N sentences in the dataset?\n\nI think the pooling methods in Table 2 should be described in Section 2. \n\nIn Table 2, it is not clear what h_i^{M_i} is. If M_i is the number of words in sentence i, that should be mentioned somewhere. \n\nSec. 3.1:\n\"For a given sentence input s with M words, suggested by (Pennington et al., 2014; Levy et al., 2015), the representation is calculated as z = (\\hat{z}_f + \\hat{z}_g) / 2, where \\hat{z} refers to the post-processed and normalised vector, and is mentioned in Table 2.\"  I don't understand. Where is this mentioned in Table 2?\n\n\nMinor issues follow:\n\nSec. 1:\n\"Distributional hypothesis\" --> \"the distributional hypothesis\"\n\"in machine learning community\" --> \"in the machine learning community\"\n\"and distributional hypothesis\" --> \"and the distributional hypothesis\"\n\"the linear/log-linear models\" --> \"linear/log-linear models\"\n\"based on distributional hypothesis\" --> \"based on the distributional hypothesis\"\n\"contraint\" --> \"constraint\"\n\nSec. 2:\n\"marry RNN-based sentence encoder\" --> \"marry RNN-based sentence encoders\" or \"marry the RNN-based sentence encoder\"\n\nSec. 2.1:\n\"only hidden state\" --> \"only the hidden state\"\n\nSec. 2.2:\n\"prior work with generative objective\" --> \"prior work with generative objectives\"\n\nSec. 2.3:\n\"with discriminative objective learns\" --> \"with the discriminative objective learns\"\n\nSec. 3.1:\nIn Table 3, I don't see where superscript 5 is shown in the table.\n\nSec. 3.2:\nIn Table 5, the numbered superscripts at the top of the table do not show up next to the methods in the actual table rows. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper presents a multiview framework for sentence representation in NLP tasks. But novelty appears too limited here.",
            "review": "This paper presents a multiview framework for sentence representation in NLP tasks. Authors propose two architectures, one using a generative objective, while the other uses a discriminative objective. Both combine a recurrent based encoding function and a linear model. Large experiments have been conducted on several NLP tasks and datasets, showing improvement of the introduced frameworks compared to baselines.\n\nThe paper is globally well written and has a clear presentation. But I'm not sure to understand why authors motivate their work on the asymmetric information processing in the two hemispheres of the human brain. It sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation..\n\nMy main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation). This is far from significant works on multiview learning, see: \"Multi-view learning overview: Recent progress and new challenges\".\n\nTable 3, where G and D refer respectively to Generative and Discriminative models. But what differences between G1, G2, G3 ; D1, D2, D3 ?\n\nInvertible constraint is a nice idea for using inverse of the decoder as the encoder. Is it really to take advantages of decoder information on the encoder/representation part? Or also to reduce the amount of parameters learnt in the model? Moreover, it is unclear on the ablation study: did you consider the original encoder ; or still the inverse of decoder but without the constraint? Unfortunately, it seems to not give significant gain, according to ablation study in table 6.\n\nIn this current form, I feel this paper does not give sufficient novelty to be accepted at ICLR.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}