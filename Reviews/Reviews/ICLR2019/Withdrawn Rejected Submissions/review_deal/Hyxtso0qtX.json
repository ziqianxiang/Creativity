{
    "Decision": {
        "metareview": "This paper proposes a method for incentivizing exploration in self-supervised learning using an inverse model, and then uses the learned inverse model for imitating an expert demonstration. The approach of incentivizing the agent to visit transitions where a learned model performs poorly. This relates to prior work (e.g. [1]), but using an inverse model instead of a forward model. The results are promising on challenging problem domains, and the method is simple. The authors have addressed several of the reviewer concerns throughout the discussion period.\nHowever, three primary concerns remain:\n(A) First and foremost: There has been confusion about the problem setting and the comparisons. I think these confusions have stemmed from the writing in the paper not being sufficiently clear. First, it should be made clear in the plots that the \"Demos\" comparison is akin to an oracle. Second, the difference between self-supervised imitation learning (IL) and traditional IL needs to be spelled out more clearly in the paper. Given that self-supervised imitation learning is not a previously established term, the problem statement needs to be clearly and formally described (and without relying heavily on prior papers). Further, the term self-supervised imitation learning does not seem to be an appropriate term, since imitation learning from an expert is, by definition, not self-supervised, as it involves supervisory information from an expert. Changing this term and clearly defining the problem would likely lead to less confusion about the method and the relevant comparisons.\n(B) The \"Demos\" comparison is meant as an upper bound on the performance of this particular approach. However, it is also important to understand what the upper bound is on these problems in general, irrespective of whether or not an inverse model is used. Training a policy with behavior cloning on demonstrations with many (s,a) pairs would be able to better provide such a comparison.\n(C) Inverse models inherently model the part of the environment that is directly controllable (e.g. the robot arm), and often do not effectively model other aspects of the environment that are only indirectly controllable (e.g. the objects). If the method overcomes this issue, then that should be discussed in the paper. Otherwise, the limitation should be outlined and discussed in more detail, including text that outlines which forms of problems and environments this approach is expected to be able to handle, and which of those it cannot handle.\n\nGenerally, this paper is quite borderline, as indicated by the reviewer's scores. After going through the reviews and parts of the paper in detail, I am inclined to recommend reject as I think the above concerns do not outweigh the pros.\n\nOne more minor comment is that the paper should consider mentioning the related work by Torabi et al. [2], which considers a similar approach in a slightly different problem setting.\n\n[1] Stadie et al. https://arxiv.org/abs/1507.00814\n[2] Torabi et al. IJCAI '18 (https://arxiv.org/abs/1805.01954)",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta review"
    },
    "Reviews": [
        {
            "title": "good training exploration, somewhat limited scope of experimental conditions",
            "review": "This paper presents a system for self-supervised imitation learning using a RL agent that is rewarded for finding actions that the system does not yet predict well given the current state.  More precisely, an imitation learner I is trained to predict an action A given a desired observation state transition xt->xt+1; the training samples for I are generated using a RL policy that yields an action A to train given xt (a physics engine evaluates xt+1 from xt and A).  The RL policy is rewarded using the loss incurred by I's prediction of A, so that moderately high loss values produce highest reward.  In this way, the RL agent learns to produce effective training samples that are not too easy or hard for the learner.  The method is evaluated on five block manipulation tasks, comparing to training samples generated by other recent self-supervised methods, as well as those found using a pretrained expert model for each task.\n\nOverall, this method exploration seems quite effective on the tasks evaluated.  I'd be curious to know more about the limits and failures of the method, e.g. in other types of environments.\n\nAdditional questions:\n\n- p.2 mentions that the environments \"are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions\".  What sort of environments would be less well fit?  Are there any failure cases of this method where other baselines perform better?\n\n- sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they?  if not, what differences caused this?\n\n- figs. 4, 5 and 6 all relate to the stabilizer value delta, and I have a couple questions here:  (i) for what delta does performance start to degrade?  At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples).  (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of \"no stabilizer\" is around 3 as well, yet \"no stabilizer\" performs poorly in Fig 5.  Why is this, if it tends to produce actions with loss around 3 in both cases?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting and potentially useful paper",
            "review": "The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. The core of the method is to train an inverse local model (a model that predicts the action that was taken from a pair of states) and its errors as an exploration bonus for a policy gradient agent. The intuition is that its a good self-regulating strategy similar to curiosity that leads the agents towards states that are less known by the inverse model. Seeing these states improves the . There are experiments run on the OpenAI gym comparing to other models of curiosity. The paper is well written and clear for the most part.\n\npros:\n- the paper seems novel and results are promising\n- easy to implement\ncons:\n- seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body.\n- only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. Nonetheless I can imagine it helping with initial random motions. \n- from (6) the bonus seems to be unbounded and (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Cool idea, but there are issues in evaluation and results are weak overall",
            "review": "The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert’s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. \n\nOverall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues:\n- the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.\n- Section 4.2.: “As opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.” - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. \n- I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce “not too hard” training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \\delta > 0.\n- Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy’s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?\n\nI do have concerns regarding the experimental evaluation:\n- the “Demos” baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform “Demos”, which it does pretty often.\n- it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?\n- the results of “HandReach” experiments, where the proposed method works much worse than “Demos” are not discussed in the text at all\n- overall, there is no example of the proposed method making a difference between a “working” and “non-working” system, compared to “Curiosity” and “Random”. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. \"Demos\" baseline doesn't perform much better, but what would happen with 10000 demonstrations?\n- there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like \"Demos\" does?\n\nI think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}