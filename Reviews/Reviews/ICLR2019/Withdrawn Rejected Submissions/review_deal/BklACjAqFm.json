{
    "Decision": {
        "metareview": "Pros:\n- interesting algorithmic idea for using successor features to propagate uncertainty for use in epxloration\n- clarity\n\nCons:\n- moderate novelty\n- initially only simplistic experiments (later complemented with Atari results)\n- initially missing baseline comparisons\n- no regret-based analysis\n- questionable soundness because uncertainty is not guaranteed to go down\n\nAll the reviewers found the initial submission to be insufficient for acceptance, and the one reviewer who read the rebuttal/revision did not change their mind, despite the addition of some large-scale results (Atari).",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Experimental results are too preliminary to assess the contribution",
            "review": "Summary/contribution:\nThis paper focuses on the problem of incorporating uncertainty into RL. The primary contribution is exploring the use of successor features for uncertainty prediction over Q-values. The proposed approach builds on O’Donoghue et al. (2018). The authors provide experiments that demonstrate improved performance on a chain MDP environment and a Tree environment. \n\nPros:\n- I found this paper to be above average in terms of clarity.\n\nCons:\n- The experiments evaluation is restricted to simplistic environments. The authors make an argument for why using successor features would be more \"stable\", but I found the experimental evidence to support this claim to be underwhelming. \n\nJustification for rating:\nThis paper does a good job of articulating an interesting approach to the exploration problem using successor representations.  In the current form however, it is really lacking in experimental evidence to support the main claims/contributions. Currently the domains considered are somewhat toy which I do not find convincing enough to demonstrate the effectiveness of their approach. \n\nOther:\n- I would appreciate a discussion on the relationship to Machado et al. 2018 which explored count based exploration with successor representations. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Exploration using successor features representations",
            "review": "The paper proposes an exploration approach (either based on posterior sampling or optimism) based on successor features representation. A high probability ellipsoid confidence set (defined by the Gram matrix \\Sigma_t) is estimated based on linear regression (using some features \\phi) to the immediate reward function. Now for any policy \\pi, a confidence interval for the Q-value of any policy \\pi can be derived by application of the \\psi transformation, where \\psi = expected sum of discounted future \\phi under \\pi. The algorithm selects action by posterior sampling (or UCB) in the \\psi-space. \n\nIt would be interesting to see if this approach would converge to a good policy, maybe by doing a regret-based analysis. \nUnfortunately there is no such analysis in the paper. However my main complaint is the soundness of the approach, for two reasons:\n- First it is not clear that the uncertainty in the Q-values decreases with time. Indeed the uncertainty on Q^{\\pi}(s,a) corresponds to the width of the confidence ellipsoid in the direction of the successor features \\psi^{\\pi}(s,a). However, although we know that the uncertainty shrinks in the directions of the features \\phi_t (when action a_t is chosen in state s_t) because we do regression of the reward function, we do not have the same property for \\psi_t, which defines the Q-function. And it is not obvious that the confidence set in the direction of \\psi_t would shrink at all. Thus it could be the case that the uncertainty on the Q-values will never decrease. \n- Second, since the successor features are learnt on-policy, the uncertainty on the Q-values (assuming we can estimate them) corresponds to a mixture of the policies which have been used in the past, but not to the policy that will be used from there on, because the policy is non stationary (since the uncertainty decreases as more information is collected). I would recommend to be very careful when defining and using the successor features by emphasizing the policy under which those features are defined. \n\nSo in the end the contribution is mainly algorithmic. However I find it hard to say anything about the proposed approach, whether it improves over previous ones or not, specially because the experiments are limited to toy problems. Theoretical analysis or more complex experiments would make the paper stronger.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Okay paper, but needs more substantiation",
            "review": "This paper tackles the classical exploration / exploitation problem in reinforcement learning.\nThe paper argues that it is necessary to propagate uncertainty correctly and argue that they can do so using the successor representation to compute the Bayesian posterior over Q-values conditioned on the data already observed.\n\nNovelty:\nThis work is similar to “The uncertainty bellman equation” (UBE) (O’Donoghue et al. 2018) which adds a head to a regular DQN agent to predict a function u which is an upper bound of the variance of the posterior distribution over Q-values. The difference here is that the successor features are used here to predict Q-values and the function u.\nThe successor features can be seen as a discounted state occupancy of the current policy and carry information about the future. While the relation with the UBE is highlighted in section 4.6 an empirical evaluation between the two methods would also be needed.\n\nClarity: The method is detailed comparing to contextual bandit methods, the authors then argue that applying directly these methods to reinforcement learning case does not propagate uncertainty over several timesteps. While this indeed true it is misleading and imply that propagating uncertainty is not considered by current exploration methods in RL.\n\nSoundness:\nThe method presented here is relatively reductive. The estimated posterior over the value function is correct only if the transition model P is already known, otherwise Equation 4 would also need to incorporate uncertainty over P. Similarly, as the authors point out, this doesn’t include the max operation. At best, we are learning a posterior over the value function for a fixed policy, for when only the reward is unknown.\n\nAs a whole, the authors argue that their method allows a better propagation of Q-values uncertainty but provide little theoretical or experimental evidence that would back this claim.\n\nFrom a deep RL perspective, the features \\phi^l only carry local information. The authors argue that this leads to more stable features as these feature do not depend on the current policy. However it also means that in a sparse reward setting the reward observed would be zero most of the time and no useful features would be learned. In practice methods using the successor representation usually share parts of the network with other tasks to improve representation learning (see e.g. Figure 1 of Machado et al., Eigenoption discovery through the deep successor representation, 2017, & also their 2018 paper).\n\nExperiments:\nThe experiment are disappointing as they are only limited to tabular and deterministic problems. An obvious missing comparison is to UBE, at the minimum; and other “deep” algorithms such as BDQN, Bootstrap DQN, etc. Some of these algorithms have been shown to perform well on the Atari benchmark, and that seems like a reasonable point of comparison also.\n\nThe method also relies on knowing the successor features. While they can be learned easily in a tabular, deterministic MDP it is not clear how the posterior would behave in larger and/or non stochastic domains when it takes more time to learn these successor features.\n\nOverall, I am not sure what I learned from reading this paper. While the idea of using the successor representation in exploration is interesting and has been considered recently, the method presented in this paper needs to be better justified and evaluated on more challenging tasks.\n\n\nMinor comments\nI would like to see a proof of Equation 4, which may be simple but is not immediate.\n\nSome papers of relevance here:\n\nAn analysis of model-based Interval Estimation for Markov Decision\nProcesses, Strehl & Littman (2008)\n(More) efficient reinforcement learning via posterior sampling. Osband et al (2013)\nCount-Based Exploration with the Successor Representation, Machado et al. (2018)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}