{
    "Decision": {
        "metareview": "This paper studies memorization properties of convnets by testing their ability to determine if an image/set of images was used during training or not. The experiments are reported on large-scale datasets using high-capacity networks. \n\nWhile acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues:\n(1) more formal justifications are required to assess the scope and significance of this work contributions -- see very detailed comments by R2 about measuring networks capacity to memorize and the role of network weights and depth as studied in MacKay,2002. In their response the authors acknowledged they didn’t take into account network weights and depth but strived at an empirical evaluation scenario. \n(2) writing and presentation clarity of the paper could be substantially improved – see very detailed comments by R3 and also R2; \n(3) empirical evaluations and effect of the negative set used for training are not well explained and analysed (R2, R3).\n\nAC can confirm that all three reviewers have read the author responses and have contributed to the final discussion.\nAC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Contributions unclear",
            "review": "==============Final Evaluation================\nI have gone through the other reviews as well as the author response.\nFirstly, I would like to thank the authors for providing detailed responses to my questions.\n\nIn general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.\n\nMoreover, from my understanding the analysis in David McKay’s book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron). As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper). Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review). In general, I feel this section could use some tighter formalism and justifications.\n\nI also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen. The authors address this by saying 3M of the 15 M negatives have been seen. That does not seem like a small enough percentage to claim that these are “unseen” images.\n\nIn general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.\n==================\n\nSummary\nThe paper trains classification models to classify a labeling of a subset of images (assigned with label 1) from the rest of the images (assigned with a label 0). Firstly, the paper shows that deep learning models are able to learn such classifiers and get low training loss. It then proposes to use this model to ``attack’’ task-specific models to perform membership inference, i.e. figuring out if an image provided in a set was used in training or not. \n\nStrengths\n+ The paper thoroughly covers related work and provides context.\n+ Results on confidence as a signature of a dataset are interesting.\n\nWeaknesses\n\n[Motivation]\n1. In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization. Thus, without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct. Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)\n\n[Capacity]\n2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization. This is something which would need to be explained/ substantiated separately. (*)\n\n3. Scenario discussed in Sec. 4 seems somewhat impractical. Given a set of m images, it is not clear that a classifier that is trained to detect between train and validation is sufficient, as one might also need to figure out if it is neither train nor val, which is a very practical scenario.\n\n4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image ‘m’ corresponds to is useful or practical, as this seems to be a property of the set ‘m’ rather than the property of the trained classification model (f_\\theta). Please clarify. On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing. (*)\n\n6. It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model. While the baseline approaches seem to make use of the model confidence, I cannot see how the proposed approach (which uses a classifier) makes use of the original model. It is also not clear why Table. 3 does not report the Bayes baseline results. Also, does this section use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?\n\n7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model. (*)\n\nMinor Points\n1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’. In addition, the paper would also need to show that such a model does not generalize to a validation set of images. This is probably obvious given the results from Zhang et.al. but should be included as a sanity check.\n2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.\n\n\nReferences:\n[A]: Blier, Léonard, and Yann Ollivier. 2018. ``The Description Length of Deep Learning Models.’’ arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1802.07044.\n\nPreliminary Evaluation\nThere are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization’’ capabilities of the classifier trained to remember train vs val images is not clear in general. Important points for the rebuttal are marked with (*).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "review of \"an empirical evolution of the memorisation properties of Convents\"",
            "review": "Summary of the paper:\n\n\nThe paper has two intertwined goals. These goals are to illuminate the\ngeneralization/memorization properties of large and deep ConvNets in\ntandem with trying to develop procedures related to identifying\nwhether an input to a trained ConvNet has actually been used to train the\nnetwork. The latter task is generalized to detecting if a\nparticular dataset has been used to train a ConvNet. These goal are\ntackled empirically with multiple sets of experiments on largescale\ndatasets such as ImageNet22k and modern deep ConvNets architectures\nsuch as VGG and ResNet.\n\n\n\nPaper's positive points\n\n+ The paper has a very comprehensive set of references in the areas it\ntouches upon.\n\n+ Some of the experimental results presented are quite\ninteresting. They show that regularization data-augmentation helps\nprevent a network from explicit memorization and could be used as a\nway to help make training data more anonymous.\n\n+ Large scale experiments are reported on modern architectures.\n\n\nPaper's negative points\n\n- The paper makes use of a result from the David MacKay textbook\n  which defines the capacity of a single layer network to memorize the\n  labelling of $n$ inputs in $d$-dimensional space. If I'm not\n  mistaken, from this result the authors extrapolate that the capacity\n  of a (deep) neural network is proportional to the number of\n  parameters in the network. This is true, but there are a\n  couple of caveats. The first is that the coefficient of\n  proportionality must depend very much on the number of layers in the\n  network. Increasing the network's depth increases the efficiency of\n  the representation (i.e. fewer total parameters needed to have the\n  same representational power as a shallow network). And as MacKay\n  also says in his book (chapter 44 quoting findings from Radford\n  Neal) that for MLPs what determines the complexity of the typical\n  function (once the network has a large enough width) represented by\n  the MLP is the \"characteristic magnitude of the weights\". So the\n  regularization technique applied is very significant in the\n  controlling the effective capacity of a network. This paper\n  experimentally shows that is the case multiple times as it is shown\n  that with increasing degrees of regularization (figure 1, figure 2)\n  it becomes harder and harder to memorize the positive training\n  images. It would be great if the paper also made some attempt to\n  consider these connections. Or at least comment on how these factors\n  could be incorporated into a more sophisticated analysis of the\n  capacity of a network.\n\n\n\n- There is a slight oxymoron in the premise of the first set of\n  experiments. The network is forced to memorize a set of\n  positive examples relative to the negative set it sees during\n  training. What is memorized I presume depends a lot on the negative\n  set used for training (its diversity, closeness to the positive set\n  and how frequently each negative example is seen during\n  training). This issue is not really commented upon in the paper. Is\n  there a training task which would allow one to more explicitly\n  memorize the image (some sort of reconstruction task) as opposed to\n  an in/out classification task?\n\n- This paper is a slightly difficult read - not because of the\n  language or the presentation of the material but more because there\n  is not one main coherent argument or goal for the paper. This is\n  reflected in the \"Related work\" section where 4 different\n  issues/tasks are referred to. Each one of these topics is worthy of\n  a paper in itself, but this paper dips into each one and then\n  swiftly moves onto the next one. For example in section 3 the paper\n  explores if a network can be forced to explicitly memorize a set of\n  images and how the size of this set is affected by the number of\n  parameters in the network and data augmentation. High-level\n  conclusions are made: more parameters in the network implies more\n  images can be memorized and data-augmentation makes explicit\n  memorization more difficult. Then it is off to considering\n  pre-trained networks and determining whether by analyzing the\n  statistics of the responses at different layers one can decide if a\n  set of images was used for training or not (or similar tasks). Yes\n  the different sections are related but it is does not feel like they\n  build upon each other to help form a clearer picture of memorization\n  within neural networks.\n  \n\n- The conclusions focus on the importance of section 3 and\n  the results of the experiments performed. Do the conclusions accurately\n  reflect the opinions of the author? If yes, would\n  it better to re-organize the paper and devote more of it to the\n  material presented in section 3 and filling this out with more\n  analysis and experiments to perhaps explore the issue of the\n  capacity of a network in more \n\n\nQueries/ points that need some clarification\n\n- I'm a little unclear when data-augmentation is included in the\n  training phase whether the goal is to be able to also recognise\n  perturbed versions of the input images at test time. In section 3 is\n  a perturbed positive image considered a positive training image? And\n  in the testing phase are only unperturbed versions of the positive\n  images given to the ConvNet as input?\n\n- Last paragraph page 4: \"when the accuracy gets over 60\\% and again\n  at 90\\%\". Is this training or validation accuracy?\n\n\n\n\nTypos possible errors spotted along the way:\n\n* First paragraph page 5: \"more shallow\" --> \"shallower\"\n* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"\n* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty\n  non-standard and most people would use \"scenarios\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"an empirical evaluation of the memorization properties of convnets\"",
            "review": "I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. \n\nPS: I am downgrading my confidence in my evaluation.\n\n---\n\nPaper 93 proposes an empirical evaluation of the memorization properties of convnets. More specifically, it evaluates three aspects:\n-\tFirst it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier. The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.\n-\tSecond, it evaluates whether we can detect that a group of samples of a dataset was used to train a model. For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions. It is shown experimentally that one can detect (even partial) leakage with such a technique.\n-\tThird, it evaluates whether we can detect that a single images was used to train a convnet. Two simple techniques are proposed. The first one considers that a sample is part of the training set if it correctly classified. The second one considers that a sample is part of the training set if its loss is below a threshold. It is shown experimentally that one can make such a decision with moderate accuracy.\n\nOn the positive side:\n-\tThis is a topic that should be of broad interest to the ICLR community.\n-\tThe paper is generally well-written.\n-\tThe experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.\n\nOn the negative side:\n-\tIt is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?\n-\tIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?\n-\tSection 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.\n-\tThe experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy. Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3). This seems to be too low to be of practical use. This might be because the Bayes and MAT attacks are too simplistic. Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}