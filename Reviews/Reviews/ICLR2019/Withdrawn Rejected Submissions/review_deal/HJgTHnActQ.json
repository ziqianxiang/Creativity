{
    "Decision": {
        "metareview": "The paper received mixed ratings. The proposed idea is quite reasonable but also sounds somewhat incremental. While the idea of separating foreground/background is reasonable, it also limits the applicability of the proposed method (i.e., the method is only demonstrated on aligned face images). In addition, combining AdaIn with foreground mask is a reasonable idea but doesn’t sound groundbreakingly novel. The comparison against StarGAN looks quite anecdotal  and the proposed method seems to cause only hairstyle changes (but transfer with other attributes are not obvious). In addition, please refer to detailed reviewers’ comments for other concerns. Overall, it sounds like a good engineering paper that might be better fit to computer vision venue, but experimental validation seems somewhat preliminary and it’s unclear how much novel insight and general technical contributions that this work provides.\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "Very structured but seemingly effective image 2 facial image translation.",
            "review": "The paper deals with image to image (of faces) translation solving two main typical issues: 1) the style information comes from the entire region of a given exemplar, collecting information from the background too, without properly isolating the face area; 2) the extracted style is applied to the entire region of the target image, even if some parts should be kept unchanged. The approach is called LOMIT, and is very elaborated, with source code which is available (possible infringement of the anonymity, Area Chair please check). In few words, LOMIT lies on a cosegmentation basis, which allows to find semantic correspondences between image regions of the exemplar and the source image. The correspondences are shown as a soft mask, where the user may decide to operate on some parts leaving unchanged the remaining (in the paper is shown for many alternatives: hair, eyes, mouth). Technically, the paper assembles other state of the art techniques,  (cosegmentation networks, adaptive instance normalization via highway networks) but it does it nicely. The major job in the paper lies in the regularization part, where the authors specify each of their adds in a proper way. Experiments are nice, since for one of the first times provide facial images which are pleasant to see. One thing I did not like were on the three set of final qualitative results, where gender change results in images which are obviously diverse wrt the source one, but after a while are not communicating any newer thing. Should have been better to explore other attributes combo.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "paper should be improved before publishing",
            "review": "Summary--\nThe paper tries to address an issue existing in current image-to-image translation at the point that different regions of the image should be treated differently. In other word, background should not be transferred while only foreground of interest should be transferred. The paper propose to use co-segmentation to find the common areas to for image translation. It reports the proposed method works through experiments.\n\nThere are several major concerns to be addressed before considering to publish.\n\n1) The paper says that \"For example, in a person’s facial image translation, if the exemplar image has two attributes, (1) a smiling expression and (2) a blonde hair, then both attributes have to be transferred with no other options\", but the model in the paper seems still incapable of transferring only one attribute. Perhaps an interactive transfer make more sense, while co-segmentation does not distinguish the part of interest to the user. Or training a semantic segmentation make more sense as the semantic segment can specify which region to transfer.\n\n2) As co-segmentation is proposed to \"capture the regions of a common object existing in multiple input images\", why does the co-segmentation network only capture the eye and mouth part in Figure 2 and 3, why does it capture the mouth of different shape and style in the third macro column in Figure 4 instead of eyes? How to train the co-segmentation module, what is the objective function? Why not using a semantic segmentation model?\n\n3) The \"domain-invariant content code\" and the \"style code\" seem rather subjective. Are there any principles to design content and style codes? In the experiments, it seems the paper considers five styles to transfer as shown in Table 1. Is the model easy to extend to novel styles for image translation?\n\n4) What does the pink color mean in the very bottom-left or top-right heatmap images in Figure 2? There is no pink color reference in the colorbar.\n\n5) Figure 5: Why there is similariy dark patterns on the mouth? Is it some manual manipulation for interactive transfer?\n\n6) Though it is always good to see the authors are willing to release code and models, it appears uncomfortable that github page noted in the abstract reveals the author information. Moreover, in the github page,\neven though it says \"an example is example.ipynb\", the only ipynb file contains nothing informative and this makes reviewers feel cheated.\n\nMinor--\nThere are several typos, e.g., lightinig.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comparison and experiment setting are not explained well",
            "review": "This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions.\n\nPros:\n* This paper proposes to jointly learn the local mask to make the translation focus on the foreground instead of the whole image.\n* The local mask-based highway adaptive instance normalization apply the style information to the local region correctly.\n\nCons:\n* There seems a conflict in the introduction (page 1): the authors clarify that “previous methods [1,2,3] have a drawback of ....” and then clarify that “[1,2,3] have taken a user-selected exemplar image as additional input ...”. \n* As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. \n* It is mentioned in the introduction (page 2) that “This approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation”. However, the differences between the proposed method with these prior works are not compared or mentioned. Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. I wonder the advantages of the proposed method compared to these works.\n* The experiment setting is not clear enough. If I understand correctly, the face images are divided into two groups based on their attributes (e.g. smile vs no smile). If so, what role does the exemplar image play here? Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? \n* The github link for code should not provide any author information.\n\n[1] Multimodal Unsupervised Image-to-Image Translation\n[2] Diverse Image-to-Image Translation via Disentangled Representations\n[3] Exemplar Guided Unsupervised Image-to-Image Translation\n[4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation\n\nOverall, I think the proposed method is well-designed but the comparison and experiment setting are not explained well. My initial rating is weakly reject.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}