{
    "Decision": {
        "metareview": "The paper can also improved thorough a more thorough evaluation. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not enough novelty and in a somewhat niche area"
    },
    "Reviews": [
        {
            "title": "Interesting topics are introduced but some corrections and clarifications are necessary",
            "review": "The authors introduce the problem of learning embeddings that consider both text information and graph structures, as well as the embedding of a sequence of nodes with embeddings.\n\nHowever, the proposed algorithm, SENSE-S, is incremental in the sense of aggregating two simple structures. In the evaluation, it is compared only with the heuristic combination of node2vec and paragraph2vec, not with any existing work about the graph embeddings that incorporate node features even though they are mentioned in the related work.\n\nFurthermore, the objective of node sequence embedding is not clear. What do we want to represent from the embedding of node sequences? It looks like we have to keep the node embeddings anyway, and then what is the problem of just storing node ordering instead of having representation? Or can we aggregate node embeddings in some way with storing the order of nodes? These kinds of questions can be raised, mainly because of uncertain objectives. The description of preserving both ordering and node properties is too vague.\n\nAlso, SENSE does not seem to have any connection with SENSE-S. Why is SENSE-S special to SENSE? Are they independent?\n\nFinally, the authors claim that SENSE is necessary to overcome the space issue that needs q*d dimension. However, from Figure 5, it seems that the proposed algorithm actually needs O(q*d) dimensions to represent the sequence correctly. It is somewhat related to the question about i.i.d. assumption in Theorem 2, where embedding does not guarantee the orthogonality across the dimensions. \n\n* Details\n- In the introduction, \"first\" is repeated in the last paragraph of Page 1.\n- N_G(v) and N_T(\\phi) are said to be independent, but it should be the assumption since they are not the fact.\n- Eq. (2) is not aligned with Eq (3) or (4). Either one needs to be fixed or the derivation needs to be described.\n- How SVM is used needs to be described. Usage of embedding might be different depending on the usage of RBF kernel or linear kernel.\n- Using the smaller number of random walks for Citation Network because it is a larger dataset needs some explanation.\n- The calculation on the improvement percentage is completely misleading. If the accuracy is improved from 95% to 96%, it is about 1% improvement, not 20% improvement based on the error rate calculation.\n- How the training/validation/test sets are split needs more description. Is it split by nodes or edges?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents two approaches: one called SENSE-S for embedding nodes in attributed networks; the other one called SENSE for embedding a sequence of nodes. SENSE-S follows the structure of Skip-gram model. The main difference is that SENSE-S considers both node and words in node content as input and output for learning their embedding. ",
            "review": "This paper presents two approaches: one called SENSE-S for embedding nodes in attributed networks; the other one called SENSE for embedding a sequence of nodes. SENSE-S follows the structure of Skip-gram model. The main difference is that SENSE-S considers both node and words in node content as input and output for learning their embedding. For generating embedding vector for a sequence of nodes, SENSE takes summation of cyclically shifted unit-vectors constructed by SENSE-S on nodes in a sequence.    \n\nThe paper is well written with a clear definition of the studied problem and a clear introduction of the presented methods. Evaluation was conducted on two real-world data sets (Wikipedia and citation network). It is an interesting idea to represent a sequence by the summation of cyclically shifted unit-vectors of nodes in a sequence. However, there are several concerns about the work presented in this paper. \n1) the evaluation of SENSE-S is not sufficient. The baseline methods used in comparison are the simple ones that take concatenation of vectors induced from text and graph, or use one for initializing the learning of the other.  There existing several approaches that learn node embedding vectors from attributed graph (considering both the node content text and graph topology structure), such as TADW [1], HSCA [2], PLANE [3],GAE[4], AANE[5], ANRL [6]. SENSE-S should be compared with these methods for showing its effectiveness. \n2) the embedding vector of a node sequence is evaluated by showing the decoding accuracy. It would be more interesting to show how these vectors can be used for some real applications. And, to have high decoding accuracy, the embedding dimension for sequences of 10 nodes should be up to 1024, which is quite expensive for computing and for storage, making the presented method unpractical in real-world applications.   \n\n\n[1] C. Yang, Z. Liu, D. Zhao, M. Sun, E. Y. Chang, Network representation learning with rich text information. IJCAI, 2015\n[2] D. Zhang, J. Yin, X. Zhu, C. ZHang, Homophily, structure, and content augmented network representation learning.  ICDM 2016. \n[3] T. M. V. Le and H. W. Lauw. Probabilistic latent document network embedding.  ICDM, 2014.\n[4] Thomas N Kipf, Max Welling. Variational Graph Auto-Encoders. NIPS Workshop on Bayesian Deep Learning.  2016\n[5] Xiao Huang, Jundong Li, Xia Hu. Accelerated attributed network embedding. SDM 2017.\n[6] Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang. ANRL: Attributed Network Representation Learning via Deep Neural Networks. IJCAI, 2018\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea and fleshed-out experiments, but somewhat niche appeal.",
            "review": "The paper proposes node embedding methods for applications where nodes are sequentially related. An example application is the \"Wikispeedia\" dataset, in which nodes are connected in a graph, but a datapoint (a wikispeedia \"game\") consists of a sequence of nodes that are visited. Each node is further attributed with textual information.\n\nThe methods proposed are most closely related to skipgrams, whereby the sequence of nodes are treated like words in a sentence. Then, node attributes (i.e., text) and node representations must be capable of predicting neighboring nodes/words. (Fig.s 1/2 are a pretty concise overview of the proposed architecture).\n\nPositively, this is a quite sensible extension and modification of existing ideas in order to support a new (or different) problem setting.\n\nNegatively, I'd say the applications for this technique are fairly niche, which may limit the paper's readership. The method is mostly fairly straightforward and not methodologically groundbreaking (probably borderline in terms of expected methodological contribution for ICLR). I also didn't understand whether the theoretical claims were significant.\n\nThe wikispedia/physics experiments feel a bit more like proofs-of-concept rather than demonstrating that the technique has compelling real-world uses. The experiments are quite well fleshed-out and detailed though.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}