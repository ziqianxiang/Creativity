{
    "Decision": {
        "metareview": "This paper demonstrated interesting observations that simple transformations such as a rotation and a translation is enough to fool CNNs. Major concern of the paper is the novelty. Similar ideas have been proposed before by many previous researchers. Other networks trying to address this issue have been proposed. Such as those rotation-invariant neural networks. The grid search attack used in the experiments may be not convincing. Overall, this paper is not ready for publication.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting observations. Novelty limited. Related work missing."
    },
    "Reviews": [
        {
            "title": "Solid experimental study of approximate worst and average case input image rotation and translation",
            "review": "Summary:\nStandard CNN models for MNIST, CIFAR10 and ImageNet are vulnerable with regard\nto (adversarial) rotation and translation of images.\nThe paper experimentally examines different ways of formulating attacks\n(gradient descent, grid search and sampling) and defenses\n(random augmentation, worst-case out of sample robust training,\naggregated classification) for this class of image transformation.\n\nThe main results are:\n- Gradient descent is not effective at generating worst-case rotations /\ntranslations due to nonconcavity of the adversarial objective\n- Grid search is very effective due to low parameter space\n- Sampling and pick the worst is also effective and cheap, for similar reasons\n- L infinity ball pixel perturbation robustness is orthogonal to the examined\ntransformations and does not provide good defense mechanism\n- Just augmenting data with random translation / rotations is not a strong\ndefense\n- Using a worst-case out of sample of 10 for training with an approximation of\na robust optimization objective combined with an aggregated result for\nclassification is a stronger defense\n\nRecommendation:\nThe paper presents a comprehensive study of a relevant class of adversarial\nimage perturbations for state-of-the-art neural network models.\nThe results are a useful pointer towards future research directions and for\nbuilding more robust systems in practice.\nI recommend to accept the paper.\n\nStrong points:\n- The paper is well written, has clear structure and is technically easy to\nunderstand.\n- The question of padding and cropping comes up naturally and is then answered.\n\nOpen questions (things that could potentially be of interest when added):\n- Loss landscapes look like most of the nonconcavity is along the translation\nparameter. Any idea why?\n- What mechanisms within CNN models do or do not learn (generalize) rotation\nand translation from provided data (including augmentation)?\n\nSpecific:\n- Page 2: perturbrbations (Typo)\n- Page 3: witho (Typo)\n- Page 3: Constrained optimization problems typically written as\nmax_{...} \\mathcal L(x', y) s.t. x' = T(...)\n(s.t. for subject to instead of for) but that's matter of taste I guess\n- Page 4: first order -> first-order (consistency)\n- Page 4: tyipcally (Typo)\n- Page 4: occurs most common(ly)\n\nI am not sufficiently knowledgable about the previous literature to ensure that\nthe claimed novelty of the paper is truly as novel.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "some interesting observations",
            "review": "The paper states that basic transformation (translation and rotation) can easily fool a neural network in image classification tasks. Thus, image classification models are actually more vulnerable than people thought. The message conveyed by the paper is clear and easy to get. The experiments are natural and interesting. Some interesting points:\n  --The model trained with data augmentation that covers the attack space does not alleviate the problem sufficiently.\n  --Gradient descent does not provide strong attack, but grid search does. This may be due to the high non-concavity, compared to the small perturbation case.\n\nOne possible question is the novelty, as this idea is so simple that probably many people have observed similar phenomenon--but have not experimented that extensively. \nAlso, there are some related works that also show the vulnerability under spatial transformations. But some are concurrent works to 1st version of the paper (though published), so I tend to not to judge it by those works.  \n\nOther comments: \n1. page 3 in the paragraph starting with ‘We implement …’, the author chooses a differentiable bilinear interpolation routine. However, the interpolation method is not shown or explained. \n2. In term of transformation, scaling and reflecting are also transformations. It should be straightforward to check the robustness with respect to them. Comments? \n3. Header in tables is vague. Like ‘Natural’ or ‘Original’, etc. More description of the Header under tables is helpful.\n4. For CIFAR10 and especially for ImageNet dataset, Aug30 and Aug40 models showed lower accuracy than No Crop model on Nat test set. This is little strange because data augmentation (such as random rotation) is commonly used strategy to improve test accuracy. I think this might mean that the model is not trained enough and underfitted, maybe because excessive data augmentation lowered the training speed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting approach but not yet matured enough. ",
            "review": "Summary\nThe authors study robustness of neural networks for image recognition tasks with respect to geometric transformations in input space. The question is posed in an adversarial setting, where the authors exploit that Conv/ResNets are not fully translation and rotation invariant. The authors propose three untargeted attacks to increase the classification error of the network: a first-order method, an attack involving random transformations and a grid search of allowed transformations. For the random and grid search the worst prediction is considered the outcome of the attack. The authors observe that first-order attacks are not very successful in fooling the network compared to the grid search. Data augmentation as a counter measure is found to be not sufficient and adversarial (robust) training with respect to the random search attack is proposed in addition.\n\nEvaluation\nThe paper is well written and particularly the empirical part is interesting. However, novelty is limited, the best approach boils down to a grid search that tests multiple hypotheses instead of a single one. There are some conceptual problems and important aspects like confidences of the classification are not addressed.\n\nNovelty:\nMany claims and observations appear trivial and well-known. E.g.,\n- the research question has already been addressed by related work, leaving the proposed attacks trivial given that the attack space (allowed transformations) is specified ad hoc and without a proper measure.\n- that data augmentation and training with the adversarial loss function (i.e. with the attack scheme in mind) is helpful is straight forward and not surprising\n\nDetailed comments:\nThe authors study whether neural networks are robust to transformations in input space and resort to a benign adversarial setting. I'm wondering whether this allows for an answer regarding general robustness? That is, the experiments are conducted wrt the worst case, while the training does not account for an attack setting. E.g, it is unclear why the classifier would not involve a pre-processing step to counter transformations in input space, see Rowley et al. (1998).\n\nTranslation and rotation invariance of neural networks has been addressed by many authors, e.g., see Jaderberg et al. (2015) and Marcos et al. (2017).\n\nAdversarial examples are defined to be similar and misclassified with high confidence.\nThe similarity of the transformation is not addressed properly. E.g., if the goal of the adversary is to force errors, why not allow for rotations of 180 degrees? Pixel-based attacks (Goodfellow et al., 2014) are more rigorous in this regard while the cited transformation-based attacks (Kanbak et al. 2017; Xiao et al., 2018) are virtually indistinguishable from the real test cases.\n\nThe effectiveness of the grid search attack seems to be connected to performing $5 * 5 * 31 = 775$ individual tests for each test case where only the worst outcome would count. The sheer number should render a misclassification more likely compared to the competitors. This is supported by empirical findings showing that only a small subset of transformations per test case accounts for the misclassification on CIFAR10 and ImageNet (Fig. 10 in the Appendix).\n\nRegarding the padding experiments, I wonder whether the network architecture is appropriate for the new input. Here, more experimentation is necessary. The conclusion with respect to the first-order method remains a conjecture.\n\nReferences:\n- Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n- Max Jaderberg, Karen Simonyan, and Andrew Zisserman. Spatial transformer networks. In Advances in neural information processing systems, pp. 2017–2025, 2015.\n- Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric robustness of deep networks: analysis and improvement. arXiv preprint arXiv:1711.09115, 2017.\n- Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n- Henry Rowley, Shumeet Baluja, and Takeo Kanade. Rotation invariant neural network-based face detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 38. sn, 1998.\n- Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed adversarial examples. arXiv preprint arXiv:1801.02612, 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}