{
    "Decision": {
        "metareview": "This paper proposes a training algorithm for ConvNet architectures in which the final few layers are fully connected.  The main idea is to use direct feedback alignment with carefully chosen binarized (±1) weights to train the fully connected layers and backpropagation to train the convolutional layers. The binarization reduces the memory footprint and computational cost of direct feedback alignment, while the careful selection of feedback weights improves convergence. Experiments on CIFAR-10, CIFAR-100, and an object tracking task are provided to show that the proposed algorithm outperforms backpropagation, especially when the amount of training data is small. The reviewers felt that the paper does a terrific job of introducing the various training algorithms --- backpropagation, feedback alignment, and direct feedback alignment --- and that the paper clearly explained what the novel contributions were. However, the reviewers felt the paper had limited novelty because it combines ideas that were already known, that it has limited applicability because it will not work with fully convolutional architectures, that the baselines in the experiments were somewhat weak, and that the paper provided no insights on why the proposed algorithm might be better than backpropagation in some cases. Regrettably, only one reviewer (R2) participated in the discussion, though this was the reviewer who provided the most constructive review. The AC read the revised paper, and agrees with R2's concerns about the limited applicability of the proposed algorithm and lack of insight or analysis explaining why the proposed training algorithm would improve over backpropagation.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Great explanation of prior work, but limited applicability and no insight or analysis"
    },
    "Reviews": [
        {
            "title": "The contribution is limited",
            "review": "This paper targets at developing new DFA method to replace BP for neural network model optimization, in order to speed up the training process. The paper is generally written clearly and relatively easy to follow. \n\nMy main concern is about significance of the contribution of this paper.\n\n1. the novelty is limited. This paper only simply combines two well-known approach BP and DFA together. \n\n2. performance contribution seems not significant from the proposed approach. In the implementation, the authors only apply their approach to optimize a few top layers. A majority of the layers in the NN model are still optimized via BP. \n\n3. the authors should provide more evaluations on different NN backbones and datasets, to make the experiments stronger and more convincing.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper propses to use a combination of Direct Feedback Allignment (DFA) and BackPropagation (BP) to improve upon standard back propagation.\nTo understand what is done, consider the following: Feedback Alignment is +- equal to back propagation when using random but fixed weights in the backwards pass. Direct feedback alignment uses random backprojections directly to the layer of interest. \nThe advantage of DFA is that It bypasses the normal computational graph. The advantage of this is that if compute is infinite, all of these updates can be computed in parallel instead of pipelining them as is done in standard BP.\n\nIn the current paper, the use of DFA for dense layers and BP for conv layers which is named CDFA is proposed.\nIn addition the paper also proposes a binarized version of BDFA to limit memory consumption and communication. It is claimed that the proposed techniques improve upon standard back propagation.\n\nOverall, the paper is easy to understand, but I lean towards rejecting this paper because I am not convinced by the experimental evidence. As outlined below, the key issue is that the baseline appears to be weak. Additionally, the main limitation of the proposed approach can only benefit a very limited set of architectures. \n\nPositive points:\n---------------------\nThe authors did an excellent job of introducing BP, FA and DFA in the paper. This makes the core concepts and ideas accessable without having to delve through prior work.\n\n\nThe own contributions and the key idea is easy to understand. \n\nLimitations and possible improvements\n-------------------------------------------------------\nA core limitation is that recent networks do not have a combination of dense layers and convolutional layers. In many cases the networks are fully convolutional, this limits the applicability of the proposed combination of DFA and BP. The use of additional networks would benefit the paper. Currently only VGG 16 on Cifar 10 is used. Also, the data augmentation strategy is not discussed. Of course, it would be nice if additional datasets could be included as well, but this of course depends on the computational resources the authors have available. \n\n\nThe key issue to me is that performance improvements for CIFAR are reported, but I fear that the baseline accuracy for VGG16 might be a bit low. If I memory serves me well, it should be able to achieve around 90% at least on CIFAR 10 using VGG style networks. I did a quick search and found http://torch.ch/blog/2015/07/30/cifar.html corroborating this but I did not verify this directly. \n\n\nRelated to the previous point, since this is an empirical paper, describing the hyper-parameter optimizations and final settings in detail  can convince the reader that the study is exectued correctly. Much of the information is missing now.\n\n\nSimilarly, I have trouble understanding section 4.1 and section 4.2 since I do not know the exact details of the experiments. This can be fixed easily however.\n\n\nProvide complexity estimates of the potential speedup or provide actual timing information. (Although this might not be that meaningful without much additional work given that gpu kernels are often heavily optimized).\n\n\nLast year there was a submission to ICLR about fixing the final output layer and only learning the convolutional layers. If we consider that random projections work remarkably well and can be considered approximations of kernels, it could be interesting to add a baseline where the fully connected layers are fixed and only the convolutional layers are trained. The error signal can be propagated using standard BP, FA or DFA methods but it would shed light on whether learning in the higher layers is actually needed or BP in the conv layers is sufficient.\n\nMinor possible improvements\n------------------------------------------\nFinally, I would strongly suggest that the authors perform some additional proofreading. There are quite a few strange formulations and spelling mistakes. That being said, it did not prevent me from understanding the manuscript so this remark DID NOT factor into my judgement.\n\nIn addition to remark above, I would suggest removing the second paragraph from the introduction. It feels out of place to me, and the vanishing gradient effects are not discussed in the remainder of the manuscript.\n\n\nThe list of possible optimizers before the selection for SGD+Momentum is not needed. Simply stating that SGD with momentum is used should be sufficient. \n\n\n“Training from scratch” instead of “Training from the scratch”\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The method needs more insight or novelty, and the results have room to improve",
            "review": "This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.\n\nOriginality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. \n\nThe experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\nTherefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.\n\nMinor typos:\nIn Equation 1, the letter i is overloaded.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}