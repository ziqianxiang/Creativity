{
    "Decision": {
        "metareview": "The paper proposes batch-constrained approach to batch RL, where the policy is optimized under the constrain that at a state only actions appearing in the training data are allowed.  An extension to continuous cases is given.\n\nWhile the paper has some interesting idea and the problem of dealing with extrapolation in RL is important, the approach appears somewhat ad hoc and the contributions limited.\n\nFor example, the constraint is based on whether (s,a) is in B, but this condition can be quite delicate in a stochastic problem (seeing a in s *once* may still allow large extrapolation error if that only observed transition is not representative).  Section 4.1 gives some nice insights for the special finite MDP case, but those results are a little weak (requiring strong assumption that may not hold in practice) --- an example being the requirement that s' be included in data if (s,a) is in data and P(s'|s,a)>0 [beginning of section 4.1].\n\nIn contrast, there are other more robust and principled ways, such as counterfactual risk minimization (CRM) for contextual bandits (http://www.jmlr.org/papers/v16/swaminathan15a.html).  For MDPs, the Bayesian version of DQN (the cited Azizzadenesheli et al., as well as Lipton et al. at AAAI'18) can be used to constrain the learned policy as well, with a simple modification of using the CRM idea for bandits.  Would these algorithms be reasonable baselines?",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Nice work with potential, but contributions need to be strengthened"
    },
    "Reviews": [
        {
            "title": "Solid approach to applying RL algorithms to batch imitation learning from noisy demonstrations",
            "review": "Summary:\nProposes BCRL for learning from a fixed collection of off-policy experience (I'll call this the \"training data\"). BCRL attempts to avoid backing up values from states that are not present in the training data, on the assumption that the current estimates of these values are likely to be inaccurate. In the continuous state-action case, this is accomplished by training a generative model to propose, given a state `s`, an action `a` such that a transition similar to `(s, a)` is in the training data. A secondary policy is then trained to perturb the proposed action within a constrained region to maximize value. BCRL outperforms DDPG and DQN when learning from fixed data, but BCRL is slightly worse than behavior cloning at learning to reproduce an expert policy that does not take exploration actions.\n\nReview:\nThe overall approach is sound. The problem of extrapolation is intuitively obvious, but not something I had thought about before. I think typically exploration would correct the problem since states with over-estimated values would become more likely to be reached, giving an opportunity to get a better estimate. \n\nThe learning setting is closer to imitation learning than to what I would call RL, since the BCRL approach essentially avoids extrapolation error by ignoring the parts of the problem that are not represented in the training data. The well-known problem with behavior cloning is compounding errors once the agent strays into areas of the state space that are far from the training data. To me \"off-policy RL\" implies that the goal is to learn a complete policy from off-policy data. I think the \"competitors\" to which BCRL should be compared are imitation learning algorithms address noisy demonstrations, and not so much off-policy RL algorithms. It would also be interesting to see the generalization performance of BCRL outside of its training data.\n\nThe BCRL idea might be applicable in a conventional RL setting as well, since the initial stages of learning could be subject to a similar extrapolation error until there has been enough exploration. A comparison to something like TRPO in this setting would be interesting.\n\nThe paper is well-written with good coverage of related literature. There are a few points where the technical content is imprecise, which I note below. \n\nComments / Questions:\n* Could one obtain a similar effect to BCRL by simply initializing the value estimates pessimistically?\n* Sec 4.1: Since B is a set of (s, a, s', r) tuples, what does it mean for a state s' to be \"in B\"? Similar question for state-action tuples (s, a).\n* As you note in the appendix, the construction in Sec 4.1 is essentially creating a new MDP that contains only the transitions that occur in the training data. I'd suggest stating as much in the main paper for intuition.\n* Sec 4.2 / 5: The perturbation constraint \\Phi is set to 0.05 in the experiments. Since the actions in these control problems are vectors, what does a scalar constraint correspond to? How is the constraint enforced during learning?\n* What are the distance functions D_S and D_A?\n\nPros:\n* A good approach to applying RL methods in the \"imitation-like\" setting. I've seen similar things attempted before, but this method makes more sense. \n\nCons:\n* The learning setting is more like \"fuzzy\" behavior cloning from noisy data than off-policy RL. Experimental comparison against more-sophisticated imitation learning approaches is missing.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting ideas, but clarity must be improved",
            "review": "Authors consider a problem of off-policy reinforcement learning in a setting explicitly constrained to a fixed batch of transitions. \nThe argument is that popular RL methods underperform significantly in this setting because they fail to account for extrapolation error caused by inevitably sparse sampling of the possible action-state space.\nTo address this problem, authors introduce the notion of batch-constrained RL which studies policies and associated value functions only on the state-space covered by the available training data.\nFor practical applications a deep RL method is introduced which enables generalisation to the unseen states and actions by the means of function approximation.\n\nI find the problem studied in the paper very important. It is indeed strongly connected to the idea of imitation learning which has been studied previously, but I like the explicit point from which authors see the problem.\nThe experimental results seem quite appealing to justify use of the proposed approach.\n\nHowever, on the clarity side the paper should be improved before publication.\n\nThe interplay between action generating VAE G_w(s) and \\pi is unclear to me.\nFirst, what does it mean that G(s) is trained to minimise the distance D_A?\n\nIf G(s) is a VAE, then it is trained to minimise the corresponding variational lower bound, how is minimisation of the distance over actions is incorporated here? And what exactly is this distance?\nSimilarly, what does “D_S will be defined by the implicit distance induced by the function approximation” exactly mean?\n\nOther comments / questions:\n\nPage 6: “Theorem 1 implies with access to only\na subset of state-action pairs in the MDP, the value function… This suggests batch-constrained policies\nare a necessary tool for combating extrapolation bias.”\nThis might be true, but it does not follow from the Theorem 1 as it only applies to the batch Bellman operator and not the standard one used in most methods.\n\nCorollary 1 and 2: What is Q^* here?\n\nPage 7, first sentence: should there be if A_s, e != \\emptyset? \n\nEpsion-Batch-constrained policy iteration: would the beam search actually maximize Q function? This needs to be proven or at least discussed.\n\nI don’t see how is epsilon used in the iteration scheme.  This needs to be clarified.\n\nEquation 11: the subscript of the max operator looks weird, should there be just a_i?\n\nFigure 4: where is “True value” curve on the plots?\n\nThe notation \\pi(s, a; \\Phi) used throughout the paper is confusing and can be interpreted as a joint distribution over states and actions.\n\nAs I said, currently the paper does not appear to be easy to follow to me and even if it does contain important ideas, I believe they must be communicated in a clearer way.\nI am eager to revise my evaluation if authors make substantial effort to improve the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "How well and why it works at states that are far from any states in the batch?",
            "review": "This paper studies extrapolation error in off-policy batch reinforcement learning (RL), where the extrapolation error refers to the overestimation of the value for the state-action pairs that are not in the training data.\n\nThe authors propose batch-constrained RL, where the policy is optimize under the constraint that, at each state, only those actions that have been taken in that state in the training data are allowed.  This is then extended to continuous space, where it allows only the state-action pairs that are close to a state-action pair in the training data.  When there is no such action for a given state, the action that is closet to a feasible action at that state is selected.\n\nIt makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?  Is it because the experiments are performed under rather deterministic settings?  How often are no state-action pairs found in the neighbor?  Is there any mechanism for recovering from \"not in the batch\"?\n\nThe paper would be much stronger if it study this challenge of \"not in the batch\" more in depth.  Technical contributions in the present paper are rather limited.\n\nA key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.  Although this assumption is stated, it would be nice to emphasize by also stating that the theorems do not hold when this assumption does not hold.  The assumption becomes less important for continuous case, because of approximation.  It might be interesting to study the performance of the proposed approach when the assumption does not hold in the continuous case.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}