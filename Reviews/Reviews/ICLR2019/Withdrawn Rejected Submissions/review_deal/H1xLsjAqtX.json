{
    "Decision": "",
    "Reviews": [
        {
            "title": "The idea of jointly training a word/phrase/sentence mask with the classifier at training time (with some added complexity) to get faster predictions at test time is interesting; however, current results are largely preliminary and requires refinement of performance and understanding",
            "review": "The authors consider the setting of training a RNN-based text classification where there is a resource restriction on test-time prediction (e.g., time). Conceptually, the approach is essentially a masking mechanism to reduce the number of words/phrases/sentences used in the prediction at test time followed by a classifier trained to handle these ‘missing’ components. This robustness is achieved by a data aggregation approach that trains on several masked variants determined by the budget level to generalize when encountered new masked instances. This work is most similar to finding ‘rationales’ in text (e.g., [Lei, Barzilay & Jakkola; EMNLP16]) except the selector is much simpler (specifically, using ‘shallow’ learners) and is compensated for with the robust classifier component. Experiments are conducted on four datasets, demonstrating varying levels of performance improvements over baseline and competing methods in terms of accuracy and test-time speedup — followed by more in-depth experiments on multi-aspect sentiment and IMDB datasets, resulting in several more detailed observations.\n\nFrom a high-level perspective, reducing resource requirements specifically at test-time is ostensibly useful in practice as many applications would likely be willing to trade-off some accuracy for {faster performance, lower power requirements, reduced communication bandwidth}, etc. Furthermore, the approach described is sensible and results in some promising empirical findings. My primary concerns with this specific paper are two-fold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what I would expect to work on a new dataset (2) the tradeoffs aren’t well calibrated in the sense that I don’t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesn’t seem trivial to set tradeoffs).  Some of this was considered with additional empirical results, but these also didn’t have sufficient clarity nor discussion for me to really understand the recommendation. Accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.\n\nEvaluating the paper along the requested dimensions:\n\n= Quality: The number of datasets are sufficient and the results are convincing for some configurations. Thus, the authors have established that the method is practical and is promising. However, there isn’t an in-depth study of when the proposed method works, under what configurations, and discussion on how to push this forward. One way to make this more transparent would be to include some curves wrt Table 1 such that I can observe the performance/speedup tradeoffs (is it monotonic?). A good start, but seems incomplete to be really convincing. (4/10)\n\n= Clarity: The writing is fairly clear and well-contextualized wrt existing work. I do think that the discussion could be improved overall in terns of better telling the reader what they should be focusing on in the empirical results. My one specific issue was I still don’t understand what is in the parentheses of {LSTM-jump, skim-RNN} in Table 1. Maybe it is in the text, but I really couldn’t decode (and I can understand the semantics of parentheses in other parts of the table). (4/10)\n\n= Originality: I haven’t seen anything specifically like this, even if it does draw on very closely related methods. However, I think putting them together required good knowledge from ML and NLP communities, and thus wasn’t trivial and the basic idea is clever. (6/10)\n\n= Significance: The underlying idea is interesting and could be significant if further developed, both in terms of better understanding the dynamics and providing better guidance regarding how to do this well in practice. However, just based on the content of the paper, I might play with this, but don’t see it as a solution to problems in low-resource settings, mixed-computation networks (like the cloud example in 4.2). Interesting, but needs more development to be widely adopted. (5/10)\n\n=== Pros ===\n+ an interesting idea that pragmatically builds on existing work to provide a practical solution\n+ experiments conducted on several datasets with some interesting results\n\n=== Cons ===\n- empirical results mixed and little clarity provide on why\n- other than speedup, doesn’t consider other settings in experiments (e.g., power, compression)\n- little theoretical development\n- nothing said about training time difference (even if not the point)\n\nOverall, I like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur). However, I think the idea needs to be further developed before being published at a top-tier venue.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A co-trained logistic regression can discover task-dependent stop words",
            "review": "This paper is unacceptable to me in current form.  I would find it acceptable if it were rewritten to be \"a much more straightforward description and demonstration of how a simple strategy is competitive with (unnecessarily) complex approaches on an important problem.\"\n\nI found this paper's layout and exposition frustrating.  There is a space for papers that demonstrate simple approaches to complicated problems are competitive (e.g., Hellinger PCA for word embeddings).  Such papers, when done well, 1) clearly indicate the importance of the problem, 2) propose a simple solution, and 3) show it is competitive.   Instead, this paper 1) defers important motivation (e.g., federated mobile/cloud computation) to the end of the experiment section, 2) disguises a simple solution as complex via excessively ornate discussion, and 3) mostly fails to show it is competitive.\n\nBag-of-words uses a one-hot vocabulary encoding of the input but trains directly on the task rather than indirectly via the classifier.  Whereas, \"word embedding\" co-trains with the classifier but uses word embeddings.  I'm really curious to see whether directly training on the class label using the word embedding representation with a sparsity-enforcing regularizer would do just as well as \"word embedding\" eliminating the need for co-training.  In any event, it would be more convincing baseline.\n\nI don't understand how the data aggregation step works when the selector and classifier are co-trained, since it appears to assume a collection of selectors is already present before training the classifier.\n\nNone of the experimental results are compelling to me.  The differences seem so slight that I would prefer the simplest approach, which is the L1 regularized logistic regression for feature selection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, more experiments needed",
            "review": "This paper proposes a framework for fast text classification that relies on a text selector and classifier that are jointly optimized using an aggregation scheme for the output of the text selector. This has an interesting application in cases where the text selector has to be implemented in a low performance device and the classification can be done on the cloud. The approach the authors take is interesting and different from what has done before, however I believe some further details / tests are needed to make this paper more convincing.\n\nIt doesn’t seem like the model has a clear advantage over existing methods. The advantage of the method could be brought up by comparing other complexity metrics such as how many words need to be transmitted (this is shown in figure 2 but not in relation to other approaches), what is a possible other advantage over the skim-RNNs etc. The interpretability of the results is encouraging and perhaps this could be explored more and leveraged as an advantage.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}