{
    "Decision": {
        "metareview": "The paper proposes a principled modeling framework to train a stochastic auto-encoder that is regularized with mutual information maximization. For unsupervised learning, this auto-encoder produces a hybrid continuous-discrete latent representation. While the authors' response and revision have partially addressed some of the raised concerns on the technical analyses, the experimental evaluations presented in the paper do not appear adequate to justify the advantages of the proposed method over previously proposed ones, and the clarity (in particular, notation) needs further improvement. The proposed framework and techniques are potentially of interest to the machine learning community, but the paper of its current form fells below the acceptance bar. The authors are encouraged to improve the clarify of the paper and provide more convincing experiments (e.g., on high-dimensional datasets beyond MNIST).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A principled modeling framework hindered by inadequate experiments and confusion notation"
    },
    "Reviews": [
        {
            "title": "Idea is promising and the derivation of the loss is informative but the evaluation seems insufficient.",
            "review": "Summary: the paper proposes a method for unsupervised disentangling of both discrete and continuous factors of variation in image data. It uses an autoencoder learned by optimising an additive loss composed of Mutual Information (MI) I(x;y,z) between the image x and the discrete+cts latents (y,z) and the reconstruction error. The mutual information is shown to decompose into I(x,y), I(x,z) and TC(y;z), and the I(x,z) is treated in a different manner to I(x,y). With Gaussian p(z|x), and it is shown that I(x,z_k) is maximal when p(z_k) is Gaussian. So KL(p(z_k)||N(0,1)) is optimised in lieu of optimising I(x,z), and I(x,y) (and TC(y;z)) is optimised by using mini-batch estimates of marginal distributions of y (and z). The paper claims improved disentangling of discrete and continuous latents compared to methods such as JointVAE and InfoVAE.\n\nPros:\n- The derivation of the loss shows a nice link between Mutual information and total correlation in the latents.\n- It is a sensible idea to treat the MI terms of the discrete latents differently to the continuous latents\n- The mathematical and quantitative analysis of MI and its relation to decoder means and variances are informative.\n\nCons:\n- There is not enough quantitative comparison of the quality of disentanglement across the different methods. The only values for this are the accuracy scores of the discrete factor, but for the continuous latents there are only qualitative latent traversals of single models, and I think these aren’t enough for comparing different disentangling methods - this is too prone to cherry-picking. I think it’s definitely necessary to report some metrics for disentangling that are averaged across multiple models trained with different random seeds. I understand that there are no ground truth cts factors for Mnist/FashionMnist, but this makes me think that a dataset such as dSprites (aka 2D Shapes) where the factors are known and has a mix of discrete and continuous factors would have been more suitable. Here you can use various metrics proposed in Eastwood et al, Kim et al, Chen et al for a quantitative comparison of the disentangled representations.\n- In figure 4, it says beta=lamda=5 for all models. Shouldn’t you be doing a hyperparameter sweep for each model and choose the best value of hyperparameters for each? It could well be that beta=5 works best for IMAE but other values of beta/lambda can work better for the other models.\n- When comparing against JointVAE, the authors point out that the accuracy for JointVAE is worse than that of IMAE, a sign of overfitting. You also say that VAT helps maintain local smoothness so as to prevent overfitting. Then shouldn’t you also be comparing against JointVAE + VAT? Looking at Appendix D, it seems like VAT makes a big difference in terms of I(y;y_true), so I’m guessing it will also have a big impact on the accuracy. Thus JointVAE + VAT might beat IMAE in terms of accuracy as well, at which point it will be hard to argue that IMAE is superior in learning the discrete factor.\n- In the first paragraph of Section 4, the authors claim results on CelebA, but these are missing from the paper. Testing the approach on datasets more complex than (Fashion)Mnist would have been desirable.\n- There aren’t any latent traversals for the discrete latents - this would be a useful visualisation to complement the accuracy plots in Figure 3.\n\nQs and comments:\n- It’s not clear why posterior approximation quality (used as a starting point for motivating the loss) is an important quantity for disentangling.\n- I see that the upper bound to I(x;z_k) in (4) and the objective in (6) have the same optimum at p(z_k) being Gaussian, but it’s not clear that increasing one leads to increasing the other. Using (6) to replace (4) seems to require further justification, whether it be mathematical or empirical.\n- In proposition 2, I’m sceptical as to how meaningful the derived bound is, especially when you set N to be the size of the minibatch (B) in practice. It also seems that for small delta (i.e. to ensure high probability on the bound) and large K_2 (less restrictive conditions on p(y) and \\hat{p}(y)), the bound can be quite big.\n- \\mathcal{L}_theta(y) in equation (10) hasn’t been introduced yet.\n- The z dimension indices in the latent traversal plots of Figure 2 don’t seem to match the x-axis of the left figure. It’s not clear which are the estimates of I(x;z_k) for k=8,3,1 in the figure.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Principled framework for auto-encoding",
            "review": "* This paper proposed a principled framework for auto-encoding through information maximization. A novel contribution of this paper is to introduce a hybrid continuous-discrete representation. The authors also related this approach with other related work such as \\beta-VAE and info-VAE, putting their work in context. Empirical results show that the learned representation has better trade-off among interpretability and decoding quality.\n\n* It seems a little strange to me to incorporate the VAT regularization to the IMAE framework in Section 4.2, as this is not included in the overall objective in Equation (10) and earlier analysis (Proposition 1 and 2). Will the conclusions in Proposition 1 and 2 change accordingly due to the inclusion of VAT regularization?\n\n* The paper states that IMAE has better trade-off among interpretability and decoding quality. But it is still unclear how a user can choose a good trade-off according to different applications. More discussion along this direction would be helpful.\n\n* I guess the L(y) term in Equation (10) is from Equation (9), but this is not stated explicitly in the paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not Compelling.",
            "review": "This paper proposes an objective function for auto-encoding they\ncall information maximizing auto encoding (IMAE).  To set the stage\nfor my review I will start with the following \"classical\" formulation\nof auto-encoding as the minimization of the following where we are\ntraining models for P(z|x) and P(x|z).\n\nbeta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1)\n\nHere H(z) is defined by drawing x from the population and then drawing\nz from P(z|x).  This is equivalent to classical rate-distortion coding\nwhen P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just\nthe L2 distortion between x and its reconstruction.  The parameter\nbeta controls the trade-off between the compression rate and the L2\ndistortion.\n\nThis paper replaces minimizing (1) with maximizing\n\nbeta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2)\n\nThis is equivalent to replacing H(z) in (1) by -I(x,z).  But (2)\nadmits a trivial solution of z=x.  To prevent the trivial solution this\npaper proposes to regularize P(z) toward a\ndesired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z))\nby minimizing\n\nbeta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3)\n\nThe paper contains an argument that this replacement is reasonable\nwhen Q(z) and P(z|x) are both Gaussian with diagonal covariances.  I\ndid not verify that argument but in any case it seems (3) is better than (2). \nFor beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value\nH(Q).  The regularization probably has other benefits.\n\nBut these suggestions are fairly simple and any real assessment of their\nvalue must be done empirically.  The papers experiments with MNIST\nseem insufficient for this.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}