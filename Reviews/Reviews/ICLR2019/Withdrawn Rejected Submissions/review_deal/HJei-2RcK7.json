{
    "Decision": {
        "metareview": "The reviewers all agree that the work is interesting, but none have stood out and championed the paper as exceptional. The reviewers note that the paper is well-written, contributes a methodological innovation, and provides compelling experiments. However, given the reviewers' positive but unenthusiastic scores, and after discussion with PCs, this paper does not meet the bar for acceptance into ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reviewers agree that work is interesting, but reviews are borderline"
    },
    "Reviews": [
        {
            "title": "This paper proposes an intereting method for graph dataset. However,  some points need to be verified.",
            "review": "This paper proposes a graph transformer method to learn features from the data with a graph structure. Actually it is the extension of Transformer network to the graph data. Although it is not very novel, yet it is interesting.  The experimental result has confirmed the author's claim.\n\nI have some concerns as follows:\n1. For the sequence input, this paper proposes to use the positional encoding as the standard Transformer network. However, for graphs, edges have encoded the relative position information. Is it necessary to incorporate this positional encoding? It's encouraged to conduct some experiments to verify it.\n\n2. It is well known that graph neural networks usually have large memory overhead. How about this model? I found that the dataset used in this paper is not large. Can you conduct some experiments on large-scale datasets and show the memory overhead?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Useful but straightforward idea",
            "review": "Summary\n========\nThe  paper  adopts  the  self-attention  mechanism  in Transformer and in message-passing graph neural networks to derive  graph-to-graph mapping. Tested on few-shot learning, medical imaging classification and graph classification problems, the proposed methods show competitive performance. \n\nComment\n========\nGraph-to-graph mapping is an interesting setting and the paper presents an useful solution and interesting applications.  The paper is easy to read.\n\nHowever, given the recent advancements in self-attention and message-passing graph modeling under various supervised settings (graph2vec, graph2set, graph2seq and graph2graph), the methodological novelty is somewhat limited. The idea of intra-graph and inter-graph message passing, for example, has been studied in:\nDo et al. \"Attentional Multilabel Learning over Graphs-A message passing approach.\" arXiv preprint arXiv:1804.00293 (2018).\n\nComputationally, the current solution is not very scalable for large input and output graphs.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting bridge paper between knowledge-based and learning approaches with good synergy",
            "review": "I am familiar with the few-shot learning literature and this particular approach is novel as far as I know. A weight generator outputs the last layer of an otherwise pre-trained model. A combination of attention over base category weights and graph neural networks is used to parametrize the generator. Results are particularly good on 1-shot miniImageNet classification, but may not be entirely comparable with previous work. Two more interesting experiments are given and have convincingly superior results (at first glance) but I am not familiar with those domains. \n\nI still think the questions in my previous post need answers! I am willing to improve my score if clarifications are added to the paper.\n\nOverall, the paper makes a convincing point that hand-engineered graphs and knowledge can be effectively used with learning methods, even in challenging few-shot settings.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}