{
    "Decision": "",
    "Reviews": [
        {
            "title": "Unclear if the approach will give practical benefits",
            "review": "\nThis paper proposes an algorithm for data compression which learns\nthe most representative samples of a dataset by solving a mixed\ninteger program. The paper then finds a stationary point of the\nprogram by a greedy method. Empirical results show that compressing\na large fraction of samples using the method does not cause a\nnoticeable drop in learning performance.\n\nThe algorithm and theory in this paper are clearly presented. The\nidea however is not quite novel. The impact of reducing samples on\nlearning problems have been studied in literature, e.g. [1][2][3].\nAlgorithms like sketching [2] and importance sampling [3] have been\nshown both in theory and practice that can reduce dataset sample\nsize without a big drop in performance. Besides, both [2][3]\nrequires less computation time than solving the original problem.\nHowever, the proposed algorithm takes more computation than\nminimizing the loss.\n\nSome applications of the proposed algorithm are presented in\nappendix B. However, it would be better to also show the learning\nor optimization problems in these applications since they can help\nus understand how to apply the proposed algorithm in practice. Some\nexperiments on these applications are also preferred.\n\nFor experiments, both synthetic data and large-scale real data are\ntested. However, it would be better comparing with other algorithms\nlike [1][2][3] to show the novelty of the proposed algorithms. In\nsection 4.2, the synthetic data experiments are conducted using\nonly one constructed dataset and multiple random noise. Generating\na random dataset each time will show that the performance of the\nalgorithm is not a special case for some specific datasets and will\nalso give insights on the robustness of the algorithm. The proposed\nmethod can be applied to both regression and classification\nproblems. Some experiments on classification would help better\nexploring this algorithm. The experiments on reduced sample size\nand learning performance are comprehensive. However, I would like\nto see experiments on running time, since running time represents\nthe key performance metric in real world applications.\n\nSome questions: In Section 2.4: Please explain what robust learning\nin your case is. In your experiments, how you choose your parameter\nepsilon? What is the average loss in your experiments? Average loss\nis more commonly used in practice. In your experiments, what if you\npick these selected samples and train them? Data compression is\nalso well studied in learning theory [4] (Part IV, Section 30). It\nis also interesting to know if the proposed algorithm can reach the\ntheoretical bound.\n\n[1] Understanding Black-box Predictions via Influence Functions\n[2] sketching as a tool for numerical linear algebra\n[3] Randomized algorithms for matrices and data\n[4] understanding machine learning from theory to algorithms",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Dataset selection and compression: a work done without much motivation, in isolation, without an interesting theoretical conclusion.",
            "review": "The paper addresses the problem of selecting a subset of a labeled dataset that retains the performance of training on the full dataset. The techniques presented in the paper are able to select just 5% of the dataset with only a small loss in error.\n\nThe paper has several severe issues with it: motivation, related work, and theoretical novelty.\n\nFirst of all, the problem setting is somewhat strange. In active learning, it is natural to select a subset of the dataset to label, for when the lableling cost is high. When there is too much labeled data, it is natural to select a subset for computational reasons (e.g. core-set selection). However, this work doesn't save on labeling cost or on computational costs. Instead, the paper claims the application is distributed learning where communication costs are expensive (and presumably where computation is not limited on the agents, which is unrealistic in some of the applications the paper mentions). However, the paper doesn't really address this problem in the experiments, algorithm, or theory they just mention it off-hand as an application. Why restrict the communication protocol to datasets rather than say, model parameters? How would \"sufficient datasets\" be combined and how would this change things?\n\nPerhaps an even larger issue is the lack of comparison to other methods. This paper doesn't have a related work section or compare to any baselines (not even random sampling). As examples, core-set selection and machine teaching are very similar setups, but these aren't mentioned anywhere. This glaringly shows up in the experiments section when only results for the algorithm presented are shown.\n\nFinally, from a theoretical perspective, the conceptual/theoretical novelty that datasets can be compressed sub-linearly for a given model is not really that interesting. Intuitively, once a model class has seen enough data, it won't be able to learn much more from further data. From theory, we expect that for a given model trained on a given dataset, there will be approximation error and estimation error. The approximation error is fixed (for the model family) while the estimation error typically goes down as 1/n or 1/sqrt{n}. For such large datasets and low-dimensional data, I do not find it very surprising that there is only a small decrease in estimation error. In fact, I wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar). Unfortunately, the paper does not compare to a random sampling baseline. \n\nAs another point, for their \"novel problem\" (2a)-(2c), I think (2c) doesn't make much sense. Why require the dataset to be of a certain size if the generalization error is already required to be low? In fact, this constraint is in the opposite direction of what we want, I could see trying to minimize the size of the dataset. I suspect that this constraint is only to make the optimization technique presented in this paper work.\n\nIn summary, this paper solves an unmotivated problem, in isolation, with a theoretically uninteresting conclusion.\n\nSmall points:\nf: X |-> Y should be f: X -> Y. \nThe natural numbers (\\mathbb{N}) are already positive so the + is unnecessary. Did you mean \\mathbb{Z}^+?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper is written reasonably clearly, and appears to be original.\n\nThis paper proposes a coupled optimization for simultaneously compressing a dataset (picking an informative subset) and learning. My major concern is that the problem itself is not formulated (nor solved) properly.\n\nThis is most easily uncovered by looking at the solution of the optimization given an oracle that provides h*. The algorithm would then pick z* by simply choosing the K lowest objective function values (indeed this is one of the block-coordinate steps). These are simply the data that the single candidate function h* fits best, and they may be very close / redundant with one another. There is no step in the procedure that forces the subset to capture the learning problem itself.\n\nFurther, each \"F-step\" requires solving a feasibility problem that is just as hard as solving the original learning problem on the full dataset. Even if I set aside that concern, there is still the matter of the arbitrary threshold epsilon, which the paper provides no guidance on. Indeed the F-step may not even be feasible if epsilon is set incorrectly. The authors incorrectly state the solution to the F-step for linear regression (for the same reason).\n\nI am also not certain the main result (proposition 4 / corollary 1) provides any useful theoretical result. In particular, for K = 1 (given an appropriate choice of epsilon) it's not hard to pick an h* such that g(h*, z*) = 0 for all N. It's also very odd that the proposition requires a lower bound on K, when increasing K just increases the optimal objective g(h*, z*) -- having a lower bound seems to suggest increasing K should result in a better objective g. The proposition also provides no constraint / assumption on epsilon, which is also odd.\n\nFinally, the paper does not compare its method to any other compression scheme -- even random subsampling. The experimental results on compression are meaningless without a baseline to say how easily compressed these datasets are.\n\nIn addition to the above major concerns, the paper does not cite any papers from the vast body of literature on coresets and model compression.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}