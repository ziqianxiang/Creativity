{
    "Decision": {
        "metareview": "This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non-zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A study on sparse properties of L1-regularization in deep neural networks, yet experimental supports seem week."
    },
    "Reviews": [
        {
            "title": "Repeating the old story from other papers, quit limited novelty, lacking solid experiments",
            "review": "The main concerns come from the following parts:\n\n\n(1) Repeating the old story from other papers:\nA large part of math is from previous works, which seems not enough for the ICLR conference.\nIt is very surprising that the authors totally ignore the latest improvements in neural network compression. Their approach is extremely far away from the state of the art in terms of both methodological excellence and experimental results. The authors should read through at least some of the papers I list below, differentiate their approach from these pioneer works, and properly justify their position within the literature. They also need to show a clear improvement on all these existing pieces of work. \n\n(2) quite limited novelty:\nIn my opinion, the core contribution is replacing SGD with Adam.\nFor network compression, it is common to add L1 Penalty to loss function. The main difference of this paper is change SGD to Adam, which seems not enough. \n\n(3) lacking solid experiments:\nIn section Experiment, the authors claim \"Finally, we show the trade-off for pruning Resnet-50 on the ILSVRC dataset.\", but I cannot find the results. \n\nIs the ResNet-32 too complex for cifar-10? Of course, it can be easily pruned if the model is too much capacity for a simple dataset.  Why not try the Resnet-20 first?\n\n[1] C. Louizos et al., Bayesian Compression for Deep Learning, NIPS, 2017\n[2] J. Achterhold et al., Variational Network Quantization, ICLR, 2018",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "an interesting perspective on the L1 regularization of neural network",
            "review": "This paper discusses the effect of L1 penalization for deep neural network. In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. \n\nThe perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. If the coefficients of the linear equation are distributed in general positions, then the number of variables should not be larger than the number of equations. \n\nWhile I mostly like the paper, I would like to point out some possible issues:\n\nmain concerns: \n\n1. the columns of V may not be independent during the optimization(training) process. In this situation, I am not quite sure if the assumption of “general position” still holds. I understand that in literatures of Lasso and sparse coding it is common to assume “general position”. But in those problems the coefficient matrix is not Jacobian from a learning procedure. \n\n2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. It is against the empirical observation that when lambda is extremely small, effect of the regularizer tends to be almost zero. Can authors also show this effects empirically, i.e., when the regularization coefficients decrease, the nnz does not vary much? (Maybe there is some optimization details or approximations I missed?)\n\nSome minor notation issues:\n1. in theorem 1: dim(W^{(j)})=d should be dim(vec(W^{(j)}))=d\n2. in theorem 1: Even though I understand what you are trying to say, I would suggest we describe the jacobian matrix V in details. Especially it is confusing to stack vec(X^J) (vec(W^j)) in the description.\n3. the notations of subgradient and gradient are used without claim\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice Theoretical Insights, but Not Sure How Experiments  Substantiate the Theory",
            "review": "The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance. Specific comments follow.\n\n1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). Experimental results of optimizing (5) seem missing. While the reviewer understands that (5) and (8)  are closely related, and the theoretical insights for (5) can potentially translate to the scenario in (8), the reviewer is not sure whether the theory for (5)  is rigorously justified by the experiments.\n\n2. It is also unclear how tight the bound provided by Theorem 1 is.  Is the bound vacuous? Relevant statistics in the experiments might need to be reported to elucidate this point.\n\n3. It is also unclear how the trade-off in point (b) of the abstract is justified in the experiments.\n\nMinor Points:\npage 2, the definition of $X^{(j)}$, the index of $l$ and $j$ seem to be typos.\npage 2, definition 1, the definition of the bracket need to be specified. \npage 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability.\npage 4, Corollary 1, should it be $nnz(\\hat{W})\\le JN k_{\\mathcal{S}}$?\npage 7, Table 2, FLOPS should be FLOP? \npage 8, is FLOP related to the time/speed needed for compression? If so, it should be specified. If not, compression runtime should also be reported.\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}