{
    "Decision": {
        "metareview": "In this paper, neural networks are taken a step further by increasing their biological likeliness.  In particular, a model of the membranes of biological cells are used computationally to train a neural network.  The results are validated on MNIST.\n\nThe paper argumentation is not easy to follow, and all reviewers agree that the text needs to be improved.  ˜The neuroscience sources that the models are based on are possibly outdated.  Finally, the results are too meagre and, in the end, not well compared with competing approaches.\n\nAll in all, the merit of this approach is not fully demonstrated, and further work seems to be needed to clarify this.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "early work with possible merit"
    },
    "Reviews": [
        {
            "title": "Not understandable",
            "review": "Thanks for submitting your paper. It takes a lot of effort and courage to put your ideas out into the world. Sometimes the hardest work for researchers is conveying their thoughts to others in a manner in which those ideas can be understood.\n\nWith that in mind, I had an extremely difficult time following your arguments. \n\nI noticed several things:\n - There are numerous places in the text that lack proper citation, or are cited improperly.\n\n - Why was there not a related methods section? I find it hard to believe that all of your ideas have no precursor.\n\n - When there are citations, there is usually only one text and it is quite old. For example, all of your neuroscience citations reference a work that is almost 40 years old. There have been quite a few improvements in our biological understanding as well as theoretical understanding since then. ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology. )\n\n- There is a claim regarding how this can be used in fintech. This statement doesn't belong in this work.\n\n - There are many different equations given throughout the text. Some of these equations come from areas like physics or information theory, and others seem to be of your own design. Regarding the latter, there is no justification or explanation for the origin of the equations. Regarding the former, if you are using equations from lots of different fields, or even field you think part of your audience might not be familiar with, you should, at the very least, include a some description of the algorithm or intuition as to why it is being leveraged.\n\n - It wasn't clear from your diagrams or your descriptions what the difference between a synapse and a neuron was in your architecture. It seemed like the name was used interchangeably in some areas, but then had a strict definition in others.\n\n - I was also not able to understand how the excitatory and the inhibitory connections that were to enter each neuron were connected to the previous layer of the network. Is a link between neurons in Figure 2 actually two links? If this is the case, then it is a direct violation of Dale's law. Again, I only mention this because most arguments seem to be of the form \"this is correct because it is how it is done biologically\".\n\n - There were a few claims made in the paper that were completely unsubstantiated. A good example of this was in the conclusion section part ii) where it was stated that \"using a large number of synapses and neurons SynaNN can solve the complex problems in the real world.\"\n\n- Also, the last sentence of the conclusion was not discussed anywhere in the rest of the paper. Nor was the statement itself supported except with a single citation and no description.\n\nRegarding the empirical testing of your algorithm, I was very dissapointed to see that the only dataset it was tested against was MNIST. Furthermore there was absolutely no benchmarking against other comparative algorithms. At the very least I would have expected a comparison to the perceptron algorithm that you use as inspiration, but that would also still not have been enough.\n\nThis paper needs heavy amounts of work to make it understandable. Once it is understandable an attempt to evaluate the merit of the scientific contribution would then be possible.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not clear if this is -- or will be -- a practically useful approach",
            "review": "The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST.\n\nI am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And it’s not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful.\n\nMoreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper is not presented with sufficient clarity for ICLR publication",
            "review": "The authors present a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells.  Unfortunately, the paper is structured incoherently, making it nearly impossible to appreciate the authors' contribution.  The introduction references neuroscience alongside ResNets, FinTech, surprisal spaces, Bose-Einstein statistics, and topological conjugacy without adequately motivating or defining any of the above.  The fundamental definition of the model synapse as a conditional probability (Eq. 1) is not guaranteed to be non-negative, casting serious doubt on any of the subsequent conclusions.   Figure 1 conveys no further information about the proposed model.  There is no explicit related work or background section.  The single experiment offers no comparison to alternative methods.   I suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologically-inspired learning models.  This is indeed a subfield of machine learning worthy of more investigation. ",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Complicated math, terminology and network to obtain standard MLP performance",
            "review": "Quality - poor\nThe highly complicated work is evaluated only on the simplest of benchmarks with no significant results. \n\nClarity - poor\nThe paper seems to amount to gobbledygook, many disparate terminology strung together. \n\nOriginality\nNo idea. \n\nSignificance \nNone. \n\ncons: the paper to me seems a hashing of citations to the main works in neuroscience and deep learning for which only the simplest network is demonstrated (single hidden layer MLP on MNIST) with results that do not exceed that of a standard MLP.\npros: the only pro I can think of for this work is that synaptic computing imo deserves more consideration, as real synapses are very complicated beasts, the functioning of which relatively little is known about. ",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}