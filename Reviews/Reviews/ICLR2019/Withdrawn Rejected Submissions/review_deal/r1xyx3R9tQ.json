{
    "Decision": {
        "metareview": "This paper considers \"prototypes\" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies.\n\nThe reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper.\n\nAlthough the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real-world applications.\n\nFor these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Make definitions more precise"
    },
    "Reviews": [
        {
            "title": "Unjustified heuristics, unclear if prototypes are useful, unconvincing experiments",
            "review": "Summary: The paper proposes methods for identifying prototypes. Unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means. The experiments are not very convincing, and often present results that are either inconclusive or negative, i.e. seem to demonstrate that prototypes are not very useful. \n\nPros:\n- The notion of prototypes is used in various papers, but a formal definition is lacking, and the usefulness of prototypes is not demonstrated. The fact that this paper sets out to do both is laudable, although the paper needs work before it can be accepted for publications.\n\nDetailed comments / cons:\n*Defining prototypes: \n  - The authors list desirable properties before defining (even informally) what a prototype is, and what its purposes are. Taking the first property as an example, is it reasonable to expect a metric for prototypes to be useful for image classification AND image generation? The answer completely depends on what one expects from a prototype, what its purpose is, etc.\n  - The second property seems to indicate that prototypes are model-independent, i.e. two models trained on the same dataset will have the same prototypes. This is confusing as the metrics proposed are clearly model-dependent (e.g. adv completely depends on the trained model's decision boundary, conf obviously depends on the model providing the confidence score)\n- The third and fourth property are poorly defined. Human intuition presupposes that humans agree on what a prototype means. Using 'modes of prototypical examples' in trying to define a metric for prototypes is circular, as a mode of prototypical example depends on a working notion of prototypical examples.\n- The last property is completely dependent on which models are trained, and how they are trained. If a model has high label complexity, maybe it does not achieve high accuracy even when trained on high quality prototypes. In any case, this property is at odds with the first two properties.\n\nIn sum: it's not clear what prototypes are, so it becomes hard to judge if the list of desiderata is reasonable. The list is in any case ill-defined, and contains contradictions.\n\n* Metrics for prototypicality\n- The second paragraph in this section is unnecessary\n- All of the metrics proposed are heuristics with little to no justification. Specific comments below.\n- Adversarial robustness is a property of a trained model, not of prototypical examples, unless prototypes are supposed to be model dependent (contra property 1). In any case, it is not clear why examples that are robust to adversarial noise are good 'prototypes'.  Using facial recognition as an example, a 'mean face' may be very robust to adversarial noise but not prototypical at all under common definitions. A face with a particular type of facial hair (e.g. nose hair) may be very representative of a class of faces (i.e. a prototype), but very susceptible to adversarial noise. In fact, any examples in the boundary of the decision function will be more susceptible to adversaries, but that does not make them 'less prototypical'.\n- Holdout retraining is again completely model dependent. Why should we expect a model to treat a prototype the same regardless of whether or not it is trained on it? This basically means that we expect the model to always be accurate on prototypes.\n- Ensemble agreement proposes a notion of prototypes that is based on prediction 'hardness'. It is clear that such a notion depends completely on which models are being considered, which features are being used, and etc, much more than on notions of prototypicality inherent in the data. The same criticism applies to model confidence.\n- Privacy preserving training assumes prototypicality has to do with the model being able to learn with some robustness to noise (related to Adversarial Robustness, but different). This assumes a definition of prototypes that is not congruent with the other metrics.\n\nIn sum: the proposed metrics are basically heuristics with little justification, and different metrics assume different notions of what a prototype is.\n\n* Evaluation\n- Section 3.1 claims that the metrics are strongly correlated, but that is not true for MNIST or CIFAR, and is somewhat true for fashion-mnist. In any case, since the metrics are so model-dependent, it is not clear if these results would hold if other models were used.\n- Section 3.2 - The question asked of turkers in the study is too vague, and borderline irrelevant for the task at hand - what does the 'best image' of an airplane mean, and how does this translate to it being a prototype? All that the study demonstrates is that the proposed metrics score malformed images with low score. The results in Table 1 are very spread out, and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users.\n- The results in Section 4 are very discouraging: sometimes it is better to train on most prototypical examples according to the metrics, sometimes it is worse, sometimes it's better to take examples in the middle. That is, prototypes don't seem to help at all. 'Prototype percentile' is uncorrelated with robustness for MNIST in Appendix E, while being correlated for other datasets. It is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected). As a whole, the results do not provide any help for a user who wants to produce a more robust model, other than 'ignore prototypes and use the whole dataset'.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting attempt at understanding prototypes but needs more work",
            "review": "Summary: This paper attempts to better understand the notion of prototypes and in some sense create a taxonomy for characterizing various prototypicality metrics. While the idea of thinking about such a taxonomy is novel, I think the paper falls in clearly justifying certain design choices such as why are the properties outlined at the beginning of Section 2 desirable. I also felt that the paper is resorting to rather informal ways of describing various properties and metrics without precisely quantifying them. \n\nPros:\n1. Novel attempt at understanding prototypes. Two specific contributions: a) outlining the properties desirable in prototypicality metrics b) proposing new prototypicality metrics and demonstrating the relevance of the various prototypicality metrics. \n2. Detailed experimental analysis along with some user studies\n\nCons:\n1. An important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained. There is often a purpose associated with identifying prototypes - are we summarizing a dataset? are we thinking about helping humans understand the behavior of a specific learning model? Answers to these questions guide the process of choosing prototypes. However, this paper seems to approach the problem of choosing prototypes via the \"one approach fits all\" strategy which I am not sure is even possible. \n2. The choice of desirable properties is not clearly justified (Beginning of Section 2). For instance, why should prototypes be independent of learning tasks? \n3. Lack of rigor in defining prototypicality metrics as well as properties in Section 2. For example, wouldn't it be possible to theoretically prove that the metrics outlined in Section 2 satisfy the desired properties? \n\nDetailed Comments: \n1. I would strongly encourage the authors to illustrate using examples in the introduction the significance of finding prototypes. What are the end goals for which these prototypes would be used? Why do you think the metric for chooosing prototypes should be independent of the learning task or model? \n2. Along the same lines as the comment above, please provide detailed justifications for the list of properties provided in the beginning of Section 2. It would be even better if you could formalize these a bit more.\n3. Would it be possible to theoretically show that the metrics defined in Section 2 satisfy any of the desirable properties highlighted in Section 2? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Prototypical Examples on Small Datasets",
            "review": "## Strength\n\nThis paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. \n\n## Weakness\n\n### Not practical\n\nThe authors report that “removing individual training examples did not have a measurable impact on model performance”. However, this seems not to be supported by experiments.\nFirst, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? \nSecond, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data?\nThird, with `\"adv\" metric, we need to perform adversarial-example attacks before training, which has little value in practice. \n\n### Datasets\n\nThey only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. \n\n## Most confusing typos\n\n1. Section 4, paragraph 5, \"However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples.\" Is there a missing \"than\"? It's confused.\n2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}