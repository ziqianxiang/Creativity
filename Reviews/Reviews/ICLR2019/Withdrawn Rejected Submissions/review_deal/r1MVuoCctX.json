{
    "Decision": "",
    "Reviews": [
        {
            "title": "Problems of clarity , novelty and significance",
            "review": "The paper proposes an architecture for language modeling composed of a bigger and smaller LSTM. The authors propose to add skip connections directly from the input layer to the layers of the \"major\" lstm. The skip-connections are computed using the small LSTM. The authors claim to reach better performance with smaller number of parameters in two language modeling tasks (PTB and WT2).\n\nI had a hard time reading and understanding the paper. The clarity and style of the writing should be improved to better stress the significance of the underlying motivation. Increasing the efficiency of learning algorithms in terms of the number of parameters is clearly an important endeavour. However, the novelty of the architecture (skip-connections are well-known in the context of language modeling, e.g. , Melis et al. (2017) for a recent account) as well as the reduction in the number of overall parameters are limited enough to make the marginal gains observed in the reported results insufficient for publication.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More efficient spending of the parameters in LSTM language models",
            "review": "The paper suggests more efficient spending of the parameters in LSTM language models: instead of increasing the hidden layer size of the LSTM, the authors suggest building a small assistant LSTM and its output is concatenated with the output of the main LSTM.\n\nThe approach reminds me the SCRN Model of Mikolov et al. (2015). I don't claim it is the same, as you have an independent LSTM, but there are some similarities: in both cases an assisting small layer is added, and in both cases one needs to choose an appropriate size of this assisting layer. It would be interesting to see if your Minor LSTM plays the same role as the context state in SCRN, e.g. does the hidden state in Minor LSTM change slower than that of the Major LSTM? If yes, then we could say that the Minor LSTM focuses more on what is being talked about (context).\n\nExperimental evaluation of the idea seems adequate. However, my feeling is that the recent optimization and regularization techniques (such as those used in AWD-LSTM) may hide the effect of your idea. If possible, could you please evaluate your approach under less recent regularization, e.g. variational dropout with tied weights as in Inan et al. (2016).\n\nMinor comment on \"more parameters means better model performance\" in the abstract: If by model performance we understand performance on held-out data, then I would say that \"more parameters means more flexible model\".\n\nNote after reading other reviews: I agree with Reviewer1 and Reviewer2 that the use of skip-connections (Melis et al., 2017; Zilly et al., 2016) is one of the main reasons why the model achieves its performance (Table 5). Without skip-connections the proposed Major-Minor LSTM is practically on par with the AWD-LSTM-MoS of Yang et al. (2017) on PTB and only 0.57 perplexity units better on WT-2.\n\nReferences\n- Inan, H., Khosravi, K. and Socher, R., 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462.\n- Melis, G., Dyer, C. and Blunsom, P., 2017. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589.\n- Mikolov, T., Joulin, A., Chopra, S., Mathieu, M. and Ranzato, M.A., 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.\n- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv preprint arXiv:1711.03953.\n- Zilly, J.G., Srivastava, R.K., Koutn√≠k, J. and Schmidhuber, J., 2016. Recurrent highway networks. arXiv preprint arXiv:1607.03474.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Missing comparison - what about highway/skip connections?",
            "review": "The authors introduce an language modeling architecture that introduces so-called \"minor-LSTMs\" computed directly on top of the input in addition to each network layers input. This approach seems hardly novel. considering approaches like skip/highway connections, e.g. (Zilly et al., 2016, https://arxiv.org/abs/1607.03474). Even if the proposed approach varies this theme in some way, a comparison to these strongly related approaches would be due.\n\nAlso, the presented approach could also be viewed as a kind of factorization, cf. (Kuchaiev et al., 2017, https://arxiv.org/abs/1703.10722).\n\nSec. 3.3: it remains totally unclear to me, how the authors reach the recursive equations (9) and (10) and how they relate to Eq. (8) mention before.\n\nIn the results tables on p. 6, at some point, perplexity numbers are given with a second position after the decimal point. I strongly doubt that these differences in perplexity are significant, especially on these small tasks.\n\nThe authors should take more care w.r.t. the notation, which is not consistent throughout the paper (e.g. x_j vs. x_t, y_j vs. y_t) and should be introduced properly.\n\nFinally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}