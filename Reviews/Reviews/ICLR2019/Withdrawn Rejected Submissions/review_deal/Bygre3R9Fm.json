{
    "Decision": {
        "metareview": "Since the reviewers unanimously recommended rejecting this paper, I am also recommending against publication. The paper considers an interesting problem and expresses some interesting modeling ideas. However, I concur with the reviewers that a more extensive and convincing set of experiments would be important to add. Especially important would be more experiments with simple extensions of previous approaches and much simpler models designed to solve one of the tasks directly, even if it is in an ad hoc way. If we assume that we only care about results, we should first make sure these particular benchmarks are difficult (this should not be too hard to establish more convincingly if it is true) and that obvious things to try do not work well.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "the reviewers are unanimous"
    },
    "Reviews": [
        {
            "title": "The paper is very poor--- not ready for publication",
            "review": "The paper proposes a conditional graph generation that directly optimizes the properties of the graph. The paper is very weak.\n1. I think almost all probabilistic graph generative models are differentiable. If the  objective is differentiable function of real   \n    variables, it is usually differentiable.\n\n2.  The authors claim that existing works Simonovsky and Komodakis (2018) and Cao & Kipf (2018) are restricted to use small graphs with predefined maximum size. This work does not overcome the limitation of small graphs issue too.\n\n3. The authors do not show any measure on validity, novelty or uniqueness which are now standard in literature.\n   Also I do not find any comparison with molGAN paper which tackles a similar objective.\n\n4. Could the authors show if the decoding process is permutation invariant? I am not really sure of that. I was trying to prove that thing formally, but I failed.\n\n\n ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review on \"DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation\"",
            "review": "This paper proposed a variant of the graph variational autoencoder [1] to do generative modeling of graphs. The author introduced an additional conditional variable (e.g., property value) into the decoder. By backpropagating through the discriminator, the model is able to find the graph with desired property value. \n\nOverall the paper reads well and is easy to follow. The conditional generation of graphs seems also helpful regarding the empirical performance. However, there are several concerns regarding the paper:\n\n1) The edge factorization-based modeling is not new. In fact [1] already uses the node embeddings to factorize the adjacency matrix. This paper models extra information including node tags and edge types, but these are not fundamental differences compared to [1].\n\n2) The paper claims the method is ‘cheaper’ and ‘scalable’. Since essentially the computation cost is similar to [1] which requires at least O(n^2) to generate a graph with n nodes, I’m not super confident about the author’s claim. Though this can be parallelized, but the memory cost is still in this order of magnitude, which might be too much for a sparse graph. Also there’s no large graph generative modeling experiments available.\n\n3) Continue with 2), the adjacency matrix of a large graph (e.g., graph with more than 1k nodes) doesn’t have to be low rank. So modeling with factorization (with typically ~256 embedding size) may not be suitable in this case. \n\nSome minor comments:\n4) Regarding Eq (2), why the lstm is used, instead of some simple order invariant aggregation?\n\n5) the paper needs more refinement. E.g., in the middle of page 2 there is a missing citation. \n\n[1]  Kipf & Welling, Variational Graph Auto-Encoders, https://arxiv.org/pdf/1611.07308.pdf\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiments and Writing Need Improvement",
            "review": "In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.\n\nStrength:\n\n1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. \n\n2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.\n\nWeakness:\n\n1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.\n\n2, Authors motive their work by saying in the abstract that “other graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters”. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.\n\n3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.\n\n4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?\n\n5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.\n\nTypos:\n\n1, There are a few references missing (question mark) in the first and second paragraphs of section 2.\n\n2, Methods in the experiment section are given without explicit reference, like GCPN.\n\n3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. \n\nOverall, I do not think this paper is ready for publishing and it could be improved significantly.\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nUpdate:\n\nThanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. \n\nHowever, I still found some claims made by authors problematic. \nFor example, it reads in the abstract that \"...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters...\". \nClearly, Li et al. 2018b has a differentiable formulation which falls under your description.\n\nBesides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. \nAlso, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.\nDirectly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.\n\nTherefore, I would like to keep my original rating. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}