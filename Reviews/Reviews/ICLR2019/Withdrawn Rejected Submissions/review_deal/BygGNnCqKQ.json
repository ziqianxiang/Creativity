{
    "Decision": {
        "metareview": "The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures. During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding. Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters. The optimized representation can then be mapped back into the discrete architecture space.\nOverall, the main idea of this work is very interesting, and the experiments show that the method has some promise. However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses. As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea, but requires additional experimentation and analyses"
    },
    "Reviews": [
        {
            "title": "Good paper, experimental validation must be improved",
            "review": "Even though many people have considered prunning as architecture search, it has not been explored enough so far. This paper comprises a good approach for compression of achitectures using pruning. Based on the uniqueness of the topological ordering of commonly used neural networks (feed forward, skip connection), the paper proposes a simple and easily manipulable vector (sequence) representation for a wide class of neural networks. Instead of using RNN networks, such long seqeunce representation are mapped to a continuous embedding by 1D-CNN. While training this embedding, for the purpose of compression, predictors needed for compression are jointly trained with embedding.\nConsequently, the proposed method presents a possiblity of including many other constraints during the architecture search.\n\nIn the specification of layers, the layer type is just appointed to an integer variable, even though in reality the layer type is a categorical variable. This choice is ok for standard neural network layers, where effectively the choice is between a single catecorical aspect. However, for more sophisticated layer configurations, where you may need many categorical choices, this model choice will not be adequate and will likely lead to artificially biased design choices. The authors should explain the limitations of this model design and propose methods these limitations can be tackled.\n\nThe overall model achieves quite good result in compression. On CIFAR10 the model show good performance as compared to existing compression methods. It should be noted that other methods start with a given stucture, so their search space is more limited than this paper's approach. Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm.\n\nCurrently, the number of experiments is borderline. They are enough to indicate the potentials of this approach. However, additional experiments would be welcome. For one, it would be to evaluate the proposed model in more challenging setups: evaluate on ImageNet dataset, using some of the recent architectures (e.g., ResNet, VGGnet, and so on). What is more, for more compressed architectures with better accuracy, when searching for a compressed architecture global optimization methods like Bayesian Optimization is worth to try, for instance using the recently proposed BOCK (Oh et al, ICML 2018).\n\nSome additional comments.\n- For a finite number of edges the number of possible graphs (including the valid architecture) are finite when the input is finite and pooling reduces the feature map size. Thus, it seems that the statement in theorem 3.2 is rather trivial and it is not worth calling it a Theorem.\n\nOverall, this was an interesting paper to read and worth of acceptance, provided that the proposed method delivers also in more competitive experimental settings.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but the approach is not rigorous",
            "review": "The paper presents a way to compress the neural network architecture. In particular, it first extracts some characteristics for the neural network architecture and then learns two mapping functions, one from the encoded architecture characteristics to the expected accuracy and the other from the same encoded architecture characteristics to the number of parameters. In the meanwhile, the proposed approach learns the encoding and the decoding for the architecture characteristics. \n\nPros:\n1. The idea of converting the architecture characteristics, which is discrete in nature, to continuous variables is interesting. The continuity of the architecture characteristics can help architecture search tasks.\n\nCons:\n1. My main concern is the validity of the compression step, Procedure COMPRESS, in Algorithm 1. First, is only one step gradient descent applied? If it is, why not minimize the L_c until convergence?  Second, it seems that minimizing L_c cannot guarantee that both error and the number of parameters are reduced. It is possible that only one of them is reduced. \n2. The writing of the paper needs to be improved. Some notations are not consistent with each other. For example, the loss notations in Line 19 in Algorithm 1 are different from those defined in Sec. 4.3. \n3. There is no step size, \\eta in Line 20 in Algorithm 1, but there is a step size in the last equation on Page 6. \n4. It is unclear to me how the hyperparameters, such as the step size and \\lambda's, are chosen. \n5. More experimental results are needed to support the proposed approach. \n\nIn summary, I think this paper is not ready to be published. \n\n ==== After rebuttal ====\nThe authors' feedback clarified some of my concerns. But my main concern about why minimizing the objective function can reduce both error and the number of parameters still remains. So I changed my rating to 4 from 3. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting underlying idea, but evaluation insufficient",
            "review": "This paper deals with Architecture Compression, where the authors seem to learn a mapping from a discrete architecture space which includes various 1D convnets. The aim is to learn a continuous latent space, and an encoder and decoder to map both directions between the two architecture spaces. Two further regressors are trained to map from the continuous latent space to accuracy, and parameter count. By jointly training all these networks, the authors are now able to compress a given network by mapping it's discrete architecture into the latent space, then performing gradient descent towards higher accuracy and lower parameter count (according to the learned regressors).\n\nThe authors perform experiments on 4 standard datasets, and show that they can in some cases get a 20x reduction in parameters with negligible performance decrease. They show better Cifar10 results than a few baselines - I am not aware whether this is SOTA for that parameter budget, and the authors do not specify.\n\nOverall I really like the idea in this paper, the latent space is well justified, but I cannot recommend acceptance of the current manuscript. There are many notational issues which I go into below, but the key issue is experiments and reproducability.\n\nThe search space is not clearly defined. Current literature shows that the performance of these methods depends a lot on the search space. The manuscript does make clear that a T-layer CNN is represented as a 5XT tensor, with each column representing layer type, kernel size etc. However the connectivity is not defined at all, which implies that layers are simply sequentially stacked. This seems to preclude even basic architectural advancement like skip connections / ResNet - the authors even mention this in section 3.1, and point to experiments on resnets in section 4.4, but the words \"skip\" and \"resnet\" do not appear anywhere else in the paper. I presume from the emphasis on topological sort that this is possible, but I don't see how.\n\nIf this paper is simply dealing with linear chains of modules, then the mapping to a continuous representation, and accuracy regression etc would still be interesting in principle. However it does mean that essentially all the big architecture advancements post-VGG (ie inception, resnet, densenet...) are impossible to represent in this space. Most of the Architecture Search works cited do have a search space which allows the more recent advances.\n\nI don't see a big reason why the method could not be extended - taking the 5D per-layer representation and adding a few more dimensions to denote connectivity would seem reasonable. If not, the authors should clearly mention the limitations of their search space.\n\n\nIn terms of experiments, Figure 3 is very hard to interpret. The axes labellings are nearly too small to read, but it's also unclear what loss this even is - I presume this is the 'train' loss of L_d + \\lambda_1L_a + \\lambda_2L_p, but it could also be the 'compress' loss. It also behaves very unusually - the lines all end up lower than where they started, but oscillate around a lot, making me wonder if the curves from a second set of runs would look anything alike. It's not obvious why there's not just a 'normal' monotonic decrease.\n\nA key point that is not really addressed is how well the continuous latent space actually captures what it should. I am extremely interested to know whether the result of 'compress', ie a new concrete architecture found by gradient descent in the latent space, actually has the number of parameters and the accuracy that the regressors predict. This could be added as columns in Table 1 - eg the concrete architecture for Cifar10 gets 20.33x compression and no change in accuracy, but does the regressor for the latents space predict this compression ratio / accuracy as well? If this is the case, then I feel that the latent space is clearly very informative, but it's not obvious here.\n\nIt would also be really useful to see some concrete input / output values in discrete architecture space. Presumably along the way to 20x compression of parameter count, the optimisation passes through a number of progressively smaller discrete architectures - what do these looks like? Is it progressively fewer layers / smaller filters / ??? Given that the discrete architecture encoding appears to have a fixed length of T, it's not even clear how layers would be removed. Figure 1 implies you would fill columns with zeros to delete layers, but I don't see this mentioned elsewhere in the text.\n\nMore minor points:\n\nEquation numbers would be extremely useful throughout the paper.\n\nNotation in section 3 is unclear. If theta represents trained parameters, then surely the accuracy on a given dataset would be a deterministic value. Assuming that the distribution P_{\\theta}(a | A, D) is used to represent the non-determinism of SGD training, is \\theta supposed to represent the initialised values of the weights?\n\nThere are 3 functions denoted by 'g' defined on page 3 and they all refer to completely different things - this is unnecessarily confusing.\n\nThe formula for expected accuracy - surely this should be averaging over N different training / evaluation runs, something like:\n\nE_{\\theta}[a | A, D] \\simto \\frac{1}{N} \\sigma_{i}^N g_{\\theta}(A, D, \\theta_i)\n\nThe decoder computes a 6xT output instead of a 5xT output - what is this extra row for?\n\nIn the definition of \"ground truth parameter count\" p^* - presumably the standard deviation here is the standard deviation of the l vector? This formulation is a bit surprising, as convolutional layers will generally have few parameters, and final dense layers could have many. Did you consider alternative formulations like simply taking the log of the number of parameters? Having a huber loss with scale 1 for this part of the loss function was also surprising, it would be good to have some justification for this (ie, what range are the p^* values in for typical networks?)\n\nIn algorithm 1 line 4 - here you are subtracting \\bar{p} from num_params before dividing by standard deviation, which does not appear in the formulation above.\n\nIn the experiments:\nHow were the 1500 random architectures generated? I presume by sampling uniformly a lot of 5xT tensors, but this encoding is not clearly defined. x_i is defined as being in the set of integers, does this include negative numbers? What are the upper / lower limits, and is there anything to push towards standard kernel sizes like 3x3, 5x5, etc? These random architectures were then trained five times for 5 epochs - what optimizer / hyperparameters / regularization was used? Similarly, the optimization algorithm used in the outer loop to learn the {en,de}coders/regressors is not specified.\n\nI would move the lemma and theorem into the appendix - they seem quite unrelated to the overall thrust of the paper. To me, saying that an embedding is not uniquely defined, but can be learnt is not that controversial, and I don't need proofs that some architecture search space has a finite number of entries. Surely the fact that the architecture is represented as a 5xT tensor, and practically there are upper limits to kernel size, stride etc beyond which an increase has no effect, already implies a finite space? Either way, this section of the paper did not add much value from my perspective.\n\n\nI want to close by encouraging the authors to resubmit after addressing the above issues, I do believe the underlying idea here is potentially very interesting.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}