{
    "Decision": {
        "metareview": "The present work proposes to improve backdoor poisoning attacks by only using \"clean-label\" images (images whose label would be judged correct by a human), with the motivation that this would make them harder to detect. It considers two approaches to this, one based on GANs and one based on adversarial examples, and shows that the latter works better (and is in general quite effective). It also identifies an interesting phenomenon---that simply using existing back-door attacks with clean labels is substantially less effective than with incorrect labels, because the network does not need to modify itself to accommodate these additional correctly-labeled examples.\n\nThe strengths of this paper are that it has a detailed empirical evaluation with multiple interesting insights (described above). It also considers efficacy against some basic defense measures based on random pre-processing.\n\nA weakness of the paper is that the justification for clean-label attacks is somewhat heuristic, based on the claim that dirty-label attacks can be recognized by hand. There is additional justification that dirty labels tend to be correlated with low confidence, but this correlation (as shown in Figure 2) is actually quite weak. On the other hand, natural defense strategies against the adversarial examples based attack (such as detecting and removing points with large loss at intermediate stages of training) are not considered. This might be fine, as we often assume that the attacker can react to the defender, but it is unclear why we should reject dirty-label attacks on the basis that they can be recognized by one detection mechanism but not give the defender the benefit of other simple detection mechanisms for clean-label attacks.\n\nA separate concern was brought up that the attack is too similar to that of Guo et al., and that the method was not run on large-scale datasets. The Guo et al. paper does somewhat diminish the novelty of the present work, but not in a way that I consider problematic; there are definitely new results in this paper, especially the interesting empirical finding that the Guo et al. attack crucially relies on dirty labels. I do not agree with the criticism about large-scale datasets; in general, not all authors have the resources to test on ImageNet, and it is not clear why this should be required unless there is a specific hypothesis that running on ImageNet would test. It is true that the GAN-based method might work more poorly on ImageNet than on CIFAR, but the adversarial attack method (which is in any case the stronger method) seems unlikely to run into scaling issues.\n\nOverall, this paper is right on the borderline of acceptance. There are interesting results, and none of the weaknesses are critical. It was unfortunately the case that there wasn't room in the program this year, so the paper was ultimately rejected. However, I think this could be a strong piece of work (and a clear accept) with some additional development. Here are some ideas that might help:\n\n(1) Further investigate the phenomenon that adding data points that are too easy to fit do not succeed in data poisoning. This is a fairly interesting point but is not emphasized in the paper.\n(2) Investigate natural defense mechanisms in the clean-label setting (such as filtering by loss or other such strategies). I do not think it is crucial that the clean-label attack bypasses every simple defense, but considering such defenses can provide more insight into how the attack works--e.g., does it in fact lead to substantially higher loss during training? And if so, at what stage does this occur? If not, how does it succeed in altering the model without inducing high loss?",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "interesting idea, good execution, but just below threshold"
    },
    "Reviews": [
        {
            "title": "Clean-Label Backdoor Attacks",
            "review": "\nThis work explores backdoor attacks -- attacks that alter a fraction of training examples which can alter inference -- while ensuring that the poisoned inputs are consisten\nt with their labels. These attacks are attained through either a GAN mechanism or using adversarial perturbations.\n\nThe ideas proposed (i.e. GAN mechanism and adversarial mechanism) are interesting additions to this literaature. I found the observation of greater effectiveness of adversa\nrial mechanism particularly interesting.\n\nThe paper also does a good job of investigating effectiveness of the attack under data augmentation and propooses a limited solution.\n\nMain criticism: there are a number of typos that need fixing.\n~                                            ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks",
            "review": "Overall I am positive about this manuscript:\n- I find the motivation is clear and valid. As far as I know, this is a novel contribution (my confidence is not very high on that one though - I might be unaware of related work).\n- The paper is well-written and organized.\n- Experiments are conducted systematically, although certain parts could be better explained (see my questions below).\n\nI think this paper adds an original and valuable angle to the existing literature on data poisoning attacks. I don't see any major flaws, therefore I think it should be accepted.\n\nA few points which might need clarification:\n- How exactly is \"attack success\" being measured?\n- Which model is used to generate the adversarial samples? Is this an (adversarially) pretrained model? (If that's the case, then what is the model architecture?) Or are adversarial samples generated on the fly using the currently trained/poisoned model?\n- At the end of Section 4.4: if the images with larger noise rely more on the backdoor, why does this have an adverse effect? Shouldn't it increase the effectiveness of the attack?\n- Was the data augmentation (flips, crops etc) performed before or after the poisoning pattern was applied?\n\nMinor comments:\n- definition of the encoding at the bottom of page 4: this should be argmax instead of max\n- typo in Sec. 5.1: \"to evaluate the uat a wide variety\"\n- repetitive sentence in Sec. 5.2: \"we find that images generated with $\\tau \\leq 0.2$ remain [fairly] plausible\"\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A nice idea which needs further in-depth exploitation",
            "review": "This paper investigates an interesting problem, backdoor attack against neural networks. The main idea is to add a watermark pattern to the corners of the training images, so that the classifier is guided to leverage the watermark as a discriminative cue as opposed to the real content of the image. At the test stage, one can hence manipulate the classifier’s predictions by adding the watermark to the test images.\n\nThis paper is heavily built upon Gu et al. (2017)’s work. It shows that Gu et al. (2017)’s method can be easily defended by a data sanitization algorithm. To improve Gu et al.’s work, the authors propose to add watermark patterns to the adversarial examples or examples interpolated in GAN’s latent space. The intuition is that these examples are adversarial and hard to learn, forcing the classifier to focus on the watermark pattern instead. \n\nIt is an interesting idea and an intuitive improvement over (Gu et al. 2017). However, the implementation of the idea could be improved. This paper does not propose any new attack algorithms. Instead, it investigates an existing adversarial attack method and the GAN based interpolation for the purpose of backdoor attack. As experiments are conducted on small-scale datasets, it is unclear how effective the improved backdoor attack is. Moreover, one of the main disadvantages of the proposed attack method is that simple data augmentation techniques, especially random cropping, can successfully defend against the attack. \n\nThe quality of the paper writing could be improved. I had to read the paper more than twice and check the references now and then in order to understand some claims of the paper. The paper’s lack of clarity was actually also raised by probably one of the coauthors of the paper; see the comment “Dimitris: clarify this point” on Page 11. Please find some concrete suggestions below.\n- Figure 1 is visually not appealing at all. Perhaps find better illustrative examples. \n- It is worth considering to add a separate section/paragraph to describe the details of Gu et al. (2017)’s method, given that this paper is heavily built upon Gu et al. (2017)’s work.\n- It was unclear what the “reduced amplitude backdoor trigger” means until Section 4. If a context-dependent term has to be used in the introduction, explain it or refer the readers to the right place of the paper. \n- Merge Sections 4.3—4.5 with the experiment section (Section 5). The results of Section 4.3–4.5 are out of context without any explanation about the experiment setups. \n\nI have some concerns about Section 3, which is the main motivation of this work. As the authors noted in Appendix A that Gu et al.’s method works well with as few as 75 poised examples, the proposed sanitization algorithm would not be able to fail Gu et al.’s method by only identifying 20 out of 100 poised examples. \n\nHow to control the parameter $\\tau$ so that the perturbation appears plausible to humans? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}