{
    "Decision": {
        "metareview": "This paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. A population-based genetic algorithm is used to find the sparse, connections between dense module units. The local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. Based on reviewers’ ratings (5,5,6), the current version of paper is proposed as borderline lean reject.\n\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A genetic algorithm to search locally-dense and globally-sparse neural network architectures. "
    },
    "Reviews": [
        {
            "title": "Interesting topic and insights, but requiring more improvements",
            "review": "The authors present the interesting and important direction in searching better network architectures using the genetic algorithm. Performance on the benchmark datasets seems solid. Moreover, the learned insights described in Section 4.4 would be very helpful for many researchers.\n\nHowever, the overall paper needs to be polished more. There are two many typos and errors that imply that the manuscript is not carefully polished. Explanations about some terms like growth rate, population, etc. are necessary for broader audience. \n\nMore importantly, while some of step jumps in Figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in Section 4.2. Thee clear explanation about that phenomena is required.\n\n* Details\n- Please represent the blocks (e.g. 1*1conv) better. Current representation is quite confusing to read. Maybe proper spacing and different style of fonts may help.\n- In Page 5, \"C_{m}ax\" is a typo. It should be \"C_{max}\".\n- Regarding the C_max, does sum(C_max) represent (D * W)^2 where D is the total depth and W is the total indicies in each layer? If so, specifying it will help. Otherwise, please explain its meaning clearly.\n- In Figure 4(a), it would be better if we reuse M_{d,w} notation instead of Module {d_w}.\n- Please briefly explain or provide references to the terms like \"growth rate\", \"population\", and \"individuals\". \n- Different mutations may favor different hyper-parameters. How the authors control the hyperparameters other than the number of epochs will be useful to know.\n- Even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated. They need to be presented to verify the hypothesis that the authors claim. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes a neural network architectures with locally dense and globally sparse connections. Using dense units a population-based evolutionary algorithm is used to find the sparse connections between modules.",
            "review": "The problem is of increasing practical interest and importance. \n\nThe ablation study on the contribution and effects of each constituent  part is a strong part of the experiment section and the paper. \n\nOne major concern is about the novelty of the work. There are many similar works under the umbrella of Neural Architecture search who are trying to connect different building blocks (modules) to build larger CNNs. One example that explicitly makes sparse connections between them is [1]. Other examples of very similar works are [2,3,4].\n\nThe presentation of the paper can be improved a lot. In the current setup it’s very similar to a collection of ideas and tricks and techniques combined together. \n\nThere are some typos and errors in the writing. A thorough grammatical  proofreading is necessary. \n\nIn conclusion there is a claim about tackling overfitting. It’s not well supported or discussed in the experiments. \n\n[1] Shazeer, Noam, et al. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\" arXiv preprint arXiv:1701.06538 (2017).\n[2] Xie, Lingxi, and Alan L. Yuille. \"Genetic CNN.\" ICCV. 2017.\n[3] Real, Esteban, et al. \"Large-scale evolution of image classifiers.\" arXiv preprint arXiv:1703.01041 (2017).\n[4] Liu, Hanxiao, et al. \"Hierarchical representations for efficient architecture search.\" arXiv preprint arXiv:1711.00436 (2017).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "LEARNING INTERNAL DENSE BUT EXTERNAL SPARSE STRUCTURES OF DEEP NEURAL NETWORK",
            "review": "The authors bridge two components (density of CNNs and sparsity structures) by proposing a new network structure with locally dense yet externally sparse connections. \n\n+ Combination of being dense and sparse is an interesting area.\n- Although experiment results demonstrate evolving sparse connection could reach competitive results, it would be interesting to show how separating a network into several small networks is useful, for example, interpretablity of deep neural network. There is an interesting work: \"Using deep learning to model the hierarchical structure and function of a cell\" https://www.nature.com/articles/nmeth.4627\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}