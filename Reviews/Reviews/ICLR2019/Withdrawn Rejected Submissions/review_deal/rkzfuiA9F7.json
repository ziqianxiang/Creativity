{
    "Decision": {
        "metareview": "The reviewers all like the idea, and though the performance is a little better when compared to prototypical networks, the reviewers felt that the contribution over and above prototypical networks was marginal and none of them was willing to champion the paper. There is merit in that there is increased robustness to outliers, and future iterations of the paper should work to strengthen this aspect.\n\nAs a quick nitpick: based on my reading, and on Figure 3, it looks like there might be a typo in the definition of X_k (bottom of page 4). Right now it is defined in terms of the original data space x, when I think it should be defined in terms of the embedding space f(x). Overall this paper is a good contribution to the few-shot learning area.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A robust extension of prototypical networks, but needs a clear motivation for this property."
    },
    "Reviews": [
        {
            "title": "Interesting extension to embedding-based approaches to few-shot learning but results are a bit disappointing",
            "review": "This paper considers the problem of few-shot learning and proposes a new embedding-based approach. In contrast to previous work (such as Matching Networks and Prototypical Networks) where distance is computed in pure embedding space, this work proposes computing a low-dimensional subspace to represent a class and using the distance from an embedded query point to this subspace. The low-dimensional subspace for a class is computed by running truncated Singular Value Decomposition on the normalized embeddings of all points in the support set for that class and using the top n left singular vectors as the basis for the class's subspace. The authors also propose an extension to their model to the semi-supervised few-shot learning setting by incorporating masked-mean computation and zero-mean cluster for distractor items (both ideas borrowed from Renn 2017 for prototypical networks). Experiments are conducted on Mini-Imagenet in the few-shot learning setting and on Mini-Imagenet and Tiered-ImageNet in the semi-supervised few-shot learning setting.\n\nPros:\n- Proposed idea is novel and proposes an interesting change to existing embedding-based few-shot learning techniques.\n\nCons:\n- Performance benefit is a bit disappointing; Mini-Imagenet few-shot performance improvement relative to Prototypical-Nets is minimal (barely 1% for 5way-5shot and 20way-5shot case). For semi-supervised experiments, there is bigger improvement for Mini-Imagenet (4% for both without distractors and with distractors) but less so for Tiered-ImageNet (close to 0% for without distractors and with distractors).\n\nRemarks:\n- The paper seems to be missing what the dimensionality of the subspace is for the experiments? Was this picked using validation set performance?\n- In first paragraph of page 2, it seems too strong to say \"...this makes our paper unparalleled to previous studies\"; maybe change to \"...this make our proposed model novel relative to previous work\"\n- Is there previous work that has involved back-propagating through SVD? It would be useful to mention these as references.\n- In Figure 1, it is visually shown how outliers can negatively impact Matching-Networks and Prototypical-Networks but not visually shown how PSN is resistant to them?\n- The claim is made that the proposed method is more robust to outliers. Is there more of a justification that can be given for this? Either in terms of some intuition or an experiment that can be run? For example, can it be shown that outliers cause the prototype of a class to move a lot (in terms of distance from original prototype without outliers) whereas the original subspace compared to subspace with outliers is less different by measuring this on Mini-Imagenet?\n- Typo on page 5: \"in what follwos\" => \"in what follows\"\n- In Discussion, paper states, \"Moreover, the Prototypical Network makes use of the class mean and can be easily incorporated in our testbed\": what does this mean exactly?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some experiments are missing. ",
            "review": "This paper proposes a Projective Subspace Network (PSN) for few-shot learning. The PSN represents each support set of classes as a subspace obtained by SVD. Then the method calculates distances between a query and classes by the projection error to the subspace. Instead of using the prototype of the class center, the subspace representation is more robust to outliers. Though the contribution seems to be incremental, it is a reasonable improvement upon Matching Networks and Prototypical Networks.\n\nPros. \n+ The proposed subspace method is simple and reasonable\n+ The performance is better than some related works on few-shot learning. \n\nCos. \n- The authors claimed that subspace representation is more robust to noise within each class samples. However, this is not supported by experiments. The authors evaluated the distractor classes. However, this is not the case when the outlier existed within each class samples. \n\n- For semi-supervised few-shot learning, the authors proposed a fake class with zero means. The effect of this fake class is not evaluated. \n\n- The dimensionality of subspace (n) seems to be not written.\n\n- The sensitivity analysis of the dimensionality of subspace is missing. For subspace methods, it is essential to evaluate the performance w.r.t the dimension. \n\n- Descriptions in the related work section should be improved. It is unclear how the proposed method is related to K-means, K-modes, and K-prototype. Also, the authors wrote that works (Chan et. 2015, Sun et al. 2017) use PCA or SVD to reduce the dimensionality of feature representation in neural networks. However, both methods do not perform dimensional reduction. PCANet (Chan et al . 2015) obtains convolutional filters by applying PCA to input images or feature maps. SVDNet (Sun et al. 2017) applies SVD for obtaining decorrelated weights in a neural network. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Neat idea but requires clarifying the merits of the approach and differences from previous work",
            "review": "This paper presents a new method for fully- and semi-supervised few-shot classification that is based on learning a general embedding as usual, and then learning a sub-space of it for each class. A query point is then classified as the class whose sub-space is closest to it.\n\nPros: This is a neat idea and achieves competitive results. Learning a sub-space per class makes intuitive sense to me since it’s plausible that there is a lower-dimensional subspace of the overall embedding space that captures the properties that are common to only examples of a certain class. If this is indeed the case, it seems that indeed classifying query examples into classes based on their distances from the corresponding sub-spaces would lead to good discrimination. \n\nCons: First, an inherent limitation is that this approach is not applicable to one-shot learning, and I have doubts in its merit for very low shot learning (explained below). Second, I’m missing the justification behind a key point used to motivate the approach, which requires clarification (explained below). Third, I feel that certain aspects of the approach were unclear (details to follow). Finally, I feel more analysis is needed to better understand the differences of this method from previous work (concrete suggestions follow). For semi-supervised learning, the novelty regarding how the unlabeled examples are incorporated is limited, as the approach used is previously-introduced in Ren et al, 2018.\n\nOverall, even though I like the idea and the results are good, there are a few points, mentioned in the above section that I feel require additional work before I can strongly recommend acceptance. Most importantly, relating to getting more intuition about why and when this works best, and tying it in better with previous approaches. \n\nA key point requiring clarification.\nThere is a key fact that the authors used to motivate this approach which remains unclear to me: why is it the case that this approach is less sensitive to outliers than previous approaches? In Figure 1, an outlier is pictured in each of subfigures (a) and (b) corresponding to Matching and Prototypical Networks, but not in subfigure (c) which corresponds to PSN. No explanation is provided to justify this conjecture, other than empirical evaluation that is based on the overall accuracy only. In particular, since SVD is used to obtain the sub-spaces, instead of an end-to-end learned projector that directly optimizes the query set accuracy, it’s not clear why if a support point is an outlier it would not affect the sub-space creation. If I’m missing something, please clarify!\n\n(A) Comments on the approach.\n(1) Why define X_k as the support set examples minus the class prototype instead of just the support examples themselves? The latter seems simpler, and should have all the required information for shaping the class’ subspace.\n(2) Note that if X_k is defined as [x_{k,1}, \\dots, x_{k,K}] as proposed in the above point (ie. without subtracting the class mean from each support point) then this method would have been applicable to 1-shot too. How would it then compare to a 1-shot Prototypical Network? Notice that in this case the mean of the class is equal to this one example.\n(3) In general, the truncated SVD decomposition for a class can be written using the matrices U, \\Sigma and V^T with dimensions [D, n], [n, n] and [n, K] respectively, where D is the embedding dimensionality and K is the number of support points belonging to the given class. The middle matrix \\Sigma in the non-truncated version would have dimensions [D, K]. Does this mean that when truncating, n is enforced to be smaller than each of D and K? This would mean that the dimensionality n of the sub-space is limited by the number of the support examples, which in some cases may be very small in few-shot learning. Can you comment on this?\n(4) How to set n (the dimensionality of each subspace) is not obvious. What values were explored? Is there a sweet spot in the trade-off between the observed complexity and the final accuracy?\n\n(B) Comparison with Prototypical Networks.\n(1) In what situations do we expect learning a sub-space per class to do better than learning a  prototype per class? For example, Figure 4 shows the test-time performance as a function of the test ‘way’. A perhaps more interesting analysis would be to compare the models’ performance as a function of the test *shot*: if more examples are available it may be less appropriate to create a prototype and more beneficial to create a sub-space? \n(2) Can we recover Prototypical Networks as a special case of PSN? If so, how? It would be neat to show under which conditions these are equivalent.\n\n(C) Clarifications regarding the semi-supervised setup.\n(1) Are distractor classes sampled from a disjoint pool of classes, or is it that, for example, a class which is a distractor in an episode is a non-distractor in another episode.\n(2) Similarly for labeled / unlabaled at training time. Can the same example appear as labeled in one episode but unlabaled in another? In Ren et al, 2018, this was prevented by creating an additional labeled/unlabeled split even for the training examples. Therefore they use strictly less overall information at meta-training time than if that split weren’t used. To be comparable with them, it’s important to apply this same setup.\n\n(D) Additional minor comments.\n(1) “To work at the presence of distractors, we propose to use a fake class with zero mean”. Note that this was already proposed in Ren et al, 2018. They used a zero-mean, high-variance additional cluster whose aim was to ‘soak up’ the distractor examples to prevent them for polluting legitimate clusters (this was the second model they proposed).\n(2) In the introduction, regarding contribution iii. A more appropriate way to describe this is as exploring generalization to different numbers of classes, or ‘ways’ at test time than what was used at training time.\n(3) Gidaris and Komodakis (2018) is described in the related work as using a more complicated pipeline. Note however that their pipeline is in place for solving a more challenging problem than standard few-shot classification: they study how a model can maintain the ability to remember training classes while rapidly learning about new ‘test’ classes.\n(4) In the last line of section 5.3, use N-way instead of K-way since in the rest of the paper K was used to refer to the shot, not the way.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}