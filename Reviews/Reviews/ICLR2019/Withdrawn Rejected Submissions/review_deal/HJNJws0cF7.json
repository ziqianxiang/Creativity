{
    "Decision": {
        "metareview": "The paper proposes a novel approach to neural net construction using dynamical systems approach,  such as higher order Runge-Kutta method; this approach also allows a dynamical systems interpretation of DenseNets and CliqueNets. While all reviewers agree that this is an intersting a novel approach, along the lines of recent developments in the field on dynamical systems approaches to deep nets, they also suggest to further improve the writing/clarity of the paper and also strengthen  the empirical results (currently, the method only provided advantage on CIFAR-10, while being somewhat suboptimal on other datasets, and more evidence for empirical advantages of the proposed approach would be great). Overall, this is a very interesting and promising work, and with a few more empirical demonstrations of the method's superiority as well as more polished wiriting the paper would make a nice contribution to ML community.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Nice approach, a bit more work is required"
    },
    "Reviews": [
        {
            "title": "Extending dynamical systems view to higher order ODE solvers",
            "review": "This paper is based on an interpretation of so-called Residual Deep Networks as a discrete realization of some dynamical system. So far, work in this area used simple, Euler-forward like discretization schemes, while the paper at hand proposes to apply higher order Runge-Kutta Methods. This approach allows also to give a different interpretation of DenseNets and CliqueNets.\n\nThe general idea of this paper is interesting. Given that it is along a position paper, I think that the small set of experiments is acceptable to demonstrate the principle. What I miss however, is a clear exposition of the detailed approach. The paper lacks a lot in clarity and quality of presentation. I also think that trying to connect the presented approach to the ventral stream in the visual cortex confuses more than it helps.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Connecting CNNs with ODE solvers",
            "review": "The paper takes the perspective that a ResNet can be seen as a discrete time dynamical system approximation, and then uses that insight to introduce a Runge Kutta style continuous dynamical system.  Results are achieved that outperform DenseNet and CliqueNet\n\nQuality\n- The connection between deep nets and dynamical systems is an important one, as a small but growing literature demonstrates.\n- The quality of this paper is difficult to extract, given rather long and unclear definitions of various key choices, at key points opting instead for a speculative connection to neurosciences (see clarity below).\n- The empirical results could make this paper compelling, but the comparisons are rather shallow, in that they compare on only a few datasets and don't compare extensively to many of the state of the art methods.   For example, in Table 2 DenseNet outperforms the IRKNets, but this fact is not highlighted.  Overall this and other key omissions create the concern that the results are not particularly strong.\n- One key reference that is missing is the careful work Chen et al https://arxiv.org/abs/1806.07366.  This work provides an excellent basis for how to present these ideas rigorously.\n\nClarity\n- The connections to the brain -- such as \"visual cortex\", \"ventral stream\", etc -- are not rigorous and are unrelated to the paper at hand; really all connections to brain should be removed.  \n- Figures 2 and 3 are the central architectural choices, but the writing around them does not clarify why all the choices have been made, and what the implications are.\n\nOriginality\n- This is one of a small but growing number of papers connecting deep nets to ODE solvers.  Chen et al should be cited (Chen et al https://arxiv.org/abs/1806.07366).  \n- There appears to be sufficient originality.\n\nSignificance\n- hard to determine with unclear exposition and rather incomplete empirical results.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review TLDR. Good subject, paper can be presented a bit more clearer ",
            "review": "\nAuthors describe a method  in which based on the dynamical system description of how a neural networks work, one can construct an analogues process based on Runge Kutta method.\n\nThe paper is sound mathematically, touches upon ideas that have been circling around for a while (https://arxiv.org/pdf/1804.04272.pdf for example) and presents a nice test case for integrating tools from dynamical systems into deep learning.\n\nThe presentation though is a bit convoluted and unclear and I would strongly encourage the writers to break it down and make it more readable \n\nThe method is based on looking at the midsection rules of RK as a feed forward process and mapping that as information propagation in the network. RK methods can be pretty sensitive to Chaos and other non linear effects if one does not take a *very small* time step discretization step. Did you perform some sort of stability analysis on networks as a function of non linear activations inside of it ?\nAlso RK methods a pretty expensive compared to most standard Suzuki-Trotter, can you remark a bit on the computational aspects of you experiments ? \nI think that a big chunk of 3.1 can be either taken out to the appendix or just give a reference to a standard text book and elaborate on training time/resources etc â€¦.\n\nAs a generic question I wonder how does this approach handle things which are weak perception or non perception methods, did you try this on any NLP problems or generic tabular data ?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}