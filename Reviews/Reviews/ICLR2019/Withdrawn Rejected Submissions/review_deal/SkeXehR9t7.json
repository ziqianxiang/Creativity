{
    "Decision": {
        "metareview": "Strengths:\nThe work proposes a novel architecture for graph to sequence learning.\nThe paper shows improved performance on synthetic transduction tasks and for graph to text generation. \n\nWeaknesses:\nMultiple reviewers felt that the experiments were insufficient to evaluate the novel aspects of the submission relative to prior work.  Newer experiments with the proposed aggregation strategy and a different graph representation were not as promising with respect to simple baselines. \n\nPoints of contention:\nThe discussion with the authors and one of the reviewers was particular contentious.\nThe title of the paper & sentences within the paper such as \"We propose a new attention-based neural networks paradigm to elegantly address graph- to-sequence learning problems\" caused significant contention, as this was perceived to discount the importance of prior work on graph-to-sequence problems which led to a perception of the paper \"overclaiming\" novelty.\n\nConsensus:\nConsensus was not reached, but both the reviewer with the lowest score and one of the reviewers giving a 6 came to the consensus that the experimental evaluation does not yet evaluate the novel aspects of the submission thoroughly enough.\n\nDue to the aggregate score, factors discussed above (and others) the AC recommends rejection; however, this work shows promise and additional experimental work should allow a new set of reviewers to better understand the behaviour and utility of the proposed method.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting ideas, but a more targeted evaluation is needed  "
    },
    "Reviews": [
        {
            "title": "Interesting work but lacking some organization",
            "review": "This work proposes an end-to-end graph encoder to sequence decoder model with an attention mechanism in between.\nPros (+) :\n+ Overall, the paper provides a good first step towards flexible end-to-end graph-to-seq models.\n+ Experiments show promising results for the model to be tested in further domains.\nCons (-) :\n- The paper would benefit more motivation and organization.\n\nFurther details below (+ for pros / ~ for suggestions / - for cons):\n\nThe paper could benefit a little more motivation:\n- Mentioning a few tasks in the introduction may not be enough. Explaining why these tasks are important may help. What is the greater problem the authors are trying to solve?\n- Same thing in the experiments, not well motivated, why these three? What characteristics are the authors trying to analyze with each of these tasks?\n\nRephrase the novelty argument:\n- The authors argue to present a “novel attention mechanism” but the attention mechanism used is not new (Bahdanau 2014 a & b). The fact that it is applied between a sequence decoder and graph node embeddings makes the paper interesting but maybe not novel.\n~ The novelty added by this paper is the “bi-edge-direction“ aggregation technique with the exploration of various pooling techniques. This could be emphasized more.\n\nPrevious work:\n~ The Related Work section could mention Graph Attention Networks (https://arxiv.org/abs/1710.10903) as an alternative to the node aggregation strategy.\n\nAggregation variations:\n+ The exploration between the three aggregator architectures is well presented and well reported in experiments.\n~ The two Graph Embedding methods are also well presented, however, I didn’t see them in experiments. Actually, it isn’t clear at all if these are even used since the decoder is attending over node embeddings, not graph embedding… Could benefit a little more explanation\n\nExperiments:\n+ Experiments show some improvement on the proposed tasks compared to a few baselines.\n- The change of baselines between table 1 for the first two tasks and table 2 for the third task is not explained and thus confusing.\n~ There are multiple references to the advantage of using “bi-directional” node embeddings, but it is not clear from the description of each task where the edge direction comes from. A better explanation of each task could help.\n\nResults:\n- Page 9, the “Impact of Attention Mechanism” is discussed but no experimental result is shown to support these claims.\n\n\nSome editing notes:\n(1) Page 1, in the intro, when saying “seq2seq are excellent for NMT, NLG, Speech Reco, and drug discovery”: this last example breaks the logical structure of the sentence because it has nothing to do with NLP.\n(2) Page 1, in the intro, when saying that “<...> a network can only be applied to sequential inputs”: replace network by seq2seq models to be exact.\n(3) Typo on page 3, in paragraph “Neural Networks on Graphs”, on 8th line “usig” -> “using”\n(4) Page 3, in paragraph “Neural Networks on Graphs”, the following sentence: “An extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs.” is missing some information, like a reference, or a proof in Appendix, or something else…\n(5) Page 9, the last section of the “Impact of Hop Size” paragraph talks about the impact of the attention strategy. This should be moved to the next paragraph which discusses attention.\n(6) Some references are duplicates:\n|_ Hamilton 2017 a & c\n|_ Bahdanau 2014 a & b\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper",
            "review": "This paper proposes a graph to sequence transducer consisting of a graph encoder and a RNN with attention decoder. \n\nStrengths:\n- Novel architecture for graph to sequence learning.\n- Improved performance on synthetic transduction tasks and graph to text generation. \nWeaknesses:\n- Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data. \n\nTransduction with structured inputs such as graphs is still an under-explored area, so this paper makes a valuable contribution in that direction. Previous work has mostly focused on learning graph embeddings producing outputs. This paper extends the encoder proposed by Hamilton et al (2017a) by modelling edge direction through learning “forward” and “backward” representations of nodes. Node embeddings are pooled to a form a graph embedding to initialize the decoder, which is a standard RNN with attention over the node embeddings. \n\nThe model is relatively similar to the architecture proposed by Bastings et al (2017) that uses a graph convolutional encoder, although the details of the graph node embedding computation differs. Although this model is presented in a more general framework, that model also accounted for edge directionality (as well as edge labels, which this model do not support). \n\nThis paper does compare the proposed model with graph convolutional networks (GCNs) as encoder experimentally, finding that the proposed approach performs better on shortest directed path tasks. However the paper could make difference between these architectures clearer, and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties. \n\nThe model obtains strong performance on the somewhat artificial bAbI and Shortest path tasks, while the strongest result is probably that of strong improvement over the baselines in SQL to text generation. However, very little insight is provided into this result. It would be interesting to apply this model to established NLG tasks such as AMR to text generation.  \n\nOverall, this is an interesting paper, and I’d be fine with it being accepted. However, the modelling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design, or more applications to real tasks and insight into the model’s performance on these tasks is required. \n\nEditing notes:\nHamilton et al 2017a and 2017c is the same paper. \nIn some cases the citation format is used incorrectly: when the citation form part of the sentence, the citation should be inline. E.g. (p3) introduced by (Bruna et al., 2013)  -> introduced by Bruna et al. (2013). \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak increment on graph to sequence tasks",
            "review": "The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder. The resulting model is evaluated on three very simple tasks, showing small improvements over baselines.\n\nI'm not entirely sure what the contribution of this paper is supposed to be. The technical novelty seems to be limited to new notation for existing work:\n- (Sect. 3.1) The separation of forward/backward edges was already present in the (repeatedly cited) Li et al 2015 paper on GGNN (and in Schlichtkrull et al 2017 for GCN). The state update mechanism (a FC layer of the concatenation of old state / incoming messages) seems to be somewhere between a gated unit (as in GGNN) and the \"add self-loops to all nodes\" trick used in GCN; but no comparison is provided with these existing baselines.\n- (Sect 3.2) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al; no comparison to these baselines is provided.\n- (Sect. 3.3) This is a standard attention-based decoder; the fact that the memories come from a graph doesn't change anything fundamental.\n\nThe experiments are not very informative, as simple baselines already reach >95% accuracy on the chosen tasks. The most notable difference between GGS-NNs and this work seems to be the attention-based decoder, but that is not evaluated explicitly. For the rebuttal phase, I would like to ask the authors to provide the following:\n- Experimental results for either GGS-NN with an attentional decoder, or their model without an attentional decoder, to check if the reported gains come from that. The final paragraph in Sect. 4 seems to indicate that the attention mechanism is the core enabler of the (small) experimental gains on the baselines.\n- The results of the GCN/GG-NN models (i.e., just as an encoder) with their decoder on the NLG task.\n- More precise definition of what they feel the contribution of this paper is, taking into account my comments from above.\n\nOverall, I do not think that the paper in its current state merits publication at ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}