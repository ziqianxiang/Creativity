{
    "Decision": {
        "metareview": "The paper proposes a framework for continual/lifelong learning that has potential to overcome the problems of catastrophic forgetting and data privacy. \nR1, R2 and AC agree that the proposed method is not suitable for lifelong learning in its current state as it linearly increases memory and computational cost over time (for storing features of all points in the past and increasing model capacity with new tasks) without account for budget constraints.\n\nThe authors responded in their rebuttal that the data is not stored in the original form, but using feature representation (which is important for privacy issues). The main concern, however, was about the fact that one has to store information about all previous data points which is not feasible in lifelong learning. In the revision the authors have tried to address some of the R1’s and R2’s suggestion about taking into account the budget constraints. However more in-depth analysis is required to assess feasibility and advantage of the proposed approach. \nThe authors motivate some of the key elements in their model as to protect privacy. However no actual study was conducted to show that this has been achieved. \nThe comments from R3 were too brief and did not have a substantial impact on the decision.\n\nIn conclusion, AC suggests that the authors prepare a major revision addressing suitability of the proposed approach for continual learning under budget constraints and for privacy preservation and resubmit for another round of reviews.  \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Continual learning approach with increasing computational cost over time",
            "review": "This paper proposes a continual learning approach which transforms intermediate representations of new data obtained by a previously trained model into new intermediate representations that are suitable for a task of interest.\nWhen a new task and/or data following a different distribution arrives, the proposed method creates a new transformation layer, which means that the model’s capacity grows proportional to the number of tasks or data sets being addressed over time. Intermediate data representations are stored in memory and its size also grows.\nThe authors have demonstrated that the proposed method is robust to catastrophic forgetting and it is attributed to the feature transformation component. However, I’m not convinced by the experimental results because the proposed method accesses all data in the past stored in memory that keeps increasing infinitely. The authors discuss very briefly in Section 5.2 on the performance degradation when the memory size is restricted. In my opinion, the authors should discuss this limitation more clearly on experimental results with various memory sizes.\n\nThe proposed approach would make sense and benefit from storing lower dimensional representations of image data even though it learns from the entire data over and over again.\nBut it is unsure the authors are able to claim the same argument on a different type of data such as text and graph.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The work present a framework for dealing with life long learning, yet it violates two important constraints which every life long learner has to obey: limited memory and computation. ",
            "review": "Summary:\n a method is presented for on-going adaptation to changes in data, task or domain distribution. The method is based on adding, at each timed step, an additional network module transforming the features from the previous to the new representation. Training for the new task/data at time t relies on re-training with all previous data, stored as intermediate features. The method is shown to provide better accuracy than naïve fine tuning, and slightly inferior to plain re-training with all the data.\nWhile the method is presented as a solution for life long learning, I think it severely violates at least two demands from a feasible solution: using finite memory and using finite computational capacity (i.e. a life-long learning cannot let memory or computation demands to rise linearly with time). Contrary to this, the method presented induces networks which grow linearly in time (in number of layers, hence computation requirements and inference time), and which use a training set growing indefinitely, keeping (representations of) all the examples ever seen so far. If no restrictions on memory and computation time are given, full retraining can be employed, and indeed it provides better results that the suggested method. In the bottom line, I hence do not see the justification for using this method, either as a life-long learner or in another setting.\n\nPros:\n+ the method shows that for continuous adaptation certain representations can be kept instead of the original examples \nCons:\n- The method claims to present a life long learning strategy, yet it is not scalable to long time horizon (memory and inference costs rise linearly with time)\n- Some experiments are not presented well enough to be understood.\n\nMore detailed comments:\nPage 3:\n-\tEq. 2 is not clear. It contains a term ‘classification loss’ and ‘feature_loss’ which are not defined or explained. While the former is fairly standard, leaving the latter without definition makes this equation incomprehensible. \no\tI later see that eq. 7 includes the details. Therefore eq.2 is redundant. \nPage 4:\n-\tEq. 5 seems to be flawed, though I think I can understand what it wants to say. Specifically, it states two sets: one of examples (represented by the previous feature extractor) and one of labels (of all the examples seen so far). The two sets are stated without correspondence between examples and labels – which is useless for learning (which requires example-label correspondence). I think the intention was for a set of (example, label) pairs, where the examples are represented using feature extractor of time t-1.\n-\tAlgorithm 1 seems to be a brute force approach in which the features of all examples from all problems encountered so far are kept (with their corresponding labels). This means keeping an ever growing set of examples, and training repeatedly at each iteration on this set. These are not realistic assumptions for a life-long learner with finite capacity of memory and computation.\no\tFor example, for the experiment reported at page 6, including 25 episodes on MNist, each feature transformer is adding 2 additional FC layers to the network. This leads to a network with >50 FC layers at time step 25 – not a reasonable and scalable network for life ling learning\nPage 6:\n-\tThe results show that the feature transformer method achieve accuracy close to cumulative re-training, but this is not too surprising, since feature transformer indeed does cumulative re-training: at each time step, it re-trains the classifier (a 2 stage MLP) using all the data at all times steps (i.e. cumulative retraining). The difference from pure cumulative re-training, if I understand correctly, is that the cumulative re-training is done not with the original image representations, but with the intermediate features of time t-1. What do we earn and what do we loose from this? If I understand correctly, we earn that the re-training is faster since only a 2-layer MLP is re-trained instead of the full network. We loose in the respect that the model gorws larger with time, and hence inference becomes prohibitively costly (as the network grows deeper by two layers each time step). Again, I do not think this is a practical or conceptual solution for life long learning.\n-\tThe experiment reported in figure 3 is not understandable without reading Lopez-Paz et al., 2017 (which I didn’t). the experiment setting, the task, the performance measurements – all these are not explained, leaving this result meaningless for a stand-alone read of this paper.\n-\tPage 8: it is stated that “we only store low dimensional features”. However, it is not reported in all experiment exactly what is the dimension of the features stored and if they are of considerably lower dimension than the original images. Specifically for the MNIst experiments it seems that feature stored are of dimension 256, while the original image is of dimension 784 – this is lower, but no by an order of magnitude (X10).\n-\tThe paper is longer than 8 pages.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Mechanical Approach to Augmenting Networks Incrementally for Lifelong Learning",
            "review": "The authors provided a training scheme that ensures network retains old performance as new data sets are encountered (e.g. (a) same class no drift, (b) same class with drift, (c) new class added). They do this by incrementally adding FC layers to the network, memory component that stores previous precomputed features, and the objective is a coupling between classification loss on lower level features and a feature-loss on retaining properties of older distributions. The results aren't very compelling and the approach looks like a good engineering solution without strong theoretical support or grounding. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}