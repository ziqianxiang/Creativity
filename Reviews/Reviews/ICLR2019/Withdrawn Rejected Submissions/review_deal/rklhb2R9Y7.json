{
    "Decision": {
        "metareview": "This paper proposes to combine rewards obtained through IRL from rewards coming from the environment, and evaluate the algorithm on grid world environments. The problem setting is important and of interest to the ICLR community. While the revised paper addresses the concerns about the lack of a stochastic environment problem, the reviewers still have major concerns regarding the novelty and significance of the algorithmic contribution, as well as the limited complexity of the experimental domains. As such, the paper does not meet the bar for publication at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "heuristic combining environment rewards with an IRL-style rewards",
            "review": "The draft proposes a heuristic combining environment rewards with an IRL-style rewards recovered from expert demonstrations, seeking to extend the GAIL approach to IRL to the case of mismatching action spaces between the expert and the learner. The interesting contribution is, in my opinion, the self-exploration parameter that reduces the reliance of learning on demonstrations once they have been learned sufficiently well.\n\nQuestions:\n\n- In general, it's known that behavioral cloning, of which this work seem to be an example in so much it learns state distributions that are indistinguishable from the expert ones, can fail spectacularly because of the distribution shift (Kaariainen@ALW06, Ross&Bagnell@AISTATS10, Ross&Bagnell@AISTATS11). Can you comment if GAN-based methods are immune or susceptible to this?\n   \n- Would this work for tasks where the state-space has to be learned together with the policy? E.g. image captioning tasks or Atari games.\n\n- Is it possible to quantify the ease of learning or the frequency of use of the \"new\" actions, i.e. $A^l \\setminus A^e$?. Won't learning these actions effectively be as difficult as RL with sparse rewards? Say, in a grid world where 4-way diagonal moves allow reaching the goal faster, learner is king 8-way, demonstrations come from a 4-way expert, rewards are sparse and each step receives a -1 reward and the final goal is large positive -- does the learner's final policy actually use the diagonals and when?\n\nRelated work:\n   \n- Is it possible to make a connection to (data or policy) aggregation methods in IL. Such methods (e.g. Chang et al.@ICML15) can also sometimes learn policies better than the expert.\n\nExperiments:\n- why GAIL wasn't evaluated in Fig. 3 and Fig. 4?\n\nMinor:\n- what's BCE in algorithm 1?    \n- Fig.1: \"the the\"\n- sec 3.2: but avoid -> but avoids\n- sec 3.2: be to considers -> be to consider\n- sec 3.2: any hyperparameter -> any hyperparameters\n- colors in Fig 2 are indistinguishable\n- Table 1: headers saying which method is prior work and which is contribution would be helpful\n- Fig. 3: if possible try to find a way of communicating the relation of action spaces between expert and learner (e.g. a subset of/superset of). Using the same figure to depict self-exploration make it complicated to analyse.\n- sec 3.2: wording in the last paragraph on p.4 (positive scaling won't _make_ anything positive if it wasn't before)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "State only demonstrations but in deterministic environments ",
            "review": "The paper proposes to combine expert demonstration together with reinforcement learning to speed up learning of control policies. To do so, the authors modify the GAIL algorithm and create a composite reward function as a linear combination of the extrinsic reward and the imitation reward. They test their approach on several toy problems (small grid worlds). \n\nThe idea of combining GAIL reward and extrinsic reward is not really new and quite straight forward so I wouldn't consider this as a contribution. Also, using state only demonstration in the framework of GAIL is not new as the authors also acknowledge in the paper. Finally, I don't think the experiments are convincing since the chosen problems are rather simple. \n\nBut my main concern is that the major claim of the authors is that they don't use expert actions as input to their algorithm, but only sequences of states. Yet they test their algorithm on deterministic environments. In such a case, two consecutive states kind of encode the action and all the information is there. Even if the action sets are different in some of the experiments, they are still very close to each other and the encoding of the expert actions in the state sequence is probably helping a lot. So I would like to see how this method works in stochastic environments. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "First review",
            "review": "This paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). \nSpecifically, the paper proposes \"self exploration\", in which it mixes the imitation reward with environment reward from the MDP itself in a gradual manner, guided by the rate of learning.\nIt also proposes a couple of variants of imitation rewards, RTGD and ATD inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (CSD, SSD), which are based on either consecutive or single states, which constitute the baseline methods for comparison.\nThe authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. \nSome moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. \nThere is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.\nThe empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. \nAlso the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}