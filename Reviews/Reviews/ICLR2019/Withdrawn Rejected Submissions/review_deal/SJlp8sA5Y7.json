{
    "Decision": "",
    "Reviews": [
        {
            "title": "A paper on mapping distributions to distributions using recurrent nets",
            "review": "This paper proposed a method for creating neural nets that maps historical distributions onto distributions.  The scheme relies on a recurrent net that takes a short list of distributions (3-5) and outputs a new distribution.  The authors apply this method to several distribution prediction tasks.  \n\nI have the following criticisms of this work:\n\n1) The test problems feel a bit like toy problems.  The distributions studied here are generally simple enough to be represented as a Gaussian mixture model (in fact, the experiments use input data distributions that are obtained via a GMM or kernel density estimation).  One would hope that the power of neural nets could be used for more complex distributions.  It would be interesting to see how this method performs on more complex / higher dimensional tasks.\n\n2) The improvement over a DRN is apparent, although not always significant.  Also, I'm curious what architectures were used for the experiments.  I was surprised to see how poorly an RNN performed, and I'm curious how much architecture search went into designing this network?\n\n3) In general, I'd like to see more meaty details about how networks propagate information between layers, how the input distributions are represented, how loss functions are calculated, and what network architecture look like.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A quite preliminary study",
            "review": "In this paper, the authors propose Recurrent Distribution Regression Network (RDRN), which uses a recurrent architecture upon a previous model Distribution Regression Network (DRN). The authors explain the technical details of the model, but fail to describe any motivation of their model as well as the key assumptions in the model. The missing part, to me, is the most important part of this paper in terms of technical contribution. Overall, some description of the technical detail is not accurate, for example, the authors used \"the density of the pdf\", which should be either \"the density of the distribution\" or \"the probability density function\".\nAnother place \"each node in the network encodes an entire probability distribution\", it is unclear to me what is an \"entire probability distribution\". Do you mean conditional distribution or marginal distribution? Does this model captures joint distribution? In addition, the paper seems to be quite preliminary with a number of typos and grammar errors. For example, \"Forward propagation is performed layer-wise obtain the output prediction.\"\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An Efficient Network for Predicting Time-Varying Distributions ",
            "review": "This paper is on regressing over probability distributions, extending from previous work, this paper studies time varying distributions in a recurrent neural network setting. Given state-of-the-art in this topic of regressions over networks, embedding the idea into a recurrent neural setting is novel, but only marginally so. The paper is well-written and easily readable. Four problems are used to illustrate the idea: (a) a synthetic Gaussian data in which the mean of a fixed-variance Gaussian is allowed to wander; (b) an Ornstein-Uhlenbeck process arising in a climate model; (c) image evolution data of images of cars; and (d) stock (distribution) prediction. That is a good coverage of different types of problems which add strength to the idea explored in the manuscript. I have a couple of issues with the empirical work reported in the paper: (i) in the first example of the roving-mean Gaussian, with the variance fixed, the task is simply tracking a sine wave. How then are the distribution networks able to outperform the vanilla RNN (or even the MLP) - the fact that there is a distribution should not matter (what am I missing?). (ii) for the stock prediction problem, it looks like the RDRN shows worse performance than DRN both in terms of mu and sigma^2 in Fig. 4. [0.37 and 0.45; and 0.23 (RDRN) and 0.36(DRN)] Why is this so? Moreover, the problem itself makes me uncomfortable and needs better motivation. The distributions are constituent stocks (returns on them) of DOW, Nikkei and FTSE. I would think there is not much overlap between the constituent assets, and, when viewed as distributions, there is no natural ordering among them. So just by re-ordering the list of assets, we get a different-looking distribution we will attempt to fit. Is this not an issue? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}