{
    "Decision": {
        "metareview": "the proposed approach of predicting k nearest neighbouring examples as an auxiliary task is an interesting idea. however, the submission should have studied further on how those examples are predicted (e.g., sequence prediction is one, but you could try set prediction, or so on) rather than how sequential prediction of nearest neighbours is done together with different types of classifiers (many of which are arguably not necessarily suitable for classification,) which was a sentiment shared by all the reviewers. \n\nmore careful investigation of different ways in which nearest neighbour prediction could be incorporated and more careful/thorough analysis on how the incorporation of this auxiliary task changes the behaviours or properties of the representation would make it a much better paper (also with clearer writing.)",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "nearest neighbour prediction as an auxiliary task"
    },
    "Reviews": [
        {
            "title": "Training to predict kNN features and labels improves generalization.",
            "review": "This work uses sequence-to-sequence and memory network neural nets to learn a\nnetwork that not only predicts a label, but also predicts nearest neighbors and\ntheir label.  The intuition is that by training on a related but harder task,\nthe network is forced to learn not just about sampled points but about the\nbehavior in regions around the actual training examples.\n\nTheir work shows that imposing these additional requirements on the model does\nresult in better performance on unseen data, where only the label of the unseen\ndata point is required as output.  They show that learning more about the \nglobal {feature,label} distribution improves F1 scores, and suggest that their\nmethods can be used as an example generator for datasets with class imbalance.\n\nThe writing was clear.  I had only 1 misunderstanding that cause me trouble,\nnamely the first sentence of \"Classification\", where I might put the word\n'benchmark' up front rather than at the end of this somewhat long sentence.\nBy the time 'knn benchmark' appeared some paragraphs later, I realized I\nhad missed something, and had to backtrack.\n\nFigs 1--3 are only viewable on screen with fairly extreme magnification. I\nfound different viewers varied in legibility at these extreme zoom settings.\nHowever, when expanded so y-axis numbers could be deciphered, it turned out\nthat actual improvements in F1 score were rather modest.\n\nFor 4 datasets, their F1 scores of their best method, V2VSLS, improved by ~ 1.5\nto 6%.  They found that their methods were less affected by out-of-core\ncomputation than the benchmark kNN F1 scores.\n\nI did have some questions about the problem formulation.  I did not really\nunderstand why the models wanted to reproduce the exact ordering of the\nnearest neighbors, other than this is easy to formulate.  This makes sense\nfor n.n. label sequences, where further points might be in some neighboring\nclass.  But for predicting the feature vectors of n.n., it seems that the\nexact order is not robust, in the sense very small distance changes can cause\nabrupt shifts in the target nearest-neghbor ordering.  Is there some way\nfor me to understand why this does not pose a problem?  Or is there perhaps\na way to make the loss function a bit less dependent on the precise order\nof the generated feature vector sequence?\n\nIn the computational experiments, with the choice of datasets, it was often\nhard for me to judge how much different aspects of their 4 network structures\nwere really being excercised.  The main issue is the all problems used only\n2--3 classes.  I could not guess what fraction of data points had actual label\nchanges within the 5 n.n..  For many datasets, \"same class\" might be a pretty\ngood predictor of nearest neighbor label.\n\nAlternatively, one can consider the other extreme, of very many classes.\nDoes it still make sense to try to predict the order of nearest neighbor labels in\nsuch a setting?\n\nThe authors provide evidence of some performance improvement, but I would\nencourage them to provide some intuition about what the networks are actually\nlearning.  Some of this can be done with their existing data.\n\nFor example, on average, what are the distances from predicted feature vectors\nto the actual nn feature vectors?  If the feature sequence is typically badly\npredicted, then this might allow the authors to propose that the network is\n*actually* learning some simpler features of the the distributions underlying\nan actual {feature,label} sequence.  This might allow simplified training losses,\nbased, on things like direction and distance to same-label cluster center,\ndirection+distance to average same-class nn.s, direction and distance to\nclosest differently labeled cluster, etc.  Or does their data suggest that\ntheir models are actually learning the precise nearest neighbor ordering?\n\nSuch considerations might be able to improve the OOC training, since\n\"global\" aspects of the distribution features (like \"cluster center\")\nremain approx. valid as training batches changes.\n\nThe other question I had was with the oversampling proposition.  The extent\nof class imbalance in the datasets is not described.  Perhaps it belongs in\nTable 1.  Their approach seems a lot of work for modest gains usually available\nwith oversampling.  Can the authors provide any guideline for how many members a\nminority class should have before using their sequence-to-sequence technique?\n\nPros: they improve generalization to unseen data.\nCons: their models are considerably more complex, and they do not analyze their\ndata in enough detail to suggest whether their complexity is necessary, or perhaps\ncould be reduced.  Figures are too small (many unreadable in printed copy).\nDatasets have very few classes and the extent to which nearest neighbors are\nof different class not reported.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "need more clear writing and strong experimental results",
            "review": "I had a hard time understanding this paper. The approach is clearly about combining kNN with neural networks, but it wasn’t clear how it is done. After reading the whole paper, my guess is that kNN is done on raw data first, and then its results are used for training a neural network. In particular, a network is trained to predict the labels of neighboring samples, which are obtained by kNN beforehand. A simple figure explaining it in the introduction would be very helpful since the idea is not that complex. \n\nAlso, the authors also fail to give an adequate explanation on why the method works. The only reason I can think of is that this regularization forces the model to detect if a sample near a class boundary. This is because when a sample is far from boundaries and surrounded by samples of the same class, the model would simply predict that class label. The same is true when predicting out-of-sample vectors because the average position of K'th neighbor is likely to overlap with the input sample due to the randomness of sampling. \n\nI don’t really see why a memory-based model is introduced. The external memory is used for holding random samples. It is not clear how the model can use such random samples for making predictions. Also, the authors give no explanation to why it should help. The results also don’t show the benefit of a memory-based model. Maybe the authors should look into models that output a set instead of a sequence since neighbors are more like a set in their structure.\n\nThe experimental results show clear improvements over basic baselines, so the method is doing some regularization. However, I'm not very familiar with datasets used here and their state-of-art. They are relatively low dimensional compared to usual datasets used in deep learning. It is not clear if the method can scale to high dimensional data such as images. The vanilla neural network is not really a strong baseline here. Since the authors proposed a regularization technique, it should be compared with other regularization techniques in neural networks.\n\nPros:\n- a simple idea\n- encouraging experimental results\n\nCons:\n- confusing read\n- no clear intuition is given\n- restricted to low-dimensional datasets\n- strong baselines needed\n- the plots are too small to see (impossible to see when printed)\n\nOther comments:\n- The authors are using the term \"feature vector\" to refer to a data point. However, in the context of neural networks, \"feature vector\" often means a hidden representation of a neural network. \n- why repeat \"randomly draw B samples\" R times? why not directly sample RxB samples?\n- \"it is quite implausible that only affine ...\" any evidence to support this?\n- The model is not really \"sequence-to-sequence\" since the input is not a sequence.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Combine KNN of each training data point into the neural network models, but the motivation does not make sense.",
            "review": "To exploit the near neighbor/manifold features, this paper proposes to combine k-nearest neighbors of each training data point into the neural network models.  Specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning. Besides, the final label of the classification task will be learned, a sequence of nearest neighbor labels and a sequence of out-of-sample feature vectors (for oversampling) will be also learned in the same time, similar with the multi-task approaches. Since the proposed models are based k-nearest neighbor calculations, which is time-consuming, they also design an algorithm for the ‘out-of-core’ situation, say load a small portion of data each time to approximately calculate the neighbors. Experiments show that some proposed models work better than baselines in classification and oversampling.\nStrong points:\n(1) As similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.\n(2) The proposed models work well on the ‘out-of-core’ situation, which shows that the models are robust.\nConcerns or suggestions:\n(1) The training data $x$ is just one data point, it is not a sequence of data. So the idea to model it in a sequence to sequence setting does not make sense.\n(2) K-nearest neighbors are a set but not a sequence. To model them as a sequence is also strange. The i-th nearest neighbor does not necessarily dependent on the i-1-th nearest neighbor. For example, we consider the one-dimensional case, the focus data may lie between its first and second nearest neighbors. In this case, there is no clear sequence dependence from the second neighbor to the first neighbor. \n(3) The experiments are not sufficient. They only compare with some weak baselines, such as KNN. As the classification task, there are many state-of-the-art models. Besides of these standard classification models, we strongly suggest comparing with the previous method, Wang et al. (2017), which also proposes to combine the k-nearest neighbors into memory network models. I am surprised that the authors did not compare with this very related work. In my opinion, the idea of utilizing nearest neighbors as external memory in Wang et al. (2017) makes more senses.\n(4) The experimental results of some proposed sub-models (key parts of final models) are even worse than the basic kNN model. I should say that the results are not good enough to support the proposed methods. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}