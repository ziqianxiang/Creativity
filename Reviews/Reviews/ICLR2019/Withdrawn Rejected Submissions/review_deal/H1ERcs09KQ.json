{
    "Decision": {
        "metareview": "While this was a borderline paper, concerns about the novelty and significance of the presented work exist on the part of all reviewers, and no reviewer was willing to argue for acceptance. Many good points to the work exist, and a stronger case on these issues would greatly strengthen the paper overall. I look forward to a future submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview for representation learning paper"
    },
    "Reviews": [
        {
            "title": "Interesting and probably useful, but presentation needs work and there are some technical issues",
            "review": "This paper proposes using a variant of the nested CRP as a prior on the latent space of a variational autoencoder. The authors demonstrate that this approach is able to simultaneously learn a meaningful latent representation of high-dimensional data (text and images) and do hierarchical clustering in that space.\n\nPros:\n* The high-level idea is compelling.\n* The empirical results are compelling, and the evaluation is thorough.\n\nCons:\n* The prose is pretty rough. The paper is full of sentences like \"VAE-nCRP trade-off is the direct dependency modeling among clusters against the mean-field variational approach\" that don't convey their intended meaning (at least to me).\n* The random variable η seems completely superfluous. It only affects the likelihood through the level indicator l, but the marginal distribution p(l) = ∫_η p(η, l)dη \\propto α is tractable, since only one level is drawn per observation. (This is not the case for the traditional nCRP as used in topic modeling, since there a different level is chosen for each word.)\n* The novelty over the nCRP-VAE approach of Goyal et al. (2017) is pretty minor. The main difference seems to be that the model can select clusters at different levels, but I didn't quite get the intuition for why this should be desirable. In topic modeling, higher-level clusters tend to contain less-specialized words, and each document is a mix of specialized and general topics. But in this model, only one level is used to explain an entire image or document, and the idea that an entire image or document is much \"more specialized\" than another doesn't seem very intuitive to me.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "nested CRP plus neural network",
            "review": "The paper proposes using the nested CRP as a clustering model rather than a topic model. The clustering is on the latent vector input into a neural network for generating the observation. A variational approach is derived.\n\nThe proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it. A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time. From the generative model it seems every data point has its own Dirichlet vector on levels. For topic models this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn. My understanding is that this isn't being done here.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Relevant problem, sound solution and convincing experimental evaluation. But limited novelty. Incremental work.",
            "review": "The paper presents a novel hierarchical clustering method over an embedding space. In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt. The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods. \n\nThe paper address a relevant problem, which is of great interest for extracting knowledge from data. In general, the quality of the paper is high. The presented approach is based on a sound formalization of hierarchical clustering and deep generative models. The paper is easy to follow in spite of the technical difficulty. The experimental evaluation is really extensive. It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view. \n\nThe only issue with this paper is its degree of novelty, which is narrow. The proposed method adapt a previously presented hierarchical clustering method in the \"standard space\" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model. The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}