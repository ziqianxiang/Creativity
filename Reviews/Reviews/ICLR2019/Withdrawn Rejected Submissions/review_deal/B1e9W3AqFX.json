{
    "Decision": {
        "metareview": "This paper presents a novel idea of transferring gradients between tasks to improve multi-task learning in neural network models. The write-up includes experiments with multi-task experiments with text classification and sequence labeling, as well as multi-domain experiments. After the reviews, there are still some open questions in the reviewer comments, hence the reviewer decisions were not updated.\nFor example, the impact of sequential update in pairwise task communication on performance can be analyzed. Two reviewers question task relatedness and the impact of how and when it is computed could be good to include in the work. Baselines could be improved to reflect reviewer suggestions.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting MTL approach, but the work could be improved in the light of suggestions."
    },
    "Reviews": [
        {
            "title": "interesting paper on improving MTL using gradient communication",
            "review": "Paper summary: \nIn this paper, the authors propose a general framework for multi-task learning (MTL) in neural models. The framework is general for including some of the current neural models for MTL. Under the framework, the author propose a new method that could allow tasks to communicate each other with explicit gradients. Based on the gradients being communicated, the system could adjust the updates of one task based on the gradient information of the other task. Also, prior task relatedness information could be incorporated to the system. \n\nThe idea of incorporating passing gradients among tasks seems very interesting, which is new as far as I am aware of. Although the idea is simple, but it seems intuitive since purely aggregating gradient updates might have undesired cancelling effects on each other.  \n\nThere are some questions I have about this method. \n1.\tIâ€™m curious about how the sequential update in pairwise task communication affects the performance. \n2.\tAlso, how does sequential update nature of the method affect the training speed, as for now, the parameter update consists of two sequential steps which also involve changes to the traditional update rule. \n3.\tWhat is fast weight for and how it is used in (9)? It would be better if there are more details on how the update is carried out during the gradient communication.\n4.\tRegarding the relatedness for List-wise communication, is it possible to update the relatedness dynamically? Since the pre-computed relatedness might not always make sense. During the learning of the representations, the task relatedness could change in the process.\nThe system framework for MTL introduced by the authors seem to be kind of isolated to the method proposed. I feel that the framework is not quite easy to understand from the way it is presented.  From my perspective, the effectiveness of analyzing MTL methods using the framework seems a bit limited to me, as it serves more like a way of abstracting MTL models instead of analyzing it. Therefore, I feel the content devoted to that part might be too much.\n\nOverall, I think the paper is interesting although the method itself is relatively simple. And the direction of utilizing gradient communication among tasks seem interesting and could be further explored. But I do feel the organization of the paper is a bit too heavy on the framework instead of the methodology proposed. And more details of the algorithm proposed could be provided.\n\nOn a side note, I think the paper exceeds the required length limit of 10 pages if appendices are counted towards it.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "just packing exsiting algorithms",
            "review": "This paper tries to address the\"pretend-to-share\" problem by designing the gradient passing schemes in which the gradient updates to specific parameters of tasks are passed to the shared parameters. Besides, the authors summarize existing multitask learning algorithms in a framework called Parameters Read-Write Networks (PRAWN). \n\nPros:\n- The view of putting existing multi-task learning algorithms in a read-write framework is quite intriguing and inspiring.\n\nCons:\n- Motivation: The whole paper is assumed to address the \"pretend-to-share\" problem, while the authors never provide any evidence that such problem really exists for any other algorithm. It seems to be an assumption without any support.\n- Method:  \n   - Though the read-write framework is very interesting, the authors do not clearly present it, so that the readers can be totally get lost. For example, what do you mean by writing {\\Theta^{*r}_k - \\theta^{swr}_k}? In the line of structural read-op, where are \\theta_3 and \\theta_4  in the column of the constituent para. ? What do you mean by writing the equation (4)? How do you define g() in equation (8)? This is a research paper which should be as clear as possible for the readers to reproduce the results, rather than a proposal only with such abstract and general functions defined. \n   - In the list-wise communication scheme, you define the task relationship in equation (11). The problem is how do you evaluate the effectiveness of such definition, since massive works in multitask learning pursue to learn the task relationship automatically to guarantee the effectiveness instead of such heuristic definition. \n- Related works: The authors do not clearly and correctly illustrate the connections between this work and meta-learning/domain adaptation. To my best knowledge, meta-learning, including MAML (Finn et al. 2017), can obviously solve both in-task setting and out-task setting. In some sense, I think this work is almost equivalent to MAML. \n- Experiments: \n   - First, several state-of-the-art baselines including MAML and cross-stitch networks should be compared. Specifically, for the text classification dataset, there have been a lot of domain adaptation works discovering the transferable pivots (shared features) and non-pivots (specific features), which the authors should be aware of and compare in Table 3.  \n   - The Figure 5 is not clear to me, and so is the discussion. The authors try to explain that the updating direction of shared parameters for PGP-SR is an integration of two private updating directions. I tried hard to understand, but still think that Figure 5(a) is even better than Figure 5(b). The updating direction of the shared parameters is almost the same as the cyan line.\n- Presentation: there are so many grammatical errors and typos. For example,\n   - In the introduction, \"...datasets, range from natural\" -> \"...datasets, ranging from natural\"\n  - In the related work, \"and they propose address it with adversarial\" -> \"and they propose to address it with adversarial\"\n - In the beginning of Section 4, \" an general\" -> \"a general\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes that models for different tasks in multi-task learning cannot only share hidden variables but also gradients.\n\nPros:\n- The overall framework is theoretically motivated and intuitive. The idea of passing gradients for multi-task learning is interesting and the execution using fast weights is plausible.\n- The experiments are extensive and cover three different task combinations in different domains.\n- The results are convincing and the additional analyses are compelling.\n\nCons:\n- I would have liked to see a toy example or at least a bit more justification for the \"pretend-to-share\" problem that models \"collect all the features together into a common space, instead of learning shared rules across different tasks\". As it is, evidence for this seems to be mostly anecdotal, even though this forms the central thesis of the paper.\n- I found the use of Read and Write ops confusing, as similar terminology is widely used in memory-based networks (e.g. [1]). I would have preferred something that makes it clearer that updates are constrained in some way as \"writing\" implies that the location is constrained, rather than the update minimizing a loss.\n\nQuestions:\n- How is the weight list of task similarities \\beta learned when the tasks don't share the same output space? How useful is the \\beta?\n- Could you elaborate on what is the difference between pair-wise gradient passing (PGP) and list-wise gradient passing (LGP)\n\n[1] Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}