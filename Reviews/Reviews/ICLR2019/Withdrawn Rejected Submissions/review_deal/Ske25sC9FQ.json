{
    "Decision": {
        "metareview": "Positives:\n\nThe paper proposes an interesting idea: to study the effect on vulnerability to adversarial attacks of training for invariance with respect to rotations.\nExperiments on MNIST, FashionMNIST, and CIFAR10.\nAn interesting hypothesis partially borne out in experiments.\n\nNegatives:\n\nno accept recommendation from any reviewer\ninsufficient empirical results\nnot a clear enough message\nvery limited theoretical contribution\n\nAlthough additional experimental results on FashionMNIST and CIFAR10 were added to the initial very limited results on MNIST, the main claim of the paper seems to be somewhat weakened.  The effect of increased vulnerability to adversarial attacks as invariance is increased is less pronounced on the additional datasets.  This calls into question how relevant this effect is on more realistic data than the toy problems considered here.\n\nThe size of the network is not varied in the experiments.  If increased invariance results in poorer performance with respect to attacks, one possible explanation is that the invariance taxes the capacity of the network architecture.  Varying architecture depth could partially answer whether this is relevant.  Given the lack of theoretical contribution, more insights along these lines would potentially strengthen the work.\n\nThe title uses the term \"equivariance,\" which strictly speaking is when the inputs and outputs of a function vary equally, e.g. an image and its segmentation are equivariant under rotations, but classification tasks should probably be called \"invariant.\"\n\nThe reviewers were unanimous in not recommending the paper for acceptance.  The key concerns remain after the author response.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Area chair recommendation"
    },
    "Reviews": [
        {
            "title": "Interesting empirical study of CNN robustness, potentially good workshop paper",
            "review": "This paper empirically studies various CNN robustifying mechanisms aiming to achieve rotational invariance. The main finding is that such robustifying mechanisms may lead to lack of robustness against pixel-level attacks such as FGSM and its variants. The paper does a comprehensive job in studying relevant robustifying schemes and attacks strategies. However, the paper does not present sufficiently new information worthy of a regular conference paper, it can be a good workshop paper though for the Robust Learning community. Some analytical insights would really strengthen the work. Also, from an empirical standpoint, the authors need to consider other data sets beyond just the MNIST data set.   \n\nxxxxxxxxxxxxxx\n\nWhile I appreciate the authors' rebuttal and revisions, I still do not see sufficient contribution here worthy of a regular ICLR paper.  ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not good enough to accept",
            "review": "Using the dataset MNIST, the authors empirically studied the robustness of several rotation-equivariant neural network models(GCNN, H-Nets, PTN, et al.) to geometric transformation and small pixel-wise perturbations. Their experiments showed that the equivariant network models(StdCNNs, GCNNs, H-Nets, et al.) are robust to geometric transformation but vulnerable to pixel-wise adversarial perturbations. These findings help us understand the  neural network models better.\nHowever, this paper is not acceptable due to lack of innovation and novelty. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Robustness and Equivariance of Neural Networks",
            "review": "This paper empirically studies the robustness of equivariant CNNs to rotations as well as adversarial perturbations. It also studies their sample efficiency, parameter efficiency, and the effect of rotation- and adversarial augmentation during training and/or testing. \n\nThe main findings are:\n1) Rotation-equivariant networks are robust to small rotations, even if equivariance to small rotations is not directly built into the architecture\n2) Applying rotational data augmentation increases robustness to rotations\n3) Equivariant networks are more sample efficient than CNNs and outperform them for all dataset sizes.\n4) Applying rotational data augmentation decreases robustness to adversarial perturbations, and this effect is more pronounced for GCNNs.\n\nIf true, this is a valuable addition to the literature. It is one of the first independent validations of claims regarding sample complexity and accuracy made by the authors of the various equivariant network papers, performed by a party that does not have their own method to promote. Many of the findings do not have an obvious explanation, so the data from this paper could conceivably prompt new theoretical questions and investigations.\n\nThe authors chose to highlight one finding in particular, namely that GCNNs become more sensitive to adversarial perturbations as they are trained on more heavily rotation-augmented data. However, this appears to be true for both CNNs and GCNNs, the difference being only in degree (see fig 4, 10, 11). This is not apparent from the text though, as e.g. the abstract notes that \"robustness to geometric transformations in these models [equivariant nets] comes at the cost of robustness to small pixel-wise perturbations\".\n\nSince HNets, GCNNs and RotEqNets should be exactly equivariant to 90 degree rotations (and some others, perhaps), it is surprising that figure 1 shows a continuing decline in performance with bigger and bigger random rotations. If the network is made rotation invariant through some pooling layer at the end of the network, one would expect to see a decline in performance up to 45 degrees, followed by an increase back to baseline at 90 degrees, etc. \n\nPolar transformer networks achieve good results in fig. 1, but I wonder if this is still true for rotations around points other than the origin.\n\nSince CNNs and GCNNs differ in terms of the number of channels at a certain number of parameters, and differ in terms of number of parameters at a certain number of channels, it could be that channel count or parameter count is the more relevant factor, rather than equivariance. So it would be good to make a scatterplot where each dot is a network (either CNN or GCNN, at various model sizes), the x-axis is parameter count (or in another plot, 2d channel count), and the y-axis corresponds to the accuracy. This can be done for various choices of augmentation / perturbation. The type of network (CNN or GCNN) could be color coded. If indeed the CNN/GCNN variable is relevant, that should be clearly visible in the plot, and similarly if the parameter count or channel count is relevant. One could also do a linear regression of accuracy or log-accuracy or something using CNN/GCCN, param-count, channel-count as covariates, and report the variance explained by each. \n\nIn several plots, e.g. fig 4, 8, the y-axes do not have the same range, making it hard to compare results between subplots. \n\nThe experiments have some weaknesses. For one thing, it seems like each accuracy value reported comes from a single training run. It would be much preferable to plot mean and standard deviation / error bars. Another weakness is that all experiments are performed on MNIST. Even just a simple validation of the main findings on CIFAR would significantly strengthen the paper.\n\nBecause of the limited scope of the experiments, it is not clear to me how generalizable and robust the experimental results are. With deep network performance it can be hard to know what the relevant hyperparameters are, and so we may well be reading tea leaves here.\n\nIt is also unfortunate that no explanation for the observed phenomena is available. However, it is conceivable that the findings presented in this paper could help researchers who are trying to understand adversarial attacks / robustness, so it is not a fatal flaw. I am certainly glad the authors did not make up some unsupported story to explain the findings, as is all too common in the literature these days.\n\nOverall, I consider this a borderline paper, and am tending towards a reject. My main considerations are:\n1. Uncertainty about generalizability\n2. Uncertainty about usefulness to practitioners or theorists (admittedly, this is hard to predict, but no clear use-case is available at this point)\n3. A lot of data, but no clear central finding of the paper",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}