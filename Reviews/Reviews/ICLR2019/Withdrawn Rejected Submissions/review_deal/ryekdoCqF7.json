{
    "Decision": {
        "metareview": "The reviewers and the AC acknowledge the paper contains interesting ideas on using an incremental sequence of multiple generators to capture the diversity of the examples. However, the reviewers and the AC also note that the potential drawback of the paper is the lack of evaluation with other metrics such as inception score, FID score, etc. Therefore the paper is not quite ready for acceptance right now, but the AC encourages the authors to submit to other top venues with more thorough experiments. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Incremental training of GANs",
            "review": "The paper introduces an incremental training method for GAN's for capturing the diversity of the input space. The paper demonstrate that the proposed method allows smaller distances between the true and generated distribution. I find the idea interesting, but fear that the 60-100 small ensemble models could be replaced by a larger model.\n\nI am curious about why we need incremental training when it seems like we could directly train all the networks jointly. The corresponding generative model is simply stronger so all the convergence arguments would still hold. Is the statistical distance a reasonable estimate for you to determine whether you need an additional generator for incremental training?\n\nAlso what are the generator architectures for the experiments? How can you put 60-100 generators within the GPU memory? The latent variable dimension seem to be only 1 for each of your generator? That seems to be seriously handicapping the capacity of each individual generator (to just some data points), so the ensemble distribution might be obtained simply by using a larger dimension z?\n\nThere are also other measurements that are used by the GAN community, such as inception score, FID score and samples. It seems also reasonable to verify the effectiveness of this method on CIFAR or LSUN datasets, where the method would have a greater improvement because the data distributions are more complex.\n\nMinor points:\n- How do you measure the \"Wasserstein distance\" for high-dimensional distributions? \n- What not set $\\omega_i$ to be always 1? The subsampling process introduced in Algorithm 2 seem to enforce this, and you do this for all the experiments.\n- Fix citation typos.\n- Fix \\mathbf for vector quantities, such as x and z.\n- Since the generative models have the same architecture, does the non-convex argument becomes moot when you have a mixture of 2 generators?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A sequential training of GANs and some theory associated with it",
            "review": "This paper proposes a method for ensembling GAN’s for capturing diversity in the target space. This done by a convex combination of GAN’s that are sequentially trained by trying to approximate the real distribution by fixing the previous generators. The paper theoretically shows that this approach converges to the optimal theoretical distribution.\n\nComments \n\n1)  What will be the performance of Original GAN and Incremental GAN by finding optimal weighting ($w_i$) parameters for each of them?\n2)  Can you increase the number of parameters of the generator by no of generators used in the incremental GAN’s and compare the performance?\n3) The abstract first line you have written ‘possibly distribution’ instead of ‘probability distribution’\n4) Table 1 ‘Incremental GAN’ doesn’t show consistently improved performance in comparison ‘Original GAN’. Can you train a few more generators and verify it?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Using multiple generators to properly capture the target (data) distribution",
            "review": "In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.\n\n1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?\n\n2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.\n\n3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?\n\n4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?\n\n5) is the Original GAN exactly the Ian's original GAN or WGAN?\n\n6) Have you tried this approach using small sized generator (having  small number of parameters)?\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}