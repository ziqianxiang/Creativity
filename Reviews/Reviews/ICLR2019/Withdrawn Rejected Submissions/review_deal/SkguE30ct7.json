{
    "Decision": {
        "metareview": "This paper formulates the recommendation as a model-based reinforcement learning problem. Major concerns of the paper include: paper writing needs improvement; many decisions in experimental design were not justified; lack of sufficient baselines; results not convincing. Overall, this paper cannot be published in its current form.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Improvements needed"
    },
    "Reviews": [
        {
            "title": "Interesting problem and ideas, manuscript may not be ready for publication yet",
            "review": "This paper proposes to frame the recommendation problem as one of (model-based) RL. The two main innovations are: 1) a model and objective for learning the environment and reward models; 2) a cascaded DQN framework for reasoning about a combinatorial number of actions (i.e., which subset of items to recommend to the user).\n\nThe problem is clearly important and the authors' approach focuses on solving some of the current issue with deployment of RL-based recommenders. Overall the paper is relatively easy to follow, but the current version is not the easiest to understand and, in particular, it may be worth providing more intuitions (e.g., about the GAN-like setup). I also found that several decisions are not properly justified. The novelty of this paper seems reasonably high but my impression is that other/stronger baselines would make the study more convincing. Copy-editing the paper would also greatly improve readability.\n\nDetailed comments:\n- I am not clear on whether or not in the proposed model, users are \"allowed\" to not click on a recommendation. It sounds like the authors in fact allow it but I think that could be made clearer.\n\n- Section 4. I am not sure that using the Generative Adversarial Network terminology is useful here. Specifically, it is not clear what is your generative model over (I imagine next state and reward?).\n  \n- Remark in Section 4.1: It seems like a user not clicking on something is also useful information. Why not model it?\n  \n- I am a bit unclear on the value of Lemma 1. Further, what are the assumptions behind it? (also what is this temperature parameter eta?)\n  \n- In Section 4.2, the size of your model seems to grow linearly with the number of user interactions. That seems like a major advantage of RNNs/LSTMs. In practice, I imagine you cull the history at some fixed point?\n  \n- What is the advantage of learning a reward? E.g., a very simple reward would be to give a positive reward if a user clicks on a recommended item and a negative reward otherwise. What does your learned reward allow beyond this?\n  \n- Section 4.3. I also found Section 4.3 to be relatively unclear. I find that more intuition would be helpful.\n\n  Also, if Eq. 7 is equivalent to Eq. 8, then why is the solution of 8 used only to initialize 7? I guess it may have to do with not finding the global optimum.\n\n- Your cascading DQN idea seems like a good one. It would be nice to check if the constraints are correctly learned. If not, this seems like it would do not better than a greedy action-by-action solution. Is that correct?\n  \n- In Section 6.1, it would be good to discuss the pre-processing in the main text since it's pretty important to understand the study (e.g., evaluate is impact).\n  \n- In 6.2, your baselines seem a bit weak. Why not compare to more recent CF models (e.g., including Session-Based RNNs which you cite earlier)?\n  \n- Related work: it would probably be good to survey some of the multi-arm bandit literature. There is also some CF-RL work which should be cited (perhaps there are a few things in there that should be compared to in Section 6.3 & 6.4). \n\n- Section 6.2 and Table 1. I believe that Recall@k is most common in recommendation-systems-for-implicit-data literature. Or, are you assuming that what people do not click on are true negatives? This doesn't seem quite right as users are only allowed to click on a single item.\n\n- In Section 6.3, could you clarify how do you learn your reward model that is used to train the various methods?\n\n- There are many typos and grammatical errors in the paper. I would suggest that the authors carefully copy-edit the manuscript.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "This paper belongs to the space of treating recommendation as a reinforcement learning problem, and proposed a model-based (cascaded DQN) approach, using a generative adversarial network to simulate user rewards.\n\nPros:\n+ proposed a set of cascading Q functions, to learn a recommendation policy\n+ unified min-max optimization to learn the behavior model and the reward function\n+ interesting idea of using generative adversarial networks to simulate user rewards.\n\nCons: \n- in Figure 6 no comparison with model-free (policy-gradient type) of approaches\n- there is not a lot of detail on the value of the generative adversarial network for the user behavior dynamics, thus this prevents the reader from fully understanding the contribution\n- only 2 datasets are used\n- only 100 users for test users seems few\n- why only 1000 active users were sampled from MovieLens?\n\nPersonally, I would prefer less details on formulating the recommendation problem as an RL problem (as there have been other papers before with a similar formulation) and more detail  on the simulation user reward model, and in general in sections 4 and 5. Also, the experiments could be strengthened.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Neural Model-Based Reinforcement Learning for Recommendation",
            "review": "The authors propose a deep reinforcement learning based recommendation algorithm. Instead of manually designing reward function for RL, a generative adversarial network was proposed to learn the reward function based on user's dynamic behavior. The authors also try to provide an efficient combinatorial recommendation algorithm by designing a cascade DQN. The authors hold their experiments on the Movielens and Ant Financial news dataset. The authors adopt logistic regression (LR) and collaborative competitive filtering(CCF) as comparison baseline to evaluate recommendation performance. The authors also compared their proposed RL policy CQDN with LinUCB.\n\n[Pros in Summary]\n1. Recommendation in the deep neural network based RL is a hot topic.\n\n2. The motivation for using a self-learned rewards function and provide efficient combinatorial recommendation is interesting.\n\n[Cons in Summary]\n1. The motivations/claimed contributions are not well supported/illustrated by the proposed algorithm or experiments.\n\n2. Some assumptions may not be realistic.\n\n3. The experiment is not sufficient without enough state of art baselines.\n\n4. The writing of this paper needs improvement.\n\n[Thoughts, Questions, and Problems in Details]\n1. The idea of using a learned reward function instead of manually defined one sound sweet. But based on (7) and (8), the reward function is essentially giving more rewards for the action that the user really clicks on. How much difference is there compared with traditional manual reward design of giving a click with a reward of 1, especially given the circumstance that a lot of manual intervention is actually used in designing loss function like (7)?\n\nMoreover, in the experiment, there is no comparison experiment evaluating the difference between using a self-learned reward function vs. a traditional manual designed reward function.\n\n2. The assumption \"in each pageview, users are recommended a page of k items and provide feedback by clicking on only one of these items; and then the system recommends a new page of k items\" does not sound realistic. What if the users click on multiple items?\n\n3. The combinatorial recommendation is useful in the recommendation setting. But it is also important to get the correct ranking order for items from the recommendation list, ie, the best item should rank on the top of the list. Is this principal guaranteed in the combinatorial recommendation proposed in this paper? It is not discussed in this paper.\n\n4. The authors claim to provide an efficient combinatorial recommendation but fail to provide any computational complexity analysis or providing any analysis on training or serving time. Is the proposed algorithm computationally practical to be deployed in a real system?\n\n5. The experiments are too weak because the baselines are old and state of art methods are missing from the comparison.\n\n6. Typos and grammar errors across the paper, to name a few\n\n\"we will also estimate a user behavior model associate with the reward function\"\n\n\"a model for the sequence of user clicking behavior, discussion its parametrization and parameter estimation.\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}