{
    "Decision": {
        "metareview": "Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The most significant concern raised is that there does not seem to be an adequate research contribution. Moreover, unsubstantiated claims of novelty do not adequately discuss or compare to past work.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Lacks demonstrated research contribution beyond past work"
    },
    "Reviews": [
        {
            "title": "An interesting problem : active learning for anomaly detection; method suffering from a lack of novelty; questions about experiments",
            "review": "The paper provided a convincing and intuitive motivation regarding the need for active learning in unsupervised anomaly detection. \nHowever the proposed approach of requesting expert feedback for the top ranked anomalies is straightforward and unsurprising, given past work on active learning. \nThe experiments on synthetic data are also unsurprising. Moreover these are based on a questionable premise: the instances that are \"hard\" to classify are treated as anomalies. This is not very realistic.\nRegarding the real data experiments: In Table 1 the results for DAE_uai are based on which budget b?  How does the result vary with b? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper that can be significantly improved by a better organization.",
            "review": "This is an interesting paper on a topic with real-world application: anomaly detection.\n\nThe paper's organization is, at times quite confusing:\n- the introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. Two additional paragraphs, covering in an intuitive manner both the proposed approach & the main results, would dramatically improve the paper's readability\n- section 2.1 starts quite abruptly with he two Lemmas 7 and Theorem 3 (which, in fact, is Theorem 1). This section would probably read a lot better without the two Lemmas, as the authors only refer to the main result in the Theorem. The second, intuitive part of 2.1 is extremely helpful.\n- it is unclear why the authors have applied the approach in \"4.3\" only to a single dataset, rather than all the 11 datasets\n\nOther comments:\n- please change the color schemes for Figures 3 & 4, where the red/orange (Fig 3) and various blues (Fig 4) are difficult to distinguish \n- bottom of page 3: \"are rare as expected\" --> \"are as rare as expected\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Active anomaly detection technique employing existing approaches and lacking appropriate literature review",
            "review": "(Since the reviewer was unclear about the OpenReview process, this review was earlier posted as public comment)\n\nMost claims of novelty can be clearly refuted such as the first sentence of the abstract \"...This work presents a new approach to active anomaly detection...\" and the paper does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.\n      \n      1. Related Works: \"...active anomaly detection remains an under-explored approach to this problem...\"\n          - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:\n            - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, \"Ai2: Training a big data machine to defend,\" International Conference on Big Data Security, 2016.\n        \n      2. \"To deal with the cold start problem, for the first 10 calls of select_top...\":\n          - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)\n        \n      3. Many arbitrary hyper parameters as compared to simpler techniques:\n          - The number of layers, nodes in hidden layers.\n            - The number of instances (k) per iteration\n            - The number of pretraining iterations\n            - The number of times the network is retrained (100) after each labeling call\n            - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).\n        \n      4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.\n      \n      5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:\n          - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (pradeepr/arxiv npmix v.pdf)\" target=\"_blank\" rel=\"nofollow\">https://www.cs.cmu.edu/ pradeepr/arxiv npmix v.pdf)\n          - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)\n          - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)\n          - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)\n          \n      6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.\n        - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.\n        \n      7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).\n        \n      8. Does not compare against current state-of-the-art Tree-based AAD\n          - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.\n        \n      9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.\n      \n      10. Does not propose any other query strategies than greedily selecting top.\n      \n      11. Question: Does this support streaming?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}