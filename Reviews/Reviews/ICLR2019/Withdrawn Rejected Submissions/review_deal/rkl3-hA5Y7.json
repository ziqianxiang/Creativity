{
    "Decision": {
        "metareview": "This paper proposes the use of holographic reduced representations in language modeling, which allows for a cleaner decomposition of various linguistic traits in the representation. Results show improvements over baseline language models, and analysis shows that the representations are indeed decomposing as expected.\n\nThe main reviewer concern was the lack of strength of the baseline, although the authors stress that they were using the default baseline from TensorFlow, which seems like it will be reasonable to me. Another concern is that there is other work on using HRR to disentangle syntax and semantics in representations for language (e.g. \"Distributed Tree Kernels\" ICML 2012, but also others), that has not been considered. \n\nBased on this, this seems like a very borderline case. Given that no reviewer is pushing strongly for the paper I'm leaning towards not recommending acceptance, but I could very easily see the paper being accepted as well.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Interesting task, but a lack of satisfaction with the results"
    },
    "Reviews": [
        {
            "title": "Decomposed Linguistic Representation with Holographic Reduced Representations ",
            "review": "The paper proposes a new approach for neural language models based on holographic reduced representations (HRRs). The goal of the approach is to learn disentangled representations that separate different aspects of a term, such as its semantic and its syntax. For this purpose the paper proposes models both on the word and chunk level. These models aim disentangle the latent space by structuring the latent space into different aspects via role-filler bindings.\n\nLearning disentangled representations is a promising research direction that fits well into ICLR. The paper proposes interesting ideas to achieve this goal\nin neural language models via HRRs. Compositional models like HRRs make a lot of sense for disentangling structure in the embedding space. Some of the experimental results seem to indicate that the proposed approach is indeed capable to discover rough linguistic roles. However, I am currently concerned about different aspects of the paper:\n\n- From a modeling perspective, the paper seems to conflate two points: a) language modeling vie role-filler/variable-binding models and b) holographic models as specific instance of variable bindings. The benefits of HRRs (compared e.g., to tensor-product based models) are likely in terms of parameter efficiency. However, the benefits from a variable-binding approach for disentanglement should remain across the different binding operators. It would be good to separate these aspects and also evaluate other binding operators like tensors products in the experiments.\n\n- It is also not clear to me in what way we can interpret the different filler embeddings. The paper seems to argue that the two spaces correspond to semantics and syntax. However, this seems in no way guaranteed or enforced in the current model. For instance, on a different dataset, it could entirely be possible that the embedding spaces capture different aspects of polysemy.  However, this is a central point of the paper and would require a more thorough analysis, either by a theoretical motivation or a more comprehensive evaluation across multiple datasets.\n\n- In its current form, I found the experimental evaluation not convincing. The qualitative analysis of filler embeddings is indeed interesting and promising. However, the comparisons to baseline models is currently lacking. For instance, perplexity results are far from state of the art and more importantly below serious baselines. For instance, the RNN+LDA baseline from Mikolov (2012) achieves already a perplexity of 92.0 on PTB (best model in the paper is 92.4). State-of-the-art models acheive perplexities around 50 on PTB. Without an evaluation against proper baselines I find it difficult to accurately assess the benefits of these models. While language modeling in terms of perplexity is not necessarily a focus of this paper, my concern translates also to the remaining experiments as they use the same weak baseline.\n\n- Related to my point above, the experimental section would benefit significantly if the paper also included evaluations on downstream tasks and/or evaluated against existing methods to incorporate structure in language models.\n\nOverall, I found that the paper pursues interesting and promising ideas, but is currently not fully satisfying in terms of evaluation and discussion.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Back to the past",
            "review": "This paper is very interesting as it seems to bring the clock back to Holographic Reduced Representations (HRRs) and their role in Deep Learning. It is an important paper as it is always important to learn from the past. HRRs have been introduced as a form of representation that is invertible. There are two important aspects of this compositional representation: base vectors are generally drawn from a multivariate gaussian distribution and the vector composition operation is the circular convolution. In this paper, it is not clear why random vectors have not been used. It seems that everything is based on the fact that orthonormality is impose with a regularization function. But, how can this regularization function can preserve the properties of the vectors such that when these vectors are composed the properties are preserved.\n\nMoreover, the sentence \"this is computationally infeasible due to the vast number of unique chunks\" is not completely true as HRR have been used to represent trees in \"Distributed Tree Kernels\" by modifying the composition operation in a shuffled circular convolution. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel approach to learning decomposable representations; some unclear parts and questionable validity; weak performance",
            "review": "\nSummary:\n========\nTheis paper proposes a method for learning decomposable representations in the context of a language modeling task. Using holographic reduced representations (HRR), a word embedding is composed of a role and a filler. The embedding is then fed to an LSTM language model. There is also an extension to chunk-level representations. Experimentally, the model achieves perplexity comparable to a (weak) baseline LSTM model. The analysis of the learned representations shows a separation into syntactic and semantic roles. \n\nThe paper targets an important problem, that of learning decomposable representations. As far as I know, it introduces a novel perspective using HRR and does so in the context of language modeling, which is a core NLP task. The analysis of the learned representations is quite interesting. I do have some concerns with regards to the quality of the language model, the clarity of some of the model description, and the validity of using HRR in this scenario. Please see detailed comments below. \n\nComments:\n=========\n1. Section 2 refers to Plate (1995) for the conditions when the approximate decoding via correlation holds. I think it's important to mention these conditions and discuss whether they apply to the language modeling case. In particular, Plate mentions that the elements of each vector need to be iid with mean zero and variance 1/n (where n is the length of the vector). Is this true for the present case? Typically, word embeddings and LSTM states are do not exhibit this distribution. Are there other conditions that are (not) met?\n2. Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation. On the other hand, if I understand correctly, this means that word embeddings are not shared between roles, because s^w_i is also a role-specific vector (not just a word-specific vector). Is that a cause of concern? \n3. It's not clear to me where in the overall model the next word is predicted. Figure 1b has an LSTM that predicts filler embeddings. Does this replace predicting the next word in a vanilla LSTM? Equation 5 still computes a word score. Is this used to compute the probability of the next word as in equation 2?  \n4. Comparison to other methods for composing words. Since much of the paper is concerned with composing words, it seem natural to compare the methods (and maybe some of the results) to methods for composing words. Some examples include [2] and the line of work on recursive neural networks by Socher et al., but there are many others. \n5. Perplexity results:\n- The baseline results (100.5 ppl on PTB) are very weak for an LSTM. There are multiple papers showing that a simple LSTM can do much better. The heavily tuned LSTM of [1] gets 59.6 but even less tuned LSTMs go under 80 or 80 ppl. See some results in [1]. This raises a concern that the improvements from the HRR model may not be significant. Would they hold in a more competitive model? \n- Can you speculate or analyze in more detail why the chunk-level model doesn't perform well, and why adding more fillers doesn't help in this case? \n6. Motivation: \n- The introduction claims that the dominant encoder-decoder paradigm learns \"transformations from many smaller comprising units to one complex emedding, and vice versa\". This claim should be qualified by the use of attention, where there is not a single complex embedding, rather a distribution over multiple embeddings. \n- Introduction, first paragraph, claims that \"such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability\" - what do you mean by these concepts and how exactly is the current approach limited with respect to them? Giving a bit more details about this point here or elsewhere in the paper would help motivate the work. \n7. Section 3.3 was not so clear to me:\n- In step 1, what are these r_i^{chunk}? Should we assume that all chunks have the same role embeddings, despite them potentially being syntactically different? How do you determine where to split output vectors from the RNN to two parts? What is the motivation for doing this?\n- In prediction, how do you predict the next chunk embedding? Is there a different loss function for this? \n- Please provide more details on decoding, such as the mentioned annealing and regularization. \n- Finally, the reliance on a chunker is quite limiting. These may not be always available or of high quality. \n8. The analysis in section 4.3 is very interesting and compelling. Figure 2 makes a good point. I would have liked to see more analysis along these lines. For example, more discussion of the word analogy results, including categories where HRR does not do better than the baseline. Also consider other analogy datasets that capture different aspects. \n9. While I agree that automatic evaluation at chunk-level is challenging, I think more can be done. For instance, annotations in PTB can be used to automatically assign roles such as those in table 4, or others (there are plenty of annotations on PTB), and then to evaluate clustering along different annotations at a larger scale. \n10. The introduction mentions a subset of the one billion word LM dataset (why a subset?), but then the rest of the papers evaluates only on PTB. Is this additional dataset used or not? \n11. Introduction, first paragraph, last sentence: \"much previous work\" - please cite such relevant work on inducing disentangled representations.\n12. Please improve the visibility of Figure 1. Some symbols are hard to see when printed. \n13. More details on the regularization on basis embeddings (page 4) would be useful. \n14. Section 3.3 says that each unique word token is assigned a vectorial parameter. Should this be word type? \n15. Why not initialize the hidden state with the last state from the last batch? I understand that this is done to assure that the chunk-level models only consider intra-sentential information, but why is this desired? \n16. Have you considered using more than two roles? I wonder how figure 2 would look in this case. \n\n\nWriting, grammar, etc.:\n====================== \n- End of section 1: Our papers -> Our paper\n- Section 2: such approach -> such an approach; HRR use -> HRR uses; three operations -> three operations*:*\n- Section 3.1: \"the next token w_t\" - should this be w_{t+1)? \n- Section 3.2, decoding: remain -> remains \n- Section 3.3: work token -> word token \n- Section 4.1: word analogy task -> a word analogy task; number basis -> numbers of basis\n- Section 4.2: that the increasing -> that increasing \n- Section 4.3: no space before comma (first paragraph); on word analogy task -> on a word analogy task; belong -> belongs\n- Section 4.4: performed similar -> performed a similar; luster -> cluster \n- Section 5: these work -> these works/papers/studies; share common goal -> share a common goal; we makes -> we make; has been -> have been  \n\nReferences\n==========\n[1] Melis et al., On the State of the Art of Evaluation in Neural Language Models\n[2] Mitchell and Lapata, Vector-based Models of Semantic Composition\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}