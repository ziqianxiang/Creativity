{
    "Decision": {
        "metareview": "While the authors made a strong rebuttal, none of the reviewers were particularly enthusiastic about the contributions of this paper and we unfortunately have to reject borderline papers. Concerns were expressed about the presentation, as well as the scalability of the approach. The AC encourages the authors to \"revise and resubmit\".",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Revise and resubmit"
    },
    "Reviews": [
        {
            "title": "Generalize mirror descent to infinite dimensional spaces. No really new theory or practice insight.",
            "review": "This paper proposes to consider the mixed equilibrium objective function for GANS. The authors generalize the mirror descent/mirror prox to handle continuous games. The technical challenge is to write those algorithms in infinite dimensional spaces. This reviewer finds this however to be a mere technicality, and there seems to be no conceptual obstruction. In fact other paper have already written this, see for example ``Mirror Descent Learning in Continuous Games\" by Zhou et al. at CDC 2017 (I'm sure there are other references too).\nWhile the theory part is not particularly exciting, the paper could be saved by the experiments. However as far I can tell the authors are only able to reproduce the results obtained with more classical approaches.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting extension of mirror-prox with some important missing pieces",
            "review": "This paper extends the mirror-descent and mirror-prox algorithms to infinite dimensional Banach spaces so that they can be applied to solve the mixed Nash equilibrium of the popular generative adversarial networks. The main technical results appear to be formal but straightforward extensions of existing techniques in finite dimensional spaces. A sample-based practical algorithm is proposed so that the infinite dimensional algorithms can still be computed. Experiments are a bit disappointing as the authors only used visual appeal as an evaluation criterion. (I understand why the authors chose to do so but as an algorithmic paper, resorting to an evaluation based on visual appeal is almost always unsatisfactory.)\n\nQuality: The quality of this work is moderate. Quite strangely, the authors made a fundamental mistake at the very beginning: their definition of approximate mixed equilibrium  (page 2, Notation) is bizarre and different from those in previous work (such as Nemirovski's MP paper). Fortunately, this is perhaps only an oversight on the definition; the algorithms and theorems are for the correct definition anyways. Example: consider min_{-1<= x <= 1} max_{-1<=y<=1} xy. Should we call (x, 0) an (approximate) NE for any x??\n\nAnother major issue with this work is its relaxation into mixed NE. The \"bilinarization\" trick in Eq (5) goes back to Kantorovich (who perhaps deserves to be mentioned), and is a relaxation in general: we now have to use a mixture of generators. Since MD/MP is not sparse, in the end we must use a large number of mixtures of generators. This certainly will create some computational issues, and make comparison to pure NE methods unfair.\n\nClarity: The writing of this work is mostly easily to follow. However, the presentation of the technical results suffers from a real dilemma: On one hand, the authors completely ignored the technical difference between infinite dimensional Banach spaces and finite dimensional spaces. In fact, the authors never even formally defined the underlying Banach spaces. Another example, is the mapping G on page 3 continuous? wrt what topology? without such discussion what do you mean by Frechet derivative on page 4? when is the entropy function well-defined? when is the integral of exponential well-defined? Part of me totally understand that these technicalities are daunting and perhaps should not appear in the main text. On the other hand, aren't these technicalities the only \"interesting and nontrivial\" part of the extension to infinite dimensional spaces? If we do not care about such technicalities and can safely \"assume they can be taken care of,\" then why is this work nontrivial? I do not see a way to resolve this dilemma here but suggest the authors consider maybe a different venue for such type of results.\n\nOriginality: The novelty of this work is limited. The extension of MD/MP to infinite dimensional spaces is mostly formal but straightforward. In fact I believe previous authors such as Nemirovski deliberately restrict to finite dimensional spaces not because of technical incapability but to avoid uninspiring technicalities. Some very related previous works were not mentioned at all:\n-- Mirror Descent Learning in Continuous Games\n-- Convex Games in Banach Spaces\n-- On the Universality of Online Mirror Descent\n\nThe sample based algorithms are more interesting because they make the infinite dimensional extensions implementable. However, one can not say much about their convergence behavior at the moment.\n\nSignificance: The main results, although not difficult to obtain, can potentially be very useful in broadening our arsenal of tools for training GANs. The claim \"resolving the longstanding problem that no provably convergent algorithm exists for general GAN\" in the Abstract is disturbing, because the authors changed the definition of GAN and because the technical contributions of this work do not live up to that strong claim. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting mixed strategy perspective to train GANs",
            "review": "This paper uses a mixed strategy perspective for GANs. With this formulation the non-convex game formulation of GANs can be transformed into a infinite dimensional problem analog to a finite dimensional bilinear problem.  \n\nI really like this approach, that tries to find methods that converge globally to (mixed) Nash equilibriums. However I have some concerns. \n\n- I'm concerned about the definition of a $O(T^{-1})-NE$. Actually, this merit function is not standard for game. It can be 0 even if $x_t,y_t$ is far from the equilibrium (for instance for the problem $\\min_{x \\in \\Delta_d}\\max_{y \\in \\Delta_d} x^\\top y$ with $x_t = (1,0,\\ldots,0)$ and $y_t= (F(x_{NE},y_{NE}),1-F(x_{NE},y_{NE}),0,\\ldots,0)$ we have $F(x_t,y_t) = F(x_{NE},y_{NE})$ but $x_{NE} = y_{NE} =(1/d,\\ldots,1/d)$). One merit function that could be considered is $\\max_{y} F(x,y_t) - \\min_{y} F(x_t,y)$. \n\n- There is a gap between the theory and the practical method that could be bridged. Actually Theorem 2 assume that the stochastic derivatives are unbiased but since your Langevin dynamics gives you an *approximate* of the next distribution an analysis taking into account this bias would provide much stronger results. More precisely, it would be interesting to have a result similar as Theorem 2 with conditions on $\\epsilon_t$ and $K_t$. For instance, if the theoretical $K_t$ is too large it would reduce the interest of your algorithm. I think this analysis is key since it allows to claim that you can properly approximate the distributions of interest.\n\nIf you are able to ease these concerns I'm eager to increase my grade.\n\n\n- \"(5) is exactly the infinite-dimensional analogue of (1):\" Actually it is not exactly the analogue since $<.,.>$ is not a scalar product anymore (particularly, $<g,\\mu>$ is not defined) but the canonical pairing between a space and its dual (we are loosing something going to infinite dimension).\nI think it should be clarified somewhere. \n\nMinor comments: \n- on the updates rules of $\\theta$ and $\\omega$ (Page 6) the Gaussian noises are missing. \n- On algorithm 3,4,5 and 6 the Gaussian noise is too wide and causes an Overfull.\n\n=== After Authors response ===\nThe authors fixed some major issues. That is why I improved my grade. \nHowever I'm still concerned about the scalability of this algorithm\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}