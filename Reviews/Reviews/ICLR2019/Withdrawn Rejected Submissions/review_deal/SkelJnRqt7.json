{
    "Decision": {
        "metareview": "This paper presents a novel technique for separating signals in a given mixture, a common problem encountered in audio and vision tasks. The algorithm assumes that training samples from only one of the sources and the mixture distributions are available, which is a realistic assumption in a lot of cases. It then iteratively learns a model that can separate the mixture by using the available samples in a clever fashion.\n\nStrengths:\n- The novelty lies in how the authors formulate the problem, and the iterative approach used to learn the unknown distribution and thereby improve source separation.\n- The use of existing GLO masking techniques for initialization to improve performance is also novel and interesting.\n\nWeaknesses\n- There are some concerns around guarantees of convergence. Empirically, the algorithm works well, but it is unclear when the algorithm will fail. Some analysis here would have greatly improved the quality of the paper.\n- The reviewers also raised concerns around clarity of presentation and consistency of notation. While the presentation improved after revision, there are parts which remain unclear (e.g., those raised by R3) that may hinder readability and reproducibility. \n- The mixing model assumed by the authors is additive, which may not always be the case, e.g. when noise is convolutive (room reverberation, for instance).\n- (Minor) Experiments can also be improved. The vision tasks are not very realistic. For the speech separation task, relatively clean speech is easy to obtain. Therefore, it would be worth considering speech as observed, and noise as unobserved. The authors cite separating animal sounds from background, but the task chosen does not quite match that setup. \n\nOverall, the reviewers agree that the paper presents an interesting approach to separation. But given the issues with presentation and evaluations, the recommendation is to reject the paper. We strongly encourage the authors to address these concerns and resubmit in the future.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Good formulation, but needs improvement in presentation and experiments"
    },
    "Reviews": [
        {
            "title": "Interesting ideas with decent empirical results",
            "review": "This article presents an interesting if heuristic approach to source separation, NES, buttressed by the use of GLO masking for initialization, with promising results on data generated from synthetic source mixing.\n\nThe paper is well written and on the whole clear. My main concern with the work is the empirical nature of the NES iterative procedure. As far as I can tell there is no guarantee of convergence (nor discussion concerning this point). Since i am not familiar with the tasks, it is hard for me to judge the quality of the empirical results -- though the results do seem promising.\n\nre: Bags & shoes task / table 1: \"...  Finetuning from GLOM, helped NES achieve stronger performance, nearly identical to the fully-supervised upper bound. It performed better than finetuning from AM (which achieved 22.5/0.85 and 22.7/0.86)\": I can't place the first number in the table, therefore i'm not quite sure what is being pointed out here.\n\nre: Music task / table 3: \"... GLOM was much better than AM initialization (that achieved 0.9 and 2.9)\": I don't see either number in the table. I'd assumed that GLOM was used to fine-tune NES, so I was expecting to see the 2.9 under \"FT\". \n\n== \n\nI think the authors' response is reasonable. They have added clarifying material to the paper addressing my concerns. I have raised my rating from a 5 to a 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Well-motivated problem, but the presentation is unclear.",
            "review": "This paper describes a signal separation method called neural egg separation (NES).\nThe separation problem is tackled in a semi-supervised setting where the observed mixture contains a target signal and a background noise, with access to the distributions of target and mixture signals.\n\nThe strength of the paper is that it describes the importance of the problem setup for practical use with some motivating examples. \nHowever, some unclear notations weaken the claim of the paper.\n\nSpecific comments follow.\n* The loss in (1) is unclear.\nAssuming latex grammar, \\| \\| is usually used to denote a vector norm, but (1) has two values inside. \nI would write \\ell(T(y_i), b_i) to show a loss function, instead of the \\| \\| style.\nMore importantly, the loss should be explicitly defined. Does this mean the l2 error?\n\n* The iterative separation process of (2) is even unclear.\nDoes T^m(b_j + x_i^m) share the parameter of that from previous iterations like T^{m-1}?\nOr are the parameters fixed throughout the iterations?\n\n* Use of \\cdot.\nThere may be a confusion between the inner product and element-wise product with the \\cdot operator.\nRight after (5), there is an inequality z \\cdot z \\leq 1, which is meant to be the inner product.\nOn the other hand, the use of \\cdot in (8) looks like the element-wise product to describe a masking operation.\n\nClarifying the objective and overall procedures is necessary for presenting the proposed method.\n\n=================================\nEDIT: I confirmed the revisions regarding the notation issues, but there still have confusing parts.\n* Definitions of norm operator \\| \\| is unclear.\n  * L_1 is mentioned below (1), and used other parts (3) or Algorithm 1. Equation (12) in Appendix uses |W1|_1^2, which looks like the l1 norm as well. Use consistent notations.\n  * Equations (12, 13, 14) uses \\|\\|_2 or \\|\\|_1 to specify the type of norm, whereas (5), (6), (7) and other parts after (15) use \\|\\|. This confuses me. What do you mean by \\|\\| without subscript?\n  * \\|\\| operator taking to symbols is a weird notation for me. Usually, norm is defined for a single vector (or a matrix). For example in (5), I would write \\| b - G(z_b) \\|, if you want to measure the difference between b and G(z_b).\n\nThe experimental result is impressive, as the other reviewers mention. I strongly recommend clarifying the notation to better deliver the method.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Approach is reasonable, but insufficient experiment and evidence",
            "review": "This paper presents an iterative approach to separate unobserved distribution signal from a mixture with observed distribution. The proposed approach looks reasonable to me, however, the experiment and analysis are insufficient.\n1. At test time, does the input also go through the same number of iterations (10)? I would like to see how the separated results evolve over iterations.\n2. It is not clear what is the quality of samples generated by GLO. In the image separation task, GLOM performs better than GAN, but worse in other tasks. Analysis is needed here.\n3.  I noticed that only in the music separation task, finetuning is significantly better than vanilla NES. Is it because generative models can synthesize more realistic data samples? For example, would the generator learn to synthesize X+B with temporal synchronization? More analysis is also needed here.\n\n============================\n\nI think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}