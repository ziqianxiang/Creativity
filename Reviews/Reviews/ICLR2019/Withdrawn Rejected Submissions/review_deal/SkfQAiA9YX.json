{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of \"In search of theoretically grounded pruning\"",
            "review": "# Summary\n\nThe article proposes a variation of magnitude-based pruning for neural networks. In a nutshell, the authors argue that standard MLE asymptotic theory is applicable to deep learning, and propose to use the Wald test statistic as a salience criterion for pruning. Limited experimental results are provided using the LeNet-300-100 and LeNet-5 architectures and the MNIST dataset.\n\n# Assessment\n\nCompression and pruning of neural networks is a topic with a vast wealth of prior work spanning several decades. Nonetheless, the interest in deploying deep learning-based models to devices with limited compute capabilities has kept the topic particularly active in the last years.\n\nDespite the relevance of this paper's topic, unfortunately I do not believe that the methodological contribution is sufficiently novel and significant nor that the experimental results are sufficiently thorough and compelling to recommend the paper for publication.\n\n# Major points\n\n1) Novelty and Significance of the methodological contribution\n\nA large part of the paper is devoted to justify the applicability of the Wald test to deep learning models. Nonetheless, I do not believe the arguments put forward are sufficiently rigorous or original to consider those as a substantial contribution. Perhaps most importantly, I have strong concerns about several of those arguments.\n\nFor example, using the Wald test requires identifiability, which is clearly severely violated by most commonly used architectures. While the paper argues that the parameter space can be restricted to contain a single minimum, that hardly resembles the way models are trained in practice. \n\nThe Wald test also requires the training to converge to a local minimum, i.e., the Hessian must be positive definite. Again, I do not believe this assumption holds in many real-world models. Convergence to saddle points often occurs, with the Hessian thus containing negative eigenvalues [1].\n\nAs accepted in the Appendix, the differentiability condition is violated by some of the most commonly used activation functions, including ReLUs.\n\nNormality of the score distribution depends on asymptotic arguments. However, many deep learning models have a much larger number of parameters than samples they are trained on. Consequently, it is unclear whether asymptotic results are applicable in the regime in which deep learning-based approaches are commonly used.\n\nFinally, perhaps less importantly, the assumption if i.i.d. samples might also be too strong for many real-world datasets.\n\nThe proposed criterion is otherwise a straightforward application of the Wald test.\n\n2) Experimental results\n\nThe experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of state-of-the-art competitors among the very many related methods published recently (e.g. [2-8], just to mention a small subset). \n\nPerhaps most importantly, the method does not appear to clearly outperform the state of the art. However, the insufficient experimental results reported in the manuscript render a precise assessment difficult.\n\n# References\n\n[1] Martens, James. \"New insights and perspectives on the natural gradient method.\" arXiv preprint arXiv:1412.1193 (2014).\n[2] Guo, Yiwen, Anbang Yao, and Yurong Chen. \"Dynamic network surgery for efficient dnns.\" Advances In Neural Information Processing Systems. 2016.\n[3] Dong, Xin, Shangyu Chen, and Sinno Pan. \"Learning to prune deep neural networks via layer-wise optimal brain surgeon.\" Advances in Neural Information Processing Systems. 2017.\n[4] Ullrich, Karen, Edward Meeds, and Max Welling. \"Soft weight-sharing for neural network compression.\" International Conference on Learning Representations. 2017.\n[5] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational dropout sparsifies deep neural networks.\" International Conference on Machine Learning. 2017.\n[6] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n[7] Louizos, Christos, Max Welling, and Diederik P. Kingma. \"Learning Sparse Neural Networks through $ L_0 $ Regularization.\" International Conference on Learning Representations. 2018.\n[8] Yang, Yibo, Nicholas Ruozzi, and Vibhav Gogate. \"Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization.\" arXiv preprint arXiv:1806.05355 (2018).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper is about obtaining a \"fully estimable empirical distribution\" for neural networks parameters in order to obtain some saliency or importance criterion for performing theoretically grounded pruning. An interesting idea at first sight, but the work and writing are done carelessly and in a rush.",
            "review": "Comments\n* all conditions are not discussed, for instance, isn't it quite strong to impose that the parameter space \\Theta is compact?\n* LDH condition: what is \\mathcal{N}, it isn't defined\n* The statement on top of page 3: \"Then, asymptotically, as the sample size n grows to infinity, the model parameters are distributed as (...)\" is awkward: is it standard? is it a new result? please consider stating it as a Proposition/Theorem or something, and explain where it comes from.\n* Actually, I am skeptical about the above statement itself: it is stated that \"the model parameters are distributed as (...)\". Does that mean that the parameters are random, in a Bayesian fashion? please state it then. Or the alluded to distribution is that of parameters' estimators? then the statement should be amended, and it should be explained what estimators are considered.\n* footnote 3: shouldn't the considered sum be divided by n?\n* the paper is full of flawed equations where, I think, parameters are mixed up with estimators. \n    * For instance, Equations (2) and (3) should express the same quantity, $W$, however the former displays the parameters’ covariance matrix, while the latter uses an estimator of it, \\hat{Avar}.\n    * Same in page 4 with the estimator of the Hessians (H) and of the score covariance matrix (\\hat Σ).\n    * Same for Equation (6): \\hat \\theta is suddenly plugged-in in place of \\theta_0, all of this conserving the equality sign.\n    * Same for Equation (7)\n\n\nTypos, minor comments and un-precise statements\n* the footnotes superscripts are misleading for footnotes 2 and 3; they could be better placed.\n* Lindeberg-Lvy should be Lindeberg-Lévy\n* after equations (6,7,8) you write \"Where all is defined as above\": this is fairly un-precise; as you use LaTeX, you should conveniently use some \\label and \\ref.\n* end of section 4.1: MINST dataset, a new dataset?\n* top of section 4.2: there are even typos in the proposed name for methodology! : \"Walt-based Pruning\"\n* Many references in the bibliography are incomplete, so that it is impossible to see where they come from (at least seven of them)",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good insights on magnitude based pruning but needs more empirical results",
            "review": "This paper proposes a statistical framework for determining parameter importance beyond magnitude based pruning.  The paper derives deep model parameter asymptotic theory to formulate a statistically-grounded pruning criterion which we compare with the magnitude pruning both qualitatively and quantitatively. The paper finds that this criterion to better capture parameter salience, since it takes into consideration the estimation uncertainty. The paper demonstrates the improved results for Lenet in terms of performance and easier post-pruned re-training.\n\nThe paper's take on the commonly used magnitude based pruning if appreciated. However, I am not fully convinced that pruning weights by magnitude close to random judgements in the general case as in the paper abstract. While the points made in Section 2 against magnitude based pruning are sound and relevant, it is unclear making such a strong general claim is wise since weights are commonly regularized. In fact, some previous work on pruning has performed these experiments and found that for common networks with default hyper-parameter configurations, weight based pruning performs better than magnitude based pruning. Li et. Al 2017 in the paper, Figure 4.4 compares random pruning with smallest L1 norm pruning and they find that smallest filter pruning has better accuracy than random filter pruning for all layers. Are there any real networks where such results would apply beyond the toy network in Figure 1?\n\nIn terms of empirical evaluation, results beyond Lenet such as on CIFAR-10 and Imagenet are required to claim Wald pruning is superior over existing methods, and comparison against other pruning methods such as dynamic pruning will make the paper stronger. It is unclear what to make of the 0.8% improvement in the Lenet results, performing extensive evaluation is necessary to show the performance of Wald pruning.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}