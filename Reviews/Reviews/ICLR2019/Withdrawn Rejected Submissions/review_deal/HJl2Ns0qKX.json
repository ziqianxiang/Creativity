{
    "Decision": {
        "metareview": "The idea of the paper -- imposing a GAN type loss on the latent interpolations of an autoencoder -- is interesting. However there are strong concerns from R2 and R3 about limited experimental evaluation of the proposed method which falls short of demonstrating its advantages over latent spaces learned by existing GANs. Another point of concern was the use of only one real dataset (CelebA). Authors made substantial revisions to the paper in addressing many of the reviewers' points but these core concerns still persist with the current draft and it's not ready for publication at ICLR. Authors are encouraged to address these concerns and resubmit to another venue. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting idea but insufficient evaluation of the method to establish benefits over existing methods"
    },
    "Reviews": [
        {
            "title": "Official review",
            "review": "Update:\n\nI appreciate the effort put by the authors into improving the paper. The revised draft is much better than the initial one. But I agree to AnonReviewer2 in that the degree to which this paper has to be modified goes beyond what the review process (even at ICLR) assumes. It is wrong to submit a very unfinished paper and then use the review period to polish it and add results. This incurs unnecessary extra load on the review process.\n\nThe added 2D results are toy-ish and somewhat confusing (I am not sure I understand what the meshgrids are and what do they tell us). Generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation. The paper should have more results on complex datasets, like for instance ImageNet or LSUN or CIFAR or so, and should have comparisons to existing VAE-GAN hybrids, like VAE-GAN. Also, since a lot of the authors’ motivation seems to come from psychophysics, showing some application to that might be a good way to showcase the value of the method (although this may not go well if submitting to machine learning conferences). \n\nI encourage the authors to further strengthen the paper and resubmit to another venue.\n\n-----\n\nThe paper proposes a model for image generation that combines an autoencoder with a generative adversarial network (GAN). The GAN is used to enforce that interpolations between latent vectors of two samples from the training set are decoded to realistic images. The method is applied to attribute manipulation and interpolation of face images.\n\nPros:\n1) A simple and reasonable formulation\n2) Visually good reconstruction of samples and convincing interpolation between samples on the CelebA-HQ dataset.\n3) Good qualitative facial attribute manipulation results on the CelebA-HQ dataset.\n\nCons:\n1) Experimental evaluation is very limited. There is just one dataset and only qualitative results. This is unacceptable: the method should be evaluated on more datasets and there should be quantitative results. I do realize it is not trivial to get quantitative, but it is possible. For instance, a user study can always be performed. But I believe one could also come up with simpler-to-measure metrics for at least some of the reported tasks. There is no comparison to other methods for facial attribute manipulation (for instance, StarGAN).\n2) There is no ablation study. To which extent is each of the model components important? For instance, the interpolation adversarial loss, the discriminator/generator balancing term, the network architecture (autoencoder discriminator)?\n3) As I understand, it is impossible to randomly sample directly from the model, only interpolate/modify existing images. This is a difference from most of prior work. It should be discussed clearly.\n4) Related work discussion is quite brief and misses some relevant work, for instance Adversarial Autoencoders (Makhazani et al., ICLR 2016, somewhat related) or Adversarially Constrained Autoencoder Interpolations (Berthelot et al., arxiv 2018, it’s concurrent, but could be good to discuss).\n5) Writing is not of very high quality. There are typos, grammatical issues, and questionable statements. The manuscript should be significantly improved. Specific comments:\n- The structure is quite strange. There is no separation between the method and the experiments, the related work comes in very late and is very brief. This is all not critical, but confusing.\n- A typo in the title: should be “encourages”\n- Second sentence of the introduction should be supported with evidence (for instance references)\n- “Two unsupervised neural network based algorithms, the Autoencoder (AE; Hinton & Salakhutdinov\n2006) and Generative Adversarial Network (GAN; Goodfellow et al. 2014), are at present the most popular tools in generative modeling.” - Vanilla autoencoders are not very popular tools for generative modeling. Variational autoencoders and some other flavors are.\n- “Unsupervised neural network approaches are often ideal generative models because they require little or no tweaking of network architectures to handle entirely new datasets.” I do not really get this sentence. What is the alternative to unsupervised generative models? Why do unsupervised approaches not require tweaking? (In my experience, they very well benefit from tweaking.)\n- “… a lack of certain constraints on the generative capacity of current neural-network based generative models make it challenging to infer structure from their latent generative representations.” What does “a lack of certain constraints” mean? There are some constraints, for instance the latent space is usually forced to correspond to a fixed distribution. Moreover, there is a lot of work on disentangling that also aims to find structure in latent spaces (for instance, InfoGAN).\n- “and promotes convexity in the model’s generative capacity.” What is convexity in the capacity? I do not think this is grammatical.\n- In 1.1 the mathematical notation seems wrong. Does X really denote a set? In what follows it seems that X is used interchangeably for three different things: a sample from the dataset, the set of training samples, and the space the samples come from.\n- “bidirectionality of adversarially generated data.” What is bidirectional data?\n- “AEs tend to produce blurry images due to their pixel-wise error functions (Larsen et al., 2015)” Perhaps this was intended to refer to VAEs. AEs can generate perfectly sharp images if given enough capacity. \n- “method more greatly resembles the original data than other GAN-based methods” Method does not resemble data\n- “Due to their exact latent-variable inference, these architectures may also provide a useful direction for developing generative models to explore latent-spaces of data for generating datasets for psychophysical experiments.” This is mentioned a few times, but never supported\n- Acknowledgements should not be in the review version (can violate anonymity)\n\n5) Minor: Why is a Gaussian around the midpoint used for interpolations? Why not all convex combinations of two, or possibly more, samples? \n\nTo conclude, the paper presents quite good qualitative results on the CelebA-HQ dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation. The paper cannot be published in its current form. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "\nUpdate:\n\nI’d like to thank the authors for their thoroughness in responding to the issues I raised. I will echo my fellow reviewers in saying that I would encourage the authors to submit to another venue, given the substantial modifications made to the original submission.\n\nThe updated version provides a clearer context for the proposed approach (phychophysical experimentation) and avoids mischaracterizing GAIA as a generative model.\n\nDespite more emphasis being put on mentioning the existence of bidirectional variants of GANs, I still feel that the paper does not adequately address the following question: “What does GAIA offer that is not already achievable by models such as ALI, BiGAN, ALICE, and IAE, which equip GANs with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates?” To be clear, I do think that the above models are inadequate for the paper’s intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but I believe this is a question that is likely to be raised by many readers.\n\nTo answer the authors’ questions:\n\n- Flow-based generative models such as RealNVP relate to gaussian latent spaces in that they learn to map from the data distribution to a simple base distribution (usually a Gaussian distribution) in a way that is invertible (and which makes the computation of the Jacobian’s determinant tractable). The base distribution can be seen as a Gaussian latent space which has the same dimensionality as the data space.\n- Papers on building more flexible approximate posteriors in VAEs: in addition to the inverse autoregressive flow paper already cited in the submission, I would point the authors to Rezende and Mohamed’s “Variational Inference with Normalizing Flows”, Huang et al.’s “Neural Autoregressive Flows”, and van den Berg et al.’s “Sylvester Normalizing Flows for Variational Inference”.\n\n-----\n\nThe paper title summarizes the main claim of the paper: \"adversarial training on latent space interpolations encourage[s] convex latent distributions\". A convex latent space is defined as a space in which a linear interpolation between latent codes obtained by encoding a pair of points from some data distribution yields latent codes whose decoding also belongs to the same data distribution. The authors argue that current leading approaches fall short of producing convex latent spaces while preserving the \"high-dimensional structure of the original distribution\". They propose a GAN-AE hybrid, called GAIA, which they claim addresses this issue. The proposed approach turns the GAN generator and discriminator into autoencoders, and the adversarial game is framed in terms of minimizing/maximizing the discriminator’s reconstruction error. In addition to that, interpolations between pairs of data points are computed in the generator’s latent space, and the interpolations are decoded and treated as generator samples. A regularization term is introduced to encourage distances between pairs of data points to be mirrored by their representation in the generator’s latent space. The proposed approach is evaluated through qualitative inspection of latent space interpolations, attribute manipulations, attribute vectors, and generator reconstructions.\n\nOverall I feel like the problem presented in the paper is well-justified, but the paper itself does not build a sufficiently strong argument in favor of the proposed approach for me to recommend its acceptance. I do think there is a case to be made for a model which exhibits sharp reconstructions and which allows realistic latent space manipulations -- and this is in some ways put forward in the introduction -- but I don’t feel that the way in which the paper is currently cast highlights this very well. Here is a detailed breakdown of why, and where I think it should be improved, roughly ordered by importance:\n\n- The main reason for my reluctance to accept the paper is the fact that its main subject is convex latent spaces, yet I don’t see that reflected in the evaluation. The authors do not address how to evaluate (quantitatively or qualitatively) whether a certain model exhibits a convex latent space, and how to compare competing approaches with respect to latent space convexity. Figure 2 does present latent space interpolations which help get a sense of the extent to which interpolates also belong to the data distribution, however in the absence of a comparison to competing approaches it’s impossible for me to tell whether the proposed approach yields more convex latent spaces.\n- I don’t agree with the premise that current approaches are insufficient. The authors claim that autoencoders produce blurry reconstructions; while this may be true for factorized decoders, autoregressive decoders should alleviate this issue. They also claim that GANs lack bidirectionality but fail to mention the existing line of work in that direction (ALI, BiGAN, ALICE, and more recently Implicit Autoencoders). Finally, although flow-based generative models are mentioned later in the paper, they are not discussed in Section 1.2 when potential approaches to building convex latent spaces are enumerated and declared insufficient. As a result, the paper feels a little disconnected from the current state of the generative modeling literature.\n- The necessity for latent spaces to \"respect the high-dimensional structure of the [data] distribution\" is stated as a fact but not well-justified. How do we determine whether a marginal posterior is \"a suboptimal representation of the high-dimensional dataset\"? I think a more nuanced statement would be necessary. For instance, many recent approaches have been proposed to build more flexible approximate posteriors in VAEs; would that go some way towards embedding the data distribution in a more natural way?\n- I also question whether latent space convexity is a property that should always hold. In the case of face images a reasonable argument can be made, but in a dataset such as CIFAR10 how should we linearly interpolate between a horse and a car?\n- The proposed model is presented in the abstract as an \"AE which produces non-blurry samples\", but it’s not clear to me how one would sample from such a model. The generator is defined as a mapping from data points to their reconstruction; does this mean that the sampling procedure requires access to training examples? Alternatively one could fit a prior distribution on top of the latent codes and their interpolations, but as far as I can tell this is not discussed in the paper. I would like to see a more thorough discussion on the subject.\n- When comparing reconstructions with competing approaches there are several confounding factors, like the resolution at which the models were trained and the fact that they all reconstruct different inputs. Removing those confounding factors by comparing models trained at the same resolution and reconstructing the same inputs would help a great deal in comparing each approach.\n- The structure-preserving regularization term compares distances in X and Z space, but I doubt that pixelwise Euclidian distances are good at capturing an intuitive notion of distance: for example, if we translate an image by a few pixels the result is perceptually very similar but its Euclidian distance to the original image is likely to be high. As far as I can tell, the paper does not present evidence backing up the claim that the regularization term does indeed preserve local structure.\n- Figures 2 and 3 are never referenced in the main text, and I am left to draw my own conclusions as to what claim they are supporting. As far as I can tell they showcase the general capabilities of the proposed approach, but I would have liked to see a discussion of whether and how they improve on results that can be achieved by competing approaches.\n- The decision of making the discriminator an autoencoder is briefly justified when discussing related work; I would have liked to see a more upfront and explicit justification when first introducing the model architecture.\n- When discussing feature vectors it would be appropriate to also mention Tom White’s paper on Sampling Generative Networks.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an autoencoder architecture and training procedure for producing high-quality reconstructions and realistic interpolations. A \"generator\" autoencoder is trained to fool a \"discriminator\" autoencoder. The generator tries to minimize its own reconstruction error and minimize the reconstruction error of the discriminator when fed with interpolated latent vectors of real datapoints. The discriminator autoencoder has three losses, corresponding to minimizing reconstruction error on real datapoints and maximizing reconstruction error on the generator's output on both real datapoints and interpolated outputs. The authors also propose a loss which encourages the distances between real datapoints and their corresponding latent vectors to be similar, as well as a heuristic procedure for stabilizing GAN training. Qualitative results are shown on CelebA.\n\nWhile the results look nice, the paper is not fit for publication in its current form. At a high level, the issues include a lack of convincing experimental verification of the method, a generally contradictory and confusing description of the methods, and frequent factual errors or mischaracterizations. Here I will try to describe many of the issues I found while reading the paper:\n- Experimental results are only given on CelebA which is a dataset with a very strong and easy-to-model structure. The experimental results are completely qualitative. No effort is made to provide a quantitative proof of claims such as \"the reconstructions are less blurry\" or \"the interpolations are higher quality\"; only a few examples are shown. The experiments are not even described in the text, and many of the figures are unreferenced. No ablation studies are done to determine the importance of different loss terms, such as L_dist. No mention is given to how hyperparameters like alpha should be chosen (and in fact, the value given for it \"1^{-4}/2\" is nonsense; 1^{-4} is just 1). No results for a baseline autoencoder (i.e., just optimizing reconstruction loss) are given.\n- At a higher level, no effort is given to argue why interpolation is a useful characteristic to try to encourage. There are no downstream applications proposed or tested. Earlier models, such as VAEGAN, also give reasonable reconstructions and good interpolations. Why is GAIA better? On what problem would I use GAIA and achieve better results apart from making nice-looking interpolations of people's faces?\n- Definitions are often unclear or contradictory. For example, the generator autoencoder is alternatingly treating as taking input X and taking input Z. I believe what is meant is that the generator consists of two networks which compute Z = encoder(X) and X = decoder(Z). Instead, the paper just switches between G(Z) and G(X) wherever convenient. Similarly, the equation for \\delta_Disc is different in Algorithm 1 and in the equation in 2.2. Interpolation, arguably one of the core parts of the model, is described as \"interpolations are Euclidean interpolations between pairs of points in Z, sampled from a Gaussian distribution around the midpoint between Zg1en and Zg2en.\" I assume the mean of this Gaussian is the midpoint; what is its covariance? Etc.\n- All autoencoders are not generative models, and in particular GAIA is not a generative model. There is no generative process. It does not estimate a data distribution. A VAE is a generative model which an autoencoder-like structure, but this does not make all autoencoders generative models.\n- GAIA is described as encouraging \"convex latent distributions\" and a convex set is defined in the text as \"A convex set of points is defined as a set in which the line connecting any pair of points will fall within the rest of the set.\" A convex set is not defined in terms of lines; it's defined in terms of convex combinations of points within the set. In the paper, only lines between points are considered. Claiming that the latent space is \"convex\" in the sense of purple blobs in B is not done - you would need to take a convex combination of multiple latent vectors and decode the results.\n\nThis is an incomplete list of the issues with this paper. The paper would need significant changes before publication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}