{
    "Decision": {
        "metareview": "The paper seeks to obtain faster means to count or approximately count of the number of linear regions of a neural network. The paper improves bounds and makes an interesting contribution to a long line of work. \n\nA consistent concern of the reviewers is the limited applicability of the method. The empirical evaluation can serve to better assess the accuracy of theoretical bounds that have been obtained in previous works, but the practical utility is not as clear yet. \n\nThis is a borderline case. The reviewers lean towards a positive rating of the paper, but are not particularly enthusiastic about the paper. The paper makes good contributions, but is just not convincing enough. \n\nI think that the work program that the authors suggest in their responses could lead to a stronger paper in the future. In particular, the exploration of necessary and sufficient conditions for different neural networks to be equivalent and the use of number of linear regions when analyzing neural networks, seem to be very promising directions. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Developments on counting linear regions, applicability uncertain "
    },
    "Reviews": [
        {
            "title": "A good paper, could be improved",
            "review": "This paper contributes to the study of the number of linear regions in ReLU neural networks. An approximate probabilistic counting algorithm is used to estimate the lower bound of that quantity, whereas an upper bound is derived analytically. The probabilistic counting algorithm is shown to be much more efficient than exact counting, and is adapted from the SAT literature. The new upper bound uses the weights of the network, a new technique compared to previous work on these bounds, and is shown to be sometimes tighter than the older bound.\n\nOverall, I am positive about the paper. Although I could not verify all proofs in detail, the ones I did verify were sound. The probabilistic counting algorithm seems like a good fit for this type of neural network problems, and is adapted and implemented nicely.\n\nIn my opinion, the paper can be improved substantially on these fronts:\n- Motivation: Can you point me to a reference where the number of linear regions is used as a measure of expressiveness, formally? I ask because the scope of the work in this paper is very much tied to that question.\n\n- Clarity: this issue must be addressed. The paper is quite technical (that's fine), but also difficult to parse. For example, it is not clear what's new in 4.1.\n\nMinor:\n- Figure1/Table1: please move them to experiments. You do not describe the tables and results early, which makes it useless at that stage of the paper. Why not just move them to experiments and describe/discuss these results in detail there?\n- Notation: In page 3, paragraph 2, you use x in many different shapes and forms (e.g. bold). Please consider making that notation consistent.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well presented but highly technical paper on counting linear regions in neural networks",
            "review": "Summary:\nThis paper builds off of previous work that has studied the counting of linear regions in deep neural networks. The function learned by a deep neural network with piecewise linear activations (such as Relus) is itself piecewise linear on the input, and a measure of expressiveness of the network has been to count the number of linear regions.\n\nHowever counting linear regions in a typical neural network is usually intractable, and there have been a sequence of upper and lower bounds proposed. Upper bounds are based on counting hyperplane arrangements (Zaslavsky, 1975; Raghu 2017; Montufar 2017; Serra 2018), and lower bounds based on counting regions in specific networks.\n\nThis paper improves the upper bound proposed in Serra (2018) by improving on a dimensionality constraint: the upper bound can be tightened if the dimensionality of the ambient space is shown to be smaller than the maximum possible value (number of neurons.) The paper defines A_l(k) -- the number of active neurons in layer l given k active neurons in layer l-1, and I_l(k) similar for inactive neurons, and proves an improved upper bound. \n\nFor the lower bound, the paper extends the existing MBound algorithm to probabilistically count the number of linear regions, with experiments (Figure 1) demonstrating the speed of this lower bound algorithm compared to counting.\n\nClarity: The presentation for this paper is relatively clear, but it is quite technical, so some parts are hard to follow, without knowing the prior work in detail.\n\nOriginality: Defining A_l(k) and I_l(k) for a refined upper bound, as well as the idea of using a probabilistic lower bound is new compared to prior work.\n\nComments on Quality and Significance: \n\nThe theoretical results presented in this paper are interesting and novel, both the bounds and the adaptation of existing methods (Nemhauser 1978; Gomes 2006) for purposes of estimating bounds. However, I'm uncertain as to the practical applications. One thing that was unclear to me was what Proposition 3, 4 mean for the quantities A_l(k) and I_l(k) in practice (in trained networks). The text makes a comment on the weights and biases having the same number of positive/negative elements but that is likely to only be true for random networks.  It would be interesting to see Figure 1 left for random and trained networks. \n\nGiven the long line of work in this area however, I think this paper will be interesting to the community.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, but incremental and of little practical use",
            "review": "The paper deals with a problem of expressiveness of a piecewise linear neural network, characterized by the number of linear regions of the function modeled. This is one of the widely accepted measure of expressiveness of a linear model. As such, it has been studied before. The main contributions of the paper are:\n1) Different algorithms are proposed that allow to compute the bounds faster, leveraging probabilistic algorithms\n2) Tighter bounds are obtained \nI find the results somewhat interesting. However, I do not think there is a lot of practical value in having faster algorithms for obtaining the bounds, as they are not used in practice anyway. I am also not convinced that the quest for tighter-and-tighter bounds in this approach is the right scientific direction. I find the paper to be an interesting contribution, but of a marginal value to the progress of the domain and for the improvement of our understanding of the models.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}