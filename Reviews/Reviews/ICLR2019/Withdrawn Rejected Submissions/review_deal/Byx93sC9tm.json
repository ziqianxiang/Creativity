{
    "Decision": {
        "metareview": "The reviewers in general found the paper approachable, well written and clear.  They noted that the empirical observation of mode collapse in active learning was an interesting insight.  However, all the reviewers had concerns with novelty, particularly in light of Lakshminarayanan et al. who also train ensembles to get a measure of uncertainty.  An interesting addition to the paper might be some theoretical insight about what the model corresponds to when one ensembles multiple models from MC Dropout.  One reviewer noted that it's not clear that the ensemble is capturing the desired posterior.\n\nAs a note, I don't believe there is agreement in the community that MC dropout is state-of-the-art in terms of capturing uncertainty for deep neural networks, as argued in the author response (and the abstract).  To the contrary, I believe a variety of papers have improved over the results from that work (e.g. see experiments in Multiplicative Normalizing Flows from over a year ago).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "A well written paper with some interesting insights but lacking novelty"
    },
    "Reviews": [
        {
            "title": "Ensemble of MC-Dropout models is not an approximation of the posterior",
            "review": "The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning. They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation. The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem. \n\nThe paper is clearly written and easy to follow. It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning. \n \nThe major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore. Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not. Therefore, it is a little misleading to still call it Bayesian active learning. Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective. \n\nThe motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout. So it seems not a reasonable solution for the mode collapse problem of MC-Dropout. It is not clear to me why we need to add MC-Dropout to the ensemble. What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?\n\nIn terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout. While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.\n\nThe labels of figures are hard to read. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper contains only little novelty and the experiments are not sufficiently thorough",
            "review": "The paper shows that Bayesian neural networks, trained with Dropout MC (Gal et al.) struggle to fully capture the posterior distribution of the weights.\nThis leads to over-confident predictions which is problematic particularly in an active learning scenario.\nTo prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.\nThe proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.\n\n\nThe paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce. \n\nHowever, the paper contains only little novelty and does not provide sufficiently new scientific insights.\nIt is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.\nFor instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores. Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.\nIt would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].\n\n\nAnother weakness of the paper is that the empirical evaluation is not sufficiently rigorous: \n\n1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.\n\n 2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M. \n \n 3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures. The same holds for the type of data, since the paper only shows results for image classification benchmarks.\n \n 4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.\n \n\n\n\n[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\nBalaji Lakshminarayanan, Alexander Pritzel, Charles Blundel\nNIPS 2017\n\n[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger\n    Snapshot Ensembles: Train 1, get {M} for free}\n    ICLR 2017\n\n[3] Bayesian Optimization with Robust Bayesian Neural Networks\n    J. Springenberg and A. Klein and S.Falkner and F. Hutter\n    NIPS 2016\n \n[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams\n    Scalable Bayesian Optimization Using Deep Neural Networks\n    ICML 2015\n\n[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling\n    Carlos Riquelme, George Tucker, Jasper Snoek\n    ICLR 2018",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clear writing but only mild improvement for computational cost.",
            "review": "This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.\n\nIn active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?\n\nThe novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.\n\nThe paper is well-written, has good experiments, and has a comprehensive related work section.\n\nOverall, this paper is good, but is not novel or important enough for acceptance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}