{
    "Decision": {
        "metareview": "The authors propose to define 'Expressiveness' in deep RL by the rank of a matrix comprising a number of feature vectors from propagating observations through the learnt representation, and show a correlation between higher rank and higher performance. They try 3 regularizers to increase rank and show that they improve the final score on Atari games compared to A3C or DQN. The AC and reviewers agree that the paper is interesting and novel and could have general significance for the RL field. Also, the authors were very responsive to the reviewers and added more details, plus several experiments and analyses to support their claims. However, the reviewers were concerned about a number of aspects and have recommended that the authors clean up their presentation and analysis a bit more. In particular, the fact that the regularization coefficient is tuned for each Atari game makes it very hard to compare to DQN/A3C which are very careful to keep the same hyperparameters across every game.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "# Summary\nThis paper proposes a simple regularizer for RL which encourages the state representations learned by neural networks to be more discriminative across different observations. The main idea is to (implicitly) measure the rank of the matrix which is constructed from a sequence of observations and state feature vectors and encourage the rank to be high. This paper introduces three different objectives to implement the same idea (increasing the rank of the matrix). The experimental results on Atari games show that this regularizer improves A3C on most of the games and show that the learned representations with the proposed regularizer has a high rank compared to the baseline. \n\n[Pros]\n- Makes an interesting point about the correlation between RL performance and state representations. \n- Proposes a simple objective function that gives strong empirical results.\n\n[Cons]\n- Needs more empirical evidences to better support the main hypothesis of the paper.\n\n# Novelty and Significance\n- This paper makes an interesting observation about the correlation between the RL performance and the how discriminative the learned features.\n- To verify it, this paper proposes a new and simple regularizer which improves the performance across many Atari games. \n\n# Quality and Experiment\n- The main hypothesis of this paper is that the expressiveness of the features (specifically the rank of the matrix that consists of a sequence of features) and its RL performance is highly correlated. Although this paper showed some plots (Figure 2, 6) to verify this, a more extensive statistical test or experiments would be more convincing to show the hypothesis. Examples would be:\n1) Measuring the correlation between the two across all Atari games.\n2) An ablation study on the hyperparameter (alpha). \n3) Learning state representations just from the reconstruction task (without RL) with/without the proposed regularizer and separately learning policies on top of that (with fixed representations). It would be much more convincing if the regularizer helps even in this setup, because this would show the general effect of the expressiveness term (by removing the effect of RL algorithm on the representation).\n- This paper only reports \"relative\" performances to the baseline. Though it looks strong, it is also important to report the absolute performances in Atari games (e.g., median human-normalized score, etc) to show how significant the performance gap is. \n- The results with DQN are not convincing because the agents are trained only for 20M frames (compared to 200M frames in many other papers). It is not much meaningful to compare performances on such a short training regime. I would suggest running longer or removing this result from the paper and focusing on more analysis.\n\n# Clarity and Presentation\n- Figure 1a is not much insightful. It is not surprising that the representations that led to a poor policy (which achieves 0 reward) are much less discriminative given five situations with five distinct optimal actions, because the the policy has no idea which action is better than the other in such situations. It would be more informative to pick N consecutive frames and show how scattered they are in the embedding space. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but many elements are lacking or unclear",
            "review": "The paper aims at characterizing and discussing the impact of the expressivity of a state representation on the performance. It then discusses three possible ways of improving performance with a specific regularization on the state representation. Overall the idea \nof studying the impact of expressiveness and comparing different ways of ensuring it is interesting. However, some parts in the paper are not well-supported and many important elements to understand the experiments are lacking.\n\nSome elements are not well supported and probably not true as stated.\n\"For fixed h, the expressiveness is related to the MDP M: 1) if the transition p(xt+1|xt, at)π(at|xt) of the MDP is not ergodic, i.e., it can only visit a subset of observations, then the matrix {h(X1 ), · · · , h(Xt ), · · · } will more possibly be low rank; 2) if the reward is very sparse, the representation matrix will be low rank.\"\nIf you compare two MDPs, where the first one has two states and is ergodic, while the second one has many more states but is not ergodic, do you think your consideration still applies? For a given h, why would the sparsity of the reward function plays any role?\n\nThe paper states that the \"method can be widely applied to current DRL algorithms, with little extra computational cost.\" However, it is not explained clearly how the regularization is enforced. If a supplementary loss is used and minimized at each step, the computational cost is not negligeable.\n\nIn the experiment section, the protocol is not clear. For instance the coefficient \\alpha is apparently set differently for some of the games? How are they chosen? And why are the hyper-parameters chosen not given?\n\"However, as the observation type and the intrinsic reward mechanism vary considerably for each game, we also use some other regularization term and coefficient for some of games.\"\n\nWhy are there no details related to the NN architecture?\nIn Table 1, what does it mean \"Times Better\" and \"Times Worse\"?\n\n\nOther comments:\n- In Definition 1, what does it mean X_t \\sim \\mathcal M? \\mathcal M is an MDP, not a distribution. Are the X_t taken following a given policy? are they taken on a given trajectory sequentially or i.i.d.?\n- (minor) \"From the above description, it is easy to see that the performance of an RL agent depends on two parts. First, it depends on whether the state extractor is good. With a good state extractor, the representation which is a depiction of the observation will retain necessary information for taking actions. Second, it depends on the accuracy of the policy: whether the feed-forward model can correctly take the optimal action given the state.\" Do the two elements in that paragraph mean the same: \"A good state extractor provides an abstract representation from which a performant policy can be obtained?\"\n- Figure 2: The name of the ATARI game is not mentioned.\n- In section 2.2, the MDP framework is introduced (with a state space \\mathcal S) but there is no mention of the concept of observation x that is used throughout afterwards.\n- In the conclusion, it is stated that \"Experiments of A3C and DQN on 55 Atari games demonstrate that ExP DRL can promote their performances significantly.\" However, the algorithm is not tested on 55 games with DQN.\n- The related work discusses papers about state representation but even more directly related to this paper, other papers have also discussed the importance of disentangled representation or entropy maximization for deep RL: https://arxiv.org/abs/1707.08475, https://arxiv.org/abs/1809.04506, ... And papers that discuss expressiveness in deep learning such as https://arxiv.org/pdf/1606.05336.pdf should also be discussed.\n- There are many typos/english errors.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting results, but \"insights\" seem misleading",
            "review": "The authors propose the notion of \"expressiveness\" of state representation by simply checking the effective rank of the state vectors formed by a sampled trajectory. The authors then propose a regularizer that promotes this expressiveness. Experiments on a large set of Atari games show that this regularizer can improve the performance of reinforcement learning algorithms.\n\n1. The authors motivate this notion through the example in section 2 where the model with higher capacity performs better. The authors note that the learned state matrix of the higher-capacity model has higher rank. But this is completely expected and well-known in machine learning in the regime where the models have insufficient capacity. Imagine the case where the true state representation consists of only a two-dimensional vector (say, the location vector), and an image is produced through a linear map. Now suppose we try to learn a K-dimensional state representation for K>>2 and promote a high-rank matrix. What do we expect to get from this?\n\n2. It seems that promoting a small gap between the singular values gives a performance improvement in the case of Atari games. One can easily interpret this as a regularizer that simply prevents overfitting by equalizing the magnitudes in most parameter values. In this sense, I do not see a fundamental difference between this regularizer and say, the L2 norm regularizer. Did the authors try comparing this with an L2 norm regularizer?\n\n\nThe authors present very interesting empirical results, but I am not convinced that the proposed notion of \"expressiveness\" properly explains the performance improvement in this set of tasks. I am also not convinced that this is the right notion to promote in general. In this sense, I am afraid I cannot recommend acceptance.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}