{
    "Decision": {
        "metareview": "This was an extremely difficult case. There are many positive aspects of Graph2Seq, as detailed by all of the reviewers, however two of the reviewers have issue with the current theory, specifically the definition of k-local-gather and its relation to existing models. The authors  and reviewers have had a detailed and discussion on the issue, however we do not seem to have come to a resolution. I will not wade into the specifics of the argument, however, ultimately, the onus is on the authors to convince the reviewers of the merits/correctness, and in this case two reviewers had the same issue, and their concerns have not been resolved. The best advice I can give is to consider the discussion so far and why this misunderstanding occurred, so that it might lead the best version of this paper possible.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Good paper, but there are some issues with the theory (either correctness or clarity) that need to be resolved."
    },
    "Reviews": [
        {
            "title": "A good paper for the conference",
            "review": "Graph representation techniques are important as various applications require learning over graph-structured data. The authors proposed a novel method to embedding a graph as a vector. Compared to Graph Convolutions Neural Networks (GCNN), the proposed are able to handle directed graphs while GCNN can not. Overall the paper is good, the derivation and theory are solid. The authors managed to prove the proposed representation is somehow lossless, which is very nice. The experiment is also convincing.\n\nMy only concern is as follows. The authors claim that Eq. (1) is able to handle features on vertices or edges. However, in the current formulation, the evolution only depends on vertex features, thus how can it handle edge features?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need some clarification",
            "review": "This paper proposes a new representation learning model for graph optimization, Graph2Seq. The novelty of Graph2Seq lies in utilizing intermediate vector representation of vertices in the final representation. Theoretically, the authors show that an infinite sequence of such intermediate representations is much more powerful than existing models, which do not maintain intermediate representations. Experimentally, Graph2Seq results in greedy heuristics that generalize very well from small training graphs (e.g. 15 nodes) to large testing graphs (e.g. 3200 nodes).\n\nOverall, the current version of the paper raises a number of crucial questions that I would like the authors to address before I make my decision.\n\nFirst, some strengths of the paper:\n- Theory: although I have not reviewed the proofs in details, the theorems are very interesting. If correct, the theorems provide a strong basis for Graph2Seq. In contrast, this aspect is missing from other work on ML for optimization.\n\n- Experiments: the experiments are generally thorough and well-presented. The performance of Graph2Seq is remarkable, especially in terms of generalization to significantly larger graphs.\n\n- Writing: the paper is very well-written and complex ideas are neatly articulated. I also liked the Appendix trying to interpret the trained model. Good job!\n\nThat being said, I have some serious concerns. Please clarify if I misunderstood anything and update the paper otherwise.\n\n- Graph2Seq at test time: in section Testing, you explain how multiple solutions are output by G2S-RNN at intermediate \"states\" of the model, and the best w.r.t. the objective value is returned. If I understand all this correctly, you take the output of the T-th LSTM unit, run it through the Q-network, then select the next node (e.g. in a vertex cover solution). Then, the complexity should be O((E+V)*T_max*V), since the Graph2Seq operations are linear in the size of the graph O(E+V), a single G2S-RNN(i) takes O(V) times if you want to construct a cover of size O(V), and you repeat that process exactly T_max times, for each i between 1 and T_max. What's wrong in my understanding of G2S-RNN here? Or is your complexity incorrect?\n\n- Local-Gather definition: in your definition of the Local-Gather model, do you assume that computations are performed for a single iteration, i.e. a single local step followed by a gather step? If so, then how is Graph2Seq infinity-local-gather? What does that even mean? I understand how some of the other GCNN-based models like Khalil et al.'s is 4-local-gather (assuming 4 embedding iterations of structure2vec), but how is Graph2Seq infinity-local-gather?\n\n- Comparison to Structure2Vec: for fair comparison, why not apply Algorithm 2 to that method? Just run more embedding iterations up to T_max, and use the best among the solutions constructed between 1 and T_max.\n\nMinor:\n- Section 4: Vinyals et al. (2015) does not do any RL.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea with weaknesses in the formal and empirical parts",
            "review": "The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. \n\nThe paper has, however, three major problems:\n\nThe motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. \n\nI’m not sure about the correctness of some of the theorems. For instance, Theorem 2 states \n“For any fixed k > 0, there exists a function f(·) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.”  I’m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven’t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. \n\nThe experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets?\nIf you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}