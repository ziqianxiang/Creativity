{
    "Decision": {
        "metareview": "The authors propose an approach for a learnt attention mechanism to be used for selecting agents in a multi agent RL setting. The attention mechanism is learnt by a central critic, and it scales linearly with the number of agents rather than quadratically. There is some novelty in the proposed method, and the authors clearly explain and motivate the approach. However the empirical evaluation feels quite limited and does not show conclusively that the method is superior to the others. Moreover, the simple empirical results don't give any evidence how the attention mechanism is working or whether it is truly the attention that is affecting the results. The reviewers were split on their recommendation and did not come to a consensus. The AC feels that the paper is not quite strong enough and encourages the authors to broaden the work with additional experiments and analysis.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Interesting contribution to multiagent RL ",
            "review": "The paper considers an actor-critic scheme for multiagent RL, where the critic is specific to each agent and has access to all other agents' embedded observations. The main idea is to use an attention mechanism in the critic that learns to selectively scale the contributions of the other agents. \n\nThe paper presents sufficient motivation and background, and the proposed algorithmic implementation seems reasonable. The proposed scheme is compared to two recent algorithms for centralized training of decentralized policies, and shows comparable or better results on two synthetic multiagent problems. \n\nI believe that the idea and approach of the paper are interesting and contribute to the multiagent learning literature. \n\nRegarding cons: \n- The critical structural choices (such as the attention model in section 3.2) are presented without too much justification, discussion of alternatives, etc. \n- The experiments show the learning results, but do not provide a peak \"under the hood\" to understand the way attention evolved and contributed to the results. \n- The experiments show good results compared to existing algorithms, but not impressively so. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting new method, though more thorough experiments are needed",
            "review": "This paper introduces a new method for multi-agent reinforcement learning. The proposed algorithm -- which uses shared critics at training time but individual policies at test time -- makes use of a specialised attention mechanism. The benefits include better scalability (as the dependency of the inputs is linear in the number of agents, rather than quadratic), and also being more amenable to diverse reward and action structures than the previous work. \n\n---------Quality and clarity---------\nThe paper is nicely written, and the ideas are developed in a clear fashion, if slightly verbose (the first 3 pages, though informative, might have been condensed a bit to make more room for the new algorithm). The problem is well-motivated and the benefits of the new algorithm are well showcased.\n\nOne negative point that does stick out is the bibliography, where papers that have been published for years (e.g. the Adam paper) are still referenced as arXiv preprints.\n\n---------Originality and significance----------\nAlthough attentive mechanisms have been around for a while, their use in this specific setting (learning shared critics for multi-agent RL) is, and yields desirable properties. The new algorithm opens the door for training in more complex environments, with a larger number of agents (although the number is still limited in the presented experiments).\n\nThe main issue I do see with the paper is its experimental section. \nThe two tasks are picked to showcase the benefits of the new approach. This does mean that the competing algorithms have to undergo significant changes (at least in the case of the DDPG-based methods), which takes away from the validity of the comparison. \n\nIdeally, there would be at least one other task on which the other algorithms have been trained on by their respective authors. As mentioned right before Section 4, MAAC can be used on continuous action spaces at the price of increased computational cost, so this should be doable.\n\n\nOverall, this is a nicely written paper which introduces an interesting new method for multi-agent RL, with promising initial results. A more thorough experimental section with slightly fairer comparisons would increase its quality significantly.\n\nPros\n- clear paper, easy to read\n- interesting application of attention mechanism to multi-agent RL\n- promising initial results\n\nCons\n- no comparison to related algorithms on tasks where they have already been evaluated externally\n- the amount of workers is still quite limited in the experiments",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple method, but gives insufficient insight in model behavior and how it could generalize",
            "review": "Summary\n\nAuthors present a decentralized policy, centralized value function approach (MAAC) to multi-agent learning. They used an attention mechanism over agent policies as an input to a central value function. \n\nAuthors compare their approach with COMA (discrete actions and counterfactual (semi-centralized) baseline) and MADDPG (also uses centralized value function and continuous actions)\n\nMAAC is evaluated on two 2d cooperative environments, Treasure Collection and Rover Tower. MAAC outperforms baselines on TC, but not on RT. Furthermore, the different baselines perform differently: there is no method that consistently performs well.\n\nPro\n- MAAC is a simple combination of attention and a centralized value function approach.\n\nCon\n- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents. \n- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.\n- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems. \n- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP). It is unclear how the model actually operates and uses attention during execution.\n\nReproducibility\n- It seems straightforward to implement this method, but I encourage open-sourcing the authors' implementation.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}