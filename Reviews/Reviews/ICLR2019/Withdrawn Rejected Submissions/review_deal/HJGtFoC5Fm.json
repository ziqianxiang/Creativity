{
    "Decision": {
        "metareview": "This paper has received reviews from multiple experts who raise a litany of issues. These have been addressed quite convincingly by the authors, but I believe that ultimately this work needs to go through another round of reviewing, and this cannot be achieved in the context of ICLR's reviewing setup. I look forward to reading the final version of the paper in the near future.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "robust reviewing process; rewrite required"
    },
    "Reviews": [
        {
            "title": "Review for \"On the Margin Theory of Feedforward Neural Networks\"",
            "review": "The authors claim to prove three things: (1) Under logistic loss (with a vanishing regularization), the normalized margin (of the solution) converges to the max normalized margin, for positive homogenous functions. This is an asymptotic result: the amount of regularization vanishes. (2) For one hidden layer NN, the max margin under l_2 norm constraint on weights in the limit, is equivalent to the l_1 constraint (total variation) on the sign measure (specified by infinite neurons) for the one hidden layer NN. (3) Show some convergence rate for the mean-field view of one hidden layer NN, i.e., the Wasserstein gradient flow on the measure (of the neurons). The author show some positive result for a perturbed version.\n\nThe problem is certainly interesting. However, my main concerns are: (1) the novelty of the main theorems given the literature, and (2) the carefulness of stating what is known in the literature review.\n\nIn summary:\n1. Theorem 2.1, Theorem 3.1, and Theorem 3.3 are anticipated, or not as critical, given the literature (detailed reasons in major comments).\n\n2. The construction in Theorem 3.5 is nice, but, it is only able to say an upper bound of the generalization of kernel is not good (comparing upper bounds is not enough). In addition, For Theorem 4.3. [Mei Montanari and Nguyen 2018] also considers similar perturbed Wasserstein gradient flow, with many convergence results. One needs to be more careful in stating what is new.\n\n\nMajor comments:\n1. Theorem 3.3 (and Theorem 3.2) seems to be the most interesting/innovative one.\nHowever, I would like to argue that it might be natural in one line proof, with the following alternative view:\n\n--\nl_2 norm constraint normalized margin, one hidden layer NN, with infinite neurons\n\ngamma^star, infty :=\n\\max \\min_i y_i int_{neuron} w || u || ReLU( x_i \\bar{u}) dS^{d-1} -- integral over normalized neurons over sphere\n\nunder the constraint\nint_{neuron} (w^2 + ||u||^2) dS^{d-1} \\leq 1\n\nThis is equivalent to the l_1 constraint margin (variation norm), one hidden layer NN,\ngamma_l_1 :=\n\\max \\min_i y_i int_{neuron} rho(u) ReLU( x_i \\bar{u}) dS^{d-1} -- integral over normalized neurons over sphere\n\nunder the constraint\nint_{neuron} |rho(u)| dS^{d-1} \\leq 1/2\n\nhere rho(u) is the sign measure represented by neurons. Simply because at the optimum\nw || u || = 1/2 ( w^2 + || u ||^2) := rho(u)\ntherefore\ngamma^star, infty =  gamma_l_1\n\nSo one see the factor 1/2 exactly.\n--\n\nIn addition, [Bach 18, JMLR:v18:14-546] discuss more in depth the l_1 type constraint\n(TV of sign measure) rather then l_2 type constraint (RKHS) for one hidden layer NN with infinite neurons. The authors should cite this work.\n\nIt is clear that l_1(neuron) < l_2(neuron) therefore\nl_2 constraint margin is always smaller than l_1 constraint margin.\n\n2. Theorem 2.1. I think the proof is almost a standard exercise given [Rosset, Zhu, and Hastie 04].\nThe observation for it generalizes to positive homogenous function beyond linear is a nice addition, but not crucial enough to stand out as an innovation.\n\nMuch of the difficulty in related paper lies in achieving non asymptotic convergence rate to max margin solution, for logistic loss [Soudry, Hoffer and Srebro 18], or what happens when data is not perfectly separable [Ji and Telgarsky 18].\n\n3. Generalization result Theorem 3.1. Maybe it is better to state as a corollary, given the known results in the literature, in my opinion. This generalization is standard result from margin-based bounds available\n[Koltchinskii and Panchenko 02, Bartlett and Mendelson 02].\n\nIn addition, the authors remark that the limit for (3.3) may not exist. You can change to limsup, your footnote[4]\nis essentially the limsup definition.\n\n4. Theorem 3.5. This construction of the data distribution is the part I like. However, you should remind the reader that\nhaving a small margin for the kernel only implies the the upper bound for generalization is bad.\nComparing the upper bound doesn't mean kernel method is performing bad for the instance.\n\nFrom a logic view, it is unclear the benefit of Theorem 3.5.\nI do agree one can try to see in simulation if kernel/RKHS approach (l_2) is performing worse for generalization, for one hidden layer NN. But this is separate from the theory.\n\n5. Theorem 4.3. This result should be put in the context of the literature. Specifically\n[Mei Montanari and Nguyen 2018], Eqn 11-12. The perturbed wasserstein flow the authors considered\nlooks very close to [Mei Montanari and Nguyen 2018], Eqn 11-12, admittedly with the logistic loss instead of the square loss.\n\nRight now, as stated in the current paper, it is very hard for the general audience to understand the contribution. A better job in comparing the literature will help.\n\nFor the technical crowd, maybe emphasize on why the \"simga\" can help you achieve a positive result.\n\nMinor Comments:\n\n6. One additional suggestion: seems to me Section 4 is a bit away from the central topic of the current paper.\n\nI can understand that the optimization/convergence result will help complete the whole picture. However, to contribute to the \"margin theme\", it would be better to state with the \"small vanishing regularization\", how it affects the convergence of Theorem 4.3.\nEven with this, it is unclear as one don't know how to connect different part of the paper: with what choice of vanishing regularization will generate a solution with a good margin, using the Wasserstein gradient flow.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theorem 2.1 -2.2 are interesting",
            "review": "Overall I found that the paper does not clearly compare the results to existing work. There are some new results, but some of the results stated as theorems are immediate consequence of existing work and a more detailed discussion and comparison is warranted. I will first give detailed comments on the establishing the relationship to existing work and then summarize my evaluation. \n\n————\nDetailed comments on contributions and relationships to existing work.\n\nA. Theorem 2.1 establishes the limit of the regularized solutions as the maximum margin separator. \nThis result is a generalization the analogous results for linear models Theorem 3 in Rosset et al. (2004) “Boosting as a regularized path to maximum margin separator” and Thm 2.1 in Rosset Zhu Hastie “margin maximizing loss functions”  (the later paper missing from references, and that paper generalizes the earlier result for multi-class cross entropy loss). \nMain difference from earlier work:\n1. extends the results for linear models to any homogeneous function \n2. (minor) the previous results by Rosset et al. were stated only for lp norms, but this is a minor generalization since the earlier work didn’t at any point use the lp-ness of the norm and immediately extends for any norms. \n\nSecondly, Theorem 2.2 also gives a bound on deviation of margin when the regularization is not driven all the way to 0. I do think this theorem would be differently stated by making the explicitly showing dependence of suboptimal margin \\gamma’ on lambda and the sub optimality constant of loss. This way, one can derive 2.1 as a special case and also reason about what level of sub-optimality of loss can be tolerated. \n\nB. Theorem 3.1 derives generalization bounds of learned parameters in terms of l2 margin. \n—this and many similar results connecting generalization to margins have already been studied in the literature (Neyshabur et al. 2015b for example covers a larger family of norms than just l2 norm). Specially an analogous bound for l1 margin can also be found in these work which can be used in the discussions that follow. \n\nC. Theorem 3.2: This result to my knowledge is new, but also pretty immediate from definition of margin. The proof essentially follows by showing that having more hidden units can only increase the margin since the margin is maximized over a larger set of parameters. \n\nD. Comparison to kernel machines: Theorem 3.3 seems to be the paraphrasing of corollary 1 in Neyshabur et al (2014). But the authors claim that the Theorem 3.3 also holds when “the regularizer is small”. I do not understand what the authors are referring to here or how the result is different form existing work. Please clarify\n\n-----------\nIn summary, The 2.1-2.2 on extension of the connection between regularized solution and maximum margin solution to general homogeneous models and to non-asymptotic regimes \n-- this is in my opinion key contribution of the paper and an important result. But there is not much new technique in terms of proof here\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of 'On the Margin Theory of Feedforward Neural Networks'",
            "review": "UPDATE: after revisions and discussion. There seems to be some interesting results presented in this paper which I think would be good to have discussed at the conference. This is conditional on further revisions of the work by the authors.\n\n\nThis paper studies margin theory for neural nets.\n\n1. First it is shown that margin of the solution to regularized problem approaches max margin solution.\n2. Then a bound is given for using approximate solution to above optimization problem instead of exact one. Note that the bound depends on size of the network via parameter a.\n3. Then 2-layer relu networks are studied. It shown that max margin is monotonically increasing in size of the network. Note however, it is hard to relate this results to inexact solutions since the bound in that case as was pointed out also depends on the size of the network.\n4. Paper also provides comparison with kernel methods, simulations and shows that perturbed wasserstein flows find global optimiziers in poly time.\n\nThe paper argues that over-parameterization is good for generalization since margin grows with the number of parameters. However, it should be also noted the radius of data may also grow (and in case of the bounds it seems to be the radius of data in lifted space which increases with the size of the network). I hope authors can clarify this and points 2 and 3 above in their response. In the current form the paper is below the acceptance threshold for me. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting insights on inductive bias of two-layer ReLU networks",
            "review": "This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations. By combining several results, the authors obtain a generalization upper bound which does not increase with the network size. Furthermore, they show that the maximum normalized margin is, up to a scaling factor, the l1 svm margin over the lifted feature space of an infinite-size network. Finally, in a setting of infinite-sized networks, it is proved that perturbed Wasserstein gradient flow finds a global minimum in polynomial time.\n\nI think that the results are interesting and relevant to current efforts of understanding neural networks. The techniques and ideas seem promising and may be applied in more general settings. The paper is mostly clearly written, but there are some issues which I outline below.\n1.\tIt is not clear what is the novelty in sections 2 and 3.1 except the combination of all the results to get a generalization bound which does not increase with network size (which on its own is non-trivial). Specifically, \na.\tWhat is the technical contribution in Theorem 2.1 beyond the results of the two papers of Rosset et al. (journal paper and the NIPS paper which was mentioned in the comment on missing prior work)?\nb.\tHow does Theorem 3.1 compare with previous Rademacher bounds for neural networks which are based on the margin? In Neyshabur et al. (2018), it is shown that margin-based generalization bounds empirically increase with network size. Does this hold for the bound in Theorem 3.1?\n\n2.\tIn the work of Soudry et al. (2018) section 4.3, they consider deep networks with an unregularized loss and show that gradient descent converges to an l2 max margin solution under various assumptions. What is the connection between this result and the l1 max margin result in section 3.3?\n\n3.\tWhat are the main proof ideas of Theorem 4.3? Why is the perturbation needed?\n\n4.\tWhat is the size of the network that was trained in Section 5 in the experiments of Figure 3? Only the size of the ground truth network is mentioned.\n\n\n---------Revision------------\n\nI have read the author's response and other reviews. I am not changing the current review.\nI have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}