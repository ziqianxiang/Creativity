{
    "Decision": {
        "metareview": "This paper proposes a method called approximate empirical Bayes to learn both the weights and hyperparameters. Reviewers have had a mixed feeling about this paper. Reviewers agree that the novelty of this paper is limited since AEB is already a well known method (in fact, iterative conditional modes is a well known algorithm). Unfortunately, the paper completely ignores the huge literature on this topic; the previous reference to use AEB is from McInerney (2017).\n\nAnother issue is that the paper seems to be unaware of any issues that this type of approach might have. Here is a reference that discusses some problems with this type of approach: \n\"Deterministic Latent Variable Models and their Pitfalls\"\nMax Welling∗ Chaitanya Chemudugunta, Nathan Sutter, 2008\n\nThe experiments presented in the paper are interesting, but then are not really doing a good job to assess why the method works well here even though in theory it should not be as good as the exact empirical Bayes method. \n\nThis paper does not meet the bar for acceptance at ICLR and therefore I recommend a reject for this paper.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Novelty is limited and related work is missing. "
    },
    "Reviews": [
        {
            "title": "Empirical Bayes ideas in neural networks is a great idea. This paper is too heuristic, but the results are encouraging.",
            "review": "This paper first proposes an intuition that when training, neural network weights tend to be correlated. It then suggests to make this correlation more specific, and does so using a heuristic algorithm based on approximate empirical Bayes.\n\nThe paper is generally well written, in the sense that everything that is done is clearly stated. However why things are done the way they are is less clear.\n\n-\tThe correlation intuition given in the introduction isn’t quite right (if the training does it anyway, why try to push it even more?). The better way to think about the benefit of correlations is to say that some weak form of weight sharing may help.\n\n-\tThe approximate notion of empirical Bayes is not well justified.  The arguments given are all about concentration and essentially unimodality. The successive MAP perspective is interesting, but again not clear why we the joint maximization of (4) won’t place us far from the smoothed version of (1).\n\n-\tNot only is the AEB principle a heuristic, but the implementation via Algorithm 1 is itself a heuristic to this heuristic. This is because the optimization of the model parameters still remains approximate (due to the local search aspect), even if the optimization of the hyperparameters is shown to be exactly solvable. In the end we end up with a stationary point of the AEB objective, for which we have even less theoretical insight, even if the empirical evidence is promising.\n\n-\tA couple of comments about algorithm 1. First, the \\theta notation is flipped to refer to the model parameters, instead of the hyperparameters, and it should be fixed. Second, the role of the constant u and v is a bit unclear. On the surface, they provide extra regularization to the hyperparameters. But then, how do we choose them?\n\n-\tThe experimental results are a bit oversold. First, we have SGD methods that do thrive in smaller batch size regimes, and it’s disingenuous to handicap them with larger batch sizes, when they were performing comparable at smaller ones. It *is* interesting that AED makes the learning batch-size insensitive, and I wish that was elaborated more, to see if it’s a prevalent property in other data sets too. Also, the authors define better local optima is by saying that they reach a lower value of training loss. This is claimed and shown. Usually we think of them as those leading to better generalization. This is claimed, but not shown that these are indeed those that generalize better. Figure 3 (which I assume shows training cross-entropy) shows one outcome among many (the many that are averaged in the other plots), and it could very well be that this outcome did not generalize as well.\n\nDespite these shortcomings, I believe this paper is another welcome push to introducing empirical Bayes ideas into neural networks (though it’s not the first), and the empirical evidence seems to indicate that there is indeed something there to investigate further, so I give it a weak accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Kronecker-factored weight-decay with parameters learned via empirical Bayes has the potential to be better than standard L2 weight decay, but the novelty, motivation and empirical evaluation are not convincing",
            "review": "Summary: The submission proposes a method to learn Kronecker factors of the covariance of a matrix-variate normal distribution over neural network weights. Given a setting of the neural network weights, the Kronecker factors can be found in closed form by solving a convex optimization problem. \n\nStrengths:\n+ The paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical Bayes setup.\n\nWeaknesses:\n- The novelty of the method is overstated.\n- The method is claimed to be efficient, but each iteration requires an inner loop of solving a MAP problem (via gradient descent on the negative log likelihood with an extra regularization term), which is at least as expensive as a standard training run.\n- The submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.\n- The experimental evaluation is not strong.\n\nMajor comments:\n- The use of empirical Bayes is not novel in the context of neural networks despite the submission's claim that \"Existing studies on\nparametric empirical Bayes methods focus on the setting where the likelihood function and the prior are assumed to have specific forms\" (pg. 1). In particular, see e.g., https://papers.nips.cc/paper/6864-an-empirical-bayes-approach-to-optimizing-machine-learning-algorithms.pdf, https://arxiv.org/abs/1801.08930, https://arxiv.org/abs/1807.01613 for the use of non-conjugate likelihoods in empirical Bayes.\n- The cited motivation for the use of a matrix-variate normal prior over the weights of a neural network is weak. In particular, one iteration of credit assignment via backpropagation in a one-layer neural network does not adequately describe the complex interactions between parameters of a nonlinear model over the course of an optimization procedure such as the one used in line 2 of Algorithm 1. In addition, a learned prior can be used to introduce additional correlations between parameters, so it is strange to describe the learned prior as \"capturing correlations\" resulting from a single weight update.\n- The submission lacks clarity on the assumptions entailed by using the proposed methodology, namely that the Kronecker factors are assumed positive definite. For instance, the logdet function is defined for positive definite (PD) matrices, the results in the paragraph titled \"Approximate Volume Minimization\" hold only for PD matrices, and the InvThresholding procedure is valid for only the same class. It should additionally be noted that this assumption is *not* required for a Kronecker factorization to be defined.\n- The submission makes heavy use of Kronecker factorization, but neglects to cite works that use a similar factorization of the covariance matrices for neural network applications (e.g., https://arxiv.org/abs/1503.05671, https://arxiv.org/abs/1712.02390). Furthermore, the method bears a strong similarity to https://arxiv.org/abs/1506.02117 in learning a Kronecker-factored covariance structure between parameters of a neural network. Can the authors comment on the similarities and differences?\n- Results are reported on a simple regression task (SARCOS) and multiclass classification problems (MNIST & CIFAR10) using a neural network with a single hidden layer. Moreover, \"in all the experiments, the AEB algorithm is performed on the softmax layer\" (pg. 7) and the justification for this in the \"Ablations\" section is opaque to me. Was a similar restriction used for L2 weight decay regularization? I can't interpret how thorough the evaluation is without such details. It is also not clear that the approach is extensible to more complex architectures, or that there would be a significant empirical benefit if this is done. \n- Figure 5 does not really exhibit interesting learned structure in the correlation matrix. Why not plot a visualization of the learned prior, rather than the weights?\n\nMinor comments:\n- The submission needs to be checked for English grammar and style.\n- abstract: \"Learning deep neural networks could be understood as the combination of representation learning and learning halfspaces.\" This is unclear.\n- pg. 2: \"Empirically, we show that the proposed method helps the network\nconverge to better local optima that also generalize better...\"  What is a \"better\" local optimum?\n- Section 3.1 describes empirical Bayes with point estimates. Please make it clearer that this methodology is not itself a contribution of paper by citing prior work.\n- pg. 5: \"Alg. 1 terminates when a stationary point is found.\" What exactly is the stopping criterion?\n- pg. 6: The labels in Figure 2 are extremely small. Moreover, please keep the y-axis range constant.\n- pg. 6, Figure 2 caption: \"AEB improves generalization under both minibatch settings and is most beneficial when training set is small.\" Do the CIFAR10 results not show the opposite effect, that the regularization is most beneficial when the training set is large?\n- pg. 7: \"Batch Normalization suffers from large batch size in CIFAR10\" weird wording\n- pg. 8: \"One future direction is to develop a better approximate solution to optimize the two covariance matrices from the marginal log-likelihood function.\" This is unclear.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper presents a novel approach to addressing the problem of overfitting when training large networks on small data sets. The main idea is to use a dependent prior on the network weights (instead of commonly used independent normal priors) in order to allow the weights \"learning from the experience of others\".",
            "review": "Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.\n\nThe current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.\n\n Strengths:\n\n This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.\n\n Minor comments:\n\n * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.\n * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}