{
    "Decision": {
        "metareview": "Authors present a technique to learn embeddings over physiological signals independently using univariate LSTMs tasked to predict future values. Supervised methods are them employed over these embeddings. Univariate approach is taken to improve transferability across institutions, and Shapley values are used to provide interpretable insight. The work is interesting, and authors have made a good attempt at answering reviewers' concerns, but more work remains to be done.\n\nPros:\n- R1 & R3: Well written.\n- R3: Transferrable embeddings are useful in this domain, and not often researched.\n\nCons: \n- R3: Method builds embeddings that assume that future task will be relevant to drops in signals. Authors confirm.\n- R3: Performance improvement is marginal versus baselines. Authors essentially confirm that the small improvement is the accurate number.\n- R2 & R3: Interpretability evaluation is not sufficient. Medical expert should rate interpretability of results. Authors did not include or revise according to suggestion.\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Method to create unsupervised feature embeddings over physiological signals. Interesting study, but needs additional work."
    },
    "Reviews": [
        {
            "title": "Transfer learning for physiological signals in the OR and ICU",
            "review": "The authors present a new method for learning unsupervised embeddings of physiological signals (e.g. time series data) in a healthcare setting. The primary motivation of their paper is transfer learning - the embeddings created by their approach are able to generalize to other hospitals and healthcare settings. \n\nOverall I did like this paper. I found it to be easy to read, well motivated, and addressing an important problem in the healthcare domain. As a researcher in this area, it is very true that we are all using our own \"siloed\" data and do not generally have access to large pre-trained models. I hope that others will produce these kinds of models for the community to use. The authors do not explicitly state that they plan to release their code and pre-trained models, but I sincerely hope that is there intent. If they do not plan to do this, then the impact of this work is dramatically reduced. \n\nHowever, I do have a few concerns about the paper, listed below:\n\n- It might not be fair to truly call this an unsupervised model. The labels used for evaluation are thresholds on the signals themselves (e.g. SaO2 < 92%) , so the \"unsupervised\" model actually receives some form of supervision, at least using the current evaluation method. Using a truly different prediction task not directly based on the physiological signals (e.g. mortality, complication during surgery, etc) would provide a cleaner example of unsupervised embeddings that are useful for transfer learning.\n\n- Differences between PHASE and EMA are statistically significant but unlikely to be clinically meaningful - the largest absolute difference in AP is 0.04, and most are much smaller than this. It's unclear if the performance gains enjoyed by PHASE would meaningfully change clinical decision making in any significant way.\n\n- I appreciate the use of XGBoost due to its impressive Kaggle performance, but it strikes me as odd that the authors did not try to fine tune their base model, as that is standard practice for transfer learning. The successes they point to in CV and NLP all use a fine tuning approach, so the evaluation seems incomplete without a performance assessment of fine tuning the base model. \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well-motivated, well-written, but some issues with the experiments.",
            "review": "Summary of the paper:\nThis paper proposes PHASE, a framework to learn the embeddings for physiological signals from medical records, which can be used in downstream prediction tasks, possibly across domains (i.e. different patient distribution). The authors employ separate LSTMs for each signal channel that are trained to predicts the minimum value of the signal in the fixed future time window (5 minutes in this paper). After training the LSTMs, the learned signal embeddings are fed to gradient boosted trees for a specific prediction task (e.g. predicting whether hypoxemia will occur in 5 minutes). Once the LSTMs are trained, they can be re-used for another dataset; the LSMTs are fixed, and generate embeddings that are fed to a new trainable gradient boosted trees for performing a similar task. The authors also combine existing attribution methods (DeepSHAP and Independent TreeSHAP) to provide some explanation of PHASE. The authors use three different datasets to test PHASE's prediction performance, transferability of the embeddings, and interpretation.\n\nPros:\n- The paper is well-motivated, well-organized and clearly written. The reading experience was smooth.\n- Given the importance of physiological signals in ICU settings, transferable embeddings can be an important technique in practice\n- As the authors claim, I am not aware of any notable prior work on transferable physiological signal embeddings. The authors tackle a relatively unexplored territory.\n\nIssues:\n- The authors claim PHASE learns signal embeddings that are transferable. However, the authors train the embeddings to predict the minimum value within the next five time steps, because the downstream tasks are all predicting whether a certain signal goes below some threshold (\"hypo\"xenia, \"hypo\"capnia, \"hypo\"tension). This means the authors designed the embedding learning process with a priori knowledge of the downstream tasks, which significantly weakens theirs claim that PHASE learns transferable embeddings. Word embeddings trained on Wikipedia, or ConvNets trained on ImageNet are not designed to be used in a specific type of downstream tasks. What PHASE demonstrates is basically that \"hypo\"xxxx predictions can be accurately made with pre-training the embeddings to predict a very relevant task. \n- The authors claim that transferred PHASE embeddings significantly outperform EMA or Raw. But I wouldn't call 0.005-0.02 AP improvement \"significant\". Model 12 in Figure 3 shows better performance than model 2 and 4, but the gap is not that large.\n- More importantly, the fact that model 10 and model 12 show similar performance is not very surprising. The two hospitals are in the same city, only miles away. Naturally the distribution of the patients would not be too different. Given this, claiming that PHASE embeddings are transferable does not have a strong ground.\n- The claim for transferable embedding is further weakened by Figure 4. Model 1^p in Figure 4 clearly performs worse than Raw, which means embeddings learned from significantly different setting (hospital P) is actually making it harder for XGB than simply looking at raw signals. If PHASE was learning a robust embeddings, then the learned embeddings should at least not hurt the performance of XGB.\n- Evaluating the interpretation of the model is weak. All the authors did was pick four examples and provide qualitative explanation. And they do not even describe whether this interpretation is from model 9 or 10. It would have been much better if at least one medical expert took a look at more than a few examples. In the current form, we cannot be sure if the model is using the SaO2 signal in a medically meaningful way. Also, if this is the interpretation of model 10 or 12, then we should look at the attributions for other signals as well.\n- Lack of description on experiment setup. The authors do not describe how they pre-trained the LSTMs to obtain Min^h, Auto^h and Hypox^h, which significantly hurts reproducibility. Also I couldn't find any description regarding train/test splits or cross validations, or size of the LSTM cells.\n- More description is necessary as to how Raw was used to train XGB. Was the entire sequence of 15 signals fed to XGB?\n- Y-axis of Figure 5 is not on the same scale. This makes it hard to intuitively understand the change of SaO2.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents an approach to produce embeddings for physiological signals that are interpretable.",
            "review": "The authors claim contributions in three areas:\n1) Learning representations on physiological signals. The proposed approach  uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms \n LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else?\n\n2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. \n\n3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. \n\nThe experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the “transference” arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare “hospitals”. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. \n\nIt is quite hard to argue from what’s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 “makes sense” as stated in the paper? What is the point that’s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability?\n\nOverall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}