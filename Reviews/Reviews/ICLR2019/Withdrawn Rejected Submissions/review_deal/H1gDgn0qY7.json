{
    "Decision": {
        "metareview": "The paper presents a novel view on adversarial examples, where models using\nReLU are inherently sensitive to adversarial examples because ReLU activations\nyield a polytope of examples with exactly the same activation. Reviewers\nfound the finding interesting and novel but argue it is limited in impact.\nI also found the idea interesting but the paper could probably be improved\nas all reviewers have remarked. Overall, I found it borderline but probably not enough for acceptance.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "interesting observation and techniques, but results leave something to be desired",
            "review": "This paper studies a non-local form of adversarial perturbation, which, to my limited knowledge is new. The form of the perturbation is specific to ReLU activations, but it may be a large set. The authors also devise an algorithm to generate natural-looking perturbations in this set. Instead of updating a seed example through gradient descent, they propose to generate perturbations by combinations of image patches from multiple seed images. The weights of the combination are optimized by a gradient descent like algorithm in a similar manner as standard gradient-based approaches to generating adversarial examples. This produces perturbations that look like ```in-paintings'' or transplants of one seed image onto another. Here are a few comments:\n\n1. The perturbation set is generally a high-dimensional polytope. Although it has a compact representation in terms of intersection of hyperplanes, it may have many more verticies, so the endeavor of attempting to characterize all the verticies of this polytope may be infeasible. \n\n2. This technique of generating adversarial examples from combinations of image patches seems generally applicable, but it does not seems to produce good results here. The perturbations are still unnatural looking (eg. the images in Figure 7 are not exactly natural looking).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but limited study",
            "review": "This paper follow recent trend of adversarial examples which is on generating images with small differences in the input space, but that are misclassified by a large margin by a neural net. The key idea of the paper is that any negative component before a ReLU activation share the same zero feature after the ReLU. Thus, any neural network that has ReLU activations have a polytope in the input space that will have identical activations in the later layers. Based on this observation, the paper assert that such polytope always exist and describe how to find its corners with a gradient descent based method. Two simple experiments on MNIST and ImageNet datasets are carried to show the feasibility of the method in practice and the existence of images with feature collision, together with their average L2 distance from real images. Since the images are clearly not \"natural\" images, a further method based on selecting patches of real images is reported and tested on ImageNet. This shows that the approach can be further applied on macro-level differences.\n\nStrengths\n+ The observation of the existence of the polytope in presence of ReLU activation is interesting and can probably be used to further refine attacks for generating adversarial examples.\n+ The paper is clear and is comprehensive of all the basic steps.\n+ Examplar experiments show the possibility of using the key idea to generate adversarial examples\n\nWeaknesses:\n- The experiments are very limited and show just 5 examples of generated images on MNIST and ImageNet. In Sect 3.2 it is observed that it is hard for human eyes to notice the difference but that is clearly not the case for the figure reported. The same for Fig. 7 on the macro-level which are even more distorted. Although this is minor, since the method is still shown to be working, the statements on the similarity of images seem incorrect. Beside the qualitative examples, the measurement of average similarity based on L2 is not so indicative at the perception level, but still interesting to see.\n- No comparison with other methods to generate adversarial examples are reported (e.g. Shafani et al 2018, Szegedy et al. 2013).\n\nMinor issues:\n- Figure 2, Figure 3 show the results, but it would also be interesting to observe what happens from the starting image to the final generated images.    \n- Personally, I prefer to see related work after the introduction section. Reading it at the end breaks the flux of the paper.\n- The observation is only applicable to ReLU activations (but other activation functions may be in the last layer), limiting the impact of the paper.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting premise of adversarial polytopes, but fall on implication and attempt to create macro-level different examples.",
            "review": "This paper presents an algorithm for finding a polytope of adversarial examples. This means that within a convex hull, you can move around freely and get a new adversarial example at each point, while still maintaining misclassification. It then couples this with a method of generating nearest neighbor patch-based images in an effort to create \"macro-level different\" examples. The premise is interesting, but the implications are questionable and I do not find the work in macro-level differences to be sound. This could be based in misunderstandings, so please let me know if you think that is the case.\n\nStrengths:\n- The notion of the polytope is interesting and the algorithm for finding such polytope seems perfectly reasonable.\n- I think the goal of macro-level adversarial examples is interesting.\n\nWeaknesses:\n- First of all, the 5 corners of the polytope all look the same to me (for instance fig 2). This is not encouraging, because it means that every single point in the polytope will also look exactly like the corners. To be frank, this means the polytope is not that interesting and has only found an extremely small pocket of adversarial examples. If you use a regular method of finding a single adversarial example, I'm sure the outcome wouldn't change within some ball around the sample (perhaps with very small radius, but nonetheless). In fact, a comparison between that ball's volume and the volume of the polytope would be interesting.\n- The implication of these polytopes is not at all clear if it doesn't really allow us to generate adversarial example of a new flavor. The investigation into macro-level differences does not help the case, as I will explain.\n- I am not at all convinced that there is any meaning to the examples with \"macro-level differences.\"  It's a bit unclear to me how many patches are used per image, but assuming that a patch is centered over each pixel,  it would mean that we have as many control parameters as we have pixels, which assuming the pixels each have three color values, is just 1/3 of the original degrees of freedoms. Now, the patches probably do constrain what we can paint a bit, but since the patches are applied with a pyramid, it means the center pixel will contribute more than any other for a given patch, so I'm not so sure. I'm not convinced that we can't come up with linear combinations of these patches that produce highly non-natural images with \"micro-level\" adversarial patterns. In fact, I think section 4.1 and figure 7 provide evidence to the contrary. Let me explain:\n    - Section 4.1: Why do you need a total variation penalty at all if you have constructed a patch-based drawing method that is supposed to be unable to produce unnatural high-frequency patterns? If you only had a handful of patches and they were all non-overlapping, then this would be impressive and.\n    - Figure 7: We can clearly see high-frequency patterns that create the shadow of an obelisk in 7(a). I think the same is true for \"erase\", although the pattern is not as recognizable. The examples actually look more suspicious than regular adversarial examples, since it looks like the original image has simply been blurred, which means the adversarial perturbations are more clear. I understand that these patterns were created using a complicated scheme of natural patches, but I think you made this method too powerful. The one interesting quality is the bottom right of the trimaran which looks like a shark - however, that is a singular occurrence in your examples and it certainly feels like the high-frequency patterns will contribute much more to class than the shark itself.\n- Please let me know if I am misinterpreting the importance of the results in Figure 7, since this is an important culmination of this work.\n\nOther comments:\n- Some of notation is a bit confusing. In (1), why is p not bold but x and t are bold? They are all vectors. In Algorithm 1, x is not bold anymore.\n- Algorithm 1 also seems quite unnecessary to include so explicitly.\n- Isn't a bounded polytope called a \"simplex\"? Perhaps there is a distinction that I'm not aware of, but the absence of the word \"simplex\" throughout the whole paper surprised me a bit. Perhaps this is a perfectly correct omission due to differences that I'm not aware of.\n\nMinor comments:\n- abstract, \"We propose a way to finding\" -> either \"to->\"of\" or \"find\"\n- page 3, \"and we can generate new colliding example\" -> \"a new colliding example\"\n- page 3, \"taking arbitrary an convex combinations\" -> \"combination\"\n- page 3, \"Given a target x\", I think you mean \"Given a target t\"\n- page 5, \"As many gradient-based method\" -> \"methods\"\n- page 8, \"carton\"? \"rubber\"? Those are not in figure 7(b).\n- page 10, \"are crucial to less non-robust\" ? This sentence (which is the final sentence of the conclusion and thus has a certain level of importance) is not something that is novel to your paper. The impact of non-linearities on adversarial examples have been well-studied.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}