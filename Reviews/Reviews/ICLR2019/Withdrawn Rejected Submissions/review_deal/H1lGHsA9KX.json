{
    "Decision": {
        "metareview": "It is a simple but good idea to consider the choice of mini-batch size as a multi-armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size.\n\nThe main concerns from the reviewers are that (1) treating the choice of hyper-parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini-batch size, (2) the improvement in the test error is not significant. The authors' feedback did not solve the concerns raised by R2.\n\nThis paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper-parameter selection problems.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Good simple idea but with limited novelty and gain of performance."
    },
    "Reviews": [
        {
            "title": "Well-motivated idea seemingly showing a small improvement in performance, but at little cost and with a potential increase in robustness",
            "review": "The authors consider the problem of determining the minibatch size for SGD by first fixing a set of candidate sizes, and then learning a distribution over those sizes using a MAB algorithm. A minibatch size is first sampled from the distribution, then one training epoch is performed. A validation error is then computed, and if it is lower than that of the last epoch, the cost of the minibatch is taken to be zero (otherwise one), and the distribution is updated. This is Algorithm 1.\n\nIn Section 4.2, they prove a regret bound, but I don’t think that regret is really the correct notion, here (although it’s very close). This is a subtle point, so I’ll set up some notation. Let w(b_1, ..., b_t) be the result at the tth epoch, if the batch sizes b_1, …, b_t were used at the 1st through tth epochs. Let y(w,b) be 0 if training one epoch starting at w with batch size b would improve the validation error, and 1 otherwise.\n\nThey show (unnumbered inequality on the middle of page 5) that \\sum_t y(w(b_1,...,b_{t-1}),b_t) is close to \\sum_t y(w(b_1,...,b_{t-1}),b^*), where b_t is the batch size that was chosen at time t, and b^* is the best fixed batch size. The key point here is that the comparator (the second sum) starts each epoch at the result that was found by their adaptive algorithm, *not* what would have been found if a batch size of b^* had been used from the beginning.\n\nIn other words, their result does *not* show that their algorithm is close to outperforming a fixed choice of batch size (for that to hold, the comparator would need to be \\sum_t y(w(b^*,...,b^*),b^*)). What they show is similar, but subtly different. They don’t put too much weight on this theoretical result, and in fact don’t even explicitly claim that the comparator in this result is that for a fixed choice of batch size, so really this is a minor issue, but I think that this is something that should be clarified, since it would be easy for a reader to draw an incorrect conclusion.\n\nWith that said, their approach is well-motivated, and their experiments seem to show consistent small improvements in performance. I don’t think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn’t be much more computationally expensive than using a fixed minibatch size. Furthermore, their approach is potentially more robust, since you can presumably be less careful about choosing the set of candidate minibatch sizes, than you would be for choosing only one. So while the experiments don’t show a big improvement, their proposal has other benefits.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting extension of minibatch GD using a straight-forward application of bandits",
            "review": "This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. Its results suggest that RMGD faster than MGD with grid search, and generalizes better.\n\nThe paper is well written. The idea itself is a simple and relatively straightforward application of bandits. The paper has some merits as it proposes an efficient and theoretically sound method to replace grid search in MGD.\n\nOne result that stands out is that RMGD achieves better results than the best performing batch size. The authors may want to discuss this in more depth. This may be due to the fact that the problem is inherently contextual: each epoch is different from other epochs, and may require a better-suited bach-size. Maybe contextual bandits would be a good candidate to try.\n\nComments:\n- offer some analysis or explanation of the surprising results\n- add equation numbers for ease of reference\n- in 4.1, why did you use this particular probability update? motivate/explain this choice.\n- appendix A: Specify that <> is dot product.\n                        introduce Beta\n                        briefly explain mirror descent\n                        why is beta z >= -1? My sense is that it is >= 0. can it be negative?\n                        explain, motivate or cite the equation following beta z >= -1\n\nI am pretty familiar wit bandit literature. Less so with GD literature. The paper's hybrid approach, although simple, exposes interesting questions. I tend towards accepting the paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The contribution is incremental due to lack of originality and somewhat narrow scope of the application",
            "review": "The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The loss of the bandit is binary: zero if the validation loss decreases and 1 otherwise. In the experiments, the Exp3 bandit algorithm is run with Adam and Adagrad on MNIST, CIFAR-10, and CIFAR-100. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.\n\nThe idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know.\n\nThe paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.\n\nIt is not clear how results scale with the number and choice of the grid values.\n\nI would have liked to see a more thorough investigation of the impact of the bandit loss on the experiments. It is true that as far as the theory is concerned, any bounded loss is OK. But I practice I would expect that a graduated loss (e.g., signed percentage of change in validation loss), would be more informative.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}