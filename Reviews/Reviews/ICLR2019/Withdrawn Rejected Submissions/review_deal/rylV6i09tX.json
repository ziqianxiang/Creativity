{
    "Decision": {
        "metareview": "This paper studies the relationship between flatness in parameter space and generalization. They show through visualization experiments on MNIST and CIFAR-10 that there is no obvious relationship between the two. However, the reviewers found the motivation for the visualization approach unconvincing and further found significant overlap between the proposed method and that of Ross & Doshi. Thus the paper should improve its framing, experimental insights and relation to prior work before being ready for publication.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Review of \"Interpreting Adversarial Robustness: A View from Decision Surface in Input Space\"",
            "review": "This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10.\n\nIn general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. \n\nWhile I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. \n\nMajor comments:\n\n1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples.\n\n2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited.\n\n3) The main result of Section 3 is that all adversarial attacks “utilize the decision surface geometry properties to cross the decision boundary within least distance.” While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. \n\n4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. \n\n5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. \n\nMinor comments:\n\n1) The paper’s first sentence states that “It is commonly believed that a neural network’s generalization is correlated to ...the flatness of the local minima in parameter space.” However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section.\n\n2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. \n\n3) Throughout the figures, axes should be labeled. \n\n4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasn’t been for several years [7].\n\n5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling.\n\n6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? \n\n\nTypos:\n\n1) Introduction, second paragraph: “For example, ResNet model usually converges to…” should be “For example, ResNet models usually converge to…”\n\n2) Introduction, second paragraph: “...defected by the adversarial noises...” should be “...defected by adversarial noise…”\n\n3) Introduction, third paragraph: “...introduced by adversarial noises...” should be “...introduced by adversarial noise…”\n\n4) Section 3.1, first paragraph: “cross entropy based loss surface is…” should be “cross entropy based loss surfaces is…”\n\n[1] Jakubovitz, Daniel, and Raja Giryes. \"Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.\" arXiv preprint arXiv:1803.08680 (2018). ECCV 2018.\n[2] Zahavy, Tom, et al. \"Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.\" arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018\n[3] Novak, Roman, et al. \"Sensitivity and generalization in neural networks: an empirical study.\" arXiv preprint arXiv:1802.08760 (2018). ICLR 2018.\n[4] Dinh, Laurent, et al. \"Sharp Minima Can Generalize For Deep Nets.\" International Conference on Machine Learning. 2017.\n[5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. \"Adversarial vulnerability for any classifier.\" arXiv preprint arXiv:1802.08686 (2018). NIPS 2018.\n[6] Gilmer, Justin, et al. \"Adversarial spheres.\" arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018.\n[7] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "good visualization for adversarial robustness analysis, unclear loss surface in weight space with adversarial data",
            "review": "The authors demonstrated that the loss surface visualized in parameter space does not reflect its robustness to adversarial examples. By analyzing the geometric properties of the loss surface in both parameter space and input space, they find input space is more appropriate in evaluating the generalization and adversarial robustness of a neural network. Therefore, they extend the loss surface to decision surface. They further visualized the adversarial attack trajectory on decision surfaces in input space, and formalized the adversarial robustness indicator. Finally, a robust training method guided by the indicator is proposed to smooth the decision surface.\n\nThis paper is interesting and well organized. The idea of plotting loss surface in input space seems to be a natural extension of the loss surface w.r.t to weight change. The loss surface in input space measures the network’s robustness to the perturbation of inputs, which naturally shows the influence of adversarial examples and is suitable for studying the robustness to adversarial examples. \n\nNote that loss surface in parameter space measures the network’s robustness to the perturbation of the weights with given inputs, which implicitly assumes the data distribution is not significantly changed so that the loss surface may have similar geometry on the unseen data.  \n\nThe claim of “these significant distinctions indicate the ineffectiveness of generalization estimation from the loss surfaces in parameter space” are not well supported as the comparison between Figure 2(a) and Figure 3 seems to be unfair and misleading. Fig 2 are plotted based on input data without any adversarial examples. So it is expected to see that Fig 2(a) and Fig 2(b) have similar contours. However, the loss surface in weight space may still be able to show their difference if the they are both plotted with the adversarial inputs. I believe that models trained by Min-Max robust training will be more stable in comparison with the normally trained models. It would be great if the author provide such plots. I would expect the normal model to have a high and flat surface while the robust model shows reasonable loss with small changes in weight space.\n\nHow to choose \\alpha and \\beta for loss surface of input space for Fig 3 and Fig 4 (first row)?\n\nHow are \\alpha and \\beta normalized for loss surface visualization in weight space as in Eq 1?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting visualization paper, but not always so convincing",
            "review": "This paper uses visualization methods to study how adversarial training methods impact the decision surface of neural networks.  The authors also propose a gradient-based regularizer to improve robustness during training.\n\nSome things I liked about this paper:\nThe authors are the first to visualize the \"decision boundary loss\".  I also find this to be a better and more thorough study of loss functions than I have seen in other papers.  The quality of the visualizations is notably higher than I've seen elsewhere on this subject.\n\nI have a few criticisms of this paper that I list below:\n1)  I'm not convinced that the decision surface is more informative than the loss surface.  There is indeed a big \"hole\" in the middle of the plots in Figure 4, but it seems like that is only because the first contour is drawn at too high a level to see what is going on below.  More contours are needed to see what is going on in that central region. \n2) The proposed regularizer is very similar to the method of Ross & Doshi.  It would be good if this similarity was addressed more directly in the paper.  It feels like it's been brushed under the rug.\n3) In the MNIST results in Table 1:  These results are much less extensive than the results for CIFAR.  It would especially be nice to see the MinMax results since those of commonly considered to be the state of the art. The fact that they are omitted makes it feel like something is being hidden from the reader.\n4) The results of the proposed regularization method aren't super strong.  For CIFAR, the proposed method combined with adversarial training beats MinMax only for small perturbations of size 3, and does worse for larger perturbations.  The original MinMax model is optimized for a perturbation of size 8.  I wonder if a MinMax result with smaller epsilon would be dominant in the regime of small perturbations.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}