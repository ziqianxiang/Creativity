{
    "Decision": {
        "metareview": "This paper proposes an 8-bit quantization strategy for rapid DNN deployment. 3 reviewers all rated this paper as marginally below acceptance threshold due to lack of novelty. 8 bit quantization (including channel-wise) is a well studied task. The paper lacks comparison with peer work. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "lacks novelty"
    },
    "Reviews": [
        {
            "title": "promising method for 8-bit quantization without sensitivity to outliers, limited in novelty and presentation clarity",
            "review": "The paper proposes channel-wise 8-bit quantization rather than layer-wise. It further takes advantage of work using moment analysis instead of just MAX values to avoid susceptibility to outliers. The main take-away seems to be that channel-wise set ups limit the need for outlier removal and the care with which you select your data subset when performing quantization.\n\nPros:\n- using channel-wise quantization (with MAX values or moment-analysis) yields improvement over layer-wise MAX approaches\n- limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)\n- shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channel-wise quantization\n\nCons:\n- unclear how much is gained over layer-wise and MAX value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channel-wise methods are the clear winner\n- unclear if the layer-wise set up with moment-analysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channel-wise set up; a few more experiments are important to determine specifically if improvement is with respect to channel-wise or moment-analysis since only layer-wise MAX results are presented\n- clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability\n\nOverall:\nThe paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channel-wise quantization (and moment-analysis), but the overall novelty is limited. With the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channel-wise setups. Comparisons with stronger baseline numbers when using layer-wise methods would give a more complete picture. In addition, having these stronger tuned baseline numbers on even more networks would be great to show that the channel-wise method has clear impact across the board, even with respect to well-tuned layer-wise baselines. These results could give better support for the importance of the novelty.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novelty is limited",
            "review": "This paper proposes an new 8-bit quantization strategy for rapid deployment. \n\n8-bit quantization has attracted many attentions recently. And it is already well used in GPU servers (cudnn), phones, ARM chips and various ASIC neural network chips. In these situations, almost no performance drop is observed for classification and detection tasks.\n\nSo, the novelty of this paper is limited.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach to channel-wise CNN quantization with adaptive bit allocation, with evaluation on eleven modern CNNs, comparison with simple layer-wise baseline",
            "review": "This paper proposes a technique for channel-wise quantization of CNNs\nto 8-bit, fixed point precision. The authors propose several\ntechniques for analyzing the statistical properties of output channel\nactivations in order to select the best fractional bit length for each\nchannel. Experimental results on eleven different CNN architectures\ndemonstrate that the approaches proposed result in significantly less\naccuracy loss when compared to a layer-wise baseline.\n\nThe paper has the following strengths:\n\n 1. The experimental results on eleven different architectures (of\n    varying depth and breadth) are convincing, and are consistently\n    better than layer-wise MAX for choosing fractional bit length.\n\nThe paper has the following weak points:\n\n 1. There is not much coherence between the description of the\n    approach in section 2.1, Figure 1, Algorithm 1, and\n    Figure 2. Notation is used in Algorithm 1 which is never defined.\n 2. Related to the previous point, the proposed technique has a lot of\n    moving parts and I don't feel that it would be easy to reproduce\n    the results of the paper. There are some vague statements, like\n    \"We resolve this complication by pre-coordinating the fractional\n    lengths of the weights\", which require significantly more\n    precision. This issue -- one of the main issues with channel-wise\n    versus layer-wise quantization -- is never returned to in the\n    definition of the method.\n 3. The experimental comparison with layer-wise quantization is\n    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN\n    quantization? The results comparing channel-wise and layer-wise\n    MAX are already convincing, but are the moment-analysis approaches\n    not equally applicable to layer-wise quantization?\n    State-of-the-art results that are less sensitive to outliers\n    should be included in Table 1. A comparison with layer-wise\n    approaches would be nice to have also in Figure 4 to show\n    sensitivity to profiling set size.\n\nThe experimental results in the paper are impressive, and the analysis\nmotivating the approach is convincing. However, there are presentation\nand clarity issues in the technical development, and the comparative\nanalysis is lacking broader comparisons with the state-of-the-art (to\nbe fair, the authors recognize that layer-wise MAX as a baseline is\nparticularly susceptible to outliers). These two aspects combined,\nhowever, lead me to the opinion that this work is just not quite ready\nfor publication at ICLR.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}