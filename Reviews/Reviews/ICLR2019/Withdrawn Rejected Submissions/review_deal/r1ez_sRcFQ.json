{
    "Decision": {
        "metareview": "Based on the majority of reviewers with reject (ratings: 4,6,3), the current version of paper is proposed as reject. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Majority reject."
    },
    "Reviews": [
        {
            "title": "A simple noisy perturbation scheme for improving robustness of CNN",
            "review": "This paper proposed the pixel redrawing approach to generate distorted training images to improve the performance of the deep networks, which hopefully can be used to prevent future attacks. The key idea is to randomly perturb the pixel values according pre-defined range and probabilities.\n\n\nThe proposed method is quite simple and is similar to denoising autoencoders in flavor.  My concern is that a pre-defined noisy perturbation may not be general enough to tackle various types of attacks. Ideally the perturbation should take into account properties of the input images, both pixel-wise and structure-wise. Unfortunately the proposed method ignores such information. The performance improvement seems quite limited judging from the results.   \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "simple but effective method in triaining but its utility as a defender not quite clear to me why it works  ",
            "review": "The authors propose a defense technique to make the NN model more robust to adversarial events by redrawing the images and use them for training the model so that the model can prevent future attacks. The idea itself is simple but seems to be effective as shown in Tables 2 and 3.\n\nWhat Iâ€™m missing here is a simple experiment to see the difference in accuracy performance between 1) when using PR for training the model (which is Case B in Table 2) against attacks versus 2) when not using PR for training the model against other attacks (similar to Table 2, Testing phase but using other attack models not PR as an attack model).\n\nIt is not quite clear to me why using PR as a defense mechanism helps the NN model.  I see its utility when training the NN model but using it as a defense mechanism is not quite clear why it works. \n\nMinor:\nIt is not quite clear how the author chose the hyperparameters, maybe by changing those hyperparameter the attacker could have much clever ways to attack the NN model.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Results are not convincing",
            "review": "This paper presents a new adversarial defense method.  I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.  The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value.  The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all).  They use both partially converged perceptron models as well as fully converged perceptron models. \n\nPros:\n\t1. The defense technique does not require knowledge of the attack method\n\nCons:\n\t1. The paper is incredibly difficult to understand due to the writing.\n\t2. The performed experiments are insufficient to determine  whether or not their technique works because:\n\t\ta. They don't compare against any other defense techniques.  In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect.\n\t\tb. They do not show the results of an attack by an adversary that is aware of their technique.  (i.e. F(PR(A(F,PR,x))))  Many alternative defense techniques will work much better if we assume the adversary does not know about the technique.\n\t\tc. Their comparison against random noise is not an apples-to-apples comparison.  Instead of perturbing uniformly within a range, they perturb according to a normal distribution.  The random noise perturbations also have a much larger L2 distance than the perturbations from their technique.  To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method.\n\t\td. They only show results for one value of epsilon and one value of \"# of colors\" for their technique.  Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large \"# of colors\"/small range per color and the attacker chooses a large epsilon).\n\t3. Their use of machine learning models is quite ad-hoc.  In particular they use a perceptron trained on algorithmically generated data.  And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model.\n\t4. The authors partly deanonymize the paper through a github link\n\t5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}