{
    "Decision": {
        "metareview": "The main issue with the work in its current form is a lack of motivation and some clarity issues. The paper presents some interesting ideas, and will be much stronger when it incorporates a more clear discussion on motivation, both for the problem setting and the proposed solutions. The writing itself could also be significantly improved. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Clarity and motivation need improvement"
    },
    "Reviews": [
        {
            "title": "Interesting direction, but clarity should be improved",
            "review": "I generally like the paper. The paper discussed a constrained value iteration setting where the safety contraints must be greater some threshold, and thresholds \\delta are parameters. The paper attempts to develop an value iteration algorithm to compute a class of optimal polices with such a parameter. The algorithm is mainly based on a special design of representation/data structure of PWC function, which can be used to store value functions and allows to efficiently compute several relevant operations in bellman equation. A graph-based data structure is developed for continuous state domains and hence value iteration can be extended to such cases. \n\nIn general, the paper presents an interesting direction which can potentially help solve RL problems with the proposed constraint setting. However, the paper spends lots of effort explaining representations, but only a few sentences explaining about how the proposed representations/data structures can help find a somehow generic value iteration solution, which allows to efficiently compute/retrieve a particular solution once a \\delta vector is specified. The paper should show in detail (or at least give some intuitive explanations) that using the proposed method can be more efficient than solving a value iteration for each individual constraint given that the constraints are independent. Specifically, the author uses the patient case to motivate the paper, saying that different patients may have different preferred thresholds and it is good to find class of policies so that any one of those policies can be retrieved once a threshold is specified. However, in this case, when dealing with only one patient, the dimension of reward is reduced to 1 (d = 1), while the computation of the algorithm is exponential in d, plus that the retrieval time is not intuitive to be better, so it is unsure whether computing such a class of policies worth.\n\nIn terms of novelty, the scalarization method of the vector-valued reward seems intuitive, since breaking a constraint means a infeasible solution. Furthermore, it is also unclear why the representation of PWC in discrete case is novel. A partial order on a high-dimensional space is naturally to be based on dominance relation, as a result, it seems natural to store value function by using right-top coordinates of a (hyper)rectangle.\n\nAs for the clarity, though the author made the effort to explain clearly by using examples after almost every definition/notation, some important explanations are missing. I would think the really interesting things are the operations based on those representations. For example, the part of computing summation of two PWC function representation is not justified. Why the summation can be calculated in that way? Though the maximum operation is intuitive, however, the summation should have some justification. I think a better way to explain those things is to redefine a new bellman operator, which can operate on those defined representations of PWC function. \n\nI think it could be a nice work if the author can improve the motivation and presentation. Experiments on some simple domains can be also helpful. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "why is this an important problem?",
            "review": "The authors provide an algorithm that aims to compute optimal value functions and policies as a function of a set of constraints.  The ideas used for designing the algorithm seem reasonable.  However, I don't fully understand the motivation here.  Given a set of constraints, one can simply carry out value iteration with what the authors call the scalarized reward in order to generate an optimal policy.  Why go through the effort to compute things in a manner parameterized by the constraints?  Perhaps the intention is to use this for sensitivity analysis, though the authors do not discuss that?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting first step, but hard to follow and no practical demonstrations.",
            "review": "Summary\n\nThe authors consider RL with safety constraints, which is framed as a multi-reward problem. At a high-level, this involves finding the Pareto front, which optimally trades off objectives. The paper primarily introduces and discusses a discretization scheme and methods to model the Q-value function as a NIPWC (non-increasing piecewise constant function). NIPWC are stored as values over discrete partitions of state-action spaces. To do so, the authors introduce two data structures DecRect and ContDecRect to store Q function values over geometric combinations of subsets of state-action space.\n\nThe authors discuss how to execute elementary operations on these data structures, such as computing max(f(x), g(x)), weighted sums, etc. The goal is to use these operations to compute Bellman-type updates to compute optimal value/policy functions for multi-reward problems. The authors also present complexity analysis for these operations. \n\nPro\n- Extensive discussion and analysis of discrete representations of Q-functions as NIPWCs. \n\nCon\n- A major issue with this work is that it is very densely written and spends a lot of time on developing the discretization framework and operations on NIPWC. However: \n- There is no clear practical algorithm to solve (simple) multi-reward RL problems with the authors' approach.\n- No experiments to demonstrate a simple implementation of these techniques.\n- Even though multi-reward settings are the stated problem of interest, authors don't discuss Pareto front computations in much detail, e.g., section 4.3 computing non-dominated actions is too short to be useful.\n- The discussion around complexity upper bounds is too dense and uninsightful. For instance, the bounds in section 5 all concern bounds on the Q-value as a function of the action, which results in upper bounds as a function of |A|. But in practice, the action is space is often small, but the state space is high-dimensional. Hence, these considerations seem less relevant. \n\nOverall, this work seems to present an interesting computational scheme, but it is hard to see how this is a scalable alternative. Practical demonstrations would benefit this work significantly.\n\nReproducibility\nN/A \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}