{
    "Decision": {
        "metareview": "This paper is essentially an application of dual learning to multilingual NMT. The results are reasonable.\n\nHowever, reviewers noted that the methodological novelty is minimal, and there are not a large number of new insights to be gained from the main experiments.\n\nThus, I am not recommending the paper for acceptance at this time.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reasonable improvements, but novelty incremental"
    },
    "Reviews": [
        {
            "title": "Novelty is not enough. More experiments needed.",
            "review": "This paper can be considered as a direct application of dual learning (He et al. (2016)) to the multilingual GNMT model. The first step is to pre-train the GNMT model with parallel corpora (X, Z) and (Y, Z). The second step is to fine-tune the model with dual learning.\n\n1. I originally thought that the paper can formulate the multilingual translation and zero dual learning together as a joint training algorithm. However, the two steps are totally separated, thus the contribution of this paper is incremental. \n\n2. The paper actually used two parallel corpora. In this setting, I suggest that the author should also compare with other NMT algorithm using pivot language to bridge two zero-source languages, such as ``A Teacher-Student Framework for Zero-Resource Neural Machine Translation``. It is actually unfair to compare with the completely unsupervised NMT, because the existence of the pivot language can enrich the information between two zero-resource languages. The general unsupervised NMT is often considered as ill-posed problem. However, with parallel corpus, the uncertainty of two language alignment is greatly reduced, making it less ill-posed. The pivot language also plays the role to reduce the uncertainty.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Some nice ideas, but would benefit from a better comparison to related work",
            "review": "Pros:\n- The paper address the problem of zero-shot translation. The proposed method is essentially to bootstrap a Dual Learning process using a multilingual translation model that already has some degree of zero-shot translation capabilities. The idea is simple, but the approach improves the zero-shot translation performance of the baseline model, and seems to be better than either pivoting or training on direct but out-of-domain parallel data.\n- The paper is mostly well written and easy to follow. There are some missing details that I've listed below.\n\nCons:\n- There is very little comparison to related work. For example, related work by Chen et al. [1], Gu et al. [2] and Lu et al. [3] are not cited nor compared against.\n\nMisc questions/comments:\n- In a few places you call your approach unsupervised (e.g., in Section 3: \"Our method for unsupervised machine translation works as follows: (...)\"; Section 5.2 is named \"Unsupervised Performance\"). But your method is not unsupervised in the traditional sense, since you require lots of parallel data for the target languages, just not necessarily directly between the pair. This may be unrealistic in low-resource settings if there is not an existing suitable pivot language. It'd be more accurate to simply say \"zero-shot\" (or maybe \"semi-supervised\") in Section 3 and Section 5.2.\n- In Section 3.1 you say that your process implements the three principles outlined in Lample et al. (2018b). However, the Initialization principle in that work refers to initializing the embeddings -- do you pretrain the word embeddings as well?\n- In Section 4 you say that the \"UN corpus is of sufficient size\". Please mention what the size is.\n- In Section 4.2, you mention that you set dropout to p=0.65 when training your language model -- this is very high! Did you tune this? Does your language model overfit very badly with lower dropout values?\n- In Section 5.2, what is the BLEU of an NMT system trained on the es->fr data (i.e., what is the upper bound)? What is the performance of a pivoting model?\n- In Section 5.3, you say you use \"WMT News Crawl, all years.\" Please indicate which years explicitly.\n- In Table 3, what is the performance of a supervised NMT system trained on 1M en-fr sentences of the NC data? Knowing that would help clarify the impact of the domain mismatch.\n- minor comment: in Section 4.3 you say that you trained on Tesla-P100, but do you mean Pascal P100 or Tesla V100?\n\n[1] Chen et al.: http://aclweb.org/anthology/P17-1176\n[2] Gu et al.: http://aclweb.org/anthology/N18-1032\n[3] Lu et al.: http://www.statmt.org/wmt18/pdf/WMT009.pdf",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Limited novelty; Experiments are not enough",
            "review": "\n[Summary]\nThis paper proposed an algorithm for zero-shot translation by using both dual learning (He et al, 2016) and multi-lingual neural machine translation (Johnson et al 2016). Specially, a multilingual model is first trained following (Johnson et al 2016) and then the dual learning (He et al 2016) is applied to the pre-trained model using monolingual data only. Experiments on MultiUN and WMT are carried out to verify the proposed algorithm. \n\n[Details]\n1.\tThe idea is incremental and the novelty is limited. It is a simple combination of dual learning and multilingual NMT. \n\n2.\tMany important multilingual baselines are missing. [ref1, ref2]. At least one of the related methods should be implemented for comparison.\n\n3.\tThe Pseudo NMT in Table 3 should also be implemented as a baseline for MultiUN experiments for in-domain verification.\n\n4.\tA recent paper [ref3] proves that using more monolingual data will be helpful for NMT training. What if using more monolingual data in your system? I think using $1M$ monolingual data is far from enough.\n\n5.\tWhat if using more bilingual sentence pairs? Will the results be boosted? What if we use more language pairs?\n\n6.\tTransformer (Vaswani et al. 2017) is the state-of-the-art NMT system. At least one of the tasks should be implemented using the strong baseline.\n\n[Pros] (+) A first attempt of dual learning and multiple languages; (+) Easy to follow.\n[Cons] (-) Limited novelty; (-) Experiments are not enough.\n\nReferences\n[ref1] Firat, Orhan, et al. \"Zero-resource translation with multi-lingual neural machine translation.\" EMNLP (2016).\n[ref2] Ren, Shuo, et al. \"Triangular Architecture for Rare Language Translation.\" ACL (2018).\n[ref3] Edunov, Sergey, et al. \"Understanding back-translation at scale.\"EMNLP (2018). \n\nI am open to be convinced.\n\n==== Post Rebuttal ===\nThanks the authors for the response. I still have concerns about this work. Please refer to my comments \"Reply to the rebuttal\". Therefore, I keep my score as 5.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}