{
    "Decision": {
        "metareview": "This paper proposes a faster approximation to batch norm, which avoids summing over the entire batch by subsampling either random examples or random image locations. It analyzes some of the tradeoffs of computation time vs. statistical efficiency of gradient estimation, and proposes schemes for decorrelating the samples to make good use of smaller numbers of samples.\n\nThe proposal is a reasonable one, and seems to give a noticeable improvement in efficiency. However, it's not clear there is a substantial enough contribution for an ICLR paper. The idea of subsampling is fairly obvious, and various other methods have already been proposed which decouple the computation of BN statistics from the training batch. From a practical standpoint, it's not clear that the observed benefit is large enough to justify the considerable complexity of an efficient implementation.\n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "a reasonable proposal for speeding up batch norm, but somewhat obvious and with limited practical benefit"
    },
    "Reviews": [
        {
            "title": "Good effort, but has not demonstrated enough evidence to convince me to accept it",
            "review": "The paper proposes a sampling-based method that aims at accelerating Batch Normalization (BN) during training of a neural network.\n\nQuality: \n   The writing of the paper needs more polishing; I saw grammatical errors here and there: for example, at the first paragraph of page 2, \"alternating\" should be alternative and \"synthetical\" should be synthetic... \n\nClarity:\n  I have not been able to fully understand why the proposed (uniform) sampling variant of BN is better than previous effort at making BN less computationally expensive in a GPU-based training environment by reading the paper: \n  1. The authors argue that the \"summation\" operation is the one that makes BN expensive; however, the authors have not demonstrated enough evidence of this argument\n  2. If \"summation\" operation is what makes BN expensive, then in a GPU-based environment, can we simply divide the data into smaller batch, and train on each GPU using a smaller batch (this is way, each GPU is essentially calculating the statistics based on a sub-sample)\n  3. The authors discussed Micro-BN, which \"alleviate the diminishing of BN's effectiveness when the amount of data in each GPU node is too small...\" This seems to show that in practice, training with BN does not suffer from having a large batch, but instead suffers from having too small batch size on each GPU node. Related to the point above, doesn't this observation play against the motivation of using a sampling-based BN?\n\nOriginality and Significance:\n  I think the effort of trying to make BN less computationally heavy is respectable. But the idea of uniform-sampling seems     rather straight-forward, and more important, I do not see its justification from reading the paper; the other technique introduced, Virtual Dataset Normalization, seems to be a direct application of Virtual Batch Normalization (Salimans et al 16).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a new technique that can reduce the computational complexity of batch normalization. Several sampling methods called NS, BS, and FS are proposed, and additionally, VDN is proposed to generate random virtual samples.  Experiment results follow to support the authors' goal.\n\npros)\n(+) The paper is clearly written and easy to follow.\n(+) The way of reducing the computational cost looks good.\n(+) The method can be easily adapted to BN or other batch-based methods.\n\ncons)\n(-) Any motivations or insights into NS, BS, and FS are not provided. Furthermore, the proposed sampling strategy looks heuristic without any studies.\n(-) For VDN, how to generate virtual samples is not clearly stated. I think the way of generating samples is critical to the performance of VDN but hard to find the exact way to do that.\n(-) How to determine the sampling ratio for each normalization method is not provided, and it would be better if the authors can show some studies about sampling ratio versus the speed gain.\n(-) It is hard to choose which normalization among FS and BS is better as looking at Table 2 and 3 only. So how about the speedup using BS+VDN?  \n\ncomments)\n- It is something strange why the authors used shallower ResNet on ImageNet and deeper ones on CIFAR datasets, maybe it was due to the training time, but the authors should clarify it.\n- What is the goal of the correlation analysis section? Especially, Figure 7 looks similar among BS, FS, VDN, and NS. Furthermore, the authors could include BN into the comparison.\n- This kind of paper should incoporate different ablation studies as much as the authors can, but it seems to be lacking.\n\n\nThe paper has an interesting idea about sampling some features to speed up the batch normalization. However, it looks quite obvious and needs more experimental grounds such as ablation studies to support the idea.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "limited improvements",
            "review": "This paper proposes to use subsampling to reduce the computation cost of BN, which buys around 20% of the computational cost. \n- In normal BN, the gradient is propagated through the normalization factor as well, how would that change in the case of subsampled BN?\n- The minimum amount of gains makes it less appealing considering the potential complexity of implementing the algorithm.\n- Did the author compared against cuDNNâ€™s native version of BN? Because random sampling is involved, this will result in less regular patterns of computation, this could likely make the implementation of BN to be less efficient.\nIn summary, the method proposed in the paper is reasonable but could be limited in practice due to only 20% maximum gain can be achieved.\n\nUpdate after the author response:\n\nThe author addressed some of the concerns raised in the review(Thanks for the detailed response), in particular, the comparison to cuDNN.  I still think the paper is still borderline but the results might be of interest to some of the ICLR audience.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}