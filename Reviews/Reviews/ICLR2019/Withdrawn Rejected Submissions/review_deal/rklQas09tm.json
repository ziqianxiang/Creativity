{
    "Decision": {
        "metareview": "This paper attempts at modeling text matching and also generating rationales.  The motivation of the paper is good.\n\nHowever there is some shortcomings of the paper, e.g. there is very little comparison with prior work, no human evaluation at scale and also it seems that several prior models that use attention mechanism would generate similar rationales.  No characterization of the last aspect has been made here.  Hence, addressing these issues could make the paper better for future venues.\n\nThere is relative consensus between the reviewers that the paper could improve if the reviewers' concerns are addressed when it is submitted to future venues.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Interesting model with somewhat promising experiments, could benefit from some more comparisons",
            "review": "This paper is about learning paired rationales that include the corresponding relevant spans of the (question, passage) or (premise, hypothesis).  Experimental results show the same or better accuracies using just the fraction of the input selected as when the whole input is used.\n\nWhile there has been prior work on learning rationales, this is the first I have seen that included this fine-grained pairing.  The paper also learns these rationales without explicitly labeled rationales but rather with only the distant supervision of the overall question answering or natural language inference task.\n\nThis paper could be made stronger by including an experimental evaluation of accuracy in an adversarial setting.  The model developed here might be well-suited for adversarial SquAD examples in which an extra sentence has been added.  It would be interesting to see these results.  This paper does include a somewhat similar adversarial evaluation (Section 4.3) but adds extra information to NLI examples.  Since for NLI, unlike QA, the extra sentence can change the correct label (can flip from entailment to contradiction), accuracy was not able to be evaluated.\n\nExperimentally, it would be good to compare against some prior work that doesn't include the pairing.  Perhaps an interpretability model based on the passage only without fine-grained pairing with the question?  My apologies if this corresponds to \"Independent\", I was somewhat confused by descriptions of the baseline.\n\nThe descriptions of the baselines was the least clear part of this paper.  It would be helpful to improve the clarity of Section 4.1 (perhaps adding a figure).\n\nOptional suggestion: consider breaking up the experiment section into two subsections: one for the cases in which the question rationales are provided (results in Table 1), and one for the cases in which the question-side rationales are learned as well (Table 2).  By putting all descriptions together, the paper explains two different settings and then needs to discuss which baselines are applicable to each setting and dataset and why.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "\nThis paper tackles the problem of generating rationales for text matching problems (i.e., two pieces of text are given). The approach is in a similar spirit as (Lei et al, 2016) while the latter mainly focuses on one piece of text for text classification problems and this work focuses on generating pairs of rationales. The approach has been evaluated on NLI and QA datasets, which demonstrates that the generated rationales are sensible and comes at a cost of accuracy.\n\nThe approach employs a generation-encoding-generation schema: it firsts generates the rationale from one side as a sequence tagging problem, re-encodes the rationales and predicts the rationale on the other side as a span prediction problem. Leveraging a match-LSTM framework and generated rationales for prediction, the model can be trained using a policy gradient method.\n\nOverall, I think this problem is novel and interesting. However, I am not fully convinced whether the proposed solution (and its implementation) is the right way to do so. Also, the paper writing needs to much improved.\n\nFirst of all, there is certainly a drop in the end task performance while it is unclear whether the derived rationales are really that useful (if the goal is interpretability) in the current evaluation. I am not convinced by the noisy SciTrail evaluation for rationales -- the noisy part p’ can be totally irrelevant and assume that the rationale generation component learns some sort of alignment between two parts, so it is not surprising that the model will not select words from p’ and it doesn’t really show that the rationales are useful. I think it is necessary to conduct some human evaluation for generate rationales and also provide some simple baselines for comparison (for example, just converting the soft-attention in math-LSTM to some hard selections) and see if this interpretability (at a cost of task performance) is really worthy or not.\n\nSecondly, I am not sure that whether the current way of generating the rationale pairs really makes sense or not.\nIt casts the rationale generation on one side as a tagging problem while the rationale generation on the other side as a span prediction problem. Why is that? Do you make any assumption that the two pieces of texts are not symmetric (e.g., one side is much longer than the other side like most of the current QA setup)?\n\nThere is a regularization term for both x and y but it seems that there isn’t any constraint that the generated rationales on the y side are not overlapping. Is it a problem or not? I don’t know how this is dealt with in the implementation.\n\nUnderstanding sec 3 takes some efforts and I think the presentation could be much improved. For example, q * {x^k} is not defined -- I assume it means extracting the subset of q based on the 1’s in {x^k}. The equations in Sec 3.2 can be made clearer.\n\nFinally, it is also unclear that how the 3 datasets were chosen. There are so many NLI and QA datasets (some of them are more popular and more competitive) at this point. Is there a reason that these datasets were chosen? There is a setup called ‘no rationalization w/ re-encoding’ which means that the rationale is already provided on one side, but is unclear that whether the OpenIE tuple and the searchQA queries can be used as rationales directly.\n\nMinor points:\n- Distal supervision -> distant supervision\n- The first paragraph of Introduction, “absent attention or rationale mechanisms”, what does it mean by ‘absent attention’? Isn’t it the case that all the models used attention mechanisms?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interpretability is important; but without human data we cannot evaluate it",
            "review": "This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:\n\n- There are versions of attention mechanisms that are spare and differentiable. See here: \nFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\nAndré F. T. Martins, Ramón Fernandez Astudillo \n\n- Why is \"rationalizing textual matching\" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.\n\n- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:\nD. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018\nhttp://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf\n\n- I don't agree that explanations are sufficient if removing them doesn't degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:\nAdversarial Examples for Evaluating Reading Comprehension Systems.\nRobin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf\n\n- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.\n\n- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.\n\n- What is the \"certificate of exclusion of unselected parts\" that the proposed method has?\n\n- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}