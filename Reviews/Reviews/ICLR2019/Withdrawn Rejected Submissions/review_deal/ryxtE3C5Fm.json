{
    "Decision": "",
    "Reviews": [
        {
            "title": "This work in a very preliminary stage.",
            "review": "This paper is concerned with improving generalization within GANs by employing adversarial training techniques. The authors propose an architecture that augments the AC-GAN architecture with a PGD attack module that generates adversarial examples for the discriminator. The authors report that an adversarially trained GAN is more robust to PGD attacks than state of the art adversarial discriminative models. Moreover, adversarial training seems to improve convergence rates for training. The authors offer some intuition behind why these phenomena take place. \n\nThe investigation of possible synergies between adversarial attacks and generative models is definitely an intriguing endeavor, however, the evidence offered in the paper (in its current form) is not solid enough to build a convincing argument. \n\nSome major issues relating to the main claims of this paper:\n\n* ideas of combining adversarial training and GANs have already appeared in the literature, and the authors even cite one such paper (Defense-GANs, Samangouei et al.). The citation appears amongst a long list of references that are summarily considered to provide \"an illusion of safety\". This to some degree challenges the novelty of the claims.\n\n* it is very hard to evaluate the generality of the claims based on what is essentially a relatively small portion of ImageNet. Especially when arguing about losses and architectures. \n\n* it is not clear whether the improved convergence rate is actually worth it -- the computational cost of running the adversarial example generation is not reflected in the epoch plot. \n\nApart from that, the paper contains a significant number of typos, and the exposition is confusing even though the arguments are not heavily technical. \nFor this paper to be considered for publication, one would expect either a stronger theoretical analysis or a more rigorous empirical evaluation that shows that the reported phenomena persist across different problems/datasets.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting approach to improve generalization ability of adversarial training by using GAN.",
            "review": "This paper touches on the limitation of adversarial training and propose a method that combines generator, discriminator, and adversarial attacker to improve the robustness and generalization ability of the network. The paper reads well (through there are a number of typos) and the source code is publicly available on the github. \n\nPros:\n- It provides insights on why adversarial training may not improve robustness on the test data set. \n- It demonstrates that one can improve robustness on the test data set by using GAN (to predict the real data distribution) and plugging it into adversarial training.\n- Potential improvements on robustness and convergence speed of GAN.\n\nCons:\n- lack of theoretical analysis.\n- needs comparison with a number of relevant work in the literature that use GAN for adversarial training, more specifically, the following works are related to this paper:\n     * Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples, 2018\n     * Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial\n        examples with adversarial networks, 2018\n\nOther comments:\n- There is a contradiction between Table 1 and the text on Page 1 (the contribution section). As per table 1, the performance of the proposed approach drops to 30.25%, whereas on page 1 it is mentioned it is dropped to 36.4%.\n- Figure 4 needs to be elaborated. The symbols are also not introduced.\n- It also helps if the authors provide a pseudocode specifying all the steps they took to generate the results. \n- Figure 6 is hard to read and interpret. It could help if the authors show fewer images and discuss some of the differences.\n- There are several typos in the paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Using GAN for data augmentation is not new and there lacks theoretical insights.",
            "review": "Authors propose to apply GAN to generate the adversarial examples efficiently when training the robust model. \nMajor issue:\n1.\tUsing GAN as the generator for sampling adversarial examples is popular these years. This work simply applies it for the data augmentation where the novelty is limited.\n2.\tThere are many strategies for data augmentation as adversarial training, and authors should include some in comparison, e.g., \nICLR’18: Certifying Some Distributional Robustness with Principled Adversarial Training\n3.\tThe experiments seem not convincing. The performance on CIFAR-10 without attacks is much worse than the-state-of-the-art. Authors claim the proposed strategy is efficient, but they didn’t provide the results on the imagenet-1k data set.\nMinor issue:\nThe first three subfigures in Fig. 4 is not clear.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}