{
    "Decision": {
        "metareview": "This paper proposes a probabilistic model for data indexed by an observed parameter (such as time in video frames, or camera locations in 3d scenes), which enables a global encoding of all available frames and is able to sample consistently at arbitrary indexes. Experiments are reported on several synthetic datasets. \n\nReviewers acknowledged the significance of the proposed model, noted that the paper is well-written, and the design choices are sounds. However, they also expressed concerns about the experimental setup, which only includes synthetic examples. Although the authors acknowledged during the response phase that this is indeed a current limitation, they argued it is not specific to their particular architecture, but to the task itself. Another concern raised by R1 is the lack of clarity in some experimental setups (for instance where only a subset of the best runs are used to compute error bars, and this subset appears to be of different size depending on the experiment, cf fig 5), and the fact that the datasets used in this paper to compare against GQNs are specifically designed. \n\nOverall, this is a really borderline submission, with several strengths and weaknesses. After taking the reviewer discussion into account and making his/her own assessment, the AC recommends rejection at this time, but strongly encourages the authors to resubmit their work after improving their experimental setup, which will make the paper much stronger.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "interesting model, weak experimental section"
    },
    "Reviews": [
        {
            "title": "Novel problem, reasonable evaluation",
            "review": "The paper motivates and provides a model to generate video frames and reconstructions from non-sequential data by encoding time/camera position into the model training. The idea is to allow the model to interpolate, and more importantly, extrapolate from frames and learn the latent state for multiple frames together. The same techniques are also applicable to 3d-reconstruction.  JUMP is very closely related to GQN with the main difference being that the randomness in JUMP is learned better using a \"global\" prior. The evaluation is reasonable on multiple synthetic experiments including a 3d-scene reconstruction specially created to showcase the consistency capabilities in a stochastic generation. Paper is mostly clear but more priority should be given to the discussion around convergence and the latent state.\n\nTo me, the 3d-reconstruction use-case and experiments are more convincing than the video generation. Interpolation between frames seems like an easier problem when specifically trained for. On the other hand, video algorithms trained on sequential prediction should be able to go forward or backward in time. Moreover, jumpy prediction throws away information (the middle frames) that might lead to a better latent state. The experiments also show certain frames where there seems to be a superposition of two frames. In this aspect, sSSM is better despite having worse video quality.\n\nFor video experiments, prediction of more complex video, with far-away frame predictions would solidify the experiments. The narratives seem somewhat limited to show what kind of advantage non-sequential context gives you.\n\nReliable convergence (less variance of training progress) of the method seems to be the strongest argument in favor of the JUMP. It is also unclear whether having a global latent variable is why it happens. More discussion about this should probably be included considering that JUMPy prediction seems to be the cause of this.\n\nBetter evaluation of the latent state might have presented a better understanding of what the model is really doing with different samples. For example, what is the model causes some frames to look like superpositions??",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A nice but non-convincing trial for indexed data modeling.",
            "review": "This paper proposes a general method for indexed data modeling by encoding index information together with observation into a neural network, and then decode the observation condition on the target index. I have several concerns regarding the way the paper using indices, and the experimental result.\n\nThe strategy this paper use for indexed data is to encode all data in a black-box, which can be inefficient since the order of temporal data or the geometric structure of spatial data is not handled in the model. These orders can be essential to make reasonable predictions, since they may encode causal relations among those observations, and certainly cannot be ignored. Another critical problem for this paper is that the relative time scale are not explicitly modeled in the context. My worry is that when putting all those informative data into a black-box may not be the most efficient way to use them.\n\nOn the other hand, experiments in this paper look quite artificial. Since sequential and spatial modeling have multiple real-life applications. It would be great if this method can be tested on more real dataset.\n\nThis paper does show some promise on sequence prediction task in a long range, especially when the moving trace is non-linear. A reasonable uncertainty level can be seen in the toy experiments. And the sample quality has some improvement over competitors. For example, JUMP does not suffer from those multi-mode issues. These experiments can be further strengthened with additional numerical results.\n\nFor now, this paper does not convince me about its method for modeling general indexed data, both in their modeling assumption, and their empirical results. In my opinion, there is still a long way to go for challenging tasks such as video prediction. This paper proposes an extreme way to use indices, but it is still far from mature. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, but not enough experimental evaluation",
            "review": "This paper presents a method for predicting future frames of a video (or unseen views of a 3D scene) in a \"jumpy\" way (you can query arbitrary viewpoints or timesteps) and \"consistent\" way (when you sample different views, the scene will be consistent). They use a VAE that encodes the input video in a permutation invariant way, which is achieved by summing the per-frame latent vectors. Then, they sample a latent vector using a DRAW prior. This latent vector can then be used to render the video/scene from different times/viewpoints via an LSTM decoder. They test the model on several toy datasets: they compare to video prediction methods on a dataset of moving shapes, and 3D viewpoint prediction on a 3D MNIST \"dice\" dataset.\n\nPros:\n- The idea of developing new methods for viewpoint and video synthesis that allow for \"jumpy\" and \"consistent\" predictions is an important problem.\n\n- The paper is fairly well written.\n\n- The design of the model is reasonable (it is a natural extension of VAE viewpoint/future prediction methods).\n\nCons:\n- All of the experiments were done on toy datasets. These are also not well-established toy datasets, and seem tailored to debugging the model, so it is not particularly surprising that the method worked. Since the main contribution is not very novel from a technical perspective (it is more about designing a model from existing, well-established components), this is a significant limitation. \n\n- The paper suggests with experiments that GQN generates predictions that are less consistent across samples, but it is not clear exactly which design decisions lead to this difference. Why is this model more jumpy and consistent than GQN?\n\n- The paper claims that JUMP trains more reliably than several video prediction methods in Figure 5. Yet, in the 3D viewpoint synthesis task, they suggest that JUMP had trouble with convergence, i.e.: \"We ran 7 runs for each model, and picked the best 6/7 runs for each model (1 run for JUMP failed to converge).\" This is confusing for two reasons. First, why was this evaluation protocol chosen (i.e. running 7 times and picking the 6 best)? If it was a post-hoc decision to remove one training run, then this should be clarified, and the experiment should be redesigned and rerun. Second, is the implication that JUMP is more stable than video prediction methods, but not necessarily more stable than GQN for viewpoint prediction?\n\n- The paper should consider citing older representation learning work that deals with synthesizing images from multiple viewpoints. For example: \n\nM. Tatarchenko, A. Dosovitskiy, T. Brox. \"Multi-view 3D Models from Single Images with a Convolutional Network\". ECCV 2016.\n\n- There is insufficient explanation of the BADJ baseline. What architectural changes are different?\n\n- The decision to use DRAW, instead of a normal VAE prior, is unusual and not explained in much detail. Why does this improve the visual fidelity of the samples?\n\nOverall:\nThe paper does not present enough evidence that this model is better at jumpy/consistent predictions than other approaches. It is evaluated only on toy datasets: if the technical approach were more novel (and if it was clearer where the performance gains are coming from) then this could be OK, but it seems to be a fairly straightforward extension of existing models. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}