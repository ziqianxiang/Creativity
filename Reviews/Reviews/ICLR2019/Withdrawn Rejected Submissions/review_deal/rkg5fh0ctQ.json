{
    "Decision": {
        "metareview": "This paper proposes a transfer learning approach based on previous works on this area, to build language understanding models for new domains. Experimental results show improved performance in comparison to previous studies in terms of slot and intent accuracies in multiple setups.\nThe work is interesting and useful, but is not novel given the previous work.\nThe paper organization is also not great, for example, the intro should introduce the approach beyond just mentioning transfer learning and meta-learning.\nThe improvements over the baselines look good, but the baselines themselves are quite simple. It'd be better to include comparisons with other state of the art methods. Also, the improvements over DNN are not consistent, it would be good to analyze and come up with suggestions on when to use which approach. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Running short on novelty, but richer on the experimental side"
    },
    "Reviews": [
        {
            "title": "The basic idea is not fully original, but the task is important and experiments are clear and complete.",
            "review": "This paper focuses on dealing with a scenario where there are \"unseen\" intents or slots, which is very important in terms of the application perspective.\n\nThe proposed approach, TSSM, tries to form the embeddings for such unseen intents or slots with little training data in order to detect a new intent or slot in the current input.\nThe basic idea in the model is to learn the representations of utterances and intents/slots such that utterances with the same intents/slots are close to each other in the learned semantic space.\nThe experiments demonstrate the effectiveness of TSSM in the few-shot learning scenarios.\nThe idea about intent embeddings for zero-shot learning is not fully original (Chen, et al., 2016), but this paper extends to both intent classification and slot filling. \n\nThe paper tests the performance in different experimental settings, but the baselines used in the experiments are concerned.\nThis paper only compares with simple baselines (MaxEntropy, CRF, and basic DNN), but there should be more prior work or similar work that can be used for comparison in order to better justify the contributions of the model.\nIn addition, this paper only shows the curves and numbers in the experiments, but it is better to discuss some cases in the qualitative analysis, which may highlight the contributions of the paper.\nAlso, in some figures of Fig. 2, the proposed TSSM is not better than DNN, so adding explanation and discussion may be better.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The topic is interesting but the novelty is incremental",
            "review": "In this paper, an efficient SLU model, called as TSSM, is proposed to tackle the problem of insufficient training data for the task of spoken language understanding. TSSM considers the intent and slot detection as a unified multi-objective optimization problem which is addressed by a meta-learning scheme. The model is pre-trained on a large dataset and then fine-tuned on a small target dataset. Thus, the proposed TSSM can improve the model performance on a small datatset in new domains.\n\nPros:\n1)\tThe transfer learning of spoken language understanding is very interesting.\n2)\tThe proposed TSSM can integrate the task of intents and slots and take the relationship between intents and slots into consideration.\n3)\tFive datasets are used to evaluate the performance of the method.\n\nCons:\nOverall, the novelty of this paper is incremental and some points are not clear. My main concerns are listed as follows.\n1)\tThe authors state that the knowledge transfer is the main contribution of this paper. However, as introduced in 3.5, the transfer scheme in which the model is first pre-trained on a large dataset and then fine-tuned on a small target dataset is very straightforward. For example, currently, almost all methods in the area of object recognition are pre-trained on ImageNet and then fine-tuned on a small dataset for particular tasks.\n2)\tAuthors also state that improvements for transferring from Restaurant, Laptop, TV, Atis to Hotel is not obvious. I think the results also need to be reported and the reasons why the improvement is not obvious should be provided and discussed.\n3)\tThe paper needs more proofreading and is not ready to be published, such as “A survey fnor transfer” and “a structured multi-objective optimization problems”.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Transferring SLU Models in Novel Domains",
            "review": "Summary: The authors present a network which facilitates cross-domain\nlearning for SLU tasks where the the goal is to resolve intents and\nslots given input utterances. At a high level, the authors argue that\nby fine-tuning a pre-trained version of the network on a small set of\nexamples from a target-domain they can more effectively learn the\ntarget domain than without transfer learning.\n\nFeedback:\n\n* An overall difficulty with the paper is that it is hard to\ndistinguish the authors' contributions from previous works. For\nexample, in Section 3.1, the authors take the model of Goyal et al. as\na starting point but explain only briefly one difference\n(contatenating hidden layers). In Section 3.2 the contributions\nbecomes even harder to disentangle. For example, how does this section\nrelate to other word-embeddings papers cited in this section? Is the\nproposed method a combination of previous works, and if not, what are\nthe core new ideas?\n\n* Some sections are ad-hoc and should be justified/explained\nbetter. For example, the objective, which ultimately determines the\ntrained model behaviour uses a product of experts formulation, yet the\nauthors do not discuss this. Similarly, the overarching message, that\nby fine-tuning a suitable model initialisation using small amounts of\ndata from the target domain is fairly weak as the authors do not\ndetail exactly how the model is fine-tuned. Presumably, given only a\nsmall number of examples, this fine-tuning runs the risk of\noverfitting, unless some form of regularisation is applied, but this\nis not discussed.\n\n* Lastly, there are some curious dips in the plots (e.g., Figure 2 bottom left, Figure 3 top left, bottom left), which deserve more explanation. Additionally, the evaluation section could be improved if the scores were to show error-bars. \n\nMinor: All plots should be modified so they are readable in grey-scale.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}