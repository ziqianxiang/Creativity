{
    "Decision": {
        "metareview": "This paper studies the behavior of gradient descent on deep neural network architectures with spatial locality, under generic input data distributions, using a planted or \"teacher-student\" model. \n\nWhereas R1 was supportive of this work, R2 and R3 could not verify the main statements and the proofs due to a severe lack of clarity and mathematical rigor. The AC strongly aligns with the latter, and therefore recommends rejection at this time, encouraging the authors to address clarity and rigor issues and resubmit their work again. \n\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Further iteration needed"
    },
    "Reviews": [
        {
            "title": "This paper proposes a new framework for understanding the Relu networks in theory. However, the assumptions are not justified and the definitions seems not clear.",
            "review": "This paper proposes a new approach to understand the theory of RELU neural networks. Using a teacher-student setting, this paper studies the batch normalization and the disentangled representations of neural networks. However, the definitions of some of the concepts and notation are not sufficiently clear. In addition, the assumptions that the main results of this paper depend on do not have clear intuitions.\n\nDetailed comments:\n\n1. It seems that this paper over claims its contribution. It is not clear why the \"teacher-student setting\" can be called a theoretical framework, even the definitions of the teacher and the student are not clear. It seems that the new framework is just a way to compute the relations of the gradients of neurons based on a few assumptions (Theorem 2).\n\n2. I found it very hard to follow the notations given in this paper. The main reason is that many of the terms appear without a definition, and the reader has to guess what they stand for. For example, in equation (2), w_{jk} seems to be the weight between nodes j and k, where k is a child of j. But this term is not defined. As another example, all the matrices in Theorem 9 are not defined. They just suddenly appear. In addition, S(f) in (11) is not defined. I would suggest the authors to spend one section to carefully define everything. \n\n3. The theorems all depends on some assumptions that are unclear whether will hold in practice or not. For example, in theorem 2, it is hard to see what kind of data distribution satisfy these three conditions. Although in Theorem 3 the author gave a sufficient condition, we still don't know what kind of $X$ satisfies this. For example, does Gaussian distribution satisfy those? This problem also happens to other theorems. It would be much better to make sure that these assumptions are unrealistic.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose a framework that utilizes the teacher-student setting and give some impressive evaluations on deep neural networks. This paper has rigorous theoretical analysis, but lacks necessary experiments.",
            "review": "The authors propose a framework that utilizes the teacher-student setting to evaluate deep locally connected ReLU network. The framework explicitly formulates data distribution, which has not been considered by previous works. The authors also show that their framework is compatible with Batch Normalization and favors disentangled representation when data distributions have factorizable structures. Based on this framework, the authors re-explain some common issues of deep learning, such as overfitting. \n\nMy major concerns are as follows.\n\n1. The framework is based on the teacher-student setting, and the authors claim that \"the teacher generates classification label via a hidden computational graph\". However, how the teacher can be designed is not clear in the paper.\n\n2. The data distribution included in this paper is $P(z_{\\alpha}, z_{\\beta})$, where $z_{\\alpha}$ and $z_{\\beta}$ are all summarization variables. From this perspective, it only has an indirect connection with original data distribution $P(x)$ or $P(x_{\\alpha}, x_{\\beta})$, and thus it could be questionable whether $P(z_{\\alpha}, z_{\\beta})$ is a convincing representation.\n\n3. The authors may want to conduct more experiments to better support their claims.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "This paper gives a model for understanding locally connected neural networks. The main idea seems to be that the network is sparsely connected, so each neuron is not going to have access to the entire input. One can then think about the gradient of this neuron locally while average out over all the randomness in the input locations that are not relevant to this neuron. Using this framework the paper tried to explain several phenomena in neural networks, including batch normalization, overfitting, disentangling, etc.\n\nI feel the paper is poorly written which made it very hard to understand. For example, as the paper states, the model gives a generative model for input (x,y) pairs. However, I could not find a self-contained description of how this generative model works. Some things are described in Section 3.1 about the discrete summarization variables, but the short paragraph did not describe: (a) What is the \"multi-layer\" deterministic function? (b) How are these z_\\alpha's chosen? (c) Given z's how do we generate x? (d) What happens if we have z_\\alpha and z_\\beta and the regions \\alpha and \\beta are not disjoint? What x do we use in the intersection?\n\nIn trying to understand the paper, I was thinking that (a)(b) The multilayer deterministic function is a function which gives a tree structure over the z_\\alpha's, where y is the root. (I have no idea why this should be a deterministic function, intuitively shouldn't y be chosen randomly, and each z_\\alpha chosen randomly conditioned on its parent?)  (c) there is a fixed conditional distribution of P(x_\\alpha|z_\\alpha), and I really could not figure out (d). The paper definitely seems to allow two receptive fields to intersect as in Figure 1(b).\n\nWithout understanding the generative model, it is impossible for me to evaluate the later results. My general comments there is that there are no clear Theorems that summarizes the results (the Theorems in the paper are all just Lemmas that are trying to work towards the final goal of giving some explanations, but the explanations and assumptions are not formally written down). Looking at things separately (as again I couldn't understand the single paragraph describing the generative model), the Assumption in Theorem 3 seems extremely limiting as it is saying that x_j is a discrete distribution (which is probably never true in practice). I wouldn't say \"the model does not impose unrealistic assumptions\" in abstract if you are going to assume this, rather the model just makes a different kind of unrealistic assumptions (Assumptions in Theorem 2 might be much weaker, but it's hard to judge that).\n\n==== After reading the revision\n\nThe revised version is indeed more clear about how the teacher network works, and I have tried to understand the later parts of the paper again. The result of the paper really relies on the two assumptions in Theorem 2. Of the two assumptions, the first one seems to be intuitive (and it is OK although exact conditional independence might be slightly strong). The second assumption is very unclear though as it is not an assumption that is purely about the model/teacher network (which are the x and z variables), it also has to do with the learning algorithm/student network (f's and g's). It is much harder to reason about the behavior of an algorithm on a particular model and directly making an assumption about that in some sense hides the problem. The paper mentioned that the condition is true if z is fine-grained, but this is very vague - it is definitely true if z is super fine-grained to satisfy the assumption in Theorem 3, but that is too extreme.\n\nOverall I still feel the paper is a bit confusing and it would benefit from having a more concrete example. I like the direction of the work but I can't recommend for recommendation at this stage.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}