{
    "Decision": "",
    "Reviews": [
        {
            "title": "The novelty is limited and the empirical study is not convincing.",
            "review": "My major concerns are as follows.\n1. Authors propose a new loss function in Eqn.17 which simply combines lifted structure loss and binomial deviance loss.\n2. The comparison is not sufficient. For the loss function, authors only compare it to binomial in Table 1 where the improvement is not significant on two datasets. Authors should include the performance of lifted structure loss which is the other block in their new loss.\n3. The improvement in Table 2&3 is suspect. Baseline methods use either different backbone network or different size of embedding (usually with smaller size). For example, 'Sampling'('Margin' in tables?) uses a simplified ResNet-50 and the size of embedding is 128. The size of embedding in 'Proxy-NCA' is only 64 while this work uses 512. \n4. The parameters in the experiments seem quite sensitive. For example, they use $K=4$ on two data sets and $K=5$ on the others. Can authors elaborate how they tune the parameters?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Strong experimental results but the theory is not that strong",
            "review": "The paper analyses the gradients some deep metric learning (DML) approaches. Most of them optimize over pair-wise or triplet-wise constraints.\nThe main claim of the paper is that, during training, most DML approaches ignore some informative pairs of examples or do not take into account the value of the distance gap between similar and dissimilar pairs.\nA specific weighting strategy is then proposed to address those problems.\n\nThe main strength of the paper are the strong experimental results on different transfer learning benchmarks. The proposed approach seems to significantly outperform existing methods thanks to the proposed strategy. \nHowever, the theoretical aspect is weak.\n\nI have some concerns about the paper.\n\n- My first concern is about terminology: I disagree with the claims of the paper that there is any theoretical demonstration in the paper. \nI do not see the novelty of the gradient equivalence theorem (Theorem 3.1). It was already explained in (Law et al., ICCV 2013) (that is cited in the paper) that pairwise and triplet-wise constraints are just specific formulations of quadruplet-wise constraints. Pair-wise and triplet-wise can then be written in their quadruplet-wise formulation: we then obtain the induced gradient formulation that depends on positive and negative pairs (as formulated in Eq. (4)). The derivation of the proof is very straightforward once we have a formulation that generalizes both triplet-wise and pairwise constraints.\n\nMoreover, the gradient equivalence definition (in Definition 3.1) is not applicable to most deep learning optimization frameworks that use momentum-based optimizers (e.g. Adam which is a momentum solver, and SGD which is often optimized with momentum). Indeed, definition 3.1 only considers the value of theta at some given iteration, but not at the previous iterations. However, momentum-based approaches keep a history of the gradients from previous iterations and will then return different gradients.\n\n- One of the main claims of the paper against the triplet loss is that the sampling strategy ignores some informative pairs. This is mainly due to the fact that the triplet loss is generally used in the context of very large datasets and large mini-batches (Schroff et al., CVPR 2015) where it is computationally expensive to generate all the possible triplet constraints. Triplet sampling strategies are formulated to ensure fast convergence while avoiding degenerate solutions induced by the chosen triplet sampling strategy.\n\nThe submitted approach does not deal with very large datasets and then does not need to consider such sampling strategies. Although some sampling strategy is proposed, it is unclear if it would be scalable (i.e. trainable in reasonable time on the same dataset as FaceNet).\nMoreover, as mentioned in Section 5.1, the proposed strategy is a straightforward extension of lifted structure.\n\n- Why did you only report the recall@K evaluation when many methods report the normalized mutual information (NMI)? (Hyun Oh Song et al., CVPR 2017)\nCould you please include NMI scores?\n\n- If the problem of the contrastive loss is the fact that the gradient considers all pairs equally, why can it not be adapted to depend on the hardness of the constraint (e.g. by taking the squared of the loss for each pair)?\n\nIn conclusion, my opinion is borderline but only leans towards acceptance because the experimental results are strong.\nNonetheless, the reported results of baselines are often for different network architectures and using different output dimensions. Can the authors try their method with the same architecture and same output dimensionality as baselines?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting observation, needs further clarification",
            "review": "Summary: The paper provides a unified view of the weight assignment strategies for pairwise similarities in DML. The authors analyze the weights assignment methods for multiple commonly used methods and provide a weighting scheme called RAW which combines the lifted structure with the binomial deviance approach. They also propose a hard mining triplet selection strategy for sampling informative triplets. The experiments indicate the advantage of combining these approaches.\n\nComments:\n- The Theorem 3.1 should merely be stated as an observation. It is a trivial observation that the partial derivative of the parameters at every step will depend positively on s_ik and negatively on s_ij. Although the transition from line 2 to 3 in Equation (3) is very unclear and needs further expansion (how do you introduce indicator variables?). Moreover, there is no need to define the concept of gradient equivalence. The whole observation can be asserted as the fact that the partial derivative of the loss at every iteration can be seen as a weight. Additionally, to make the derivations mathematically sound, you should state that the weights in (2) are updated using the new similarity values at every iteration and are kept fixed during the next iteration.\n\n- The proposed weighting strategy is a combination of two previous approaches. Although there is not much novelty in combining the two, the experiments indicate that the performance of DML can improve using the new approach. The triplet mining on the other hand is a bit dubious. Sampling only hard triplets, in my experience with triplets, may cause the loss to be saturated by unsatisfied hard triplets. Also, the embedding might not learn anything about the global structure of the dataset (i.e. how are the clusters relatively located w.r.t each other, which points are outliers, etc.). It would be nice to have a visualization of the learned embeddings to get an idea about the separability of the clusters.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}