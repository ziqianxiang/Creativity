{
    "Decision": {
        "metareview": "With ratings of 6, 5 & 3 the numerical scores are just not strong enough to warrant acceptance.\nThe author rebuttal was not able to sway opinions.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reviews not strong enough to justify acceptance"
    },
    "Reviews": [
        {
            "title": "The authors propose an end-to-end trainable model which leveraged pair-wise relationships between objects to predict objects 3D shape and pose given a single 2D image.  The proposed method outperforms independent prediction approaches on two publicly available datasets.",
            "review": "The paper is well-written with a few figures to illustrate the ideas and components of the proposed method. However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18. The remaining components of the proposed method are not very new. Hence, I am not very sure whether the novelty of the paper is significant. Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods. \nI also have a few questions:\n1. How did you get the instance boxes, union boxes, and binary masks in testing?\n2. What are the training and inference time? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental technical novelty, issues with experiments and results, unclear presentation",
            "review": "<Summary>: This paper presented a method for incorporating binary relationship between objects (relative location, rotation and scale) into single object 3d prediction. It is built on top of previously published work of [a] and used same network architecture and loss as of [a] and only added the binary relations between objects for object 3d estimation. The results are shown on SUNCG synthetic dataset and only *4 image* instances of NYUv2 dataset which is very small for a computer vision task.\n\n[a] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2d image of a 3d scene. In CVPR, 2018.\n\n<Pros>: The paper tackles a problem of obvious interest to computer vision research community. It shows better results compared to previous similar work of [a] without considering binary relation between objects.\n\n<Cons>:\n\n*Technical details are missing:\n\nThe set of known and unknown variables are not clear throughout the paper:\n-The extrinsic camera parameters are known or estimated by the method?\n-The intrinsic camera parameters are known or estimated by the method?\n-What are the properties of ground truth bounding boxes in 2D camera frame and 3D space?\n-What is the coordinate of translation? is it in camera coordinate or world coordinate?\n-What are the variations of camera poses in training and testing for synthetic dataset and how are the samples generated? Are the train/test images generated or are rendered images from previously published work of [b] used?\n\n[b] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In CVPR, 2017.\n\n*The proposed method is trained on synthetic dataset of SUNCG and their object relations have biases from scene creators. While using binary relation between objects increase the recall in prediction it can also make the predictions bias to the most dominant relations and decrease the precision of detection in rare cases in synthetic dataset. Also, such bias can decrease prediction precision in images of real scenes.\n \n*One of the main issues in this paper is that the result of fully automated pipeline versus having ground-truth annotation at test time are mixed up. For example, in the teaser figure (Figure 1-b), does the proposed method use ground truth bounding boxes or not? It is mentioned in figure caption: “(b) Output: An example result of our method that takes as input the 2D image and generates the 3D layout.”. Is the input only 2D image or 2D image + ground truth object bounding boxes?\nIn order to make sure that reader understands each qualitative result, there should be a column showing the “Input” to the pipeline (Not “Image”). For example, in Figure 3 and Figure 4, the image overlaid with input ground-truth bounding boxes should be shown as input to the algorithm. \n\n\n*The experiments and results does not convey the effectiveness of the proposed approach. There are major issues with the quality of the experiments and results. Here are several examples:\n\n- Missing baseline: Comparison with the CRF-based baseline is missing. This statement is not convincing in the introduction: “One classical approach is to use graphical models such as CRFs. However, these classical approaches have usually provided little improvements over object-based approaches.” For a fair comparison with prior works, reporting results on a CRF-based baseline using similar unary predictions is necessary. \n\n-The experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted.\n\n-If the ground truth boxes are given at the test time, it means that the ground truth binary relations between objects are given and it makes the problem trivial.\n\n-It is not clear what is the ground truth box in experimental setup. Is it amodal object box or the ground truth box contains only the visible part of the object? \n\n-The qualitative results shown in Figure 4 have full objects in voxel space with predicted rotation, scale and translation. In the qualitative result of Figure 3 and Figure 5 the voxel prediction is shown as final output. Why the result of full object in voxel space with predicted (rotation, scale and translation) is not shown in Figure 3 and Figure 5 and why it is shown in Figure 4?\n\n\n*Very limited results on real images:\n\n-Quantitative result on a dataset of real images is missing. The results on synthetic datasets is not a good proxy for the actual performance of the algorithm in real use cases and applications.\n\n- The paper only shows few results of NYUv2 on known ground truth boxes. The errors in object detection can be propagated to the 3D estimation therefore these qualitative results are not representative of the actual qualitative performance of the proposed algorithm. Several randomly selected qualitative results on a dataset of real images “without ground-truth boxes” are needed for evaluating the performance of the proposed method on real images. \n\n-Reporting variation in all parameters of scale, rotation and translation is necessary in order to find the difficulty of the problem. For example, what is the distribution of object scale in different object categories. What is the error of scale prediction of we use mean object scale for each object category for all object instance at test set?\n\n\n*Unclear statements and presentation:\n\n- It is mentioned in the paper: “While the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and non-linearity of the manifold.”\n\n-Is it necessary for the relative rotation to be formulated to classification task? \n\n-If not the comparison of modeling relative rotation via linear constraints is missing.\n\n- In some of the tables and figures the “know ground-truth boxes/detection setting” are in bold face and in some cases are not. This should be consistent throughout the paper.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A scene parsing model that takes both objects and their relations into account",
            "review": "This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets.\n\nThe paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it’s unclear what we can really learn from it. \n\nApproach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan’s group and Song-Chun Zhu’s group (see some examples below). Apart from that, this paper doesn’t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture.\n\nThe results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive.\n\nIn general, I’m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers.\n\nA minor issue:\n-\tIn fig 5. The object colors are not matched for GT and Factor3D and ours.\n\nRelated work\nHolistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV’18.\nConfigurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV’18.\nCharacterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH’11.\nExample-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia’12.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}