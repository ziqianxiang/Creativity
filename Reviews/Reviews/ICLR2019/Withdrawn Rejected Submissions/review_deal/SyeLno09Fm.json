{
    "Decision": {
        "metareview": "This work proposes to use the MAML meta-learning approach in order to tackle the typical problem of insufficient demonstrations in IRL.\n\nAll reviewers found this work to contain a novel and well-motivated idea and the manuscript to be well-written. The combination of MAML and MaxEnt IRL is straightforward, as R2 points out, however the AC does not consider this to be a flaw given that the main novelty here is the high-level idea rather than the technical details.\n\nHowever, all reviewers agree that for this paper to meet the ICLR standards, there has to be an increase in rigorousness through (a) a more close examination of assumptions, sensitivity of parameters and connections to imitation learning (b) expanding the experimental section.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Well-motivated idea but execution and analysis is not convincing"
    },
    "Reviews": [
        {
            "title": "An interesting application of MAML to Inverse RL but lacks rigorousness and persuasive experimental results",
            "review": "This paper aims to address the problem of lacking sufficient demonstrations in inverse reinforcement learning (IRL) problems. They propose to take a meta learning approach, in which a set of i.i.d. IRL tasks are provided to the learner and the learner aims to learn a strategy to quickly recover a good reward function for a new task that is assumed to be sampled from the same task distribution. Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. The proposed algorithm is evaluated on a synthetic grid-world problem, SpriteWorld. The experimental results suggest the proposed algorithm can learn to mimic the optimal policy under the true reward function that is unknown to the learner. \n\nStrengths: \n\n1) The use of meta learning to improve sample efficiency of IRL is a good idea.\n2) The combination of MAML and MaxEnt IRL is new to my knowledge. \n3) Providing the gradient expression is useful, which is the main technical contribution of this paper. (But it needs to be corrected; see below.)\n4) The paper is well motivated and clearly written \"in a high level\" (see below). \n\nWeakness: \n\n1) The derivation of (5) assumes the problem is tabular, and the State-Visitations-Policy procedure assumes the dynamics/transition of the MDP is known. These two assumption are rather strong and therefore should be made explicitly in the problem definition in Section 3.\n\n2)  Equation (8) is WRONG. The direction of the derivation takes is correct, but the final expression is incorrect. This is mostly because of the careless use of notation in derivation on p 15 in the appendix (the last equation), in which the subscript i is missed for the second term. The correct expression of (8) should have a rightmost term in the form  (\\partial_\\theta r_\\theta) D  (\\partial_\\theta r_\\theta)^T, where D is a diagonal matrix that contains \\partial_{r_i} (\\E_{\\tau} [ \\mu_\\tau])_i and i is in 1,...,|S||A|. \n\n3) Comparison with imitation learning and missing details of the experiments. \na) The paper assumes the expert is produced by the MaxEnt model. In the experiments, it is unclear whether this is true or not, as the information about the demonstration and the true reward is not provided. \nb) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. In imitation learning, it is known that the expert policy is often sub-optimal, and therefore the goal in imitation learning is mostly only to achieve expert-level performance. Given this, the way this paper evaluate the performance is misleading and improper to me, which leads to an overstatement of the benefits of the algorithm. \nc) It would be interesting to compare the current approach with, e.g., the policy-based supervised learning approach to imitation learning (i.e. behavior cloning). \n\n4) The rigorousness in technicality needs to be improved. While the paper is well structured, the writing at the mathematical level is careless, which leads to ambiguities and mistakes (though one might be able to work out the right formula after going through the details of the entire paper). Below I list a few points. \n    a) The meta-training set {T_i; i=1,...,N} and the meta-test set {T_j; i=1,...,M} seems to overload the notation. I suppose this is unintentional but it may appear that the two sets share the first T_1,.., T_M tasks, e.g., when N>=M, instead of being disjoint. \n    b) The set over which the summation is performed in (4) is unclear; alpha in (4) is not defined, though I guess it's a positive step size.\n    c) On p4, \"we can view this problem as aiming to learn a prior over the intentions of human demonstrators\" is an overstatement to me. At best, this algorithm learns a prior over rewards for solving maximal entropy IRL, not intention. And the experiment results do not corroborate  the statement about \"human\" intention.\n    d) On p4,  \"since the space of relevant reward functions is much smaller than the space of all possible rewards deï¬nable on the raw observations\" needs to be justified. This may not be true in general, e.g., learning the set of relevant functions may require a larger space than learning the reward functions.\n    e) The authors call \\mu_\\tau the \"state\" visitation, but this is rather confusing, as it is the visiting frequency of state and action (which is only made clear late in the appendix). \n    f) On p5, it writes \"... taking a small number of gradient steps on a few demonstrations from given task leads\" But the proposed algorithm actually only takes \"one\" gradient step in training. \n    g) The convention of derivatives used in the appendix is the transpose of the one used in the main paper.\n\nMinor points: \n1) typo in (2) \n2) p_\\phi is not defined, L_{IRL} is not defined, though the definition of both can be guessed.\n3) T^{tr} seems to be typo in (11)\n4) A short derivation of (2) in the Appendix would be helpful.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Novel solution to an important problem, but needs further details and experimentation",
            "review": "This paper attempts to the solve  data-set coverage issue common with Inverse reinforcement learning based approaches - by introducing a meta-learning framework trained on a smaller number of basic tasks. The primary insight here is that there exists a smaller set of unique tasks, the knowledge from which is transferable to new tasks and using these to learn an initial parametrized reward function improves the coverage for IRL. With experiments on the SpriteWorld synthetic data-set, the authors confirm this hypothesis and demonstrate performance benefits - showcasing better correlation with far fewer  number of demonstrations.\n\nPros:\n+ The solution proposed here in novel - combining meta-learning on tasks to alleviate a key problem with IRL based approaches.\nThe fact that this is motivated by the human-process of learning, which successfully leverages tranferability of knowledges across a group of basic tasks for any new (unseen) tasks, makes it quite interesting.\n+ Unburdens the needs for extensive datasets for IRL based approach to be effective\n+ To a large extent, circumvents the need of having to manually engineered features for learning IRL reward functions\n\nCons:\n- Although the current formulation is novel, there is a close resemblance to other similar approaches  - mainly, imitation learning. It would be good if the authors could contrast the differences between the proposed approach and approach based on imitation learning (with similar modifications). Imitation learning is only briefly mentioned in the related work (section-2), it would be helpful to elaborate on this. For instance, with Alg-1 other than the specific metric used in #3 (MaxEntIRLGrad), the rest seems close similar to what would be done with imitation learning?\n- One of main contributions is avoiding the need for hand-crafted features for the IRL reward function. However, even with the current approach, the sampling of the meta-learning training and testing tasks seem to be quite critical to the performance of the overall solution and It seems like this would require some degree of hand-tuning/picking. Can the authors comment on this and the sensitivity of the results to section of meta-learning tasks and rapid adaption?\n- The results are limited, with experiments using only the synthetic (seemingly quite simple)  SpriteWorld data-set. Given the stated objective of this work to extend IRL to beyond simple  cases, one would expect more results and with larger problems/data-sets.\n\t- Furthermore, given that this work primarily attempts to improve performance with using meta-learned reward function instead of default initialization - it might make sense to also compare with method such as Finn 2017, Ravi & Larochelle 2016.\n\nMinor questions/issues:\n> section1: Images are referred to as high dimensional observation spaces, can this be further clarified?\n>  section3: it is not immediately obvious how to arrive at eqn.2. Perhaps additional description would help.\n> section4.1 (MandRIL) meta-training: What is the impact/sensitivity of computing the state visitation distribution with either using the average of expert demos  or the true reward? In the reported experiments, what is used and what is the impact on results, if any ?\n> section4.2: provides an interesting insight with the concept of locality of the prior and establishes the connection with Bayesian approaches.\n> With the results, it seems like that other approaches continue to improve on performance with increasing number of demonstrations (the far right part of the Fig.4, 5) whereas the proposed approach seems to stagnate - has this been experimented further ? does this have implications on the capacity of meta-learning ?\n> Overall, given that there are several knobs in the current algorithm, a comprehensive sensitivity study on the relative impact would help provide a more complete picutre",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice and novel idea but not tested enough",
            "review": "The paper defines a new machine learning problem setup by applying the meta-learning concept to inverse reinforcement learning (IRL). The motivation behind this setup is that expert demonstrations are scarce, yet the reward functions of related tasks are highly correlated. Hence, there is plenty of transferrable knowledge across tasks.\n\nStrengths:\n--\n * The proposed setup is indeed novel and very ecologically-valid, meaning meta-learning and IRL are natural counterparts for providing a remedy to an important problem.\n\n * The paper is well-written, technically sound, and provides a complete and to-the-point literature survey. The positioning of the novelty within literature is also accurate.\n\n\nWeaknesses:\n--\n\n * The major weakness of the paper is that its hypothesis is not tested exhaustively enough to draw sound conclusions. The paper reports results only on the SpriteWorld data set, which is both synthetic and grid-based. Having acknowledged that the results reported on this single data set are very promising, I do not find this evidence sufficient to buy the proposed hypothesis. After all, IRL is meant mainly for real-world tasks where rewards are not easy to model. A single result on a simplified computer game does not shed much light on where an allegedly state-of-the-art model stands toward such an ambitious goal. I would at least like to see results on some manipulation tasks, say on half-cheetah, ant etc.\n\n * Combination of MaxEnt IRL and MAML is novel. That said, the paper combines them in the most straightforward way, which does not incur any complications that call for technical solution that can be counted as a contribution to science. Overall, I find the novelty of this work overly incremental and its impact potential very limited.\n\n * A minor issue regarding clarity. Equation 3 is not readable at all. The parameter \\phi and the loss \\mathcal{L}_{IRL} have not been introduced.\n\nThis paper consists of a mixture of strong and weak aspects as detailed above. While the proposed idea is novel and the first results are very promising, I view this work to be at a too early stage to appear in ICLR proceedings as a full-scale paper. I would like to encourage the authors to submit it to a workshop, strengthen its empirical side and resubmit to a future conference.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}