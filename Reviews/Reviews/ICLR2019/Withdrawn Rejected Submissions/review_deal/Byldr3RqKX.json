{
    "Decision": {
        "metareview": "This paper explores an interpretation of generative models in terms of interventions on their latent variables.  The overall set of ideas seems novel and potentially useful, but the presentation is unclear, the goal of the method seems poorly defined, and the qualitative results (including the videos) are unconvincing.\n\nI recommend you put work into factoring the ideas in this paper into smaller ones.  For instance, definition 1 is a mess.  I would also recommend the use of algorithm boxes.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting ideas but unclear presentation"
    },
    "Reviews": [
        {
            "title": "causality based investigation of the modular structure of the deep generative model",
            "review": "The work provides a way to investigate the modular structure of the deep generative model. The key concept is the “distribute over channels of generator architectures”. \nstrong points:\n1) using the causality to investigate the modular of the deep generative model. \n2) the key concept is interesting and straightforward. \n3) the observations in the experiments are interesting. \n\nBut I have the following concerns, \n1) the concept of counterfactual is consistent with that in the causality context? \n2) more details of the causal model of the deep learning are needed,\n3) more details of section 3.1 and 3.2 are needed, especially why these processes are proper interventions?  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting concept but not ready for publication.",
            "review": "This paper proposes to examine generative adversarial networks by using counterfactual reasoning. The authors propose to examine modularity through the lens of interventions on the generative networks. After observing that the nodes within the generative network obey a deterministic relationship, they propose a proxy for intervention which takes samples and creates “hybrid” samples by replacing the activation output of one sample with the others. Given the vast number of nodes that exist within a generative network, the authors propose a heuristic for choosing the nodes to perturb. \n\nI found the underlying premise of this paper to be very strong (identifying modularity in generative networks), however I think there is a substantial amount of work that should go into this paper before acceptance. While the authors begin by working within the framework of causal reasoning there is no mention of what the effect is that they are seeking to measure, i.e. what is the causal estimand here? The influence maps provide an intuitive answer to this, but not one that defines a clear estimand. I would like to see additional evaluation. The evidence provided largely leaves the reader to interpret results subjectively, rather than providing clear evidence. I was also uncomfortable with the selection on hyperparameters (3 clusters). It would be very nice to either have a selection criterion or show the sensitivity of the proposed methodology to other choices. \n\nOverall, I think this is an interesting idea in a very important area, but one that is not quite ready for publication.\n\nSome editorial comments:\n\nThe layout of this paper is slightly strange. After the introduction, the authors introduce the notion of disentanglement and lead with an example from optics. This motivation should either be moved to the introduction or removed. After the definitions the authors jump into a related work section that feels slightly disjointed from the previous section.\n\nI found definition 1 to be abstruse. In addition there are a couple of typos that should be addressed (“consists in a distribution” → “consists of a distribution”). It is non-standard to lead with the latent variables. I think it makes for a much easier narrative to describe the observed variables and structure first, before carrying on to the latent variables. Additionally, I believe you are stating an observation made by Pearl (2001) that after observing the noise variable, relationships become deterministic. This is slightly non-obvious from the wording used (and is also missing the proper reference).\n\nParens are missing from the following citation:\n“generative models encountered in machine learning Besserve et al. (2018).”  → “generative models encountered in machine learning (Besserve et al., 2018).” ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors of this paper propose a method for assessing modularity in deep networks and more specifically on deep generative networks.",
            "review": "AFTER REBUTTAL:\nI think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. \n\n------------\nThe method tackles the problem of interpretability that is a very important issue for usually black-box deep networks. Unfortunately it is not very clear how is the achieved. I have read several times the part explaining the influence maps and the clustering based on them and it still doesn't make a lot of sense to me. I think that part has to be better justified and exposed. Moreover, results do not support the claim which makes me doubt even more about how effective the method proposed actually is. In conclusion, I think that better exposition and more solid experimental analysis is needed.  \n\nAlso please check some writing problems: \n\n> Introduction: \n\"to acquire a generative function mapping a latent space (such as Rn)\" >  difficult to read, rephrase. \n\"making it difficult to add human input\" > confusing. What do you mean by human input? I assume you refer to having control to make decisions about design. \n\n> Section 3.1\n\"the internal variable may leave the manifold it is implicitly embedded in as a result of the model’s training\" : not clear, rephrase. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}