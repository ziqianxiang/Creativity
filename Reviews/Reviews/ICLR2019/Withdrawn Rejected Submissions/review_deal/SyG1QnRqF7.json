{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting idea, but the novel contributions are unclear and experimental results don't seem to match earlier published results",
            "review": "This paper proposes a way to use generative modeling (specifically, introspective learning) to handle data variations at test time. \n\nWhile the proposed idea is interesting, many of the core ideas seem to have been proposed in earlier work (Jin et al., 2017; Lazarow et al., 2017; Lee et al., 2018) and the novel contributions of this work are not clearly explained. It would be useful to split Section 3 into background section which reviews prior work and a section which describes the proposed idea and how it differs from prior work. \n\nExperiments: \n- The performance numbers in Table 1 seem quite far from the earlier published results. For instance, the ICN paper reports 6.5% test error on CIFAR and 1.9% test error on SVHN, which are much lower than the results reported in this paper \nhttps://papers.nips.cc/paper/6684-introspective-classification-with-convolutional-nets.pdf. It would be better to similar architectures as the original paper for a fair comparison.\n\n\nOther comments:\n- Section 3: it might be worth making a distinction between the two notions in which generative versus discriminative methods are discussed (i) using a discriminative binary classifier within a generative model and (ii) using the classifier that predicts class labels (e.g. the 10 classes in MNIST). \n- The extension to multi-class problems is only briefly discussed in page 4. Which version (series of one-vs-all or a single CNN classifier) is used in the experiments?  What's the computational complexity and how would it scale with the number of classes?\n- The semi-supervised learning results only discuss GAN-based methods, but there are a lot of VAE-based methods (cf. https://arxiv.org/abs/1406.5298) that would be worth discussing too\n- I think itâ€™s a bit confusing to label the methods \"WGAN-GP\", \"DCGAN\" as these methods uses both CNN for classifier in addition to the generative part.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good discussion, improved comparisons needed",
            "review": "This paper suggests the use of learned transformation networks, embedded within introspective networks to improve classification performance with synthesized examples.\n\nThe authors cite a number of related works, and give a good introduction to both introspective learning, as well as the particular usage for large variation resistance. This discussion forms the majority of the paper, and while the explanation seems clear it would be nice to have a stronger dedicated section on exact relations to both GAN and potentially VAE mathematically. Any kind of discussion which could tie the thorough derivation to some of the contemporaries in generative modeling would help the accessibility of the paper.\n\nMy primary concerns are from the experimental sections of the paper. The setup of the problem seems such that any strong baseline could be directly tested, since the final CNN is ultimately trained in a two stage setup on the aggregated dataset (as per subsection Baselines). Here it is also worth mentioning DAgger https://ri.cmu.edu/pub_files/2010/5/Ross-AIStats10-paper.pdf / https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf which is used in another context but in a similar way for improving imitation and online learning. \n\nMany of the baseline methods for table 1 seem far from what I would consider \"strong\" baselines. Given that the core proposal of the paper is improving classifiers, the importance of having high quality baselines cannot be overstated. Particularly, baseline CNN numbers for MNIST, SVHN, and CIFAR-10 are far from what has been seen in simple papers such as Wide ResNet https://arxiv.org/abs/1605.07146, ResNet https://arxiv.org/abs/1512.03385, Universum Perscription (which bears some resemblence to this work in high level concept) https://arxiv.org/abs/1511.03719, or even older work such as Maxout Networks http://proceedings.mlr.press/v28/goodfellow13.pdf . Particularly, these papers show examples of simple CNNs which outscore the best values reported in this table, on the same datasets. Running the same setup but with these improved classifiers as baselines would make a much stronger support for the core hypothesis that ITN can be used to improve strong classifiers.\n\nTable 2 seems to me an improper comparison. Methods such as zero-shot or meta-learning type approaches seem much more appropriate for testing cross-generalization improvement. In some sense, none of the tested methods besides ITN should even be able to cross-generalize well, so the fact that ITN does better here is not surprising to me. While this is a benefit of ITN as stated, seeing a comparison to methods deisgned for cross-generalization as well would make the results of Table 2 much stronger.\n\nTable 3 also seems have improper comparisons, in that there are a large number of works using semi-supervised generative models (Improved Techniques for Training GANS, which is already cited, SS-VAE https://arxiv.org/abs/1406.5298, Temporal Ensembling https://arxiv.org/abs/1610.02242, VAT https://ieeexplore.ieee.org/abstract/document/8417973/, Ladder Networks http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks, Auxiliary Deep Generative Models https://arxiv.org/abs/1602.05473, Manifold Tangent Classifier https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier) for improved classification in low data domains. Adopting the same settings and comparing directly to these methods would greatly strengthen this section as well, as simple classifiers (as shown in many of these previous papers) are not generally great baselines for semi-supervised modeling.\n\nIn addition, there should be a direct case where becoming robust to these kinds of transformations fails. For example, if my classification task is the rotation/position of a repositioned MNIST digit, becoming robust to these types of transformations may be harmful. An experiment or discussion about when robustness to large data variation might be harmful would be a good inclusion as well. As a more general comment, this method seems applicable outside of image domains, and it would be interesting to see it applied in other settings though it is likely outside the scope of this particular paper.\n\nFormatting of the paper (specifically spacing between sections and subsections) seems a bit off in general. If the authors are applying /vspace tricks to shrink spaces in the format, I would recommend taking a closer look at how and where, to make spacing more consistent over the whole document. Comparing to past ICLR papers (best papers, or high rated from past conferences) to see how they approach formatting could improve the visual appeal of this paper.\n\nOverall, this paper has a lot in its favor. The experimental section is thorough, if not as strong as I would like. The derivation of the method and motivation is clear, and there are a lot of avenues explored. However, as it currently stands the experimental sections should be stronger to really prove out the core claim \"Our method, ITN strengthens the classifiers by generating unseen variations with various learned transformations.\" compared to other methods using generative and semi-supervised methods in a similar vein. In addition, the clarity and approachability of the paper could be improved by drawing a relation to parallel related work such as GAN or VAE in more detail.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper builds on top introspective learning to bridge the gap between training and test set distributions. Overall, it is an interesting paper showing significant better performance results with and without existing data augmentation.",
            "review": "This paper proposes to combine the generative capabilities of introspection networks with geometric transformations in order to augment the dataset for training image based classifiers.  The paper is well written and shows promising results on standard datasets, (albeit the SVHN and cifar-10 results are not near STOA).\n\n- There are a few areas where the writing could be more clear: \\omega_t is introduced right after equation 5, but it is unclear what is that parameter. Is it the parameter of a separate f_t(x;\\omega_t) function used to generate the pseudo-negative examples?  Equation 5 is also confusion with f_t conditioned on both \\theta_t and \\omega_t.\n\n- How does the data-argumentation's parameter range compare to the learned \\sigma and g(.), does g(.) learn significantly different/bigger transformations?\n\n- Regarding computation time, how long does it takes to generate the augmented data set using the proposed method? Do you have to keep a series of f_t in-order to transform the pseudo-negative points (equation 12)?\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}