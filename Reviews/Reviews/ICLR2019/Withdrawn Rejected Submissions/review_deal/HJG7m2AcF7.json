{
    "Decision": {
        "metareview": "The paper proposes to build word representation based on a histogram over context word vectors, allowing them to measure distances between words in terms of optimal transport between these histograms. An empirical analysis shows that the proposed approach is competitive with others on semantic textual similarity and hypernym detection tasks. While the idea is definitely interesting, the paper would be streghten by a more extensive empirical analysis. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Specific tasks and mixt results in empirical analysis limit significance "
    },
    "Reviews": [
        {
            "title": "Interesting method, but needs more to show it is useful",
            "review": "The submission explores a new form of word representation based on a histogram over context word vectors, allowing them to measure distances between words in terms of optimal transport between these histograms. The authors speculate that this may allow better representations of polysemous words. The approach is mathematically elegant, and requires no additional training on top of existing approaches like Glove. To improve efficiency, they use clustering on context vectors. They present results on various semantic textual similarity and hypernym detection tasks, outperforming some baselines.\n\nThe paper presents itself as an alternative to word embeddings as a way of representing words. As far as I can tell, their method only really allows a way of computing distances between pairs of word representations, which hasn't been a useful concept for the vast majority of cases that word embeddings have been used for (translation, QA, etc.). Point estimates are at least very convenient to work with. That doesn't mean the proposed approach is useless, but the paper needs to give a much stronger motivation for when and why measuring distances between words may be helpful.\n\nThe experiments are a bit underwhelming. STS and hypernymy detection are somewhat unimpressive tasks to work - I'm not aware of any results on these tasks that have generalized to more realistic applications like translation or question answering. I think for publication with just these tasks, the method would need to show a dramatic breakthrough, which the submission definitely does not. The STS baselines are very simple bag-of-words approaches, and even then the results for the SIF baseline are much lower than those reported by Arora et al. (2017). At the least, there should be a comparison with the current state of the art. On the hypernymy task, what validation data was used? Unfortunately I'm not able to suggest better experiments, because I can't think of cases where their method would be useful.\n\nThe paper is significantly weakened by frequently making very strong claims based on rather limited experimental results (for one example, \"we illustrate how our framework can be of significant benefit for a wide variety of important tasks\" feels like quite a stretch). It would be much improved if some of the language was toned down. \n\nOverall, the paper introduces a mathematically elegant method for representing words as distributions over contexts, and for computing distances between these words. For acceptance, I think the paper needs to better motivate why the method could be useful, and back that up with more convincing experiments.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach, proposes representation augmentation as opposed to representation learning and the proposed distance not used for training.",
            "review": "The paper proposes a method to augment representation of an entity (such as a word) from standard \"point in a vector space\" to a histogram with bins located at some points in that vector space. In this model, the bins correspond the context objects, the location of which are the standard point embedding of those objects, and the histogram weights correspond to the strength of the contextual association. The distance between two representations is then measured with, Context Mover Distance, based on the theory of optimal transport, which is suitable for computing the discrepancy between distributions. \nThe representation of a sentence is proposed to be computed as the barycenter of the representation of words inside.\nEmpirical study evaluate the method in a number of semantic textual similarity and hypernymy detection tasks. \n\nThe topic is important. The paper is well written and well structured and clear. The method could be interesting for the community. However, there are a number of conceptual issues that make the design a little surprising. First, the method does not learn the representations. Instead, augments a given one and computes the context mover distance on top of that. But, if the proposed context mover distance is an effective distance, maybe representations are better to be \"learned\" based on the same distance rather than being received as inputs.\nAlso, whether an object is represented as a single point or as a distribution seems to be an orthogonal matter to whether the context predicts the entity or vice versa. This two topics are kind of mixed up in the discussions in this paper.\n\nOther issues:\n\n- One important technicality which seems to be missing is the exact value of p in Wp which is used. This becomes important for barycenters computations and the uniqueness of barycenters. \n- Competitors in Table 1 are limited. Standard embedding methods are missing from the list.\n- Authors raise a question in the title of the paper, but the content of the paper is not much in the direction of trying to answer the question. \n- It is not clear why the \"context\" of hyponym is expected to be a subset of the context of the hypernym. This should not always be true.\n- Table 4 gives the impression that parameter might not be set based on performance on validation set, but instead based on the performance on the test set.\n\n- Minor:\nof of\ndata ,\nby\nbyMuzellec\nCITE\n\nOverall, comparing strengths and shortcomings of the paper, I vote for the paper to be marginally accepted.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The method is not very novel or not very well-motivated. The experiment results are interesting but mixed.",
            "review": "\nPros:\nI also study some related tasks and suspect that Wasserstein is helpful for measuring co-occurrence-based similarity. It is nice to see the effort in this direction. \n\nCons:\nThe methods are either not very novel or not very well-motivated. The experiment results are interesting but mixed. If the doubts about the experiments are clarified and the methods are motivated better (or the strengths/weaknesses are better analyzed), I will vote for acceptance.\n\nRelated work:\nIn addition to the work in the related work section, some other work also studied the NLP applications of Wasserstein, especially the ones (such as [1,2,3]) which are related to similarity measurement. The authors should include them in the related work section. \n\nQuestion about experiments:\n1. Why are the SIF scores reported in Table 1 much lower than the results reported in Arora et al., 2017 and in [4]?\n2. If we compare CMD with DIVE + C * delta S, the proposed method wins in EVALution and Weeds, loses in Baroni, Kotlerman, BLESS, and Levy. If you compare DIVE + delta S (Chang et al. 2017) with DIVE + C * delta S, delta S also wins in EVALution and Weeds, loses in Baroni, BLESS, Kotlerman, and Levy (although CMD seems to be better than DIVE + delta S). \nBased on the fact that your method has a high correlation with DIVE + delta S (Chang et al. 2017), I guess that CMD does not work very well when the dataset contains random negative samples, but work well when all the negative samples are similar to the target words. If my guess is right, the performance should be improved on average if you multiply the scores from CMD with the word similarity measurement.\n3. To make it efficient, CMD seems to sacrifice some resolutions by using the K representative context. Does this step hurt the performance? Could you provide some performance comparison with different numbers of K to let readers know whether there is a tradeoff between accuracy and efficiency?\n4. Since the results are mixed, I suppose readers would like to know when this method will perform better and the reasons for having worse results sometimes.\n\nWriting and presentation suggestions/questions:\n1. If the proposed method is a breakthrough, I am fine with the title but I think the experiment results tell us that Wasserstein is not all you need. I understand that everyone wants to have an eye-catching title for their paper. The title of this paper indeed serves this purpose. Since the strategy is effective, more and more people might start to write papers with a title like this. However, having lots of paper called \"XXX is all you need?\" or \"Is XXX all you need?\" is definitely not good for the whole community. Please use a more specific title such as Context Mover Distance or something like that.\n2. The last point in the contribution is not supported by experiments. I suggest that the authors move this point to the future work section.\n3. It is good to see some negative results like Baroni in Table 2. Results on other datasets should not be put into Table 4 in Appendix.\n4. Using Wasserstein barycenter to measure sentence similarity seems to be novel, but the motivation is not very clear. Based on A.6, we could see that for each sentence, authors basically find the representative word which is most likely to co-occur with every word in the sentence (has the highest average relatedness rather than similarity) and measure the Wasserstein distance between the co-occurrence probability distribution. I suppose sometimes relatedness is a better metric when measuring sentence similarity, but I think authors should provide some motivative sentence pairs to explain when that is the case.\nUsing Wasserstein to detect hypernym seems to also be novel, but the motivation is also not clear. Again, a good example would be very helpful.\nThis point is also related to the last question for experiments.\n\nMinor writing suggestions:\n1. In section 3, present the full name of CITE\n2. If you put some important equations to the appendix (e.g., the definition of SPPMI_{alpha,gamma}), remember to point readers to the appendix. \n3. In the second paragraph of section 7, Nickel & Kiela, 2017 is a method supervised by a hierarchical structure like WordNet rather than a count-based or word embedding based methods. \n4. In Chang et al., the training dataset is not Wikipedia dump from 2015. This difference of evaluation setup should be mentioned somewhere (e.g., in the caption of Table 2).\n5. The reference section is not very organized. For example, the first name of Benotto is missing for the PhD thesis \"Distributional Models for Semantic Relations: A Study on Hyponymy and Antonymy\". The arXiv papers are cited using different formats. Only some papers have URL. The venue's names are sometimes not capitalized. Gaussian embedding is cited twice, etc.\n\n\n[1] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015). From word embeddings to document distances. In International Conference on Machine Learning (pp. 957-966).\n[2] Xu, H., Wang, W., Liu, W., & Carin, L. (2018). Distilled Wasserstein Learning for Word Embedding and Topic Modeling. NIPS \n[3] Rolet, A., Cuturi, M., & Peyré, G. (2016, May). Fast dictionary learning with a smoothed wasserstein loss. In Artificial Intelligence and Statistics (pp. 630-638).\n[4] Perone, C. S., Silveira, R., & Paula, T. S. (2018). Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}