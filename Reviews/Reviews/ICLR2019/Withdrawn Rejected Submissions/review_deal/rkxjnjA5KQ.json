{
    "Decision": {
        "metareview": "The paper proposes an transfer learning approach to reinforcement learning, where observations from a target domain are mapped to a source domain in which the algorithm was originally trained. Using unsupervised GAN models to learn this mapping from unaligned samples, the authors show that such a mapping allows the RL agent to successfully interact with the target domain without further training (apart from training the GAN models). The approach is empirically validated on modified versions of the Atari game breakout, as well as subsequent levels of Road Fighter, showing good performance on the transfer domain with a fraction of the samples that would be required for retraining the RL algorithm from scratch. \n\nThe reviewers and AC note the strong motivation for this work and emphasize that they find the idea interesting and novel. Reviewer 3 emphasizes the detailed analysis and results. Reviewer 2 notes the innovative idea to evaluate GANs in this application domain. Reviewer 1 identifies a key contribution in the thorough empirical analysis of the generalization issues that plaque current RL algorithms, as well as the comparison between different GAN models and finding their performance to be task-specific.\n\nThe reviewers and AC noted several potential weaknesses: The proposed training based on images collected by an untrained agent focus the data on experience that agents would see very early on in the game, and may lead to generalization issues in more advanced parts of the game. Indeed these generalization issues are one possible explanation for the discrepancies between qualitative and quantitative results noted by reviewer 1. While the quantitative results indicate good performance on the target task, the image to image translation makes substantial errors, e.g., hallucinating blocks in breakout and erasing cars in Road Fighter. To the AC, the current paper does not provide enough insight into why the translation approach works even in cases where key elements are added or removed from the scene. The paper would benefit from a revision that thoroughly analyses such cases as well as the reason why the trained RL policy is able to generalize to them.\n\nR1 further notes that the paper does not address the RL generalization issue, but rather presents an empirical study that shows that in specific cases it is easier to translate from a target to a source domain, than to learn a policy for the target domain. The AC shares this concern, especially given the limited error analysis and conceptual insights derived from the empirical study. There are further concerns about the experimental protocol and hyper-parameter selection on the target tasks. Finally reviewer 1 questions the claim of whether data efficiency matters more than training efficiency in the proposed setting.\n\nThere is disagreement about this paper. Reviewers 2 and 3 gave high scores and positive reviews, but did not provide sufficient feedback to the concerns raised by reviewer 1, who put forward significant concerns. \n\nThe AC is particularly concerned about the experimental protocol and hyper-parameter tuning directly on the test tasks. The authors counter this point by noting that \"We agree that selecting configurations based on the test set is far from ideal, but we also note that this is the de-facto standard in video game-playing RL works, so we do not believe our work is any worse than others in the literature in this regard.\" The AC worries about the lack of motivation to identify a strong empirical setup to arrive at the strongest possible contribution. A key concern here is that the results seem to vary substantially by task, GAN model used, etc. and substantial tuning on the target domain seems to be required. This makes it hard to draw any generalizable conclusions. This concern can be alleviated by including additional analysis, e.g., error analysis of where a proposed approach fails, or additional experiments designed to isolate the factors that contribute to a particular performance level. However, the current paper does not go to this detail of empirical exploration. Given these concerns, I recommend not accepting the paper at the current stage.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "interesting result, too little conceptual contribution"
    },
    "Reviews": [
        {
            "title": "An interesting method to improve transfer learning between related tasks. The motivation is strong, explanations are intuitive, technical parts are solid, experiments are sufficient. ",
            "review": "This paper propose an intermediate stage before transfer learning on playing new games that is with slight visual change. The intermediate stage is basically a mapping function to translate the images in the new game to old game with certain correspondence. The paper claims that the adding of intermediate stage can be much more efficient than re-train the model instead. \nThen the paper compares different baselines without the mapping model. The baselines are either re-trained from scratch or (partially) initialized with trained model. The learning curves show that fine-tuning fails to transfer knowledge from the source domain to target domain. The mapping model is constructed based on unaligned GAN. And the experiments are setup and results are shown.\n\nPros:\n+ The paper makes a very good start from analogizing human being adjusting himself between similar tasks. \n+ The paper demonstrates strong motivation on improving the existing transfer learnings that are either fail or take too much time to train from scratch.\n+ The paper clearly illustrate the learning curve of multiple approaches for transferring knowledge across tasks.\n+ The paper proves detailed analysis why using unaligned GAN to learn the mapping model, and gives\n+ I also like the experiment section. It is well written, especially the discussions section answer all my questions. \n\nQuestions:\n1.\tWhy fine-tuning from a model that is trained from related task does not help, even decelerate the learning process? Could you explain it more?\n2.\tCould you please also include in figure 2 the proposed transfer learning curve with the mapping model G? Iâ€™m curious how much faster it will converge than the Full-FT. And I suppose the retrain from scratch can be extremely slow and will exceed the training epoch scope in the figure.\n3.\tIn dataset collection, you use untrained agent to collect source domain image. Will it improve the results if you use well trained agent, or even human agent, instead? \n4.\tI hope, if possible, you can share the source code in the near future.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "TRANSFER LEARNING FOR RELATED REINFORCEMENT LEARNING TASKS VIA IMAGE-TO-IMAGE TRANSLATION",
            "review": "The paper seeks to generalize the reinforcement learning agents to related tasks. The authors first show the failure of conventional transfer learning techniques, then use GANs to translate the images in the target task to those in the source task. It is an interesting attempt to use the style-transferred images for generalization of RL agents. The paper is well written and easy to follow.\nPros:\n1.\tIt is a novel attempt to use GANs to generate pictures that help RL agents transfer the policies to other related environments.\n2.\tIt is an interesting viewpoint to use the performance of RL agent to evaluate the quality of images generated by GANS.\nCons:\n1.\tThe pictures generated by GANs can be hardly controlled, and extra noise or unseen objects might be generated, and may fool the RL agent during training.\n\nOther feedback:\nIn Figure 2, it seems the fine-tuning methods also achieve comparable results (Full-FT and Partial-FT), such as Figure 2(b) and Figure 2(c). Besides, the plot is only averaged over 3 runs, whereas the areas of standard deviation still overlap with each other. It may not be convincing enough to claim the failure of fine-tuning methods.\n\nMinor typos:\n1.\tIn 2.1, second paragraph: 80x80 -> $80 \\times 80$\n2.\tIn 2.1, second paragraph: chose -> choose\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Problematic qualitative results, generic unsupervised domain adaptation",
            "review": "# Summary\n\nThis paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work.\n\n\n# Strengths\n\nControlled toy experiments of Deep RL generalization issues:\nThe experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper.\n\nInvestigating the impact of different GANs on the end task:\nThe experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below).\n\n\n# Weaknesses\n\nDiscrepancy between quantitative and qualitative results:\nThe good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the \"translated\" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games).\n\nDoes not address the RL generalization issues:\nAlthough it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here).\n\nExperimental protocol:\nThe experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment.\n\nData efficiency vs actual training efficiency:\nThe main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware?\n\n\n\n# First Recommendation\n\nUsing image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions.\n\nI believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results.\n\n\n# Post-rebuttal Recommendation\n\nThanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I  also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result.\n\nNonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., \"de facto standard in RL\"), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work.\n\nAs there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of  just observing  / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience.  This might also require adding a third game to ensure more generalizable experimental insights.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}