{
    "Decision": {
        "metareview": "The proposed method proposes a new architecture that uses mixture of experts to determine what to share between multiple languages for transfer learning. The results are quite good.\n\nThere is still a bit of a lack of framing compared to the large amount of previous work in the field, even after initial revisions to cover reviewer comments. I think that probably this requires a significant rewrite of the intro and maybe even title of the paper to scope the contributions, and also make sure in the empirical analysis that the novel contributions are evaluated independently on their own (within the same experimental setting and hyperparameters).\n\nAs such, and given the high quality bar of ICLR, I can't recommend this paper be accepted at this time, but I encourage the authors to revise this explanation and re-submit a new version elsewhere.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Nice results, but somewhat limited novelty and framing compared to previous work"
    },
    "Reviews": [
        {
            "title": "Interesting idea!",
            "review": "This paper presents a multilingual NLP model which performs very well on a target language with any leveraging labeled data. The authors evaluated their framework on there different tasks: slot filling, named entity recognition and text classification. Overall, the results look very promising.\n- Strengthens:\n+ The proposed idea is novel.\n+ The results are very good for all three tasks.\n- Weaknesses:\n+ The authors claimed that their model knows what to share. However, they did not provide any evidence proving this hypothesis. Only the experimental results are not enough.\n+ The paper also lacks an analysis to show to some extent what the model learned, e.g. the attention weights or the value of the gate. Is there any correlation between the similarity among languages (source and target) and the attention weights.\n- What are not clear:\n+ It is not clear to me what exactly has been done with the CharCNN embeddings in Section 4.2? How did the authors train the embeddings (only with the source languages or also with the target language)? It seems to me that the proposed model did not work well in this case. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited novelty, nice write-up, impressive results, little/no analysis",
            "review": "My main reservation with this paper is the limited novelty. The approach seems to be a rather direct application of a subset of the sluice network architecture in [0] - which has been available on ArXiV since 2017 - with MUSE pre-trained embeddings. In particular, I don’t think the claim that the authors “propose the first zero-resource multilingual transfer learning model” is necessary - and I think it is way too strong a claim. Training an LSTM on English data with MUSE/vecmap embeddings is pretty standard by now, and this does not require any target language training data or cross-lingual supervision either. See zero-shot scenarios in [1-2], for example.\n\nApart from that, I think the write-up is nice, the approach makes a lot of sense, and results are impressive. I would have liked to see a bit more analysis. In particular, the fact that you learn gate values, makes it easy to analyze/visualize what and how your networks learn to share. \n\nI think there’s a few baselines in between BWE and MAN, e.g., simple adversarial training and adversarial training with GradNorm [3], that would put your results in perspective. Finally, I would like to encourage the authors to run experiments with actual low-resource languages: A literature on cross-lingual transfer experimenting with German, Spanish, and Japanese, could end up being very heavily biased. For tasks with data in more languages, consider, for example, POS tagging [4], morphological analysis [5], or machine translation [6]. \n\n[0] https://arxiv.org/abs/1705.08142\n[1] http://aclweb.org/anthology/P18-1074\t\n[2] http://aclweb.org/anthology/P18-2063\n[3] https://arxiv.org/abs/1711.02257\n[4] http://universaldependencies.org/\n[5] http://unimorph.org/\n[6] http://christos-c.com/bible/",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice idea but not very novel, extensive evaluation setup with positive resutls, unawareness of some important previos work",
            "review": "This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:\n\n1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.\n\n2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).\n\n3. It demonstrates the value of the proposed approach through an extensive experimental setup.\n\nAt the same time, I see two major limitations to the paper:\n\n1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.\n\n2. The authors claim that: \"To our best knowledge, this work is the first to propose\nan unsupervised CLTL framework without depending on any cross-lingual resource\"\n\nThis is, unfortunately, not true. I refer the authors to the paper:\n\nDeep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.\n\nIn their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}