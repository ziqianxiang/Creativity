{
    "Decision": {
        "metareview": "The proposed method is compressing video sequences with an end-to-end approach, by extending a variational approach from images to videos. The problem setting is interesting and somewhat novel. The main limitation, as exposed by the reviewers, is that evaluation was done on very limited and small domains. It is not at all clear that this method scales well to non-toy domains or that it is possible in fact to get good results with an extension of this method beyond small-scale content. There were some concerns about unfair comparisons to classical codes that were optimized for longer sequences (and I share those concerns, though they are somewhat alleviated in the rebuttal).\n\nWhile the paper presents an interesting line of work, the reviewers did present a number of issues that make it hard to recommend it for acceptance. However, as R1 points out, most of the problems are fixable and I would advise the authors to  take the suggested improvements (especially anything related to modeling longer sequences) and once they are incorporated this will be a much stronger submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "An interesting proposal for deep-learning-based video compression, but somewhat limited experimental results and unclear applicability. ",
            "review": "The paper is well written and the basic ideas are reasonably well explained and supported. However, several aspects are insufficiently explained. Several examples follow.\n\nIn Figure 1, it is not clear at all how the bitstream is formed; frames 1 to T are compressed jointly with frame t; but frame t is part of the set of frames from 1 to T. How the global state updated when compressing frame t+1? Using frames 2 to T+1?\n\nWriting that you use a Laplacian distribution because l1 regularized loss typically outperforms the l`2 loss for autoencoding images is clearly an insufficient justification, if not backed by experiments or references. Moreover, the authors seem to confuse regularization with loss; by using a Laplace density for the generative model, they are using a l1 loss, not an l1 regularizer. \n\nThere is absolutely no information about implementation details.\n\nThe video sequences used in the experiments are extremely small, both in spatial and temporal terms. A collection of 10 64*64 frames has fewer pixels than even a moderately sized still image. As the authors acknowledge, standard video codecs are far from optimized for video sequences of this size, making the comparisons unfair. The extreme compression results on the sprites and BAIR datasets may be quite misleading, since the data lives in a very low dimensional manifold, due to the simplicity of the scenes. For the more realistic Kinetics dataset, the proposed method is competitive with H264 and H265, but only in a very limited range of bit rates. In fact, the authors do not explain why they have not show results for wider ranges of bitrates.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good approach to deep learning based video compression, but empirical section needs work",
            "review": "Summary\n=======\nThis work on video compression extends the variational autoencoder of Balle et al. (2016; 2018) from images to videos. The latent space consists of a global part encoding information about the entire video, and a local part encoding information about each frame. Correspondingly, the encoder consists of two networks, one processing the entire video and one processing the video on a frame-by-frame basis. The prior over latents factorizes over these two parts, and an LSTM is used to model the coefficients of a sequence of frames. The compression performance of the model is evaluated on three datasets of 64x64 resolution: sprites, BAIR, and Kinetics600. The performance is compared to H.264, H.265, and VP9.\n\nReview\n======\nRelevance (9/10):\n-----------------\nCompression using neural networks is an unsolved problem with potential for huge practical impact. While there has been a lot of research on deep image compression recently, video compression has not yet received much attention.\n\nNovelty (6/10):\n---------------\nThis approach is a straightforward extension of existing image compression techniques, but it is a reasonable step towards deep video compression. \n\nWhat's missing from the paper is a discussion of how the proposed model would be applied to model video sequences longer than a few frames. In particular, the global latent state will be less and less useful as videos get longer. Should the video be split into multiple sequences treated separately? If yes, how should they be split and what is the impact on performance?\n\nEmpirical work (2/10):\n----------------------\nUnfortunately, the experiments focus too much on trying to make the algorithm look good at the expense of being less informative and potentially misleading.\n\nExisting video codecs such as H.265 and software like ffmpeg are optimized for longer, high-resolution videos, but even the most realistic dataset used here (Kinetics600) only contains short (10 frames) low-resolution videos. I suggest the authors at least add the performance of classical codecs evaluated on the entire video sequence to their plots. The current reported performance can be viewed as splitting the videos into chunks of 64x64x10, which makes sense for an autoencoder which has been trained to learn a global representation of short videos, but is clearly not necessary and detrimental to the performance of the classical codecs. I think adding these graphs would provide a more realistic view of the current state of video compression using deep neural nets.\n\nFor the classical codecs, were the binary files stripped of any file format container and headers before counting bits? This would be crucial for a fair comparison, especially for small videos where the overhead might be significant.\n\nMore work could be done to ensure the reader that the hyperparameters of the classical codecs such as GOP or block size have been sufficiently tuned.\n\nWhat is the frame rate of the videos used? I.e., how much time do 10 frames correspond to?\n\nThe videos were downsampled before cropping them to 64x64 pixels. What was the resolution before cropping?\n\nThe authors observe that the Kalman prior performs worse than the LSTM prior. This may be due to limitations of the encoder, which processes images frame-by-frame, which makes it hard to decorrelate frames while preserving information. I am wondering why the frame encoder is not at least processing one neighboring frame. (Note: A sufficiently powerful encoder could represent information in a fully factorial way; e.g. Chen & Gopinath, 2001).\n\nClarity:\n--------\nThe paper is well written and clear.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting, but very limited idea",
            "review": "This method deals with compressing tiny videos using an end-to-end learned approach. However, the paper has a significant number of limitations, which I will discuss below.\n\n1. The method has only been trained on very small videos due to the fact that fully connected layers are used. I don't really understand why was this necessary, and it's not explained in the paper at all. Just this fact makes it completely infeasible for any \"real\" application.\n2.  The evaluation was done on very limited domains. Of huge concern to me is the fact that very good results are presented on the sprites dataset. However, that dataset can be literally encoded by providing an index in a lookup table of sprites, so it's absolutely ludicrous to compare learned methods on that set to general video compression methods. The results look a lot less exciting when looking at the Kinetics 64x64 dataset. \n3. The evaluation (again) is problematic because the results refer to PSNR. PSNR for video is a very overloaded term. In fact, just the way to compute PSNR is not very clear for video. Video compression papers in general compute it in one of two ways: take the mean squared error over all the pixels in the video, then compute PSNR; or compute per frame PSNR then average. Additionally, none of the papers in this domain use RGB, because the human visual system is much more sensitive to detail preservation (the Y/luminance channel) than they are to chroma (color) changes. When attempting to present results for video, I would recommend to use PSNR-Y (and explain which type it is!), while also mentioning which ITU recommendation is used for defining the Y channel (there are multiple recommendations). \n4. It is not very clear how the global code is obtained. It is implied that all frames get processed in order to come up with f, but does this mean that they're processed via an LSTM model, or is there a single fully connected layer which takes as input all frames? In terms of modeling f, it sounds like the hyperprior model from Balle et al is employed, but again it's not clear to me how (is it modelling an entire video or a sequence?). I would really like to see a diagram for the network structure that computes f.\n\nOnt he positives of the paper: I applaud the authors with respect to the fact that they made an effort to explain how the classical codecs were configured and being explicit about the chroma sampling that's employed. \n\nI think all the problems I mentioned above can be fixed, so I don't want to reject the paper per se. If possible, should the authors address my concerns (i.e., add more details), I think this could be an interesting \"toy\" method. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}