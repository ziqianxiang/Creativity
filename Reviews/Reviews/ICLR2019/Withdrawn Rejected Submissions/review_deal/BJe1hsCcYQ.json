{
    "Decision": {
        "metareview": "Dear authors,\n\nThe reviewers all appreciated the treatment of the topic and the quality of the writing. It is rare for all reviewers to agree on this, congratulations.\n\nHowever, all reviewers also felt that the paper could have gone further in its analysis. In particular, they noticed that quite a few points were either mentioned in recent papers or lacked an experimental validation.\n\nGiven the reviews, I strongly encourage the authors to expand on their findings and submit the improved work to a future conference.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "A well-written paper that is a bit lacking in novelty"
    },
    "Reviews": [
        {
            "title": "Review for Lorentzian Distance Learning",
            "review": "Summary\n\nLearning embeddings of graphs in hyperbolic space have become popular and yielded promising results. A core reason for that is learning hierarchical representations of the graphs is easier in hyperbolic space due to the curvature and the geometrical properties of the hyperbolic space. Similar to [1, 2], this paper uses Lorentzian model of the hyperbolic space in order to learn embeddings of the graph. The main difference of the proposed approach in this paper is that  they come up with a closed-form solution such that each node representation close to the centroid of their descendants. A curious property of the equation for the centroids proposed to learn the embeddings of each node also encodes information related to the specificity in the Euclidean norm of the centroid. Also this paper introduces two additional hyperparameters. Beta hyperparameter is selected to control the curvature of the space. Depending on the task the optimal curvature can be tuned to be a different value. This also ties closely with the de-Sitter spaces. Authors provide results on different graph embedding benchmark tasks. The paper claims that, an advantage of the proposed approach is that the embedding of the model can be tuned with regular SGD without needing to use Riemannian optimization techniques.\n\nQuestions\n\nHave you tried learning beta instead of selecting as a hyperparameter?\nThe paper claims that Riemannian optimization is not necessary for this model, but have you tried optimizing the model with the Riemannian optimization methods?\nEquation 11, bears lots of similarity to the Einstein gyro-midpoint method proposed by Abraham Ungar which is also used by [2]. Have you investigated the relationship between the two formulations?\nOn Eurovoc dataset the results of the proposed method is worse than the d_P in H^d. Do you have a justification of why that happens?\n\n\nPros\nThe paper delivers some interesting theoretical findings about the embeddings learned in hyperbolic space, e.g. a closed for equation in the\nThe paper is written well. The goal and motivations are clear.\n\n\nCons\nExperiments are only limited to small scale-traditional graph datasets. It would be more interesting to see how those embeddings would perform on large-scale datasets such as to learn knowledge-base embeddings or for recommendation systems.\n\nAlthough the idea is interesting. Learning graph embeddings have already been explored in [1]. The main contribution of this paper is thus mainly focuses on the close-form equation for the centroid and the curvature hyperparameter. These changes provide significant improvements on the results but still the novelty of the approach is in that sense limited compared to [1].\n\n\nMinor comment:\n\nIt is really difficult to understand what is in Figure 2 and 3. Can you reduce the number of data points and just emphasize a few nodes in the graph that shows a clear hierarchy.\n\n[1] Nickel, Maximilian, and Douwe Kiela. \"Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry.\" arXiv preprint arXiv:1806.03417 (2018).\n[2] Gulcehre, Caglar, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia et al. \"Hyperbolic Attention Networks.\" arXiv preprint arXiv:1805.09786 (2018).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review - Lorentzian Distance Learning",
            "review": "The paper proposes a new approach to compute hyperbolic embeddings based on the squared Lorentzian distance. This choice of distance function is motivated by the observation that the ranking of these distances is equivalent to the ranking of the true hyperbolic distance (e.g., on the hyperboloid). For this reason, the paper proposes to use this distance function in combination with ranking losses as proposed by Nickel & Kiela (2017), as it might be easier to optimize. Moreover, the paper proposes to use Weierstrass coordinates as a representation for points on the hyperboloid.\n\nHyperbolic embeddings are a promising new research area that fits well into ICLR. Overall, the paper is written well and good to understand. It introduces interesting ideas that are promising to advance hyperbolic embeddings. However, in the current version of the paper, these ideas are not fully developed or their impact is unclear.\n\nFor instance, using Weierstrass coordinates as a representations seems to make sense, as it allows to use standard optimization methods without leaving the manifold. However, it is important to note that the optimization is still performed on a Riemannian manifold. For that reason, following the Riemannian gradient along geodesics would still require the exponential map. Moreover, optimization methods like Adam or SVRG are still not directly applicable. Therefore, it seems that the practical benefit of this representation is limited.\n\nSimilarly, being able to compute the centroid efficiently in closed form is indeed an interesting aspect of the proposed approach. Moreover, the paper explicitly connects the centroid to the least common ancestor of children in a\ntree, what could be very useful to derive new embedding methods. Unfortunately, this is advantage isn't really exploited in the paper. The approach taken in the paper simply uses the loss function of Nickel & Kiela (2017) and this loss doesn't make use of centroids, as the paper notes itself. The only use of the centroid seems then to justify the regularization method, i.e., that parents should have a smaller norm than their children. However, this insight alone seems not particularly novel, as the same insight can be derived for standard hyperbolic method and has, for instance, been discussed in Nickel & Kiela (2017, 2018), Ganea et al (2018), De Sa (2018). Using the centroid to derive new hyperbolic embeddings could be interesting, but, unfortunately, is currently not done in the paper.\n\nFurther comments\n- p.3: Projection back onto the Poincaré ball/manifold is only necessary when\n  the exponential map isn't used. The methods of Nickel & Kiela (2018), Ganea et al (2018) therefore don't have this problem.\n- p.7: Since MR and MAP are ranking measures, and the ranking of distances between H^d and the L^2 distance should be identical, it is not clear to me why the experiments show significant differences for these methods when \\beta=1\n- p.7: Embeddings in the Poincaré ball and the Hyperboloid are both compatible with the regularization method in eq.14 (using their respective norms). It would be interesting to also see results for these methods with regularization.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An incremental work on hyperbolic embedding with Lorentzian Distance",
            "review": "This paper proposed an unsupervised embedding method for hierarchical or graph datasets. The embedding space is a hyperbolic space as in several recent works such as (Nickel & Kiela, 2017). The author(s) showed that using the proposed embedding, the optimization has better numerical stability and better performance.\n\nI am convinced of the correctness and the experiment results, and I appreciate that the paper is well written with interesting interpretations such as the demonstration of the centroid. However, the novelty of this contribution is limited and may not meet the publication standard of ICLR.  I suggest that the authors enhance the results and resubmit this work in a future venue.\n\nTheoretically, there are three theorems in section 3:\n\nTheorem 3.1 shows the coordinate transformation from the proposed parametrization to hyperboloid then to Poincare ball preserves the monotonicity of the Euclidean norm. This is straightforward by writing down the two transformations.\n\nLemma 3.2 and theorem 3.3 state the centroid in closed expression based on the Lorentzian distance, taking advantage that the  Lorentzian distance is in a bi-linear form (no need to take the arcosh()  therefore the analysis are much more simplified) These results are quite striaghtforward from the expression of the energy function.\n\nTheorem 3.4  (centroid computation with constraints) shows that minimizing the energy function\n$\\sum_{i} d_L^2 (x_i, a)$, when a is constrained to a discrete set, is equivalent to minimizing $d_L(c,a)$, where $c$ is given by Lemma 3.2.\nThis is interesting as compared to the previous two theorems, but it is not clear whether/how this equivalence is used in the proposed embedding.\n\nTechnically, there are three novel contributions,\n\n1. The proposed unconstrained reparametrization of the hyperboloid model does not require to project the embedding points onto the hyperbolic manifold in each update.\n\n2. The cost is based on the Lorentzian Distance, that is a monotonic transformation of the Riemannian distance (without taking the arccosh function). Therefore the similarity (a heat kernel applied on the modified distance function) is measured differently than the other works. Informally one can think it as t-SNE v.s. SNE which use different similarity measures in the target embedding.\n\n3. The authors discussed empirically the different choice of beta, which was typically chosen as beta=1 in previous works, showing that tunning the beta hyperparameter can give better embeddings.\n\nThese contributions are useful but incremental. Notably, (1) needs more experimental evidence (e.g. a toy example) to show the numerical instability of the other methods, and to show the learning curves of the proposed re-parametrization against the Riemannian stochastic gradient descent, which are not given in the paper.\n\nBy listing these theoretical and technical contributions, overall I find that most of these contributions are incremental and not significant enough for ICLR.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}