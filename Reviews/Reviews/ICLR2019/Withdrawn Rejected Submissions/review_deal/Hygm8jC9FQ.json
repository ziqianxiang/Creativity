{
    "Decision": {
        "metareview": "This paper introduces an autoencoder architecture that can handle sequences of data, and attempts to automatically disentangle multiple static and dynamic factors.\n\nQuality:  The main idea is relatively well-motivated.  However the motivation for the particular technical choices made seems a little lacking, and the complexity of the proposed model put a lot of strain on the experiments.  A lot of important updates were made by the authors in the rebuttal period, however I feel the number of changes are a lot to ask the reviewers to re-evaluate.\n\nClarity:  The English of the paper isn't great, including the title (should be \"Using an ...\" or \"Using the ...\").  The intro is clear enough, but belabors a relatively simple point about how an image model can't model factors in video.  There were some concerning parts where major issues seemed to be glossed over.  E.g. \"FHVAE model uses label information to disentangle time series data, which is different setup with our FAVAE model.\"  As far as I understand, they both are trained from unsupervised data.\n \nOriginality:  This paper does a good job of citing related work, but seems incremental in relation to the FHVAE.  But the main problem is that the proposed method makes a lot of changes from a standard time-series VAE, and the limited number of experiments means it's hard to say what the important factor in this model's performance is.\n\nSignificance:  Ultimately it's hard to say what the takeaway from this paper is.  The authors motivated and evaluated a new model, but the work wasn't done in a systematic enough way to make an strong conclusions.  What conclusion were asserted seem specious and overly general, e.g. \" Since dynamic factors have the same time dependency, these models cannot disentangle dynamic factors.\".  Why not?  Why can't a dynamic model learn the time-scales of each of its factors automatically?\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Close, but a few foundational issues, and too many major changes to re-evaluate"
    },
    "Reviews": [
        {
            "title": "Good potential but needs more work",
            "review": "This paper presents a new approach to learning disentangled representations of sequential data. The model, FAVAE, is based on the information bottleneck framework (Alemi et al, 2016; Achille et al, 2016) and extends the recent beta-VAE (Higgins et al, 2017) and CCI-VAE (Burgess et al, 2017) work by changing the encoder/decoder to a Quasi-Recurrent Neural Network (QRNN) and adding multiple latents through a ladder VAE approach (Zhao et al, 2017). The authors demonstrate that their approach is able to learn a more disentangled representation than the limited set of baselines on three toy datasets.\n\nThe authors address a very important problem, since the ability to learn disentangled representations of sequential data can have far reaching consequences, as the authors rightfully point out in their introduction. I also like the approach that the authors are taking, which appears to be very general and does not seem to involve the need to have any domain knowledge. However, the paper could be improved by clarifying certain key parts and extending the experimental section, which is currently not quite as convincing as I would have hoped.\n\nThe summary of my concerns is the following: \n\nI would like to see more baseline comparisons 1) to an FAVAE with a different recurrent encoder/decoder; 2) to at least one other approach to disentangled representation learning on sequences; 3) an FAVAE without the capacity increase. I would also like to see all the baselines run on a non toy dataset of video data. Finally, I would like to see an expanded discussion of what the different latent variables at the different levels of the ladder architecture are learning. I recommend that the authors remove the MIE metric and shorten Section 3 to make space for the expanded experiments section.\n\nI do hope that the authors are able to address my concerns, because their method has a lot of potential and I am excited to see where they take it during the rebuttal period. Please see the more detailed comments below:\n\n1) Section 4.2 should be expanded to include a more detailed description of QRNN. This is one of the key modifications of FAVAE compared to CCI-VAE or beta-VAE, and it is currently not clear how QRNN works unless one reads the original paper referenced. The current paper needs to be self contained. It would also be nice to get a better understanding why QRNN was chosen over an LSTM or a GRU. It would be useful to see the results of the baseline experiments with an LSTM compared to the equivalent QRNN-based version of FAVAE.\n\n2) Why do the authors introduce the new MIE metric? The reported results do not show a significant problem with the MIG metric, and the need for the new MIE metric is not well motivated. If the authors insist on introducing this new metric, it is important to demonstrate cases where MIG fails and MIE performs better. Otherwise I would advise removing the new metric and using the space to expand section 4.2 instead. \n\nAnother point on the metric, Eq. 16 seems to be missing a term that goes over latents z_j. I assume there should be either a max_j or a mean_j in front of the I(z_j; v_k) term in the first part of Eq. 16?\n\n3) The related work section should be re-worded. Currently it reads as if the other approaches do not optimise a loss function at all. It would also be good to include one of the mentioned models as a baseline, and to run both FAVAE and one of the previously introduced models for disentangled sequential representation learning on a more complex dataset of video data.\n\n4) In section 7.1, it would be good to expand the discussion of how latent traversal plots are obtained. In particular, I do not understand how the different latent variables in the ladder architecture of FAVAE are traversed. In general, it would also be nice to expand the discussion of what the different latents at the different ladder stages learn, and how the number of ladder stages affects the nature of learnt representations.\n\n5) In terms of the baselines, it would be good to see the full ablation study. The way I see it, FAVAE has three modifications on the standard variational approaches: 1) the use of a recurrent encoder/decoder (R); 2) the use of the ladder architecture (L); and 3) the use of the capacity constraint objective (C). Currently the authors show the results of the R--, R-C, and RLC conditions. I would also like to see the results of the RL- condition (where beta=1 and C=0).\n\n6) In terms of the results presented in Tbl. 1, it would be nice to include the hyperparameters that the authors have swept over in the appendix, as well as a table of the architecture parameters and the best hyperparameters found for the models presented in the Experiments section. In Tbl.1 it is currently unclear how many seeds the authors used to calculate the standard deviations. The units of the standard deviations presented are also not clear. Finally, it is not clear whether the differences presented in the table are significant.\n\n7) It would be useful to include some details of the 2D Wavy Reaching dataset in the main text, even if it is just listing the nature of the 5 factors.\n\n8) It would be useful to expand the section that talks about the different settings of C explored (page 7, paragraph 2). Currently the point that the authors are trying to make in that paragraph and Fig. 4 is not clear. I would also recommend having beta in Fig. 4 on linear rather than log scale, as the log scale seems to be somewhat confusing.\n\n\n\n\nMinor points:\n\n-- Page 2/paragraph 2: used disentangle -> used to disentangle\n-- P4/p5: FAVAE is disentangled representation -> is for(?) disentangled representation\n-- P6/p1: the priors time dependency -> the priors' time dependency\n-- P7/p1: scores for2D -> scores for 2D\n-- P7/p4: MIE score (gray curve) -> MIE score (green(?) curve)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting and valuable model extension, but with rather early results",
            "review": "This paper proposes an extension to VAE to model sequential datasets and extract disentangled representations of their evolution.\nIt consists of a straight extension of CCI-VAE (Burgess et al 2018) to accept sequential data, combining it with a Ladder VAE architecture (VLAE, Zhao et al 2017).\nThey show early results on fitting toy domains involving 2D points moving in space (reaching, reaching in sub-sequences with complex dynamics, gripper domain).\n\nOverall, I think this is an interesting piece of work, which proposes a good model extension and assessment of its characteristics. The model is well presented, the different components are sufficiently motivated and they perform just enough experiments to showcase the effectiveness of their method, although with some reservations.\n\nCritics:\n1.\tThe comparison to Beta-VAE is a straw man, and I’m not sure it’s a valid way to introduce your model. You are basically saying that treating sequential data as if it was non-sequential is bad, which is clearly not surprising? Hence any comparison with Beta-VAE that you show, Figure 1 and Figure 3, are not appropriate (the caption of Fig 1 is particularly bad in that aspect). A more correct comparison would be to directly feed x_1:T to a Beta-VAE and see what happens, maybe with a causal time-convolution as well if you want to avoid 3D filters.\n2.\tYou are also not comparing to the FHVAE model you present in your Introduction, which would have been nice to see, given that your model is simpler and requires less supervision. Does FAVAE perform better than these?\n3.\tSection 3 could use a citation to Esmaeili et al 2018, which breaks out the ELBO even further and compares multiple models in a really nice way (e.g. Table A.2). Overall Section 2 and 3 feel a bit long and pedantic, you could just point people to the original papers and move some of the justification to Appendix (e.g. the IB arguments are not that required for your model. ).  The main point you want to put across is that you want to have your “z” compress a full trajectory x_1:T, under a single KL pressure (i.e. last sentence of Section 4).\n4.\tFigure 3 was hard to interpret at first, specifically for panels b and c. Maybe if you showed the “sampled trajectory” only once in another plot it would make it clearer.\n5.\tTime-convolution seems to wrongly be using the opposite indexing? With z_tk = \\sum_{p=0}^{H} x_{t+p}, you have an anti-causal filter which looks at the future x_t’ for a z_t? That does not sound right? Also, you should call these “causal convolutions”, which is the more standard term.\n6.\tThe exact format of the observations was never clearly explained. From 7.1 I understood that you input 2D positions into the models, but what about the Gripper?  As you’re aware, Beta-VAE and others usually get RGB images as inputs, hence you should make that difference rather clear. This is a much simpler setting you’re working in.\n7.\tDid you anneal the C as was originally proposed in Burgess et al 2018? With which schedule? This was not clear to me. The exact choices of C for the different Ladder levels lacked support as well. An overall section in the Appendix about the parameter ranges you tried, the architectures, the observation specifications, the optimisation schedule etc etc would be useful.\n8.\tI appreciate the introduction of the MIE metric, which seems to slightly improve over MIG in a meaningful way. However, it would be good to show situations where the two metrics disagree and why MIE is better, because in the current experiments this is unclear.\n9.\tOverall the Gripper experiments seem to merit a more complete assessment. Figure 7 was hard to understand, and I am not sure it shows clearly any disentangled factors. Its caption was strange too (what are the “(1, 8)”, “(2, 1)” things referring to?). \n10.\tI would have liked more interpretation and comments on why the Ladder is needed, and why FAVAE (without Ladder and C) does so badly in Table 1.\n11.\tIt would be good to know if you really find that the different levels of the Ladder end up extracting different time scales, as you originally claim it can. There are no results supporting this assumption in the current version.\n12.\tFigure 4B uses a bad scale, which makes it hard to assess what happens between the two C conditions for Beta \\approx 100, where they seem to differ the most in Fig 4A.\n13.\tFigure 5 could use titles/labels indicating which “generative factors” you think are being represented. Just compare them to your Figure 8 in Appendix.\n14.\tFigure 6 MIE scores look all within noise between models considered. How sensitive is the metric to actual differences in the disentanglement?\n\nOverall, I think this is an interesting improvement to disentangled representations learning, but suffers a bit from early experimental results. I would still like it to be shown at ICLR though as it really fits the venue.\nI'm happy to improve my score given some improvements on the points mentioned above.\n\nReferences:\n-\tBurgess et al., 2018: https://arxiv.org/abs/1804.03599\n-\tZhao et al., 2017: https://arxiv.org/abs/1702.08396 \n-\tEsmaeili et al., 2018: https://arxiv.org/abs/1804.02086 \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting, but not clear enough.",
            "review": "The paper proposes the factorized action variational autoencoder (FAVAE) as a new model for learning disentangled representations in high-dimensional sequences, such as videos. In contrast to earlier work that encouraged disentanglement through a carefully-designed dynamical prior, the authors propose a different encoder-decoder architecture combined with a modified loss function (Beta-VAE with a “shift C scheme”). As a result, the authors claim that their approach leads to useful disentangled representation learning in toy video data, and in data taken from a robotic task.\n\nThe paper appears to combine multiple ideas, which are not cleanly studied in isolation from each other. Several claims may be a bit oversold, such as potential applications for stock price data. But more importantly, the reasons why I don’t recommend accepting the paper are the following ones:\n\nLack of clarity:\nI found that the paper lacks clarity in its presentation. Equation 7 presents a model that seems to have only a latent variable z without time dependence, but how can dynamic and static factors be separately controlled? I don't see this question addressed in the experiments. Also, what is the significance of the model architecture (ladder network) as compared to the modified loss function?  Furthermore, Fig. 7 is hard to read.\n\nLack of Experiments:\nThe currently presented experiments are all on rather simple data. I recommend extending the experiments to the Sprites data set, used in (Li and Mandt 2018), or to speech data. Also, the paper lacks comparisons to the recently proposed disentangled representation learning models (FHVAE and Disentangled Sequential Autoencoder).\n\nWhile it is apparent that the model achieved some clustering, it is unclear to me if the final goal of separately controlling for static and dynamic aspects was really reached.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}