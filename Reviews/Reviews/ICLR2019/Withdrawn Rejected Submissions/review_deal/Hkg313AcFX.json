{
    "Decision": {
        "metareview": "This paper provides a good finding that maximizing a lower bound of the M-H acceptance rate is equivalent to minimizing the symmetric KL divergence between target the proposal. This lower bound is then used to learn sampler for both density and sample-based settings. It also nicely connects GAN with MCMC by providing a novel loss function to train the discriminator. Experiment on MNIST dataset in Sec 4.2 shows training the proposal with the symmetric KL is better than variational inference that optimizes KL(q||p).\n\nHowever, there are a few concerns raised in both the reviews and other comments that should be further clarified.\n1. Training an independent proposal may reduce the rate of convergence. \n2. In the density-based setting experiments, the learnt independent proposal is only used to provide an initial point and a random-walk kernel is actually used for sampling. This is different from what is proposed algorithm in Section 3.\n3. The proposed algorithm is only compared with VI in density-based setting, and there are no comparison with other baselines in the sample-based setting, despite the close connections of the proposed method with other models. Stochastic gradient MCMC methods, A-NICE-MC, GAN will be good baselines for empirical comparisons. Also, the dataset in Sec 4.2 is a subset of the standard MNIST, which makes comparison with other literatures difficult.\n\nFor the first concern, the authors provided new experiments for low-dimensional synthetic distributions. It is very helpful to show the comparable performance with A-NICE-MC in this case, but the real challenge in high-dimensional distributions remains unexamined. For the second concern, the authors consider the use of random-walk as a heuristic that allows to obtain better samples from the posterior, but that significantly changes the proposed transition kernel in Alg. 1.\n\nThis paper would be significantly stronger and make a very good contribution to this area by addressing the problems above.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Nice theoretic finding and connection of GAN with MCMC, but some concerns are to be addressed for a strong publication"
    },
    "Reviews": [
        {
            "title": "Empirical evaluation lacking to test the validity of the proposed objective",
            "review": "The paper proposes to learn transition kernels for MCMC by optimizing the acceptance rate (or its lower bound) of Metropolis-Hastings.\n\nMy main reason for worry is the use of independent proposals for this particular learning objective. While I can buy the argument in the Appendix on how this avoids the collapse problem of Markov proposals, I think the rate of convergence of the chain would greatly reduce in practice because of this assumption. \n\nUnfortunately, the empirical evaluation in this work is lacking to formally confirm or reject my hypothesis. In particular, it is absolutely crucial to compare the performance of this method with Song et al., 2017 (which does not make this assumption) using standard metrics such as Effective Sample Size. Another recent work [1] optimizes for the expected square jump distance and should also have been compared against.\n\n[1]: Levy, Daniel, Matthew D Hoffman, and Jascha Sohl-Dickstein. 2017. “Generalizing Hamiltonian Monte Carlo with Neural Networks.” ArXiv Preprint ArXiv:1711.09268.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The theoretic finding that maximizing the Metropolis Hastings acceptance rate is equivalent to minimize the symmetric KL divergence between target and proposal is great. Though, I have doubts about the correctness of the (main) proposed algorithm leveraging those insights and would expect, at least, an empirical verification of correctness.",
            "review": "One of the main contributions of the paper is showing how maximizing the acceptance rate in Metropolis Hastings (MH) translates in minimizing the symmetric KL divergence of the target and the proposal distribution. The result aligns with the intuition that if this variance is 0, detailed balanced is satisfied and, hence, we can always accept the proposal. Also Equation 11 nicely fits the intuition that a good (independent) proposal should minimize the KL divergence between proposal and target (as in VI) under the constraint that the proposal has full support compared to the target distribution, which is enforced by the last term. Theorem 1 and its proof are great.\n\nHowever, the proposed algorithms leveraging these findings are problematic. Algorithm 1 suggest independent Metropolis-Hastings in order to avoid the proposal to collapse to a point distribution, that is, a Dirac delta function centered at the current position. However, in the experiments, the authors study a \"modified\" version using a random walk proposal parameterized with the current (diagonal) covariance estimate. This is surprising as the authors explicitly avoided this proposal when motivating MH acceptance rate maximization.\n\nIn any case, the correctness of the algorithm is neither shown for an independent proposal nor a Markov Chain proposal. Indeed, I would argue that Algorithm 1 generally does not find a sample distributed according to the target distribution. The algorithm assumes that we can create a sample from the target p(x) that can be used to approximate the loss (a bound on the expected acceptance rate). However, if we could find an unbiased sample in the very first iteration of the algorithm, we could immediately stop and there wouldn't be a need for the algorithm at all. Hence, we must assume that the sample drawn from the target is biased (in the beginning); which is indeed a realistic assumption as neither independent MH nor random walk MH will yield an unbiased sample in any reasonable time (for any practical problem). However, this would bias the loss and, consequently, the parameter update for the proposal distribution. In particular, for random walk MH, I would expect the covariance to contract such that the final proposal indeed collapses to a point distribution. This is because the proposal is only required to have support around the observed samples and this area will become smaller over time. I would expect a proof that the sample at iteration k is \"better\" than a sample drawn at iteration k-1, to show that the bias vanishes over time. Though, I assume that this is hard to show as the proposal parameters form a Markov Chain itself. So at least a rigor empirical study is needed.\n\nTherefore, I would expect a metric measuring the quality of the final sample. The log-likelihood is not such a measure. While the marginalized log-likelihood could measure the quality of the sample, we cannot compute it for any real data/model (which is why we use sampling in the first place). So we need some artificial settings. However, the 1-dimensional toy example is insufficient as MH primarily suffers in high-dimensional spaces. It would be interesting to also report the acceptance rate depending on the number of dimensions of the target distribution. I would assume an exponential decay; even with learning, which might be the reason why the authors only report random walk MH in Section 4.2.\n\nAlgorithm 2 does not require to sample from some target distribution but can leverage the observed sample. While the algorithm nicely connects GANs and sampling, the actual value of the algorithm is not fully clear to me. Learning an independent proposal reduces the problem to learning a GAN; and learning a Markov Chain seems only relevant for sampling-based inference; however, we already have a sample from the target distribution, and we can sample more data using a trained GAN.\n\nMinor comments:\n- The prefix/in-fix notation of integrals is mixed, e.g. in Eq 19, \"dx\" appears before the integrand, but \"du\" appears after the integrand of the inner integral.\n\n\nUPDATE:\n\nThe revised version is much better in empirically demonstrating the value of the method; though, there is still some work needed. First, the artificial examples are still rather low-dimensional where independent MH is expected to perform well. Second, ESS does not help to assess the biasedness of the sample; maybe [1] can help with this. Third, NUTS might be a better baseline than standard HMC which is know to be sensitive to the stepsize/number of leapfrog steps. An effective sample size of 1 suggests that the method did not even start to sample - likely because of a bad choice of the stepsize and/or mass matrix. I would suggest using PyMC3's NUTS implementation. Finally, to show the correctness of the method, I would suggest to 1) let alpha converge to zero such that \\phi will be fixed at some point, and 2) ensure that the proposal has full support under the target for any value of \\phi. In this case, the sample drawn from the target will be unbiased for large enough n (same arguments as for adaptive MCMC should apply).\n\nThe idea of reusing the samples from previous iterations for approximating the loss is interesting and worth exploring.\n\n[1] Jackson Gorham, Lester Mackey. \"Measuring Sample Quality with Kernels\", https://arxiv.org/abs/1703.01717\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some points must clarified",
            "review": "I think the paper could be published, however I have some concerns.\n\nMayor comments:\n\n- My main concern is that I do not understand why do not directly apply the  KL divergence with respect to p(x) and q(x) instead of considering p(x)\\times q(x') and  p(x')\\times q(x). More specifically, I have understood that your approach is motivated by Theorem 1 (nice result, by the way) but I am not sure it is better than just applying the KL divergence with respect to p(x) and q(x), directly.\n\n- The state-of-the-art discussion for MCMC schemes in the introduction must be completed at least including the Multiple Try Metropolis algorithms, \n\nJ. S. Liu, F. Liang, W. H. Wong, The multiple-try method and local optimization in metropolis sampling, Journal of the American Statistical Association 95 (449) (2000) 121–134.\n\nL. Martino, \"A Review of Multiple Try MCMC algorithms for Signal Processing\", Digital Signal Processing, Volume 75, Pages: 134-152, 2018. \n\nThe sentence about adaptive MCMC's should be also completed.\n\nMinor comments:\n\n- Why do you say that \"MCMC is non-parametric\" in the introduction? in which sense? MCMC methods are sampling algorithms. Please, clarify.\n\n- In my opinion, Eq. (5)-(6)-(8)-(9)-(11)-(12)-(13) are not proper mathematically written  (maybe the same \"wrong\" way of written that, is repeated  in other parts of the text).\n\n- The results in the Figures in the simulations should be averaged more. Specially, Figure 3.\n\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}