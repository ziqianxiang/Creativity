{
    "Decision": {
        "metareview": "The paper contributes to the theoretical understanding of finite width ReLU networks. It contributes new ideas and constructions to investigate the representational power of such networks. In particular, the analysis works without skip connections. Referees found the paper refreshingly well-written and pleasant to read. \n\nThere is a concern that the paper may be overstating the novelty and innovation of the results, as some of them are easy implications, and there are other previous works that have obtained results on finite width networks (see AnonReviewer4's comments).  On the other hand, the authors were careful to cite when they reuse proof techniques from these and other works (AnonReviewer2). Another concern is that the considered target function space might be too narrow (see AnonReviewer2's comments). The authors clarify that the choice was because the considered classes are known to be hard to approximate and there are no known classical methods that would yield exponential approximation accuracy. Another concern is that the results might not be suitable to ICLR, having an emphasis on approximation theory and less on learning (see AnonReviewer3's comments). \n\nThe reviewers consistently rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below acceptance threshold ratings. \n\nWhile this appears to be a well written paper with valuable new ideas in regard to the approximation properties of networks, the contributions were not convincing enough. I would suggest that developing a clearer connecting to learning and broader classes of target functions could increase the appeal of the paper. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Well written paper, missing a clear selling point "
    },
    "Reviews": [
        {
            "title": "The paper considers uniform approximations of functions by ReLU neural networks. The authors derive bounds on the depth and width of such networks to achieve a certain degree of accuracy (\\eps) over classes of functions such as polynomials, sinusoidals, oscillatory textures, etc.",
            "review": "A main theme of the paper is showing that constant-width ReLU networks with depth increasing ploy-logarithmically in 1/\\eps can achieve the desired accuracy over the classes considered. \n\n- It seems to me that a major claim of the paper is that previous results did not have constant-width approximations. \n\nHowever, this does not seem accurate to me. For example, for polynomials, the statement “the width of the approximating network does not grow with the degree of the polynomial as is the case in Yarotsky (2016)...” does not seem true. The constructions in Yarotsky (2016) which much of the present work appears to be based on, in fact, allow for a constant-width approximation of polynomials of degree “n” in “d”-variables, over the cube, with \n\nwdith = 9, and depth ~ m \\log m [\\log (1/\\eps) + \\log m] where m = n + d. (*)\n\nThis bound is quite similar to Prop. 3.3 (with maybe even sharper dependence on “m”: m(\\log m)^2 versus m^2 in the paper. PS. I would also  double-check C in Prop. 3.3 which could be growing logarithmically in m.)\n\n(*) can be seen by inspecting the arguments around (14)-(15) in Yarotsky (2016) and noting that c_1 ~ \\log m in that argument. For example, considering d=1 which is the focus of the present paper, Yarotsky (2016) shows a constant-width approximation to the product function (x,y) \\mapsto xy which can be used to build a constant width-approximation to the monomial of the highest degree in the polynomial by recursive composition. All other monomials can be accessed serially at various depths of that architecture.\n\n- Much of the subsequent results in the paper are based on this constant-width approximation of polynomials as the authors point out. This is not that surprising given the Taylor approximation. For example, in Theorem 4.1, the first few lines of the proof show that the cosine can be approximated with a polynomial of degree m = O(\\log (1/\\eps)). Combining this with the polynomial approximation result one gets a constant-width approximation with \ndepth ~ \\log(1/\\eps)^2 or \\log(1\\eps)^2 \\log \\log (1/\\eps) \ndepending which version of the polynomial approximation result one believes (m \\log m versus m as the prefactor as discussed above).  In other words, it appears to me that the proof of Theorem 4.1 can be shortened considerably. It would be a corollary of the polynomial approximation. \n\n- A novelty of the present work over Yarotsky (2016) that the authors point out is avoiding skip connections in Proposition 3.1. (Perhaps this is true also for Prop. 3.3? Not discussed.) I haven’t checked the details here, but assuming correctness, I agree that it is quite interesting. I am not sure however if it is a significant improvement over the existing results.\n\n- The discussion of Section 5 and 6 might be new in this context and somewhat interesting. However, at least that of Section 5 again seems to be a natural byproduct the polynomial approximation result.\n\n- I would like to point out that there are other results on finite-width approximation by ReLU networks, establishing some quite sharp bounds, for example,\nHanin and Sellke 2017: arXiv:1710.11278\nYarotsky 2018: arXiv:1802.03620\nIn light of these, it wouldn’t be that accurate to claim that the issue of constant-width approximation is considered for the first time in the present paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "approximation theory using relu networks",
            "review": "This paper describes results regarding approximations of certain function families using ReLU neural networks. The authors emphasize two points about these networks: finite width and depth that is logarithmic in the approximation error parameter $\\epsilon$. \n\nThe first result concerns approximation of polynomials, which is used as a building block for all subsequent results. This result itself is quite simple and mostly follows from simple observations or known results, though it is possible that these have not been explicitly written in this form anywhere. The other results concern smooth functions, and some kinds of non-smooth functions such as the Weirstrass function. There are two neat observations (i) using the sawtooth function to approximate sinusoidal ones and (ii) using overlapping \"approximation\" to simulate an indicator. \n\nThe paper is refreshingly well-written and pleasant to read. Most of the results are tailored to work for either periodic functions, or can be expressed as: if piecewise polynomials are a good approximation, then so are constant depth neural networks with ReLU. I'm not sure that ICLR is the best venue for these kinds of results, as any connection with learning is at best tenuous, and the kind of approximation results don't seem to have any direct bearing on machine learning.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "narrow choice of function space, unclear how it relates to holder, sobolev",
            "review": "The focus of this paper is to show that finite-width deep neural networks with fully connected layers and ReLU activations are rate-distortion optimal approximators of certain classes of functions, meaning the approximation error decays exponentially in the number of neurons in the network. The function classes explored in this paper are: 1-d polynomials (on bounded intervals), 1-d sinusoidal functions (on bounded intervals), and other 1-d functions built from compositions or linear combinations of these, such as the so-called class of “oscillatory textures” and a class of continuous but nowhere differentiable functions known as Weierstrass functions. Finally, the paper also shows that as the desired approximation accuracy goes to zero finite-width deep ReLU networks require asymptotically fewer neurons than finite-depth wide ReLU networks in approximating a broad class of smooth functions.\n\n\nThe paper is well-written and the technical results are presented in a way that is easy to understand. The results are somewhat novel, although they do build off other recent works, namely Yarotsky (2016) and Telgarsky (2015). However, the authors were careful to cite when they reuse proof techniques from these and other works. The results in the main text appear to be technically sound. I did not check carefully all the proofs in the supplemental materials.\n\n\nMy major criticism is that the focus on certain specific function classes (oscillatory textures, Weierstrauss functions) seems arbitrary, and leaves open many questions. For example, there is existing work on the approximation ability of deep ReLU networks for functions in more general Holder and Sobolev spaces:\n\nHadrien Montanelli and Qiang Du. Deep ReLU networks lessen the curse of dimensionality. arXiv preprint arXiv:1712.08688, 2017.\n\nJ. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv e-prints, August 2017.\n\nI was left wondering how the present results relate to these works, and what insight we get from understanding these particular function classes that we don't get from understanding Holder or Sobolev spaces.\n\n\nMajor comments\n\n\nIn Section 3, I found the progression of the results from approximation of x^2, to multiplication xy, and to general smooth functions to be very natural and well-motivated. However, sections 4 and 5 seem lack somewhat in motivation, since here the authors focus on very specific function classes (sinusoidal functions, oscillatory textures, and Weierstrass functions). While these results are still interesting, focusing on such specific functions is less satisfactory, since it raises questions about the true scope of the results (e.g., will similar approximation rates extend to other fractal functions, or just Weierstrauss functions?). Could the authors give further justification for why these function classes are interesting to focus on, or why they limit themselves in this way? Can the authors also put these results more into context with existing results on the approximation with ReLU networks?\n\n\nThe authors state multiple times that “all our results apply to the multivariate case” but that they restrict themselves to the univariate case for simplicity of presentation. While this is fine, some indication of how the results are altered in the multivariate case would be useful. For example, does the fixed-width M in multivariate generalizations of Prop 3.1--3.3 need to be bigger, smaller, or the same? What other constants are dimensionally dependent? Do the multivariate generalization of their results bear the \"curse of dimensionality\", i.e., does the number of neurons needed to reach epsilon accuracy depend geometrically on the dimension?\n\nMinor comments\n\n\nA conclusion or discussion section summarizing the overall technical contribution would be useful for the reader. Also, it would be useful to include some discussion on remaining open problems or future work.\n\nOn pg. 2, the authors state “the approximation results throughout the paper guarantee that the magnitude of the weights in the network does not grow faster than polynomially in the cardinality of of the domain over which the approximation takes place”. What does “cardinality of the domain” here mean? I think the authors mean the size D of the interval [-D,D] over which the approximation is valid.\n\nOn pg. 7, the authors say “We note that this result allows to show that local cosine bases (cite) can be approximated by deep ReLU networks with exponential error decay…”. I think the authors mean to say “...this result allows us to show…” or “this result allows one to show…”. Although it’s not clear to me whether this means it has been shown (it’s a direct corollary), or could possible be shown (it’s a corollary, but needs some non-trivial work). Also, one line to specify what a “local cosine basis” is would be helpful.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}