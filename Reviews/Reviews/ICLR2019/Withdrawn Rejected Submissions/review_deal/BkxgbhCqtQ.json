{
    "Decision": {
        "metareview": "The reviewers agree this paper is not good enough for ICLR.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not acceptable for ICLR in current form"
    },
    "Reviews": [
        {
            "title": "Interesting topic but contributions are not well-motivated",
            "review": "The authors propose “Stochastic Quantized Activation Distributions” (SQUAD). It quantizes the continuous values of a network activation under a finite number of discrete (non-ordinal) values, and is distributed according to a Gumbel-Softmax distribution. While the topic is interesting, the work could improve by making more precise the benefit of (relaxed) discrete random variables. This will also allow the authors to more precisely display in the experiments why this particular approach is more natural than other baselines (e.g., if multimodality is the issue, compare to a mixture model; if correlation is a difficulty, compare to any structured distribution such as a flow).\n\nDerivation-wise, the method ends up resembling Gumbel-Softmax VAEs but under an information bottleneck (discriminative model) setup rather than under a generative model. Unfortunately, that in and of itself is not original. \n\nThe idea of quantizing a continuous distribution over activations using a multinomial is interesting. However, by ultimately adding Gumbel noise (and requiring a binning procedure), the resulting network ends up looking a lot like continuous values but now constrained under a simplex rather than the real line. Given either the model bias against a true Categorical latent variable, or continuous simplex-valued codes, it seems more natural as a baseline to compare against a mixture of Gaussians. They have a number of hyperparameters that make it difficult to compare without a more rigorous sensitivity analysis (e.g., bin size).\n\nGiven that the number of bins they use is only 11, I’m also unclear on what the matrix factorization approach benefits from. Is this experimented with and without?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Too many moving parts",
            "review": "The authors consider uncertainty estimation in deep latent variable models. They propose to use quantised latent variable and argue that this solves the overconfidence problem, commonly encountered in variational inference. The proposed approach relies on optimizing an information bottleneck objective instead of  the ELBO.\n\nWhile the approach is of interest, a number of questions, central to the work, remain. For example, it is not clear how parameter \\beta is chosen/optimised, how the number of bins C is chosen and how the annealing scheme is tuned. The authors do not discuss the quantisation parameters, such as bin size and location, which are likely to have a major effect on the performance (and the complexity). Then the authors propose to use a hierachical set of latent variables without properly justifying the need, nor discuss how to select the depth and its impact on the performance. Finally the authors propose yet another extension based on a matrix-factorization with little justification.\n\nOverall, this paper does not fully develop the ideas proposed in the paper or discuss them in sufficient detail. The experiments do not provide additional intuition on what's going on and why this helps and are insufficiently documented/made accessible to be convincing. For example, I am not sure what to conclude from experiments that rely on no (or \"light\") hyperparameter tuning, when the proposed method has many and not discussion is provided about how to set them or how sensitive results are to their actual value. More importantly, the initial claim that uncertainty is better captured relies on SGR, a metric which is not standard and mentioned in passing without being properly defined. The evaluation further depends on a \"selective classifier\" which is not detailed, but critical to understanding the experiments.\n\nFinally, the presentation of Section 3 could be significantly improved. For example, I would suggest distinguishing the neural network parameters of the encoder and the decoder as well as the encoder and decoder networks.  I would also refrain using notations like \"...\" or and always specify what is left and right of an equality. Please spell out all abbreviations at least once in the paper and define all important quantities and concepts.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Variational inference with discrete distribution for uncertainty estimation",
            "review": "This paper proposes runs variational inference with discrete mean-field distributions. The paper claims the proposed method is able to give a better estimation of uncertainty from the model. \n\nRating of the paper in different aspects ( out of 10)\nQuality 6, clarify 5, originality 8, significance of this work 5 \n\nPros: \n\n1. The paper proposes a generic discrete distribution as the variational distribution to run inference for a wide range of models. \n\nCons:\n\n1. When the method begins to use mean-field distributions, it begins to lose fidelity in approximating the posterior distributions. Even the model is able to do a good job in approximating marginal distributions, it is hard to evaluate whether the model is gaining benefit overall. \n\n2. I don't see a strong reason for using discrete distributions. In one dimensional space, a distribution can be approximated in different ways. Using discrete distributions only increases the difficulty of reparameterization. \n\n3. In the experiment evaluation, the algorithm seems only marginally outperforms competing methods. \n\n\nDetailed comments: \n\nIn the motivation of the paper, it cites low-precision neural networks. However, low-precision networks are for a different purpose -- small model size and saving energy. \n\nequation 6 is not clear to me.\n\nIn equation 10, how are these conditional probabilities parameterized? Is it like: z ~ Bernoulli( sigmoid(wz) ) ?\n\nIt is nice to have a brief introduction of the evaluation measure SGR. \n\nIn table 3, 1st column, the third value seems to be the largest, but the fourth is bolded. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}