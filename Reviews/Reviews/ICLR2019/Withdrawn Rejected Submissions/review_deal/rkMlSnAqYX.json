{
    "Decision": "",
    "Reviews": [
        {
            "title": "A great attempt but not clear experiment and unsatisfactory impact yet",
            "review": "This paper proposes two adversarial learning frameworks to make a neural entailment more robust against the annotation artifacts or biases. It is clearly written and easy to understand. Especially, I am very impressed by the survey on previous works which covers a lot of recent studies on biases in NLI dataset and robust NLI models. And, this leads to a clear motivation of the work and a well-defined problem setting. \nHowever, weakness of this paper is the lack of detailed analysis in the experiment and unclear impact of it. Here are more specific reasons for that. \n\n# unclear evidence for the generalization of the model \nAuthors mention a possibility of generalization of the proposed techniques to other tasks, but without any empirical evidence. Would you please conduct a simple experiment on synthetic data which are artificially generated by a certain degree of bias you set. Then, as the bias degree increases, the proposed method needs to show linear improvements against the bias. If this is true, I would believe the generalization capability of the proposed model. Otherwise, given the ambiguous observation from the absolute accuracy improvements between the baseline and the proposed model, I can’t believe it so. \n\n# lack of explanation for the relevance to active/transfer learning\nRemoving the bias in the training data seems to be relevant to active learning setting which finds informative points from the training data in general ML. Have you found any theoretical relevance of your adversarial training model with active learning? Otherwise, at least authors need to refer some recent studies. How is the adversarial training of two objectives analogous to the sampling mechanisms in active learning?\n\n# lack of further analysis of between bias and performance\nYou propose an architecture that can remove the bias of data. However, I can’t find any following analysis between the degree of biases and the improvements in performance. For example, at least authors need to show how much bias each of the target NLI datasets have. What happened if the dataset has no bias? What parts of the proposed model can handle/control this? Such a lack of details in the analysis makes me difficult to believe the robustness of the model. For instance, in the last paragraph of Section 6, the authors didn’t provide a clear effect of hyperparameters yet. \n\n# transfer setting is not an answer to the hypothesis\nThe most tricky part to understand is the Experiment section. The accuracy difference between the baseline and the proposed adversarial model doesn’t give the answer for the question of the authors, whether the proposed architecture actually removes the (hypothesis) bias in NLI dataset. Do authors perform the same experiment with the training and testing data from the same type of dataset for each? If then, how much improvement does the model achieve? How does your transfer setting -- training only with the SNLI dataset and testing on other datasets -- provide an evidence of your model that actually removes the data bias in NLI dataset and appropriately deals with them in the testing set? I don’t get quite convinced how transfer setting could be a practical solution for this problem. What is your clear conclusion here? By removing the bias in the dataset, do you actually make a better performing model than the baselines or only show that the two proposed architectures could possibly remove a few biases without a clear logic behind it? In order to better answer the original hypothesis you made, in my opinion, you need a clear dataset that does not contain any biases or some of the biases with a certain degree that you can control, and then show the correlation between the degree and the improvement (i.e., accuracy or whatever). \n\n# little or ambiguous impact of the result\nThe accuracy difference in Table 1 seems to be very minor and difficult to understand it due to lack of additional information about (1) how much bias each target dataset has, (2) a statistical test, and (3) real examples where the adversarial model can only answer. The absolute accuracy scores don’t include much information to provide any scientific observation of your original hypothesis. Especially, please provide a few examples where the hypothesis biases are actually resolved by the given architecture.\n\n# some questions on the model design\nThe double/single classifier designs only penalize the hidden representation of the f_H to remove the hypothesis bias. However, as the authors mentioned, the key part of the NLI task is appropriately finding the proper mapping function (g_NLI) between two sentences. How do you guarantee that the adversarial losses (i.e. L_Adv, L_RanAdv) actually remove the hypothesis bias while preserving the original meaning representation and so not hurting the performance in g_NLI?\n\nInferSent (Conneau et al., 2017) is not the state-of-the-art (SOTA) model for the tasks. Are the accuracy difference in Table 1 similar to other SOTA baselines? I know this might be an unnecessary question because authors like to show a proof-of-concept system of the proposed architecture instead of proposing another SOTA model. But, I am just curious about how the adversarial loss could be independent of other types of complicated classifier function (i.e. g_NLI). \n\nThe proposed architecture is not actually making the original g_NLI more robust, but it (probably) hurts the performance of it, even though it resolves the hypothesis bias in some sense. So, this is not a robust model but a trade-off design at this moment. Would you provide some experimental results that show improvements on the same training/testing data for each type of dataset? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Cool idea on how to mitigate biases in NLI data, but unconvincing results",
            "review": "In this paper, the authors attempt to mitigate annotation artifacts in natural language inference data. The authors propose an adversarial setup to discourage the model from overfitting to examples that can be solved by only considering the hypothesis. They consider two variants of the adversarial setup: 1) an independent adversarial hypothesis-only classifier trained jointly with an NLI model (“double classifier”) and 2) a single classifier that is trained adversarially with randomized premises (“single classifier”). \n\nWhile the idea of this paper is appealing and well-motivated. The results are not super strong, but they are relatively consistent (at least for the double classifier method). I also liked the extensive analysis. My main concern is that I am not entirely convinced that the experimental results support the main claim made by the authors---that the proposed adversarial approach is a viable solution to mitigating annotation artifacts. \n\nIf I understand correctly, the authors make two explicit assumptions: (a) that the biases in SNLI are somewhat responsible to its success. As a result, eliminating them also leads to performance degradation both in SNLI and on other datasets with similar biases (page 6: “This is expected, as we saw relatively small gains with the adversarial models, and can be explained by SNLI and MNLI having similar biases”). (b) that these biases are preventing the model from generalizing to other datasets that do not contain biases (or alternatively---contain other biases).\nAs a result, eliminating bias is desired, as it will improve generalizability, at the expense of lower in-domain results, that are somewhat inflated to begin with.\n\nThis is a nice and appealing story (though it could have been written more coherently). What I am concerned is whether the experimental gains observed actually result from mitigating biases. I can think of two alternative (though related) explanations for these gains: \nWere the pretrained SNLI baseline hyperparameters tuned on each corresponding dataset as well, or was the same model used in all cases? If the latter, this could partially explain the observed gains using their method: although the model has not seen any development example, selecting the best model based on the dev results gives it an unfair advantage over a model tuned on the original SNLI dataset.\nThe adversarial models suggested by the authors can be seen as a type of regularization. Did the authors try more traditional regularization methods such as L1/L2/dropout on the original SNLI model? Using high regularization values could have a similar effect (reduced performance on SNLI, increased performance on other datasets). Assuming you tune the regularization values on the target dataset (as you do with your methods), this is expected to lead to similar phenomena. I suspect that most gains observed in Table 1 are small enough in most cases to be obtained by such methods.\n\n\n \n\n\nOther points:\n\nOne experiment that I would like to see is whether biases are actually eliminated from SNLI. The authors could test the original SNLI model and the adversarial ones on a dataset containing the NULL string as premise (with the original hypothesis). Although the models are not trained on such data, based on the hypothesis-only results, I would expect the original model to do fairly well on it. If the authors’ claim is correct, we should see significantly lower performance by the adversarial ones.\nThe results on testing the adversarial models on the SNLI test set (Table 4), as well as Figure 4 should be in the main text rather than the appendix.\n\nMinor: \n— Page 6: “The fact that the two architectures agree to a large extent on which datasets benefit from adversarial training is a validation of our basic approach.”: this claim is unconvincing: the double classifier improved on 9/12 cases (75%). Selecting 5 of them at random, it is likely that 4 of them will show improvement with double.\n\n-- Typos and such:\n- Last paragraph on page 1: \"...biases.In this way...\" (missing white space)\n- Scitail is inconsistently spelled throughout the paper\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach for removing bias while learning sentence representation",
            "review": "This paper presents a method for removing bias of a textual entailment model through an adversarial training objective. While existing textual entailment datasets such as SNLI have been crucial for driving research on natural language inference and universal sentence representations, recent work showed that models only processing the hypothesis can achieve 67% accuracy, indicating that such models heavily exploit biases in the dataset. To mitigate these biases, the authors propose to let the model predict the label both from the premise-hypothesis representation and the hypothesis-only representation. On the backward pass, the sign of the gradient going into the hypothesis-only representation is flipped, making that representation invariant to biases that would otherwise allow for predicting the entailment label from the hypothesis only. The paper is written very clearly and the analysis of the methods is thorough. The method itself is a fairly simple trick and one could argue that, overall, this is incremental work. However, in my view learning less biased sentence representations is very relevant to the community and this paper is executed well. Particularly the improvements on the variety of downstream task compared to InferSent are impressive. I would be interested in hearing whether the authors have suggestions for applying similar techniques to entailment models that do not build up a specific premise and hypothesis representation (e.g. attention-based methods proposed for SNLI).",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}