{
    "Decision": {
        "metareview": "The paper investigates problems that can arise for a certain version of the dual form of the Wasserstein distance, which is proved in Appendix I. While the theoretical analysis seems correct, the significance of the distribution is limited by the fact, that the specific dual form analysed is not commonly used in other works. Furthermore, the assumption that the optimal function is differentiable is often not fulfilled neither. The paper would herefore be significantly strengthen  by making more clear to which methods used in practice the insights carry over. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Intersting theoretical analyis of a compact dual form of the Wasserstein distance which however is not widely used in literature. "
    },
    "Reviews": [
        {
            "title": "Lipschitzness of the discriminator is more critical than the choice of the divergence",
            "review": "The authors study the fundamental problems with GAN training. By performing a gradient analysis of the value surface of the optimal discriminator, the authors identify several key issues. \n\nIn particular, for a fixed GAN objective they consider the optimal discriminator f* and analyze the gradients of f* at points x ~ P_g and x~P_d. The gradient decouples into the magnitude and direction terms. In previous work, the gradient vanishing issue was identified and the authors show that it is fundamentally only controlling the magnitude. Furthermore, controlling the magnitude doesnâ€™t suffice as the gradient direction itself might be non-informative to move P_g to P_d. The authors proceed to analyze two cases: (1) No overlap between P_g and P_d where they show that the original GAN formulation, as well as the Wasserstein GAN will suffer from this issue, unless Lipschitzness is enforced. (2) For the case where P_g and P_d have overlap, the gradients will be locally useful which the authors identify as the fundamental source of mode collapse. \n\nThe main theoretical result suggests that (1) penalizing the discriminator proportionally to the square of the Lipschitz constant is the key -- the choice of divergence is not. This readily implies that pure Wasserstein divergence may fail to provide useful gradients, as well as that other divergences combined with Lipschitz penalties (precise technical details in the paper) might succeed. Furthermore, it also implies that one can mix and match the components of the objective function for the discriminator, as long as the penalty is present, giving rise to many objectives which are not necessarily proper divergences. Finally, one can explain the recent success of many methods in practice: While the degenerate examples showing deficiencies of current methods can be derived, in practice we implement discriminators as some deep neural networks which induce relatively smooth value surfaces which in turn make the gradients more meaningful.\n\nPro:\n- Clear setup and analysis of the considered cases. Interesting discussion from the perspective of the optimal discriminator and divergence minimization. The experiments on the toy data are definitely interesting and confirm some of the theoretical results. \n- A convincing discussion of why Wasserstein distance is not the key, but rather it is the Lipschitz constant. This brings some light on why the gradient penalty or spectral normalization help even for the non-saturating loss [2]. \n- Discussion on why 1-Lip is sufficient, but might be too strong. The authors suggest that instead of requiring 1-Lip on the entire space, it suffices to require Lipschitz continuity in the blending region of the marginal distributions. \n\nCon:\n- Practical considerations: I appreciate the theoretical implications of this work. However, how can we exploit this knowledge in practice? As stated by the authors, many of these issues are sidestepped by our current inductive biases in neural architectures.\n-  Can you provide more detail on your main theorem, in particular property (d). Doesn't it imply that the discriminator is constant?\n- Which currently known objectives do not satisfy the assumptions of the theorem?\n- The work would benefit from a polishing pass.\n\n========\nThank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets\"",
            "review": "The authors try to claim that Lipschitz continuity of the discriminator is a fundamental solution of GANs, and that current methods do not satisfy this approach in principle.\n\nThere are several false statements in this paper. In particular, sections 2.3 and 4.4 are wrong (and most of the paper is based on statements made there). The necessary constraint for the Wasserstein distance is NOT f(x) - f(y) <= d(x, y) for all x ~ Pr, y ~ Pg. It has to actually be 1-Lipschitz in the entire space. See Chapters 5 and 6 of [1], for example remark 6.4 or particular cases 5.16 and 5.4. Indeed, this is how it is written in all of the literature this reviewer is aware off, and it's a fact well used in the literature. Indeed, all the smoothness results for optimal transport in [1] heavily exploit the fact that the gradient of the critic is in the direction of the optimal transport map, which wouldn't be the case in the situation the authors try to claim of 'f not being defined outside of the support of Pr or Pg'.\n\nFurthermore, the relationship between Lipschitz continuity and having a gradient is elaborated in [2] https://arxiv.org/abs/1701.07875 , for example figure 2 clearly show this. Furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold *without* the Wasserstein distance, but with Lipschitz continuity are already elaborated in the wgan paper. See in fact, appendix G.1 [3], where this is described in detail.\n\n[1]: http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf\n[2]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf\n[3]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Although the proposed general formulation is itself interesting, some of the arguments are not sound, and the proposed scheme is somehow similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).",
            "review": "\n[pros]\n- It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases.\n- It also proposes use of the penalty term in terms of the Lipschitz constant  of the discriminative function.\n\n[cons]\n- Some of the arguments on the Wasserstein distance and on WGAN are not sound.\n- Theorem 3 does not make sense.\n- The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).\n\n[Quality]\nI found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below.\n\n[Clarity]\nThe main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions.\n\n[Originality]\nDespite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\\phi,\\varphi,\\psi$ and the form of the gradient penalty, $\\max \\|\\nabla f(x)\\|_2^2$ in this paper versus $E[(\\|\\nabla f(x)\\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal.\n\n[Significance]\nThis paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning.\n\nDetailed comments:\n\nIn Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation.\n\nIt is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the \"critic\" $f$ via a multilayer neural network with weight clipping.\n\nOne can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading.\n\nOn optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\\sim P_g}[f(x)]-E_{x\\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. \n\nI do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes \"$\\forall x \\not= y$\", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\\|x-y\\|$ to obtain $f(x)-f(y)=k\\|y-x\\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \\not= y$ such that $f(y)-f(x)=k\\|x-y\\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\\|x-y\\|$ under the Lipschitz condition.\n\nAppendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\\partial J_D/\\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as \"arg min\" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function.\n\nPage 5, line 36: $J_D(x)$ appears without explicit definition.\n\nPage 23, lines 34 and 38: Cluttered expression $\\frac{\\partial [}{\\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}