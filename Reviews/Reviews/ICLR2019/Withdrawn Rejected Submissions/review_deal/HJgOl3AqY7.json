{
    "Decision": {
        "metareview": "This paper proposes a VAE-based model which is able to perform musical timbre transfer.\n \nThe reviewers generally find the approach well-motivated. The idea to perform many-to-many transfer within a single architecture is found to be promising. However, there have been some unaddressed concerns, as detailed below. \n\nR3 has some methodological concerns  regarding negative transfer and asks for more extended experimental section.  R1 and R2 ask for more interpretable results and, ultimately, a more conclusive study. R2 specifically finds the results to be insufficient.\n\nThe authors have agreed with some of the reviewers' feedback but have left most of it unaddressed in a new revision. That could be because some of the recommendations require significant extra work.\n\nGiven the above, it seems that this paper needs more work before being accepted in ICLR. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Good motivation but important reviewers' concerns remain unaddressed"
    },
    "Reviews": [
        {
            "title": "Interesting but is hardly to ready due to the confusing introduction",
            "review": "The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. The authors define timbre transfer as applying parts of the auditory properties of a musical instrument onto another. It replaces the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective. By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.\nSome detailed comments are listed as follow,\n1 The implementation steps of the proposed method (MoVE) are not clear. Some details are missing, which is hardly reproduced by the other researchers.\n2 The experimental settings are not reasonable. The current experimental settings are not matched with the practice environment. \n3 The proposed method can transfer the positive knowledge. However, some negative knowledge information can be also transferred. So how to avoid the negative transferring? \n4 For the model, the optimization details or inferring details are missing, which are important for the proposed model.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and well written, but the evaluation is difficult to interpret",
            "review": "Summary\n-------\nThis paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.\nThe proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres).\nThe method is evaluated on a collection of individual note-level recordings from 12 instruments, grouped into four families which are used as domains.\nThe method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy.\nThe proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy.\nQualitative investigation demonstrates that the learned representation can approximate several coarse spectral descriptors of the target domains.\n\n\nHigh-level comments\n-------------------\nOverall, this paper is well written, and the various design choices seem well-motivated.\n\nThe empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.  Specifically, the authors introduced an extra input (control) to encode the pitch class and octave information during encoding.  I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.\n\nWhile I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.  Some of this comes down to incomplete definition of the metrics (see detailed comments below).\nHowever, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer). The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?  What is the criteria for bolding here?  It would be helpful if these scores could be calibrated in some way, e.g., with reference to\nMMD/KNN scores of random partitions of the target domain samples.\n\nSince the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD.  This could provide a slightly stronger version of the comparison in Figure 3, which shows that the overall distribution of spectral centroids is approximated by transfer, but does not demonstrate per-sample correspondence.\n\n\n\nDetailed comments\n-----------------\nAt several points in the manuscript, the authors refer to \"invertible\" representations (e.g., page 4, just after eq. 1), but it seems like what they mean is approximately invertible or decodable.  It would be better if the authors were a little more careful in their use of terminology here.\n\nIn the definition of the RBF kernel (page 4), why is there a summation? \n What does this index? How are the kernel bandwidths defined?\n\nHow exactly are reconstruction errors calculated: using the NSGT magnitude representation, or after resynthesis in the time domain?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice idea but falls short of what it promises",
            "review": "This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.\n\nUnfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.\n\nI have several further concerns about this work:\n\n* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.\n\n* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?\n\n* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.\n\nI appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.\n\nOverall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.\n\n\n\nOther comments:\n\n* In the introduction, an adversarial criterion is referred to as a \"discriminative objective\", but \"adversarial\" (i.e. featuring a discriminator) and \"discriminative\" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.\n\n* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.\n\n* Some turns of phrase like \"recently gained a flourishing interest\", \"there is still a wide gap in quality of results\", \"which implies a variety of underlying factors\", ... are vague / do not make much sense and should probably be reformulated to enhance readability.\n\n* Introduction, top of page 2: should read \"does not learn\" instead of \"do not learns\".\n\n* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a \"domain confusion loss\"), contrary to what is claimed in the introduction.\n\n* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.\n\n* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.\n\n* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. \"matching samples\").\n\n* Section 3.1, \"amounts to optimizing\" instead of \"amounts to optimize\"\n\n* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to \"Generating Sentences from a Continuous Space\" by Bowman et al. (2016). This should probably be cited instead.\n\n* \"circle-consistency\" should read \"cycle-consistency\" everywhere.\n\n* MMD losses in the context of GANs have also been studied in the following papers:\n- \"Training generative neural networks via Maximum Mean Discrepancy optimization\", Dziugaite et al. (2015)\n- \"Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy\", Sutherland et al. (2016)\n- \"MMD GAN: Towards Deeper Understanding of Moment Matching Network\", Li et al. (2017)\n\n* The model name \"FILM-poi\" is only used in the \"implementation details\" section, it doesn't seem to be referred to anywhere else. Is this a typo?\n\n* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?\n\n* The descriptor distributions in Figure 3 don't look like an \"almost exact match\" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}