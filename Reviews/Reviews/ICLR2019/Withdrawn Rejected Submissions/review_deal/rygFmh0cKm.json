{
    "Decision": {
        "metareview": "The paper proposes new methods for optimization of optimization of KL(student_model||teacher_model). \n\nThe topic is relevant. The paper also contains interesting ideas and the proposed methods are interesting; they are elegant and seems to work reasonably well on the tasks tried.\n\nHowever, the reviewers do not all agree that the paper is well written. The reviewers have pointed out several issues that need to be addresses before the paper can be accepted.\n\n\n\n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Interesting ideas but the paper has some issues",
            "review": "This paper proposes new methods for distilling a feed-forward generative model (student) from an autoregressive generative model (teacher) as an alternative to the reverse-KL divergence. The first part of the paper analyses optimization issues with the reverse KL divergence while in the second part of the paper alternatives are proposed (x-reconstruction and z-reconstruction).\n\nDetailed comments:\n\n1.\nIn abstract and other places: \"sparse gradient signal from the teacher\".\nSparsity implies that many of the values are exactly zero, while Section 3.1 seems to imply that some of the values might be small (or pointing towards the origin).\n\n2.\nIn Section 3.1 and 3.2 the authors discuss a potential failure mode of the reverse KL:\n\nBut, proposition 1 boils down to the fact that if the student's mass is more spread out than the teacher is some direction, that it should shrink that mass closer to zero as well.\n\nIn the example of the paper: if an eigenvalue of T is smaller than 1, it would mean that the student which is spherical Gaussian, would adjust its probability mass to also be smaller in that eigenvector's direction.\n\nAs training progresses, the students mass would be much closer to the teacher and the probability of 'pointing away' from the origin would be about as likely as pointing towards.\n\nSo it's not clear at all that the described property is problematic for optimization, as it could as well be interpreted as the student trying to fit the teacher's distribution better.\n\n3.\nWas the KL between P_S(x_i | z_<i) and P_T(x_i | x_<i) computed analytically? If these conditional distributions are Gaussian (which they are in many of the examples) this should be trivial.\n\n4.\nSection 4 about the neural vocoder needs to be expanded: many details are missing here and although it's one of the more important experiments in the paper it's relatively neglected compared to the other parts of the paper.\n\n5.\nIn the Section 4: the experiment with reverse-KL is a straw man comparison: For audio the reverse KL was only proposed in combination with the power loss (Oord et al). Two additional experiments would make the result a lot stronger: KL+power-loss and X-recon+power-loss. Because if the x-recon method does not work well together with the power-loss, its practical applicability seems limited.\n\n\nThe proposed methods are interesting, because they are elegant and seems to work reasonably well on the tasks tried. The first part of the paper about gradient sparsity/orientation needs to be addressed. Section 4 should be expanded and an additional comparison should be made.\n\nI would change my rating if these issues were addressed.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Convincing paper about of a potentially not-too-widespread technical issue.",
            "review": "The paper studies the problem of distilling a student probabilistic model (that\nis easy to sample from) from a complex teacher model (for which sampling is\nslow).  The authors identify a technical issue with a recent distillation\ntechnique, namely that positive gradient signals become increasingly unlikely\nas the dimensionality of the teacher model increases.  They then propose two\nalternative technique that sidestep this issue.\n\nThe topic is definitely relevant.  The paper focus on a single method for\nprobability distillation, which limits the significance of the contribution.\n\nThe paper is very well written and well structured.  Section 4 is may be a bit\ntoo dense for the uninitiated; it may make sense to clarify that calT and calS\nrefer to the teacher and student models---it is only obvious while reading this\nsection for the second time around.\n\nAll contributions seem novel.  The fact that the (reverse) KL can lead to bad\nmodels is known; the issue identified in this paper, however, seems novel.\n\nI could not spot any major flaws with the paper.\n\nThe evaluation is satisfactory.  The issue of KL-based training is very clear,\nas is the advantage of the encoder-decoder alternatives.\n\nI especially appreciated the link between distillation and encoder-decoder\narchitectures.\n\nDetailed comments:\n\n1 - How widespread is the issue identified in this paper?  In other words, is\nreverse KL realistically used in applications other than probability\ndistillation?\n\n2 - It is unclear to me why Proposition 2 is important.  This should be\nexplicitly stated.\n\n3 - It would make sense to add a forward pointer to Figure 3c in Section 3.1,\nto provide another example of mode-seeking.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting idea & fair results",
            "review": "This paper analyzes the limitation of probability density distillation with reverse KL divergence, and proposes two practical methods for probability distillation.\n\nDetailed comments:\n\n1) Typo: should be WaveNet, not Wavenet.\n\n2) In Proposition 1. $c_i$ should be $\\rho_i$.\n\n3) One may explain “path derivative” with more details. Also, I am really confused by Proposition 1 and its underlying implication. Given p_s and p_t are centered at the origin, isn’t p_s(x) already the optimal if it’s just a unit Gaussian. Why do we need a derivative pointing away from the origin? At least, one need parameterize p_s as N(0, \\phi)?\n\n4) In section 3.2, “set $\\mu = [2, 2]^T$”? Isn’t $\\mu$ a T dimensional vector?\n\n5) A lot of important details are missing in neural vocoder experiment. For x-reconstruction, do you use L1 or L2 loss?  For student model, do you use Gaussian IAF with WaveNet architecture as in ClariNet, or Logistic IAF as in Parallel WaveNet? Following this question, do you compute KLD in closed-form? Do you use the regularization term introduced in ClariNet? Student with KL loss and power loss outperforms x-reconstruction. Did you try x-reconstruction along with power loss?\n\nPros:\nCertainly, there are some interesting ideas in this paper. \n\nCons:\nThe experiment results are not good enough. The paper is poorly written. A lot of important details are missing.  \n\nHowever, I would like to raise my rating to 6, if these comments can be properly addressed.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}