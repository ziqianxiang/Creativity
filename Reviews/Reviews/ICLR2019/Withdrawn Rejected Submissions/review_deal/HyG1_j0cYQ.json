{
    "Decision": {
        "metareview": "The paper presents an approach to mitigate the presence of noisy labels during\ntraining by trying to forget wrong labels. Reviewers pointed out a few\nconcerns, including lack of novelty, lack of enough experimental support, and\nlack of theoretical support. Authors have added some experiments and details\nabout the experimental section, but reviewers still think it's not enough\nfor acceptance. I concur with the reviewers to reject the paper.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels.",
            "review": "This paper presents a meta algorithm to improve the robustness of learning methods under noisy labels. The idea is to squeeze out the negative effects of noisy labels actively. The paper trains deep neural networks by stochastic gradient descent on “fitting” labels; while trains deep neural networks by scaled stochastic gradient ascent on “not-fitting” labels. Experimental results show the improvement on robustness. \n\nThe good things of the paper are clear. \n1.\tTechnical sound with reasonable idea\n2.\tProblem is well motivated\n3.\tPaper is general well written.\n\nSome comments\n1.\tThe idea using instance selection is not new. The novelty could be improved. If the paper could make more insight from either theoretical or application value, would be more interesting.\n2.\tExperiments are too standard. More divers and various data sets would be more convincing. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Original idea with promising experimental results, but a limited contribution",
            "review": "A new method for defending against label noise during training of deep neural networks is presented. The main idea is to “forget” about wrongly labeled examples during training by an ascending step in the gradient direction. This is a meta-approach that can be combined with other methods for noise robustness; the detection of the noisy examples is specific of the base method utilised. Experimental results are promising.\n\nIn general, I find the idea original, and of potential practical use, but I believe the contribution of paper to be limited and not well supported by experiments. Moreover, I think that some claims made in this work are poorly justified.\n\n== Method\n\nThe paper would greatly benefit from some theoretical backing of the proposed optimization scheme, even on a simplified scenario. An idea would be to prove that, given a dataset with noisy labels, PumpOut converges close to the best model (= the one learned without noise), for certain hyperparameters. I think this would be new and interesting. A result of similar fashion was proven in [A].\n\nI found the following arguments not well or only heuristically supported:\n* section 2: the scaling factor \\gamma. Why using \\gamma=1 is suboptimal? One could claim that as much as you want to memorize the true image-label patterns, you also want to forget the image-noise ones.\n* Why MentorNet + PumpOut does not suffer from the selection bias effect of CoTraining? This is unclear to me\n* The non-negative version of the BackwardCorrection, and its appeal to Kiryo et al 17 is interesting, but it sidesteps its justification. A loss that can be negative does not necessarily means that it not lower-bounded. In fact, for a minimization problem to be well-defined, all you need is a lower bounded objective. Then, adding the lower bound makes your loss non-negative. Notice that Patrini et al 17 did not state that BC is unbounded, but only that it can be negative. Can you show more that that -- maybe, at least experimentally?\n\nThe statement of Theorem 2 is trivial. In fact, no proof is given as it would be self-evident. Moreover, the Theorem is not used by PumpOut. Algorithm 3 uses a if-else conditional on the scale of the backward correction, without the max(0, .). I suggest to remove this part. I have notice later that the non-negative version of BC is used as a baseline in the experiment, but I think that is the only use.\n\nRegarding the presentation, in section 3, I suggest to move the explanation of MentorNet and BackwardCorrection before their upgrade by PumpOut.\n\n== Experiments\n\nTable 1 can be removed as these are extremely common datasets.\n\nThe experimental results look very promising for applications. As a side effect of this analysis, I can also notice an improvement over BC given to the nnBC, which is nice per se. Although, I would have strengthen the empirics as follow.\n* SET2 is only run on MNIST. Why not even on CIFAR10 which is used in SET1? Any future reader will wonder “did it work on CIFAR10?\"\n* A much harder instance of noise, for instance open set [B] or from a real dataset [Xiao et al 15] would have more clearly supported the use of PumpOut for real applications.\n* Can the authors elaborate on “the choices of \\beta and \\gamma follows Kirkyo et al 2017” ? And how assuming their knowledge gives a fair comparison to BC which does not require them? I believe this is a critical point for the validity of the experiments.\n\nMinor:\n* “LRELU active function’ -> activation function. What is a LRELU? LeakyReLU?\n\n[A] Malach, Eran, and Shai Shalev-Shwartz. \"Decoupling\" when to update\" from\" how to update\".\" Advances in Neural Information Processing Systems. 2017.\n[B] Veit, Andreas, et al. \"Learning From Noisy Large-Scale Datasets With Minimal Supervision.\" CVPR. 2017.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Non convincing experiments   ",
            "review": "The paper proposes a meta algorithm to train a network with noisy labels.\nIt is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. \n\nMy main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. \nHence it is difficult to evaluate to performance of the proposed method.\nAt the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. \n    \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}