{
    "Decision": "",
    "Reviews": [
        {
            "title": "summary",
            "review": "This paper is not very well written. I cannot find what is the main contribution of this paper. For example, equation (1) seems out of the scope of the main discussion of this paper. It does not look a big deal employing EM algorithm for estimating the HMM parameters, but it seems the main technical contribution of the paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A method to model multiple sequential data sources, with interconnections among them obeying a weighted undirected graph. The paper is not well organized and the model description is not convincing. Potentially important empirical exploration is missing.",
            "review": "The authors propose a method to model sequential data from multiple interconnected sources using a mixture of common pool of HMM's. The composition of the mixture for each data source is both sparse and similar to that of other data sources that are close to it, with the closeness determined by the weights of the edges of an undirected graph.\n\nThe introduction section is unfocused and sloppy. HMM's are well understood models in machine learning but the paper falls short in explaining the particular distributed scenario of interest. The narrative jumps from sports, to neuroscience to wireless communications to fMRI,  without mentioning the common denominator. The proposed model section lacks also some focus, jumping from distributed sparse representations to multitask learning. The key concept here seems to be the poorly defined concept of 'a sparse HMM mixture over a large dictionary of HMM's'. The formalization of this rather complicated object is not well justified and leaves a lot of guesswork to the reader. \n\nInstead of maximizing the likelihood, an alternative objective function (4) is proposed as maximizing the inner products of posterior probability vectors at each node. The authors probably try to say something sensible but the sloppy notation is not very helpful here.   \n\nThe way authors introduced the graph structure into their cost function creates potentially a flexibility. However, the authors could have spent more energy on explaining why sparseness of the mixtures should be a desirable property for the problems they hope to solve with the model. The graph structure could potentially be used without necessitating sparsity, so opting for sparsity needs to be justified.\n\nOne also suspects that the authors could have written a clearer generative model instead of modifying the maximum likelihood criterion for learning. This would have enabled the readers to appreciate the generative model better.\n\nMoreover, the parameter $\\lambda$ controls how much effect the graphical interrelations is to have on the final learned parameters of the model. The authors however do not present a detailed examination of empirical results of varying $\\lambda$, and only suffice to determine it with cross-validation. A more interpretive stance towards lambda would confer increased understanding of how sparsity functions in this model. The cluster analysis at the end of the experiments indeed provide a more tangible demonstration of how sparsity affects the results obtained and why it might be desirable.\n\nOverall, the paper is not of sufficient quality to be presented at ICLR.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Use of mixtures questionable",
            "review": "The paper is fairly easily understandable in content. The question addressed in the work is very significant as is shown by multiple applications on real datasets in the paper. The reviewer is not aware of any prior work in connection to the question.\nThis work appears to be a first of its kind. However, there are some issues which the reviewer is not clear on.\n1. The penalization criteria enforced leads to sharing of one active component among all connected components of positive weights, irrespective of the magnitude of the weightage, thereby reducing it to a single component mixture distribution for  every connected subgraph of positive weights, which eliminates the need to use a mixture distribution in the first place. It is unclear why the use of mixtures could be useful therein.\n2. Although pairwise maximization for distributions does achieve the results in Proposition 1, however, simultaneous maximization does not guarantee sparsity, unlike as pointed out by the authors. It has not been clearly explained why the results would continue to hold true for pairwise maximization setting.\n3. A regularizer based on the value of the weights rather than only on the sign could prove more effective.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}