{
    "Decision": {
        "metareview": "This paper presents an algorithm for combining various feature types when training recurrent networks. The features are handled by modifying the update rules and cell states based on the features' type -- dense, sparse, static, w/ decay, etc.\n\nStrengths\n- The model handles each feature according to its type and handles cell state and transitions appropriately. \n- Extends earlier work to handle more feature types, like sparse features.\n\nWeaknesses\n- Limited novelty. Models similar to various aspects of the proposed system have been presented in prior works. For example: TLSTM, which the authors use as a baseline. Although some components are novel, like the treatment of sparse features, contributions, in my opinion, are not sufficient to be accepted at ICLR.\n- Presentation: Confusing and not enough information for reproducing results; multiple reviewers raised concerns about presentation of the feature types and experimental results. There were suggestions to improve, which the authors did consider during revision, but some concerns still remain.\n\nIn the end, the reviewers agreed about the limited novelty of this work, given existing literature. The recommendation, therefore, is to reject the paper. \n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Limited novelty, and needs better presentation"
    },
    "Reviews": [
        {
            "title": "Modified LSTM cell updates considering different feature types",
            "review": "This paper proposes a new type of recurrent neural network which takes into account five different features: in addition to the prevalent dense features, the author(s) also consider(s) sparse features, time features, global static and changing features. The differences in feature types are reflected in the cell state or output state update rules. Experiments on a modified UCI dataset and a proprietary dataset show that the proposed model outperforms the time-variant LSTM (TLSTM).\n\nPros:\n1. By decomposing the cell state into different components for different feature types (dense vs sparse, short term vs long term) and update them in different manners, the model takes advantage of the feature type information.\n2. By updating sparse feature related cell states only when sparse features are present, it could be potentially computationally cheaper than treating everything as dense (although in the paper due to more parameters the proposed model is actually slower).\n\nCons:\n1. The contributions are not significant. It seems that TLSTM already \"accounts for asynchronous feature sampling\" (Sec 1) and the novelty here lies most in how sparse features are treated.\n2. The presentation is sometimes confusing. For example, in Figure 5 which presents the main results, what's the \"relative change in F1 score\" and what's the unit in the plot? If it's percentage the gains seem to be too small and could be potentially due to the additional parameters. Besides, what does \"group sampling\" mean exactly? Furthermore, legend seems to be missing.\n3. Crucial implementation details are missing. The paper mentions that \"the number of features grows\" in the proposed model. Are sparse features and static features used or not in TLSTM? \n4. What's the difference between a static decay feature (Sec 3.5) and decay features (Sec 3.2)? Isn't the static decay feature varying with time as well?\n\nMinor comments:\n1. Figure 1 and 5 are too small and hard to read.\n2. Sec 3.3, \"updated based on Equation 1 and Equation 2\", but none of the equations are numbered in this paper.\n3. Some discussions on the proprietary dataset seem to be irrelevant. I'd rather see how are sparse features generated for the UCI dataset.\n4. The decay function $g= 1 / log (e + \\alpha^T x_t^{\\delta})$, how can we make sure that as time passes it decreases as time passes?\n\n\nOverall, I think explicitly taking into account different feature types in the LSTM cell update rules is interesting, but the contributions of this paper compared to TLSTM are not significant enough for acceptance, and the presentation can be made more clear.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a study of time series feature types for RNN models",
            "review": "The paper addresses limitations of the LTSM method for modeling time series. The work is motivated by applications where multiple time series need to be combined while they may get updated in an asynchronous fashion. Authors mention IoT applications in the Intro and give examples from a power consumption data set and a churn prediction application in their numerical experiments sections. \nThe paper's main contribution is to shrink down time series into 5 different categories which have different sampling schemes: (1) dense (2) sparse (3) decay (4) static decay (3) static standard, and proposing building blocks to incorporate these in a unified recurrent mechanism. \nThe paper's motivation for introducing sparse input to LTSM was rather straightforward and convincing, and the churn prediction application is an excellent motivation for this. I had a harder time following why the 3 other types of features needed to be included as well at this point. Perhaps a more problem oriented explanation with concrete examples could have helped. Or those features should not yet be used as generalizations but kept for future work.\nOverall I think the paper has several interesting ideas. Even though not all are as convincing, I think the paper is thought provoking and may interest the ICLR community.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting problem often faced in practice",
            "review": "Summary\n========\nThe paper addresses the problem of irregular spacing in sequential data with five different irregularity types. The solution is based on modifying LSTM cell.\n\nComment\n========\nIrregular and multiple spacing presents in many real world applications with time-stamped events. The problem is therefore important to address properly. However, I found the evaluation of the proposed solution is less convincing. The main results in Figure 5 are not informative. \n\nThere have been related works addressing irregular and multiple spacing (not just missing data as cited in the paper):\n\n- Pham, T., Tran, T., Phung, D., & Venkatesh, S. (2016, April). DeepCare: A deep dynamic memory model for predictive medicine. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 30-41). Springer, Cham.\n- Koutnik, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014). A clockwork RNN. arXiv preprint arXiv:1402.3511.\n- Chen, C., Kim, S., Bui, H., Rossi, R., Koh, E., Kveton, B., & Bunescu, R. (2018, October). Predictive Analysis by Leveraging Temporal User Behavior and User Embeddings. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 2175-2182). ACM.\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Brings together existing work and adds a bit, somewhat limited experiments",
            "review": "[Relevance] Is this paper relevant to the ICLR audience? yes\n\n[Significance] Are the results significant? somewhat\n\n[Novelty] Are the problems or approaches novel? reasonably\n\n[Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations.\n\n[Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat\n\n[Clarity] Is the paper well-organized and clearly written? yes, except the experiments\n\nConfidence: 2/5\n\nSeen submission posted elsewhere: No (but I did find it on arXiv after writing the review)\n\nDetailed comments:\n\nIn this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (“normal” RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases.\n\n=== Comments\n\nI found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the “non-decaying time series” observations.\n\n However, I had difficult coming up with an example of a “static decay feature”. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a “static decay” feature rather than just a “static” feature.)\n\nMy main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While “meaningfulness” may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.\n\n=== Typos, etc.\n\nThe plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}