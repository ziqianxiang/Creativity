{
    "Decision": "",
    "Reviews": [
        {
            "title": "A poorly motivated attempt at reducing the number of CNN model parameters",
            "review": "Poly-CNN is an attempt at reducing the number of filter variables by using the polynomial transformation of filters to create blow-up the filter responses. In the most aggressive scenario, a single filter and its transformations are used. I think that this approach might be motivated in the very early layers of a vision network, but can hardly be a good idea in higher layers, since they require increasingly specific features for a small subset of the overall image-space to be learned.  So, in my opinion one cannot expect good results with unaltered versions of this idea. The exceedingly lackluster results on imagenet and cifar10 (compared to very weak baselines) back up this intuition. In addition, the paper completely ignores the literature on reducing the parameter count with truly minimal effect on quality (e.g. hashing trick https://arxiv.org/pdf/1504.04788.pdf or Xception https://arxiv.org/pdf/1610.02357.pdf).\n\nThe idea of using polynomial transformation is not new, as the author(s) note(s), but in this setup it is backed up with neither with intuitive arguments nor with decent experimental results.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reduced parameter convolutions",
            "review": "It is an interesting idea for reducing the number of parameters, but I don't think the experimental section is adequate to judge it.\n\nModern ConvNet architectures achieve pretty good accuracy with fairly low overheads in terms of FLOPs and parameters:\n- SqueezeNet is a compressed version of AlexNet\n- MobileNet and ShuffleNets are compressed versions of ResNeXt\n- ConDenseNets are a compressed version of DenseNets.\n\nRegarding Table 7: It seems you are losing 7-8% absolute accuracy while still having the same computation overhead in terms of FLOPs. You do have few parameters, but trained ResNet models can be compressed substantially with relatively little loss of accuracy.\n\nSection 4.2 You say \"Since the convolutional weights are fixed, we do not have to compute the gradients nor update the weights.\"\nI don't understand this. Aren't the other (m-1) filters functions of the first filter? Should you not backpropagate through them to get an accurate gradient?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review of PolyCNN: Learning Seed Convolutional Filters",
            "review": "Review of PolyCNN: Learning Seed Convolutional Filters\n\nSummary:\nIn this paper, the authors propose the PolyCNN weight sharing architecture for cutting down the number of parameters in a given CNN architecture. The authors employ a bank of pointwise nonlinearities and a single seed filter to emulate a full convolutional filter bank. The authors demonstrate that their resulting model achieves performances comparable to modern CNN architectures on SVHN, MNIST, CIFAR-10 and ImageNet.\n\nMajor Comments:\n\nMy largest concern is the degree to which the PolyCNN weight sharing architecture is 'generic' and not specialized for a particular network architecture. If one has a convolutional filter bank inside a CNN layer, what happens if I just swap that architecture for the PolyCNN representation? This question is not directly address in Table 1, nor in Table 4, or 5 as far as I can tell (but happy to be corrected).\n\nThe 'baseline' network the authors compare against the PolyCNN appears to be a custom CNN architecture in which they swap in a PolyCNN representation. This is a reasonable result, but it is not clear the degree to which their results are tied to the specific 'baseline' architecture.\n\nTo argue that the method works generically, the authors must show a stronger result -- namely, for each or many of the baseline's considered (e.g. BC, BNN, ResNet, NIN, AlexNet), what happens if you swap in PolyCNN for the CNNs? This would generate a PolyCNN-variant of each baseline network for a side-by-side comparison. For each of these apples-to-apples comparisons, we would then observe the trade-offs by employing a PolyCNN weight scheme. (See Table 2 and 3 in [1] for such an analysis with another CNN module.)\n\nAlso, note that the CIFAR-10, ImageNet and MNIST results, the results are not near state-of-the-art (see [2] for many publications). Thus, the authors should consider comparing against more modern baselines. For example, fractional max-pooling (published in 2015; see [2]) achieves 96.5% accuracy but the top comparisons presented are 93.5%.\n\n\nMinor Comments:\n- In each table row corresponding to a network architecture, the authors should add a column indicating the number of parameters in the network architecture so the reader may clearly observe parameter savings.\n\n- Given that the PolyCNN architecture only has to perform 1 dot-product corresponding to the seed kernel and given that the dot-product is often the vast majority of the computational cost, do the authors likewise see a speed up in terms of inference time (or just measuring Mult-Add operations)?\n\n- The authors need to document what data augmentation techniques were used for each result. For instance, the state-of-the-art in CIFAR-10 is often broken down into \"best method with no augmentation\", or \"best method with XXX augmentations/permutations\"\n\n[1] Squeeze-and-Excitation Networks\nJie Hu, Li Shen, Gang Sun\nhttps://arxiv.org/abs/1709.01507\n\n[2] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}