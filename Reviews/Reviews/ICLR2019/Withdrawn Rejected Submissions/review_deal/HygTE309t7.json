{
    "Decision": {
        "metareview": "The paper proposes a decision forest based method for outlier detection.\n\nThe reviewers and AC note the improvement over the existing method is incremental.\n\nAlthough  the problem is of significant practical importance, AC decided that the authors should do more works to attract the attention of a broader range of ICLR audience.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Incremental contribution"
    },
    "Reviews": [
        {
            "title": "Review of submission 1492",
            "review": "Summary: This paper modifies an existing technique designed for image classification to make it applicable to outlier detection. \n\n\nStrengths: The outlined problem is of significant practical importance.\n\nWeaknesses: \n- The improvement over the existing method is incremental; \n- The regularization on routing decision may not really be necessary as, in DNDF, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the DNDF paper;\n- the datasets tested are standard image datasets, not even captured from vehicles or video surveillance. The SVHN (street view numbers) dataset is the closest the experiments get to the motivating application. \nOverall assessment: reject\n\nRecommendations for the authors: Test on a surveillance or street view benchmark. Even then, it's questionable whether the paper is suitable for ICLR due to lack of methodological novelty.\n\n\nNote: I'd like to apologize to the authors for the delay in submitting this review. It was due to a technical error on my part (I thought the reviews had posted, but they had not). In the spirit of independent evaluation, this review was not influenced by the other comments on this paper. I will follow-up with a response which will take into account the existing dialogue.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "outlier detection using decision forest",
            "review": "The paper proposes a decision forest based method for outlier detection and claims that it is better than current methods. \n\nA few questions:\nWhat is the precise definition of maximum weighted sum? Why not using maximum probability instead in Figure 1? Are they equivalent? What does this 8.1701 threshold refer to? What architectures you use for the experiment in Section 2?\n\nComments: \nThe observation that simple methods for outlier detection are not good enough is interesting, and deserves deeper understanding. \nHowever, directly calculating max. prob. may be a weak baseline. A stronger method to compare with would be using dropout during testing, see [1], which is easy to calculate and very practical (can easily be deployed to other tasks such as sequence tagging). \nThe extensibility of the proposed method is not clear to me. \n\nAlso, the reason that the observed failure of detection happens may due to the optimization procedure, i.e., how you train the model matters. The authors should provide the details of the training methods and architectures, along with the observation. \n\nThe baseline compared in the experiments are methods that do not use the classification feature. It would be necessary to compare with stronger baselines, such as using dropout. \n\nTypo:\n'a sample x $\\in$ based on its features'\n\nReference:\n[1] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, by Yarin Gal, Zoubin Ghahramani ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting promising solution to outlier detection; application of proposed scheme to general outlier detection seems limited",
            "review": "Pros\n----\n\n[Originality/Clarity]\nThe manuscript presents a novel technique for outlier detection in a supervised learning setting where something is considered an outlier if it is not a member of any of the \"known\" classes in the supervised learning problem at hand. The proposed solution builds upon an existing technique (deep neural forests). The authors clearly explain the enhancements proposed and the manuscript is quite easy to follow.\n\n[Clarity/Significance]\nThe enhancements proposed are empirically evaluated in a manner that clearly shows the impact of the proposed schemes over the existing technique. For the data sets considered, the proposed schemes have demonstrated significant improvements for this scoped version of outlier detection.\n\n[Significance]\nThe proposed scheme for improving the performance of the ensemble of the neural decision trees could be of independent interest in the supervised learning setting.\n\nLimitations\n-----------\n\n[Significance]\nBased on my familiarity with the traditional literature on outlier detection in an unsupervised setting, it would be helpful for me to have some motivation for this problem of outlier detection in a supervised setting. For example, the authors mention that this outlier detection problem might allow us to identify images which are incorrectly labelled as one of the \"known\" classes even though the image is not a true member of any of the known classes, and might subsequently require (manual) inspection. However, if this technique would actually be used in such a scenario, the parameters of the empirical evaluation, such as a threshold for outliers that considers 5000 images as outliers, seem unreasonable. Usually number of outliers (intended for manual inspection) are fairly low. Empirical evaluations with a smaller number of outliers is more meaningful and representative of a real application in my opinion.\n\n[Significance]\nAnother somewhat related question I have is the applicability of this proposed outlier detection scheme in the unsupervised scheme where there are no labels and no classification task in the first place. Is the proposed scheme narrowly scoped to the supervised setting?\n\n[Comments on empirical evaluations]\n- While the proposed schemes of novel inlier-ness score (weighted sum vs. max route), novel regularization scheme and ensemble of less correlated neural decision trees are extremely interesting and do show great improvements over the considered existing schemes, it is not clear to me why the use of something like Isolation Forest (or other more traditional unsupervised outlier detection schemes such as nearest/farthest neighbour based) on the learned representations just before the softmax is not sufficient. This way, the classification performance of the network remains the same and the outlier detection is performed on the learned features (since the learned features are assumed to be a better representation of the images than the raw image features). The current results do not completely convince me that the proposed involved scheme is absolutely necessary for the considered task of outlier detection in a supervised setting.\n- [minor] Along these lines, considering existing simple baselines such as auto-encoder based outlier detection should be considered to demonstrate the true utility of the proposed scheme. Reconstruction error is a fairly useful notion of outlier-ness. I acknowledge that I have considered the authors' argument that auto-encoders were formulated for dimensionality reduction.\n\n[Minor questions]\n- In Equation 10, it is not clear to me why (x,y) \\in \\mathcal{T}. I thought \\mathcal{T} is the set of trees and (x,y) was the sample-label pair. \n- It would be good understand if this proposed scheme is limited to the multiclass classification problem or is it also applicable to the multilabel classification problem (where each sample can have multiple labels).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}