{
    "Decision": {
        "metareview": "The authors present a method for training a policy for a self-driving car. The inputs to the policy are map-based perceptual features and the outputs are waypoints on a trajectory, and the method is an augmented imitation learning framework that uses perturbations and additional losses to make the policy more robust and effective in rare events. The paper is clear and well-written and the authors do demonstrate that it can be used to control a real vehicle. However, the reviewers all had concerns about the oracle feature representation which is the input and also concerns about the lack of baselines such as optimization based methods. They also felt that the approach was limited to self-driving cars and thus would have limited interest for the community.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "Flawed Approach with Poor Results",
            "review": "The paper describes a framework for training a self-driving policy by augmenting imitation loss with additional loss terms that penalize undesired behaviors and that encourage progress. The policy takes as input a parsed representation of the scene (rather than raw images) and outputs pose trajectories for a down-stream controller. The method is trained on simulated data that includes perturbations to improve generalizability. The framework is evaluated in simulation through a series of ablations to better understand the contribution of the different loss terms.\n\n\nSTRENGTHS\n\n+ Paper acknowledges the difficulty of end-to-end (pixels-to-torque) learning for autonomous driving and instead reasons over pre-processed inputs in the form of lower-dimensional images (and image sequences) that capture obstacles as bounding boxes and simple lines for routes, grayscale intensities, etc. Similarly, the output is a trajectory that is then fed to a controller responsible for tracking this trajectory.\n\n\nWEAKNESSES\n\n- The insufficiency of behavioral cloning is not surprising, as noted, given the covariate shift. It would be interesting to consider a  no-regret formulation analogous to Ross et al., 2011, even though it would require interaction with a human.\n\n- The limitation of producing paths in this way is that the network does not explicitly reason over the feasibility of the path, which is important for non-holonomic vehicles. Instead, the network must learn the kinematic and dynamic constraints.\n\n- Perturbations of the simulated trajectories are used to expose the model to collisions and other rare events, but is not clear that simple trajectory perturbations such as those used here provide a sufficient exposure to these rare events.\n\n- The fact that the 2D image that expresses the vehicle's position is absolute limits the environment in which the network is valid. The experiments are conducted on images corresponding to an 80m x 80m environment, which is trivially small.\n\n- The proposed framework is highly specific to self-driving and the extent to which it provides insights for other domains is not clear.\n\n- The ablation experiments are not very compelling. In the case of the nudging experiment, all models result in collisions with M4 being the best model with a 10% collision rate. The trajectory perturbation results are better. In the case of the slowing experiment, M3 is the only version to not result in collision, whereas M4 collides 5% of the time. It isn't clear than which model is preferable since, while M3 never collides in the case of the slowing down experiment, it collides 45% of the time in the nudging experiment, almost as frequently as the M0 baseline.\n\n- The paper claims that the model was run on a real robot, but there is no experimental evaluation of the results, only a reference to videos. The results of these experiments should be quantified and discussed or the reference to running on a real vehicle should be toned down, if not removed.\n\n- Equation 3 requires knowledge of the ground-truth distribution. How is this determined?",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting examples, somewhat weaker evaluation",
            "review": "The authors present a very interesting work on predicting future motion of a self-driving vehicle given image inputs that represent its surrounding and history. The authors use RNNs for this task, and data augmentation to make their model more robust. They also present a number of very interesting videos showcasing the performance.\n- Several key aspects of the work are not well explained. E.g., what are pixel sizes, time resolution, where is the vehicle positioned within the image? All this is missing.\n- Traffic lights are represented as \"a sequence of grayscale images\", how exactly, one for each state? Or some other way.\n- How were videos generated, how were various channels collapsed?\n- Dashed arrows not explained in Fig 2.\n- \"a small regression tower\", this needs to be elaborated. As well as other mentions of \"towers\".\n- In (3), is the sum over all pixels missing?\n- Section 4.1.2 is not clear, this needs to be expanded. It is not well explained how exactly these losses are computed and used.\n- For past-motion dropout, then you simply give blank input?\n- Figure 6 is referenced in the regular text although it is located in the appendix.\n- Orienting vertical axis with delta of +-25deg (as explained in Section 6.2) is not observed in the given videos, seems that there is no delta there. Is that done only during training?\n- What is the exact difference between open- and closed-loop experiments? Given that a number of other key aspects are missing, I am not sure I fully understand a difference here as well.\n- One of major issues in the evaluation section is that other baselines are missing (especially in the context of Fig 5). Even the more obvious ones would help a lot with understanding the performances, such as vehicle continuing to do what it was doing, or baseline predicting the route). This is a major flaw of the paper.\n- Some recent related work missing, see [1], [2], and related work.\n[1] Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net, Luo, Wenjie, Bin Yang, and Raquel Urtasun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[2] Short-Term Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, Djuric, N., Radosavljevic, V., Cui, H., Nguyen, T., Chou, F.-C., Lin, T.-H., Schneider, J., arXiv preprint:1808.05819, 2018.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A reasonable approach for self-driving vehicle control.",
            "review": "Summary.\nThe paper proposes a vehicle’s trajectory planner that iteratively predict next-step (longitudinal and latitudinal) position of an ego-vehicle. Instead of using a raw image, a set of handcrafted features (i.e., the status of traffic lights, route, roadmap, etc) are mapped onto a fixed-size of bird-eye view map, which is then fed into the recurrent neural network. Additional regularizing loss terms are explored for the robustness of the model. The effectiveness of the method is demonstrated in simulation and real-world experiment.\n\nStrengths.\n- Impressive demonstrations in simulation and real-world experiments.\n- The paper is generally well-written and easy to follow.\n\nvs. Existing motion planning approaches.\nThere exists a large volume of papers on vehicle motion planning, which has largely been explored for controlling self-driving vehicles. Some of them successfully demonstrated their effectiveness for navigating a vehicle in typical driving scenarios, including “slowing down for a slow car”.\nA notable survey may include:\n\n[1] Paden et al., “A survey of motion planning and control techniques for self-driving urban vehicles,” IEEE Transactions on intelligent vehicles, 2016. \n\nHowever, the paper provides neither any works of literature on existing motion planners nor any types of comparison with them. This makes hard to judge the proposed learning-based motion planner outperforms others including conventional optimization-based methods. \n\nMissing data collection details.\nThis work depends hugely on its own human-designated oracle-like map, which provides driving-related features, such as lane, the status of traffic lights, speed limits, desired route, dynamic objects, etc. Generating this map would not be a trivial task, but details are missing on (1) how this data collected and (2) how this data can be collected during the testing time (especially for dynamic objects/traffic light status). Section 6.2 should be explained more in detail.\n\nA weak novelty of using intermediate-level input/output representation.\nThere exist similar approaches that utilized similar representations to determine a vehicle’s behaviour, examples may include:\n\n[1] Lee et al., “Convolution Neural Network-based Lane Change Intention Prediction of Surrounding Vehicles for ACC,” IEEE ITSC 2017.\n[2] We et al., “Modeling trajectories with recurrent neural networks,” IJCAI, 2017.\n\nMissing evaluation details.\nIn Section 6.2, (though not mentioned) it seems that a training dataset is collected from 60-days of real-world driving (given the context). But, in the testing phase, it seems that the authors used a simulator to evaluate different driving scenarios with various initial condition (i.e., speed, heading angle, position, etc). Can authors clarify details of the evaluation environment?\n\nMinor concerns.\nA paragraph of contribution summary (in Introduction section) will help. \nTypos (e.g., Section 2 line 17: ‘off of’)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}