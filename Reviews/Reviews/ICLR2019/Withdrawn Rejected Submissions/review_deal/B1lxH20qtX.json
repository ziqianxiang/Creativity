{
    "Decision": {
        "metareview": "Strengths: A co-evolution of body connectivity and its topology mimicing control policy is presented.\n\nWeaknesses: Reviewers found the paper to be lacking in detail. The importance of message passing in achieving the given results is clear on one example but not some others. Some reviewers had questions regarding the baseline comparisons.\nThe authors provided lengthy details in responses on the discussion board, but reviewers likely had limited time to fully reread the many changes that were listed.\nAC:  The physics in the motions shown in the video require signficant further explanation. It looks like the ball joints can directly attach themselves to the ground, and make that link stand up. Thus it seems that the robots are not underactuated and can effectively grab arbitrary points in the environment. Also it is strange to see the robot parts dynamically fly together as if attracted by a magnet.  The physics needs significant further explanation.\n\nPoints of Contention: The R2 review is positive on the paper (7), with a moderate confidence (3).\nR1 contributed additional questions during the discussion, but R2 and R3 were silent.\n\nThe AC further examined  the submission (paper and video). \nThe reviewers and the AC are in consensus regarding\nthe many details that are behind the system that are still not understood.  The AC is also skeptical\nof the non-physical nature of the motion, or the unspecified behavior of fully-actuated contacts\nwith the ground.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "many missing details; strange physics; "
    },
    "Reviews": [
        {
            "title": "Collection of primitive agents is interesting, but",
            "review": "This paper investigates a collection of primitive agents that learns to self-assemble into complex collectives to solve control tasks.\nThe motivation of the paper is interesting. The project videos are attractive. However there are some issues:\n1. The proposed model is specific to the \"multi-limb\" setting. I don't understand the applicability to other setting. How much generality does the method (or the experiment) have?\n\n2. Comparison to other existing methods is not enough. There are many state-of-the-art RL algorithms, and there should be natural extension to this problem setting. I can not judge whether the proposed methods work better or not.\n\n3. The algorithm is not described in detail. For example, detail of the sensor inputs, action spaces, and the whole algorithm including hyper-parameters are not explained well.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea of dynamical \"self-assembly\" but unclear implications of the proposed message passing ",
            "review": "The paper describes training a collection of independent agents enabled with message passing to dynamically form tree-morphologies.  The results are interesting and as proof of concept this is quite an encouraging demonstration.\n\nMain issue is the value of message passing\n- Although the standing task does demonstrate that message passing may be of benefit. It is unclear in the other two tasks if it even makes a difference. Is grouping behavior typical in the locomotion task or it is an infrequent event?\n  - Would it be correct to assume that even without message passing and given enough training time the \"assemblies\" will learn to perform as well as with message passing? The graphs in the standing task seem to indicate this. Would you be able to explain and perform experiments that prove or disprove that?\n  - The videos demonstrate balancing in the standing task and it is unclear why the bottom-up and bidirectional messages perform equally well. I would disagree with your comment about lack of information for balancing in the top-down messages. The result is not intuitive.\n  - Given the above, does message passing lead to a faster training?  Would you be able to add an experimental evidence of this statement?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting ideas and the setup, but virtually no details provided",
            "review": "Summary:\n--------------\nThe paper considers the problem of constructing compositional robotic morphologies that can solve different continuous control tasks in a (multi-agent) reinforcement learning setting. The authors created an environment where the actor consists of a number of primitive components which interface with each other via \"linking\" and construct a morphology of a robot. To learn in such an environment, the authors proposed a graph neural network policy architecture and showed that it is better than the baselines on the proposed tasks.\n\nI find the idea of learning in environments with modular morphologies as well as the proposed tasks interesting. However, the major drawback of the paper is the lack of any reasonable details on the methods and experiments. It's hard to comment on the novelty of the architecture or the soundness of the method when such details are simply unavailable.\n\nMore comments and questions are below. I would not recommend publishing the paper in the current form.\n\n\nComments:\n----------------\n- If I understand it correctly, each component (\"limb\") represents an agent. Can you define precisely (ie mathematically) what the observations and actions of each agent are?\n\n- Page 4, paragraph 2: in the inline equation, you write that a sum over actions equals policy applied to a sum over states. What does it mean? My understanding of monolithic agents is that observations and actions must be stacked together. Otherwise, the information would be lost.\n\n- Page 4, paragraphs 3-(end of section): if I understand it correctly, the proposed method looks similar to the problem of \"learning to communicate\" in a cooperative multi-agent setting. This raises the question, how exactly the proposed architecture is trained? Is it joint learning and joint execution (ie there's a shared policy network, observation and action spaces are shared, etc), or not? All the details on how to apply RL to the proposed setup are completely omitted.\n\n- Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?\n\n- From the videos, it looks like certain behaviors are very unphysical or unrealistic (eg parts jumping around and linking to each other). I'm wondering which kind of simulator was used? How was linking defined (on the simulator level)? It would be nice if such environments with modular morphologies were built using the standard simulators, such as MuJoCo, Bullet, etc.\n\n\nAll in all, despite potentially interesting ideas and setup, the paper is sloppily written, has mistakes, and lacks crucial details.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}