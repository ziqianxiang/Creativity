{
    "Decision": {
        "metareview": "This paper proposes to approximate arbitrary conditional distribution of a pertained VAE using variational inferences. The paper is technically sound and clearly written. A few variants of the inference network are also compared and evaluated in experiments.\n\nThe main problems of the paper are as follows:\n1. The motivation of training an inference network for a fixed decoder is not well explained.\n2. The application of VI is standard, and offers limited novelty or significance of the proposed method.\n3. The introduction of the new term cross-coding is not necessary and does not bring new insights than a standard VI method.\n\nThe authors argued in the feedback that the central contribution is using augmented VI to do conditioning inference, similar to Rezende at al, but didn't address reviewers' main concerns. I encourage the authors to incorporate the reviewers' comments in a future revision, and explain why this proposed method bring significant contribution to either address a real problem or improve VI methodology.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Not well motivated and lack of novel contribution"
    },
    "Reviews": [
        {
            "title": "A paper that needs work in terms of motivation, exposition, and evaluation",
            "review": "(apologies for this belated review)\n\nSummary\n\nThe authors consider the task of imputing missing data using variational auto-encoders. To do so, they assume a fixed pre-trained generative model, perform variational inference to infer a posterior on latent variables given a partial image, and then use this approximate posterior to predict missing pixels. They compare a variety of parameterizations of the variational distribution to HMC inference, and evaluate on MNIST, Celeb-A and the Anime data. \n\nComments\n\nThere are many things about this paper that I don’t understand. My main concern is that I fail to follow why the authors are interested in this task. In what settings would we be interested in performing non-autoencoding variational inference in order to impute missing data? Moreover, in cases where are interested in performing such imputations, what would we like to use the results for? This paper seems like a nice demo, but I’m not entirely convinced I see a compelling application. \n\nMy second concern is about the baselines that are considered. If I were interested in carrying out this inference task, my inclination would not be to run an HMC chain to convergence, but instead to do something like annealed importance sampling (AIS), where at each step I run an iteration of HMC on a large batch of samples on a sequence of target densities that interpolate between the prior and full joint p(x, Z). If computational cost is a concern, I imagine this would not be more expensive than training a density estimator. Moreover, whereas HMC is generally not known to be a good method for estimating marginal likelihoods, AIS methods generally perform much better.\n\nFinally I find the language used in this paper confusing. Cross-coding seems a misnomer for the technique that the authors propose. Isn’t this simply a form of variational inference in which qψ(Z) approximates pθ(Ζ | x)? The term “-coding” suggests that we somehow define an encoder that accepts the query as input. Moreover, isn’t the XCoder network just a neural density estimator? \n\nFinally, Lemma 1 seems like a really roundabout way of deriving a lower bound. The authors could instead just write:\n\n\tlog p(x)\n\t>=\n\tE_q(Z,Y)[log p(x, Y, Z) - log q(Z, Υ)]\n\t=\n\tE_q(Z,Y)[log p(x | Ζ) + log p(Y | Z) + log p(Z) - log q(Z) - log p(Υ | Z)]\n\t=\n\tE_q(Z,Y)[log p(x | Ζ) + log p(Z) - log q(Z)]\n\t=\n\tE_q(Z)[log p(x, Ζ) - log q(Z)]\n\nThis avoids confusing terminology such as cross-coding, and shows that what the authors are doing is in fact just variational inference. Am I missing something here?\n\nI am also confused about how the comparison to HMC is set up. If you’re training qψ(Z), then you presumably need generate a certain number samples at training time. Shouldn’t you add this number of samples number of samples you generate in HMC, in order to get a more apples to apples comparison in terms of the amount of computation performed? As it stands, it is hard to evaluate whether these methods are given a similar number of samples. \n\nFinally, I am not quite sure what to make of the experimental evaluation. We see some scatter plots on MNIST with a 2D latent space, and some faces of celebrities in which there is arguably some sample diversity, although most of this diversity arises in blurry looking hairstyles. However, since the authors condition on the eyes, rather than, say, the nose or mouth, it is hard to know how good a job the network is doing at generalizing to multiple plausible faces. \n\nOverall, I find it difficult to judge the merit of this paper. Is this task in fact hard? Is it useful? Are the results good? Maybe the authors can give us some additional guidance on these questions.\n\nQuestions\n\n- I’m a bit worried that not all the samples that we see in Figure 6 may have equally high probability under the posterior. Could the authors compute and report importance weights?\n\n\tW = p(x, Z) / q(Z)\n\t\n- Could the authors say something about the effective sample size that we obtain when using the learned distribution q(Z) as a proposal? \n\t\n\tESS = (Σ_k w^k)^2 / (Σ_k (w^k)^2)\n\n- Should it be the case that the ESS is low, and the weights are high variance, could the authors generate a sufficient number of samples to ensure the the ESS = 25 (i.e. the number of images in the figure) and then show the 25 highest-weight samples (or resample 25 images with probability proportional to their weight)?\n\n\t\nMinor \n\n\n- Equation (3): There’s an extra p_θ in the first integral\n\n- In the proof in Appendix 6.1 \n\n\tKL[ qψ(Z) ‖ pθ(Z | x) ] + KL[ qψ(Y | Z) ‖ pθ(Y | Z, x)]\n\nit would be clearer to explicitly denote the expectation over qψ(Z)\n\n\tKL[ qψ(Z) ‖ pθ(Z | x) ] + E_qψ(Z)[ KL[ qψ(Y | Z) ‖ pθ(Y | Z, x)] ]\n\t\n(I had to google lecture notes to find out that this expectation is sometimes implicit, which \nas far as I know is not very standard). \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "I don’t quite see what is new about this paper",
            "review": "This paper proposes the use of unamortized Black Box Variational Inference for data imputation (given a fixed VAE with a factorized decoder), where the choice of variational distribution is a standard flow model. \n\nThe exploitation of the decoder factorization and the choice to set q(y | z) = p(y | z) was explored in the Bottleneck Conditional Density Estimation paper.\n\nTo my understanding, this paper fails to contextualize their work with the existing literature and is simply an exercise in the rote application of existing inference procedures to a well-established inference problem (data imputation). \n\nUnless the authors can convince me of the novelty of their approach or what I have overlooked in their proposal, I do not recommend this paper for acceptance.\n\nReferences:\nRanganath, et al. Black Box Variational Inference. AISTATS 2014.\nShu, et al. Bottleneck Conditional Density Estimation. ICML 2017.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting read, but little original contribution",
            "review": "Paper summary:\n\nGiven a pre-trained VAE (e.g. over images), this paper is about inferring the distribution over missing variables (e.g. given half the pixels, what is a plausible completion?). The paper describes an approach based on variational inference with normalizing flows: given observed variables, the posterior over the VAE's latents is inferred (variationally) and plausible completions for missing variables are sampled from the VAE decoder.\n\nTechnical quality:\n\nThe presented method is technically correct. The evaluation carefully compares different types of normalizing flow and HMC, and seems to follow good practices.\n\nI have a suggestion for improving the GVI method. The way it's described in the paper, GVI requires computing the determinant of a DxD matrix, which costs O(D^3), and there is no guarantee that the matrix is invertible. However, this approach over-parameterizes the covariance matrix of the modelled Gaussian. Without losing any flexibility, you can use a lower triangular matrix with strictly positive diagonal elements (e.g. the diagonal elements can be parameterized as the exp of unconstrained variables). That way, the determinant costs O(D) (it's just the product of diagonal elements) and you ensure that the matrix is invertible (because the determinant is strictly positive), without hurting expressivity. You can think of this as parameterizing the Cholesky decomposition of the covariance matrix.\n\nAlso, there are more flexible normalizing flows, such as Inverse Autoregressive Flow, that can be used instead of the planar flow used in the paper.\n\nClarity:\n\nThe paper is written clearly and in full detail, and the mathematical exposition is clear and precise.\n\nSome typos and minor suggestions for improvement:\n- It'd be good to move Alg. 1 and Fig. 1 near where they are first referenced.\n- Page 2: over to \\theta --> over \\theta\n- Eq. 3: p_\\theta appears twice in the middle.\n- one can use MCMC to attempt sampling --> one can use MCMC to sample\n- Eq. 5: should be q_\\psi as subscript of E.\n- Fig. 7, caption: should be GVI vs. NF.\n- In references, should be properly capitalized: Hamiltonian, Langevin, Monte Carlo, Bayes, BFGS\n- Lemma 1: joint divergence is equivalent to --> joint divergence is equal to\n- Lemma 1: in the chain rule for KL, the second KL term should be averaged w.r.t. its free variables.\n\nOriginality:\n\nIn my opinion, there is little original contribution in this paper. The inference method presented (variational inference with normalizing flows) is well-known and already in use. The paper applies this method to VAEs, which is a straightforward application of a well-known inference method to a relatively simple graphical model (z -> {x, y}, with x, y independent given z).\n\nI don't see the need for introducing a new term (cross-coder). According to the paper, a cross-coder is precisely a normalizing flow (i.e. an invertible smooth transformation of a simple density). I think new terms for already existing ideas add cognitive load to the community, and are better avoided.\n\nSignificance:\n\nIn my opinion, constructing generative models that can handle arbitrary patterns of missing data is an important research direction. However, this is not exactly what the paper is about: the paper is about inference in a given generative model. Given that there is (in my opinion) no new methodology in the paper, I wouldn't consider this paper a significant contribution.\n\nI would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant. Is it just for image completion / data imputation, or are there other practical problems? Is it important as part of another method / solution to another problem?\n\nReview summary:\n\nPros:\n- Technically correct, gives full detail.\n- Well and clearly written, precise with maths.\n- Evaluation section interesting to read.\n\nCons:\n- No original contribution.\n- Could do a better job motivating the importance of the problem.\n\nMinor points:\n- I don't completely agree with the way VAEs are described in sec. 2.1. As written, it follows that VAEs must have a Gaussian prior and a conditionally independent decoder. Although these are common choices in practice, they are not necessary: for example, one could take the prior to be a Masked Autoregressive Flow and the decoder a PixelCNN.\n- Same for observation 1. This is not an observation, but an assumption; that is, the paper assumes that the decoder is conditionally independent. This is of course an assumption that we can satisfy by design, but it's a design choice that restricts the decoder in a specific way.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}