{
    "Decision": {
        "metareview": "The paper presents an unsupervised visual abstraction model, used for reinforcement learning tasks. It is trained through intrinsic rewards, generated from temporal differences of inputs. This is similar to \"learning to control pixels\". The method is tested in DM Lab (3D environment, 2D navigation tasks) and Atari (Montezuma's Revenge).\n\nThe paper is at times hard to follow, and it seems the improvements accompanying the rebuttals did not convince reviewers to change their notes significantly. The experiments do not contain enough comparisons to other models, baselines, nor ablations, to sustain the claims.\n\nIn its current form, this is not acceptable for publication at ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting structured exploration idea, not clear nor detailed enough"
    },
    "Reviews": [
        {
            "title": "The paper is unfortunately written quite confusingly such that it is hard to evaluate the contribution of the potentially interesting ideas.",
            "review": "The appproach introduces visual abstractions that are used for reinforcement learning. The abstractions are learned using a lower bound on the mutual information and options are created to generate different measurements for each abstraction. The algorithm hence learns to \"control\" each abstraction as well as to select the options to achieve the overall task. The algorithm is tested on a 3D navigation task and a few Atari tasks which are known for difficult exploration.\n\nThe paper might contain some interesting ideas, however, I am quite confused about the paper due to lack of clarity in writing. The approach is not properly motivated, many equations are not really eplained and important information is missing, so it is really hard to evaluate the contribution of the approach. Please see below for more comments:\n- It is unclear how the intrinsic reward is defined (which is critical to understand the approach).\n- It is unclear what the M different measurements are or for what they are used for. \n- It is unclear qhy equation 1 defines a classification loss. Distribution q is not defined in Eq (1).\n- I do not understand the description of Q-meta in caption of Figure 2, \"Qmeta acts every T steps, which is the fixed temporal\ncommitment window, and outputs an action to select and execute either: (1) composition over Q\nfunction from the option bank indexed by a particular entity and an intrinsic reward function or (2)\nthe Qtask policy which outputs raw actions.\" How can an action be a composition over Q-function and a intrinisic reward function? Please clarify what Qmeta and Qtask do in the text right in the beginning. \n\nI have to say that the paper confused me too much that it is likely I missed the point of the paper. On the positive side, I think the learning of the abstractions using lower bounds of the mutual information is very interesting. The authors should work on their presentation and this could be a very nice paper.  \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A  Structured Exploration Algorithm with Visual Abstractions",
            "review": "This paper proposed an algorithm for structured exploration in deep reinforcement learning via learning the visual abstractions from pixels. The proposed method learns discrete visual abstractions and derives intrinsic reward functions from them so as to help the agent to optimize the policy. \n\nThe proposed method is interesting in that learning the visual abstractions together with the policy may assist in computing an optimal policy. The method is learning a meta Q function and (E * M+1) other Q functions. The authors mentioned that their work is most similar to hierarchical-DQN (Kulkarni et al., 2016) but this work required hand-crafted instance segmentation and the agent architecture do not learn about many intrinsic rewards learners. However, I am concerned if the proposed method solved the problem with the need of hand-crafted instance segmentation since, as shown in the Algorithm 1 and the caption of Figure 2, Q_{meta} acts every T steps. I do not understand why the meta Q function is used to propose actions for every fixed number of steps. Besides that, though the proposed method does have many intrinsic reward functions (in fact, there are E * M additional intrinsic reward functions). However, the authors did not show in the experiments if having too many intrinsic reward functions helps a lot. It will be better if the authors can show that, larger values for E or M can make the performances better.\n\nAnother concern I have is on some of the experiment results. For the experiment results in Figure 5 and 6, only in the left figures can the results of the proposed methods outperform the baselines. Besides that, the authors may need to describe the baseline methods in the experiments in more details.\n\nAlso, it will be better if the authors can improve the paper a little bit with the writing. For example, it will be better if the authors can explain the variables X, Y and the distribution q when mentioning Equation 1 so that it is easier to understand the paper. Also, there are some typos, such as the section reference on line 10 of Algorithm 1, the definition of the g function on the last line of page 3 (I guess the authors want to write \"{0...E}\" instead of \"{0, E}\") and the second sentence of the experiment section (at least I did not see the supplementary sections, but the authors mentioned that). It is better if these typos can be fixed. \n \n\nReferences:\n\nTejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pp. 3675â€“3683, 2016.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insufficient clarity",
            "review": "REVISION: thanks for the clarification. I have slightly increased my rating (to 4).\n\nThis paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.\n\nFirst, there are many sentences which are not well-formed or are ambiguous (in pretty much all the sections). Then there are terms which are introduced without being first clearly explained or defined. Finally, there are issues with the mathematical clarity as well, with many notations which are used without being explained or defined. Sometimes one can figure out the missing information later (e.g., fig 1 talks about mutual information objectives without stating if we want to maximize or minimize it, but later in the text we figure that out) but it makes reading very difficult.\n\nWhat is a 'transformed one' (on page 2)\nWhat is a 'geometric intrinsic reward'?\nWhere are the intrinsic rewards defined?\nWhat is a 'non-parametric classifier'? A neural net? an kernel SVM?\n\nThere are also some mathematical problems:\n- if f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)\n- what are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?\n- in eqn 2, I do not think that the log q_c term maximizes the mutual information between actions and (G(t),G(t+1)), i.e. it would be missing an entropy term\n- what is Z_c in eqn 2?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}