{
    "Decision": {
        "metareview": "This paper presents two extensions of Relation Networks (RNs) to represent a sentence as a set of relations between words: (1) dependency-based constraints to control the influence of different relations within a sentence and (2) recurrent extension of RNs to propagate information through the tree structure of relations.\n\nPros:\nThe notion of relation networks for sentence representation is potentially interesting.\n\nCons:\nThe significance of the proposed methods compared to existing variants of TreeRNNs is not clear (R1). R1 requested empirical comparisons against TreeRNNs (since the proposed methods are also of tree shape), but the authors argued back that such experiments are necessary beyond BiLSTM baselines.\n\nVerdict:\nReject. The proposed methods build on relatively incremental ideas and the empirical results are rather inconclusive.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Relatively incremental ideas with inconclusive empirical results"
    },
    "Reviews": [
        {
            "title": "motivation is confusing, needs to be better situated relative to related work",
            "review": "The paper presents an extension of relation networks (RNs) for natural language processing. RNs are designed to represent a set as a function of the representations of their elements. This paper treats a sentence as a set of words. Whereas regular RNs assume that the representation of a set is a uniform aggregation of the representation of the pairs of elements in the set, this paper proposes to weight the relevance of the pairs according to their tree-structured dependency relations between the words. The authors evaluate on a suite of NLP tasks, including SNLI, Quora duplicate question ID, and machine translation. They show marginal improvements over naive baselines, and no improvement over SOTA.\n\nI am concerned about both the motivation for and the novelty of this work. My reading of this work is that the authors try to reverse engineer a TreeRNN in terms of RNs, but I am not sure what the reason is for wanting to use the RN framework in order to derive an architecture that, IIUC, essentially already exists. I can't find any fundamentally meaningful differences between the proposed architecture and the existing work on TreeRNNs, and the results suggest that there is nothing groundbreaking being proposed here. It is possible I am missing some key insight, but I do believe the burden is on the authors to highlight where the novelty is. The intro *and* related work sections should both be rewritten to answer the question: what is the insufficiency with current sentence encoding models that is addressed by this architecture? Currently, the intro addresses the tangential question: what is the insufficiency with RNs for NLP that is addressed by this architecture? If the latter is the question the authors want to answer, they need to first answer: why should we want to cast sentence encoders as RNs as opposed to any of the (many) other available architectures? Without a firmer understanding of what this paper contributes and why, I can't recommend acceptance. More detailed comments for the authors below. \n\n- You introduce a few naive baselines, but none of these is a TreeRNN. TreeRNNs are the obvious baseline, and you should be comparing on each and every evaluation task, even if there is no previously published result for using tree RNNs on that task. For the one result (SNLI, table 1) on which there is previous work using TreeRNNs, the table confirms my intuition that the proposed model is no improvement over the TreeRNN architecture. It seems very important to address this comparison across all of the evaluation tasks.\n- I like the notion of marginalizing over latent tree structures, but the related work section needs to make clear what is being contributed here that is different from the cited past work on this problem\n- On the MT eval, why are you missing values for zh-en on the NMT models that are actually competitive? I think many of these models are open-source or easy to reimplement? Its hard to draw conclusions when from such a gappy table.\n- Only table 2 has significance values (over naive baseline that is) which implies that the other results are not significant? That is disconcerting. \n- I am disappointed in the analysis section. As is, you provide an ad-hoc inspection of some inferred trees. I find this odd since there is no evidence that the tree-ness of the architecture (as opposed to, e.g., recurrence or attention) is what leads to quantitative improvements (at least according to the experimental results in the tables), so there is no reason we should actually expect the trees to be good or interesting. My interpretation of these cherry-picked examples is that the learning is fighting the architecture a bit, basically \"learning a tree\" that reduces to being an attention mechanism that up-weights one or two salient words. \n- The analysis I *wanted* to see instead was why recursion helped for sentence classification, it did not for MT. You give an intuition for this result but no evidence. (That is assuming that, quantitatively, this trend actually holds. Which maybe is not the case if none of the results are significant.)\n- In general, regarding evaluation, SNLI is overfit. You should use MNLI at least. I have trouble interpreting progress on SNLI as \"actual\" progress on language representation.\n- The related work section as a whole is too short. If you need to cut space, move technical content to appendix, but don't compromise in related work. You listed many relevant citations, but you have no context to situate your contribution relative to this past work. What is the same/different about your method? You should provide an answer to that for each and every paper you cite. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The actual effectiveness of the proposed method is unclear",
            "review": "\n[Summary]\nThe main purpose of this paper is to propose an extension of relation networks.\nThe proposal consists of two parts: 1) to integrate constraints of dependency syntax to control which relations influence the representation, and 2) utilize a recurrent computation to capture higher-order relations.\n\n[clarity]\nThis paper is basically well written.\nMotivation and goal are clear.\n\n[originality]\nThe idea of utilizing supervised or unsupervised dependency tree as constraints to control which relations influence the representation seems novel and interesting.\nHowever, technically it consists of the combination of the previous methods, such as matrix-tree theorem for calculating conditional probabilities, and structured attention.\nTherefore, the proposed method is incremental rather than innovative.\n\n[significance]\nExperiments on several varieties of datasets revealed that the proposed method consistently improved the performance from the baseline RN.\nIn contrast, it did not outperform the current best scores for all experiments comparing with the current published best methods.\nObviously, we have no reason that we must use RNs for such tasks.\nTherefore, the actual effectiveness of the proposed method in terms of the actual task settings is unclear for me.\nI concern about the actual calculation speed of the proposed method.\nThe proposed method seems to require much higher computational cost against the baseline RNs.\n\n[Questions]\n1, Regarding the approach in general, it would be nice to see how much it depends on the quality of the dependency parse. \nFor example, we cannot always prepare a good parser for experiments on MT such as low-resource languages.\nDo you have any comments for this?\n\n2, Some experimental results showed that “RN intra-attn” was better than “Reccurent RNs”.\nThis implies for me that the higher-order dependency is useless for such tasks.\nAre there any analyses why “Reccurent RNs” did not work well?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper presents a competitive baseline method for generic sentence representation learning. ",
            "review": "The main idea is to incorporate linguistic-based constrains in the form of dependency trees into different variations of relation networks.\nIn general, the paper is well written and organized, the presented problem is well motivated, and the approach is very strait forward. The experimental setting is comprehensive, and the results are indeed competitive in a wide range of tasks.\nI think that using linguistic knowledge to improve Neural networks performance is very promising field, I think that you could get a much more substantial gains when applying your method in less resource-rich setups (maybe using some small subset of training for the SNLI and question duplication datasets).\nIt seems that your method relies heavily on previous works (RN, RNN-RN, latent dependency trees ,intra-sentence attention), can you please state clearly what your contribution is? does your model has any advantages over current state-of-the-art methods?   \n\nedit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}