{
    "Decision": {
        "metareview": "This paper introduces a recurrent neural network approach for learning diffusion dynamics in networks. The main advantage is that it embeds the history of diffusion and incorporates the structure of independent cascades for diffusion modeling and prediction. This is an important problem, and the proposed approach is novel and provides some empirical improvements. \nHowever, there is a lack of theoretical analysis, and in particular modeling choices and consequences of these choices should be emphasized more clearly. While there wasn't a consensus, a majority of the reviewers believe the paper is not ready for publication.\n\n",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Interesting idea, but paper not ready for publication"
    },
    "Reviews": [
        {
            "title": "This paper proposes a neural network architectures with locally dense and globally sparse connections. Using dense units a population-based evolutionary algorithm is used to find the sparse connections between modules.",
            "review": "The problem that the paper tackles is very important and the approach to tackle it id appealing. The idea of regarding the history as a tree looks very promising. However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events. \n\nUsing neural network if an interesting choice for capturing the influence probability and its timing.\n\nThe authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks? \nThe experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works. This could have made the paper much stronger.\n\nIt was nice that the paper iterated and reviewed the possible inference and learning ways. There is one more way. Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.\n\nThe paper can benefit from a proofreading. There are a few typos throughout the paper such as:\nReference is missing in section 2.1\nPage 2 paragraph 1: “an neural attention mechanism”\n\n[1] Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades, AISTATS 2015",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Poor presentation",
            "review": "The authors of this paper are proposing a neural network approach for learning diffusion dynamics in networks. The authors argue that the main advantage of their framework is that it incorporates the structure of independent cascades into the model which predicts the diffusion process.\n\nThe primary difficulty in reviewing this paper is the poor presentation of the paper. There are many typos and mistakes (e.g., the last paragraph of the paper starts with a sentence that does not make any sense), missing references (e.g., there is an empty parenthesis at the end of the second paragraph on the second page) and in at least two cases, there are references to a formula that is not in the manuscript (e.g., reference to formula 15 on line 3 of page 5). This issues makes reviewing this paper very difficult.\n\nIn the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, where p(I|D) is the conditional probability that a particular node infected an observed infected node first. Plugging p(I|D) in Eq. 12 and using decomposition of p(D ,I) used in Eq. 10, we arrive at a formulation which drops all p(I|D) terms. This results in an objective function which only involves infected nodes (and no term associated with the parent node), weighted by likelihood of each node j infecting the node at step i. This should make the training more simplified than what is discussed in Algorithm 2. Beyond this simplification, I am not clear if that is actually intended by the authors.\n\nThe experiments demonstrate a superior performance of the proposed model compared to alternative benchmarks. The authors explain how they trained their own model but there is no mention on how they trained benchmark models. However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.\n\nDue to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental, writing could be much improved",
            "review": "The paper proposes a generative infection cascade model based on latent vector representations. The main idea is to use all possible paths of infections in the model. \n\nComments:\n- The papers clarity could be much improved. It is not easy to follow, is overflowing with notation, and lengthy. Sec. 2.1 for example can easily be made much more concise. Secs. 3.1 and 3.2 are especially confusing. In the first equation in Sec. 3, what is \\phi with and without sub/superscript? In Eq. (2), what is k - a probability, or an index? And what is the formal definition \"infection\" and \"future\" in the description of k stating that it is \"the probability that u infects v in the future\"?\n\n- The authors mention that the actual infectors in a diffusion process are rarely observed. While this might be true, in many types of data include infection attempts. This should be worthwhile to model - there are many works on reconstructing cascades from partial data.\n\n- The authors note (rightly) the Eq. (9) is hard to solve, and propose a simple lower bound based on (what I think is) a decomposition assumption.  Unless I misunderstood, this undermines the contribution of the structure of past infections. Could the authors please clarify?\n\n- The results mention 5 (tables?), but only 4 are available, of which one appears floating on the last page.\n\n- Why are methods discussed in the introduction (e.g., DeepCas, Wang 2017a,b 2018) not used as baselines?\n\nMinor:\n- Wang 2017a and Wang 2017b are not the same Wang\n- Several occurrences of empty parentheses - ()\n- ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}