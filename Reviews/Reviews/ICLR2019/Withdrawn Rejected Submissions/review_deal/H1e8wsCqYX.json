{
    "Decision": {
        "metareview": "The paper proposes a new graph-based regularizer to improve the robustness of deep nets. The idea is to encourage smoothness on a graph built on the features at different layers. Experiments on CIFAR-10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization. The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization. Despite the revision, the reviewers maintain their concerns. For this reason this work is not ready for publication.",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Meta-review"
    },
    "Reviews": [
        {
            "title": "Graph-regularized NNs",
            "review": "This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. The regularisation enforces smoothness on a graph built on the different features at different layers of the NN system. The proposed ideas are quite interesting, and integrates nicely into NN architectures. \n\nA few paths for improvements:\n\n- the 'optimal' choice of the power of the Laplacian, in 3.5, is eluded\n- the figures are not presented ideally, nor in a very readable form - for example, their are 90-degree rotated compared to classical presentations, and the plots are hardly readable\n- the might exist a tradeoff between robustness, and performance (accuracy), that seem to be explaining the proposed results (see Fawzi - Machine Learning 2018, for example)\n- in 4.2, what is a mean case of adversarial noise? Also, it would be good to see the effect of the regularizer of both the 'original' network, and on the network trained with data augmentation. It is not clear which one is considered here, but it would be interesting to study both, actually. \n- the second paragraph of the conclusion (transfer of perturbations) opens interesting perspective, but the problem might not be as trivial as the authors seem to hint in the text. \n\nOverall, very interesting and nice work, which might be better positioned (especially in terms of experiments) wrt to other recent methods that propose to improve robustness in NNs.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting idea but the significance of the experimental results is unclear and the motivation should better match the evaluation ",
            "review": "The paper proposes to use a regularization which preserves nearest-neighbor smoothness from layer to layer. The approach is based on controlling the extent to which examples from different classes are separated from one layer to the next, in deep neural networks. The criterion computes the smoothness of the label vectors (one-hot encodings of class labels) along the nearest-neighbor graph constructed from the euclidian distances on a given layer's activations. From an algorithmic perspective, the regularization is applied by considering distances graphs on minibatches. Experiments on CIFAR-10 show that the method improves the robustness of the neural networks to different types of perturbations (perturbations of the input, aka adversarial examples, and quantization of the network weights/dropout0.\n\nThe main contribution of the article is to apply concepts of graph regularization to the robustness of neural networks. The experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.\n\nWhile the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear. There does not seem to be any proof that adersarial examples are supposed to be classified better by keeping the smoothness of class indicators similar from layer to layer. Section 3.4 seem to motivate the use of the smoothness from the perspective of preventing overfitting. However, I'm not sure how adversarial examples and the other forms of perturbations considered in the experiments (e.g., weight quantization) are related to overfitting.\n\nstrengths:\n- practical proposal to use graph regularization for neural network regularization\n- the proposal to construct graphs based on the current batch makes sense from an algorithmic point of view\n\n\ncons: experimental results are a bit weak -- the most significant results seem to be obtained for \"implementation robustness\", but it is unclear why the proposed approach should be particularly good for this setting since the theoretical motivation is to prevent overfitting. The results vs Parseval regularization and the indications that the metohd works well with Parseval regularization is a plus, but the differences on adversarial examples are tiny.\n\nother questions/comments:\n- how much is lost by constructing subgraphs on minibatches only?\n- are there experiments (e.g., on smaller datasets) that would show that the proposed method indeed regularizes and prevents overfitting as motivated in Section 3.4?\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Concerns in its significance",
            "review": "To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network.\n\nThe paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness.\n\nHowever, my main concern about the paper is still about its significance. \n1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, “lower value of \\sigma^\\ell(s) are indicative of better separation between classes”, what is the reason of not directly penalizing this value, rather than requesting a “stability” property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper.\n2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of “Proposed + Parseval”. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings.\n\nOther comments:\n1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. \n2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}