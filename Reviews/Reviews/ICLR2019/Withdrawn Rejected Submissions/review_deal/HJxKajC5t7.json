{
    "Decision": "",
    "Reviews": [
        {
            "title": "Self-Binarizing Networks",
            "review": "This is a good paper on an important topic of low precision representation in neural network weights and signal representations. As we move more and more into making neural networks deployed in low power applications such as mobile and wearable devices, this topic is of increasing importance. This is an area explored by other researchers -- and the manuscript does a good job of surveying related work. The novel contribution in the paper is to hide neural network weights and activation at nodes behind use hyperbolic tangent functions, thereby working in a continuous space in which differentiation is straightforward. The sharpness of hyperbolic tangent is gradually increased (in an ad-hoc way every epoch), so that gradually everything become binary. A clearly written paper. However, the empirical work reported is somewhat weak: (a) there is no uncertainty provided in the results of Table 1. When the differences in performance quoted are so small, it is important to give uncertainties (by cross validation) so that a reader can judge the significance of the results; (b) very little effort is made to carry out an analysis of errors; where do the gains come from? If, for example, one looks at the confusion matrix of the ten-class problem, can one identify where the differences are -- are they random or is there anything systematic one can pull out? Lack of error analysis is particularly striking when one notes that in both the CIFAR10 and CIFAR100 problems, there are instances where the binary low precision method actually outperforms the full precision method. How is this so? (c) following on from this, what does the paper bring to this particular conference on \"learning representations\"? Is there anything we can pull out from the work of representing features in a binary space that is specific to the problems considered? \nIn conclusion, the paper has a novel idea I like, it is explained clearly, but the work has to mature a bit more in terms of  empirical work and interpretation of results.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough contribution and not very clear experiments",
            "review": "This paper presnets a different way to train a binary network. Instead of using the sign function for the forward pass and the straight trough approximation for the backward pass, as in previous work, it uses continuation, i.e. weights and activations are converted to the range [-1,1] with a hyperbolic tangent, with a multiplicative factor that initially is low, and then though iterations becomes larger and forces the output of htan to be close to the binary values. The paper also uses a different way to compute batch normalization (BN), which manages to use 8 bits fixed point instead of 32 bits floating point. Results are shown on CIFAR 10, 100 and ImageNet, and compared to previous approaches.\n\nPros:\n- The paper is well written\n- The idea of fully avoiding float32 is intriguing, but it would be much more interesting if it can be done also at training time.\n\nCons:\n- Reading the introduction gives a high expectation about a method that can actually learn directly on binary values. Then, in practise, the real contribution of the paper is just a different binarization of the network and a different binary BN.\n- The first contribution of the paper is to use continuation instead of straight trough for using back-propagation. To me it makes a lot of sense, but in the experimental part I could not clearly see if the improvement in performance is due to this representation of the binarized BN.\n- The second contribution of the paper is on batch nomralization. The authors show another way to compute jointly the network activation and batch normalization using a 8 bit fixed point instead of a 32 bit floating point. This can help on practical implementations to use the network on machines that do not have floating point operations. However, I do not see this contribution enough for a ICLR paper.\n- The presentation of the results seems biased. In tab. 1, the authors show that their approach requires only 1 bit for the weights and 1 bit for their binary BN. However, they also use a 8 bit fixed point for BN, but it is not shown. \n- In Fig. 3, the improvement in computational cost and memory is only for the binarized BN. Globally I think that this is not affecting much the entire network speed and memory. Can the authors say more about that? \n\nGlobal Evaluation:\nI thinks that a representation of a binary net based on a continuation approach is quite interesting. However, in the experimental results the authors did not really clarify if the obtained improvements are really due to this part or not. More experiments and an ablation study is necessary. The second contribution about binarized BN does not seem very important to me. It can reduce the BN computational cost and memory consumption, but globally,in terms of the full network it does not change much.\n\n\nAdditional comments:\n- Fig. 1 is quite confusing. There are many bars and it is not clear what they represent in the network.\n- I do not think it is necessary to describe all the method for reducing networks computation in related work\n- in related work: \"which requires only one bit to represent\" needs an object or a passive form.\n- missing the very relevant reference to: \"How to Train a Compact Binary Neural Network with High Accuracy?\" Wei Tang, Gang Hua, Liang Wang.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple but not vey novel bianrization method for weights and activations",
            "review": "The authors propose to use a scaled hyperbolic tangent function (soft quantization function) to mimic the hard quantization function, and gradually sharpen it during the training process to diminish the gap between the soft and hard binarization functions. The authors also propose a more efficient batch normalization method.\n\nThis method is simple yet general as the soft quantization function can be used for both weights and activations, which is a desirable property. One major concern is that the proposed continuous relaxation training trick is previously studied and  used in  applications like hashing and this may not be treated as an inspiring technical contribution.  Moreover, some more recent state-of-the-art approaches achieve better results (e.g., [1]) than XNOR-NET, and discussions on these methods may further improve the proposed method. Yet another concern is that the accuracy results  (top1-37.84/top5-64.06) for XNOR-NET (Bw=1, Ba=1, Bbn=32) in this paper is much lower that the reported (top1-44.2/top5-69.2) in the original XNOR-NET paper. Even the results of the proposed method is much lower than the original XNOR-NET results. This may raise concern on the efficacy of the proposed method whether the proposed method is good or not. Can the authors use the same setting as the original XNOR-NET paper and do the comparison?\n\n[1]. Towards accurate binary convolutional neural network, NIPS2017. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}