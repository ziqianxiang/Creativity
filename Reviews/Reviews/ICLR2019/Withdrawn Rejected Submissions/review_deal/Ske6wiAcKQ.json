{
    "Decision": {
        "metareview": "All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Weak baseline comparisons and insufficient comparison with prior work",
            "review": "This paper describes a search space reduction method for neural network based keyboard input methods. The paper discusses two different sampling methods to restrict the vocabulary size during beam search.  \n\nTitle: The title of the paper is too generic to describe what is actually being done in the paper. Input methods in mobile devices could have also meant speech based input or handwriting based input or swipe based input. It would be very convenient for the readers if the authors use more specific wording in the title to clarify that they are talking about neural network based keyboard typing input.\n\nComparison with prior work: Neural network based on-device keyboard input is a research topic with a lot of previous contributions and the existing literature survey seems lacking. Further it does not even cover popular techniques for inference speed-up like hierarchical softmax computation. It would be easier for the reader to appreciate the  contributions of this paper if the authors compare and contrast with more relevant prior work.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "insufficient novelty, missing competitive baselines",
            "review": "The paper demonstrates the main challenge of using LSTM-based language models for input method in real time is the huge amount of computation in the softmax. The authors present a system to speed up the inference by avoiding computing the full softmax in the Japanese conversion task, where the number of output words can be limited from the mapping of the input sequence through a lexicon. The experiment result is encouraging in that the proposed incremental selective softmax approach significantly reduces latency over the standard inference with the full softmax computation while not hurting accuracy much. The paper also evaluates the effect of quantization for LSTM LM model compression in terms of size and accuracy.\n\nHowever, there are a few major problems of the paper as follows:\n\n1. The main weakness in the experiment setup is that it misses a few competitive baselines in terms of inference speed, notably hierarchical softmax[1] and self normalization[2]. In the Japanese conversion task in the paper, it only needs to evaluate the scores of limited output words that are given from the mapping of the input sequence through the lexicon. This is exactly like the rescoring setup in speech recognition and machine translation, where self-normalization is typically used for efficient inference to avoid computing the expensive softmax normalization term [2,3]. Assuming the number of selected output words is K and the entire vocabulary size is V, then the time complexity is O(K logV) for the hierarchical softmax, O(K) for self normalization, but O(V) for all the baselines in the paper. Self normalization is simple to implement and works well in practice, while the proposed incremental selective softmax approach in the paper needs an additional step to sample most frequent words to adjust the normalization term. Without showing the self normalization result, I am not convinced that the proposed approach is better and needed.\n\n[1] F. Morin and Y. Bengio. \"Hierarchical Probabilistic Neural Network Language Model,\" in Proc. of AISTATS, 2005,\n[2] J. Devlin et al., \"Fast and Robust Neural Network Joint Models for Statistical Machine Translation,\" in Proc. of ACL, 2014.\n[3] Y. Shi, W. Zhang, M. Cai and J. Liu, \"VARIANCE REGULARIZATION OF RNNLM FOR SPEECH RECOGNITION,\" in Proc. of ICASSP, 2014.\n\n2. The proposed approach would only be useful in speeding up the conversion task, but not applicable to the prediction task where it needs to evaluate all words and choose the top hypotheses. Also how is the latency of the prediction task compared to conversion task? Please also add it to the experiment result.\n\n3. The idea of using quantization for neural network model compression is not novel (even for language model), although it is listed as one of the main contributions in Section 1.\n\nSo in general, I think the paper is insufficient in novelty and missing competitive baselines.\n\nSome specific comments:\n4. Figure 2(b) is not clear what it means, and not referenced anywhere in the paper.\n5. The last 3 lines in Section 3: \"as each path has different missing vocabularies\": Why is that? The candidates of the output words should only depend on the input sequence and the lexicon, based on Eq(1)(2).\n6. It is not clear how to adjust the probability in the second pass of incremental selective softmax. The description \"we compute a union of all missing vocabularies, and then recompute the logits of them in batch.\" is unclear what it means.\n7. Section 4.2: \"is measure with numpy\" -> \"is measured with numpy\".\n8. Section 4.4: It is not clean how \"76x speedup\" is computed from Table 2 since all the time numbers are rounded. Consider also showing one digit after the decimal point.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not enough novelty or significance, unsatisfactory experimental evaluations.",
            "review": "Summary: Authors proposed a model for input method for mobile or desktop devices. The goal is to convert the input sequence (from one language to another) or predict the next word. Their model is based on an LSTM with modified softmax activation function that is adjustable for large vocabulary sizes. They showed experimental results on Japanese BCCWJ data set.\n\nClarity: Paper is well-written and well-organized. Notions and methods are clearly expressed. \n\nOriginality: This paper builds on an LSTM model without enough work or idea to show novelty. \n\nSignificance: It is below average. Using LSTM is a well-known method for these types of tasks in the literature. Incremental selective softmax is potentially a good approach, however, this work lacks showing significant improvement. The experiments are limited and are done only on one data set.\n\nMore detailed comments:\n\n- My concerns about this work are both on modeling aspects and experiments. Authors mainly focus on highlighting the benefits comparing to n-gram models, and briefly discuss the ongoing developments in neural based models. For example sequential modelings using RNN's have shown promising results in capturing long-term dependencies [1]. Unfortunately authors did not include any discussion on how their approach would compare to that framework nor did they present any experimental comparisons to them.\n\n- Although mentioned briefly in the introduction and related work sections, no analytical or experimental comparisons are made to machine translation approaches when their work is closely related to it. I strongly suggest that authors compare their experimental results to some of benchmarks in neural based machine translation discussed in the related works.\n\n- In the incremental selection softmax, they use \"match\" to return all lexicon items matching the partial sequence. How is this done and what are the effects of it on the computational time of the algorithm? Also, It is not clear how authors correct old probabilities in IS softmax step. As mentioned, they add logits of missing vocabulary to the denominators, how do they keep the properties of softmax so that it sums up to 1? And later in the discussion authors mentioned that in practice they compute union of all missing vocabularies, it is not clear how this is done since the advantage of using IS softmax is expressed to be incremental increasing. \n\n[1] A.B. Dieng, C. Wang, J. Gao and J. Paisley. TopicRNN: A recurrent neural network with long-range semantic dependency, International Conference on Learning Representations (ICLR), 2017.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}