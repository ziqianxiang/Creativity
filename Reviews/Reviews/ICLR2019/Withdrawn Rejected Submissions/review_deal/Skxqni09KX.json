{
    "Decision": "",
    "Reviews": [
        {
            "title": "2 time scales stochastic approximation for approximately minimizing the Bellman residual",
            "review": "The paper analyzes a 2 time scales stochastic approximation algorithm for approximately solving a minmax formulation of a Bellman residual minimization problem. The minmax approach is used as a way to avoid the bias introduced when directly trying to minimize the squared norm of the Bellman residual from transition samples. As acknowledged by the authors this formulation is not new and has been described in [Dai et al. 2017] and can be traced back to [Antos et al. 2008]. Now the way to solve it using a 2 timescale SA constitutes the main contribution of the paper. \n\nHowever, I am surprised by the requirement of Assumption 4.2 (that the class of functions under which we project the Bellman residual is linear). Indeed, under this assumption, the inner maximization problem could be solved by least squares instead of gradient ascent, which could be much faster. So my question is why do you think a 2 time scales SA algorithm is better than a single time scale SA using as inner loop the LS solution? \n\nI would think the main merit of the 2 scales SA algo would be to get rid of this assumption of linear function space for the inner loop (in order to reduce the bias as much as possible by optimizing over a larger class of functions), so I find that the need for Assumption 4.2 reduces considerably the motivation for a 2 time scales algo, which is your main contribution.\n\nOther comments:\n- You should provide arguments for why minimizing the Bellman residual (eq. (3.1)) is a good things for approximating the optimal value function or the optimal policy. This is not clear, specially in the control case when the behavior policy is fixed.\n- Minimizing (3.10) (using approximate function spaces) may not give you a solution close to that of the (3.5) problem. It would be interesting to analyse how far the solution to those two problems are as a function of the capacity of the spaces.\n- Also the local minimum reached by the algorithm may be far away from the solution to (3.10). So in the end, it's not clear if we can say anything about the Bellman residual of the solution found by the algorithm.\n- It’s not clear how to implement the projection onto a compact set of parameters (in Algorithm 1), specially using neural nets.\n- Finally the experiments are not very convincing. The proposed algorithm does not seem to do better than a 5 years old DQN algorithm… I know this was not the main contribution of the paper, which is theoretical, but then it’s not clear to see the added value of the experiments.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "promising approach, but the problem formulation has some flaws",
            "review": "The paper studies the behavior of two-timescale stochastic approximation methods in the two fundamental approximate dynamic programming problems of policy evaluation and optimal control. Specifically, the authors pose the problem of Bellman residual minimization as a saddle-point problem and apply stochastic gradient updates for the primal and dual variables, and show that this procedure converges to a certain fixed point under rather general conditions. Notably, the results are claimed to hold even when the objective function is not convex-concave. The authors also highlight that the eventual solution may not necessarily be sensible in general, and also analyze a variant in which the dual variables can be optimized with bounded error in each iteration. In this setup, the main result is showing that the proposed algorithm converges to a neighborhood of the optimal solution of the original problem, and the size of the neighborhood is controlled by the optimization error in the dual objective.\n\nI enjoyed reading the paper for the most part: the proposed approach is simple and natural (closely resembling GTD algorithms), and the core technical results seem plausible (although I didn't have the capacity to check the details this time). I appreciated the openness of the authors to discuss the limitations of their result in Theorem 4.3. The writing is generally good, up to some minor typos.\n\nThat said, I do have some relatively serious concerns about the nature of some of the results. Specifically, the objective function considered for optimal control seems fundamentally flawed and can be shown to lead to meaningless results even when optimized to zero error. To see this, observe that the objective (3.1) involves an expectation taken under the behavior policy \\pi_b which may be arbitrarily far away from the optimal policy. This can lead to easily-seen problems in extreme cases, e.g., when pi_b only visits a recurrent class of states that are not visited at all by the optimal policy, and all rewards are zero in this recurrent class and its neighboring states. Under these conditions, the constant zero function trivially minimizes the Bellman error, even though being a potentially terrible solution. This is a well-known issue with this objective, and it can be solved by making some stronger assumptions on the behavior policy and using importance weighting (as done by, e.g., Antos et al. 2008). Incorporating these elements in the present analysis seems nontrivial, although certainly not impossible.\n\nAnother concern is that the optimization oracle required for Theorem 4.5 is unrealistically strong, and not just due to computational issues. The problem is that the optimization target for the oracle considered in this theorem is not an observable quantity, but rather an expectation over random variables with unknown distribution (that can only be estimated with an infinite sample size). Making such an assumption effectively eliminates the main technical challenges related to the double sampling issue, so it feels a bit like cheating. In fact, one can use such an assumption to completely circumvent the need for a saddle-point formulation and prove convergence results for Q-learning-style methods directly in a single-timescale SA framework. Right now, this aspect remains hidden due to the writing---the authors should discuss this assumption more honestly in the paper.\n\nIn order to resolve the above issues, I suggest that the authors limit their presentation to policy evaluation problems where at least some of these technical problems are not present. I believe that a paper focusing only on policy evaluation should have sufficient merit for publication---but then again, rewriting the paper accordingly should be a bit too much to ask for in a conference revision. In either case, I am looking forward to hearing the opinion of the authors on the issues raised above.\n\nDetailed comments\n=================\n- pp.1, intro: \"the statistical and computational properties of the RL algorithms are well-understood under these settings\"---well, let's just say that *some* of these properties are well understood. E.g., I don't think that many people would consider \"concentrability coefficients\" to be really well-understood :)\n- pp.2, top: \"Bellman residue\" -> \"Bellman residual\" (many times in the paper)\n- pp.2, related work: \"our study\" -> \"we study\"\n- pp.2, bottom: \"could stringent\" -> \"could be stringent\"\n- pp.3, below (2.2): \"applying the Bellman optimality operator repeatedly gives the classical Q-learning algorithm\"---this is actually value iteration, Q-learning also uses other ideas related to stochastic approximation. Please clarify a bit.\n- pp.3, above Sec 3: \"saddle framework\" -> \"saddle-point framework\"\n- pp.4, 2nd paragraph: \"when the capacity of S is large\"---what does this mean?\n- pp.4, Eq.(3.3): extra \\cdot\n- pp.5, below (3.7): \"since the saddle point problem is nonconvex, we project the iterates\"---can you explain what this projection has to do with nonconvexity?\n- pp.5, below (3.7): \"such a condition on the learning rates ensures that the dual iterates asymptotically track the sequence [...]\"---this is technically only true if the faster-timescale can be first shown to converge at all, which is not trivial!\n- pp.6, Assumption 4.1: missing comma between Q_max and \\|\\nabla_\\theta...\\|. It would also be worth noting that this assumption technically excludes neural nets with RELU activations, so I'm not sure if your theory actually applies to the setup in the experiments.\n- pp.6, below (4.1): \"most online TD-learning algorithms uses [sic] linear function approximation\"---this is a quite weak argument given that the main contribution of the paper is precisely going beyond these settings; this claim suggests that linear FA methods are OK after all.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solving the Saddle-Point objective of Dai et al. by two-timescale SA",
            "review": "The contribution of this paper is to solve the optimization problem from the problem \"Smoothed dual embedding control\" by Dai et al. (2017) using two-timescale stochastic approximation. The inner \"max\" problem is optimized by stochastic gradient ascent at a higher learning rate, in parallel with the outer \"min\" so that the inner problem appears quasi-stationary to the outer one. In my understanding, the analysis follows the usual template per Borkar or Konda in actor-critic methods and is not new. The proposed method is demonstrated in Cartpole and Mountain car and against a DQN baseline.  It is not clear why the author chose the heavy-duty DQN as a baseline in such simple environments when they could have compared directly against Dai et al. (2017) using the same architecture. While the idea of using two-timescale stochastic approximation to solve the saddle point formulation of Dai & al. (2017) is sound, the contribution of this paper is insufficient for the main conference.  The saddle-point formulation in the title really comes from Dai: the title should rather include \"two-timescale\" somewhere because this is main contribution.\n\nI would suggest enriching the experiments section by including an empirical study of the effect of the ratio of the learning rates on the stability and convergence rate. You should also really compare against Dai et al. (2017) and highlight why your method would be preferable (potentially computationally cheaper and more scalable than other alternatives). \n\n# Typos \n\n> an stochasitc \n> the iterates onto [the] compact sets\n> in (3.5) for [a] fixed θ\n> in different [s]paces",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}