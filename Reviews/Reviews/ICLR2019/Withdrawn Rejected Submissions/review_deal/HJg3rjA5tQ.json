{
    "Decision": {
        "metareview": "This manuscript proposes spread divergences as a technique for extending f-divergences to distributions with different supports. This is achieved by convolving with a noise distribution. This is an important topic worth further study in the community, particularly as it related to training generative models.\n\nThe reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work, or expressing issues about the clarity of the presentation. Further improvement of the clarity, combined with additional convincing experiments would significantly strengthen this submission.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "Interesting idea but the paper needs work",
            "review": "This paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue.  The writing needs quite a bit oof polish for the motivations to clearly stand out. Also, some of the notation makes things way more confusing that it should be. Is it possible to use something other than p() for the noise distribution, since the problem itself is to distinguish between p() and q(). I understand the notational overload, but it complicates the reading unnecessarily. I have the following questions if the authors could please address:\n\n1) The inequality of Zhang et a. (2018) that this paper uses seems to be an easy corollary of the Data Processing Inequality :https://en.wikipedia.org/wiki/Data_processing_inequality Did I miss something? Can the authors specify if that is not the case?\n\n2) In terms of relevance to ICLR, the applications of PCA, ICA and training of NNs is clearly important. There seems to be a significant overlap of Sec 5.3 with Zhang et al. Could the authors specify what the differences are in terms of training methodology vis-a-vis Zhang et al? It seems to me these are parallel submissions with this submissions focussing more on properties of Spread Divergences and its non deep learning applications, while the training of NNs and more empirical evidence is moved to Zhang et al. \n\n3) I am having a tough time understanding the derivation of Eq 25, it seems some steps were skipped. Can the authors please update the draft with more detail in the main text or appendix ?\n\n4) Based on the results on PCA and ICA, I am wondering if the introduction of the spread is in some ways equivalent to assuming some sort of prior. In the PCA case, as an exercise to understand better, what happens if some other noise distribution is used ? \n\n5) I do not follow the purpose of including the discussion on Fourier transforms. In general sec 3 seems to be hastily written. Similarly, what is sec 3.2's goal ? \n\n6) The authors mention the analog to MMD for the condition \\hat{D}(p,q)=0  \\implies p =q. From sec 4, for the case of mercer spread divergence, it seems like the idea is that  \"the eigenmaps of the embedding should match on the transformed domain\" ? What is [a,b] exactly in context of the original problem? This is my main issue with this paper. They talk about the result without motivation/discussion to put things into context of the overall flow, making it harder than it should be for the reader. I have no doubt to the novelty, but the writing could definitely be improved.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A narrow focused paper for an interesting idea, forgetting the SOTA for the formal part, whose shape could be much improved",
            "review": "\nPros:\n\n- interesting idea.\n\nCons:\n\n- the paper forgets the state of the art for comparisons (optimal transport, data processing inequalities)\n- the paper formally shows little as most results are in fact buried in the text and it is hard to tell the formal from the informal.\n- experiments fall short of really using the setting proposed\n- the paper focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties (including downsides, such as variance increase)\n\nDetail:\n\n* The paper claims to propose a \"theory\" for spread divergences (conditioning a f-divergence by a \"third-party\" conditional distribution on supports which makes supports match) still keeping the identity of indiscernibles. \n\n* The paper recycles the notion of Spread f-divergences from Zhang et al. (which makes a circular reference to this paper for the introduction of these divergences).\n\n* The paper motivates the notion of spread divergences by the fact that f-divergences impose matching supports (Section 1), not mentioning that optimal transport theory is a much natural fit for any such kind of setting (e.g. Wasserstein distances). This is a big omission and a missed occasion for a potentially interesting discussion.\n\n* The paper then claims that \"spread noise makes distributions more similar\" (Section 2.1), not mentioning that equation (8), which it claims to have been shown by Zhang et al. paper (see below), is in fact a data processing inequality *long known*. They will find it, along with a huge number of other useful properties, in series of IEEE T. IT papers, among which Pardo and Vajda's \"About distances of discrete distributions satisfying the data processing theorem of information theory\",  Van Erven and Harremoes, \"Re ́nyi Divergence and Kullback-Leibler Divergence\" (for the KL / Rényi divergence, but you have more references inside), etc. .\n\n* The paper then goes on \"showing\" (a word used often, even when there is not a single Theorem, Lemma or the like ever stated in the paper...) several properties (Section 2.2). The first states that (9) is equivalent to P being invertible. It is wrong because it is just in fact stating (literally) that P defines an injective mapping. The second states that (11) is equivalent to (12), without the beginning of a proof. I do need to see a proof, and in particular how you \"define\" an invertible transform \"p^-1\".\n\n* The paper then gives two examples (Sections 3, 4). In Section 3, I am a bit confused because it seems that p and q must have supports in IR, which limits the scope of the example. The same limitation applies to Section 4, even when it is a bit more interesting. In all cases, the authors must properly state a Lemma in each Section that states and shows what is claimed before Section 3.\n\n* The paper then makes several experiments. Unless I am mistaken, it seems that Section 5.1 relies on a trick that does not change the support from x to y. Therefore, what is the interest of the approach in this case ? In Section 5.2, isn’t the trick equivalent to considering ICA with a larger \\gamma ?\n\n* A concern is that the paper says little about the reason why we should pick one p(y|x) instead of another one. The focus is on the identity of indiscernibles. The paper also forgets some potential drawbacks of the technique, including the fact that variance increases — the increase can be important with bad choices, which is certainly not a good thing.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good theoretical contribution; empirical results could be more extensive",
            "review": "Summary\n=======\nThis paper introduces spread divergences. Spread divergences are obtained by taking the divergence between smoothed/noisy versions of two distributions. A spread divergence between two distributions of non-overlapping support can be defined even when the corresponding divergence is not. The authors discuss conditions under which the data generating process can be identified by minimizing spread divergences and apply spread divergences to the examples of PCA, ICA, and noiseless VAE.\n\nReview\n======\nWith a lot of papers focusing on generative modeling, divergence minimization is of great relevance to the ICLR community. Adding noise to distributions to ensure overlapping support is intuitive and has been used to stabilize training of GANs, but I am unaware of any work focusing on questions of identifiability and efficiency. I especially like the example of slowing EM in ICA with small noise. Here, some empirical results are lacking which analyze the speed/correctness of the identification of parameters for various choices of divergence/model noise. These would have greatly enhanced the paper. Instead, the available space was used to show model samples, which I find less helpful.\n\nIn Section 3.2 and Section 6 the authors argue that choosing noise which maximizes the spread divergence is optimal or at least preferable. This seems counterintuitive, given that the point of the noise was to make the distributions more similar. Please elaborate on why maximizing the divergence is a good strategy.\n\nMinor\n=====\nThe paper seems hastily written, with some grammar issues, typos, and sloppy use of LaTeX, e.g.:\n\n– \"-\\log\" instead of \"\\log\" in definition of KL divergence in the introduction\n– \"Section 2.2\" not \"section(2.2)\", \"Equation 24\" not \"equation(24)\"\n– \"model (Tipping & Bishop, 1999)\" instead of \"model Tipping & Bishop (1999)\"\n– \"\\mid\" instead of \"|\"\n– \"x\" instead of \"y\" in Equation 28\n\nPlease provide a reference for the EM algorithm of ICA.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}