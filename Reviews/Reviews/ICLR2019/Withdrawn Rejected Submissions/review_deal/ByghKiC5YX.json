{
    "Decision": {
        "metareview": "I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.\n\nA paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:\n\n1. Show that the errors found can be used to meaningfully improve the models. \n\nThis requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).\n\n2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field.\n\nThis is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary.\n\n3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.\n\nI do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).\n\n4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models\n\nGiven that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326\nHowever, I believe the paper would need to be rethought and rewritten to make this sort of contribution.\n\n\nUltimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. \n\nHowever, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as “adversarial examples”, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. \n\nWhile the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust … etc (for example, see [1]).  \n\nThe attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. \n\n[1] Generalisation in humans and deep neural networks, arXiv:1808.08750 ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important task; very poorly written",
            "review": "This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques:\n1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1.\n2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision.\nSpecifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner.\n\nThe results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines.\nMoreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process?\n\nMy major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear \"expectation (E)\" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found  the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Exciting advance in discrete adversarial attacks",
            "review": "In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary.\n\nOverall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. \n\nI only have a few questions and remarks:\n\n* What’s the “random attack” baseline in these tasks? In computer vision it’s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.\n\n* Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also “fooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores.\n\n* Are you planning to release the code? Will it be part of CleverHans or Foolbox?\n\nOverall, I find this work to be a really exciting advance on discrete adversarial attacks.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Novel probabilistic framework for making adversarial attacks on deep networks with discrete valued inputs; flexible framework that allows solving the trade-off between attack success rate and computation time",
            "review": "The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). \nThe proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}