{
    "Decision": {
        "metareview": "The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Not quite enough for acceptance"
    },
    "Reviews": [
        {
            "title": "Variational Auto-Encoder",
            "review": "\n\nThe authors made a method that consider latent binary representation instead of continuous representations in Seq2seq models, and showed that it could help regularization in multiple sequence prediction tasks. \n\nTo be effective, the authors considered a simple approximation of the average embedding that prevent them from using REINFORCE. \n\nThe intuition is that hard latent binary units might be better at representing vocabulary choices and leads to a better inductive bias of the model. However, no inspection of the model to understand in which sentences the averaging of binary variables makes a real difference in prediction.\n \nThere is no analysis of the condition under which this approximation is correct. What is the impact of this approximation? Are the results the same if reinforce is used?\n\nOverall, the article is clearly written and conclusions sufficiently relevant for this article to be published in ICLR.\n\nMinor comments:\n- the term Amortized Variational Inference is not the most intuitive and Variational Auto-Encoder seems to be the de-facto standard term for these techniques.\n- For the MT results, why the authors did not consider more common language pairs so that the results are readily comparable?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Amortized Context Vector Inference for Sequence-to-Sequence Networks",
            "review": "Summary:\nThis paper tries to introduce stochasticity over context representation to enable the neural networks to search for effective representation so as to avoid getting trapped into local optima. The basic idea is to formulate the posterior probability of context representation via a mixture gaussian model. The standard attention model SA can be thought of as a special case of the proposed method. The proposed method outperforms a number of baselines on ADS, video captioning, and machine translation datasets.\n\nStrength:\n  - Generalize better via considering uncertainty and exploring more search space with negligible computational overheads\n  - Outperforms a number of baselines.\n\nComments:\n  - For video captioning task, various methods perform quite similar on test set, but vary a lot on validation set? \n  - Any chances to compare bounds of different variational methods from optimization perspective?\n  - In order to further validate the generalization capacity, any experiments on low-source data or domain adaptation?\n  - Compare with transformer?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents a new way to construct the context vector in seq2seq networks.",
            "review": "This paper presents a new way to construct the context vector in seq2seq networks. Specifically, the context vector in the proposed model is treated as a latent variable with a normal prior. The posterior is \"defined\" as a mixture of the latent presentations of the input sequence and the mixing weight is the attention. To do inference, maximising VAE-like \"ELBO\" is used. The proposed model is reported to achieve better performance in the seq2seq tasks including document summarisation and video caption.\n\nMy comments are as follows:\n\n1. Although the authors claim a full statistical treatment of the context vector, the proposed model is not a proper statistical setting to me. I do not mean the setting is wrong, but it is really weird to me. The paper treats the context vector as a latent variable, which is used to generate the target sentence (take document summarisation task for example). The proper setting is to use the target sentence to do inference of the posterior of the context vector. However, the paper directly defines the format the \"posterior\", which is a mixture of the hidden representation of the source sentences, ignoring the target sentence. Therefore, I do not think this is the true posterior of the context vector. Moreover, seriously speaking, the \"ELBO\" of the proposed model is not a real ELBO. It is a training object function looked like an ELBO. Therefore, the difference of the proposed context vector with the standard one is instead of using a fixed hidden representation, the proposed one uses a stochastic one drawn from a Gaussian, the covariance of which is from a neural network.\n\n2. To me, it is less intuitive to analyse why the performance gain comes from, given the proposed context vector works so better than the previous ones. I am expecting the authors to give some intuitive explanations and empirical results on this.\n\n3. Minor comments on clarity: The abstract says both the mean and the covariance are modelled with neural networks, but in the paper, it seems that only the covariance is from a neural network. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}