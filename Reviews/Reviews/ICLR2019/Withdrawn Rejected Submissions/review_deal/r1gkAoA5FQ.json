{
    "Decision": {
        "metareview": "This paper attempts at modeling coherence of generated text, and proposes two kinds of discriminators that tries to measure whether a piece of text is coherent or not.\n\nHowever, the paper misses several related critical references, and also lacks extensive evaluation (especially manual evaluation).\n\nThere is consensus between the reviewers that this paper needs more work before it is accepted to a conference such as ICLR.\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Missing relevant comparisons, evaluations, and references",
            "review": "This paper addresses long-text generation, with a specific task of being given a prefix of a review and needing to add the next five sentences coherently.  The paper proposes adding two discriminators, one trained to maximize a cosine similarity between source sentences and target sentences (D_{coherence}) and one trained to maximize a cosine similarity between two consecutive sentences.  On some automatic metrics like BLEU and perplexity, an MLE model with these discriminators performs a little bit better than without.\n\nThis paper does not include any manual evaluation, which is critical for evaluating the quality of generated output, especially for evaluating coherence and cohesion.  This paper uses the task setup and dataset from \"Learning to Write with Cooperative Discriminators\", Holtzman et al., ACL 2018.  That paper also includes many specified aspects to improve the coherence (from the abstract of that paper \"Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.\").  But this paper:\n--Does not compare against the method described in Holtzman et al., or any other prior work\n--Does not include any human evaluations, even though they were the main measure of evaluation in prior work.\n\nThis paper states that \"To the best of our knowledge, this paper is the first attempt to explicitly capture cross-sentence linguistic properties, i.e., coherence and cohesion, for long text generation.\"  There is much past work in the NLP community on these.  For example, see:\n \"Modeling local coherence: An entity-based approach\" by Barzilay and Lapata, 2005 (which has 500+ citations). \nIt has been widely studied in the area of summarization, for example, \n\"Using Cohesion and Coherence Models for Text Summarization\", Mani et al., AAAI 1998, and follow-up work.\nAnd in more recent work, the \"Learning to Write\" paper that the dataset and task follow from addresses several linguistically informed cross-sentence issues like repetition and entailment.  \n\nThe cosine similarity metric in the model is not very well suited to the tasks of coherence and cohesion, as it is symmetric, while natural language isn't.  The pair:\n\"John went to the store to buy some milk.\"\n\"When he got there, they were all out.\"\n\nand \n\n\"When he got there, they were all out.\"\n\"John went to the store to buy some milk.\"\n\nwould have identical scores according to a cosine similarity metric, while the first ordering is much more coherent than the second.\n\nThe conclusion says \"we showed a significant improvement\": how was significance determined here?\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "overall weak evaluation and too many unsubstantiated claims ",
            "review": "The paper proposes a method for improving the quality of text generation by optimizing for coherence and cohesion. The authors develop two discriminators--a \"coherence discriminator\" which takes as input all of the sentence embeddings (i.e. averaged word embeddings) of the document and assigns a score, and a \"cohesion discriminator\" which takes as input the word embeddings of two consecutive sentences and assigns a score. In the former, the score is the cosine similarity between the encodings of the first and second half of the document. In the latter, the score is the cosine similarity between the encodings of the two sentences. Both discriminators use CNNs to encode the inputs. The discriminators are trained to rank true text over randomly drawn negative samples, which consist of randomly permuted sentence orderings and/or random combinations of first/second half of documents. This discriminators are then used to train a text generation model. The output of the text generation model is scored by various automatic metrics, including NLL, PPL, BLEU, and number of unique ngrams in the outputs. The improvements over a generically-trained generation model are very small.\n\nOverall, I did not find this paper to be convincing. The initial motivation is good--we need to find a way to capture richer linguistic properties of text and to encourage NLG to produce such properties. However, the discriminators presented do not actually capture the nuances that they purport to capture. As I understand it, these models are just being trained to incentivize high cosine similarity between the words in the first/second half of a document (or sentence/following sentence). That is not reflective of the definitions of coherence and cohesion, which should reflect deeper discourse and even syntactic structure. Rather, these are just models which capture topical similarity, and naively at that. Moreover, training this model to discriminate real text from randomly perturbed text seems problematic since 1) randomly shuffled text should be trivially easy to distinguish from real text in terms of topical similarity and 2) these negative samples are not (I don't think) at all reflective of the types of texts that the discriminators actually need to discriminate, i.e. automatically generated texts. Thus, even ignoring the fact that I disagree with the authors on exactly what the discriminators are/should be doing, it is still not clear to me that the discriminators are well trained to do the thing the authors want them to do. I have various other concerns about the claims, the approach, and the evaluation. A list of more specific questions/comments for the authors is below.\n\n- There are a *lot* of unsubstantiated claims and speculation about the linguistic properties that these discriminators capture, and no motivation of analysis as to how they are capturing it. Claims like the following definitely need to be removed: \"learn to inspect the higher-level role of T, such as but not limited to, whether it supports the intent of S, transitions smoothly against S, or avoids redundancy\", \"such as grammar of each of the sentences and the logical flow between arbitrary two consecutive sentences\"\n- You only use automated metrics, despite acknowledging that there is no good way to evaluate generation. Why not use human eval? This is not difficult to carry out, and when you are arguing about such subtle properties of language, human eval is essential. There is no reason that BLEU, for example, would be sensitive to coherence or cohesion, so why would this be a good way to evaluate a model aimed to capture exactly those things?\n- Also related to human eval, there should be an intrinsic evaluation of the discriminators. Do they correlate with human judgments of coherence and cohesion? You cannot take it for granted that they capture these things (I very much believe they do not), so present some evidence that the models do what you claim they do.\n- The reported improvements are minuscule, to the extent that I would read them as \"no difference\". The only metric where there is a real difference is on number of unique ngrams generated cross inputs, which is presumably because its just learning (being encouraged to) spit out words that were in the input. I'd like to see the baseline of just copying the input as the output.\n- You mention several times that these models will pick up on redundancy. It is not clear to me how they could do that. Aren't they simply using a cosine similarity between feature vectors? Perhaps I am missing something, but I don't see how this could learn to disincentivize redundancy but simultaneously encourage topical similarity. Could you explain this claim?  ",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting proposal to use discriminators to model coherence in NLG, but completely ignores prior work and presentation is confusing",
            "review": "The idea of training discriminators to determine coherence and cohesion, and training those discriminators as part of an NLG system using policy gradients, is an interesting one. However, there are two major problems with the papers as it stands:\n\n1) it completely ignores the decades of NLG literature on this topic before the \"neural revolution\" in NLP;\n2) the presentation of the paper is confusing, in a number of respects (some details below).\n\nTo claim that this is the first paper to capture cross-sentence linguistic properties for text generation is the sort of comment that is likely to make experienced NLG researchers very grumpy. A good place to start looking at the extensive literature on this topic is the following paper:\n\nModeling Local Coherence: An Entity-Based Approach, Barzilay and Lapata (2007)\n\nOne aspect in which the presentation is muddled is the order of the results tables. Table 2 is far too early in the paper. I had no idea at that point why the retrieval results were being presented (or what the numbers meant). You also have cohesion in the table before the cohesion section in 3.2. Likewise, Table 1, which is on p.2 and gives examples of system output, is far too early.\n\nPerhaps the biggest confusion for me was the difference between cohesion and coherence, and in particular how they are modeled. The intro does a good job of describing the two concepts, and making the contrast between local and global coherence, but when I was reading 3.1 I kept thinking this was describing cohesion (\"T that follows S in the data\" - sounds local, no?). And then 3.2 seems to suggest that coherence and cohesion essentially are being modeled in the same way, except shuffling happens on the word level? I suppose what I was expecting was some attempt at a global model for coherence which goes beyond just looking at consecutive sentence pairs.\n\nI wonder why you didn't try a sequence model of sentences (eg bidirectional LSTM). These are so standard now it seems odd not to have them.\n\nDo you describe the decoding procedure (greedy? beam?) at test time anywhere?\n\nI liked Table 4 and found the example pairs with the scores to be useful qualitative analysis.\n\n\"Based on automated NLP metrics, we showed a significant improvement\" - which metrics? not clear to me that the improvements in Table 3 are significant.\n\nMinor presentation points\n--\n\n\"followed by a logically sound sentence\" - might want to rephrase this, since you don't mean logical soundness in a technical sense here (I don't think).\n\nThe comment in the conclusion about being \"convinced\" the architecture generalizes well to unseen texts is irrelevant without some evidence.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}