{
    "Decision": {
        "metareview": "This paper introduces a planning phase for NMT.  It first generates a discrete set of tags at decoding time, and then the actual words are generated conditioned on those tags.  The idea in the paper is interesting.\n\nHowever, the paper's experimental settings could improve by comparing on larger datasets and also using stronger baselines.  The writing could also improve -- why were only the few coarse POS tags used?  Have the authors tried a larger set?  I think without such controlled comparisons, it would be hard to understand why only those coarse tags are used.\n\nThe reviewers express concern about some of the above issues and there is consensus that the paper should be improved for acceptance at a venue like ICLR.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Poorly motivated and confusing",
            "review": "\nThis paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:\n\n\n§1 ¶1  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vague—I’m not convinced this is meaningful.\n\n§1 ¶2-3 These paragraphs also vague.\n\n§1 ¶5 Why is this approach naive? Is this a well-known method? There are no citations.\n\nFig.1 Very confusing: it looks like the target sentence, “structural tags” and “coding model” form a loop! This example is also confusing because the “structural tags” are non-sensical… they have no relation to this example sentence! I can’t tell if this is because they were made up without relation to the input sentence, or worse, that they’re an actual example from the data, in which case there is something very wrong with the tagger used in the “naive” experiments.\n\nSec. 2.1 What is the motivation behind the heuristics for the “two-step process that simplifies the POS tags”?\n\nSec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these “codes\" (in the form of “simplified” POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called “softplus” in Eq. 2) are never explained.\n\nSec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isn’t needed.\n\nTable 1. I’m not sure what the code accuracy tells us. It’s also unclear to me what is means to “reconstruct” the “original tag sequence” from the codes, esp. given the description in Sec 2.1.\n\nTable 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading… this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.\n\nTable 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; I’m not convinced that the sequences mean anything per se, but it’s a bit like adding some random noise to the decoder state before generating the word sequence.\n\n5.1 “Instead of letting the beam search decide the best … we use beam search to obtain three code sequences with highest scores.” I’m confused: what is the difference?\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Attacking an interesting but mostly solved problem with weak baselines and questionable ML techniques",
            "review": "The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the \"Discrete Autoencoders for Sequence Models\" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:\n* comparing with a stronger NMT architecture and larger data-set\n* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.\n* comparison to fully unsupervised latents and some other system, e.g., the system from [1] above\n\nIn the absence of these comparisons and with little novelty, the paper is a clear reject.\n\n[Revision]\n\nGreatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach to translation diversity, but experiments somewhat lacking and details missing",
            "review": "The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.\n\nContributions\n-----------------\nA simple method for improving structural diversity.\n\nThe use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.\n\nExperiments suggest that the method is rather effective (albeit only improving translation quality marginally)\n\nI like the proposed discrepancy score based on pairwise BLEU scores.\n\nIssues\n---------\nIt is not clear if teacher forcing was used in the \"tag planning\" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the \"tag planning\" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).\n\nIt is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.\n\nNo details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.\n\nHaving a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.\n\nMinor issues\n-----------------\nCitation for the Xavier method is missing.\n\nNotation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.\n\nThere are many grammatical errors. Please make sure to proofread!\n\n\"Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap.\" What does this mean?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}