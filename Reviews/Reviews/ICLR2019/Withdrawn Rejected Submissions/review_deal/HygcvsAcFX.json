{
    "Decision": {
        "metareview": "The paper proposed an optimal margin distribution loss and applied PAC-Bayesian bounds that are from Sanov large deviation inequalities to give generalization error bounds for such a loss. Some interesting empirical results are shown to support the proposed method. \n\nThe majority of reviewers think the paper’s empirical results are encouraging, although still in premature stage. The theoretical analysis is a kind of being standard. After reading the authors’ response and revision, the reviewers do not change much of their opinions and think the paper better undergoes systematic further study on their proposal for big improvement.  \n\nBased on current ratings, the paper is therefore proposed to borderline lean rejection. \n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "A new optimal margin distribution loss with some good empirical results, yet premature "
    },
    "Reviews": [
        {
            "title": "Promising work, but an in-depth study of the handcrafted margin loss function is lacking",
            "review": "I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results.\n\nThe proposed margin loss (Equation 1) is said to be \"specially adapted for accelerating the convergence velocity of networks by [the authors]\". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \\gamma,\\mu) should be discussed thoughtfully; at some points in the paper, r and \\gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \\mu.\nBy considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\\gamma, r+\\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. \n\nThe empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the \"hinge loss\" and the \"soft hinge loss\" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process.\n\nTypos and minor comments:\n- Abstract: \"And our ODN model also outperforms the other three loss models...\" Which three loss models?\n- Section 3: \"Specially, define L_0 as r=\\theta...\" I think it should be r=0\n- Section 4.1: model-s => models\n- Page 7 (and elsewhere): Table. 2 => Table 2\n- Please specify that \"Xent\" stands for cross-entropy\n- Figure 3: Please use larger font sizes\n- Proof of Lemma 2: Equation. 4 => Equation 4 \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "PAC-Bayesian analysis for DNNs",
            "review": "This paper presents a PAC-Bayesian bound for a margin loss.\n\nTheorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1?\n\nThere are some typos in this paper.\n“To derive a expected risk bound”: a -> an\n“used to formalize error-resilience in Arora et al. (2018) as following:”: following: -> follows.\n“the deep network from layer i to layer j”, “injected before level i”: i,j should be in the math mode.\n“dependent on the network structure .” there is an additional blank space after ‘structure’.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "just seem interesting",
            "review": "The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].\nMore precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical \nbounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and \n[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some \nconditions) on the change of the layers (layer and interlayer cushion) as well as the activation \ncontraction. It is also worth noting that the paper is using a differrent loss function comparing \nto [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.\nAlthough the results seem interesting, the analysis is not convincible for me.\nA plus point is that the paper presents interesting numerical experiments showing the promising of the approach.\n\nMajor comments:\n1) The statement of the Theorem 1 is not clear: \nis it just under the assumptions of the lemmas\nor is it under all definitions and lemmas?\n2) The proof of Theorem 1 is not clear:\n how do you get the inequality (5)?\nhow do you get an upper bound on the KL divergence?\n This is not trivial for me!\n3) What is \\rho in Theorem 1 and in Definition 2?\n4) Your remark after Theorem 1 is not clear for me.\n  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?\n a simple counter example would fit better the explanation here, I guest.\n\nMinor comments:\n1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]\nwithout precisely citations. I wonder how do you obtain your Lemma 1?\n2) page3, after formula (1), your loss will first DECREASING, not \"increasing\".\nCheck the sentence \"Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....\"\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}