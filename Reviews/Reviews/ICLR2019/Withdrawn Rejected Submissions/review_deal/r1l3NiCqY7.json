{
    "Decision": {
        "metareview": "This paper is entitled Lipschitz regularized deep networks generalize. In fact, the paper has nothing in particular to do with neural networks. It is really the study of minimizers of a Lipschitz-regularized risk functional over certain nonparametric classes. The connection with neural networks is simply that one can usually achieve zero empirical risk for (overparametrized) neural networks and so, in deep learning practice, neural networks behave like a nonparametric class. Given the lack of connection with neural networks, one cannot logically learn anything specific about neural networks from this paper. It should be renamed... perhaps \"Lipschitz regularization with an application to deep learning\".  One could raise issues of technical novelty, as it seems many of the key results are known.\n\nI also question the insight that the bounds provide: they end up depending exponentially on the dimension of the data manifold. In the noiseless case, this exponential dependence arises from a triangle inequality between an arbitrary data point x and the nearest training data point! In the noisy case, this exponential dependence appears in a nonasymptotic uniform law of large numbers over the class of L-Lipschitz functions. There's no insight into deep learning here. It's also hard to judge whether these rates are what is explaining deep learning practice: it's unclear what the manifold  dimensionality is, but it seems unlikely that this bound explains empirical performance (even if it describes the asymptotic rate of convergence). \n\nOne of the main results shows that, in the face of corrupted label (corrupted in a particular way), Lipschitz regularization can ```\"undo\" the corruption. However, convergence is not measured with respect to the true labeling function, but rather to the solution to the population regularized risk functional. How this solution relates to the true labeling function is unclear.\n\nThe paper also purports to resolve a mystery of generalization raised by Zhang et al (ICLR 2017). In that paper, the authors point to the diametrically opposed generalization performance on \"true\" and \"random\" labels. In fact, this paper does not resolve this problem because Zhang et al. were interested in how SGD solves this problem without explicit regularization. That Lipschitz regularization could solve this problem is borderline obvious.\n\nI wanted to make a few comments.\n\nIn the rebuttal with reviewers, the question of parametric rates comes up. I think there's some confusion on both the part of the reviewer and authors. The parametric rates are often apparent but not real. The complexity terms often have an uncharacterized dependence on the number of data (through the learning algorithm) and on the size of the network (which is implicitly chosen based on the data complexity). In practice, these bounds are vacuous.\n\nAt some point, the authors argue that \"In practice, u_n(x) is rounded to the nearest label, so once |u_n-u_0| < 1/2, all classification results will be correct after rounding.\" I'm not entirely sure I understand the logic here. First, convergence to u_0 is not controlled, but rather convergence to u*. u* may spend most of its time near the decision boundary, rendering uniform convergence almost useless. One would need noise conditions (Tysbakov) to make some claim.\n\nSome other issues:\n\n1. in (1), u ranges over X\\to Y, but is then applied also to a weight vector.\n\n2.  Is\"continuum variational problem\" jargon? If so, cite. Otherwise, taking limits of rho_n and J makes sense only if J is suitably continuous, which depends on the loss function. You later address this convergence and so you should foreshadow.\n\n3. Notation L[u;\\rho] in (5) should be L[u,\\rho], no?\n\n4. (Goodfellow et al., 2016, Section 5.2) is an inappropriate citation for the term \"Generalization\".\n\n5. in Thm 2.7.,  there is reference to a sequence mu_n and i assume the sequence elements is indexed by n, but then  n appears in the probability with which the bound holds, and so this bound is not about the sequence but about a solution for \\rho_n for fixed n.\n\n6. Id should not be italicized in the statement of Lemma 2.10.  Use mathrm not text/textrm.  it should also be defined.\n\n7. \"convex convex\" typo.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Cannot escape the curse"
    },
    "Reviews": [
        {
            "title": "interesting approach to generalization, but are the guarantees relevant?",
            "review": "The paper studies generalization through a general empirical risk minimization procedure with Lipschitz regularization.\nGeneralization is measured through distance of the empirical minimizer function to a true labeling function u_0, or to the minimizer of the expected regularized loss.\n\nThe approach of studying generalization through the lens of Lipschitz stability over the data is interesting,\nand the study directly considers minimizers of regularized optimization objectives, which is different than\nrecent generalization bounds which often provide guarantees on a given network regardless of how it was trained.\n\nHowever, various aspects of the setup seem quite disconnected from practice in the context of deep neural networks,\nand the obtained guarantees are quite weak and not always connected to generalization:\n\n- the approach relies on a constant L_0 in the objective which is assumed known in advance (although it is usually set to 0 in practical methods), and determines the nature of convergence. In particular, the results for small L_0 (referred to as \"noisy labels\") only shows convergence to a minimizer u^* of the expected *regularized* loss, thus does not characterize generalization in the usual sense since u^* is biased. More generally, the distinction between 'clean' and 'noisy' labels is confusing and should be clarified: the paper seems to assume that the true labeling function u_0 may itself produce incorrect labels deterministically even with infinite data, which is an odd way to formulate the learning problem.\n\n- the convergence rates obtained in the paper exhibit a curse of dimensionality (O(n^{-1/m}) where m is the dimensionality of the data manifold). Given that most other bounds for neural networks do not exhibit such a dependence on dimension, this seems to be a weaker guarantee, unless the setting captures an improvement in a different setting. (edit: I removed the previous remark on parametric rates, which was inaccurate) Some of the constants also seem to grow exponentially with m. Either way, this should be discussed in the paper.\n\n- possibly related to the previous point, all the theory in the paper is agnostic to the function class considered, given that it simply considers all lipschitz functions in the variational problem (1). Given that the authors attempt to explain generalization of neural networks, this seems like a non-negligible disconnect since there could be approximation errors. In particular, even if deep networks can perfectly fit training data, it is not clear that they can achieve the best trade-off with the Lipschitz constant in the regularized objective (1).\n\n- the assumptions on the prediction space (simplex) and the used loss functions also seem disconnected from practice (the cross-entropy loss usually includes a softmax). Note that while usual networks can fit randomly labeled data (Zhang et al.), this does not mean their loss is 0. Yet the analysis seem to rely on having zero loss on training points, even in the case of 'noisy labels'.\n\nAdditionally, the use of covering arguments in the input space in the proposed approach is related to the study of generalization through robustness [Xu & Mannor (2010), \"Robustness and Generalization\"]. The authors should discuss the relationship to this work.\n\nMore comments:\n- the term 'converge' in the title is not clear. In the abstract, what does 'verification' mean?\n- Section 2.3: \n  m_0 == m ?\n  How does C grow with m in Theorem 2.7 and Corollary 2.8?\n  What is meant by 'perfect generalization'?\n  Is eq. (6) realistic for classification losses?\n- Section 2.4: clarify what is meant by 'noisy labels'",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially significant results, some question marks.",
            "review": "Disclaimer:  I am not a working expert in this specific area.  I have used spectral normalization for my own applications, and have working expertise in leveraging Lipschitz properties for various flavors of stability analyses.\n\nThis paper proposes a error convergence analysis for Lipschitz-regularized neural nets.  The analysis is framed in function space of the neural net and assumes the ability to solve the learning minimization problem.   The authors contrast their analysis with other analysis approaches in several ways.  First is that their analysis is \"more direct\" and second is that their analysis is independent of the learning approach (e.g., spectral normalization + SGD). \n \n\n***Clarity***\n\nThe paper is mostly clearly written.  Some of the statements are presented without sufficient description.  For instance:\n\n-- What does it mean when the paper states that their analysis is \"more direct\" than previous work?  There was no discussion of previous work beyond that comment.\n\n-- The statement of Lemma 2.9 is not entirely clear.  Is \\sigma_n a vector of white gaussian random variables?\n\n-- Definition 2.3 precludes the hinge loss.  Comments?\n\n-- Example 2.6, the regularized cross entropy loss doesn't satisfy L(y,z) = 0 if and only if y=z.  It might not even satisfy L >= 0. \n Comments?\n\n-- Since the function is Lipschitz, in the noisy case, can one say anything about the guarantees near the manifold for some definition of near?  E.g., can one bound how bad the tail parts of Figure 3 can be?\n\n\n***Originality***\n\nIt seems to me that the analysis aims to set up the problem such that one can leverage standard results in probability theory.    For instance, the proof of Theorem 2.7 is quite straightforward given Lemma 2.9.  Of course, setting up the problem properly is 90% of the work.  The analysis for the noisy case is much more involved.  It is unfortunately beyond my expertise to properly judge originality in this case.  Perhaps the authors can comment on how the way they set up the problem is novel?\n\n\n***Significance***\n\nMy biggest question mark w.r.t. significance are the claims of how this analysis compares with previous work.\n\n-- What does \"more direct\" analysis mean?\n\n-- What is the significance of an algorithm-agnostic analysis?  I understand the appeal from a certain perspective, but can the authors point to previous literature (perhaps not in deep learning) where an algorithm-agnostic analysis was shown to give more insight?\n\n\n***Overall Quality***\n\nConditioned on the problem setup being novel and the comparison with related work clarified, I think this is a solid contribution.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An interesting paper providing learning guarantees for unconstrained size neural network classifiers with explicit (exact) Lipschitz regularization.",
            "review": "The paper gives convergence guarantees to the true neural network classifier for the networks that are explicitly regularizing the (exact) Lipschitz constant of the network. The bound stated decays as ( log(n)/n )^{1/m}, where n is the number of training points and m is the dimension of the data manifold. Thus the decay rate is pretty slow when the data lies on a high-dimensional manifold. I believe it should also depend on the volume of the data manifold.\n\nComputing the exact Lipschitz constant of the network is intractable. All the theorems apply to minimization problem defined in (1) with the exact Lip(u,X) being regularized, and not to the minimization problem of the lower bound (2). I believe there are also no convergence guarantees how quickly this lower bound on the Lipschitz constant approaches Lip(u,X), unless some assumptions are made on the smoothness of the data manifold (comments and insights would be appreciated). Thus in my opinion the current results have little practical importance. Nevertheless, it’s still interesting to see some ideal-setting guarantees being established.\n\n\nTheorem 2.7: Is m == m_0 (the dimension of the data manifold)? Shouldn’t the bound be 2*C*L_0….? (assuming lemma 2.9 is correct)\n\nCor 2.8: Where does the volume Vol(M) of the manifold disappear? Is C in equation (6) the same C as in Theorem 2.7? Also, it looks like the bound should bound should have C^2 (assuming theorem 2.7 is correct, and the C’s are the same).\n\n\nIntroduction: I assume u(x,w) is not the last layer map, but a map from the input space to the labels (i.e., the whole neural network function and not just the map from the last hidden layer to the labels). If I am correct, it’s misleading to refer to u(x,w) as the last layer map. And if it is the last layer map, please justify why it is enough to consider the Lipschitz constant of the last layer.\n\nThe term “clean data” is never defined. My guess is that “clean data” refers to the realizable  setting, and “noisy” to agnostic, where the hypothesis space consist of neural networks of arbitrary size.\n\n\n“Our analysis can avoid the dependence on the weights because we make the assumption that there are enough parameters so that u can perfectly fit the training data. The assumption is justified by Zhang et al. (2016).”\nNote, that the network used in practice achieve zero classification error, as demonstrated by Zhang et al, but I doubt the cross entropy loss (that is usually being minimized) is exactly zero.\n\nRemark 1.4 “will result in an expected loss..”  (there is also a typo here) should specify that you are talking about empirical error (0-1 loss), since I don’t think the loss function is fixed anywhere earlier in the text.\n\nRemark 2.2 : Just wanted to note, that it is more common to call L(u,\\rho) risk. The gap between L(u,\\rho) and the empirical risk L(u,\\rho_n) is usually called the generalization error (and only in the case of zero empirical risk, L(u,\\rho) is equal to the generalization error). I did check the reference in Goodfellow et al. book, and I see that it is consistent with your definition.\n\nJust below Remark 2.2:\n“We would also expect the sequence of generalization losses L[u_n ; \\rho] to converge to zero in the case of perfect generalization.”\nOnce again, this is true only in the realizable setting.\n\nCould the authors comment on the connection to Cranko et al. 2018 work?\n\nTypos:\nIn the abstract, no which: “..corrupted labels which we prove convergence to...”\n“...a candidate measure for the Rademacher complexity, which a measure of generalization...”.\n“1-Lipschitz networks with are also important for Wasserstein-GANs”\nSection 2.1 “is it clear that u0, is a solution of “, should be “it is”\n\n\n---------\n[UPDATE]\n\nRegarding the comment \"Our paper resolves the question posed in ICLR Best paper 2017 \"Understanding deep learning requires rethinking generalization\"\", I don't think that analyzing networks with explicit regularization resolves the questions stated in Zhang et al paper. As other reviewers mentioned, there are a number of other papers that formally define quantities that correlate with the generalization error, and are larger for random vs true labels. There are also other papers showing that one can tune some parameters of the optimization algorithm to avoid overfitting on random labels (while it is a modification to the algorithm, it is still similar to explicitly regularizing the Lipschitz constant of the network) (see e.g., Dziugaite et al work on SGLD).\n\nTherefore, the claim in the abstract \"A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?\" needs to be toned down a bit.\n\nIn my opinion, the work presented in this paper is a valuable contribution to learning theory. The new version of the paper is easy to read. Therefore, I recommend acceptance if the authors change the claim about resolving  the questions posed by Zhang et al.\n\nAnother typo:\n - for convergence, we require that the network also grow(s), ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}