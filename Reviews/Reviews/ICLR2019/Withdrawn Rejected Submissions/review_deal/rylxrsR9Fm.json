{
    "Decision": "",
    "Reviews": [
        {
            "title": "addressing an interesting problem but current contribution borderline",
            "review": "The paper proposes a method for neural architecture search. It is observed that in standard layer-wise neural networks, there is mass redundancy in the model weights. The authors propose to go beyond the hierarchy of layers, prune low-sensitivity neurons and add new neurons to the network. The concept of layer is weakened, and the relation graph between individual neurons are highlighted. Overall, the proposed research problem is interesting.\n\nHowever, one concern I have is the authors are not going as far as they claimed. The backbone of the neural network in Figure 2 is still a layered network structure. In a nutshell, a neural network is a computation graph. If the layers are not essential as claimed, why not go beyond such structures.\n\nThe method proposed for location and direction search is not entirely satisfactory. Given the typically large network structure, there has to be more informed methods/heuristics for selecting where to adjust the structure. Relying on genetic algorithms to find such locations and changes do not seem to be promising.\n\nFinally the results are not so convincing. The authors claim to have obtained higher test accuracy than state of the art architectures. But their method's results were worse than several existing methods on the same datasets. The definition of \"score\" is rather ad hoc; no (sufficient) justifications are given as to why the particular choice/design. Even using \"score\", the proposed method is not really the best among competing methods.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review.",
            "review": "Summary:\nThis paper proposes a method to design architectures for neural networks. Their method has 3 steps.\n\n1. Search for an architecture using an existing automatic model designing algorithm. ENAS (Pham et al 2018) is used in this work.\n\n2. Prune some existing neurons in the model found in step (1), using a pruning algorithm. The algorithm (dynamic network pruning, DNP) comes from an Anonymous submission, so I don’t know what it is. However, I suspect that other algorithms can also be used here.\n\n3. Add extra neurons and computations to the pruned network using evolutionary search.\n\nThe ultimate architecture resulting from the search indeed improves compared to the architecture found in step (1): having fewer parameters and achieving a smaller error rate on CIFAR-10.\n\nStrengths:\nThe NHN method is well-motivated. Pruning of networks is an important technique, but there does not seem to be methods to undo over-prunning. NHN can be applied there.\n\nThe empirical results of the paper also demonstrated that (ENAS + pruning + NHN) is better than both ENAS and (ENAS + pruning). I think this is a strong point.\n\nWeaknesses:\n1. The paper is not very well-written. Followings are some points that confused me:\n- Section 2.3: “the standard deviation thresholds are used to determine the optimal add-on directions”. Which stddev is it? Are you running the existing structure through a batch of data and measure the stddev of each neuron?\n\n- Figure 3 is not very illustrative. My understanding of Figure 3 is as follows. Each foll. Please correct me if I’m wrong, but if I am correct, then I strongly suggest that you explain each circle in Figure 3 corresponds to a channel in the input and output layer.\n\n- When you perform add-on search, how are the add-on neuron used? Are there outputs added to the existing output layers, or are they concatenated, followed by 1x1 convs etc.?\n\n- In Section 2.2, the dynamic network pruning (DNP) algorithm is from a paper in submission. In this case, I suspect that the authors should take the responsibility to briefly explain how the algorithm works. At least, some intuitions should be given.\n\n2. Experimental results of this paper are relatively weak.\n\nFirst, the accuracy achieved by NHN is almost always behind that of block-based search methods, as reported in Table 3 (Zoph et al 2018; Pham et al 2018; Liu et al 2018). It has already been established that block-based search methods are stronger, since they have a smaller search space. Perhaps NHN should be applied on a cell found by a search-based method, rather than on a whole convnet found by ENAS.\n\nSecond, I am not a fan of the Score metric in Table 1. \\sqrt{(e/2)^2 + (n/25)^2}, where e is the error rate and n is the number of parameters, is a strange metric. I wonder where do the numbers 2 and 25 come from, and had they been chosen differently (really, one can SGD-search for these numbers!), some other entries in Table 1 would have been bolded.\n\n3. It’s unclear how much SmoothInit affects (i.e. improves) the performance of NHN. Can you comment on this?\n\n4. Missing citations: Net2Net [1] also expands existing network architectures, albeit with methods to ensure the stability of the “add-on layers”, which can perfectly be applied to the search for add-on neurons in your method.\n\n5. Nit-picking:\n- Section 2.2: grouped convolutions where the number of groups is equal to the number of input (and output) channels is called “depthwise convolutions”. I think this is a less confusing name.\n\nReferences.\n[1] Net2Net: Accelerating Learning via Knowledge Transfer. https://arxiv.org/pdf/1511.05641.pdf\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok, but not good enough",
            "review": "This paper is more like a network pruning paper, where we first get an initial network structure by some method (ENAS in this case), prune unimportant connections and add some neurons back to remedy the performance degradation. The novelty lies on the third step, using evolutionary algorithm to insert the add-on neurons across layers. In this way, the paper got the state-of-the-art result on Cifar-10 with less parameters and less searching time. However, the paper has following problems.\n1. As a general method, the proposed neuron search not only can work with structures found by ENAS, but also other network structures, for example ResNet. What are the effects on other common network structures? It would be better to also try other network architectures and evaluate on several datasets or tasks to show its generality, instead of just 12-layer CNN found by ENAS and on CIFAR 10. Without further experiments, the authors can only claim that their method is effective (to some extent) when coupled with ENAS.\n2. The neuron adding procedure increases the accuracy from 96.20% to 96.52%, which is not significant.\n3. The authors did not give enough details for others to reproduce their work. For example, the subroutines \"Mutate\", \"GenerateLoc\" and \"GenerateDir\" are not described in details. It is also not clear how the number of segments and the number genes in each segment are determined, and how they affect the performance. \n4. The paper says “Experiments show that the NHN outperforms the original ENAS architecture” (in Conclusion). But from Section 3 “Layer hierarchy search”, it says the ENAS achieves test accuracy of 96.55% on Cifar-10 with Cutout, while the final test accuracy of NHN is 96.52%. Apparently, NHN does not outperform the unpruned ENAS structure.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}