{
    "Decision": {
        "metareview": "The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \\hat{y} which contains information about y. Since the encoder also predicts \\hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores. \n\nThough none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant 'z' is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work. \n\nWith the borderline review scores, the paper can go in either of the half-spaces (accept/reject) but I am hesitant to recommend an \"accept\" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Good results on classification and attribute manipulation but has considerable overlap with Fader networks"
    },
    "Reviews": [
        {
            "title": "Method clarity can be improved, and lacks some key comparisons experimentally.",
            "review": "In this paper, the authors introduce a neural network architecture that has three components.\nFirst a VAE is used to encode images in to two latent states \\hat{y} and \\hat{z}, with \\hat{z}\nintended to be class (e.g. face attribute) agnostic. The decoder reconstructs images from \\hat{y}\nand \\hat{z} concatenated together. A GAN style discriminator attempts to distinguish the \ndecoded image from the original input image as real or fake, allowing the decoder to produce \nhigher quality decoded images. An auxiliary network A attempts to classify the face attribute y\nfrom the class agnostic features \\hat{z}, with the idea being that the encoder should try to produce \n\\hat{z} vectors from which the class cannot be predicted. An additional classifier is trained\nusing a classification loss \\hat{L}_{class} on the encoded reconstructed image, the use of which \nI don't understand.\n\nI think additional work on section 2.5 through section 3 would be helpful to improve clarity.\nAs one example, \"y\" is unnecessarily overloaded: y denotes a specific attribute, \\hat{y}\ndenotes a latent vector that is intended to not be class agnostic, \\tilde{y} denotes the\nprediction of an auxiliary network on an intended class-agnostic latent vector \\hat{z} of\nthe presence of the original attribute y, and \\hat{\\hat{y}} denotes the non agnostic latent\nvector achieved by passing the decoded image back through the encoder.\n\nThis notational complexity is compounded by the fact that a number of steps in the method are\nnot well motivated in the text, and left to the reader to understand their purpose. For example,\nthe authors state that \"we incorporate a classification model into the encoder so that our model may\neasily be used to perform classification tasks.\" What does this mean? In the diagram (Figure 1),\nwhere is this classification model? Why in the GAN loss is there a term that compares the\nfake loss with the result of classifying a decoded z vector? Is this z \\hat{z}, or a latent vector\ndrawn from a distribution p(z)? If it is the former, how does this term differ from the second\nterm in the GAN loss. If it is the latter, then shouldn't it be concatenated with some y in order to\nbe used as input to the decoder D_{\\theta}?\n\nWhy is it important to extract \\hat{\\hat{y}} from \\hat{x}? In the paper you state that the loss\n\"provides a gradient containing label information to the decoder,\" but why can't we use the known label y\nof the original input x to ensure that the encoder and decoder preserve this information if it is used as \\hat{y}?\nLater in the paper, you explicitly state that \\hat{\\mathcal{L}_{class}} \"does not provide any clear benefit.\"\nIf that is the case, then you should ideally include it neither in the model nor in the paper. If it was\nincluded primarily because previous models included it, then I would recommend you introduce its use\nin a background section on Bao et al., 2017 rather than including it in your model description with an\nexplanation like \"so that our model may easily be used to perform classification tasks.\"\n\nUltimately, this last point brings us to a good summary of my concerns with the model: the inclusion\nof too many moving parts, some of which the authors explicitly say later on provide no benefit.\n\nMoving on to experimental results, I think this is another area where I have a few concerns. First, in\nFigure 2, the authors argue that your model is \"better for 6 out of 10 attributes\" and comparable results for most others. The authors include a gap of 0.1 in the \"Gray_hair\" category as \"better\" but label a gap of 0.5\nin the Black hair category as \"comparable.\" I think results in several of the categories are sufficiently close\nthat error bars would be necessary to draw actual conclusions. If \"better\" were to mean \"better by 0.5\" for example,\nthen the authors method is better on 4 tasks (smiling, blonde hair, heavy makeup, mustache) and worse on 3 (black hair, brown hair, wavy hair).\n\nWith respect to the actual attribute editing, my main concern here is a lack of comparison to models other than Bao et al., despite the fact that face attribute changing is an exhaustively studied task. A number of papers like Perarnau et al., 2016, Upchurch et al., 2017, Lample et al., 2017 and others study this task from machine learning perspectives, and in some cases can perform photorealistic image attribute editing without complicated machinery on megapixel face\nimages. At least the images in Figure 3 and 4 are substantially downsampled from the typical resolution found in the Celeba dataset, suggesting that there was some failure mode on full resolution images.\n\n----\n\nEdit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "missing references to previous work ",
            "review": "Summary: \n\nThis paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. \n\nIn order to disentangle the identity information from the attributes the paper proposes adversarial information factorization : let z be the latent code and y be the attribute the paper proposes to have p(y) =  p(y|z= E_phi(x)), i.e to have z independent of y.  This disentanglement is implemented through a GAN on the variable y  min _phi Distance (p(y), p(y|z)), the distance is defined via a discriminator on y.  \n\nExperiments are presented on celeba dataset,  1) on attribute manipulation from smiling to non smiling for example, on 2) attribute classification results are presented , 3) ablation studies are given to study the effect of each component of the model highlighting the effect of the adversarial information factorization. \n\nOriginality Novelty: \n\nThere is a large body of work on disentanglement that the paper does not cite or compare to for instance, InfoGAN,  Beta- VAE https://openreview.net/pdf?id=Sy2fzU9gl and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdf\n\nNote that for example that in beta- VAE it is a similar idea where but it is on z and z|x and the distance used is KL (since it is has closed form with gaussian) , min_phi Loss+ beta KL (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.  \n\nThe work is also related to MINE https://arxiv.org/pdf/1801.04062.pdf where one would like to minimize the mutual information I(z;y)  this mutual information is estimated through a min/max game.\n \nQuestions: \n\n-  why is RMSprop used for optimization, your model and the Bao et al baseline might benefit from the use of Adam?\n\n- (Table 3 in appendix ) Have you tried higher values of alpha the weight of KL, with the model of Bao et al (it is recommended in beta VAE to have high value of what you call alpha)?\n\nOverall assessment: \n\nThe paper novelty is using min/max game to estimate the mutual information between y (attribute) and z (identity code). Disentanglement and use of min/max games for estimating mutual information has been explored before.  Further discussion and comparaison to previous work is needed. \n\n\n\n \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, Too complex model",
            "review": "This paper proposed a generative model to learn the representation which can separates the identity of an object from an attribute. Authors extended the autoencoder adversarial by adding an auxiliary network. \n\nStrength\nThe motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear.\nExperiments illustrate the advantage of using auxiliary network and demonstrating the role of classify. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.\n\nWeakness \nThe proposed model seem to be unnecessarily complex. For example, the loss of  in (6) actually includes 6 components (5 are from L_enc) and 4~5 tuning hyper-parameters. The L_gan also includes 3 parts. The reason of adding gan loss lacks either theoretical or empirical analysis. So as L_KL. In addition, the second term in L_gan is unnecessary since you already have a reconstruction loss. It also make it to be unclear what we obtain if the equilibrium of the GAN objective achieved.\n\nThe written of this paper can be improved to make it more clear. \nIt looks \\hat_y and \\tilde_y are same thing. \nHow do you get \\hat_z? Do you assume the posterior distribution is Gaussian and use the reparameterization trick? What are \\hat_y and \\hat_\\hat_y? Are they binary or a scalar between 0 and 1?  How do you generate \\hat_x? When generating \\hat_x, do you sample \\hat_z and \\hat_y? If so, how do treat the variance problem of \\hat_y? \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}