{
    "Decision": {
        "metareview": "As all the reviewers have highlighted, there is some interesting analysis in this paper on understanding which models can be easier to complete. The experiments are quite thorough, and seem reproducible. However, the biggest limitation---and the ones that is making it harder for the reviewers to come to a consensus---is the fact that the motivation seems mismatched with the provided approach. There is quite a lot of focus on security, and being robust to an adversary. Model splitting is proposed as a reasonable solution. However, the Model Completion hardness measure proposed is insufficiently justified, both in that its not clear what security guarantees it provides nor is it clear why training time was chosen over other metrics (like number of samples, as mentioned by a reviewer). If this measure had been previously proposed, and the focus of this paper was to provide empirical insight, that might be fine, but that does not appear to be the case. This mismatch is evident also in the writing in the paper. After the introduction, the paper largely reads as understanding how retrainable different architectures are under which problem settings, when replacing an entire layer, with little to no mention of security or privacy. \n\nIn summary, this paper has some interesting ideas, but an unclear focus. The proposed strategy should be better justified. Or, maybe even better for the larger ICLR audience, the provided analysis could be motivated for other settings, such as understanding convergence rates or trainability in neural networks.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "This paper provides some interesting ideas, but has a mismatch between the title and motivation and what is provided"
    },
    "Reviews": [
        {
            "title": "Review: Problem motivation and analysis",
            "review": "This paper proposes and studies the “model completion” problem: given a trained network (and the data on which is was trained), if a subset of the network is reinitialized from scratch, how many retraining iterations are needed to achieve the original network accuracy (or some percentage of it)? For a variety of networks and problems in both supervised and reinforcement learning, model-completion (MC) hardness is quantified for individual network layers/sections. The experiments are the core of the paper and are generally well documented and seem reproducible.\n\nHowever, there are two issues that cloud the paper:\n\t1. The problem motivation (bounding the security of model splitting) is a bit odd. Has model splitting been proposed in the literature as a potential solution to shared model governance? Otherwise it feels like the problem setting was invented to justify the analysis in this paper: “the tail wagging the dog” as the saying goes…\n\t2. Model completion yet still be an interesting analytical tool for deep networks, but this requires a different evaluation. For instance, model completion provides a way to study how complicated different network layers are to learn or maybe to quantify how much of the inference task may be contained in each. (Though these concepts would need precise language and experimental evidence.) But how do these observations compare to other ways of obtaining similar observations? For instance, from the pruning literature, (Molchanov, 2017, ICLR, https://openreview.net/pdf?id=SJGCiw5gl) includes several figures detailing the statistics of individual network layers and how “prunable\" are the filters in each.\n\nThis is largely an analytical paper, and I’ll readily acknowledge that it is difficult to pull a clear and insightful study out of a jumble of experimental observations (and hard to review such a paper too). But the limitations of the problem motivation (point #1) and (in my opinion) the misaligned focus of the analysis (point #2), hurt the clarity and significance of this paper. For it to really be a useful tool in understanding deep learning, some additional work seems to be needed.\n\nOther notes:\n\t3. Pruning literature would be a reasonable comparison in the related work. For instance, (Han, ICLR, 2017, https://arxiv.org/abs/1607.04381) describes a dense-sparse-dense method where a (dense) model is pruned (sparse), after which the pruned connections are reinitialized and retrained (dense) leading to improved accuracy relative to the original dense model.\n\t4. Consider replacing the uncommonly used “ca.” with “~”, e.g. “~1000x” instead of “ca. 1000x”.\n\t5. The specifics about ImageNet in the intro to Section 3 should be moved to Section 4.\n\t6. In Section 3.2 paragraph 2, clarify if “loss” refers to test loss as stated in the intro to Section 3.\n\t7. In Figure 2 (alpha=0.9) and Figure 3 (alpha=1.0, bottom), why are the values constant?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but needs better positioning, metrics, and analysis",
            "review": "This paper proposes the interesting idea of analyzing how difficult\nit is to re-initialize and re-train layers in neural networks.\nThey study these techniques in the context of ImageNet classification\nand reinforcement learning in the Atari and DeepMind lab domains.\nWhile these are interesting ideas and domains to study, I have\nconcerns with the positioning and execution of the paper.\n\n[Positioning, execution and motivation]\nOn the positioning of the paper, a significant part of the introduction\nand related work section is spent arguing that this approach can be used\nfor shared model governance in contexts where homomorphic encryption\nor secure multi-party computation would instead be used.\nComparing the approaches studied in this paper to these\nsophisticated cryptographically-motivated techniques seems\nlike too much of a stretch, as the methods serve very different\npurposes and in most cases cannot even be directly compared.\n\nThe first and second paragraph discuss the vision of distributing\nthe training of models between multiple parties.\nI agree that this is a useful area to study and direction\nfor the community to go, but as the introduction of this paper\nstates, this is the most interesting when the parties have\ncontrol over logically separate components of the modeling pipeline\nand also when joint training of the components is being done,\npotentially on disjoint and private datasets.\nThe empirical results of this paper do none of this,\nas they only look at the case when a single layer is being\nreplaced.\n\nFurthermore the motivation and positioning of the paper is\nnot carried through in the empirical setup, where they\ninvestigate approaches that do training over all\nof the parameters of the model, breaking the assumption\nthat the parties should be independent and should\nnot share information.\n\n[Metrics for measuring model completeness]\nSection 3.1 defines the metric of completion hardness that is\nused throughout the rest of the paper. The metric looks at the\nnumber of iterations that re-training the model takes to\nreach the same performance as the original model.\nIt's not clear why this is an important metric and I am\nnot convinced it is the right one to use as it:\n1) does not give a notion of how nicely the missing portion\nwas recovered, just that the accuracy reached the\nsame accuracy as the original network, and\n2) methods with a very long per-iteration runtime such as\nsecond-order and sampling-based methods could be used to\nreach a good performance in a small number of iterations,\nmaking these methods appear to be very \"good\" at\ncompleting models. I don't think it is nice that this\nmetric relies on the same optimizer being used for the\noriginal model and the completed model.\n\nI think it's more interesting to study *how much* data is\nrequired to recover missing portions of the model instead\nof how many iterations are needed to recover the same performance.\nThe supervised learning experiments appear to be done\nusing the entire dataset while the RL experiments do\npresent a setting where the data is not the same.\n\n[Empirical results]\nI am also surprised by the empirical finding in Section 5.1\nthat T1 outperforms T2, since it seems like only optimizing\nthe parameters of the missing layer would be the best\napproach. I think that if a similarity metric was used\ninstead, T2 would be significantly better at finding the\nlayer that is the most similar to the layer that was removed.\n\nSome smaller comments:\n\n1. In Section 3.1, the definition of C_T does not use T explicitly\n   inside of it.\n2. In the last paragraph of Section 3.1 and first paragraph of\n   Section 3.2, N should be defined as an iteration that\n   reaches the best loss.\n3. The description of T3 does not say what method is used to\n   optimize the over-parameterized layer, is it T1 or T2?\n4. Why does T4 use T1 instead of T2?\n5. In the experimental setup, why is T2 applied with a different\n   learning rate schedule than the original training procedure?\n6. Why is T2 not shown in the AlexNet results for Figure 2?\n7. The dissimilar axes between the plots in Figure 2 and\n   Figure 3 make them difficult to compare and interpret.\n8. It's surprising that in Figure 3, the hardness of \\alpha=1.0\n   for T2 is 1.0 for everything.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting new nugget of a problem",
            "review": "The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.\n\nThe authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.\n\nI find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  \n\nMore broadly, there's an additional axis to the optimization problem which is \"How much does the training scheme know about the particulars of the problem?\", ranging from \"Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)\" to \"knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)\" to \"knows a little bit about the problem structure, and uses hyperparameter tuned ADAM\" to \"knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD\".\n\nModel completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.\n\nAn excellent contribution, and I'm excited to see follow-up work.\n\n\n\n* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}