{
    "Decision": {
        "metareview": "This paper proposes a method to compute embeddings of states and actions that facilitate computing measures of surprise for intrinsic reward. Though some of the ideas are quite interesting, there are currently issues with the experiments and the motivation.\n\nThe experiments have high variance across the 5 runs, with significant overlap of shaded regions representing just one standard deviation from the mean. It is hard to draw any conclusions about improved performance, and statements like the following are much too strong: \"For vision-based exploration tasks, our results in Figure 5 show that EMI achieves the state of the art performance on Freeway, Frostbite, Venture, and Montezuma’s Revenge in comparison to the baseline exploration methods.\" Further, the proposed approach has three new hyperparameters (lambdas), without much understanding into how to set them or their effect on the results. Specific values are reported for the different game types, without explanation for how or why these values were chosen. \n\nSimilarly strong claims, that are not well substantiated, are given for the proposed approach. This paper seems to suggest that this is a principled approach to using surprise for exploration, contrasted to other ad-hoc approaches (\"Other approaches utilize more ad-hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to approximate surprise.\"). Yet, the paper does not define surprise (say by citing work by Itti and Baldi on Bayesian surprise), and then proposes what is largely a intuitive approach to providing a good intrinsic reward related to surprise. For example, \"we show that imposing linear topology on the learned embedding representation space (such that the transitions are linear), thereby offloading most of the modeling burden onto the embedding function itself, provides an essential informative measure of surprise when visiting novel states.\" This might be intuitively true, but I do not see a clear demonstration in Section 4.2 actually showing that this restriction provides a measure of surprise. Additionally, some of the choices in Section 4.2 are about estimating \"irreducible error under the linear dynamics model\", but irreducible error is about inherent uncertainty (due to stochasticity and partial observability), not due to the choice of modeling class. In general, many intuitive choices in the algorithm need to be better justified, and some claims disparaging other work for being ad-hoc should be toned down. \n\nOverall, this paper is as yet a bit preliminary, in terms of clarity and experiments. In a further iteration, with some improvements, it could be a useful contribution for exploration in image-based environments. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Some interesting ideas with concerns about motivation and experiments"
    },
    "Reviews": [
        {
            "title": "Interesting paper about intrinsic rewards for exploration, via embeddings which improve mutual information",
            "review": "The paper proposes an approach for exploration via reward bonuses based on a form of surprise. The surprise factor is based on the next state of a particular transition, and the error in the embedding space to satisfy a linear dynamics formulation. The embedding space of the states and actions are optimized to increase the mutual-information in predicting next state, and current action - encouraging meaningful embeddings with more training, and hence gradual fading away of the extrinsic rewards.\n\nThe paper is mostly well-written, and the idea is interesting. The experimental results do show that the proposed reward augmentation leads to better performing policies, but the claims in the experimental section need to be less strong (\"outperforms the baseline by a large margin\" - Figure 4 - overlapping error bars; \"state of the art\" - Figure 5 - again, error bars, and no improvement in some domains.) But overall, I think the paper can be accepted as it is an interesting approach.\n\nBelow are some comments that I hope the authors address in their rebuttal, followed by some possible typos in the current draft.\n\n- Theorem 1 content placement: The organization here is rather unclear. Currently, you present Theorem 1, and then talk about using \"JSD instead of MI\". Maybe this is a last minute mistake. In either case, it is strongly suggested that the section be reworked to be clearer.\n\n- JSD is upper bounded by ln(2); your bounds (7) and (8) would change consequently too.\n\n- Training regime employed:\n  3 epochs-512 minibatch -> assuming distinct minibatches are sampled, (512x3) samples used\n  Collected (5k*500; sparsehalfcheetah)/(50*500; swimmercatcher)/(100k*4500; atari)\n  Is this the sample usage for training? Axis labels for all plots are missing -- specifically scale of x-axis.\n  Commenting on the sample complexity -- especially as the embedding network seems easy-to-train (or insufficiently trained), would be good; optimizing a lower-bound insufficiently leads one to doubt if the bound is meaningful at all. Is the huge batch of samples mostly used in TRPO/RL part of the infrastructure?\n\n- Discussing extrinsic rewards: the pros. vs. cons of the two reward formulations, why both are used etc. would be useful.\n\n- Embedding dimension: d=8 in Gravitar and Solaris, but performance is less significant (no significance) in these domains. Is this due to insufficient training?\n\n- RL method: Including details about form of TRPO used in appendix would be good (vine/single-path). Further if entropy regularization is used, how does the exploration interplay work.\n\n- A.2 is an interesting section. A linear dynamics model being effective in MuJoCo tasks seems plausible. But an Atari example is definitely more interesting. Therefore this section can be clearer - specifically distinction between residual error and sample error.\n\nTypos:\n- Appendix \\lambda parameters unclear\n- Appendix step-size information contradictory.\n\n\nPost-response comment: while I do think the approach is interesting, the utility of it is mostly demonstrated currently through empirical experiments. These experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach. Further, upon highlighting this in my review, the authors disagree and think it’s empirical validity is rather superior. This leaves me concerned, and after thinking about it further, I do not think this is sufficient for acceptance. Therefore, I’m reducing my score to a 5.\n\nPS: the characterization of irreducible error as a product of the limitation of a linear model may be inaccurate.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of EMI",
            "review": "This is a very interesting paper about a novel approach to exploration in agents with state and action representations, making heavy use of recent progress in the use of deep learning for estimating and maximizing mutual information, as well as introducing an approach to model the latent space dynamics with a linear models with sparse errors.\n\nA closely related work which is not mentioned is the work of Thomas et al 2017 arXiv:1708.01289 where they also maximize mutual information between distributed representations of actions (policies, actually) and of distributed representations of changes in the state (as the result of applying the policy).\n\nThe phrase 'functionally similar states' is used several times and would require a bit of explanation.\n\nI would also like to see more motivations for the two different reward functions r_e and r_d, and why one should be computed before the update while the other should be computed after.\n\nRegarding the experiments, and this is probably the weakest part of this paper, I would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided). Only one (EX2) was compared. The comparison with TRPO is without exploration (if I understand well, but should be stated clearly).  It's also not clear how these results compare to the best reported results on these games (whether or not exploration is used).\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of EMI",
            "review": "This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward.\n\nI found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good.\n\nI had some issues with the motivations of some of the loss functions. \n- The JSD-based objective makes sense, but I don't think it's correct to call it an \"approximation\" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals.\n- It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings.\n- For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)?\n\nOther notes:\nPage 2:\nFigure 1 is awfully confusing. Could this be clarified a little bit? I’m not sure what the small dots or their colors are supposed to represent.\n\nCould diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)?\n\nWhy were the vision experiments stopped at 500 x 100k (500 million) frames?  I can’t validate the SOTA claims, but it seems like the model is still improving: are there’s further experiments?\n\nAn ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}