{
    "Decision": {
        "metareview": "\npros:\n- nicely written paper\n- clear and precise with a derivation of the loss function\n\ncons:\n\nnovelty/impact:\nI think all the reviewers acknowledge that you are doing something different in the neural brainwashing (NB) problem than is done in the typical catastropic forgetting (CF) setting.  You have one dataset and a set of models with shared weights; the CF setting has one model and trains on different datasets/tasks.  But whereas solving the CF problem would solve a major problem of continual machine learning, the value of solving the NB problem is harder to assess from this paper...  The main application seems to be improving neural architecture search.  At the meta-level, the techniques used to derive the main loss are already well known and the result similar to EWC, so they don't add a lot from the analysis perspective.  I think it would be very helpful to revise the paper to show a range of applications that could benefit from solving the NB problem and that the technique you propose applies more broadly.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "good work but more is needed to have impact"
    },
    "Reviews": [
        {
            "title": "The technique in this paper feels more or less identical to the ideas from Kirkpatrick et al (catastrophic forgetting). The difference seems to be one of application (different tasks vs same task), and as such feels like an incremental advance.",
            "review": "There is certainly additional novelty in that this paper focuses on models performing same/identical tasks (compared the results from the catastrophic forgetting paper), and because this model more clearly delineates the parameters that are shared across the models, vs those that are not. But both of those advances feel incremental.\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Not sufficiently novel",
            "review": "\n- This \"neural brainwashing\" is catastrophic forgetting. Technically speaking, this is catastrophic forgetting. \n\n- Also, some works targeting NAS (which I reckon should as well be cited due to being quite related) have targeted similar forgetting issues, e.g. Xu and Zhu, NIPS 2018 \"Reinforced continual learning\". It is nice to enrich the literature with new terms, when there is a need to. In my opinion, in this particular case, neural brainwashing is catastrophic forgetting. \n\n- Forgetting is not necessarily an \"individual problem\", sticking to the language used in the third paragraph of the first page. The same applies to \"single-model forgetting\". \n\n- page 1 \"Our work is the first of which we are aware to identify neural brainwashing and to propose a solution.\": According to the authors' argument, this is the case. Again, mine is different.\n\n- Novelty w.r.t. works tackling catastrophic forgetting, most notably EWC, is minimal. Also, comparing to other state-of-the-art algorithms targeting catastrophic forgetting can further enrich the experiments. \n\n- 3.1.1. On a technical level, there is no inherent difference, between EWC and the proposed algorithm. \n\n- Writing can improve, both in terms of the flow and the language. There are also a few typos, e.g. in the first line of the caption of Figure 2.\n\n- Apart from the aforementioned issue (comparing to other state-of-the-art catastrophic forgetting algorithms), the experiments are rigorously prepared. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "a good work on one shot model training",
            "review": "This paper discusses the phenomena of “neural brainwashing”, which refers to that the performance of one model is affected via another model sharing model parameters. To solve the issue, the authors derived a new loss out from maximizing the posterior of the parameters. With the new loss, the neural brainwashing is largely diminished. \nThe derived new loss looks meaningful to me and I think this is a valuable work for handling the weights coadaptation between two neural models, which with no doubt will bring great interests within the community of neural architecture search.\n\nHere are some comments on the aspects that this paper can be improved: \n\n1)\tA very important related work [1] is missed in this paper. [1] discussed the properties of “one-shot model”, which means that several different architectures are unified into the same model by sharing model weights. Furthermore, [1] discussed “neural brainwashing” (although not with the same name) and how to handle it in a very simple way (by randomly dropping path). This definitely should be a baseline to compare with. In addition, a very recent work [2] also leverages model sharing to conduct neural architecture search.\n\n2)\tAlthough I understand that to improve accuracy of NAS is not the main goal of this paper, the baseline number to be improved over is too weak. For example, 4.87 of CIFAR10 in ENAS. Per my own hands on experience, it does not need too many hyperparameter tuning of ENAS to obtain < 4% error rate. Please provide more convincing baseline numbers and supporting evidences of the better performance of WPL in NAS.\n\n\n[1] Bender, Gabriel, et al. \"Understanding and simplifying one-shot architecture search.\" International Conference on Machine Learning. 2018.\n[2] Luo, Renqian, et al. \"Neural architecture optimization.\" NIPS (2018).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}