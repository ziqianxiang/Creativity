{
    "Decision": {
        "metareview": "The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "The technical contribution is minor",
            "review": "Summary: \nThe authors investigate the Breiman’s dilemma in the context of deep learning. They show generalization bounds in terms of the margin distribution. They also perform experiments showing the Breiman’s dilemma.\n\nComments: \nI am afraid the authors miss an important related paper:\n\nLev Reyzin, Robert E. Schapire:\nHow boosting the margin can also boost classifier complexity. ICML 2006: 753-760\n\nReyzin and Schapire explain the Breiman’s dilemma based on base classifiers’ complexity. In particular, their experiments show that arc-gv tends to use more complex decision trees than AdaBoost while it achieves better margin distribution over sample. That is, not only margin distribution, but also the complexity of base classifiers’ class matters. This is already explained by known Rademacher complexity based margin bounds.\n\nAs for quiantile-based analyses on margin bounds the following result is known:\n\nLiwei Wang et al: A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin, \nJournal of Machine Learning Research 12 (2011) 1835-1863.\n\nThey proved a shaper bound using the notion of equibrium margin. The authors should compare the presented results with this. \n\nThe technical results of the paper look quite similar to known margin bounds and I am afraid the contribution is minor or redundant.\n\nAfter the rebuttal:\nI read the authors' comments and understand more the technical results. I raised my score. But I still feel that the techniccal contribution is a bit weak.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper started from Breiman's dilemma and showed that it relies on dynamics of normalized margin distribution.",
            "review": "The authors found that general generalization bounds fail to capture the ramp loss. However, once the network scaled by its Lipschitz constant, it becomes efficient to get an upper bound of generalization error, while also needs to trade-off the constant in the margin error. Due to the limitation of fixing the constant in margin error, the authors  tried to use the quantile margin to change the bound, which is easy to tune the hyper-parameter. They also conducted the experiments that the quantile margin generalization bound could be used to predict the tendency of loss curve both in training and test in some sense.\n\nIt's really an interesting work to provide a way for early stopping and to show the quantile margin maybe a substitution of tendency in training error as well as test error.\n\nQuestions: \n\nIt could be difficult to judge from the phase transition, if exists, in the evolution of normalized margin distributions curve. Maybe  some quantitative descriptions are needed. \n\nBesides, the authors' quantile margin bound (Theorem 2) shows the upper bound of margin (or say margin error). But the bound is not direct to support the powerful experiments results, the relationship between the tendency of quantile margin, training and test error.\n\nTypos:\n In Eqn. (10), the first $f_t$ should be $\\widetilde{f}_t$ .\nIn Eqn. (9) and (11), there is  $1$.\nIn Proof in Lemma A.1, the convolution operator is $x(v)$ not $x(u)$, since Lemma is also true.\nIn Proof in Lemma D.4, though the proof is same in the book `Foundations of machine learning' by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, please check the typo.\nIn Proof of Proposition 1, lack $\\frac{1}{n}$ in Rademacher complexity.\nIn Proof of Theorem 1, maybe you should take $\\mathbb{E}$ not $\\mathbb{P}$ before $\\ell_{\\gamma_1,\\gamma_2}(\\xi(\\widetilde{f}(x,y))))$.\nIn Proof of Theorem 2, page 18 the last line in the equation, why can the second term after divided by $L_f$ bounded by $L$, maybe need some conditions or I missed something.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": ".",
            "review": "The submission explores Breiman's dilemma: training margin is not always a good predictor of test error.\n\nIn particular, the authors show that:\n\n- For under-parametrized CNNs, the training prediction margin is a good predictor of the test error.\n- For over-parametrized CNNs, the training prediction margin is not a good predictor of the test error.\n\nThroughout the submission, I suspect that the authors compute the \"functional margin\", that is, the difference between the largest label score and the second largest score, for correctly classified examples. Functional margins ignore the smoothness of the underlying function, a critical factor for generalization. For instance, the function f(x) = 1[x > 0] has large functional margin, but any perturbation around the x-origin would drastically change the prediction. For this reason, I think the authors should consider the \"geometrical margin\" instead, which is unfortunately difficult to compute for general neural networks. Their theory tries to reflect on this issue by using spectrally-normalized bounds, but the practice ignores this issue completely (as far as I can tell).\n\nTherefore, we may be looking at the wrong statistic to predict generalization error. Is Breiman dilemma solved by re-defining margin properly? Geometrical margin can be computed in closed-form for linear classifiers, so perhaps this would be a first step in this investigation.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}