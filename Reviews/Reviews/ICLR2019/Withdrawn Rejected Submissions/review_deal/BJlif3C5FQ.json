{
    "Decision": "",
    "Reviews": [
        {
            "title": "Essentially a better retrieval method",
            "review": "The paper tackles the problem of multiple-choice reading comprehension problem, notably ARC, along with two datasets that the authors create from RACE and MCScript. The paper proposes to retrieve a passage by concatenating only the essential terms with each choice, and then use a reading model to obtain the answer from question-choice-passage triple. The paper obtains ~3% improvement over the previous SOTA on ARC and non-trivial improvements on the other two datasets that the authors created.\n\nThe paper is generally well-written and I did not have much problem understanding it. Also, the result is encouraging in that it was able to improve ARC by identifying essential terms and then just using them (concatenated with each choice) to retrieve a good passage. However, I have two concerns:\n\nOne is that the test dataset is very small. If I understood it correctly, it only has 200 questions (10% of 2k). 3% improvement hence corresponds to 6 questions. Since the dataset is 4-choice, getting 6 more questions right is certainly within the range of randomness. So it is not clear if the improvement is statistically significant. While the model also gets an improvement over the other two datasets, they are created by the authors and only have relatively weak baselines (which were implemented by the authors).\n\nThe second weakness is that the model is essentially proposing a better way to retrieve a passage. TF-IDF is a completely unsupervised method of weighing the importance of each word. Hence it is not surprising that the supervision of essential terms (on their importance) improves retrieval accuracy. There is nothing much new in the reader model (quite standard approaches for MRC), so I am worried about the novelty of the paper.\n\nOverall, while I think it is encouraging to know that a carefully designed retrieval model can improve ARC, the contribution does not seem to be significant enough for ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A interesting Ideas, but the evaluation is not enough. ",
            "review": "This paper presents a retriever-reader model for open domain QA, which follows the pipeline of 1) selecting the keywords/essential terms of a question; 2) retrieving related evidence for passage construction; and 3) predicting final answer via a proposed attention-enhanced reader.  \n\nAlthough the proposed method achieves the good performance on ARC, I have concerns about both of the model design and evaluations.  \n\nI am wondering how useful the essential term selection is?  The module is trained as a separate binary sequence tagging task on the dataset of Khashabi.   I am wondering the difference between the Khashabi's dataset and these datasets used for evaluation in this paper.   Authors need to show how well the trained essential term selection module on Khashabi can be transferred on the question of ARC, RACE, etc.   In the paper, authors mentioned that they removed questions of that less than a specific number of words for both RACE and MCScript dataset to ensure a sufficient number of retrieved data.   Such a kind of data manipulation is unacceptable and unfair to other baseline methods.   Authors need to demonstrate the generated essential terms on these short questions.  My guess is that the essential term selection fails on these short questions due to the mismatch between the source dataset it trained on and the target dataset.   The ablation study in table 8 does not convince me that the essential term selection has a significant effect.  \n\nThe novelty in terms of the RC model of this paper is limited.   Authors claim one of their contributions is the design of the attention and fusion layers that aggregates information among questions, passages, and answers choices.  However, the design is heuristic, complex and without any intuition.   Three Match-LSTM-like sequence encoders are used for sequence modeling, which makes the number of the parameters of the model large (authors needs to report the total number of parameters of their model for the experiment).   The sequence encoder is followed by the fusion layer, choice interaction and the output layer.    I cannot identify the significance of each of the design to the final performance.  Although authors had an ablation study shown in table 6, it is hard to understand.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Further experiments to support the paper contributions?",
            "review": "This paper has made two major contributions: (1) a new neural reader architecture for multi-choice QA; (2) it is the first to introduce essential term selection to open-domain QA, to the best of my knowledge. The above two proposed modules (reader and selector) are not very novel, but are still valid contributions to me. Experiments on the ARC dataset shows that (1) the proposed reader itself improves over the state-of-the-art on the leaderboard; (2) introducing the essential term selector further improves the above results by about 2%.\n\nAlthough the paper also provides additional experiments on other datasets, I feel that the contributions of the proposed methods are not sufficiently verified. I would suggest the authors consider the following further experiments that I believe could improve its ratings:\n\n(1) The proposed reader works very well on ARC. However, besides the BiDAF, there is no comparison between the proposed reader and previous models on datasets other than ARC. In order to know whether the result generalizes or not, I think the authors should conduct experiments on the regular RACE or other multi-choice QA datasets, to fully test the reader model.\n\n(2) It is not clear whether the essential term selector could help on datasets other than science questions. Again, the authors reported results on two other datasets. However, on neither of these datasets the ET-RR was compared with ET-RR (Concat). Therefore, I have concerns that the proposed framework may only be significant on the ARC dataset.\n\nMoreover, it will be interesting to see whether the essential term selector can be learned from distant supervision. For example, using REINFORCE to learn the selector model with the rewards from the end-task performance. The current framework heavily relies on the supervised training data from (Khashabi et al., 2017), which may limit its usage to other datasets.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}