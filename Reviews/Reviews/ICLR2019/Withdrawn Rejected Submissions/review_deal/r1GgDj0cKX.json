{
    "Decision": {
        "metareview": "This paper propose to obtain high pruning ratio by adding constraints to obtain small weights. Reviewers have a consensus on rejection due to not convincing experiments and lack of novelty.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": " not convincing experiments and lack of novelty"
    },
    "Reviews": [
        {
            "title": "network pruning in training",
            "review": "This manuscript presents a method to prune deep neural networks while training. The main idea is to use some regularization to force some parameters to have small values, which will then be subject to pruning. \nOverall, the proposed method is not very interesting. More importantly, the manuscript only lists the percentage of pruned parameters, but did not compare the actual running time before and after pruning. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs more formalism regarding the regularization path",
            "review": "\n\n==Major comments==\n\nYou need to better explain how the regularization path is obtained for your method. It is not clear to me at all why the iterates from lines 5-8 in Alg 2 provide a valid regularization path. \n\nI am very confused by section 3.4. Is the pruning strategy introduced in this section specific to LBI? In other words, is there some property of LBI where the regularization path can be obtained by sorting the parameters by weight? It seems like it's not specific to LBI, since you use this pruning strategy for other models in your experiments. Is this the right baseline? Surely there are other sparsification strategies.\n\n\nHow/why did you select 5e-4 for lambda? You should have tuned for performance on a validation set. Also, you should have tuned separately for each of the baselines. There is no reason that they should all use the same lambda value.\n\nCan you say anything about the suboptimality of the support sets obtained by your regularization paths vs. if you had trained things independently with different regularization penalties?\n\nI am very concerned by this statement: \n\"However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers.\"\nThis is important. Do you have idea of why it is true? This instability seems like an obstacle for large-scale deployment.\n\nAre your baselines state of the art? Is there anything discussed in the related work section that you should also be comparing against?\n\n\n==Minor comments==\nThe difference between Alg 2 and 3 is mechanical and should be obvious to readers. I'd remove it, as the notation is complex and it doesn't add to the exposition quality. Instead, you should provide an algorithm box that explains how you postprocess W to obtain a sparse network.\n\nYour citation format is incorrect. You should either have something along the lines of \"foo, which does X, was introduced in author_name et al. (2010)\" or \"foo does X (author_name, 2010).\" Perhaps you're using \\citet instead of \\citep in natbib.\n\nAlgorithm box 1 is not necessary. It is a very standard concept in machine learning. \n\nOn the left hand of (5), shouldn't Prox be subscripted by L instead of P?\n\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "pruning in training using lasso and split LBI penalties. Not convincing experiments",
            "review": "This paper introduces an approach to pruning while training a network. This is interesting and experiments show interesting results in several datasets including ResNet18\n\nHere are a few comments:\n\n - Pruning or regularization for compression is not new. Alvarez and Wen have used group lasso types as suggested in the paper and some others such as Alvarez and Salzmann (Compression aware training NIPS 2017) and Wen (Coordinating filters ICCV2017) have used low-rank type of while training. How is this different to those? They also do not need any sort of fine tuning and more importantly, they show this can scale to large networks and datasets. \n\n- These last two works I mentioned promote redundancy, similarly to what is suggested in the paper. Would be good to get them cited and compared. Important from those is the training methodology to avoid relevant overheads. How is that happening in the current approach\n\n\n- While I like the approach, would be nice to see how this scale.  All for methods above (and others related) do work on full imagenet to show performance.  For ResNet, cleaning the network is not really trivial (near the block), is that a limitation?\n- Why limiting experiments to small networks and datasets? Time wise, how does this impact the training time?\n- Why limiting the experiments to at most 4 layers? \n- I am certainly not impressed by results on fully connected layers in MNIST. While the experiment is interesting does not seem to be of value as most networks do not have those layers anymore.\n\n- Main properties of this approach are selecting right filters while training without compromising accuracy or needing fine tuning. While that is of interest, i do not see the difference with other related works (such as those I cited above)\n\n- As there is enough space, I would like to see top-1 results for comprehensive comparison. \n\n- I think tables need better captions for being self-contained. I do not really understand what i see in table 5 for instance. \n- Droping 13% of top5 accuracy does not seem negligible, what is the purpose there? Would also be interesting to compare then with any other network with that performance. \n- What about flops and forward time? Does this pruning strategy help there?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}