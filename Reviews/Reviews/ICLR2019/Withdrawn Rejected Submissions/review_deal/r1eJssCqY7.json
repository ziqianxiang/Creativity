{
    "Decision": {
        "metareview": "All reviewers agree in their assessment that this paper has merits but is not yet ready for acceptance into ICLR. The area chair commends the authors for their responses to the reviews.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Reject"
    },
    "Reviews": [
        {
            "title": "Review of submission 585",
            "review": "Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs.\n\nStrengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;\n\nWeaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.\n\nMinor typos:\n(abstract)\n- \"NN has achieved\" => \"Neural Networks have achieved\"\n- \"performances\" => performance\n- \"explicitly leverages\" => \"explicitly leverage\"\n\nQuestions:\n- (top of p. 2) What exactly is the difference between \"implicit feature combinations\" and \"explicit (?), expressive feature combinations\"\n- (top of p. 2) \"encourage parameter sharing\" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]\n- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.\n\n\nRecommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An applied paper addressing a significant problem and research direction but missing the novel foundations getting to the bottom of the problem.",
            "review": "This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN). The intended research direction on tabular data is essential and promising. However, the proposed technique does not seem to be handling the problem foundationally well. It seems heavily dependent on GBDT. It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results. Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem. \n\nPros:\n-This is a crucial line of research direction that aims to make DNNs applicable to many real-world problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search.  \n-The starting point of using GBDT seems like a good choice. \n-The Paper is mostly well written except occasional repetitions and missing acronym definitions.\n\nCons:\n-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well. I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented. The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times). This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.   \n-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.\n-In the provided benchmark data sets the depth of the analysis seems to be enough. However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features  (e.g., unstructured raw text for the search queries.) In such scenarios, it had been showed that wide-and-deep NNs perform decently. However such problems are entirely missing in the results section. I also think that this is a lost opportunity for the authors as they could be showing that it is the NN part contributing.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea",
            "review": "The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning. My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small. \n\nAlso, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost. \n\nIn chapter 2, related work. The authors state that \"tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data. \n\nTo me these two reasoning statements are not particularly convincing. One could also say:\n\nNN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...\n\nActually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}