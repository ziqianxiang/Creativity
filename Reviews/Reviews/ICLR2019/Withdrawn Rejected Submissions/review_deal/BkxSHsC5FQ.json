{
    "Decision": {
        "metareview": "The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation).",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "meta-review"
    },
    "Reviews": [
        {
            "title": "SupportNet offers a new method to perform class incremental learning with an support vectors from the last layers and subsequent two learning constraints, showing some improved performance compared to previous approaches.",
            "review": "Summary:\nThe authors offer a novel incremental learning method called SupportNet to combat catastrophic forgetting that can be seen in standard deep learning models. Catastrophic forgetting is the phenomenon where the networks don’t retain old knowledge when they learn new knowledge.  SupportNet uses resnet network with 32 layers, trains an SVM on the last layer and the support vector points from this SVM are given to the network along with the new data. Furthermore, two regularizers, feature and EWC regularizer, are added to the network. The feature regularizer forces the network to produce fixed representation for the old data, since if the feature representation for the old data changes when the network is fine-tuned on the new data then the support vectors generated from the old feature representation of the old data would become invalid.  The EWC regularizer works by constraining parameters crucial for the classification of the old data, making it harder for the network to change them. SupportNet is compared to five methods (all data: network is re-trained with new and old data, upper bound for performance, iCaRL: state-of-the-art method for incremental learning, EWC: Only EWC regularizer added, Fine tune: Only new data, Random guessing: Random guess to assign labels) on six datasets (MNIST, CIFAR-10, CIFAR-100, Enzyme Function Prediction, HeLa Subcellular Structure Classification, Breast Tumor Classification). It shows some improvement in overall accuracy with each newly added class when compared to iCaRL, EWC, Fine Tune and Random guessing.  Additionally, they show that overfitting for the real training data (a chosen subset of old data and the new data) is a problem for the competition iCaRL and affects SupportNet to a much lesser degree.\n\nPros:\n(1)\nThe authors propose a sensible approach, which is also novel to be best of our knowledge, using SVM to select support data from old data to be fed to the network along with the new data in the incremental learning framework to avoid catastrophic forgetting. Additionally, they offer a feature regularizer that penalizes the network for changing the feature representation of the support data when training the network on new data and an EWC regularizer that constrains the parameters that are crucial for the classification of the old data and makes it harder to change them. \n(2)\nThe authors use six different datasets and several other approaches (subsets of their method’s components, other competing methods) to show these three components alleviate catastrophic forgetting and show improvement in overall accuracy.\n(3)\nThe paper is well written and easy to follow. \n\n\nCons:\nMajor Points:\n(1)\nTo show that the method proposed in the paper addresses catastrophic forgetting, in addition to the overall accuracy shown in Figure 3, it is also necessary to show the accuracy of different models on old classes when new classes are added to the network. This will strengthen the argument that the improvement in accuracy is indeed due to correct classification on old data.\n (2)\nThe authors claim that iCaRL suffers from overfitting on real training data (section 4.1) however Table 2 shows iCaRL only on the enzyme function prediction which is also the dataset where the difference in performance between iCaRL and SupportNet is the largest. To support the general overfitting claim made in section 4.1, the authors should repeat this analysis on any of the other five datasets where the performance difference between the two methods is much smaller.  SupportNet also suffers from overfitting (Table 3, Accuracy: test data: 83.9%, real training data: 98.7%) although to a lesser extent than iCaRL.\n(3)\nThe individual impact of the support points and the joint impact of support points with feature regularizer on accuracy is not assessed. To prove their usefulness, add two methods to Figure 3:\n(a)A method that uses support points without any regularizer.\n(b) A method that uses support points with just the feature regularizer. \n\nOther points:\n(1) \nIn section 2.3.2, EWC regularizer, Eq. 9: We think F(theta_new) should be F(theta_old) since we want to constrain parameters crucial for classification of old data and should be computing Fisher Information for the old parameters. \n(2)\nIn section 2.1 Deep Learning and SVM: additional steps are needed to show how Eq. 3 is derived from Eq. 2.\n(3)\nIn section 2.1 Deep Learning and SVM: In the line before Eq. 4. “t represtent” instead of “t represents”.\n(4)\nFigures are small and hard to read. Please increase the size and resolution of the figures.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but limited comparisons",
            "review": "This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Merging the support vector data and new data, the network can keep the knowledge on the previous task. The use of support vector concept is interesting, but this paper has some issues to be improved.\n\nPros and Cons\n  (+) Interesting idea \n  (+) Diverse experimental results on six datasets including benchmark and real-world datasets\n  (-) Lack of related work on recent catastrophic forgetting\n  (-) Limited comparing results\n  (-) Limited analysis of feature regularizers\n \nDetailed comments\n- I am curious how we can assure that SVM's decision boundary is similar or same to NN's boundary\n- SupportNet is a method to use some of the previous data. For fair comparisons, SupportNet needs to be compared with other models using previous samples such as GEM [Lopez-Paz and Ranzato, 2017]. \n- Following papers are omitted in related work:\n  1. Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n  2. Shin et al. Continual Learning with Deep Generative Replay, NIPS 2017.\n   Also, the model needs to be compared with two models.\n- There is no result and analysis for feature regularizers. As the authors referred, the features of support vector data continuously change as the learning goes on. So, I am curious how the feature regularizer has effects on the performance.  This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds   \n- The authors used 2000 support vectors for MNIST, Cifar-10, and Cifar-100. However, this size might be quite large considering their difficulty. \n- How is the pattern of EwC using some samples in the old dataset?\n- iCaRL was evaluated on ImageNet. Is there any reason not to be evaluated on ImageNet? \n- What kind of NNs is used for each dataset? And what kind of kernel is used for SVM?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Redundant idea",
            "review": "This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. Specifially, these representative samples for each task are selected as support vectors of a SVM trained on it. The proposed method, SupportNet, is validated on a continual learning task of a classifier against two existing continual learning approaches, which it outperforms.\n\nPros\n- Idea of using SVM to identify the most important samples for classification makes sense.\n\nCons\n- The idea of storing a small subset of a original dataset for each task has been already explored in [Nguyen et al. 18], and thus is not novel.\n- Thus the contribution of this work reduces to the use of SVM to identify the most important samples, but the effectiveness of this approach is not validated since it does not compare against [Nguyen et al. 18].\n- Also it leaves out many of the recent work on continual learning.\n- The idea of using SVM for identifying important samples is not very attractive since an SVM will have a very different decision boundary from the one from the trained DNN.\n- Also this method is only applicable to a classification task and not to other tasks such as regression or RL.\n\nThus considering the lack of novelty and experimental validation, I recommend rejecting this paper.\n\n[Nguyen et al. 18] Variational Continual Learning, ICLR 2018",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}