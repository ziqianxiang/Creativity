{
    "Decision": "",
    "Reviews": [
        {
            "title": "Potentially interesting ideas but exposition and motivation are too imprecise",
            "review": "The authors suggest a novel approach, ExL to adversial training using multiplicate noise that is learned jointly with model parameters using SGD. They propose a likelihood framework to interpret why the approach is successful. PCA of intermediate layers is used to suggest that ExL trained NNs are more robust because they better cover space around the observed data manifold. Results on three canonical datasets using blackbox and whitebox adversarial attacks suggest that ExL can be helpful in defending against BB attacks, and is easily combined with other adversial training approaches such as PGD to further improve robustness. \n\nI think there are some interesting ideas in the paper but the motivation is too hand-wavey and the method exposition is insufficient. Firstly, refering to N as \"noise\" seems problematic. N is learned so is more akin to a latent variable than noise. In particular it is not random at any point apart from initialization (which is equally true for model parameters). Additionally it is very strange to me that these \"learned masks\" are fixed in position in the mini-batch, since an index in the minibatch has no external meaning. The authors don't say whether the data points are permuted at each epoch, which is important since it effects whether the same data point at subsequent epochs uses the same mask (I don't even know from the exposition whether this is intended or not). \n\nSecondly, the \"likelihood framework\" is very hand-wavey. When modeling P(Y|X) we think of Y and X as random variables, with the training set (X_train, Y_train) as samples. Nothing in this says that P(Y|X) should not hold for X other than X_train, e.g. X=A. Thus the introduction of P(Y|X,A) is superfluous. While Eq 1 is mathematically correct it is therefore pretty meaningless in terms of understanding what ExL is doing. The text does little to help. \n\nThirdly, \"Explainable\" in the title is dangerously close to \"interpretable\", which ExL is certainly not. The N give only the vaguest sense of the features being used in the data (see Fig 1b). \n\nThere might be Bayesian/likelihood-based interpretation of ExL, other than that proposed by the authors. I was surprised the similarity to dropout wasn't mentioned. Yarin Gal's work pointed out the similarity between dropout and variational Bayes with a specific variational posterior, and the ExL looks a look like opitmizing the resulting objective including per pixel dropout weights (although not stochastically which is strange). Alternatively one could think of X*N as latent variables and we are modeling P(Y|X*N). This could be interpreted as modeling noise in X for example. \n\nThe connection to dropout makes me suspicious that while ExL is proposed as an adversarial training method it is really just performing more effective regularization, which should in itself smooth the prediciton surface and improve robustness to adversarial examples. It would be feasible to test this, for example by comparing different formes of regularization with and without ExL. \n\nWhy threshold grad L < 0 when learning N? What stops the model from just setting N=0? \n\nMinor comments\n- Abstact: \"prove\" -> \"show\" (proofs require proofs!)\n- Don't re-use N as the number of training images. \n- page 5. X=X+alpha... ugh. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Trained multiplicative noise improves robustness to adversarial attacks",
            "review": "This paper includes multiplicative noise N in training data, such that it trains a model on X\\timesN.  The model then trains on both model parameters theta and on the noise itself in an effort to achieve adversarial robustness\n\nQuality\n- The empirical results appear sound, which suggest that multiplicative trained noise is a sensible approach\n- The writing that leads up to these results is in my opinion problematically vague, as it makes a variety of loose claims and theoretical connections and uses imprecise language.  This results in scientific imprecision, which I detail below in \"clarity\".\n- Some key choices are unsupported.  For example, the choice of fixing noise across batch (as in, there are k noise maps N that are reused across each minibatch) is unclear.  Certainly if you want to train these noise maps, some reuse is required, but the  given sentence \"since we want to learn the noise, we use the same k noise templates across all mini-batches...\" does not explain this critical choice.\n- What does it mean to train a noise map?  I know what it means mechanically, but there is a vague set of explanations that do not leave an empirical or theoretical understanding of why this is a sensible choice.  Connecting this to similarly ill-defined comments on \"excessive linearity\" or \"off manifold\" deepen this issue.\n\nClarity\n- The mechanics of the algorithm are sufficiently clear, but the interpretation is deemed highly vague, such that the result is a rather simple algorithm that is wrapped up with loose and unclear explanations.  Some examples are below:\n- critical terms are introduced without technical explanation.  For example, \"model viability\" is introduced in \\emph{} as the key questoin in the third paragraph of the introduction.  However, after reading this sentence several times, I don't know what this question is trying to ask.  Is it just \"we seek to improve robustness to adversarial attacks\"?  Is \"such out-of-sample data\" meant to point to \"off manifold\" points?  Other terms like \"inculcates\", \"off manifold\", \"inherits\", \"right prediction vs right explanation\", etc. add to this issue.\n- On that point, the term \"off manifold\" is used often but is highly vague.  We all know what this means colloquially, but if you want to make a scientific point about it, there needs to be rigor applied to this definition.\n- The likelihood perspective (section 2.2) is not rigorous.  These distributions are undefined to a problematic extent.  For example, p(Y|X,\\theta) is clear enough, but then what is p(Y|X,A,\\theta)?  I understand A is the adversarial inputs, but what *specifically* is this model?  How *specifically* does A represent the adversarial inputs?  Absent this level of definition and detail on this and other points, the likelihood section is deemed highly vague. \n\nOriginality and Significance\n- At a basic level, this manuscript offers multiplicative and trained noise maps.  That is a meaningful (albeit small) contribution.  The results are rigorous enough to make it interesting in its own right, but the remainder of the choices are justified via vague language rather than rigorous empiricism.  This limits the perceived significance.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Training networks with multiplicative input noise: interesting but with questions",
            "review": "In this paper, a so-called explainable adversarial learning approach is proposed. It is shown that the use of multiplicative input noise can enhance the robustness of neural networks. My detailed comments are as below. \n\n1) The definition of 'Explainable' is not convinced. The authors claimed that \"the model not only finds the right prediction but also the right explanation. Noise inculcates this explainable behavior by discovering some knowledge about the input/output distribution during training.\" The distribution of the learnt noise is not surprising since by minimizing the training loss, the noise should reflect the example pattern. I am not convinced about the name 'explainable adversarial learning'.\n\n2) In ExL (Algorithm 1), why not consider the affine transformation X N_1 + N_2, where both N_1 and N_2 are learnt from the training process? Is this better than X N_1? What is the rationale behind using universal noise pattern over different minibatches. \n\n3) The proposed ExL framework is quite similar to training over perturbed examples. Here the perturbation is given by X \\times N.  It is expected that ExL will not outperform adversarial training but will outperform the plain training in robustness. From this perspective, I feel that the results are not very impressive. \n\n4) The study on Sec. 2.3 via PCA is nice and Figure 3 is informative. \n\n5) Since the proposed ExL training framework is scaled to large datasets, it is better to cover more experiments, e.g., ImageNet. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}