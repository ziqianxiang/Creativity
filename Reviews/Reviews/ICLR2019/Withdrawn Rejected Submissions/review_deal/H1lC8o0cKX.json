{
    "Decision": {
        "metareview": "This paper is borderline for publication for the following reasons:\n1) the title is misleading. The majority of the ICLR audience understands by \"spatial structure\" the structure of the external 3D world, as opposed to the position of the sensors in the internal coordinate system of the agent. Though the authors argue that knowing the positions of the sensors eventually leads to learning the 3D world structure, this appears like a leap in the argument. \n2) The equation s=\\phi(m) described a mapping from robot postures to sensory states. This means the agent should remain within the same scene. The description of this equation in the manuscript as \"The mapping \u001e\u000f can be seen as describing how “the world” transforms changes in motor states into  changes in sensory states ...\" makes this equation appear more general than what it is. s'=\\psi(s,m) would be better described by such sentence.\n\n\n",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "uncertain link between inferring sensor locations and spatial structure "
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper investigates the exploratory conditions under which spatial representations will emerge as a byproduct of learning to predict the next sensory observation. In particular, the authors test three exploratory conditions:\n(i) When the set of sensory-motor interactions (st, mt, s_{t+1}, m_{t+1}) are inconsistent. \n(ii) Environment is static and set of sensory-motor interactions are consistent. \n(iii)  Environment is dynamic and the set of sensory-motor interactions are consistent. \n\nAuthors measure the quality of learned spatial representations by computing disparity between the set of agent positions and the corresponding embedding of motor commands learned by predicting the next sensory observation. They conclude that under condition (iii), the disparity is minimum -- i.e. the agent is best able to discover the spatial structure of it environment. \n\nPhilosophically, I love the direction of this work -- understanding the origins of our spatial representations. But, I am concerned with the delivery. My concerns/questions are as following:\n\n(a) Firstly, the writing is too verbose and vague without clarifying the details and there are too many references to Laflaquiere et al. I would recommend the authors to be more precise, i.e. define topological/metric invariants, clarify how in-consistent sensory/motor pairs are sampled in MTM condition and how environment is perturbed in MMT condition. These things are defined at a high-level and not precisely. \n\n(b) The whole premise of comparing the embedding of motor commands and the agent's spatial configuration only works under a special condition -- s = φε(m) (section 3 of the paper), which is not general. For e.g. if an arm is “torque controlled” it is not possible to predict the location of the arm (i.e. a potential sensory observation) just from the torques. Additional knowledge of the agent’s state such as current position and the velocity of the arm is required. In the examples mentioned in the paper, the arm is “position” controlled, i.e. given the orientation of each joint (i.e. the motor command) it is possible to predict the sensory observation.  This is a very special case. In biology for example, we control the flexing of muscle fibers using the motor system, we can’t directly output positions of the arm. The general, condition of operation should be: s_{t+1} = φε(m_t, s_t). \n\n(c) Authors argue that in order to learn metric invariants, the agent needs to observe the same sensory state under different motor commands. They further argue that in the MMT condition, where the environment also translates, this affect is achieved and therefore metric invariants are learned. My position is that this is simply an artifact of the restricted problem setup where  s = φε(m) holds. In a more general setup, s_{t+1} = φε(m_t, s_t) there is no requirement for the environment to move. An arm with different torques can be at the same position and hence the condition imposed by authors should be specified naturally even in MM environments. What do the authors think? \n\n(d) Under the condition of,  s = φε(m) difference between MM and MMT appears that in one case (MM), neural network is trained without translation perturbations, and in other case MMT is a form of data augmentation with translation perturbations. I am not sure if there is any other justification for why only topological invariants should be learned with MM and metric invariants with MMT. To me it seems like training with data augmentation leads to better metric learning. Do the authors have any other insights — I would love to know. \n\n(e) Finally the embeddings are useful, if they are useful for an end-task. I would love, if the authors evaluated the learnt embeddings in each of the three conditions for some end tasks such as reaching in case of an arm or something else that is more feasible. \n\nDespite it being a very interesting topic, due to theoretical concerns outlined above, I cannot recommend the paper for acceptance. With a strong rebuttal it is possible to convince me otherwise.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting topic, contribution questionable, better write-up required",
            "review": "The paper proposes that agents can extract the spatial structure of the world if consistently explored solely from sensorimotor predictions.\n\nThe topic of the paper is relevant and fits the conference. However, I have doubts about the generality of the claims made.\n\nStrong aspects:\nAbstract makes the reader curious and excited about the content\nInteresting and important topic for representation learning.\n\nWeak aspects:\nThe paper is written in a way that it is not easy to follow. \nImportant details are in the Appendix and some more formal description would be helpful. A clearer presentation of the three different exploration types for instance would help the reader. \nThe paper is not using standard notation and deviates from the POMDP description for not clear reason. \nIt is hard to compare and the results are in a very restricted setup.\nNo baseline or comparison to other methods are given.\nWhy does space emerge in the motor-state (h) not the sensor state?\nNot enough details given to reproduce the paper. It is nothing mentioned about code to be made public.\n\nDiscussion:\nThe argument of the paper is that it is sufficient to do sensor prediction when the world is consistently explored and changing in a consistent manner. There is no need for additional losses or the like. However, I am not convinced about the generality of this statement. What if the world is much richer and the agent and the environment undergo the typical transformations. Rotation and movement in 3D space + non-linear distortions by sensors (e.g. a camera). I am not seeing why a representation of space should emerge without any pressure for minimality or the like. The paper also assumes that the sensor is moved by the motor command in space directly and not, for instance, as a double integrator equations where the motor command is a force that accelerates a body. \nIn the current form, what it really says, is that the network figures out the actual effect of the motor command in moving the sensor around. The sensor is moved in a 2D space by the actions such that the network recovers this 2D manifold. And this can only happen if it actually observes movements and the dataset is rich enough. This distinguishes the 3 different exploration settings. \n\nDetails:\nSec 2:  Prior work: Which priors in Jonschkowski and Brock 2015 do you mean that are specific to the emergence of space? Slowness?\nSec 3: what do you really mean by motor state? In MDPs there is the notion of action that effects the state of the system. There is not really a state of the motor system? Do you mean proprioceptive sensors or actually the motor command/action.\nThe definition s = \\phi_e(m) is kind of nonstandard as one would expect a dependency on s_{t-1} which is in your notation hidden in \\epsilon.\nThe logic is not so straight forward to me. Isn't the logic such that: Given a rigid metric space: when the agent moves around the same type of movements lead to the same kind of transformations independently of there the agent is located?\nI believe this section could be streamlined and illustrated by some examples to drive the message home. Also, making a clear and potentially more explicit statement of these invariances (e.g. by an equation) and why they will be revealed by learning predictions.\n\nSec 4.2:\nImprove the description of the settings. In the first setting (MTM) subsequent sensor/motor values are independent right? So it is the same as having randomly selected isolated data points from a normal interaction. \n\nMMT setting: you write ... which can translate randomly after each transition.  This is almost the same as in MTM where you write ...translates randomly between t and t+1\nIn the Appendix you write that in MMT that environment moves only every 100 steps? \n\nBTW:\nIs the env-movement a smooth movement or a jump?\n \nFig 2: why does the green curve end so early? Is it because of your stopping criteria for training. I would like to see the same training time for all settings.\n\nFig 3 and text: Should one not expect a torus in (a)? The world is not a square but a torus, as you have cyclic boundary conditions. I am surprised to not see this in the plots. The current result somehow violates the smoothness because there is a big jump between the boundaries although in environment there is none.\n \nSec 6: par 2: ...these invariants represents for the predictive model?\n\nOut of my curiosity:\nyou write that ... casts some doubts on purely passive and observational approaches....\nIn which sense did the actions help here? Do you mean that the agent needs to know its own actions right? \nSo when it is to be done from a video (just sensor information) than the actions would need to be inferred first?\n\nTypos:\np1 par2: approache\np8 last par: could be merge\n\nupdated score after authors revision\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Unconventional and interesting approach to unsupervised learning of sensorimotor perception",
            "review": "This very interesting paper is based on the sensorimotor contingency theory, which grounds the perception of the agent (motor perception and sensory perception) in the capacity to learn to predict future sensory experience and to build a compact internal representation of proprioception and motor states. They show that through active experience (exploration) of an environment, an agent can encode its motor state in a way that captures both the topology and the relative distances. The authors show that in the case of an environment consistent with sensorimotor transitions and with some changes in the environment that are not inconsistent with the sensorimotor transitions (resets?), the network can learn the representation h of motor that project to the actual position of the agent in the environment (albeit in very contrived 2D toy tasks).\n\nThe model does not rely on RL at all; rather, it uses the representation of motor states (proprioception) to make predictions about the sensory observations of the environment. If such an agent were to act, I presume that there would be a search procedure across motor states to make the agent reach a desired state. The paper merits publication at ICLR provided very extensive revisions are made, and I am listing here the improvements to be made.\n\nIt is cool to cite Kant, Poincaré and Nicod, whose philosophical work underlies subsequent work on representing space and sensory experience. When citing Kant, please cite the primary source, not the 1998 re-edition.\n\nThere are missing references to Wayne et al (2018) \"Unsupervised Predictive Memory in a Goal-Directed Agent\" arXiv:1803.10760 and Ha & Schmidhuber \"World Models\" arXiv:1803.10122, where an agent is shown to build a representation of the world that can be decoded into spatial position and even a map of obstacles, on previously unseen environments, using only prediction of images, rewards and actions. Please include them in your work as they considerably change the narrative of section 2 (related work). Essentially, while the claims of the paper are interesting and relevant for the representation learning community, similar work has already been done, at much larger scale, from visual observations, using RL and self-supervised learning.\n\nSection 2 is also somewhat overly critical of previous work: in (Banino et al, 2018; Cueva & Wei, 2018) \"rely[ing] on extraneous spatial supervision signals [do not] counter any claim of autonomy\", first because these signals can come from sensory perception (e.g., smell) and also because the agent is still autonomous at test time. Similarly, the depth prediction task in (Mirowski et al, 2017) is rather intuitive (stereo-vision).\n\nPart 3 is difficult to parse: it would help to use the word proprioception (or explain why it is inappropriate) when talking about motor states, and exteroception (sensing the environment). I understood the first assumption, which is local continuity in sensory and motor space as well as the ambiguity of redundant motor systems that can generate the same sensory states, but not the second assumption. From what I understand, there are two invariants in building the model of proprioception: invariance to the topology of the environment and to the distances between objects, but then this is hard to reconcile with the setup of (in)consistent transitions in a moving environment and consistent transitions in a static environment. Please rewrite this section in a way that is easier to parse for people who know state-space models and RL for navigation and grasping (who may be your audience). Moreover, all the references point to a single work, which suggests that it is a very peculiar way of approaching a much more general problem of sensorimotor prediction, and therefore begs for a clear and simple explanation.\n\nThe architecture of the model is interesting: typically deep RL papers encode the sensory observations s into a hidden representation h, to take actions and produce a motor state m. Here, the current and future motor states m_t and m_{t+1} are embedded into h_t and h_{t+1} using a siamese MLP and used, in combination with the current sensory observation s_t, to make a prediction of s_{t+1}. This is somewhat related to learning the dynamics in model-based RL; please look into and cite Pathak et al (2017) \"Curiosity-driven exploration by self-supervised prediction\", ICML and other work on intrinsic curiosity.\n\nThe experiments are in very simplistic 2D grid world environments, but it makes the analysis and understanding of the 3D representations h much more simpler to follow. On the other hand, the discrete world task is very contrived (especially the weird mapping from m to h and from p to s) and hard to relate to existing work. One difficult problem that is solved by (Banino et al, 2018) is that of path integration in 2D from egocentric velocity. Could the authors present results on such a nonlinear case?\n\nRevision: Score updated from 6 to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}