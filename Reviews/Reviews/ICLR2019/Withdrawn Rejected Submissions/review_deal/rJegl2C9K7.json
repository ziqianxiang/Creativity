{
    "Decision": "",
    "Reviews": [
        {
            "title": "Clear description but lack of novelty",
            "review": "Summary:\nThe paper explains a stage by stage knowledge transfer method. The authors used different structures of resnets to transfer knowledge from a teacher model to a shallower student network. The L2 norm is used to minimize the difference between features. The authors argue that this type of transfer while ignoring the labels or logits improves the performance and makes the network task independent. The number of stages is fixed and at each stage the rest of the network does not get updates. The method is evaluated on several benchmarks.\n\nStrengths:\nThe description is clear and good analytic experiments are performed. The method is practical for tasks that are not classification. The datasets chosen for experiments are challenging enough to support the argument.\n\nWeaknesses:\n-While a lot of benchmarks are considered and the achieved performance is good, the improvement over competing KD methods is marginal.\n-CNN citation is missing.\n-In introduction section, authors mention performance drop when number of classes are small. Can they provide a reference for details?\n-How do you decide on k for number of stages?\n-There is not enough details to reproduce the experiments such as learning rates, layer sizes, regularizers (if any).\n-The authors claim they fix parameters to prevent vanishing the knowledge, is that really necessary? Can you present a comparison?\n-As far as i understood the proposed way of training restricts you to have same number of features between teacher and student. This is a big constraint and if not, one option would be to use regressors as in Fitnets paper.\n-The novelty is limited. If authors studied how to break down the teacher or student model in a principled way, it could have been a strong contribution but the current version does not differ much from Fitnets idea.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important direction of research, unfortunately vague and lacks content",
            "review": "Reliably transferring knowledge between networks is a crucial task in many aspects of training. It helps reduce the size of networks without sacrificing performance, it helps the student network to achieve good results on expert tasks trained by the outputs of various specialized teacher models, or it can speed up training to name a few benefits... The work \"Feature Matters\" is interested in the idea of expending the knowledge transfer to the hidden layers of a given network. It's promise of being a method that is task and dataset independent is quite big and punchy. However, the paper falls short in many aspects that, unfortunately, renders it as a work in progress.\n\nTo start on a positive note, knowledge transfer at the level of hidden layers is an important area that should be explored deeper, and the experiments that are presented in the paper are strong indicating the paper is indeed in the right direction. And therefore I strongly encourage the authors to revise the presentation of their work and solidify their methodology further. \n\nA major flaw in the present paper is in its omission of several lines of research that are focusing on representations of hidden layers, see for instance http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf There is a deep literature on representations of hidden layers that is relevant when one is interested in transferring the knowledge of the hidden layers. Are they generic representation, do they depend on the task or dataset? All are relevant questions that fit into the framework of the present paper. In section 3, at Observation 1, the paper mentions that networks are separated into two parts: feature extraction and feature application. However, the feature extractor is almost all of the network in ResNets except for the last layer. What exactly is meant by this separation? How is it related to the hidden layer representations? All similar questions stay up in the air.\n\nA second major flaw is seen in the way the method is proposed: it is entirely mysterious how the method is supposed to be implemented from the point of view of the reader who is interested in re-implementing the proposed method. The figure is not descriptive enough, and the text is only bloated with vague and unclear descriptions. The reader is only provided with some queues here and there (e.g. the first paragraph of section 4.3). Clearly, it is not enough details to replicate the setup. \n\nOverall, the direction in which the paper is taking us is very interesting, and I can imagine a re-written version of the present paper in the previous years' ICLR Workshop format to be a very good candidate but for a paper that has no theory, that is very light on its experimental descriptions and details, that is very light on relevant research review, and that is very loaded with vague and imprecise descriptions, I can not recommend the submission for ICLR as it stands. \n\nTo improve the paper further, I would recommend:\n- Shorten and sharpen the exposition as opposed to the current casual style\n- Include a thorough literature review with more precise (but short) explanation of their contributions\n- Include a sketch of the proposed algorithm (make sure a reader can replicate the core algorithm)\n- An attempt at formalization of the method would be also helpful for the theory-minded readers\n- Systematic reports of the experiments, resources used, time and memory analysis etc would be useful additions (since it is also part of the claim)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good but contribution is limited",
            "review": "Summary:\nThe paper proposed to divide a network into multiple parts and distill each part sequentially. The purpose is to improve the distillation performance when the teacher network is deep.\n\nQuality:\n- I think the multi-stage idea makes sense and I agree it can improve the accuracy. But the improvement seems to be very limited. More experiments would be necessary to justify its value.\n- Also, the stage-by-stage approach might also produce more overhead in computation. More analysis is needed for the computation.\n\nClarity:\n- The paper is a bit hard to follow especially in the \"observations\" part, even though eventually I managed to get the core idea of the paper.\n\nOriginality:\n- The contribution is incremental.\n- I feel the multi-stage idea has been used extensively.\n\nSignificance:\nI believe the paper could be improved significantly in order to produce desired impact.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}