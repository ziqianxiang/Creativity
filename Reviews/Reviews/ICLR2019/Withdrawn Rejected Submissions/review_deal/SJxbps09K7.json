{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting analysis, but not much new",
            "review": "This paper presents a series of empirical observations when aligning word embeddings in different languages with the unsupervised method of Conneau et al. (2018). While I am the first to defend that research should not be only about fighting for SOTA and I would love to see more analysis papers like this, I think that this one in particular does not have enough novelty/substance to get accepted at such a competitive venue. The paper is well written and I enjoyed reading it, but I was left with the feeling that I did not learn much from it.\n\nGiven that the paper is well structured into 9 different observations, I will next comment them one by one:\n\n- Observation 1: Impossible cases are not solely the results of linguistic differences, but also of corpus characteristics. This was already shown in Sogaard et al. (2018) and, in my opinion, their paper does a better work at systematically studying this effect in their ablation study.\n\n- Observation 2: Impossible cases can also be the result of the inductive biases of the underlying word embedding algorithms. Same as above: this was already shown (and better supported in my opinion) in Sogaard et al. (2018).\n\n- Observation 3: GAN-based UBDI becomes more unstable and performance deteriorates with unit length normalization. This is a somewhat minor detail, but I find it interesting, as it challenges a common practice in supervised bilingual dictionary induction.\n\n- Observation 4: GAN-based UBDI becomes more unstable and performance deteriorates with PCA pruning. I also find this one interesting, although it is essentially a negative result.\n\n- Observation 5: GAN-based UBDI is largely unaffected by noise injection. I do not see why this could be relevant, as one should never find this type of synthetic noise in realistic settings.\n\n- Observation 6: In the hard cases, GAN-based UBDI gets stuck in local optima. I might be missing something here, but this seems pretty obvious. Why would the method fail to converge to the correct solution otherwise (assuming that the unsupervised objective function is appropriate)?\n\n- Observation 7: Over-parametrization does not consistently help in the hard cases. This is another negative result: not bad to know, but does not have much value on its own.\n\n- Observation 8: Changing the batch size or the learning rate to hurt the discriminator also does not help. Just another negative result. The authors themselves state that \"it seems the MUSE default hyperparameters are close to optimal\". Again, not bad to know, but this only confirms that Conneau et al. (2018) did a good job.\n\n- Observation 9: In the hard cases, model selection with cosine similarity can stabilize GAN-based UBDI. Your model selection criterion is the one proposed by Conneau et al. (2018) themselves, so I do not see where the novelty is here.\n\nOther contributions:\n\n- As cited in the paper, the procrustes fit was proposed by Kementchedjhieva et al. (2018) for this exact same problem, so there is no original contribution here either (other than applying it to a different dataset).\n\n- The paper classifies different language pairs as \"easy\", \"hard\" and \"impossible\". I think that the distinction between \"easy\" and \"hard\" is well supported, as it is putting Bengali and Cebuano into a third (even harder) group. However, I do not see enough evidence to name this third group \"impossible\". Such a strong statement should be rigorously supported, and I do not find this to be the case. At most, you could speculate that some language pairs might be impossible, but we cannot be 100% sure that they are with the provided evidence.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper provides interesting insight into the usefulness of GANs to perform alignment of embedding spaces across languages. The paper is well motivated and introduces the problem well. While existing experiments are sufficient, there is scope to add more experiments.",
            "review": "This paper is clear and well written, except for the occasional typos, eg. missing a bracket in the very first equation.\nThe problem of learning transformations to align vector spaces across languages is extremely interesting and relevant. While the paper does a thorough job in explaining the metrics used and in detailing the procedure followed, I would have liked to see the following the details in the paper,\n1)Number of unique word tokens in Bengali, Cebuano etc. While clicking on the reference link to MUSE provides these details, including the same in the paper, would make the writing wholesome.\n2)While a thorough theoretical analysis is provided to explain the quality of the transformation/rotation matrix, there are no empirical results provided on any cross-lingual classification tasks. While not too important to the paper, such an analysis would provide the reader with an idea of how much can the proposed techniques be used in end-to-end train system for cross lingual tasks.\n3)The authors comment on the quality of embeddings for languages such as Bengali, when studying the problem of alignment. Do the authors have any opinion if use of meta-embeddings i.e a weighted combination of say GloVe, word2vec and FastText can over come some of this limitations?Since each of these methods exploits a different feature of the training corpus, and there is rich literature showing the effectiveness of meta-embeddings, would this be something worth pursuing?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Another empirical analysis related to instability of fully unsupervised cross-lingual word embedding models",
            "review": "The recent methods on learning cross-lingual word embeddings without any bilingual supervision (i.e., in a fully unsupervised way) have generated a lot of traction in NLP and beyond. Follow-up papers from Artetxe et al. (ICLR 2018), and Lample et al. (ICLR 2018, EMNLP 2018) have shown that such cross-lingual word embeddings can seed unsupervised (neural and statistical) machine translation, and other applications are also possible (e.g., unsupervised cross-lingual retrieval). \n\nThis paper in particular focuses on further empirical analyses related to previously observed instability of unsupervised mapping-based approaches for learning unsupervised cross-lingual word embeddings, but remains focused on the task of unsupervised bilingual dictionary induction (UBDI). The paper is largely a continuation of the preliminary paper which focuses on the same problem and research question: the paper of Sogaard et al. (ACL 2018). In fact, this paper reads a lot like a more elaborated (or incremental) version of the preliminary paper of Sogaard et al. (ACL 2018), with the paper structure and some descriptions borrowed from the previous work (e.g., the focus on the MUSE algorithm, the description of k-isospectrality), with two main enhancements compared to the prior work: 1) The introduction of Procrustes fit as a means to diagnose and anticipate (im)possibility of unsupervised embedding learning. Arguably, the proposed diagnostic tool is more efficient than the alternatives; 2) A very simple method for unsupervised model selection (Section 4) based on the mean cosine similarity predicted by the CSLS method.\n\nAlthough some of the listed empirical observations are very straightforward if the reader knows anything about the entire procedure, it is good to see them empirically supported and presented in a very organised fashion. However, the two main methodological contributions of the paper seem quite thin to me. First, the method for unsupervised model selection is based on the unsupervised stopping criterion of Conneau et al. (2018) and it is a very simple (even simplistic) idea which happens to work well in practice already. Second, I am not fully bought by the Procrustes fit test and its comparison to two other tests has to be further examined, especially its relation to two similarity measures from Sogaard et al. For instance, k-subgraph isospectrality in the work of Sogaard et al also displayed a very strong correlation with UBDI performance (>0.9), while the authors in this paper claim that the correlation coefficient with their data in this paper is -27% for the same method. Could the authors explain the discrepancy?\n\nThe authors claim that the Procrustes fit is much less computationally demanding as a test than the two alternative tests. However, I would like to see this claim empirically validated by running some actual tests and measuring test times. How are the two alternative tests impractical if they were used previously by Sogaard et al.?\n\nSome observations have been already confirmed in prior work. For instance, Observation 2 is already analysed in the work of Sogaard et al. Observation 3 and Observation 5 are also straightforward given the technicalities and hyper-parameter setup of the MUSE procedure. \n\nI wonder if the authors are aware of the more recent work on UBDI from Artetxe et al. (ACL 2018). A crucial analysis for this paper should focus on Artetxe's more recent work rather than the MUSE algorithm. I acknowledge the fact that this field in particular is moving rapidly, but the paper would be much more impactful by focusing on the more recent and more appropriate starting ground rather than recycling the ideas already covered in prior work (e.g., in Sogaard et al.'s paper). \n\nThe paper operates only in the fully unsupervised setting, which is, in my opinion, an artificial (even \"artistic\" setting). As Artetxe et al. (ACL 2017) showed and Sogaard et al. verified: relying on some sort of supervision (identical words, cognates, even numerals for languages with different scripts) already mitigates the problems of instability to a large extent. Why would we want to tie our analysis only to the fully unsupervised setting and go about it at length when it seems obvious that some sort of (cheap) supervision can already help the mapping-based algorithms to a large extent? I would like to see more experiments in such weakly-supervised settings. I would also like to see if the model selection criterion is applicable to this more real-life setting.\n\nAlthough I am happy to see a clear division into possible and impossible UBDI setups, the paper also does not provide any solutions (or rules of thumb) on how to proceed if we end up having an impossible setup. What are the alternatives to the fully unsupervised BDI?\n\nI am also missing explanations to some empirical observations: e.g., while the discrepancy between this work and the original work of Hoshen and Wolf has been reported, there is no insight on what causes the discrepancy and how this could be theoretically justified. In simple words, it is just observed, without getting a proper explanation supporting it.\n\nThe paper is very well written and easy to follow, but my impression is that, as an empirically driven paper, it could contribute from further experimentation as well as from linking key empirical findings to theoretical justifications. Also, the two main contributions of the work are not so substantial and overall the paper mostly proves an already established fact: that fully unsupervised models for cross-lingual word embedding learning are very unstable and their success relies on a large spectrum of design choices and hyper-parameters. I would also like to see the analysis expanded beyond UBDI to other (more downstream) tasks where such low-resource representation learning regimes might be useful.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}