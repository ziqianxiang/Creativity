{
    "Decision": {
        "metareview": "This paper investigates copying mechanisms and reward functions in sequence to sequence models for question generation. The key findings are threefold: (1) when the alignments between input and output are weak, it is better to use latent copying mechanism to soften the model bias toward copying, (2) while policy gradient methods might be able to improve automatic scores, their results poorly align with human evaludation, and (3) the use of adversarial objective also does not lead to useful training signals. \n    \nPros:\nThe task is well motivated and the paper presents potentially useful negative results on policy gradient and adversarial training.\n\nCons:\nAll reviewers found the clarity and organization of the paper requires improvements. Also, the proposed methods are reletively incremental and the empirical results are not strong. While the rebuttal answered some of the clarification questions, it does not address major concerns about the novelty and contributions.\n\nVerdict:\nReject due to relatively weak contributions and novelty.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "relatively weak contributions and novelty"
    },
    "Reviews": [
        {
            "title": "Some mixed results and needs further analysis",
            "review": "This paper presents question generation models by designing variations of copying mechanism and reward functions. Experimental results show that different copying mechanism can improve upon basic seq2seq models, some of the reward functions also produce better results. I think the results are interesting, especially the ones compared with human evaluation (fig. 1), but it's might be better to explain on which aspect each of the feature contributes to the improvement. For instance, the authors can give some insights based on empirical results on what kind of questions will benefit from each type of copying. \n\n\nThe authors should better organize table 1 and 2, and inform the readers on what is the consistent conclusion (if any). For table 2, there is no result for adding \"adversarial discriminator\" only. Also the item \"+226\" on the second row, is that an error?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Studied the problem of question generation. However the paper is hard to follow and the proposed model lacks novelty.",
            "review": "The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:\n\n1) In model section, the task definition is not clear. It is expected to see what's the question generation task studied in this paper. An example or a model overview will definitly help.\n\n2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf \"Seo, Minjoon, et al. \"Bidirectional attention flow for machine comprehension.\" arXiv preprint arXiv:1611.01603 (2016).\"  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.\n\n3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?\n\n4) In the training, how to deal the ground truth that are not in the vocab? The authors stated \"using a modified heuristic described below\", but no follow-ups in the paper.\n\n5) The paper is not well written and organized. Small typos: in introduction, 'and and answer span', 'and output and output sequences'. In model, 'Glorot initialization', 'Bahdanau attention', it is not the common way to cite others' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder.\n\nI have read the authors' detailed rebuttal. Thanks.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty limited and experiments not convincing enough",
            "review": "In the paper, author investigate the use of copy mechanisms for the question generation task. It evaluates on the SQuAD dataset. The model is a popular seq2seq/encoder-decoder model with copy mechanisms using pointer networks. \n\nPros:\nIt is well motivated. For the question generation task, a word to be predicted can be from either a global vocabulary list or copied from the given documents (location vocabulary).  There are some overlap between these two vocabulary lists.  This paper mainly investigates this issue.\n\nIt is well written and easy to follow.\n\nInteresting analysis of human/automatic metrics.\n\nCons:\nThe tricks here are a bit of ad hoc. It is better to have a systemic study.\n\nBaseline results are too low. E.g., officially QANet results (from the paper) on SQuAD v1 is around 82.7 (my implementation obtains 83.1). However in the paper, its best result is 72.6 in terms of F1 score. \n\nThe authors only evaluated on one dataset. It is hard to convincing.\n\nIt is lack of comparison results of question generation in literature. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}