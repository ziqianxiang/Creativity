{
    "Decision": {
        "metareview": "The reviewers and this AC agree that the paper is not of acceptable form due to several issues: (1) limited novelty, (2) limited/unclear experimental validation, and (3) presentation issues.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "decision"
    },
    "Reviews": [
        {
            "title": "Interesting extension but lack of meaningful evaluation",
            "review": "The submission proposes a new model for the task of fast style transfer with arbitrary input images.\nThe model combines the existing approach of Li et al. 2017 [1] and the idea of inter-layer correlations into a new model architecture for fast style transfer. Notably the idea of inter-layer correlations is not new, it was first proposed by Novak and Nikulin [2] and recently picked up by Yeh and Tang[3] (both methods are not referenced in the present submission).\nTo evaluate the model, visual comparisons with other methods are shown and quantitative loss values for content and style loss are computed. \n\nMy main issue with the submission is that the scope of the evaluation is not suited to demonstrate a clear advantage of the proposed method over existing work. The qualitative comparison remains highly subjective. Based on figure 7 I could not tell which method performs the best style transfer and this might well be because the systematic perceptual differences between the methods are small. \nTo demonstrate a clear advance of the method in style transfer, I do believe it is necessary to run a user study showing that users systematically prefer the output of the proposed method over that of competing methods.\n\nAdditionally the quantitative evaluation leaves several open questions:\n- Why is there no comparison with Ghiasi et al. 2017 [4]? This is the natural extension of Dumoulin et al. 2017 [5] for arbitrary fast style transfer.\n- What exactly is the content and style loss that these methods are compared on? Is it the one used for the models in this submission? If so, were the compared methods also trained to minimise this loss function? If not this does not seem to be a fair comparison. \n- Even if all methods were trained to minimise the same loss function, without user study it is unclear if that loss function is a good approximation of perceptual preference, which is the actual underlying target that style transfer aims to optimise.\n\nAs a side note, the provided baseline of the Gatys et al. [6] method is somewhat misleading because it is using the Adam optimiser. The original work used LBFGS to optimise the loss function and it is fairly well known that the choice of optimiser can have significant impact on the quality of the results.\nI do not think this comparison is critical for the submission because the proposed method mainly competes with other fast style transfer methods, but I would recommend more care to be taken when reproducing existing work. \n\nAll in all I think the submission proposes an interesting combination of existing methods leading to reasonable extension of the body of work in fast style transfer. However, given the lack of meaningful evaluation I am not convinced that the proposed method substantially advances the field of fast style transfer to warrant publication at ICLR.\n\n\n[1] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. In Advances in Neural Information Processing Systems, pp. 385– 395, 2017b.\n[2] Improving the Neural Algorithm of Artistic Style. R Novak, Y Nikulin - arXiv preprint arXiv:1605.04603, 2016 - arxiv.org\n[3] Improved Style Transfer by Respecting Inter-layer Correlations MC Yeh, S Tang - arXiv preprint arXiv:1801.01933, 2018 - arxiv.org\n[4] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, and Jonathon Shlens. Explor- ing the structure of a real-time, arbitrary neural artistic stylization network. In British Machine Vision Conference (BMVC), Sep 2017. \n[5] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. In International Conference on Learning Representations (ICLR), Apr 2017. \n[6] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good ideas, but insufficient experimental validation",
            "review": "SUMMARY\nThe paper is concerned with the problem of arbitrary feed-forward style transfer, where a feed-forward model receives a content image and a style image as input, and must produce as output an image matching the content of the former and the style of the latter. The approach roughly follows that of [Li et al, NIPS 2017]: An encoder network (VGG pretrained on ImageNet) is used to extract features from both the style and content image; the features from the content image are adjusted to match the statistics from the style image, and the adjusted features are passed to a decoder network which generates the output image.\n\nCompared to prior work, the main innovations are:\n- Considering correlations between features from different encoder layers rather than only correlations within a single layer in both the feature adjustment step as well as in the loss\n- An improved encoder / decoder architecture which uses skip connections in the decoder, allowing for a single encoder / decoder pair rather than a cascade of encoder / decoder pairs for each layer.\n\nPROS\n- Considering correlations between features from different encoder layers is a good idea\n- The improved encoder / decoder architecture is significantly more efficient than the cascaded approach of [Li et al, NIPS 2017]\n\nCONS\n- Somewhat incremental\n- Limited experimental evaluation\n- Qualitative results not clearly better than existing methods\n- Missing citation for multi-scale losses\n\nLIMITED EXPERIMENTAL EVALUATION\nOne of the key claims of the paper is that “our method with inter-scale\n(fig.7(f)) or intra-scale feature transform (fig.7(g)) are more similar to the target style than those of single-scale style transfer without considering inter-channel correlation” (Figure 7 caption); this claim is substantiated primarily by qualitative results in Figures 4, 7, and 8. Personally I don’t find the results with inter-feature correlations to be much better than those with only intra-feature correlations or the results from prior work. All recent style transfer methods depend on a host of hyperparameters like style and content weight, learning schedule, etc; in my experience differences in these hyperparameter settings can have large effects on the qualitative appearance of the generated images, and by varying these hyperparameters it is common to see qualitative differences similar to those shown in Figure 4 and 5. From the small number of qualitative results presented I do not think that the benefits of inter-scale correlations have been clearly demonstrated. \n\nI appreciate that the authors tried to quantify their results by comparing loss values (Table 1) but unfortunately it’s hard to know how much the values of different losses correlate with human judgement of style quality.\n\nIn addition to selected qualitative results I would have liked to see a user study demonstrating human preference for images generated using inter-scale correlation losses, ideally across a range of different hyperparameter settings for each method.\n\nGATYS BASELINE\nFrom Table 1, the proposed method with intra-scale features achieves lower content loss than the direct optimization baseline (a) from [Gatys et al.] This is very surprising to me - typically direct optimization leads to much lower losses than any feedforward methods. From Section 4.4.1 this baseline uses Adam; in my experience using L-BFGS tends to achieve lower losses which may explain the results from Table 1.\n\nMISSING CITATION FOR MULTI-SCALE STYLE TRANSFER\nInstead of computing correlations between features from different encoder layers, [Wang et al, CVPR 2017] define a loss that considers the generated and style image at multiple spatial scales. This should be discussed in relation to the proposed method.\n\nENCODER / DECODER DETAILS\nThere are some missing details about exactly how the encoder and decoder are initialized and trained. I assume that the encoder was pretrained on ImageNet; is it updated during training or kept fixed? Is the decoder initialized randomly or does it mirror the pretrained ImageNet weights?\n\nTYPOS / FORMATTING\nThere are minor typos throughout, e.g. “verity”, “sinlge” in Section 4.3, Paragrah 1. I also found the citation style to be somewhat jarring, especially in the introduction; parenthetical citations and better spacing may improve readability.\n\nOVERALL\nOn the whole I feel that the paper is somewhat incremental. The inter-scale loss seems intuitively like a good idea, but I don’t think the paper presents sufficient experimental evidence to justify it. On the other hand the proposed decoder architecture seems like a clear improvement over the cascaded approach from [Li et al, NIPS 2017], as it is significantly more efficient without sacrificing quality. However I don’t feel that this alone is enough novelty for ICLR, so I lean slightly toward rejection.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting architectures with limited novelty, also lack of clear presentation",
            "review": "This work proposes to use a single feed-forward network with two types of multi-scale transformers (MST) for image style transfer. The first transformer cascades existing single-scale transforms (SSTs), and the second one applies SST to the stacked feature maps. Skip connection is used between the MSTs and the decoder.\n\nPros\n\n- Has quantitative evaluation\nTable 1 includes quantitative results for different approaches, which is essential for proper evaluation.\n\nCons\n\n- The problem is not very well motivated and the novelty is limited. \nWhy shall we care about multi-scale style transformation? Style transformation is about modifying an image to match certain style WITHOUT completely destroy its content. Allowing low level details to interfere high level content seems to be a bad idea.\nAs for novelty, it is claimed that this work is the first to use skp connection. However, Avatar-Net uses skip connection before.\n\n- Empirical results not significant\nIn Table 1, why is (d) missing for the small set and (b) missing for the large set? It seems that (a) and (b) are already very good. Then why do we need the proposed methods? It is said that (b) is not extendable to arbitrary style transfer. How so? How do you define \"arbitrary style transferring methods\"?\nI do not see how Fig.6 can tell us useful information about the skip connections. The corresponding paragraph in Sec.4.3 is not very convincing or informative.\n\n- Writing can be improved.\nIn terms of content, terminologies are used without clear definitions. For example, what is \"inter-scale dependency\" in the introduction and what does \"merges multi-scaled styles optimally\" mean in Sec.3.1.3 (what is the optimality here)? It is confusing what corresponds to \"intra-scale\" or \"inter-scale\" throughout the paper. For example, the direct connection between relu_3_3 to relu_2_2 of the decoder in Fig.1b can also be interpreted as \"inter-scale\". As another example, \"4 batches of random image pairs\" in the experiment, do you mean a batch of 4 random pairs?\nIn terms of presentation, the grammar needs more careful checking. For examples, \"in the remained of this paper\", \"each methods\", etc. The meaning of the transpose in Eq.(3) is not clear. How do we transpose 3-dimensional tensor F? Also, know the difference between \\citet and \\citep and when to use them.\n\nMinors\n- In Eq.(4), according to the definition of C_i, for i > 1, the index should be (\\sum_k C_k + 1):(\\sum_k C_k + C_i)\n- Above Eq.(5), \"inter-scale feature transform (Sec.3.1.1)\" should be Sec.3.1.2.\n- In Fig. 7, the last column should be (g) instead of (e).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}