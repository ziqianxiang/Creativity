{
    "Decision": {
        "metareview": "The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions; specifically using a variational auto-encoder to generate conditionally independent data samples with the same joint distribution. \n\nThe reviewers and ACs noted weakness in the original submission related to the clarity of the presentation, relationship to already published work, and concerns about the correctness of some main claims (this mostly seems to have been fixed after the rebuttal period). There are additional concerns about a thorough evaluation of the claimed results, as the ground truth is unknown. The authors (and reviewers) also note a similar paper submitted to ICLR with the same goal but implemented using GANs. Nevertheless, there remain significant concerns about the clarity of the presentation.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Metareview"
    },
    "Reviews": [
        {
            "title": "The presentation of this paper can be improved. The notation is not very clear. ",
            "review": "This paper proposes a new approach to construct model-X knockoffs based on VAE, which can be used for controlling the false discovery rate. Both numerical simulations and real-data experiments are provided to corroborate the proposed method.  \n\nAlthough the problem of generating knockoffs based on VAE is novel, the paper presentation is not easy to follow and the notation seems confusing. Moreover, the main idea of this paper seems not entirely novel. The proposed method is based on combining the analysis in ''Robust inference with knockoffs'' by Barber et. al. and the VAE.  \n\nDetailed comments:\n\n1. The presentation of the main results is a bit short. Section 2, the proposed method, only takes 2 pages. It would be better to present the main results with more details. \n\n2. The method works under the assumption that there exists a random variable $Z$ such that $X_j$'s are mutually independent conditioning on $Z$. Is this a strong assumption? It seems better to illustrate when this assumption holds and fails.\n\n3. The notation of this paper seems confusing. For example, the authors did not introduce what $(X_j, X_{-j}, \\tilde X_j, \\tilde X_{-j} )$ means. Moreover, in Algorithm 1, what is $\\hat \\theta$ and $\\hat f$. \n\n4. I think there might be a typo in the proof of Theorem 2.1. In the main equation, why $\\tilde Z$ and $\\tilde X$ did not appear? They should show up somewhere in the probabilities.\n\n5. In Theorem 2.2, how strong is the assumption that $\\sup_{z,x} | log (density ratio)| $ is smaller than $\\alpha_n$? Usually, we might only achieve nonparametric rate for estimating the likelihood ratios. But here you also take a supremum, which might sacrifice the rate. The paper suggested that $\\alpha_n$ can be o( (n \\log p)^{-1/2}). When can we achieve such a rate?\n\n6. Novelty. Theorem 2.2 seems to be an application of the result in Barber et. al. Compared with that work, this paper seems to use VAE to construct the distribution $ P_{\\tilde X| X}$ and its analysis seems hinges on the assumptions in Theorem 2.2 that might be stringent.\n\n7. In Figure 1 and 2, what is the $x$-axis?\n\n8. A typo: Page 2, last paragraph. \"In this paper, we relaxes the ...\"",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A reasonable but unfortunately flawed approach to FDR control in feature selection combining neural networks and the knockoff",
            "review": "This manuscript tackles an important problem, namely, generating the knockoff procedure for FDR-controlled feature selection so that it can work on any data, rather than only for data generated from a Gaussian (as in the original work) or several specific other cases (mixtures of Gaussians or HMMs).  The basic idea is to exploit ideas from variational autoencoders to create a generic knockoff generation mechanism. Specifically, the main idea of the paper is to map the input covariates X into latent variable Z using a variational auto-encoder, generate the knockoffs \\tilde{Z} in the latent space, and then map \\tilde{Z} back to the input space to get the knockoffs \\tilde{X}. The authors claim that their contribution in threefold: \n\n(1) Given the assumption is valid that X is mutually independent conditional on Z, the generated knockoffs \\tilde{X} is proved to be valid in terms of satisfying the necessary swap condition. \n\n(2) Given (1) holds, and given that the discrepancy (measured by KL-divergence) between the true conditional probability Q(Z|X) and its estimate using variational auto-encoder is bounded by o((nlogp)^{-1/2}), the FDR is also bounded.\n\n(3) The proposed knockoffs generating procedure can achieve controlled FDR and better power. \n\nIn agreement with the above intuition, I have major concerns about the correctness of the paper.\n\nThe cornerstone of the proof in contribution (1) relies on the assumption that X is mutually independent conditional on Z. However, this assumption is invalid if there are dependencies between x_i and x_j. Therefore, the proposed knockoffs \\tilde{X} cannot be proved valid by Theorem 2.1.\n\nThe erroneous proof in contribution (1) leads to the failure of contribution (2) stated in Theorem 2.2. The FDR is no longer controlled. Intuitively, according to algorithm 1, \\tilde{Z} and \\tilde{X} are expected to have the same distribution as Z and X, respectively; therefore, the FDR cannot be controlled.\n\nThe experimental results are suspicious. In general, it seems fishy that the proposed VAE approach outperforms Model X in the fully Gaussian case.  In this setting, Model X should have an advantage, since it is specifically designed for Gaussian generated data.  Related to this, the text is confusing: \"Since the data were not Gaussian, the second-order matching method has the lowest power. The assumptions of the Fixed knockoff generations holds for the Gaussian cases, â€¦\" In the figure, the second-order matching method has low power even in the Gaussian case.\n\nMinor comments:\n\np. 2: The exposition should explain the motivation for Knockoff+.\n\nThe manuscript contains numerous grammatical errors, a few examples of which are below:\n\np. 1: \"biological linked\" -> \"biologically linked\"\n\np. 1: \"associated certain\" -> \"associated with a certain\"\n\np. 1: \"showed light\" -> \"shed light\"\n\np. 1: \"which has very limited\" -> \"which has limited\"\n\np. 1: \"leveraging on the power of of\" -> \"leveraging the power of\"\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good results, but a slight gap between theory and practice.",
            "review": "In the paper , the authors proposed the use of autoencoder for Model-X knockoffs. The authors proved that, if there exists latent factors, and if the encoders and the decoders can approximate conditional distributions well, the autoencoder can be used for approximating Model-X knockoff random variables: one can find relevant features while controlling FDR (Theorem 2.2).\n\nI think the theoretical part is good, and the experimental results seem to be promising.\n\nMy concern is the gap between theory and practice. In the manuscript, the authors used VAE for approximating conditional distributions. The question is how we can confirm that the trained encoder and decoder satisfy the assumptions in Theorem 2.2. If the trained models violate the assumptions, the control of FDR is no longer guaranteed, which may lead to false discoveries. As long as this gap remains unfilled, we cannot use the procedure reliably: we always need to doubt whether the encoders and decoders are trained appropriately or not. I think this gap is unfavorable for scientific discovery where only rigid procedures are accepted.\nHow we can use the proposed procedure reliably, e.g. for scientific discovery? Is there any way to confirm that the encoders and decoders are appropriate? Or, is there any way to bypass the gap so that we can guarantee the FDR control even for inappropriate models?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}