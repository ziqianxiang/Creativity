{
    "Decision": "",
    "Reviews": [
        {
            "title": "Poorly motivated, vague, and low-quality.",
            "review": "Summary: The authors propose a type of black box (gradient free) optimization algorithm for training neural networks. The method is a type of genetic algorithm (similar to particle swarm optimization), with particular choices for how to generate new proposals (perturbations) and how to combine them at every iteration (generation). The authors apply their method to some standard non-convex low-dimensional test functions, as well as to train a CNN on MNIST.\n\nMajor concerns: There are serious issues with the quality, clarity, originality and significance of this work.\n\nFirst and foremost, the ideas are vague and poorly motivated. In the abstract, the authors propose that their algorithm will be used to train deep networks, and \"eventually act as an alternative to Stochastic Gradient Descent (SGD)\". However, this overlooks theory in the optimization literature on how convergence rates for first-order (gradient) methods are *independent* of the parameter dimension. This amazing fact is why they are so powerful for solving high-dimensional optimization problems. On the other hand, gradient-free search methods are not independent of the parameter dimension, and slow down considerably when the effective parameter dimension is large. From the abstract, and parts of the text, it seems as if the authors think that gradient-free algorithms can be used to train neural networks even when gradient information is available (e.g. for training image recognition models, like their example application of training a CNN on MNIST). However, there are fundamental theoretical reasons why this is not a good idea, which are glossed over or ignored by the paper.\n\nIf the authors want to target problems which are non-differentiable (they reference a couple of examples where gradient-free optimization has been used for non-differentiable problems such as in reinforcement learning or architecture search), then they should apply their algorithm directly to these problems. Related to this point, after referencing papers on gradient-free algorithms applied to reinforcement learning, the authors state that this \"reflects the awareness within the broader research community about ... the need for alternatives to SGD.\" I strongly disagree with this reading of that literature, rather, those papers point out that gradient-free algorithms are useful/competitive with other techniques for a particular domain (RL) which contains non-differentiable optimization problems. Equating that with the field at large needing alternatives to SGD is an over-generalization.\n\nAdditionally, it is unclear if the authors are proposing an optimization algorithm particularly for neural networks, or something more general. The state that they are interested in training neural networks, but then why compare their algorithm on low-dimensional test functions (such as Rastrigin or Ackley) which have very a different loss landscape compared to that of neural networks?\n\nWhen it comes to the algorithm itself, the presentation is vague, introduces unnecessary complexity, and fails to reference significant related work.  The algorithm is introduced using a lot of colloquial language (Elites, Anchors, Probes, and Blends) which are hard to follow. It seems as if, at a high level, the authors are proposing a genetic algorithm--one where there is a procedure to generate new proposal iterates, and a method for combining them to form the next generation. A lot of the particular choices for how to generate these proposals and combine them seem arbitrary (a lot of hyperparameters are introduced). Finally, it is unclear how many function evaluations are required to evaluate one step (generation) of their algorithm, which is critical to comparing performance to other algorithms.\n\nWhen it comes to comparing performance, there are fundamental flaws with the study. One, the authors compare do not tune hyperparameters for the algorithms they compare against (going as far as to state that \"tuning those parameters is beyond the scope of this paper\"), but presumably tune the hyperparameters for their algorithm. This results in unfair comparisons. Two, the authors report performance in terms of \"generations\", but they should be reporting performance in terms of the number of function evaluations, since different algorithms may require more function evaluations (and thus take more time) per iteration/generation. This is particularly problematic for the comparison against SGD, where every step of SGD only requires one forward/backward pass but presumably their algorithm requires many forward passes per \"generation\". This makes the comparisons that are currently in the paper meaningless.",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An evolutionary algorithm that performs much worse than SGD",
            "review": "This paper describes an evolutionary algorithm, with the goal of providing an alternative to SGD. Unfortunately, the only comparison to SGD provided is an experiment where the proposed approach is orders of magnitude *worse* than SGD. And that is the only machine learning experiment in the paper. Thus, the results obviously do not warrant publication at ICLR; in fact, I am wondering why the paper was submitted to this conference.\n\nSome more details about the one machine learning experiment reported in the paper:\nHere the authors train a ConvNet on a subset of MNIST. While MNIST is by now a toy problem, it is OK to start with it. Only reaching a validation accuracy of 90% on it sets off an alarm in my head (since the state of the art is beyond 99.5%), but even the time to reach this performance is worse than that of standard SGD (with unspecified, and possibly untuned, learning rate, momentum, etc). And not just a little worse, but requiring almost 10x the number of \"generations\", with a generation of the authors' method performing 50 evaluations and a generation of SGD performing 1 evaluation. So in total the authors' method is about 500 times slower.\n\nI do not expect the authors to be able to modify this paper to be accepted at a machine learning venue, but the experiments for the blackbox functions appear more promising. I am wondering whether these positive results would hold up when comparing to good implementations of the other algorithms. Therefore, I encourage the authors to evaluate their algorithm on the benchmark functions in the BBOB challenge (https://bbcomp.ini.rub.de/); if they can beat the algorithms that participated in that challenge they should be able to publish their results at one of the evolutionary search conferences, such as GECCO. ",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting concept, not thoroughly backed up by experiments.",
            "review": "The paper proposes a new, evolutionary, gradient-free algorithm, aimed at training deep networks. The algorithm is compared against other evolutionary algorithms and against Stochastic Gradient Descent.\n\nHigh level comments:\n* Clarity: The paper is written very clearly and easy to follow.\n* Quality: The introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing.\n* Originality: I am not familiar with the literature in evolutionary methods, therefore I cannot evaluate this fairly. \n* Significance: Based on the presented experiments, I cannot see a use case for choosing this method instead of SGD or other competitors.\n\nPros:\n-\tThe introduced concept is interesting\n-\tThe language in the paper is simple, making it easy to follow.\n-\tSeveral of the decisions in the paper are explained intuitively. \n-\tI appreciate the honest comparison when specifying that the competitors’ implementation is suboptimal, and in the comparison with SGD.\n\nCons:\n-\tThe experiments do not entirely support the utility of the method, even as proof of concept (e.g. not varying the pool size, no results on computation speed comparison, competitors are not optimized, MNIST experiments have not converged).\n-\tSome details are missing (e.g. what is the loss function? Are there any constraints on choosing it?)\n-\tSome choices of parameters in the paper that are not justified (e.g. numbers in equation 1, loose stopping criteria)\n-\tThere are some minor typos and editing issues that need to be fixed.\n\nDetailed comments:\n•\tMajor issues:\n   1.\tPart of your motivation in the intro states that your algorithm is not replacing SGD, but “complementing” it. Can you please describe more specifically how this would be done? Give examples. When should one use your algorithm, and how do you use it “in tandem” with SGD. It is not clear from the paper when/how your algorithm should be used.\n   2.\tYou mention that: “Our algorithm should be able to handle sparse-rewards, exploration, exploitation, high-dimensionality, computational efficiency, pool-efficiency and sample-efficiency”. None of these are proved by experiments:\n      a.\thigh-dimensionality is not addressed\n      b.\tcomputational efficiency is mentioned in the conclusions, but there are no experiments comparing run times, and no discussion about algorithmic complexity.\n      c.\tpool-efficiency: indeed you use a small pool, but you didn’t mention how this is for other competitors, and you didn’t vary the size of this pool to see what happens.\n      d.\tsample-efficiency: you run MNIST on a subset of samples, but the accuracy you get is nowhere near state-of-the-art. The accuracy you get is the same as SGD, but in Figure 1,  none of the two losses has converged (see downward trend  of the curve), so maybe SGD would be better at convergence.\n   3.\tRL vs not RL: often times in discussion you mention “agents” and RL (e.g. entire section 1.1), but none of the experiments are about RL. While RL may be a natural application, since it is not evaluated here, I suggest reformulating the paper using supervised learning terms.\n   4.\tFrom the comparison with SGD in terms of number inference calls it sounds like your algorithm would be very expensive on large networks. Because of that, it would be good to see come computation time plots on larger networks. However, I appreciate that you did bring this comparison up!\n\n•\tMinor issues:\n   1.\tThe entire section 2 relies on picking samples based on some performance measure. However, you never mention what this measure should be. Is it a loss function? If so, are there any constraints on this?\n   2.\t“The historical elite is the best performing sample across all generations”. This is very vague. How is this quantified? It would help to introduce some mathematical notation for the loss, and describe how the elite is chosen in terms of this loss.\n   3.\tThe experimenter has to choose several constants in your model. Is there some intuition how to do that?\n   4.\tSection 1.1, paragraph 2: RL doesn’t necessarily need labeled data. What are other potential issues with simulations and how is this related to the proposed method?\n   5.\t“It is typical that any optimization algorithm is sensitive to the choice of parameters. However, tuning those parameters is beyond the scope of this paper.”  -- not true. If  you don’t make a sensible choice of  step size in SGD, the comparison will not be fair.  The fairest thing to do is to compare these methods, each using its best hyperparameters.\n   6.\tSection 3.2: “The test is terminated once the algorithm achieves a loss of 0.15 or lower”. You don’t mention what loss is used to be able to tell if 0.15 is  small enough. From the plot it seems the models have not converged.\n   7.\tTable 1 caption does not specify the metric of the numbers in the table.\n   8.\tThe conclusion mentions new results on a RL experiment, which are not discussed in the paper.\n   9.\tPlease briefly define what are evolutionary optimization algorithms in the intro.\n\n•\tTypos and writing style:\n   1.\tThe citation style is most often used incorrectly. Put the reference in brackets when is not part of the sentence (e.g.  in first paragraph: Object Detection (Liu et. Al, 2016), … ), and without brackets when you specifically refer to that paper title.\n   2.\tTypos: last paragraph of background (“also use computer vision is a starter”),  Section 3 second paragraph (“That is, the models for copied into each”).\n\nFinal remarks and advice: \nIt seems that the authors pose this work more as a proof-of-concept, than a new algorithm ready to be used in practice. While the idea definitely has value, I believe the experiments are not sufficiently convincing or adequately chosen, to prove what you want to show. If sample complexity is your selling point, gear the experiments more in that direction and show what happens for various sample sizes and pool sizes, for you and competitors. If it is computationally efficient (as you say in the conclusion), show experiments comparing run times with other competitors. If complexity is the selling point, tell us exactly how much computation is required in every generation, compared to other methods. Also, if SGD is your competitor, find a scenario where SGD doesn’t work but your method does. Moreover, for the presentation of the algorithm in section 2, an algorithm box summarizing the method would be of great help. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}