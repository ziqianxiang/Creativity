{
    "Decision": {
        "metareview": "The paper proves that the Donsker-Varadhan lower bound on KL divergence cannot be used to estimate KL divergences of more than tens of bits, and that more generally any distribution-free high-confidence lower bound on mutual information cannot be larger than O(ln N) where N is the size of the data sample. As an alternative for applications such as maximum mutual information predictive coding, a form of representation learning, the paper proposes using the cross-entropy upper bound on entropy and estimating mutual information as a difference of two cross-entropies. These cross-entropy bounds converge to the true entropy as 1/\\sqrt(N), but at the cost of providing neither an upper nor a lower bound on mutual information. There was a divergence of opinion between the reviewers on this paper. The most negative reviewer (R3) thought there should be experiments confirming that the DV bound fails when mutual information is high, was concerned that the theory applied only in the case of discrete distributions, and was concerned that the proposed optimization problem in Section 6 would be challenging due to its adversarial (max-inf) structure. The authors responded that they felt the theory could stand on its own without empirical tests (a point with which R1 agreed); that although their exposition was for discrete variables, the analysis applies to the continuous case as well; and that they agreed with the point about the difficulty of the optimization, but that GANs face similar difficulties. Because R3 did not participate in the discussion and the AC believes that the authors adequately addressed most of R3's issues in their response and revision, this review has been discounted. The next most negative reviewer (R2) wanted a discussion relating the ideas in this paper to kNN and kernel-based estimators of mutual information, wanted an empirical evaluation (like R3), and was concerned about whether the difference of cross-entropies provides an upper or lower bound on mutual information. In their response and revision the authors added some discussion of kNN methods (but not enough to make R2 happy) and clarified that the difference of cross-entropies provides neither an upper nor a lower bound. The most positive reviewer (R1) thinks the theoretical contribution of the paper is significant enough to justify publication in ICLR. The AC likes the theoretical work and feels that it raises important concerns about MINE, but concurs with R2 and R3 that some empirical validation of the theory is needed for the paper to appear in ICLR. The authors are strongly encouraged to perform an empirical validation of the theory and to submit this work to another machine learning venue.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Strong theoretical analysis that highlights a weakness in MINE, but some experiments would have been nice"
    },
    "Reviews": [
        {
            "title": "Timely discussion of approaches for approximating mutual information ",
            "review": "This paper considers the problem of estimating bounds on the mutual information. It begins by showing that popular recent estimators (e.g. MINE) are flawed, since they rely on the Donsker-Varadhan bound that cannot be estimated efficiently. They then point to entropy upper bounds as a much more feasible approach to MI approximation, and propose a framework for using them in practice. These upper bounds converge as 1/sqrt(N) to the true entropy value, making them potentially viable in practice to obtain reasonable approximations of MI. \n\nGiven the significant recent attention payed to the MINE estimator and to MI estimation in machine learning generally, I think the message of this paper is very critical to the machine learning community. This analysis of the MINE estimator alone would warrant publication. \n\nThere are currently some weaknesses to this paper, however, when compared to the typical ICLR paper. If the following issues are addressed I am prepared to raise my scoring of this paper:\n1. Provide some intuition on how to apply this type of analysis to the continuous-valued case, or some reason why such an analysis would require a different framework. The more detail the better, if bounds analogous to Theorem 1 could be proved for continuous variables and added to the paper, that would be excellent.\n2. Section 4 as written is intuitively clear, but could greatly benefit from a full rigorous analysis. There is plenty of space in the paper to do this (and the supplement is available if needed). If such an analysis can’t be done, this section should be deleted.\n3. Some empirical example of MINE converging slowly in practice would greatly add to the impact of the paper.\n\nMinor issues: \nLast sentence of Section 1 should clarify what “true entropy” means. As written it is ambiguous between the actual entropy or the cross-entropy, since both are entropies.\n\nI understand the nested optimization problem in Section 6, but the presentation is somewhat unclearly written. More exposition here would help, along with a more clear step-by-step explanation of the practical procedure.\n\nEDIT: The authors have addressed my concerns and I have raised my score.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Promising work from theoretical standpoint",
            "review": "This paper is about estimating mutual information in high dimensional settings. This is a very challenging open problem, that is of interest to a diverse set of research communities.\n\nIn this paper, it is theoretically argued that the recent proposed mutual information (lower bound) estimator, MINE, that is based on the Donsker-Varadhan representation of the corresponding KL divergence expression for mutual infortmation, is fundamentally flawed for high dimensions (of discrete variables).  It is further shown that lower bounds for joint entropy are hard to obtain due to exponential sample complexity. So, the authors suggest to instead obtain an upper bound for each entropy term in the mutual information expression; cross-entropy is the suggested upper bound for an entropy term.\n\nI have some basic questions as the following.\n\nSince the recent KL divergence based MI estimator, MINE, is inaccurate in high dimensions, there should be at least a discussion on connections between your estimator and the classic nearest neighbor distances based estimator of Kraskov et al. and the extensions (I suppose, even for discrete variables, one can compute distances to obtain nearest neighbors efficiently). Also, there are kernel functions based estimators.\n\nThere is no discussion in the paper about the errors accumulating from individual entropy terms in the mutual information expression. Kraskov et al. talk about this problem of accumulating errors in their seminal paper and propose not to compute the entropy terms individually. What you are proposing is in contrast to their clever observations.\n\nDoes the analysis on upper bound for entropy term also apply to the conditional entropy in the mutual information expression ? I think, there are more subtleties that should be explained.\n\nSince the proposed approach upper bounds entropy using cross entropy term (i.e. using some machine learning model like a neural network), it is even more important to show solid empirical evaluation, for synthetic as well as real world data.\n\nThere is a subtle difference between estimating mutual information and proposing an upper/lower bound for it. At present, it is not clear if the proposed upper bound of entropy would lead to an overall upper bound or lower bound for the mutual infortmation expression. The latter is important to know both in the context of optimization based on mutual information maximization (it should be lower bound in such case), as well analyzing mutual infortmation to under complex dynamics such as in brain.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "ICLR 2019 Conference Paper277 AnonReviewer3",
            "review": "This paper studied the Donsker-Varadhan lower bound of KL-divergence. The authors show that with high probability, the DV lower bound is upper bounded by log of the sample size, so if the true KL-divergence is very large, then exponential sample size is needed to make the DV lower bound tight. The same argument holds true for any distribution-free high-confidence lower bound (such as DV lower bound) for KL divergence. Then the authors proposed to use an upper bound for entropy instead of lower bound for mutual information. \n\nThe idea of the paper is interesting and the proof of Theorems 1 and 2 are valid. Especially I like the idea of Theorem 1, which proves that any distribution-free high-confidence lower bound for KL divergence is upper bounded by log of sample size. This idea is similar to the paper in (Gao et al 15') which shows that mutual information estimator is upper bounded by log(N).\n\nHowever, this paper contains many fatal flaws, which significantly weaken the quality of this paper. Precisely,\n\n1. The DV lower bound is just an alternative of mutual information, helping MMI predictive coding algorithm to find good coding functions C_x and C_y. The goal of MMI predictive coding is not to estimate the mutual information I(C_x(x), C_y(y)) precisely, instead, the goal is to find good coding functions. The fact that DV lower bound is small means that we can not estimate mutual information through DV lower bound, but it does not directly imply that we can not find the coding functions. I expect some experiments to show that when mutual information is large, MMI predictive coding using DV lower bound can not find good coding functions.\n\n2. In Section 3, the formula after the proof of Outlier Risk Lemma and before Theorem 1 (btw, it is better to have numbers for these formula) seems to be problematic. The first formula shows that E_{z~q} e^{F(z)} >= (1/N)e^{F_max}, then we plug it in (4). But in (4) there is negative ln of E_{z~q} e^{F(z)}, so we should have KL(P,Q) <= something, correct? This may be a typo but this typo is so important such that it affect the readability a lot. Theorem 1 is correct but the paragraphs before Theorem 1 confuse the reader a lot.\n\n3. In Section 4, are you considering classical entropy for discrete random variables, or differential entropy for continuous random variables? I assume you are considering the latter, since most people of machine learning community are interested in continuous random variables. Then your statement of I(X;Y) <= H(X) is incorrect, since for continuous random variables, H(X|Y) can be negative. See (Thomas & Cover, Chapter 8) for a reference.\n\n4. Related to problem 3, if you are considering continuous random variables, then the statement I(X;Y)=H(X)+H(Y)-H(X,Y) is not always correct. There are cases that H(X) is infinite, H(Y) is infinite, H(X,Y) is infinite but I(X;Y) is finite. These cases does not only exist in mathematical books, but also exists in practice, especially when the data is located on a low-dimensional manifold embedded in a high-dimensional space. Therefore, your approach of decompose the mutual information is not always possible.\n\n5. Regarding to your proposed optimization problem in Section 6 (also it is better to have a number), I have some concerns. Since it involves max over \\Psi outside, and inf over \\Theta and inf over \\Phi inside, so I wonder how do you solve this problem? Can you guarantee that the solution can provide good coding functions C_x and C_y? Also, it seems that this optimization problem is proposed as an improvement over the DV lower bound method, so I wish to see some experiment showing that this method is better than the DV lower bound method, at least for some synthetic datasets.\n\nBecause of the above mentioned flaws (especially 3 and 4, and lack of experiments), I think the paper is below the standard of ICLR conference.\n\nReferences:\n[1] Efficient Estimation of Mutual Information for Strongly Dependent Variables, by Gao, Ver Steeg and Galstyan, AISTATS15'\n[2] Elements of Information Theory, 2nd edition, by Thomas and Cover.\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}