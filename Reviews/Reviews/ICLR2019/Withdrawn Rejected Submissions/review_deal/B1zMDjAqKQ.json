{
    "Decision": {
        "metareview": "The submission introduces a model that does learning of multisensory representations (by predicting one from the other), with an autoencoder structure. Generally, the reviewers liked the overall idea of the work, but found the clarity lacking, the evaluation insufficient (and not particularly state of the art), the requirement for paired training data quite limiting and the choices (VAE sometimes, autoencoder other times) somewhat ad hoc.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "metareview"
    },
    "Reviews": [
        {
            "title": "unsupervised approach to learning multi sensory binding with expectation learning",
            "review": "The paper develops a multi sensory model that binds audio/visual features using a self-organizing binding learning layer that takes as input the latent layers of two separate auto-encoders.  The binding is learned in an unsupervised way through Hebbian learning, which is an interesting way to implement temporal binding. \n\nMy concerns are as below:\n\n- How is the accuracy computed for the experimental data? Is it done moment-by-moment (per frame of the video?)? And if so, how is the accuracy of the audio assessed, since it's sampled at a much higher frequency?\n\n- Fig 2 should be a bar graph, since each class of animal is categorical, and the accuracies should be shown with standard deviations.\n\n- Abstract is written in a very confusing way that does not make clear succinctly what are the contributions of the paper. Generally, the paper may have been assembled in some haste.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a model that mimics expectation learning i.e. learning multisensory representations by training to predict the other modalities from a given modality",
            "review": "Overview and contributions: The authors introduce a model that mimics expectation learning (i.e. learning multisensory representations by training to predict the other modalities from a given modality, for example, image to audio, audio to image). The proposed model is based on an autoencoder structure with a recurrent self-organizing network for\nmultisensory binding of latent representations. The authors perform experiments to show the reconstruction of image and audio signals given the other, as well as discriminative results on audio and image classification.\n\nStrengths:\n1. The paper is well motivated by the point of view of human learning. I really liked the abstract and introduction!\n2. I liked the recurrent self-organizing network presentation and usage.\n\nWeaknesses:\n1. While the paper is well motivated, I believe that the presence of multisensory expectation learning depends heavily on the type of multisensory data. For some modalities such as audio, it is clear what the mapping to language is (audio-to-language transcribing). For language-to-audio, there are multiple audio translations depending on the different tone of voice used by each person. Image-to-audio translation is also a one-to-many mapping. So I have concerns about how the model would work in these cases depending on the data used.\n2. I don't believe that the proposed model achieves state-of-the-art results: from Table 1, image classification performance is outperformed by Inception V3, and for both modalities, I'm not sure why the authors did not compare with more recent baselines. The best audio baseline is from 2016...\n3. It seems that this approach needs paired multisensory data for training, which limits the amount of training data as compared to unisensory models. Also, what if some sensors are noisy or missing? Is this model robust to such cases?\n\nQuestions to authors: \n1. Refer to weakness points.\n2. Can you comment on when you think multisensory expectation learning would work, and when it wouldn't? What types of data do we need, and from which modalities/sensors?\n\nPresentation improvements, typos, edits, style, missing references:\n1. Page 2: Our hybrid approach allowed -> Our hybrid approach allows\n1. Page 7: audiotry recognition -> auditory recognition\n2. Page 7: while improved the visual stimuli in 3% -> and improving ... by 3%\n3. Multiple other typos and awkward phrasing, I would suggest the authors spend more time proof-reading their paper before submission.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting idea, but writing quality could be improved",
            "review": "The authors proposed an unsupervised learning framework to learn multisensory binding, using visual and auditory domain from animal videos as example. First, the visual and auditory inputs are autoencoded, and these latent codes are binding using a recurrent self-organizing network (Gamma-GWR). Furthermore, the authors proposed the expectation learning idea, which is inspire by psychology literature. In short, after the first pass of training using the real data. The authors fine tuned the model to bind the real data from one domain and the reconstructed data from another domain. This could be a good idea, as the authors pointed out, human usually bind all kinds of yellow bird to a same mental 'chirping' sounds. So, this expectation learning could potentially group the representation to a canonical one. Also, the authors showed in Table 1 that with the expectation learning, the model's recognition accuracy is improved a bit. I think it would be interesting to show the reconstruction output example (as in Fig. 3) for both model with and without expectation learning. To see if it is as the authors claim, that the model with expectation learning is reconstructing the missing modality with more canonical images/sounds. (This may not be the goal in other practice, though I'm convinced it is a potentially good psychological model as it explain well the multisensory imagery effect (Spence & Deroy, 2013). \n\nI found this manuscript quite hard to follow though. The description seems sometime not flowing very smoothly. And there are some clear typos and mess up of math notations make the reading unpleasant. I have noted down several points below, and hope the authors could improve in the next iteration.\n\n1. The description of variational autoencoder is not well written. The citation (Chen, 2016) is not the standard VAE paper people usually cite (unless the author is adopting something specific from the Chen's paper.). For example, the authors wrote \"the KL divergence between the encoded representation and a sample from the Gaussian distribution\" which sounds incorrect to me.\n\n2. Why a Variational autoencoder is necessary for visual domain, but a regular autoencoder is used in auditory domain?\n\nTypos:\n1. page 2, 2nd line: a online --> an online\n2. Use subscript I-1 to mean the winner neuron at t-1, I think this is not quite clear. I suggest to follow the notation in (Parisi & Wermter 2017), use I(t-1), which is easier to follow.\n3. page 7, 2nd line: more than 17% for audio.  -> for vision.\n4. page 8, 3rd line: not on the original high-abstraction data. Do the authors mean highly specific data? That seems make more sense.\n5. Several notation mismatch here and there. for example, in formula 6 it is w_j^s, but in the text below it become w_{j,s}.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}