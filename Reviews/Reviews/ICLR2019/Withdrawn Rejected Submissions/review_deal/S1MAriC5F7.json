{
    "Decision": {
        "metareview": "The paper proposes and evaluates an asynchronous hyperparameter optimization algorithm.\n\nStrengths:  The experiments are certainly thorough, and I appreciated the discussion of the algorithm and side-experiments demonstrating its operation in different settings.  Overall the paper is pretty clear.  It's a good thing when a proposed method is a simple variant of an existing method.\n\nWeaknesses:  The first page could have been half the length, and it's not clear why we should care about the stated goal of this work.  Isn't the real goal just to get good test performance in a small amount of time?  The title is also a bit obnoxious and land-grabby - it could have been used for almost any of the comparison methods.  The proposed method is a minor change to SHA.  The proposed change is kind of obvious, and the resulting method does have a number of hyper-hyperparameters.\n\nConsensus:  Ultimately I agree with the reviewers that is just below the bar of acceptance.  This does seem like a valid contribution to the hyperparameter-tuning literature, but more of an engineering contribution than a research contribution.  It's also getting a little bit away from the subject of machine learning, and might be more appropriate for say, SysML.",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Solid experiments, nice and simple method, but too incremental"
    },
    "Reviews": [
        {
            "title": "Good paper, but I have queries about the experiments",
            "review": "In this paper, the authors propose an extension of the Successive Halving Algorithm (SHA) allowing its deployment in a distributed setting, a so-called massively parallel setting. The proposed algorithm is relatively straightforward and the main contribution appears to be in the experimental validation of the method. \n\nQuality: The work appears to be of a good quality. The experiments could benefit from some more rigorous statistical analysis, but that would most likely require a higher number of repetitions, which is understandably hard to do in a large-scale setting. \n\nClarity: In general, the paper is well written. However, the presentation of Algorithm 3.2 was confusing. More comments on this below. \n\nOriginality: The contribution is incremental, but important.\n\nJustification: The problem is an important one. There is room for more research exploring the optimization of hyperparameters for large architectures such as deep networks.\n\nOverall I think the paper is good enough for acceptation, but I found some elements that deserve attention. The experimental section is a bit perplexing, mostly in the first experiments on the sequential approaches. The final experiment on the large-scale setting is disappointing because it only compares ASHA with an underpowered version of Vizier, so the demonstration is not as impressive as it could be. Furthermore, PBT was discarded based on results from a small-scale benchmark, which were not very convincing either (possibly significantly better on one of two versions of the CIFAR-10 benchmark). If the authors have other reasons as to why PBT was not a good candidate for comparison, they should bring them forth. \n\nFind below some more comments and suggestions:\n\nRegarding Algorithm 3.2, promotability (line 12) is never defined explicitly, so this can lead to confusion (it did in my case). I think promotability also requires that at least eta jobs are finished in the rung k, am I correct? If so it is missing from the definition. Perhaps a subroutine should be defined?\n\nIt feels odd to me that the first experiment mentioned in the paper is tucked away in Appendix 1. I find it breaks the flow of the paper. The fact that SHA outperforms Fabolas I believe is one of the important claims of the paper. Hence, the result should probably not be in an Appendix. I would suggest putting Figure 3 in the Appendix instead, or removing/condensing Figure 2, which is nice but wastes a lot of space. I also fail to grasp the difference between the CIFAR-10 benchmarks in Appendix 1 and those in Section 4.2. It seems they could be joined in one single experiment comprising SHA v Hyperband v Fabolas v PBT (perhaps removing a variant of Hyperband to reduce clutter). \n\nI also do not think the comparison between SHA and ASHA in a sequential case is relevant. The behavior of ASHA in the distributed case will be different than in the sequential case, so the comparison between the two variants of SHA does not bring any useful information. If I followed the method correctly, in the 9 worker example with eta=3, the first round of jobs would be all jobs at the lowest bracket (s=0), which would be followed by a round of jobs at the next bracket (3 jobs at s=1), and so on. Hence, the scheduling behavior would be exactly the same as SHA (albeit distributed instead of sequential). Am I correct in my assessment? If so, perhaps ASHA should just be removed from the sequential experiments. \n\nAs a point of sale, it might be interesting to provide the performance of models with manually tuned hyperparameters as a reference (i.e., the recommended hyperparameters in the reference implementations of those works that were cited).\n\nAppendix A.2 serves no purpose and should probably be removed.\n\nSection 4.3 & 4.3.1: In both cases, what is the running time R for a single model?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "well written paper but with only little novelty and missing baseline comparison",
            "review": "The paper describes a simple, asynchronous way to parallelize successive halving. In a nutshell, this method, dubbed ASHA, promotes a hyperparameter configuration to the next rung of successive halving when ever possible, instead of waiting that all configurations of the current rung have finished. ASHA can easily be combined with Hyperband which iteratively runs different brackets of successive halving. The empirical evaluation shows that the proposed method outperforms in the most cases other parallel methods such as Vizier and population based training.\n\n\nOverall, the paper is well written and addresses an interesting and important research problem.\nHowever, the paper contains only little novelty and proposes a fairly straight-forward way to parallelize successive halving.\nEven though it shows better performance to other approaches in the non-sequential setting, its performance seems to decay if the search space becomes more difficult (e.g CNN benchmark 1 and 2 in Section 4.2), which might be due to its increasing number of mispromoted configurations. \n\nBesides that the following points need to be clarified:\n\n1) I am bit worried that asynchronous Hyperband performs consistently worse than asynchronous successive halving (ASHA). Why do we need Hyperband at the first place if a fixed bracket for ASHA seems to work better?\n   It would be interesting to see, how the performance of ASHA changes if we alter its input parameters.\n\n2) Why is PBT not included for the large-scale experiments in Section 4.3 given that it is a fairly good baseline for the more difficult search space described in Section 4.2?\n\n3) Even though I follow the authors' argument that the parallel version of Hyperband described in the original paper is slower than ASHA / asynchronous Hyperband, it doesn't promoted suboptimal configurations and hence might achieve a better performance at the end. Why is it not included as a baseline?\n\n4) I am also missing a reference and comparison to the work by Falkner et al., that introduced a different way to parallelize Hyperband which can make use of all workers at any time. \n\n\nFalkner, Stefan and Klein, Aaron and Hutter, Frank\nBOHB: Robust and Efficient Hyperparameter Optimization at Scale\nProceedings of the 35th International Conference on Machine Learning (ICML 2018)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Massively parallel hyperparameter tuning ",
            "review": "\n\nAuthors describe a massively parallel implementation of the successive halving algorithm (SHA). Basically, the difference between SHA and its asynchronous version ASHA, is that the later can promote configurations to the next rung, without having to wait that a previous rung is completed. \n\nI think this is a valuable contribution, although I am not sure if it has the level required by ICLR. The technical contribution of the paper is minor: it is a simple modification to an existing  methodology. However, authors perform an extensive evaluation and show that the implementation reaches SOTA performance. \n\nAuthors list 5 bullets as contributions of this paper. Whereas it is clear that the main contribution is the ASHA algorithm, the rest of contributions are in fact results that show the validity of the first contribution. I mean those contributions are the experimental evaluation to prove ASHA works. \n\nCan authors provide more details on what a configuration is? I could think of as a snapshot of a model (with fixed architecture and hyperaramenters), but I do not think my understanding is totally correct: the x-axis in Figure 1 has 9 configurations which are explored throughout rungs, hence the same configuration is evaluated many times?  \n\nAuthors claim that the method can find a good configuration in the time is required to train a network. How is this possible?, I guess is because the evaluation of each configuration is done in a small validation subset, this, however is not stated by the authors. Also, depending on the size of the validation set and implementation details, this is not necessarily an \"amazing\" result. \n\nWhy results from Section 4.1 are a contribution?, what is the impact of these results? Basically you compare existing methodologies (plus the proposed ASHA method). \n\n\n\nState-of-the ART (abstract)\nAuthors mention the theoretical benefits of SHA, but they do not emphasize these in the paper, can they elaborate on this?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}