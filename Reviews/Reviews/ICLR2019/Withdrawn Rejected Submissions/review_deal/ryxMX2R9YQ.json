{
    "Decision": {
        "metareview": "\nThis paper introduces conditional graph neural fields, an approach that combines label compatibility scoring of conditional random fields with deep neural representations of nodes provided by graph convolutional networks. The intuition behind the proposed work is promising, and the results are strong.\n\nThe reviewers and the AC note the following as the primary concerns of the paper: (1) The novelty of this work is limited, since a number of approaches have recently combined CRFs and neural networks, and it is unclear whether the application of those ideas to GCNs is sufficiently interesting, (2) the losses, especially EBM, and the use of greedy/beam-search inference was found to be quite simple, especially given these have been studied extensively in the literature, and (3) analysis and adequate discussion of the results is missing (only a single table of numbers is provided).\nAmongst other concerns, the reviewers identified issues with writing quality, lack of clear motivation for CRFs, and the selection of the benchmarks.\n\nGiven the feedback, the authors responded with comments, and a revision that removes the use of EBM loss from the paper, which the reviewers appreciated. However, most of the concerns remain unaddressed. Reviewer 2 maintains that CRFs+NNs still need to be motivated better, since hidden representations already take the neighborhood into account, as demonstrated by the fact that CRF+NNs are not state-of-art in other applications. Reviewer 2 also points out the lack of a detailed analysis of the results. Reviewer 2 focuses on the simplicity of the loss and inference algorithms, which is also echoed by reviewer 2 and reviewer 1. Finally, reviewer 1 also notes that the datasets are quite simple, and not ideal evaluation for label consistency given most of them are single-label (and thus need only few transition probabilities).\n\nBased on this discussion, the reviewers and the AC agree that the paper is not ready for acceptance.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "Simple loss/inference, lack of thorough evaluation"
    },
    "Reviews": [
        {
            "title": "Novelty is incremental.",
            "review": "This paper proposes a conditional graph neural network to explore the label correlation. In fact, this method combines graph convolutional neural network and CRF together. The novelty is incremental. No new insight is provided toward this topic. \n\nFor experiments, how did you train DeepWalk (Node2vec)? By using all nodes or the selected training set? It should be clarified in the paper. \n\nAdditionally, Table 3 says the result of semi-supervised methods. But how did you conduct semi-supervised learning for DeepWalk or Node2vec?\n\n===================\nAfter feedback:\nThanks for authors' feedback. Some of my concerns have been addressed. But the novelty is still not significant. On the other hand, the dataset used in this paper is simple. Specifically, at least the first 3 datasets are single-label and the number of classes is not large. They are too simple to support the claim. It's better to use multi-label datasets to show that the proposed method can really capture the correlation of labels. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but not solid enough",
            "review": "The authors combine graph convolutional neural network and conditional random fields to get a new model, named conditional graph neural fields. The idea of the paper is interesting, but the work is not solid enough. Detailed comments come as follows,\n\n1. In the proposed model, the authors treat the pairwise energy as prior and they do not depend on any features. Unlike the usual graph model in Eq (4), the authors further use the normalized \\hat A as a scale factor in the pairwise term. What is the intuition for this?\n\n2. The loss (10), which the authors claim that they are using, may not work. In fact, the loss cannot be used to use for training most architectures: ``while this loss will push down on the energy of the desired answer, it will not pull up on any other energy.''(LeCun et. al. 2006, A tutorial on energy-based learning). For deep structured model learning, please using piecewise learning, or joint training using some common CRF loss, such as log-loss. In fact, the authors are not using the energy-based loss as they have constraints on unary and pairwise terms. In fact, if we ignore the pairwise term in (11), the loss becomes log-loss for GCN. With the pairwise term, the loss is somehow like the loss for piecewise learning but the constraints on U is wrong (for piecewise learning, U should sum to 1).\n\n3. The inference procedure is too simple that it can hardly find the near-optimal solutions. In fact, there exists an efficient mean-field based inference algorithm (Shuai Zheng et. al., 2015). Why did the authors choose a simple but poor inference procedure?\n\nComments After rebuttal\n==========\nThank you for adress my concerns.\n\nThe response and the revision resolved my concern (1). However, the most important part, the possibly problematic loss is not resolved. It is true that sometimes (10) can achieve good results with good regularizers or a good set of hyperparameters. However, theoretically, the loss is ]only pushed down the desired answer, which may make the training procedure quite unstable. Thus I still think that a different loss should be used here.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Reviewer comment",
            "review": "This paper proposed a combination of graph neural networks and conditional random field to model the correlation between node labels in the output.  In typical graph neural nets the predictions for nodes are conditionally independent given the node representations.  This paper proposes to use a CRF to compensate for that.  In terms of the approach, the authors used GCNs to produce unary potentials for the CRF, and have the pairwise potentials on each edge to model the correlation of labels of neighboring nodes.  Learning is done by optimizing pseudo-likelihood and the energy loss, while inference is performed through a couple heuristic processes.\n\nCombining neural nets with CRFs is not a new idea, in particular this has been tried before on image and sequence CRFs.  It is therefore not surprising to see an attempt to also try it for graph predictions.  The main argument for using a CRF is its ability to model the correlations of output labels which was typically treated as independent.  However this is not the case for deep neural networks, as it already fuses information from all over the input, and therefore for most prediction problems it is fine to be conditionally independent for the output, as the dependence is already modeled in the representations.  This is true for graph neural networks as well, if we have a deep graph neural net, then the GNN itself will take care of most of the dependencies between nodes and produce node representations that are suitable for conditionally independent output predictions.  Therefore I’m not convinced that CRFs are really necessary for solving the prediction tasks tried in this paper.\n\nThe learning and inference algorithms proposed in this paper are also not very convincing.  CRFs has been studied for a long time, and there are many mature algorithms for learning them.  We could do proper maximum conditional likelihood learning, and use belief propagation to estimate the marginals to compute the gradients.  Zheng et al. (2015) did this for convnets, we could also do this for graph CRFs as belief propagation can be easily converted into message passing steps in the graph neural network.  Pseudo-likelihood training makes some sense, but energy loss minimization doesn’t really make sense and has serious known issues.\n\nOn the other hand, the proposed inference algorithms does not have good justifications.  Why not use something standard, like belief propagation for inference again?  Our community has studied graphical models a lot in the last decade and we have better algorithms than the ones proposed in this paper.\n\nLastly, the experiments are done on some standard but small benchmarks, and my personal experience with these datasets are that it is very easy to overfit, and most of the effort will be put in to prevent overfitting.  Therefore more powerful models typically cannot be separated from overly simple models.  I personally don’t care a lot about the results reported on these datasets.  Besides, there are a lot of questions about the proposed model, but all we get from the experiment section are a few numbers on the benchmarks.  I expect studies about this model from more angles.  One more minor thing about the experiment results: the numbers for GraphSAGE are definitely wrong.\n\nOverall I think this paper tackles a potentially interesting problem, but it isn’t yet enough to be published at ICLR due to its problems mentioned above.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}