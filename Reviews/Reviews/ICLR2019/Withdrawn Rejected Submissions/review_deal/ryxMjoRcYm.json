{
    "Decision": "",
    "Reviews": [
        {
            "title": "somewhat unclear, possible modeling contributions",
            "review": "The paper considers the problem of solving a continuous-state MDP where the traces of the resulting policy satisfy some Linear Temporal Logic property. The authors use a version of finite-state machines called the Limit Deterministic BÃ¼chi automaton to represent the desired logic. The paper then defines a product MDP that extends the state space of the original MDP to incorporate those logic states in the automaton. Finally, to solve this extended MDP, the paper proposes to use neural fitted Q-iteration. Let n be the number of those logic states. They propose to use n neural networks to represent the value function, one Q-network for each logic state. \n\nI think the problem formulation is a little confusing. It is unclear whether there is a given reward function and the objective is to maximize the discounted cumulative reward, or the objective is to reach some predefined accepting states. It seems to be the latter, but the objective is not clearly stated in the paper. Also, it is unclear to me why the authors choose to define rewards as in equation (4) in their proposed algorithm. There is hardly any explanation on the parameters or justification for their choice. \n\nI think the main contribution of this paper is on the modeling side, that one may enlarge an MDP to incorporate certain logic information, which may be helpful in defining tasks.  There doesn't seem to be a significant contribution on the algorithmic side.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting direction but not easy to understand and weak empirical evaluation and weak discussion of related work ",
            "review": "The paper extends neural fitted q-iteration in such a way that \nthe traces of the resulting policies satisfy a Linear Temporal \nLogic (LTL) property. \n\nThe main idea is to define a product MDP of the original \nMDP and a Limit Deterministic Buechi Automaton (LDBA).\nUnfortunately, the paper is not well written and presented.  \n\n\nThe part on LTL is hard to grasp for an informed outsider such as the reviewer. \nThe meanings of the operators are not introduced in an easy to grasp way. For instance, \nwhat exactly is L in the Definition of 3.1? What are words in general? Likewise, the product MDP defined is hard to grasp since no intuitive examples are provided. The intuition is that the LDBA is constraining the MDP according to the LTL. However, how \nthis is actually done algorithmically is not explained. The paper, as presented, just boils down to \"we build the product MDP\". In this way, one can also use say some first order logic, since properties of the product MDP are not discussed. For instance, \ncan the product MDP explode in size? Likewise the resulting LCNFQ is not really providing more insights. It essentially just says, learn n feed-forward neural networks. Does\nthe order of the Rprop upates really make no difference? Should be follow the temporal order? Is it really the best to train the networks independently? Why not training a single neural network that take the LDBA state as input (using some embedding)? Remark 4.2\nonly shows that one particular encoding does not work. This is even more important given that there are relational MDPs in the literature that are unfortunately not discussed, \nsee e.g. \n\nScott Sanner, Craig Boutilier:\nPractical solution techniques for first-order MDPs. \nArtif. Intell. 173(5-6): 748-788 (2009)\n\nand the overview in \n\nLuc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole:\nStatistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers 2016 \n\nThe authors should discuss the difference to these approaches. Likewise, \nthe authors should connect to the large body of work on safe reinforcement learning, \nwhere general constraints on the traces of a RL agent are imposed, too. Finally, the experimental evaluation should be improved. Right now, only one domain is considered: \nautonomous Mars rover. To show that this is of general interest, other domains should be considered. \n\nTo summarize, the general direction is really important. However, the paper is hard to follow, the experimental evaluation is not convincing, and there is missing related work.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Neat idea but somewhat underwhelming proposition",
            "review": "    The authors of the article propose to use logical propositions to simplify the expression of high-level objectives in reinforcement learning problems. For that, from logical formulas, an automaton is generated and, from this automaton, is deduced a reward function. The reward function is defined in such a way that the agent reaches an acceptable absorbing state. Thus, it is easier to define sub-objectives that should have been encoded by hand in the reward function. The authors formally define the combined framework of MDP with LDBA, then they describe an adaptation of neural fitted iteration, Voronoi quantizer and fitted value iteration in this context. They compared their new methods in 2 different environments with different constraints.\n\n\n    The paper is well written and easy to read. However, these works do not really overcome the problem of safety or expressing \"complex high-level control objectives\" because logic rules are only used to define a reward function. Once this function is defined, there is no other applied constraints, so during the learning phase there is no more safety than using conventional RL. Moreover, on the example studied, adding a dimension to the state and defining an adequate reward function with this additional dimension would allow obtaining the same behaviors. The proposed framework would be more convincing if it were illustrated with an example where the reward function is really easier to define with the proposed approach, but difficult to define without it.\n\n    Pros :\n    - beautiful clean formalization\n    - interesting experimentation\n\n    Cons :\n    - policies/algorithms are not directly logically constrained but more guided by constraints\n    - the obtained reward function is sparse, e.g., subgoals don't receive any additional rewards\n    - experiments are difficult to reproduce (no link to source, missing a full description of neural networks used, etc.)\n    - some details are not necessary for the overall understanding, while some essential elements are missing (what happens if a non-accepting absorbing state is reached during the training phase, what is the concrete inputs of your neural networks ? how your discrete actions are handled in your neural networks, what happens if y = 0 for LCNFQ?)\n    - satisfiability of the generated policies is only little studied : what their proportions during the learning phase are, ...\n\n\n    Others questions/remarks :\n    - it seems that you do not use any exploration strategy, is it because your environment is already stochastic on the action ?\n    In this case, \"LCNFQ stops when a satisfying policy is generated.\" seems to be a bad idea as the algorithm might just have been lucky.\n    - it would be better if the definition of the reward function of the general framework does not contain a LCNFQ specific hack\n    - In (4), the reward function should also depend on the next state if it is deterministic. Besides, the definition of rand is slightly ambiguous, does it return a new random value every time the reward is evaluated?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}