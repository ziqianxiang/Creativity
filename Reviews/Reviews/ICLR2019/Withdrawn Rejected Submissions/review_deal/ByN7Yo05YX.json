{
    "Decision": {
        "metareview": "This paper proposes adaptive neural trees (ANT), a combination of deep networks and decision tress. Reviewers 1 leans toward reject the paper, pointing out several flaws. Reviewer 3 also raises concerns, despite later increasing rating to marginally above threshold.  Of particular note is the weak experimental validation.\n\nThe paper reports results only on MNIST and CIFAR-10. MNIST performance is too easily saturated to be meaningful. The CIFAR-10 results show ANT models to have far greater error than the state-of-the-art deep neural network models.\n\nAs Reviewer 1 states, \"performance of the proposed method is also not the best on either of the tested datasets. Please clearly elaborate on why and how to address this issue. It would be more interesting and meaningful to work with a more recent large datasets, such as ImageNet or MS COCO.\"\n\nThe rebuttal fails to offer the type of additional results that would remedy this situation. Without a convincing experimental story, it is not possible to recommend acceptance of this paper.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "metareview: unconvincing experiments"
    },
    "Reviews": [
        {
            "title": "This paper proposes the Adaptive Neural Trees (ANT) approach to combine the two learning paradigms of deep neural nets and decision trees (DT).",
            "review": "The presented method is a generalization of a number of existing methods, which can be regarded as special cases. Overall the method seems to be novel. Meanwhile, I have two major questions:\nTo account for the bias issue, instead of a single DT, ensemble methods such as random forests are the popular choices. How ANT could benefit from relying on a single DT instead of a random forest type?\nThe datasets of MNIST and CIFAR-10 are used for many years and the performance is already saturated. As presented in Table.3, the performance of the proposed method is also not the best on either of the tested datasets. Please clearly elaborate on why and how to address this issue. It would be more interesting and meaningful to work with a more recent large datasets, such as ImageNet or MS COCO. \n\nThe response does not fully address my concerns. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "limited experiments, doubts about the method",
            "review": "The paper is written rather well, however I find the experiments incomplete and have some reservations about the\nmethod. My main points of critique are:\n\n1.  Combining DT & NN \nI have doubts that the way you combine DT &NN  you get the \"Best of both world\". In some ways your architecture also \nshares disadvantages of both:\n\n1.1 Interpretability\nBecause each node in the tree can a neural network (with arbritrary complexity), this approach looses one central advantage of DT, that is the interpretability of the result.    Each node in the tree can perform arbritrary complex (and hierarchical) \ncomputations. The authors only show one particular example (Fig. 2a), where the model has learned is a reasonable \nstructure.\n\n1.2 Complexity:\nThe whole architecture is much more complex than either a neural network or a decision tree. I expect that therefore training these is not easy, and expert knowledge in either DT  or NN may not be enough to use this model.\n\n\n2. Limited experiments\n\n2.1 The authors only consider 2 experiments from vision (MNIST & CIFAR 10) while proposing a universal method.  To show universality the authors should use data sets from different domains (e.g UCI data sets)\n\n2.2 The authors argue that a  strength of the method   is  that it uses a low number of parameters on average for a forward path (compared to the total parameter size).  I don't find this argument to be convincing. In the limit this would imply a high memorization of the  data.  Also, a similar case can be made for standard CNN, when a particular filter is mostly inactive for some data points.\n\n2.3 The interpretability of DT compared to NN I mentioned earlier.  To make the argument that their method learns the\nhierarchical structure of the data , the authors should have added experiments to support this, where  such a hierarchical structure is clearly present and can be evaluated empirically.\n\n\n--\n\nIn light of the extended experiments w.r.t. to 2.1 I increased my score from 5 to 6.  Overall, I still have doubts about the interpretability and complexity of the proposed method.  \n\nComplexity:  \"but all the intuitions needed would come solely from training NN\".    I disagree with this response.   The architecture is a mix between a tree (hard, decision-tree like error surface,  non-local) and neural network (smooth, mostly convex error surface). This also implies that the training process and its behavior will possess patterns and challenges of both approaches. \n\nInterpretability:  I think the method misses \"priors\" that enforce credit assignment.  Partitioning the problem in subp-roblems should be done via the tree components, whereas processing (such as image filtering) should be done in the network nodes. However,  the method does not enforce, or encourage this behavior, for instance\nvia constraints:   also nodes can do partitioning (because neural networks can approximate decision trees)  and edges can do processing (e.g. decisions-trees can be used for mnist).\n\nSo I still believe this to be a borderline paper, however, the experiments support a more general applicability.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "acceptable",
            "review": "\nThe authors proposed a new model Adaptive Neural Trees(ANTs) by combining the representation learning and gradient optimization of neural networks with architecture learning of decision trees. The key advantage of the new model ANTs  over the existing methods(Random forest, Linear classifier, Neural decision forest, et al) is: it may achieve high accuracy(above $90\\%$) with relatively much smaller number of parameters, as shown by the experiments on the datasets MNIST and CIFAR-10. Besides, the authors proposed single-path inference based on the greedily-selected leaf node to approximate the multi-path inferences with the full predictive distribution. The experiments show the single-path inference doesn't lose much accuracy but it saves memory and time. This paper is acceptable after minor modification.\n\n\nQuestions:\nIn the second line below equation (1), $n$ in $t_{e_{n(j)}}^{\\psi}$ is not defined. Also, should $t_{e_{1}}^{\\psi}$ be $t_{e_{n(1)}}^{\\psi}$? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}