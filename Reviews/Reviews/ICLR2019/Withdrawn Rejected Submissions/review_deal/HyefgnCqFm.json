{
    "Decision": {
        "metareview": "This paper introduces a few training methods to fit the dynamics of a PDE based on observations.\n\nQuality:  Not great.  The authors seem unaware of much related work both in the numerics and deep learning communities.  The experiments aren't very illuminating, and the connections between the different methods are never clearly and explicitly laid out in one place.\nClarity:  Poor.  The intro is long and rambly, and the main contributions aren't clearly motivated.  A lot of time is spent mentioning things that could be done, without saying when this would be important or useful to do.  An algorithm box or two would be a big improvement over the many long english explanations of the methods, and the diagrams with cycles in them.\nOriginality:  Not great.  There has been a lot of work on fitting dynamics models using NNs, and also attempting to optimize PDE solvers, which is hardly engaged with.\nSignificance:  This work fails to make its own significance clear, by not exploring or explaining the scope and limitations of their proposed approach, or comparing against more baselines from the large set of related literature.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Interesting area but doesn't meet quality or clarity standards"
    },
    "Reviews": [
        {
            "title": "Review of \"Learning space time dynamics with PDE guided neural networks\"",
            "review": "I very much like the aim of this work. This is a problem of interest to a wide community, which as far as I'm aware hasn't yet had much focus from the deep learning community. However, perhaps in part because of this, the paper reads as naive in places. Pages 1-4 are all background saying nothing new, but ignoring the effort made on this problem by other communities. There has been some work done o this problem within statistics, and within the Gaussian process community, to which no reference is made at all by the paper.\n\nThere are two novelties as far as I can see (these may or may not be novel - but they were novel to me). The first is use of NNs to model the system. The second is the multiple state restimation (MRSE) on page 5. I struggled to get a feeling about how successful these two aspects of the work are. The results section is difficult to follow, and doesn't compare the method to existing methods and so there is no baseline to say that this is successful or not. Thus I find it hard to judge the execution of the idea. What I really want to know reading a paper like this is should I use this approach? Because there is no comparison to existing methods, it leaves me unsure.\n\nOther comments:\n- Is the title correct? I don't see how these are PDE guided NNs? You've used data from a PDE to train the network and as a test problem. A PDE guided NN would, for me, know something about the dynamics (compare with recently work in the GP community where kernels are derived that lead to GPs that analytically obey simple PDEs). \n- There is an obvious link to work in the uncertainty quantification community, particularly around the use of multi-fidelity/ multi-level simulation. This paper is likely to be of interest to them and the link could be more explicit.\n- Page 3, after eq 2 - there is notation used here that is undefined Y_{t-k}^t\n- The simplifying assumption on page 3 is very strong and unlikely to hold for many systems. But it isn't clear to me whether this is necessary or not? Presumably if it doesn't hold then we may still get an approximation that could be useful, but it is just that we lose any guarantee the method will work.\n- I thought the MSRE idea was interesting. It wasn't very well explained or motivated, and it was unclear to me whether it works well or not from the results, or whether it is novel to this paper or not. But I'd like to have read more about it.\n- Is the trick in Section 8.2 original to this paper? If so, it seems a nice idea (I've not checked the detail). \n- Most of section 8.1 strikes me as unnecessary.\n- There are quite a few typos. In particular, words such as Markovian, Newtonian should be capitalised. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good step in learning Navier-Stokes equations, but lack compelling results",
            "review": "+ An interesting idea to learn the hidden state evolution and the state-observation mapping jointly\n+ The experiments on Euler's equation are slightly better than ResNet for 30 steps ahead forecasting in terms of MSE\n+ The paper is clearly written and well-explained\n\n- The model is not new: ResNet for state evolution and  Conv-Deconv for state-observation mapping\n- The difference between ResNet and the proposed framework is not significant, ResNet is even better in Figure 2\n- Missing an important experiment:  test whether the model can generalize, that is to forecast on different initial conditions than the training dataset\n- How does the model compare with GANs (Y. Xie* , E. Franz* and M. Chu* and N. Thuereyy, “tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow”)?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Some questions in lieu of a review, for now",
            "review": "I feel like I am missing something about this paper, so rather than a review, this is just mainly a long question making sure I understand things properly.  Ignore the score for now, I'll change once I get a clearer picture of what's happening here.\n\nThe network you propose in this paper is motivated by solving PDEs where, as in (1), the actual solution as they are computed numerically depends on the current spatial field of the state, as well as difference operators over this field (e.g., both the gradients and the Laplacian terms).  So, I naturally was assuming that you'd be designing a network that actually represented state as a spatial field, and used these difference operators in computing the next state.  But instead, it seems like you reverted to the notion of \"because difference operators can be expressed as convolutions, we use a convolutional network\", and I don't really see anything specific to PDEs thereafter, just general statements about state-space models.\n\nAm I understanding this correctly?  Why not just actually use the PDE-based terms in the dynamics model of an architecture?  Why bother with a generic ResNet? (And I presume you're using a fully convolutional ResNet here?)  Wouldn't the former work much better, and be a significantly more interesting contribution that just applying a ResNet and a generic U-Net as a state estimator?  I'm not understanding why the current proposed architecture (assuming I understand it correctly) could be seen as \"PDE guided\" in all but the loosest possible sense.  Can you correct me if I'm misunderstanding some element here?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}