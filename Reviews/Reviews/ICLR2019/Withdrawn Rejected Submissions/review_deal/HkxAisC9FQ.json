{
    "Decision": {
        "metareview": "This paper suggests augmenting adversarial training with a Lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. The idea of using such regularization seems novel. However, several reviewers were seriously concerned with the quality of the writing. In particular, the paper contains claims that not only are not needed but also are incorrect. Also, the Reviewer 2 in particular was also concerned with the presentation of prior work on Lipschitz regularization. \n\nSuch poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution. ",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "The quality of the presentation makes it hard to properly assess the quality of the results"
    },
    "Reviews": [
        {
            "title": "Possibly a good paper but not my area of expertise at all",
            "review": "The authors propose a novel method of training neural networks for robustness of adversarial attacks based on 2-norm and Lipschitz regularization. Unfortunately I'm not at all familiar with the literature on adversarial attacks so it is difficult for me to judge the quality and significance of this work. The theoretical results look plausible and clearly stated. The experiments show improvements over existing methods but I can't tell whether the right baselines were used. Overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Interesting idea -- could be significantly strengthened",
            "review": "Summary: this paper uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.\n\npros:\n\n- interesting idea\n- experiments are interesting\n\ncons:\n\n- formal results are either trivial or could be improved in their statements \n- experimental guarantees only, up to what is hidden in the Big-Oh notations of Theorem 2.2, 2.3.\n\ndetails:\n\n* In Theorem 2.2, you need to remove the $O(epsilon^2)$, unless you point to the Taylor theorem that guarantees that for the identity you claim before (5). The closest one I see is that the O(||a||^2) is in fact $||a|| u(||a||)$ with $\\lim u(x) = 0$ as $x \\rightarrow 0$, which does not guarantee the $O$ notation for any $a$.\n\n* In Theorem 2.2, how do you pass from the solution of (5) (which is indeed a vector) to the solution of the following equation, which, without constraint, gives a dim > 1 subspace in the general case ?\n\n* In all cases, you do not get Theorem 2.3 in its form as the $O$ notation just guarantees you an upperbound. You need to rephrase.\n\n* Figure ?? (twice) before Section 3\n\n* Define the “group norm” notation appearing with the max in (8) (isn’t one redundant ?)\n\n* Section 3.4 is interesting. Have you looked at generalising your observation in the last identity to more losses  = f-divergences (hence, proper losses modulo assumptions) ? \n\n* Section 4: many Figure ??",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but poorly written",
            "review": "This paper explores augmenting the training loss with an additional gradient regularization term to improve the robustness of models against adversarial examples. The authors show that this training loss can be interpreted as a form of adversarial training against optimal L2 and L_infinity adversarial perturbations. This augmented training effectively reduces the Lipschitz constant of the network, leading to improved robustness against a wide variety of attack algorithms.\n\nWhile I believe the results are correct and possibly significant, the paper is poorly written (especially for a 10 page submission) and comparison with prior work on reducing the Lipschitz constant of the network is lacking. The authors also made little to no effort in writing to ensure the clarity of their paper. I would like to see a completely reworked draft before opening to the idea of recommending acceptance.\n\nPros:\n- Theoretically intuitive method for improving the model's robustness.\n- Evaluation against a wide variety of attacks.\n- Empirically demonstrated improvement over traditional adversarial training.\n\nCons:\n- Lack of comparison to prior work. The authors are aware of numerous techniques for controlling the Lipschitz constant of the network for improved robustness, but did not compare to them at all.\n- Poorly written. The paper contains multiple missing figure references, has a duplicated table (Tables 1 and 3), and the method is not explained well. I am confused as to how the 2-Lip loss is minimized. Also, the paper organization seems very chaotic and incoherent, e.g., the introduction section contains many technical details that would better belong in related works or methods sections.\n\n--------------------------------------------\n\nRevision:\n\nI thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns.\n\nA more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}