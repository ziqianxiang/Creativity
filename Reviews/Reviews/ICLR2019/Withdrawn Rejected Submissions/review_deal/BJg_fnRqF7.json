{
    "Decision": "",
    "Reviews": [
        {
            "title": "Deep clustering based on a mixture of autoencoders",
            "review": "This paper presents a deep clustering based on a mixture of autoencoders - which follows the well-known idea of a mixture of experts, and the k-means. Different from k-means, in this method, each data point is allocated to a cluster based the representation error if the autoencoder network were used to represent this data point. \n\nAlthough the method is developed based on some existing concepts in the literature, the use of mixture of autoencoders for deep clustering is novel. The paper is also very well presented and I enjoyed reading the paper. I have however several questions:\n\n(1) I feel the performance of the algorithm is most likely very dependent on the pre-training for initialisation that was used by the authors. How much is the performance dependent on pre-training? Is it possible to use random initialisation, and what's going to happen if you use random initialisation? \n\n(2) Do you have to assume that you know how many clusters are in the data? This appears to be assumed known in your experiments. What happen if it is not known?\n\n(3) The proposed method is only compared with the deep clustering baselines, and k-means. Are they state-of-the-art? Have you also considered subspace clustering algorithms, such as the sparse subspace clustering algorithms and low-rank subspace clustering algorithms?\n\nAlthough the paper is well written, there are still a few typos - for example, \"should results in\", \"Once all the network parameter are\", \"Autoencodr\", \"a variant the\", etc.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting approach but questionable motivation and inadequate evaluation",
            "review": "\nSummary:\nThis paper proposes a deep clustering approach that learns a low-dimensional embedding of the data simultaneously while clustering data using a deep neural network. An autoencoder framework is used to learn embeddings and a joint loss term is used to couple the clustering with the AE loss. Their results on MNIST and newsgroups dataset compare with a couple of other deep clustering approaches and show modest improvements in clustering performance.\n\nClarity:\nThe write-up is reasonably clear, but in print the equations are rendered poorly. Some suggestions for improvements are described later.\n\nOriginality:\n- The idea of learning embeddings while learning a clustering is not novel and doing this using a deep NN is not novel either. But incorporating an AE component is novel. \n- The method builds on the overall idea behind an earlier approach, DEC (ICML, 2016) but has one key difference -- a group of autoencoders (AE) is trained simultaneously with the DNN and used to \"weigh\" the clustering loss\n- The other difference from DEC is in that their deep network is the \"encoder\" part of a SAE whereas in this paper, the deep network seems to be a general DNN. \n\nSignificance:\n- Clustering is an important problem and the authors show some results, but on only three datasets: MNIST, newsgroups, Reuters. They should have included the STL dataset from prior work to make it more comparable.\n\nMain concerns:\n- What is the state-of-the-art on MNIST and their other datasets? To be convincing, they should compare with that algorithm rather than just prior deep clustering approaches. (Note: k-means is not the state-of-the-art method for clustering many datasets).\n- The motivation behind learning the AEs is not clear as the learned embedding is not used anywhere in the clustering. Instead, the reconstruction loss is used to influence the clustering loss by acting as a \"weight\". Does this \"couple\" the two components in a desirable manner? It is not intuitive why a wrongly clustered point will always produce a change to h(xi), rather than to the AE part of the objective? That is, the corresp AE might \"accommodate\" the wrongly clustered point rather than the clustering modifying. This is a standard problem with non-convex functions involving products of functions/parameters.\n- Is the purpose of the AEs just as a means of regularization?\n- The authors should report standard error for experiments by trying different train/test splits. This standard ML practice of giving an average performance is being skipped by many papers these days and should be taken seriously for the results to be convincing.\n- There is no mention or citation of any of the Variational AE based clustering methods. There are methods using Gaussian mixtures of AEs. These should be cited and compared to (GMVAE, VaDE, DLGMM etc)\n\n\nOther comments:\n- For the architecture of the deep network used to train the non-linear representation, have they tried any other architectures or CNNs?\n- The DAE expertise section is not surprising at all. Since the initialization was based on an already clustered set of images, one would expect each DAE to have learned a representation of that digit.\n- How will their method perform with noisy data and outliers? Have they tried corrupting the images to see if the approach is robust?\n- Details of the DNN should be described early on and the choice of the particular size explained.\n- In addition to the shown clustering metrics, another useful metric is AUC-PR (AUC precision-recall curve). Here, precision and recall are computed by considering pairs of data-points and the binary classification task of whether the two belong to the same class. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting but the performance not good enough. ",
            "review": "The paper presents a deep clustering method which represents each cluster with different auto-encoders. The idea is simple but seems interesting. Benefiting from the neural network, the proposed model works in an end-to-end manner. It also can be used to cluster new incoming data without redoing the whole clustering procedure. The paper is clearly written and some experiments are conducted. However, I have some concerns as below:\n\n1. The theoretical detail should be given on why no trivial solution will be given.\n2. In experiments, the proposed method outperforms the compared methods. However, I still feel that the experimental comparison may be the biggest disadvantage of this paper as the reported results are remarkably worse than the state-of-the-art results, e.g. VaDe, SpectralNet, etc. which has achieved >91% ACC/NMI on mnist.\n3. It is unclear why the proposed method makes sense. Comparing with recent developments such as VaDe, I think that the novelty of this work may be on the borderline of ICLR. \n3. Some writing mistakes: \n   - Table 1: The Deep** Autoencodr MIxture** Clustering (DAMIC) algorithm.\n   - It seems that the diagram (a) (b) in Fig.4 is incomplete.\n4. DAE#1 and DAE#9 achieve a similar result, could explain more and check your result? Does this indicates that the proposed model cannot cluster '1' and '9' correctly?\n\n[1] Variational deep embedding: A generative approach to clustering\n[2] SpectralNet: Spectral Clustering using Deep Neural Networks\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}