{
    "Decision": "",
    "Reviews": [
        {
            "title": "The idea is intuitive, and the paper has many experimental analysis.",
            "review": "The motivation of this paper is clear, since the sentence of same meaning in different languages should have similar representation in the high level latent space. The experimental analysis of the proposed alignment method is detailed. \n\n1. The method ``Pivot through English``: does it mean to use the baseline model decoding twice,  source->en and en->target? If true, because you already have the two src<->en and tat<->en NMT systems, you can fully use your model and should consider the baseline as the pivot method, whereas the disadvantage is the doubled latency. So the power of your model is not increased, your contribution is to make it more efficient during inference.\n2. Figure 2 is interesting. Even when you are trying to optimize the cosine distance, the actual cosine distance is better when using adversarial training.\n3. In your pre-training optimizer, the learning rate is either 1.0 or 2.0. If Adam is used, it is abnormal.\n4. Another concern is the paper did not compared with another zero-shot NMT or pivot method, e.g., https://arxiv.org/pdf/1705.00753v1.pdf . \n5. In section 4.4, the lambda to balance alignment loss is 0.001. Since the cosine distance is usually less than 1. I am wondering how significant this extra regularizer is in such case.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "No proper comparison with previous work",
            "review": "This paper proposes an explicit alignment loss for the encoder representations in multilingual NMT, which substantially improves the zero-shot translation performance.\n\nWhile the paper contains some interesting ideas, I think that it does a poor job at comparing the proposed method with existing techniques. In particular, the paper takes Johnson et al. (2016) as its only baseline, and tends to ignore all the other relevant literature in its experiments and most of the discussion. This makes it difficult to put this work into perspective, and I was left with the impression that it oversells to some extent. I would appreciate if the authors could address my concerns below, and I am open to modify my score accordingly.\n\nFirst of all, the claim that \"the quality of zero-shot translation has lagged behind pivoting through a common language by 8-10 BLEU points\" seems deceptive if not wrong. For instance, Chen et al. (2017) (http://aclweb.org/anthology/P17-1176) claim that their method \"significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs\", while your method only matches pivot-based performance. If not a direct comparison, this requires at least some discussion, but the current version does not even cite it. You could argue that, unlike Chen et al. (2018), your method is multilingual, but even from that perspective I think that the paper needs to do a better job at putting the proposed method into context.\n\nIn addition to that, Ha et al. (2017) and Lu et al. (2018) also focus on zero-shot multilingual translation as you do. While these two papers are cited, the proposed method is not compared against them. I think that some further discussion is necessary, and a direct comparison would also be desirable.\n\nSestorain et al. (2018) (https://arxiv.org/abs/1805.10338) also seems like a very relevant reference, but it is not even cited. Again, I think that some discussion (if not a direct comparison) is necessary.\n\nOther comments:\n\n- In your IWSLT experiments, you point out that \"the data that we train on is multi-way parallel, and the English sentences are shared across the language pairs\", which \"may be helping the model learn shared representations with the English sentences acting as pivots\". Something similar could also happen in your WMT experiments, as some of the underlying training corpora (e.g. Europarl) are also multi-way parallel. I do not aim to question the experimental settings, which seem reasonable, but this raises some flags to me: the proposed evaluation is not completely realistic, as (some) training corpora are indirectly aligned, and the system could be somehow exploiting that. I think that it would be great to see some further discussion on this and, ideally, having experiments that control this factor would also be desirable.\n\n- Your analysis reveals that, to a great extent, the poor performance of the baseline is caused by the decoder generating its output in the wrong language. While your proposed method does not suffer from this issue, this suggests that the baseline is very weak, as one could think of different hacks to mitigate this issue with minimal effort (e.g. simply filtering an n-best list with language detection should greatly improve results). Going back to my previous point, it seems hard to believe that such a weak baseline represents the current state-of-the-art in the topic as the paper seems to suggest.\n\n- Artetxe et al. (2018) do not use adversarial training with a language detecting discriminator as stated in page 3.\n\n- Artetxe et al. (2018) is duplicated in the references.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Maybe an improvement to zero-shot MT, but methodological questions make the results unconvincing",
            "review": "Pros:\n- The paper proposes two loss terms to help improve zero-shot translation performance. The proposed supervised alignment loss term is simple and would be straightforward to implement. The proposed adversarial loss term is also conceptually simple, although perhaps a bit difficult to tune.\n- Both loss terms seem to improve zero-shot translation performance over the models trained without them.\n- The paper is fairly well written. It's easy to follow, although there's a few details that are missing which might make it difficult to reproduce.\n\nCons:\n- There are several methodological concerns that make it difficult to evaluate the correctness/significance of the results. For example, the authors use a non-standard valid/test set for the WMT'14 experiments (newstest2012/13 instead of newstest2013/14) which makes the results difficult to compare to other supervised baselines.\n- The results would be more convincing if the authors had evaluated on more language pairs and more standard datasets. Most of the results are based on a model trained on just two language pairs, making it difficult to evaluate how well the approach actually works.\n- There is no direct quantitative comparison to any other published results.\n- The results presented over the IWSLT dataset are heavily summarized, making it difficult to compare to other work.\n\nMethodological questions:\n- In Table 1, why do you use newstest2012 as the valid set and newstest2013 as the test set? Most other work on this dataset (e.g., Vaswani et al.) uses newstest2013 as the dev set and newstest2014 as the test set.\n- The baseline \"Direct translation\" results given in Table 1 are quite a bit lower than those reported in Vaswani et al. In particular, Vaswani uses newstest2013 as their dev set (which you use as test) and report an En-De BLEU of 25.8 in their ablation study, compared to 24.5 in your Table 3. Why is there such a large discrepancy?\n- In Section 4.3, when examining the attention context vectors, can you explain more about how you computed this? In particular, each Transformer decoder layer has an attention over the encoder outputs, so for 6 layers you'll actually have 6 attention context vectors, each depending on the previous decoder layer as well, right?\n\nMisc questions/comments:\n- Did you try combining the adversarial and pool-cosine loss terms?\n- How does your approach compare to adding identity (autoencoder) mappings, as in this WMT paper: http://www.statmt.org/wmt18/pdf/WMT009.pdf\n- You claim that prepending the <tl> token to the source has the same effect as putting it as the first token in the decoder. Did you actually test this or have some reference/citation for this? Since a lot of the problem seems to be getting the decoder to produce the correct output language, it seems possible that changing the method that you specify the desired target language could have a non-negligible effect.\n- What does a learning rate of 1.0 mean? In the original Vaswani et al. paper they use Adam and a dynamic learning rate schedule that adjusts based on the number of updates -- the effective learning rate is typically << 1.0. I believe the peak learning rate that they used is closer to 5e-4.\n- Which \"language identification tool\" do you use?\n- Table 4 seems to only give averages over the different zero-shot language pairs. Please add the full results for each language pair in the Appendix.\n- In Section 4.4, what do you mean by \"scores [...] are suspiciously close to that of bridging\"? What do you mean by bridging?\n\nThere are also a couple uncited related papers:\n- http://aclweb.org/anthology/N18-1032\n- http://www.statmt.org/wmt18/pdf/WMT009.pdf",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}