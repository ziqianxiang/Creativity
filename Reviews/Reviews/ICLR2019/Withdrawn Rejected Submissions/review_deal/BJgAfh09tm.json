{
    "Decision": "",
    "Reviews": [
        {
            "title": "The paper could be more readable, the contribution is limited. The machine translation experiments should be compared with previous work on a benchmark setup. The generation experiments should consider obvious baseline such as sampling from a language model.",
            "review": "Bilingual GANs\n\n\nPaper Summary:\n\nThe paper proposes to learn a generative model of sentences leveraging machine translation.\nA first task learns a model which auto-encodes and translates sequences in a language agnostic continuous space (a first GAN objective penalizes including language information in the code). The second GAN task learns a generator that allows sampling could that cannot be distinguished with genuine sentence codes from the encoder of the first task. The first model is evaluated in a supervised and unsupervised translation setup. The second model is evaluated by generating text, measuring language model perplexity.\n\nReview:\n\nReadability:\n\nThis paper could be more readable. In particular, it is not easy to understand how the two models interact: do you share the encoder and decoder parameters between the two tasks? Does the generation task fine tune these parameters? The evaluation protocol is unclear: what is the difference between supervised and half supervised MT? I do not understand what BLEU means for unconditional generation. E.g. when you generate 40k sentence sentences from the Europarl model, you compute the precision of your prediction with respect to which reference?\n\n\nContribution:\n\nThe contribution in terms of (un)-supervised MT is limited at best compared to (Artetxe et al 2017,2018) or (Lample et al, 2017, 2018). In particular, I do not see any reason not to use the empirical setup used in these papers and compare your results with them. The experimental setup seems weak with only short sentences < 20 words, with a small vocabulary and no attempt to deal with unknown replacements or subword units.\n\nThe contribution for unconditional generation is more interesting as relying on a space defined by translation seems a good idea. But it is hard to assess. Again, the paper lacks comparison with baselines. In particular, sampling from a large language model to generate text seems an obvious baseline. It is also necessary to compare with a monolingual GAN (Zhao et al 2017), since it would quantify the benefit of the bilingual latent space.\n\nRelated Work:\n\nPrevious work is mischaracterized: Schwenk & Douze 2017, Lample et al 2018, Artexte et al 2017b are not relying on adversarial training. \n\nReview Summary:\n\nThe paper could be more readable, the contribution is limited. The machine translation experiments should be compared with previous work on a benchmark setup. The generation experiments should consider obvious baseline such as sampling from a language model.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, weak experiments",
            "review": "This paper proposes a new method to generate parallel sentences combining NMT and GANs. While I think that the paper contains many interesting ideas, it lacks a good motivation and, more importantly, I find the experimental design (as well as some results) to be very weak. Please find my comments below:\n\n- The paper is not properly motivated. Why would one want to generate parallel sentences? I do not mean that the problem is uninteresting, I am just saying that the motivation is not there. My initial expectation (before I read the paper) was that the NMT part would benefit (or at least be influenced by) the text generation part, but that is not the case, as the NMT model is trained offline before the generator.\n\n- You do not compare your results to any other system or baseline in your experiments. A simple baseline would be to generate sentences monolingually, and translate them using machine translation (either supervised or unsupervised). I think that some baseline like this is necessary.\n\n- The translation BLEU scores are very weak. 8 BLEU points in fr-en Europarl seems way too low to take these results seriously.\n\n- Given that the (unsupervised) machine translation part is completely independent from the text generation unit, its evaluation seems of little relevance, as there is nothing new on it.\n\n- The training set seems very small for natural language generation (200k sentences for Europarl and 12-29k for Multi30k), so I am not sure about how meaningful the reported results are.\n\n- I do not understand what the \"generation BLEU\" is, but evaluating a text generator with BLEU does not seem to make much sense, as there is no reference to compare to.\n\n- I am not sure if I understand how perplexity is used in your evaluation. Do you train a separate language model and use it to measure the perplexity of your generated text? If so, this seems unusual and problematic to me. Do you have any reference of anybody else doing this?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiments not convincing; not well-motivated",
            "review": "[Summary]\nThis paper proposed an adversarial latent space based architecture capable of generating parallel sentences in two languages concurrently and translating bidirectionally. More specifically, there are two units in the framework: (1) A translation unit, which consists of two NMT models, that can translate sentences from one domain to another. The two NMT models are trained in a similar way to (Lample et al 2018a). The outputs from the two encoders of NMT models are regularized by a GAN. (2) A text generation unit, that can map a random noise into a code $c$. $c$ can be decoded into two different languages with the decoders of the NMT models. Experiments on on Europarl and Multi30k datasets are carried out to verify the proposed algorithm.\n\n[Details]\n1.\tWhat is the motivation of this work? I am not fully convinced that building “an adversarial latent space based architecture capable of generating parallel sentences in two languages concurrently and translating bidirectionally” is a well-motivated topic, because in this system, (1) Do the NMT models have better performance due to the joint training? I found that leaving out the GAN loss can lead to better performance. (See Table 1) (2) Why do I need such a generator? Can it generate sentences with specific semantic? Hope the authors can give stronger motivations for this work.\n2.\tFor NMT experiments, why don’t you work on WMT En->Fr (36M data after filtration) or En->De (4.5M data) tasks, which are more popular. It is strange to see that: (1) In Section 4.1, 2nd paragraph, “we removed sentences longer than 20 words”. That is, you only work on extremely short sentences, which is not a common practice. You should also use the official test sets, i.e., newstest series to evaluate your models. (2) For supervised setting, why don’t you use Transformer [ref1], a much strong baseline in NMT? Even if for using LSTM, a deep model like GNMT [ref2]? For unsupervised settings, why not follow (Lample et al 2018), which is the state-of-the-art unsupervised NMT model? (3) In Table 1, the rows with “NoAdv” have higher BLEU scores. Then, what is the role of the GAN in your NMT system?\n3.\tI am very confused by the evaluation of text generation. (1) Can you give a detailed explanation how the sentences are generated? (2) After generating a pair of sentences, how to choose the references for “Generation BLEU”? (3) Even if for translation BLEU, given the input is a synthesis sentence instead of a natural sentence, how can Google Translation give a correct “reference sentence”? \n\n[Typos]\n(1) Section 4.3, first line: results=> results \n(2) reference (Lample et al 2018a) and (Lample et al 2018b) are the same one. Please correct them.\n\n[ref1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n[ref2] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}