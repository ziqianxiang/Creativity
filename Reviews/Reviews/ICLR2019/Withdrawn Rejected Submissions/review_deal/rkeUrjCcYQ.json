{
    "Decision": {
        "metareview": "This paper develops a generative density model based on continuous-time flows on a potential field. \n\nStrengths:  The paper contains interesting ideas and connections to physics, in particular the enforcement of symmetry in a computationally cheap way.\n\nWeaknesses:  The main quantitative results of this paper are undercut by the numerical error introduced by the approximate, fixed-step integrator used.  In the paper, the authors did not check the degree of numerical error (or to what extend their reported likelihoods do not normalize) as a function of the step size.  This was partially addressed in a comment below.\n\nThere does seem to be some novelty but the lack of concrete experiments is a letdown. One could e.g. verify that the samples have similar properties (e.g. moments) to the ground truth, which are known for the Ising model. Regarding clarity, the symmetry constraints are never clearly specified.\n\nThis paper contains many ideas that would have been novel, but were scooped by [1] which was put on arXiv 3 months before the ICLR submission date.  The authors have added appropriate references to this paper, but this still undercuts the originality of the contribution.\n\nThe explanation of how and which symmetries are enforced is a little bit buried and unclear.\n\nPoints of contention:  Two of the reviewers didn't seem to be aware that the main mathematical results of the model are special cases of results from [1].\n\nConsensus:  All reviewers agreed that there were interesting ideas in the paper, and that it was close to the bar.\n\n[1] Chen, Tian Qi, et al. \"Neural Ordinary Differential Equations.\"",
        "confidence": "2: The area chair is not sure",
        "recommendation": "Reject",
        "title": "Interesting ideas but has fundamental technical problems"
    },
    "Reviews": [
        {
            "title": "Continuous time flows with symmetries motivated from the Monge-Ampere equation",
            "review": "Summary:\nThis paper introduces a continuous-time flow, which is motivated from continuous-time gradient flow in the Monge-Ampere equation in optimal transport theory. They relate the resulting generative model to a dynamical system with a learnable potential function that controls a compressible fluid (representing the probability density), towards the target density. The resulting set of differential equations for the evolution of the samples and the density is solved through a fourth-order Runge-Kutta ODE solver. By imposing symmetries on the scalar potential function, symmetries of the resulting target distribution can also be enforced, a property that is desirable in many applications.\n\nThe scalar potential is modeled using a fully connected MLP with a single hidden layer. Forward propagating of samples requires obtaining the gradient of the scalar potential (output of MLP) with respect to its input (the sample). Forward propagation of the log density requires computation of the Laplacian (not the hessian) of the scalar potential. Both of these quantities can easily be computed with automatic differentiation (in O(D) where D is data dimension). The potential is kept constant over time, although this is not necessary.\n\nThe proposed method is evaluated on density estimation for MNIST, and variational inference with respect to the Boltzmann distribution of the 2D Ising model at the critical temperature. \nOn MNIST, comparison is done with respect to MADE, MAF and realNVP. Monge-Ampere flows outperforms the baselines. On the variational inference task one baseline is used, and the result is compared to the exact known free energy. Monge-Ampere flows are reported to approximate the exact solution to comparable accuracy as a baseline. As the authors show that they can easily enforce symmetries, it would be very informative to see the performance of Monge-ampere flows with and without these symmetries enforced on for instance the Ising model. Have the authors looked at this?\n\nIt is not clear from the paper how much the ODE solvers used in the forward pass, as well as backpropagating through it with respect to model parameters, will influence the run time. I suspect the training time of models like MAF to be significantly shorter than that of Monge-Ampere flows. For sampling, the comparison would also be interesting. Where sampling from MAF is O(D), with D the data dimension, sampling from the Monge-Ampere flows requires propagating through an ODE solver. Can the authors comment on the runtimes for these settings?\n \nThe experimental validation is not extensive, but the proposed method is well motivated and as far as I can tell original. It is a useful contribution to the field of normalizing flows/invertible networks. The ability to easily enforce symmetries into the density seems to be promising and could lead to interesting future work on permutation invariant systems.\n\nSee below for comments and more questions:\n\nQuality\nThe paper is well structured. The experimental validation is not extensive, and perhaps even on the low side.\n\nClarity\nThe paper is overal clearly written. One small nuisance is that the citations are not in brackets in sentences, even if they are not part of the actual sentence itself. This interrupts reading. It would be greatly appreciated if the authors could change this. The authors leave out some details with regards to the experiments, but with code available this should be sufficient for reproducibility. \n\nOriginality\nTo my knowledge the idea of using the Monge-Ampere equation for continuous normalizing flows is new. Note that it is also significantly different from a concurrent ICLR submission entitled ‘Scalable Reversible Generative Models with Free-form Continuous Dynamics’, which also discussed continuous normalizing flows with ODE’s.\n\nSignificance\nThis work is of use to the research community. The method is memory efficient and appears to perform well. Especially the ability to enforce symmetries seems very appealing. If the authors can comment on the runtime in comparison to other flow methods, both in terms of training time and sampling, this would enable a better view on practical use. \n\nDetailed questions/comments:\n\n1. In Fig. 3a, the train and test error during training are shown to follow each other very closely. How long was the model trained, and did the train and test curve at some point start to diverge?\n2. In Section 4.2, the results are said to be of comparable accuracy as the baseline by Li & Wang. It would be informative to actually state the result of Li & Wang, so that the reader can judge too if this is comparable. \n3. Out of curiosity, did the authors also consider using other activation functions that have higher order derivatives, such as tanh?\n\n\n***** EDIT ******\n\nI thank the authors for their clarifications. They have sufficiently answered my questions/comments, so I will stick with my score.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but needs more work: clarifications to existing work and ablations would make this paper more appealing",
            "review": "This paper proposes a continuous-time gradient flow as an alternative normalizing flow. The formulation is motivated from optimal transport and the Monge-Ampere equation. Symmetry constraints motivated by the use of a potential function can be enforced during training.\n\nI'm surprised [1] was mentioned only for their backpropagation scheme. Much of this paper is similar to theirs, such as Eqs (2) and (3) being the \"instantaneous change of variables\" in [1], the mention of efficient reversibility, experimenting with forward and reverse KL objectives, and parameter efficiency. \n\nGiven the different angle of approach in this work, I'm willing to believe some of this is independently done. This work contains interesting derivations and a different parameterization, with enough contributions to potentially be interesting in its own right. However, I firmly believe in proper attribution and believe [1] should at least be mentioned in the introduction and/or theoretical background. \n\nPros:\n - The potential function and the resulting gradient flow parameterization is interesting.\n - Parameterizing a potential function motivates some symmetry constraints.\n - Interesting application of normalizing flows to the Ising model.\n \nCons:\n - Paper presentation needs some work on clarity.\n - Relation to existing work needs to be clarified.\n - Experiments lack ablations and proper comparisons. e.g. the effect of using symmetry constraints, the effect of using a gradient flow parameterization.\n - If I understood correctly, the symmetry \"constraints\" are really data augmentation during the training phase, rather than hard constraints on the model.\n\nMain questions:\n- It seems the potential function plays a similar role to the negative log-likelihood in [2]. \n- Does having symmetry constraints lead to a better model when the constraints are justified? ie. can you provide comparisons for the Ising model experiment in 4.2?\n- What are the set of constraints you can specify using a potential function? Permutation of the variables is very briefly mentioned in the experiment section, but this could be clarified much earlier. \n- I may have missed this, but what exactly are the symmetry conditions that were used in the experiments?\n- It seems that the proposed permutation constraints could be part of the training algorithm rather than the model. How different would it be if you permute the data samples and use an existing normalizing flow algorithm? ie. can you provide comparisons where randomly permuted data samples are also used during training with existing algorithms?\n- Since you used a fixed-step size solver, do you have some guarantees, theoretical or empirically, that the numerical integration has low error? e.g. what is the reconstruction error from doing a forward and reverse pass, and what would the error be if compared to a much smaller step size?\n\nMinor:\n- The potential function is parameterized directly but is not integrated to infinity. Since the resulting gradient flow is time-invariant, how this would affect the expressivity of the flow? Could a time-variant potential function be used?\n- Eq (5) is also the Liouville equation, which I think should be mentioned.\n- MNIST digits have completely black backgrounds, so I don't understand why Figure 3 samples have grey backgrounds. Could this have something to do with numerical issues in reversing the numerical integration?\n- It's awkward that Figure 3 contains the loss over training for Monge-Ampere flows but only the final loss for the rest. Table 1 sufficiently summarizes this figure, so unless you can show the loss over training for all methods I think this figure is redundant.\n- Equations are referenced both with and without parenthesis. It'd be best if this is consistent across text.\n- There are quite a few grammar mistakes, especially around important digression. (e.g. top of page 3 \"experienced by someone travels with the fluid\" -> \"experienced by someone traveling with the fluid\".)\n- Please use citep and citet properly. Many references should be done using citep (with brackets around the author-year), when the author is a not a part of the sentence.\n\n[1] Chen, Tian Qi, et al. \"Neural Ordinary Differential Equations.\"\n[2] Tabak, Esteban G., and Eric Vanden-Eijnden. \"Density estimation by dual ascent of the log-likelihood.\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Proposes a novel parameter-efficient generative modeling approach based on the Monge-Ampere equation. Applications section is not convincing enough.",
            "review": "This paper proposes a novel parameter-efficient generative modeling approach that is based on the Monge-Ampere equation. In the proposal, a feed-forward neural network is trained as an ODE integrator which solves (2) and (3) for a fixed time interval $[0,T]$, so that the distribution $p(x,t)$ at time 0 is a simple base distribution such as a Gaussian, and that at time $T$ mimics the target distribution.\n\n[pros]\n- The proposal provides a parameter-efficient approach to generative modeling, via parameter sharing in the depth direction.\n- I think that the idea itself is quite interesting and that it is worth pursuing this direction further.\n\n[cons]\n- The Applications section is not convincing enough to demonstrate usefulness of the proposal as an approach to generative modeling.\n- How the gradient-based learning in the proposal behaves is not discussed in this paper.\n\n[quality]\nHow the gradient-based learning in the proposal behaves is not discussed. I understand that the non-convex nature of the loss function poses problems already in the conventional back-propagation learning of a multilayer neural network. On the other hand, in the proposal, the loss function (e.g., (4)) is further indirectly parameterized via $\\varphi$. It would be nice if the parameterization of the loss in terms of $\\varphi$ is regular in some sence.\n\n[clarity]\nDescription of this paper is basically clear. In the author-date citation style employed in this paper, both the author names and publication year are enclosed in parentheses, with exception being the author names incorporated in the text. This paper does not follow the above standard convention for citation and thus poses strong resistance to the reader. For example, in the first line of the Introduction section, \"Goodfellow et al. (2016)\" should read \"(Goodfellow et al., 2016)\".\n\n[originality]\nThe idea of considering the Monge-Ampere equation in its linearized form to formulate generative modeling seems original.\n\n[significance]\nIn the experiment described in Section 4.1, it is not clear at all from the description here whether the learned system is capable of successfully generating MNIST-like fake images, which would question the significance of the proposal as a framework for generative modeling. It is well known that the KL divergence $D(P\\|Q)$ tends to put more penalty when $P$ is large and $Q$ is small than the opposite. One can then expect in this experiment that it tolerates the model, appearing as $Q$ in $D(P\\|Q)$, to put weights on regions where the data are scarce, which might result in generation of low-quality fake images. It would be nice if the authors provide figures showing samples generated via mapping of Gaussian samples with the learned system.\nAlso, in the experiment described in Section 4.2, I do not see its significance. It is nice to observe in Figure 4 that the loss function approaches the true free energy as well as that the snapshots generated by the model seem more or less realistic. My main concern however is regarding what the potential utilities of the proposal are in elucidating statistical-physical properties of a system. For example, it would be nice if the proposal could estimate the phase-transition point more easily and/or more accurately compared with alternative conventional approaches, but there is no such comparison presented in this paper, making the significance of this paper obscure.\n\nMinor points:\n\nThe reference entitled \"A proposal on machine learning via dynamical systems\" would be better cited not as \"E (2017)\" but rather as \"Weinan (2017)\".\n\nPage 6, line 10: the likelihoods of these sample(s)\n\n----Updated after author feedback----\nUpon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.\nI still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}