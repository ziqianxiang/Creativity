{
    "Decision": "",
    "Reviews": [
        {
            "title": "Missing critical references; lacking novelty",
            "review": "The first abstract reads, \"The field of deep learning has been craving for an optimization method that shows outstanding property for both optimization and generalization\", what does it mean to say that the field is \"craving\"? Yes, a rigorously (mathematically proven or empirically tested extensively) better algorithm (compared to the existing algorithm) to optimize the ERM problems that appear in training deep neural networks is always valuable. But I find that this paper is not delivering on both these ends. To summarize, the main idea of the paper is to view ERM optimization in continuous time to develop algorithm to train neural networks. Section 2 introduces geodesic flows with NO references which can easily mislead readers that the math is new. For a field that is known in mathematics for essentially essentiall hundreds of years, I'd expect that the section is filled with references. Here's one in the recent years that tries to give a good overview -- https://arxiv.org/abs/1609.03890.\n\nEssentially Theorem 2.1 is a restatement of Gronwall's Lemma with the implications of Theorem 2.1 not clear at all. Yes, we can choose F(t) to be anything and get exponential convergence rate in the continuous time, but it is *not* true that it is possible to get such exponential convergence in the discrete time (which is where training happens eventually). The technical difficulty is in figuring out the right discretization that can achieve the maximum possible convergence rate. See https://arxiv.org/abs/1805.00521 for example. The authors Euler's explicit discretization which is not the best even in simple settings.  \n\nAll the modification the authors propose in Section 3, 4 and 5 are generic and straight out of standard numerical linear algebra textbooks.\n\nAs far as I can see, the experiments are setup in a somewhat standard way with CIFAR 10/100. I think MSGD should be tested with mini-batch 64 not 250 since smaller batch size leads to better generalization performace as recent works indicate (See for example, https://openreview.net/forum?id=HyWrIgW0W). ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A lot of sophisticated mathematical concepts, not much explained, hard to understand.",
            "review": "\nI found the paper poorly written, with many typos and incorrect formulations. It contains several sophisticated mathematical concepts (probably from differentiable geometry, dynamical systems and differential equations) that I believe a majority of ICLR audience would not be so familiar with; and these notions are not defined nor explained (e.g Riemannian metrics/manifold, Krylov subspaces). Section 2 presents some theoretical results and formal definition of exponentially decaying flows but I could not see the intuition or the goal of these results. In Section 3, for the main\noptimization problem (13), it is assumed that \"there exists no stationary point except for a minima.\" and so, I understand that there is only \"a minima\" or at least that all the local minima are assumed to have the same value, which not realistic for deep learning.\n\n\nComments and questions\n\nSection 1:\n\nAfter speaking of gradient descent methods, the authors say \"Another class of methods... is adaptive methods such as AdaGrad and Adam\": These are all based on stochastic gradient descent algorithm, so this distinction is surprising.\n\nSection 2:\n\n\"Jacobian with variable w\": does not make sense (although I can guess the author mean w.r.t.)\n\n\"J is regular and the equation has a unique solution\": what is regular here? No equation has been mentioned before\n\n\"...becomes a Riemannian manifold under some appropriate conditions\": which conditions?\n\nSection 3:\n\n\"Applying Theorem 2.2, we obtain the differential equation\": I can't see how applying Theorem 2.2 leads to a differential equation.\n\nSection 4:\n\n\"we consider a projection which maps r to a vector P_k(A, v) in the k-th order Krylov subspace such that r = P_\\infty(A, v)\": I don't understand.\n\n\"Particularly, in the case that χ = −1, we set λ = akJ φ T F k b with a, b > 0, so that the convergence rate of (24) stays exponential\": why is the case χ = −1 relevant to point out?\n\n\"Finally, to accelerate the EDF-based methods, it is sometimes effective to change equation (18) into a second-order differential equation\": is it an empirical observation? or is there an explanation?\n\nSection 5:\n\nAgain, so many notations (not all conventional: e.g. \\theta denotes the softmax function), just to present a standard loss for a deep learning model.\n\nSection 6:\n\n\"As has been found, second-order-methods on full-batch training converge to the solution within a few iterations.\":  Reference?\n\nThe method is compared to Nesterov accelerated gradient (NAG) for the data-fitting problem (fig 1) but not on the other tasks (figs 2 to 5).\n\nIn the end, it was still unclear to me what the training consists of with the introduced exponentially decaying flows. I understand that a differential equation was formulated but then how is it solved iteratively? What is the cost of solving it? In Section 6, different algorithms are compared in terms of number of steps. What about the computational cost?\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper needs a major revision, too many unjustified claims are made",
            "review": "The paper is overall unclear and not well-written. There are lots of typos, grammar mistakes, misuse of terminology,... that obscure the clarity of the paper. More importantly, the contribution is unclear to me, and I think the paper needs a major rewrite in order to comply with the type of standard we expect from an international publication in a machine learning venue. Below I give some comments that I hope will help the authors to revise their submission.\n\n1) “In addition, it is unclear whether the method has advantages in generalization performance.“\nThere is some theoretical  results that are worth citing such as\nBottou, L., & Bousquet, O. (2008). The tradeoffs of large scale learning. In Advances in neural information processing systems (pp. 161-168).\nUnder certain assumptions, second-order methods are known to generalize more poorly.\n\n2) Page 2, “positive matrix G”\nYou mean positive-definite matrix since G is a covariance matrix. Positive and Positive-definite matrices are different concepts.\n\n3) Assumption section 3\nThe authors make the assumption “there exists no stationary point except for a minima”. This is a rather strong assumption which of course does not apply do deep neural networks. The authors should discuss how to relax this assumption.\n\n4) Derivation section 3\nThe authors drop the first term in the decomposition of the Hessian-momentum term without providing any justification. Why are you making this simplification? It is probably worth pointing out this resembles the Gauss-Newton approximation and you are left with a positive-definite approximation of the Hessian, therefore ignoring any negative curvature.\n\n5) Computation of the matrix\n“ One of the basic methods to construct such a projection... requires only the matrix multiplication”\nI would also suggest mentioning that these methods usually construct sparse matrices such as tri-diagonal matrices, see e.g. Nocedal, J., & Wright, S. J. (2006). Numerical optimization 2nd.\n\n6) Suggested approach\na) The whole derivation is rather unclear, the author seem to arrive to a second-order equation similar to the one derived for Nesterov accelerated gradient in Su et al. 2014. You should contrast how your equation differs from theirs.\nb) What are the convergence guarantees for your approach? Consider deriving a proof of convergence as in Su et al. 2014 (at the very least I would expect an asymptotic result)\nc) Second-order ODES are difficult to discretize (see discussion in Su et al.) and even if the continuous method is guaranteed to converge, the discretization procedure might not have such guarantees. You need to add a discussion about discretization and explain what approach you used in practice.\n\n7) Deterministic vs Stochastic setting\nThe derivation presented in the main text assumes full gradients are computed but of course, in practice, one would only use mini-batches. The authors make some rather bold and unjustified claims regarding the ability of their method to generalize to a stochastic setting. In particular they claim “setting the hyperparameter k small makes EDF be compatible with stochastic\napproaches and take their advantages”. This statement requires a solid justification. I do not believe this is true. If you add noise to your differential equation, you need to ensure the noise has certain properties (vanishing noise in the limit or bounded noise) if you want to guarantee convergence. I recommend the authors read the relevant literature, for instance:\nLi, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.\nKrichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous\nand discrete time. In Advances in neural information processing systems, pages 2845–2853.\n\n8) Experiments\nChoice of hyper-parameters: the authors need to explain how they pick the hyper-parameters. Simply listing what values are used is not sufficient. You need to show you’ve tried different settings for the competing methods. You said\n“The step sizes for Momentum, NAG, and Adam were fixed to 0.01, 0.001, and 0.001, respectively”. How did you pick these values?\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}