{
    "Decision": {
        "metareview": "The paper's contribution lies in using cross-lingual sharing of subword representations for improving document classification.  The paper presents interesting models and results.\n\nWhile the paper is good (two out of three reviewers are happy about it), I do agree with the reviewer who suggests the experimentation with relatively dissimilar languages and showing whether or not the approach works for those cases.  I am also not very happy with the author response to the reviewer.  Moreover, I think the paper could improve further if the authors presented experiments on more tasks apart from document classification.",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Meta Review"
    },
    "Reviews": [
        {
            "title": "Straightforward model, sub-par experiment setup",
            "review": "The paper proposes to transfer document classifiers between (closely) related languages by exploiting cross-lingual subword representations in a cross-lingual embedder jointly with word-based classifier: the embedder represents the words, while the classifier labels the document. The approach is reasonable, albeit somewhat unexciting, as the basic underlying ideas are in the vein of Pinter et al. (2017), even if applied on a different task.\n\nThe main concern I have with the paper is that it leaves much open in terms of exploring the dimension of (dis)similarity: How does the model perform when similarity decreases across language pairs in the transfer? The paper currently offers a rather biased view: the couplings French-Italian-Spanish, Danish-Swedish are all very closely related languages, and Amharic-Tigrinya are also significantly related. Outside these couplings, there's a paragraph to note that the method breaks down (Table 5 in the appendix). Sharing between Romance and Germanic languages is far from representative of \"loosely related languages\", for all the cross-cultural influences that the two groups share.\n\nWhile the experiment is reasonably posed, in my view it lacks the cross-lingual breadth and an empirical account of similarity. What we do in cross-lingual processing is: port models from resource-rich to low-resource languages, and to port between very similar languages that already have resources is a purely academic exercise. This is not to say that evaluation by proxy should be banned, but rather that low-resource setups should be more extensively controlled for.\n\nThus, in summary, a rather straightforward contribution to computational modeling paired with sub-par experiment setup in my view amounts to a rejection. The paper can be improved by extending the experiment and controlling for similarity, rather than leaving it as implication.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need more insights",
            "review": "Summary: The authors address the task of cross language document classification when there is no training data available in the target language but data is available a closely related language. The authors propose forming character-based embeddings of words to make use of sub-word similarities in closely-related languages. The authors do an extensive evaluation using various combinations of related languages and show improved performance. In particular,  the performance is shown to be competitive with word-based models, which are tied to a requirement of resources involving the original language (such as MT systems, bilingual lexicons, etc). The authors show that their results are boosted when some additional resources (such as bilingual dictionaries of minimal size) are used in a multi-task learning setup.\n\n\n- I would have liked to see some comparison where your model also uses all the resources available to CLWE based models (for example, larger dictionary, larger parallel corpus, etc)\n\n- It is mentioned that you used parallel projection only for Amharic as for other languages you had enough RCV2 training data. However, it would be interesting to see if you still use parallel projection on top of this.\n\n- I do not completely agree with the statement that CACO models are \"not far behind\" DAN models. IN Table 1, for most languages the difference is quite high. I understand that your model uses fewer resources but can it bridge the gap by using more resources? Is the model capable of doing so ?\n\n- How did you tune the lambdas in Eqn 11? Any interesting insights from the values of these lambdas? Do these lambda values vary significantly across languages ?\n\n- The argument about why the performance drops when you use language identifiers is not very convincing. Can you please elaborate on this ?\n\n- Why would the performance be better in one directions as compared to another (North Germanic to Romance v/s ROmance to North Germanic). Some explanation is needed here.\n\n- One recurring grievance that I have is that there are no insights/explanations for any results. Why are the gains better for some language pairs? Why is there asymmetry in the results w.r.t direction of transfer ? In what way do 2 languages help as compared to single source language? What is you use more that 2 source languages?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-presented but some shortcomings in experiments",
            "review": "Overview:\n\nThis paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.\n\nMain strengths:\n\n1. The paper is well written. The model description in Section 2 is very clear and precise.\n2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.\n3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.\n\nMain weaknesses:\n\n1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning are well-established). So this raises a concern about novelty (although the experimental results are new).\n\n2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses \"far more resources.\" The best ALL-CACO model also relies on several annotated but \"smaller\" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a \"similar\" amount of data from the specific task?\n\nOverall feedback:\n\nThis is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.\n\nEdit: I am changing my rating from 5 to 6 based on the authors' response.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}