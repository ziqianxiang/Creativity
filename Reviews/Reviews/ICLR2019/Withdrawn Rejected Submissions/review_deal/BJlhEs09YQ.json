{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice analysis but needs strong assumptions",
            "review": "This paper provides theoretical guarantees for learning deep convolutional neural networks using rank-one tensor decomposition. In particular,  a tensor is first constructed from the given data. Then the authors show that the error between this tensor and its corresponding population counterpart can be bounded. They also show that the population tensor can be approximated by a rank-one tensor whose components are convolutional kernels. Finally, by doing decomposition of this tensor, the kernels can be recovered up to some error. \n\nThe paper is in general easy to read even if there are a lot of technical notations in it. The analyses also look rigorous. The tensor formulation provides some theoretical insights for deep neural networks since most existing tensor approaches for neural networks can only handle one-layer neural networks. \n\nHowever, I have several concerns about this paper. \n\n1. The assumptions are very strong and mostly unrealistic. For example, 1) the input needs to follow Gaussian distribution; 2) the stride is equal to the kernel size; 3) the input and activations are one-dimensional. 4) the data comes from a planted model. \n\n2. Another caveat of the analyses is that it assumes the constructed tensor can be approximated by a rank-one tensor. However, according to Theorem 4.7, this error highly depends on the ground truth kernels as well as the activation function. This error does not converge to zero asymptotically and is not proved to be a sufficiently small number. In other words, this error may be a significant number that invalidates the proposed tensor decomposition approach for learning deep networks. \n\nRecovery guarantees for deep convolutional neural networks are challenging. This paper provides a tensor approach with rigorous analyses. However, the assumptions are too strong and the error bound does not converge to zero asymptotically . \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting direction, but lacks practical relevance",
            "review": "==== Summary ====\n\nThis paper proposes a learning method for a restricted case of deep convolutional networks, where the convolutional layers are limited to the non-overlapping case (receptive field = stride) and having just a single output channel per layer. Additionally, the method is analyzed only for the case of normally distributed inputs and realizable settings, i.e., there exists some set of weights that correctly solve the learning problem. The proposed method is based on a rank-1 tensor approximation to a tensor “summary” of the training set and is proven to result in a good estimation to the ground-truth parameters when the number of samples exceeds the number of parameters.\n\n==== Detailed Review ====\n\nThe paper is well written, easy to follow, and I generally liked reading it. More specifically, I have found the method of aggregating the training set into a tensor that directly reflects the parameters of the model to be quite elegant. Though the general learning scheme is quite similar to previous works (see below), the way it leverages tensor decomposition is unique to the best of my knowledge. Additionally, unlike previous works which dealt with \"shallow\" non-linear models (e.g., neural networks with a single hidden layer), this work manages to address the learning of deep non-linear models, at least to some degree. Despite the above, the paper has two significant shortcomings:\n\n1. The results on their own are limited to a very restrictive case of neural networks that has little to no practical relevance. While it is known that non-overlapping ConvNets are significantly less expressive than overlapping ones [1], they can at least approximate any function if given a sufficient number of hidden channels [2], but the restriction to a single channel per layer means that they can represent only a limited set of functions. I would have expected the paper to include a short discussion on these facts given its subject. The lack of relevance to practical networks would not be that bad if the paper included a sketch of how DeepTD could be extended to more complex networks. It would be helpful if the authors would comment on how might this method could be generalized to these cases.\n\n2. The novelty of the method and the strength of the theoretical results are a bit lacking. First, the use of Stein’s lemma with E[y * x] for the estimation of the parameters of a non-linear model was used before in [3,4], and moreover in both cases they were able to extend their results to non-gaussian inputs and more complex models using higher moments, i.e. E[y * x * x] and E[y * x * x * x]. The submitted paper does cite these papers but does not directly discuss the similarities between them. While these works also rely on tensor decompositions and Stein's theorem,  DeepTD makes a different and novel use of tensor decompositions, but it does so in a way that is highly tuned to non-overlapping single-channel networks (i.e., the tensor T(x) is shaped according to the size of the kernels), and does not seem to be easily extendable to other settings. Additionally, the actual guarantees of theorem 4.8 show that for non-linear networks the bound on the error does not decrease to zero (even with infinite samples). Moreover, it seems that under common settings the resulting bound is meaningless (i.e., a negative lower bound for a positive quantity). I actually found it interesting that despite this bound, the reported empirical results were actually quite good — perhaps there is more to uncover here and maybe this method has potential beyond what can currently be proven. The only case where it seems to be relevant is when the activations are linear (or with smoothing factor so low which they are practically linear). However, under this trivial case the population tensor T = E[y*x] = E[grad(f_cnn)] is precisely the rank-1 tensor of the kernels, which makes me think that perhaps DeepTD can merely be seen as a kind of linear relaxation to the learning problem. The general point here that though there is a clear distinction between DeepTD and prior works, the delta is not significant, and though the results seem promising, they do not seem fully fleshed out.\n\nIn conclusion, while I do like the general direction that the paper takes with learning the parameters of a neural network, and it does seem like it could lead to exciting results in the future, I do not think it is ready for publication in its current form. Nevertheless, I strongly encourage the authors to keep developing this work (e.g., extend it to multi-channel networks, improve the bounds, drop gaussian input assumption).\n\n[1] Sharir et al. On the Expressive Power of Overlapping Architectures of Deep Learning. ICLR 2018. \n[2] Cohen et al. On the Expressive Power of Deep Learning: A Tensor Analysis. COLT 2016. \n[3] Janzamin et al. Beating the Perils of Non-Convexity:\nGuaranteed Training of Neural Networks using Tensor Methods. Arxiv preprint.\n[4] Sedghi et al. Provable Tensor Methods for Learning Mixtures of Generalized Linear Models. AISTATS 2016.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recovery of true weights for a special class of CNNs by reduction to tensor decomposition",
            "review": "Summary:\n\nThis paper analyzes the problem of learning a very special class of CNNs: each layer consists of a single filter, applied to non-overlapping patches of the input. Specifically, the task is: given data generated from a planted CNN of this kind with gaussian inputs, recover the ground truth weights. In the main result (Theorem 4.8), the authors show that the true weights can be approximately recovered, with high probability, by applying rank-one tensor decomposition to a special tensor derived from the generated data. Importantly, achieving a good approximation requires (a) observing more examples than the total number of network parameters, and (b) (somewhat confusingly) the kernels must be large relative to the square of the network depth.\n\nReview:\n\nThe paper is clearly written. The results seem mathematically interesting and substantial. The connection to practical deep CNNs may be overstated however.  The nature of the result is ultimately that this very special CNN learning problem is really tensor decomposition in disguise. But why does the reduction in this special case teach us something fundamental about CNN training? One approach to addressing this issue would be to demonstrate that this class of networks can achieve reasonable performance on a baseline task (hence captures something important about the general case). However, this evaluation is not given.\n\nComments:\n\n1. It seems that a stronger result than 4.7 (hence 4.8) is true. Looking at (A.14) on p. 14, I believe you should have that for $i=1, \\dotsc, p$, $\\phi_{\\i(i)}^\\prime (x)$ are iid copies, since $x \\sim N(0, I)$. Thus, the vector $v$ itself is a constant, and the population tensor $T$ is rank-one.  So, if you define instead $\\alpha_{CNN} = \\EE[\\phi_{\\i(1)}^\\prime(x)]$, you get $T = L_{CNN}$. If I'm confused, can the authors please help clear up my confusion?\n\n2. In order for the bound in 4.8 to be meaningful, you must have $D^2 \\leq d_\\min$. But since $d_\\min \\leq p^{1/D}$, this implies awkwardly that the input dimension be exponential in the depth of the network: $p \\geq D^{2D}$.  This should be mentioned, as it significantly challenges the claim that the results apply to arbitrarily deep networks.\n\n3. Can the authors comment on the challenges for extending these results, say to multiple kernels per layer, or non-gaussian inputs? The latter seems hard, but maybe the former translates to higher rank tensor decomposition?\n\n4. Can the authors include a line in the numerical experiment figures indicating the lower bound from 4.8? This way it is possible to judge how sharp the result is.\n\n5. Perhaps a different activation than ReLU should be used in the numerical experiments, since it violates the smoothness assumption.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}