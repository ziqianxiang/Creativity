{
    "Decision": {
        "metareview": "On the positive side, this is among the first papers to exploit non-Euclidean geometry, specifically curvature for adversarial learning. However, reviewers are largely in agreement that the technical correctness of this paper is unconvincing despite substantial technical exchanges with the authors.  ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "Technical correctness issues"
    },
    "Reviews": [
        {
            "title": "Promising performance",
            "review": "In the paper, the authors proposed to solve the learning problem of adversarial examples from Riemannian geometry viewpoint. More specifically, the Euclidean metric in Eq.(7) is generated to the Riemannian metric (Eq.(8)). Later, the authors built the correspondence between the metric tensor and the higher order of Taylor expansions.  Experiments show the improvement over the state-of-the art methods.\n\nSome questions:\nFirst of all, the idea of introducing Riemannian geometry is appealing.\nIn the end, a neural network can be roughly viewed as a chart of certain Riemannian manifold.\nThe challenging part is how can you say something about the properties of the high dimensional manifold, such as curvature, genus, completeness etc.\nUnfortunately, I didn't find very insightful analysis about the underlying structure.\nWhich means, hypothetically, without introducing Riemannian geometry we can still derive Eq.(14) from Eq.(12), Taylor expansion will do the work.\nSo more insights about metric tensor G determined manifold structure can be very helpful.\n \nSecond, Lagrange multipliers method is a necessary condition, which means the search directions guided by the constraint may not lead to the optimal solutions.\nIt would be better if the authors can provide either theoretical or experimental study showing certain level of direction search guarantee.\n \nLast, the experiment results are good, though it lacks of detailed discussion, for example could you decompose the effect achieved by proposed new Riemannian constraint and neural network architecture? Merely demonstrating the performances does not tell the readers too much.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Promising results but proposed framework is not general enough for a Riemannian manifold and seems wrong",
            "review": "1.\tSome motivation of extending the adversarial examples generation on manifold should be there.\n2.\tEven if \\epsilon is small, if x is on a manifold, x+\\epsilon may not, so I am not sure about the validity of the definition in Eq. (7) and what follows from here. One solution is putting the constraint d(x, Exp_x(\\epsilon)) \\leq \\sigma, which implies that g(\\epsilon, \\epsilon) \\leq \\sigma. \nAlso, x and \\epsilon lies in completely different space, \\epsilon should lie on the tangent space at x. So, I don’t understand why x+\\epsilon makes sense? It makes the rest of the formulation invalid as well.\n3.\tI don’t understand why in Eq. (12), d(x, x+\\epsilon)^2 = |m(x)|? Do authors want it to be equal, otherwise, I can not see why this equality is true.\n4.\tIn Lemma 2.3, please make H in \\mathbb{R}^{n\\times n} instead of \\mathbb{R}^n \\times \\mathbb{R}^n (same issue for Lemma 2.4), later does not make sense in this context. Also, why not write |H|=U|\\Sigma|U^T, instead of what you have now. \n5.\tNo need to prove Lemma 2.3 and 2.4. These are well-known results in matrix linear algebra.\n6.\tIt’s nice that the authors generalize to l_p ball and can show FGSM as a special case.\n7.\tSome explanation of Algo. 2 should be there in the main paper given that it is a major contribution in the paper and also authors put a paper more than 8 pages long, so as a reader/ reviewer I want more detailed explanation in the main body.\n8.\tIn Algorithm 1, step 7: “Update the parameters of neural network with stochastic gradient” should be updated in the negative direction of gradient.\n9.\tAlgorithm 2 is clearly data driven. So, can authors comment on special cases of Algorithm 2 when we explicitly know the Riemannian metric tensor, e.g., when data is on hypersphere.\n10.\tCan authors comment on the contemporary work https://arxiv.org/pdf/1807.05832.pdf, as the purpose is very similar.\n11.\tThe experimental validation is nice and showed usefulness of the proposed method.\n\n\nPros:\n1.\tA framework to show the usefulness of non-Euclidean geometry, specifically curvature for adversarial learning.\n2.\tNice set of experimental validation.\n\nCons:\n1.\tSome theorems statement can be ignored to save space, e.g., Lemma 2.3 and 2.4. And instead, need some explanation of Algorithm 2 in the main text. Right now, not enough justification of additional page.\n2.\tNot sure about the validity of the main formulation, Eq. (7) and other respective frameworks when data x is on a manifold.\n\nMinor comments:\n1.\tIn page 2, “In this case, the Euclidean metric would be not rational.”-> “In this case, the Euclidean metric would not be rational”.\n2.\t “However, in a geometric manifold, particularly in Riemannian space, the gradient of a loss function unnecessarily presents the steepest direction.” Not sure what authors meant by “unnecessarily presents”\n3.\tNo need to reprove Lemma 2.2, just give reference to a differential geometry textbook like Chavel or Boothby.\n\nI want the authors to specifically address the cons. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A weak submission, the Riemannian analysis seems flawed.",
            "review": "The authors argue that they propose a method to find adversarial examples, when the data lie on a Riemannian manifold. In particular, they derive a perturbation, which is argued to be the worst perturbation for generating an adversarial example, compared to the classical Euclidean derivation.\n\nI strongly disagree with the proposed (Riemannian) geometric analysis, because there are several technical mistakes, a lot of arbitrary considerations, and flawed assumptions. In particular, my understanding is that the proposed method is not related at all with Riemannian geometry. For justification I will comment some parts:\n\n#1) In Section 1 paragraph 4, and in Section 2.3 after Eq. 14, the sentences about the gradient of a function that is defined on a manifold are strange and unclear. In general, the gradient of a function defined on a manifold, points to the ascent direction. Thus, if I understood correctly the sentences in the paper support that the gradient of such a function is meaningless, so I think that they are wrong.\n\n#2) How the $\\ell_2$-ball on a manifold is defined? Usually, we consider a ball on the tangent space, since this is the only Euclidean space related to the manifold. Here, my understanding is that the authors consider the ball directly on the manifold. This is clearly wrong and undefined.\n\n#3) To find the geodesic you have to solve a system of 2nd order non-linear ODEs, and there are additional details which I will not include  here, but can be easily found in the Riemannian geometry literature. Also, I think that the Lemma 2.2. is wrong, since the correct quantity of Eq. 3 is $ds^2 = g_ij(t)d\\theta^i d\\theta^j dt^2$, where $dt\\rightarrow 0$ based on the included proof. This is clearly not a sensible geodesic, it is just the infinitesimal length of a line segment when $t\\rightarrow 0$, which means that the two points are infinitesimally close.\n\n#4) If $x, y$ is on a Riemannian manifold then the $x+y$ operator does not make sense, so Eq. 7 is wrong. In particular, for operations on Riemannian manifolds you need to use the exponential and the logarithmic map.\n\n#5) Continuing from #4). Even if we consider the perturbation to be sufficiently small, still the $x+\\epsilon$ is not defined. In addition, the constraint in Eq. 8 is wrong, because the inner product related to the Riemannian metric has to be between tangent vectors. Here the $\\epsilon$ is an arbitrary quantity, since it is not defined where it actually lies. In general, the derivation here is particularly confusing and not clear at all. In my understanding the constraint of Eq. 8 is a purely linear term, and $d$ is not the geodesic distance on a manifold. It just represents the Mahanalobis distance between the points $x$ and $x+\\epsilon$, for a matrix $G$ defined for each $x$, so it is a linear quantity. So Eq. 9 just utilizes a precondiner matrix for the classical linear gradient.\n\n#6) The Eq. 12 is very flawed, since it equalizes a distance with the Taylor approximation error. I think that this is an unrealistic assumption, since these terms measure totally different quantities. Especially, if $d$ is the geodesic distance.\n\n#7) The upper bound in inequality Eq. 13 comes from Eq. 12, and it is basically an assumption for the largest absolute value of the Hessian's eigenvalues. However, this is not discussed in the text, which are the implications?\n\n#8) I find the paper poorly written, and in general, it lacks of clarity. In addition, the technical inconsistencies makes the paper really hard to follow and to be understood. I mentioned above only some of them. Also, there are several places where the sentences do not make sense (see #1), and the assumptions made are really arbitrary (see #6). The algorithms are not easy to follow. Minor comments, you can reduce the white spaces by putting inline Eq. 3, 4, 5, 6, 9, 10, 14, and Figure 2. The notation is very inconsistent, since it is very unclear in which domain/space each quantity/variable lies. Also, in Section 2.5. the authors even change their notation.\n\nIn my opinion, the geometrical analysis and the interpretation as a Riemannian manifold is obviously misleading. Apart from the previously mentioned mistakes/comments, I think that the proposed approach is purely linear. Since actually the Eq. 14 implies that the linear gradient of the loss function, is just preconditioned with the Hessian matrix of the loss function with respect to the input $x$. Of course, if this function is convex around $x$, then this quantity is the steepest ascent direction of the loss function, simply on the Euclidean space where $x$ lies. However, when this function is not convex, I am not sure what is the behavior when all the eigenvalues of the Hessian are set to their absolute value. Also, the (arbitrary) constraint in Eq. 13 implicitly sets a bound to the eigenvalues of the Hessian, which in some sense regularizes the curvature of the loss function. To put it simple, I think that the proposed method, is just a way to find a preconditioner matrix for the linear gradient of the loss function, which points to the steepest direction. This preconditioner is based on the Hessian of the loss function, where the absolute values of the eigenvalues are used, and also, are constrained to be bounded from above based on a given value.\n\nGenerally, in my opinion the authors should definitely avoid the Riemannian manifold consideration. I believe that they should change their perspective, and consider the method simply as a local preconditioner, which is based on the Hessian and a bound to its (absolute) eigenvalues. They should also discuss what is the implication by setting the eigenvalues to their absolute values. However, since I am not an expert in the field of adversarial examples, I am not sure how novel is this approach.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}