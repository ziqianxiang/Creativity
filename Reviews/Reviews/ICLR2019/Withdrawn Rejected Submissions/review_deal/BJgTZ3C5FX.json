{
    "Decision": {
        "metareview": "This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions.\n\nAs the reviewers point out, the primal approach has been studied by other papers (which this submission doesn't cite, even in the revision), and suffers from a well-known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don't think this work is ready for publication in ICLR.\n",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "lack of novelty, variance in high dimensions"
    },
    "Reviews": [
        {
            "title": "promising results and idea",
            "review": "The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. To this end, the authors formulated the optimal transport cost as a linear programming problem. The quantitative results-- empirical Wasserstein distance show the superiority of the proposed methods.\n \nMy concerns come from both theoretical and experimental aspects:\nThe linear-programming problem Eq.(4)-Eq.(7) has been studied in existing literature.\nThe contribution is about combining this existing method to supervise a standard neural network parametrized generator, so I am not quite sure if this contribution is sufficient for the ICLR submission.\nIn such a case, further experimental or theoretical study about the convergence of Algorithm 1 seems important to me.\n \nAs to the experiments, firstly, EWD seems to be a little bit biased since EWD is literally used to supervise the training of the proposed method.\nOther quantitative metric studies can help justifying the improvement.\nAlso, given that the paper brings the WGAN family into comparison, the large scale image dataset should be included since WGAN have already demonstrated their success.\n \nLast things, missing parentheses in step 8 of Algorithm 1 and overlength of url in references.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review for \"Generative model based on minimizing exact empirical Wasserstein distance\".",
            "review": "The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models.\n\nThis is an approach that has been tried[1,2] (even with the addition of entropy regularization) and studied [1-5] extensively. It doesn't scale, and for extremely well understood reasons[2,3]. The bias of the empirical Wasserstein estimate requires an exponential number of samples as the number of dimensions increases to reach a certain amount of error [2-6]. Indeed, it requires an exponential number of samples to even differentiate between two batches of the same Gaussian[4]. On top of these arguments, the results do not suggest any new finding or that these theoretical limitations would not be relevant in practice. If the authors have results and design choices making this method work in a high dimensional problem such as LSUN, I will revise my review.\n\n[1]: https://arxiv.org/abs/1706.00292\n[2]: https://arxiv.org/abs/1708.02511\n[3]: https://arxiv.org/abs/1712.07822\n[4]: https://arxiv.org/abs/1703.00573\n[5]: http://www.gatsby.ucl.ac.uk/~gretton/papers/SriFukGreSchetal12.pdf\n[6]: https://www.sciencedirect.com/science/article/pii/0377042794900337",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Title claim seems wrong",
            "review": "The paper ‘Generative model based on minimizing exact empirical Wasserstein distance' proposes\na variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying\non the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.\nComparisons with other variants of Wasserstein GAN is proposed on MNIST.\n\nI see little novelty in the paper. The derivation of the primal version of the problem is already \ngiven in  \nCuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).\n\nUsing optimal transport computed on batches rather the on the whole dataset is already used in (among\nothers)\n Genevay, A., Peyré, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS\n Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  \n\nAlso, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on \nbatches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very \nwell described in the paper (Section 3): \nhttps://openreview.net/pdf?id=S1m6h21Cb\n\nComputing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be\nproved and discussed.\n\nFinally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).\n\n\nTypos:\n Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}