{
    "Decision": {
        "metareview": "This paper proposes search-guided training for structured prediction energy networks (SPENs).\n\nThe reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method.\n\nR1 was positive and recommends acceptance; R2 and R3 thought the paper was on the incremental side and recommend rejection. Given the space restriction to this year's conference, we have to reject some borderline papers. The AC thus recommends the authors to take the reviewers comments in consideration for a \"revise and resubmit\".",
        "confidence": "3: The area chair is somewhat confident",
        "recommendation": "Reject",
        "title": "Incremental improvement over rank-based training of SPENs "
    },
    "Reviews": [
        {
            "title": "Incremental improvement over rank-based training of SPENs",
            "review": "# Summary\n\nThis paper proposes search-guided training for structured prediction energy networks (SPENs). SPENs are structured predictors that learn an input-dependent, non-linear energy function that scores candidate output structures. Many methods have recently been proposed for training SPENs. One in particular, rank-based training, has the advantage of supporting training from weak supervision in the form of a reward function. By performing gradient descent on this reward function, rank-based training generates output, improved output pairs that become margin-based constraints on the learning objective. Each constraint specifies a pair of outputs for a given input, and penalizes the current weights if the improved output is not scored higher than the other output by a certain margin.\n\nThis paper addresses a limitation of rank-based training, that this gradient descent procedure for finding output pairs may get stuck in plateaus. In search-guided training, truncated randomized searches are performed starting at an initial output to find an improved output. The paper says that the random search procedure is informed by the reward function, but it is not specific. Are steps in the search space performed uniformly at random? The paper only says that the returned improved example must score higher in the reward function by some margin \\delta that is \"based on the features of the reward function (range, plateaus, jumps)\" but it is not discussed how to identify these features of the reward function or how to set \\delta accordingly.\n\nExperiments are conducted multi-label classification, citation field extraction, and shape parsing. On multi-label classification search-guided SPENs (SG-SPENs) outperform structural SVM training of SPENs. Why is it not compared with rank-based training (R-SPENs)? On citation field extraction, SG-SPENs improves accuracy by two percentage points over R-SPENs. On shape parsing, R-SPENs fail because it cannot produce valid parsing programs as improved outputs. SG-SPENs perform well relative to other methods like iterative beam search and neural shape parsing.\n\n# Strengths\n\nSG-SPENs are better across the experiments than other SPEN training methods, though I do not know why they are not compared against R-SPENs on multi-label classification.\n\n# Weaknesses\n\nThe work seems incremental without any major new insights beyond the work on R-SPENs. The idea seems to reduce to doing random search instead of gradient descent on a reward function in order to produce output pairs.\n\nAs mentioned above, the paper is also light on details about how the experiments were conducted, such as setting \\delta and creating the space of operators to use when searching for improved outputs.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful extension of prior weakly-supervised SPEN work",
            "review": "Summary:\nThis paper discusses a method to train SPENs when strong supervision is not provided. Instead, training feedback comes in the form of a scalar-valued scoring function for a provided input as well as a prediction. The approach taken here is similar to that described in [1] in that score-violating pairs are found using some procedure, which are then used to update the parameters of the model. The primary difference here is that a random search procedure is used to find score violations rather than the test-time inference procedure; this is justified by noting that the gradient descent procedure may become stuck in flat areas of the optimization surface and thus not encounter high-reward areas. Experiments are run on multilabel classification, citation field extraction, and shape parsing tasks to demonstrate the validity of this approach.\n\nComments:\nOverall, this paper is very nicely written and presents its ideas very clearly. The base approach is the same as presented in [1], but the changes to the learning procedure are adequately justified (and the experiments corroborate this). Furthermore, everything is explained in sufficient detail to be easy to follow. The main detail that I didn’t notice anywhere was a sentence or two describing the random search procedure used - adding this would further clarify your approach.\n\nThe tasks chosen to evaluate these methods are diverse and indicate that this approach is broadly useful in situations where strong supervision may be hard to come by. I think it would have been interesting to see how the model performs in a semi-supervised task (i.e. where some small fraction of the data has labels), but perhaps this is better suited for future work. The one question I have regarding your results is the following: you include the average reward for the citation-field extraction task in your results table, but don’t seem to comment on this anywhere. Are there any conclusions that you think these results imply?\n\nThis paper is an excellent addition to the field of structured prediction, and thus I think it should be accepted.\n\n[1] Rooshenas, A., Kamath, A., & McCallum, A. (2018). Training Structured Prediction Energy Networks with Indirect Supervision. NAACL HLT 2018\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposes to use a reward function to guide the learning of energy-based models for structured prediction. The idea is to update the energy function based on a random search algorithm guided by a reward function. At each iteration, the SPEN proposes a solution, then a better one is found by the search algorithm, and the energy function is updated accordingly.  Experiments are made on three use-cases and show that this method is able to outperform other training algorithms for SPENs. \n\nIn term of model, the proposed algorithm is interesting since it can allow us to learn from weakly supervised datasets (i.e a reward function is enough). Note that in Section 3, the reward function R is never properly defined which would be nice. The algorithm is quite simple and well presented in the paper. The fact that it is based on a margin could be discussed a little bit more since the effect of the margin is not clear in the paper (the value of alpha). Moreover, the structured prediction problem has already been handled as the maximization of a reward function using RL techniques (see works by H. Daume, and works by F. Maes) and the interest of this approach w.r.t these papers is not clear to me. A clear discussion on that point (and experimental comparison) would be nice. \n\nThe experimental section could be improved. First, the experiments on multi-label classification do not provide any comparison with SoTA methods while the two other use-cases provide some comparisons. Moreover, as far as I understand, the different use-cases could be fully supervised, and different reward functions could be defined. So investigating more deeply the consequences of the nature of the supervision/reward on these use-cases could be interesting and strengthen the paper.  Moreover, training sets are very small and it is difficult to know if this method can work on large-scale problems. \n\nPro:\n* interesting algorithm for structured prediction (base on reward)\n* interesting results on some (toy) use-cases\n\nCons:\n* Lack of discussion on the positive/negative point of the approach w.r.t SoTA, and on the influence of the reward function\n* Lack of experimental comparisons \n* Only toy (but complicated) problems with limited training sets\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}