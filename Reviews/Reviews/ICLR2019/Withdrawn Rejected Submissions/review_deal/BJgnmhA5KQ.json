{
    "Decision": {
        "metareview": "+ a simple method\n+ producing diverse translation is an important problem \n\n- technical contribution is limited / work is incremental\n- R1 finds writing not precise and claims not supported,  also discussion of related work is considered weak by R3\n- claims of modeling uncertainty are not well supported\n\n\nThere is no consensus among reviewers.  R4 provides detailed arguments why (at the very least) certain aspects of presentations are misleading (e.g., claiming that a uniform prior promotes diversity). R1 is also negative, his main concerns are limited contribution and he also questions the task (from their perspective  producing diverse translation is not a valid task; I would disagree with this).  R2 likes the paper and believes it is interesting, simple to use and the paper should be accepted. R3 is more lukewarm. \n\n",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Reject",
        "title": "writing needs to be improved / contribution limited"
    },
    "Reviews": [
        {
            "title": "Good direction, though problematic assumptions",
            "review": "# Summary of model \n\nThe paper proposes a mixture model formulation of NMT where the mixing coefficients are uniform and fixed. The authors then proceed to derive a lowerbound on the marginal likelihood \n\np(y|x) = \\sum_z p(z)p(y|x,z) > 1/K \\max_z p(y|x,z)\n\nby picking the component z for which the joint likelihood is maximised. With a uniform p(z) this clearly selects the z for which the conditional p(y|x,z) is maximum. I use strictly greater here because p(z) > 0 and p(y|x,z) > 0 for every z.\n\nThe loss L(\\theta|x,y) for an observation (x,y) is \\min_z - \\log p(y|z,x; \\theta)\nwhose gradient with respect to NN parameters (theta) is \\grad_theta \\log p(y|z,x; \\theta) for the component z that minimises the negative log-conditional and 0 for every other component, thus while this requires K forward passes (to solve \\min_z), it only takes 1 backwards pass.\n\n# Discussion\n\nI appreciate model-based (as opposed to search-based) attempts to improve diversity for generation tasks such as MT. Latent variable modelling aims at a more explicit account of the generative procedure, namely, the joint distribution, which can potentially disentangle and explain different modes of the marginal. Thus from that point of view, this paper points to an exciting direction. That said, in my view, the assumptions behind the proposed approach are not justifiable and some of the claims are simply not appropriate. Below I try to support this view.\n\nA stepping stone of this model is that p(y|x,z) must be \"large for only one value of z\" (as authors put it), and authors *assume* that will be the case. \n\nWhile the bound in equation (2) holds, whether or not p(y|z,x) turns out to be \"large for only one value of z\", it will be a very loose bound unless that happens. \n\nThe key point is that one cannot *assume* it to be the case. One could perhaps *promote* it to be the case, but there's no aspect of the model formulation (or objective) that promotes such behaviour.\n\nBackpropagating through whichever component happens to assign the largest likelihood does not guarantee (nor encourages) the other conditionals to *independently* end up going to zero. \n\nGiven the level of parameter sharing, I'd even consider the possibility that the exact opposite happens. As authors put it themselves \n\n\"Instead, by sharing parameters, even unpopular experts receive some gradients throughout training.\"\n\nIt's true they do, but they are being updated on the basis of the unilateral opinion of the selected component about the likelihood of the data.\n\nNote that the true posterior p(z|x,y) is exactly proportional to the likelihood, as the prior is *uniform and fixed*:\n  p(z|x,y) \\propto 1/K p(y|x,z) \\propto p(y|x,z)\nThis means that the authors expect the likelihood to do component allocation on its own. That is, the conditionals p(y|x,z=1), ..., p(y|x,z=K) must somehow coordinate themselves in making good use of the latent components. Without any mechanism to promote \"competition\" (in the parlance of Jacobs et al 1991), I don't see how this can work.\n\nAlso, the paper claims to model uncertainty, if I take the posterior to fulfil this claim, then I'm just left with a likelihood (again, due to uniform prior). In any case, a notion of uncertainty here would be conditioned on a point estimate of the network's parameters and should thus be worded carefully.\n\n# Clarifications\n\n1. \"we aim to explicitly model uncertainty during training\" can you make a case for where that happens in your model?\n\n2. \"prevents the gating from training well and the latent variable embeddings from specializing\" which gating?\n\n3. \"While they showed improvements due to the regularization effect of the Monte Carlo gradient estimate”. I find it strange to talk about the “regularisation effect” of a gradient estimate, perhaps you can be a bit more precise here? Or perhaps you are referring to some specific component of the objective function whose gradient we are estimating via MC and perhaps that component may have some regularisation effect.\n\n4. if you aim to have p(y|x,z) high for a single latent variable at a time, you are implicitly saying that every x has at most (or rather exactly) K translations with non-negligible probability. Is that sensible? \n\n# Pros/Cons\n\nPros\n\n* simple: the approach presented here requires no significant changes to otherwise standard architectures, it instead concentrates in a change of objective and training algorithm.\n* assessment of variability in translation: this paper proposes to use BLEU and a corpus of multiple references in an interesting (potentially novel) way. \n\nCons\n\n* problematic assumptions: e.g. posterior will turn out sparse without any explicit way to promote such behaviour\n* unrealistic claims: e.g. modelling uncertainty\n* imprecise use of technical language: some technical terms are not used in their strictly technical sense (e.g. uncertainty, degeneracy), some explanations employ loosely defined jargons (e.g. regularisation effect of the gradient estimate) \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but somewhat incremental approach; related work a bit weak",
            "review": "The authors aim to increase diversity in machine translation using a multinomial latent variable that captures uncertainty in the target sentence. Modeling uncertainty with latent variables is of course relatively common in ML, and this work has similarities with latent variables models for MT [Zhang et al., 2016] and for other generation tasks such as dialogue [Serban et al., 2017; etc.]. The key difference is that the authors here use a Mixture of Expert (MoE) approach while most relevant prior works use variational approaches. Experiments show improvements in diversity over variational NMT [Zhang et al., 2016] and decoding-time approaches (e.g., diversity constraints [Vijayakumar et al., 2016]).\n\nOverall, the proposed approach (hard-MoE) is well motivated and the experimental results are relatively promising. I think the authors did a good job analyzing and justifying their approach against the soft version of their model (i.e., soft-MoE causes experts to “die” during training) and variational alternatives (i.e., variational approaches often have failure modes where the latent variable is effectively ignored.) \n\nHowever, I find related work a bit weak because the problem of producing diverse output has been a much bigger focus in tasks other than MT, such as dialogue and image captioning. The paper glosses over related approaches on these tasks, but the need to model uncertainty for these other tasks is much bigger since source and target are usually not semantically equivalent. So it would have been nice to see argumentative (or even empirical) comparisons with popular models such as VHRED for dialogue [Serban et al., 2017], as many of these models are not intrinsic to either MT or dialogue (the only aspect specific to dialogue in VHRED is context, but it can be set to empty and thus VHRED could have been used as a baseline in the paper.) It would be interesting to compare the work against Serban et al. [2017]’s justification for using a latent variable, which is quite different (see their bit on “shallow generation”, and the idea that their latent variable encapsulates “the high-level semantic content of of the output”).  \n\nOne technical caveat is that there appears to be some inconsistency in the comparison between human and systems in Table 1. If N is the number of references, then systems are evaluated on N references while the human “system” on only N-1 because of leave-one-out. While this difference might have less of an impact on “average oracle BLEU” than standard BLEU, having one less reference might still penalize the human “system”, and this might partially explain why “beam search’s average oracle BLEU is fairly close to human’s average oracle BLEU”. The right thing to do would be to evaluate both human and all systems in a leave-one-out approach (i.e., let references [r1 … rN] and systems [s1 … sM], then evaluate each element of [s1 … sM r1] on references [r2 … rN], etc.). In that manner, all the “systems” including human are consistently evaluated on *exactly* the same references. \n\nMinor comments: \n\n “By putting the model in evaluation mode during minimization we also speed up training and reduce memory consumption, since the K forward passes have no gradient computation or storage.” In other words, does this mean the algorithm is easy to *parallelize* because sharing parameters is often what kills the effectiveness of parallelized SGD and variants? If so, “parallelizing” is key word to mention here otherwise I don’t see how we can speed that up by increasing K.\n\nFigure 2: performance drops with K approaching 20. What happens with K=50 or 100 or more? This is a bit of a concern because (1) larger K could require a massive amount parallelization and (2) competing approaches such as VHRED can handle latent variables with higher capacities.\n\nPractical considerations subsection is too vague: parameter sharing is not formally/mathematically explained and the work could be hard to reproduce exactly (as there are often different ways to share parameters). \n\nWhy no “#ref covered” for human in Table 1, and why no comparison with Variational NMT? Zhang et al [2016] is the most talked about competing model, so it should probably be evaluated on both settings.\n\nMissed reference: Mutual Information and Diverse Decoding Improve Neural Machine Translation.\nJiwei Li, Dan Jurafsky. https://arxiv.org/abs/1601.00372",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for Diverse MT with a Single Multinomial Latent Variable",
            "review": "This paper studies the diverse text generation problem, specifically on machine translation problem. The authors use a simple method, which just using a single multinomial latent variable compared with previous approaches that using multi latent variables. They named the approach: Hard-MoE. They use parallel greedy decoding to generate the diverse translations and the experiments on three WMT datasets show the approach make a trade-off between diversity and quality.\nIn general, I think generating the diverse translations for machine translation problem may not so important and piratically in actual scenarios. In fact, how to generate fluent and correct translations is more important. \n\nFor the details, there are some problems. 1) The only modification for this work is to make the soft probability of p(z|x) to be 1/K. The others are several experimental studies. To be an formal ICLR paper, this may not be interesting enough to draw my attention. 2) In case of the results, though the authors claimed they achieved better trade-off between diversity and quality, in my opinion, the beam original beam search is good enough from the results in Table 1. 3) In table 2, what means k=0 for the BLEU score? 4) I want to indicate that the purpose of VAE approach related to this work is to increase the model performance w.r.t. the BLEU score instead of the diversity, same as the original MoE method. 5) There are some related works to this work, but their methods are also very effective in terms of the BLEU score, e.g., the author can check this one in EMNLP this year: “Sequence to Sequence Mixture Model for Diverse Machine Translation”. Authors may need a more discussion between those works and this work.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting contribution",
            "review": "This paper proposes a sequence to sequence model augmented with a multinomial latent variable. This variable can be used to generate multiple candidate translations during decoding. This approach is simpler than previous work using continuous latent variables or modifying beam search to encourage diversity, obtaining more diverse translations with a smaller drop in translation accuracy.   \n\nStrengths:\n- Simple model that succeeds in achieving its goal of generating diverse translations.\n- Provides insights into training models with categorical latent variables. \nWeaknesses:\n- More insight into what the latent variable is learning to represent would strengthen the paper. \n\nWhile the model is simple, its simplicity has significant strengths: In contrast to more complex latent space, the latent variable assignments can be enumerated explicitly, which enables it to be used to control the generation and compare outputs. The simplicity of the model will force the latent variable towards capturing diversity - modelling uncertainty in how to express the output rather than uncertainty in the content. \n\nOne question about the model architecture is just whether it is sufficient to feed the latent variable embedding only once, as it effect might be diluted across long output sequences (as opposed to, say, feeding the latent variable at each time step). \n\nThe paper provides some interesting insights, such as the need to do hard EM-style training and turning off dropout when inferring the best latent variable assignment during training, to avoid mode collapse. \n\nWhat is the effect of initialization? This often has a large impact in EM-style training, and could also lead to mode collapse, though in this case the restricted parameterization might prevent that. \n\nWhat is the training time and computational resource requirements? Are multiple DGX-1s running in parallel required to train the model?\n\nWhat is not clear enough from the paper is what kind of structure the latent variables learn to capture. In particular this model is not biassed towards any explicit notion of the kind of diversity one would like to learn. While there is some qualitative analysis, further analysis would strengthen the paper. \n\nOverall this is a very interesting contributions that offer useful insights into designing controllable sequence generation models.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}