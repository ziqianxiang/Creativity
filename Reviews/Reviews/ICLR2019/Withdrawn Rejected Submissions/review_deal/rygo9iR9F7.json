{
    "Decision": {
        "metareview": "The paper proposes a progressive pruning technique that achieves high pruning ratio. Reviewers have a consensus on rejection. Reviewer 1 pointed out that the experimental results are weak. Reviewer 2 is also concerned about the proposed method and experiments. Reviewer 3 is is concerned that this paper is incremental work. Overall, this paper does not meet the standard of ICLR. Recommend for rejection. ",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Reject",
        "title": "incremental work"
    },
    "Reviews": [
        {
            "title": "The paper proposes a progressive pruning technique which imposes structural sparsity constraint on the weight parameter. Since solving the minimization with sparsity constraint is hard in general, the paper rewrites the optimization as an ADMM framework. While ADMM method suffers from slow convergence, a progressive weight pruning approach is proposed, which falls into curriculum learning.",
            "review": "The authors argue that ADMM-based approach achieves higher accuracy than projected gradient descent. However, experimental evidence is lacking. The authors should compare to a trivial variant of Adam that a projection step is followed by the gradient update.\n\nExperimental results are weak. It seems that the proposed method only works on small networks such as AlexNet and LeNet. On larger networks such as VGG-16 and ResNet, the proposed method achieves higher compression rates at the expense of lower accuracies compared to the related works. Thus, the authors should compare with other methods with the same compression rates.\n\nAs ADMM is sensitive to the penalty parameter, the authors should also conduct more experiments to show robustness of the choice of the penalty parameter across different experiments.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Concerns about proposed method and experiments",
            "review": "This paper focus on weight pruning for neural network compression. The proposed method is based on ADMM optimization method for neural network loss with constraint on the l_0 norm of weights, proposed in Zhang et al. 2018b. Two improvements, masked retraining and progressive pruning, are introduced. Masked retraining set the weights to zero at early stages and stop updating those weights. Progressing pruning keeps a buffer of partial pruning results and select the best performed model for further pruning. The proposed method achieves 30x compression rate for AlexNet and VGG for ImageNet.\n\nI have the following concerns about the proposed method. \n- It is unclear to me what is the benefit of ADMM for solving the sparse regularized NN optimization problem. Why is it better than projected gradient descent or proximal gradient method used in previous network pruning? I understand the proposed method is based on Zhang et al. 2018b, but a strong argument will support the draft.\n- I fail to understand the claim ``at convergence, the pruned DNN model will not be exactly sparse’’ in section 2.3. Z will always be sparse after the projection step in (5). At convergence, the linear constraint should be satisfied, which makes W = Z to be sparse. \n- Please describe the the proposed method in detail. The current description is very vague and I do not think it can be reimplemented based on the current draft. In each outer loop of ADMM, (4)(5) and dual update is applied (I consider solving (4) is the inner loop). How is the mask generated and fit into these equations? For progressive pruning, it looks to me there is an outer loop outside the outer loop of ADMM. Please provide details on how many iterations, and how the compression rate is decided for each iteration.\n- The hyper-parameters of the proposed method is unclear. It is a bit strange the optimization parameter \\rho could control the pruning rate (section 3.1). As described before, I guess the proposed method has three loops. How is the iterations counted, like for Figure 3. Please clarify the experiments are fair comparison, the better results are not because of more weight updates from the three loops. \n- It is unclear what is the benefit of masked retraining. It looks to me this kind of greedy approach will harm the performance (I have to guess if a weight is masked to be zero, it will never be updated or recovered). What happens if there are a lot of weights (Z in (5)) are zero at the early stage?\n- The progressive pruning looks heuristic and I am not convinced the buffer is necessary. There is always a progressive pruning trace that can directly lead to the results without selecting from candidates. For example, in Figure 2, we can just train model from 15x to 24x to 30x.  \n- The following works are related. \nLi et al. Pruning Filters for Efficient ConvNets. ICLR 2017\nAlvarez et al. Learning the number of neurons in deep networks. NIPS 2016\n\n=============== after rebuttal ===================\nI appreciate the authors' feedback and slightly raise the score. \n\nThough the compression results look good, I still have some concerns about the method. The motivation of the proposed method is not strong. The proposed mask is greedy and sounds ad-hoc. The proposed progressive pruning looks expensive. \n\nThe proposed method looks time consuming. For the experiments, I would love to see the training time comparing with baselines in table 1, not only the ADMM method in table 2. A fair comparison could be wall-clock time, or number of gradient updates for neural networks. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good compression rate of weights empirically, but lack of idea novelty",
            "review": "This paper proposed a progressive weight pruning approach to compress the learned weights in DNN. My major concerns about the paper are as follows:\n\n1. Novelty: The proposed approach heavily relies on the one in (Zhang et. al. 2018b) as shown in Sec. 2.2 for 1 page, making the paper as being an incremental work, like finding better initialization for (Zhang et. al. 2018b).\n\n2. Faster convergence: First of all, from Fig. 3 I do not believe that both methods converged, as both performances vary a lot with significant gaps. In terms of being faster, I do not think that it makes sense by comparing numbers of epochs in training with only one approach. There is no theoretical or empirical evidence (e.g. running time) to support this claim.\n\n3. I do not understand how the proposed approach is motivated by DP. To me it is more like a greedy search algorithm, while DP has the ability to locate global maximum. Does the proposed approach guarantee to find the maximum accuracy? Also, in Fig. 2 why was the best partial model replaced with the new one, rather than the worse one? There is no explanation to this at all. Besides, I do think this approach is very heuristic, same as some other approaches in the related work.\n\n4. Experiments: Since the performance varies a lot as shown in Fig. 3, how are the numbers calculated? Average? Best one? With/without cross-validation to tune parameters? How much gain in terms of running time in testing can you get with more compact models in practice? A training/testing behavior analysis is highly appreciated.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}