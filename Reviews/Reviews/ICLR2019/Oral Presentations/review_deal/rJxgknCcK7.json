{
    "Decision": {
        "metareview": "This paper proposes the use of recently propose neural ODEs in a flow-based generative model. \n\nAs the paper shows, a big advantage of a neural ODE in a generative flow is that an unbiased estimator of the log-determinant of the mapping is straightforward to construct. Another advantage, compared to earlier published flows, is that all variables can be updated in parallel, as the method does not require \"chopping up\" the variables into blocks.  The paper shows significant improvements on several benchmarks, and seems to be a promising venue for further research.\n\nA disadvantage of the method is that the authors were unable to show that the method could produce results that were similar (of better than) the SOTA on the more challenging benchmark of CIFAR-10. Another downside is its computational cost. Since neural ODEs are relatively new, however, these problems might resolved with further refinements to the method. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "Meta-Review"
    },
    "Reviews": [
        {
            "title": "Nice algorithm but incremental ",
            "review": "This paper discusses a technique for continuous normalization flow in which the transformations are not required to be volume preserving (the transformation with unity Jacobian), and architecture of neural network does not need to be designed to hold such property. Instead authors proposed no restriction on architecture of neural network to design their reversible mapping.\nThe Paper has good background and literature review, and as authors mentioned  this paper is base on the idea of  Chen, Tian Qi, et al. \"Neural Ordinary Differential Equations.\" arXiv preprint arXiv:1806.07366 (2018). Chapter two of this paper is summary of  \"Neural Ordinary Differential Equations.\" and chapter Three is main contribution of this paper that can be summarized under two points:\n\n1- Authors borrowed the \"continuous normalizing flow \" in Chen et al. and they have designed unbiased log density estimator using Hutchinson trace estimator and evaluated the trace with complexity of O(D) (dimension of data) instead of O(D^2) that  is used in chen et al. Paper\n\n2- They proposed by reducing the hidden layer dimension of neural network, it is possible that variance of estimator to be reduced \n\nNovelty and Quality:\nthe main contribution of this paper is summarized above.\nThe paper do not contain any significant theorem or mathematical claims, it is more focused on design of linear algorithm that estimate continuous normalizing flow that they have borrowed from the Chen et al. paper.  This is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. Also no guarantees or bound has been given about the variance reduction of  estimator and it is more based on the authors intuition.\n\nClarity:\nThe paper is well written and previous relevant methods have been reviewed well. There are a few issues that are listed below:\n1-in section 3 the reason that dimensionality of estimator can reduce to D from D^2 can be explained more clearly \n\n2- Figure 1 is located on first page of the paper but it has never been referred in main paper, just it is mentioned once in appendix , it can be moved to appendix.\n\n3- in section 3.1.1 the “view view” can be changed to “view”\n\nsignificance and experiments:\nThe experiments are very detailed and extensive and authors have compared their algorithm with many other competing algorithms and showed improvement in many of the cases. \nAs mentioned in Quality and Novelty part of the review, just one comparison is missing and that is the comparison to method that the paper is inspired by. It would be interesting to see how much trace estimator approach that has been used in this paper, would sacrifice the negative log-likelihood or ELBO specially in real data like MNIST and CIFAR 10.  it seems original paper has not reported the performance on those data-sets as well, is this difficult as chen et. al. paper algorithm for trace calculation has complexity of O(D^2)? \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "generative modeling with ODE's and Hutchinson's trace estimator, decent paper.",
            "review": "Summary:\nThis paper discusses an advance in the framework of normalizing flows for generative modeling, named FFJORD. The authors consider normalizing flows in the form of ordinary differential equations, as also discussed in [1]. Their contributions are two-fold: (1) they use an unbiased estimator of the likelihood of the model by approximating the trace of the jacobian with Hutchinson’s trace estimator, (2) they have implemented the required ODE solvers on GPUs. \n\nThe models are evaluated on a density estimation task on tabular data and two image datasets (MNIST and CIFAR10), as well as on variational inference for auto-encoders, where the datasets MNIST, Omniglot, Freyfaces and Caltech Silhouettes are considered. \n\nThe authors argue that the trace estimator, in combination with reverse-mode automatic differentiation to compute vector-Jacobian products, leads to a computational cost of O(D), instead of O(D^2) for the exact trace of the jacobian. \nThey compare this to the cost of computing a Jacobian determinant for finite flows, which is O(D^3) in general. They argue that in general all works on finite flows have adjusted their architectures for the flows to avoid the O(D^3) complexity, and that FFJORD has no such restriction.\nHowever, I would like the authors to comment on the following train of thought: autoregressive models, such as MAF, as well as IAF (inverse of an autoregressive model) do not require O(D^3) to compute jacobian determinants as the jacobian is of triangular form. Note however, they are still universal approximators if sufficient flows are applied, as any distribution can be factorized in an autoregressive manner. With this in mind, I find the red cross for MAF under free-form Jacobian slightly misleading. Perhaps I misunderstood something, so please clarify. \n\nAnother topic that I would like the authors to comment on is efficiency and practical use. One of the main points that the authors seem to emphasise, is that contrary to autoregressive models, which require D passes through the model to sample a datapoint of size D, FFJORD is a ‘single-pass’ model, requiring only one pass through the model. They therefore indicate that they can do efficient sampling. However, for FFJORD every forward pass requires a pass through an ODE solver, which as the authors also state, can be very slow. I could imagine that this is still faster than an autoregressive model, but I doubt this is actually of comparable speed to a forward pass of a finite flow such as glow or realNVP. \nOn the other hand, autoregressive models do not require D passes during training, whereas, if I understand correctly, FFJORD relies on two passes through ODE solvers, one for computing the loss, and a second to compute the gradient of the loss with respect to model parameters. So autoregressive models should train considerably faster. The authors do comment on the fact that FFJORD is slower than other models, but they do not give a hint as to how much slower it is. This would be of importance for practical use, and for other people to consider using FFJORD in future work. \n\nFor the density estimation task, FFJORD does not have the best performance compared other baselines, except for MNIST, for which the overall best model was not evaluated (MAF-DDSF). For variational inference FFJORD is stated to outperform all other flows, but the models are only evaluated on the negative evidence lower bound, and not on the negative log-likehood (NLL). I suspect the NLL to be absent from the paper as it requires more computation, and this takes a long time for FFJORD. Without an evaluation on NLL the improvement over other methods is questionable. Even if the improvement still holds for the NLL, the relative improvement might not weigh heavily enough against increased runtime. FFJORD does require less memory than its competitors.\n\nThe improved runtime by implementing the ODE solvers on GPU versus the runtime on a CPU would be useful, given that this is listed as one of the main contributions.\n\nBesides these questions/comments, I do think the idea of using Hutchinsons trace estimator is a valid contribution, and the experimental validation of continuous normalizing flows is of interest to the research community. Therefore, in my opinion, the community will benefit from the information in this paper, and it should be accepted. However I do wish for the authors to address the above questions as it would give a clearer view of the practical use of the proposed model. \n \nSee below for comments and questions:\n\nQuality\nThe paper has a good setup, and is well structured. The scope and limitations section is very much appreciated. \n\nClarity\nThe paper is clearly written overall. The only section I can comment on is the related work section, which is not the best part of the paper. The division in normalizing flows and partitioned transformations is a bit odd. Partitioned transformations surely are also normalizing flows. Furthermore IAF by Kingma et al. is put in the box of autoregressive models, whereas it is the inverse of an autoregressive model, such that it does not have the D-pass sample problem. For a reader who is not too familiar with normalizing flows literature, I think this section is a little confusing. Furthermore, there is no related work discussed on continuous time flows, such as (but not limited to) [2].\n\nOriginality\nThe originality of the paper is not stellar, but sufficient for acceptance. \n\nSignificance\nThe community can benefit from the experimental analysis of continuous time flows, and the GPU implementation of the ODE solver. Therefore I think this work is significant. \n\nDetailed questions/comments:\n\n1. In section 4.2, as an additional downside to MAF-DDSF, the authors argue that sampling cannot be performed analytically. Since FFJORD needs to numerically propagate the ODE, I do not think FFJORD can sample analytically either. Is this correct?\n2. The authors argue that they have no restriction on the architecture of the function f, even if they have O(D) estimation of the trace of the jacobian. However, they also say they make use of the bottle-neck trick to reduce the variance that arises due to Hutchinson’s estimate of the trace. This seems like a limitation on the architecture to me. Can the authors comment?\n3. In B.1 in the appendix, the street view house numbers dataset is mentioned, but no results appear in the main text, why not?\n4. In the results section, it is not clear to me which numbers of the baselines for different datasets are taken from other papers, and which numbers are obtained by the authors of this paper. Please clarify.\n5. In the conclusions, when discussing future work, the authors state that they are interested in reducing the number of function evaluations in the ODE solvers. In various disciplines many people have worked on this problem for a long time. Do the authors think major improvements are soon to be made?\n6. In section 5.2 the dependence of the number of function evaluations (NFE) on the data dimension D is discussed. As a thought experiment they use the fact that going from an isotropic gaussian distribution (in any D), to an isotropic gaussian distribution has a corresponding differential equation of zero. This should convince the reader that NFE is independent of D. However, this seems to me to be such a singular example, that I gain no insight from it, and it is not very convincing. Do the authors agree that this particular example does not add much? If not, please explain. \n\n[1] Chen et al. Neural ordinary differential equations. NIPS 2018\n[2] Chen et al. Continuous-time flows for deep generative models.\n\n**** EDIT *****\n\nI have read the response of the authors and appreciate their clarifications and the additional information on the runtimes. See my response below for the concern that remains about the absence of the estimate of the log likelihood for the VAE experiments. Besides this issue, the other comments/answers were satisfactory, and I think this paper is of interest to the research community, so I will stick with my score.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Scaling up of Neural ODE model for generative model",
            "review": "This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.\nThe contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. \nThe density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. \nAlthough the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.\nConcerning discussions and analysis:\n- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?\nIn algorithm 1, is \\epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}