{
    "Decision": {
        "metareview": "This paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder. In the limit the paper is able to show the random auto encoder transformation  as doing an approximate inference on data. The paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis. Even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem. ",
        "confidence": "4: The area chair is confident but not absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "ICLR 2019 decision"
    },
    "Reviews": [
        {
            "title": "Very interesting contribution on weighted-tied random auto-encoder",
            "review": "Building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder.  Technically, the paper is using the state evolution formalism. In particular the main theorem uses the analysis of the multi-layer version of these algorithm, the so-called state evolution technics, in order to analyse the behaviour of optimal decoding in weight-tied decoder. It is based on a clever trick that the behaviour of the decoding is similar to the one of the reconstruction on a multilayer estimation problem. This is a very orginal use of these technics.\n\nThe results are 3-folds: (i) a deep analysis of the limitation of weight-tied DAE, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that  to train a DAE.\n\nPro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and \nCon: I do not see much against the paper. A minor comment: the fact that the DAE is \"weight-tied\" is fundamental in this analysis. It actually should be mentioned in the title!\n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Through analysis on weight-tied autoencoders may benefit from more clear presentation",
            "review": "This work applies infinite width limit random network framework (a.k.a. Mean field analysis) to study deep autoencoders when weights are tied between encoder and decoder. Random network analysis allows to have exact analysis of asymptotic behaviour where the network is infinitely deep (but width taken to infinite first). This exact analysis allows to answer some theoretical questions from previous works to varying degrees of success. \n\nBuilding on the techniques from Poole et al (2016) [1], Schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties. The fact that the network component are split into encoder/decoder architecture choice along with weight tying shows various interesting phase of network configuration. \n\nMain concern with this work is applicability of the theoretical analysis to real networks. The autoencoding samples on MNIST provided in the Appendix at least visually do not seem to be a competitive autoencoder (e.g. blurry and irrelevant pixels showing up). \n\nAlso the empirical study with various schemes is little hard to parse and digest. It would be better to restructure this section so that the messages from theoretical analysis in the earlier section can be clearly seen in the experiments.\n\nThe experiments done on fixed learning rate should not be compare to other architectures in terms of training speed as learning rates are sensitive to the architecture choice and speed may be not directly comparable. \n\nQuestions/Comments\n- Without weight tying the whole study is not much different from just the feedforward networks. However, as noted by the authors Vincent et al (2010) showed that empirically autoencoders with or without weight tying performs comparably. What is the benefit of analyzing more complicated case where we do not get a clear benefit from? \n\n- Many auto encoding networks benefit from either bottleneck or varying the widths. The author’s regime is when all of the hidden layers grows to infinity at the same order. Would this limit capture interesting properties of autoencoders?\n\n- When analysis is for weight tied networks, why is encoder and decoder assume to have different non-linearity? It does show interesting analysis but is it a practical choice? From this work, would you recommend using different non-linearities?\n\n- It would be interesting to see how this analysis is applied to Denoising Autoencoders [3], which should be straightforward to apply similar to dropout analysis appeared in Schoenholz et al [2].\n\n[1] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential\nexpressivity in deep neural networks through transient chaos. In Advances in neural information\nprocessing systems, pp. 3360–3368, 2016.\n[2] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[3] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371–3408, 2010.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An intriguing work bringing non-trivial analytical insights on the behaviour of deep autoencoders with random tied weights. Likely to generate more work in this direction. ",
            "review": "This paper studies auto-encoders under several assumptions: (a) the auto-encoder's layers are fully connected, with random weights, (b) the auto-encoder is weight-tied, (c) the dimensions of the layers go to infinity with fixed ratios. The main contribution of the paper is to point out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations. The idea is original and will probably lead to new directions of research. Already the first applications that the paper suggests are exciting.\n\nThe paper does a good job in justifying assumptions (a), (b) and (c) in the introduction. It is convincing in the fact that this point of view may bring practical insights on training initialization for real-world autoencoders. Thus my opinion is that this paper brings original and significant ideas in the field.\n\nOne flaw of this paper is that the writing might be clearer. For instance when presenting the technical theorem (Theorem 1), it would be useful to have an intuitive explanation for the theorem and the state-evolution-like equations. However, I believe that there are some easy fixes that would greatly improve the clarity of the exposition. Here is a list of suggestions: \n\n- In Section 2.1, a large number of notations are introduced. It would help a lot if the authors made a graphical representation of these. For instance, a diagram where every linearity / non-linearity is a box, and the different variables $x_l$, $\\hat{x}_l$ appear would help a lot. \n\n- Section 2.2 is rather technical. The authors could try to give some more intuition of what's happening. For instance, they could spend more time after the theorem explaining what $\\tau_l, \\gamma_l$ and $\\rho_l$ mean. They could also introduce the notation S_sig and S_var early and this section (and not in Section 3), because it helps interpreting the parameters. It would also help if they could write a heuristic derivation of the state-evolution-like equations. From the paper, the only way the reader can understand the intuition behind those complicated equations is to look at the proof of Theorem 1 (which is rather technical). \n\n- In Section 3.1, I did not understand the difference between interpretations 1 and 2. Could the authors clarify? \n\n- In Section 3.4, I did not understand the sentence: \"In particular, near the phase transition of \\gamma, S_sig/S_var = \\Omega(\\beta^{1.5}\". If one uses the \\Omega notation, it means that some parameter is converging to something. What is the parameter? As a consequence, I did not understand this paragraph. \n\n- In Section 3.5, the authors should make clear from the beginning why they are running those specific simulations. What hypothesis are they trying to check? I finally concluded that they are running simulations to check if the hypothesis they make in the first paragraph are true. They also want to compare with some other criteria in the literature, named EOC, that also gives insights about the trainability of the network. However, they could explicitly say in the beginning of the second paragraph that this is the goal.\n\n- In a similar spirit, the authors should end Section 3.5 with a clear conclusion on whether or not the framework enables us to predict the trainability of the autoencoder. \n\n\n\nMinor edits / remarks: \n\n- Typo: last but one paragraph of the introduction: \"whose analysis is typically more straighforwards\" -> \"straightforward\".\n\n- At the end of Section 3.2: what can be proved about the behavior of \\gamma / \\sqrt{\\rho}? It is obviously a central quantity and the authors do not say what happens in the phases where \\gamma and \\rho go to infinity for instance. Is it because it is hard to analyse?\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}