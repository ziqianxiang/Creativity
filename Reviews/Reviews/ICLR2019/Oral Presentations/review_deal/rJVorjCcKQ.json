{
    "Decision": {
        "metareview": "The authors propose a new method of securely evaluating neural networks. \n\nThe reviewers were unanimous in their vote to accept. The paper is very well written, the idea is relatively simple, and so it is likely that this would make a nice presentation.",
        "confidence": "5: The area chair is absolutely certain",
        "recommendation": "Accept (Oral)",
        "title": "A very interesting new contribution to privacy and neural networks"
    },
    "Reviews": [
        {
            "title": "A nice systems approach to certain ML security problems with good performance",
            "review": "In this paper, the authors consider solving three ML security related challenges that would primarily arise\nin the cloud based ML model. Namely, they consider the setting where a client wishes to obtain predictions\nfrom an ML model hosted on a server, while being sure that the server is running the model they believe is being run\nand without the server learning nothing about their input. Additionally, the server wishes for the user to learn \nnothing about the model other than its output on the user's input. To solve this problem, the authors introduce a\nnew scheme for running ML algorithms in a trusted execution environment. The key idea is to oursource expensive\ncomputation involved with forwarding images through a model to an untrusted GPU in a way that still allows for\nthe TEE to verify the integrity of the GPU's output. Because the authors' method is able to utilize GPU computing,\nthey achieve substantial speed-ups compared to methods that run the full neural network in trusted hardware.\n\nOverall, I found the paper to be very well written and easy to digest, and the basic idea to be simple. The \nauthors strike a nice balance between details left to the appendix and the high level overview explained in\nthe paper. At the same time, the authors' proposed solution seems to achieve reasonably practicable performance\nand provides a simple high-throughput solution to some interesting ML security problems that seems readily\napplicable in the ML-as-a-cloud-service use case. I only have a few comments and feedback.\n\nI would recommend the authors use the full 10 pages available by moving key results from the appendix to the main\ntext. At present, much of the experimental evaluation performed is done in the appendix (e.g., Figures 3 through \n5). \n\nThe notation PR_{s \\overset{s}{\\gets}\\mathbb{S}^{n}}[...] is not defined anywhere as far as I can tell\nbefore its first usage in Lemma 2.1. Does this just denote the probability over a uniform random draw of\ns from \\mathbb{S}? If so, I might recommend just dropping the subscript: A, B, and C being deterministic\nmakes the sample space unambiguous. \"negl(\\lambda)\" is also undefined. \n\nIn section three you claim that Slalom could be extended to other architectures like residual networks.\nCan you give some intuition on how straightforward it would be to implement operations like concatenation\n(required for DenseNets)? I would expect these operations could be implemented in the TEE rather than \non the coprocessor and then verified. However, the basic picture on the left of Figure 1 may then change,\nas the output of each layer may need to be verified before concatenation? I think augmenting the right\nof Figure 1 to account for these operations may be straightforward. It would be interesting to see\nthroughput results on these networks, particularly because they are known to substantially outperform\nVGG in terms of classification performance.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Overall solid paper, would be stronger if it included a discussion of limitations of the approach",
            "review": "The authors propose a new method of securely evaluating neural networks. The approach builds upon existing Trusted Execution Environments (TEE), a combination of hardware and software that isolates sensitive computations from the untrusted software stack. The downside of TEE is that it is expensive and slow to run. This paper proposes outsourcing the linear evaluation portions of the DNN to an untrusted stack that's co-located with the TEE. To achieve privacy (i.e., the input isn't revealed to the untrusted evaluator), the approach adds a random number r to the input vector x, evaluates f(x+r) on the untrusted stack, then subtracts off f(r) from the output. This limits the approach to be applicable to only linear functions. To achieve integrity (verify the correctness of the output), the paper proposes testing with random input vectors (an application of Freivalds theorem, which bounds the error probability). The techniques for integrity and privacy works only on integer evaluations, hence the network weights and inputs need to be quantized. The paper tries to minimize degradation in accuracy by quantizing as finely as numerically allowable, achieving <0.5% drop in accuracy on two example DNNs. Overall, compared to full evaluation in a TEE, this approach is 10x faster on one DNN, and 40x to 64x faster on another network (depending on how the network is formulated).\n\nDisclaimer: I am a complete outsider to the field of HW security and privacy. The paper is very readable, so I think I understand its overall gist. I found the approach to be novel and the results convincing, though I may be missing important context since I'm not familiar with the subject.\n\nTo me, the biggest missing piece is a discussion of the limitations of the approach. How big of a network can be evaluated this way? Is it sufficient for most common applications? What are the bottlenecks to scaling this approach?\n\nIt's also not clear why integrity checks are required. Is there a chance that the outsourcing could result in incorrect values? (It's not obvious why it would.)\n\nLastly, a question about quantization. You try to quantize as finely as possible (to minimize quantization errors) by multiplying by the largest power of 2 possible without causing overflow. Since quantization need to be applied to both input and network weights, does this mean that you must also bound the scale of the input? Or do you assume that the inputs are pre-processed to be within a known scale? Is this possible for intermediate outputs (i.e., after the input has been multiplied through a few layers of the DNN)?\n\nPros:\n- Simple yet effective approach to achieve the goals laid out in the problem statement\n- Clearly written\n- Thorough experiments and benchmarks\n- Strong results\n\nCons:\n- No discussion of limitations\n- Minor questions regarding quantization and size limits\n\nDisclaimer: reviewer is generally knowledgeable but not familiar with the subject area.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "strong paper, significant and solid results",
            "review": "\nGiven the growing interest in building trust worthy and privacy protecting AI systems, this paper demonstrates a novel approach to achieve these important goals by allowing a trusted, but slow, computation engine to leverage a fast but untrusted computation engine. For the sake of protecting privacy, this is done by establishing an additive secret share such that evaluation on one part of the share is performed offline and the computation on the other part of the share is performed on the untrusted engine. To verify the correctness of the computation on the untrusted server, a randomized algorithm is used to sample the correctness of the results. Using these techniques, the authors demonstrate an order of magnitude speedup compared to running only on the trusted engine and 3-4 orders of magnitude speedup compared to software-based solutions.\n\nOverall this is a strong paper which presents good ideas that have influence in ML and beyond. I appreciate the fact that the authors are planning to make their code publicly available which makes it more reproducible. Below are a few comments/questions/suggestions \n\n1.\tThis papers, and other papers too, propose mechanisms to protect the privacy of the data while outsourcing the computation on a prediction task. However, an alternative approach would be to bring the computation to the data, which means performing the prediction on the client side. In what sense is it better to outsource the computation? Note that outsourcing the computation requires both complexity on the server side and additional computation on the client side (encryption & decryption).\n2.\tYou present the limitations of the trust model of SGX only in the appendix while in the paper you compare to other techniques such as Gazzelle which have a different trust model and assumption. It makes sense to, at least, hint the reader on these differences. \n3.\tIn section 2.2: “has to be processed with high throughput when available” is it high throughput that is required or low latency?\n4.\tIn Section 4.3: in one of the VGG experiment you computed only the convolution layers which, as you say, are commonly used to generate features. In this case, however, doesn’t it make more sense that the feature generation will take place on the client side while only the upper layers (dense layers) will be outsourced?\n5.\tIn section 4.3 “Private Inference” : do you include in the time reported also the offline preprocessing time? As far as I understand this should take the same amount of time as computing on the TEE.\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}