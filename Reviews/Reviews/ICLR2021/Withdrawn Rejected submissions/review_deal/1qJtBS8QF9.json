{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors consider view-consistency when learning graph neural networks. However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing. More deep discussions about related papers and analytic experiments are required to support this work. Additionally, I have concerns about the scalability of the method --- whether it can deal with more than two views and how it will perform are not studied in this work. I tend to reject it based on its current status. "
    },
    "Reviews": [
        {
            "title": "A multi-view contrastive learning framework for node classification with fewer labels.",
            "review": "Advantage:\n\nThe paper concentrated on an important perspective of graph learning: to utilize a small number of labels for large-scale graph learning. The framework is well demonstrated and the paper is easy to follow. \n\nWeakness:\n\n1. Novelty: My main concern of the paper is about the paper's novelty. There are already some works having similar multi-view contrastive learning frameworks, such as [1]. `Besides, The learning loss of the proposed method is similar to the self-training loss in SimCLR v2 [2].\n\n2. Experiment: The experiment results are not sufficient. The author claimed the proposed method is efficient for graph classification on large training datasets. But the experiment is conducted on three small graph datasets. Results on more larger datasets [3] are expected to support the effectiveness of the proposed method.\n\nReference:\n[1] Contrastive Multi-View Representation Learning on Graphs, ICML 2020\n[2] Big Self-Supervised Models are Strong Semi-Supervised Learners, NeurIPS 2020\n[3] Open Graph Benchmark: Datasets for Machine Learning on Graphs",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lack of innovation and lack of clear architecture analysis",
            "review": "This paper proposes a view-consistent framework to address the issues of expensive labels. In particular, this work first uses graph neural networks and graph attention networks to construct two different latent features of the same data. Then, it uses the same classification neural networks to produce the node classification outcomes. Finally, it uses the classification outcome to construct a so-called \"view loss\". In addition, it uses an incremental strategy to gradually included pseudo labels until some termination conditions are satisfied. \n\nOverall, the paper is easy to understood. However, I think the paper can be improved in each sections:\n[Introduction & Related work]\nThe authors can better organize their presentation on the development and understanding of Graph Neural Networks. At the current stage, these content does not seem to connect to the current development of GNN. \n\n[Model architecture]\n3.1\n1, can the authors explain the reason of using three-head representation? Also, why do the authors use the same non-linear graph convolution layers? Is it because the feature is different already? Can the authors specify the detail settings on this graph convolution layers? I did not find it in other places?\n\n3.2\n2, I can roughly understand the reason of introducing the contrastive learning and co-training. However, maybe the authors should put of the content in the related work part and emphasise the difference of view-consistent algorithm from these two methods. Plus, I did not find experiments that uses the data augmentation method (instead of the view-consistent method). I can see there is contrast learning comparison, however, the other settings of constrast learning may be different?\n3, the inclusion of pseudo labels are not well explained in this section. I was expecting to see more systematic analysis on this procedure, however, the current version can not fully convince me on this procedure. \n4, small issues: view 1 and viewer 1 are both used in the paper and they should be consistent. Eq. (5) and Eq. (6) can be well formatted to save more space for presentation. \n\n[Experiments]\n5, I am expecting to see the implementation code for at least the neural network specification in the main paper or supplementary material. However, they are missing. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A multi-view learning approach for inferring graph representation with incremental improvement in performance",
            "review": "This paper adopts a multi-view learning approach for graph representation learning where some labels are assumed to be available. It uses graph convolution network (GCN) and graph attention network (GAT) to create two different views of the same graph and then define a loss function to force the output due to the two views to be consistent. The low label rate scenario is considered and pseudo labels are created to define an additional loss function to better enforce consistency. Three datasets are used for performance evaluation.\n\nPros:\n- The problem addressed is an important one.\n- The problem formulation is a reasonable one.\n- The paper is clearly presented in general.\n\nCons:\n- The view-consistency idea is good but not particularly new. The two graph “views” are based on existing graph embedding methods. So, I consider the originality of this work is limited.\n- Only incremental improvement in performance is demonstrated in the experimental results.\n- The graphs being evaluated are not particularly large.\n\nComments:\n- In Eq 7, the two terms for supervised loss l_sup^(1) and l_sup^(2) are not clearly defined.\n\nQn:\n- In the conclusion, there is some discussion about how pseudo labels contribute. It will be interesting to see how crucial the pseudo label term contributes to the overall the performance. \n- How will the proposed method be compared with some contrastive learning in terms of performance?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper tries to address the limited training data problem by exploiting the consistency between different views of the graph data. However, important details and justification are missing.",
            "review": "This paper tries to address the limited training data problem by exploiting the consistency between different views of the graph data. However, important details and justification are missing. The major problems are:\n\n(1)\tThe proposed model lacks detailed explanation and justification. How do you generate the view for the graph from its features? How and why do you obtain three head representations using graph convolution or graph attention? Why using convolution and attention for the two views respectively? How do you do dropout? Why using the same graph convolution layer later for the two views? After seven lines of introduction of the model, the paper is focused on training, and leave all the above questions behind.\n(2)\tFor training, how do you judge ˋˋhigh confidence predictions’’ for generating pseudo-labels? What do you mean by ˋˋthe same prediction representations’’, ˋˋverification set’’ and ˋˋlabel rates’’? Moreover, with limited training data, how do you obtain ˋˋwell trained’’ two-view networks? What is the ˋˋstop condition’’, simply max epochs?\n(3)\tThe authors give some settings of parameters and say that the other parameters are specified in the appendix. However, after references there is no appendix.\n\nTo summarize, without justification, the proposed model is not convincing. Without details of the model and implementation, this work is difficult to reproduce.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "[Summary]\nIn this paper, a graph view-consistent learning framework (GVCLN) is proposed. Specifically, two view learners are used to give predictions for the input. Then, a consistency loss is employed to force the two viewers giving the same predictions. Moreover, a co-training scheme is proposed to alleviate the label sparsity problem.\n\n[Pros]\n+ This paper is easy to follow.\n\n[Cons]\n- The novelty of this work is limited. It seems that this work is a simple combination of [1] and [2], with slight modification. Also, the authors are suggested to include more baselines, especially augmentation-based methods, e.g., [3].\n- Three studied datasets are of small scales, which are well know to have unstable results.\n- It is not clear how to select high-confidence pseudo labels. More experiments, e.g., parameter sensitivity analysis wrt the confidence threshold, ablation studies of the two loss, are needed.\n\n[Evaluation]\nOverall this paper presents a simple yet effective framework to semi-supervised node classification. However, the novelty of this work is limited and more experiments are necessary.\n\n[Ref]\n[1] A Simple Framework for Contrastive Learning of Visual Representations, in ICML, 2020\n\n[2] Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes, in AAAI, 2020\n\n[3] NodeAug: Semi-Supervised Node Classification with Data Augmentation, in KDD, 2020\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}