{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting modification to DOA estimation using DNNs",
            "review": "This paper proposes an algorithm for predicting the direction of arrival (DOA) of multiple speakers in a given audio signal. The authors do this using relative transfer function (RTF) features computed from multiple microphones and a UNet. The UNet maps the input features to the DOA of the dominant speaker at each time-frequency bin; the DOAs are discretized and represented using a one-hot representation, reducing DOA estimation to a classification task.  The DOA of each speaker is then computed by summarizing the information across frequencies and, optionally, across time-frames if the speakers are stationary.\n\nThe authors mainly evaluate this method on test sets generated by convolving WSJ utterances with simulated and real room-impulse-responses (RIR). Results show that the method outperforms other traditional signal processing based approaches, and an alternative DNN based approach (CMS-DOA). The authors also do qualitative evaluations on real recordings of moving sound sources, and show that the presented algorithm does a better job compared to CMS-DOA.\n\nThe main novelty of the paper is that the authors estimate DOA at a finer-scale -- per time-frequency bin instead of per frame. This, followed by summarizing information per-frame  or per-utterance likely makes the model more robust to errors, and also makes it easier to track moving sound sources.\n\nAs the authors correctly note, the use of DNNs for DOA estimation is not new. The main algorithm that the authors compare with, CMS-DOA, is one such example. The differences with CMS-DOA are:\n- Different feature set, and training corpora / conditions.\n- Frame-level vs. time-frequency (TF) level estimation.\n- Use of features from multiple frames instead of a single frame (since the estimation is done simultaneously for multiple time frames).\n- CNN vs. UNet architecture. \n\nIn my opinion, these differences are interesting but the overall novelty is still limited. Also, from comparisons, it’s not clear what specific differences account for the improved performance.\n\nFrom Sec. 3, the authors directly use the models trained by the authors of CMS-DOA for comparison. This gives several unfair advantages to the proposed model:\n- CMS-DOA is trained using *white noise* as the target source and spatially uncorrelated Gaussian noise as the background. The authors train and test their algorithm using speech samples from WSJ.\n- CMS-DOA is *not* trained using multiple directional sound sources. The authors train and test their algorithm using multi-speaker speech samples from WSJ utterances.\n\nThe test settings that the authors choose are closely matched to their training setting. So it is not surprising that the algorithm performs significantly better. \n\nIf the claim is that per TF-level estimation followed by summarization works better, perhaps it will be worth retraining CMS-DOA using the same training set as the authors are using, and show that the presented algorithm works better.\n\n\nOther remarks:\nDoes the algorithm generalize to unseen microphone geometry? What about more than 2 speakers? What about different SIRs? And different room sizes?\n\nHow well does the algorithm work with background noise?\n\nHow is \\eps determined in Eq. 6? Is it tuned per test condition? \n\nTab. 2: “with added noise”: Does noise here refer to distance perturbation or background noise? Consider clarifying or rephrasing.\n\nFor the real-life test condition, what is the source of audio? Is it text from WSJ, to match training conditions?\n\nIt would be interesting to study how relative closeness of multiple sound sources affect DOA estimation accuracy.\n\nIs there a way to add quantitative results on a larger real dataset as opposed to just DOA tracks from 2 utterances?\n\nHow do the authors estimate DOAs when the input utterance has > 256 frames? \n\n\nTypo:\nSec. 1: high reverberation conditions  -> highly reverberant conditions\nSec. 2: utterance-based level -> utterance-level",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A DNN method for multi-speaker localisation by predicting DOA for each time-frequency bin",
            "review": "The paper presents a fully convolutional network based method for dynamic multi-speaker localisation. The key idea is to train the network to take instantaneous spatial features as the input and the direction of arrival for each time-frequency bin as the training target. During test, time-frequency level decisions are combined to frame-level ones. Experiments were conducted to validate the proposed method. \n\nThe paper aims at improving DNN-based approaches for direction of arrival (DOA) estimation by introducing high-resolution supervision, namely the training target is DOA for each time-frequency (TF) bin, in contrast with the commonly used frame level supervision/training target. This is a natural extension of existing methods. Furthermore, relative transfer function (RTF) is chosen as the feature. In experiments, the proposed method is compared favourably (with large margins) against two classical signal processing approaches (MUSIC and SRP-PHAT) and one deep learning approach called CMS-DOA. The data set used for the experiments is planned to be released upon the publication of this paper, which is a plus. \n\nRegarding CNN multi-speaker DOA (CMS-DOA), the paper refers to (Chakrabarty and Habets, 2019), which deals with MULTI-speaker DOA. However, at the same time/place, the paper uses a footnote to refer to a trained model available online, which is for SINGLE speaker localisation and the linked GitHub page also refers to (Chakrabarty and Habets, 2017), which deals with single-speaker DOA. A more fair comparison should be made with the multi-speaker one. This is to be clarified to avoid any confusion. \n\nFurthermore, for the key baseline method CMS-DOA, the model trained in the reference/GitHub is used. This raises a concern that there will likely be a mismatch between the CMS-DOA model and the test data used in this work; more precisely, the mismatch to the test data is higher for the CMS-DOA model than for the proposed method that uses authors' own data for training model. In this way, the experimental setting is in favour of the proposed method.  Less likely for the real-life dynamic scenario, of course, which is presented in the end of the experimental section.\n\nRTF is used as the feature. It would be interesting how it compares with other features used in the literature. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "DOA Estimation in TF Bins ",
            "review": "This paper describes a neural network based approach for DOA estimation in multi-talker scenarios. The paper proposes a U-Net based method where DOAs are estimated for each TF bin, which are then aggregated over all frequency bins for a given frame to estimate the DOA for a frame. The main idea is fairly simple and straightforward and is shown to give considerable improvement in DOA estimation in the experiments. It uses the well known Unet architecture, which have been used in several speech enhancement and source separation tasks. \n\n1. One of my major concerns with the paper is limited empirical evaluation. (a) Noise is not considered in the stimulated as well as real data. Noise can have considerable impact on the performance and as of now it is not clear to me how well the results will hold in noisy conditions. Ideally it would be interesting to see how the approach works out in different SNR conditions.  (b.) Experiments with just 1 array configuration (linear) (c. ) Number of speakers is limited to just 2. Overall, the current evaluation conditions seem fairly restricted and limited. \n\n2. Once the DOA is estimated for each TF bin, they are averaged overall frequency bin to obtain frame level estimation. Is averaging the best we can do ?\n\n3. How many audio samples were used for the results reported in Table 4 and 5 ? Are these results average MAE/Accuracy over multiple samples ? \n\n4. What features are used for CMS-DOA method. Can one use iRTF feature for CMS-DOA ? It seems like iRTF features are better than spectrograms and in that case it would be interesting to see how CMS-DOA work with iRTF. \n\n5. I am not convinced by the arguments provided for trend in performance for different distance and RT60(paragraph 1, page 7). It is still not clear why RT60=0.36s is working so well in 2m distance case. I would suggest the authors give a more thorough discussion on the trend here. \n\n6. It seems that small changes in MAE can lead to large change in Accuracy. I think it worth pointing out why that is the case and which one should we focus on to understand trends etc. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good, but missing insight",
            "review": "> Summary of paper:\n\nThis paper presents an approach that uses spectral features as an input to a neural net that predicts the DOAs for multiple speaker recordings.  The inputs are described in terms of multi-mic STFT ratios, and the neural net used is a U-Net.  The DOA estimation itself is seen as a classification problem (each 5° are a class), which simplifies the otherwise more difficult regression to an angle.\n\n> Strengths and weaknesses of paper\n\nOn the plus side: This paper is well written, the experiments are fairly well done, the results are good, and the outlined approach is sensible.\n\nOn the minus side: It isn't clear what the scientific contribution is. Using a known network structure on a known feature to perform a known process doesn't really provide much insight. I do feel that there is room for insight in this paper, but the authors stick to a very high-level description of their experiments.\n\n> Recommendation\n\nI would recommend rejection.  I think this is good work, but it doesn't convey to me anything that I didn't already know (other than this particular combination of feature and network seems to work well). I expect papers to teach me something, not just report a finding without any analysis or explanation. As is, the paper is really not much more than what is simply contained in table 1. This paper could have in it a lot more information and analysis to make it a much stronger submission, but the current approach is simply a report on a single point estimate. Yes it seems to work well, but I don't believe that just that is the standard for a strong publication.\n\n> Questions for clarifications:\n\n• A hugely influential factor in multi-source localization is how one defines a peak.  This is not described adequately in this paper and I think needs some clarification. I understand that for each time frame we obtain a location posterior. But it isn't clear how to a) Find out how many speakers are active, and b) finding out the peaks that correspond to each speaker. If we have e.g. two speakers and six substantial nonzero posterior values, how does one pick the two that correspond to a speaker?  We cannot use the two loudest ones, they might be both from the same speaker.  What if the amplitude difference between the speakers is large?  This is not a question that can be brushed away so easily and should be directly addressed.\n\n• It would be interesting to see something akin to a confusion matrix. Arrays are not equally good at detecting all directions and understanding how this algorithm behaves with respect to classic array behavior would be an interesting bit of information.\n\n> Provide additional feedback with the aim to improve the paper\n\n• I wouldn't use the terms \"spectral and phase\".  The distinction should be \"magnitude and phase\"; \"spectral\" usually implies the whole complex-valued output of the DFT.  But that entire sentence is a little out of place, since you claim that phase is more useful than magnitudes, and then you use the complex data directly.  I would simply state which features you use and leave it at that. The extra discussion there is an unnecessary distraction.\n\n• \"since it is known to encapsulate the spatial fingerprint for each sound source\", I assume when you say source you imply a spatial location.  I believe \"source\" tends to be usually interpreted as the actual sound (hence \"source separation\"), which is independent of the location.\n\n• Your VAD description is puzzling.  What is stated in the paper simply discards any TF bins that have a magnitude of less than epsilon. If this is what you do I wouldn't call it a VAD, you are simply discarding TF bins with zero magnitude that will result in a division by zero. A VAD is supposed to look for the presence of speech (not just energy), and is also very unlikely to be defined over frequency, it's usually only over time.\n\n• I understand that for the sake of conforming to current APIs, complex numbers are being avoided here. But, for something on paper, it strikes me as quite strange to stack real and imaginary parts when a complex-valued representation is clearly much easier to describe and manipulate. I can predict your rebuttal to this point, but papers are supposed to set forth the science from which we will write code, and not directly describe the code.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}