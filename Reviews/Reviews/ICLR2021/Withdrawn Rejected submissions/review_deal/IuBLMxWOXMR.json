{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper presents the first unsupervised model for learning-based scene depth prediction from a single or a stack of defocus input images. ",
            "review": "This paper presents using content consistency between two correlated networks' output, ie, DefocusNet and FocusNet, to achieve the unsupervised learning for depth prediction. The idea is good since it can waive the annotation burden. And the paper is well-written as well as containing a good amount of experiments supporting its claim. One concern is on the \"easiness to collect the training pairs (focus and defocus maps)\" for model training to have this model achieve practical significance? The other question is on the stability and quality of predicted depth map from the DefocusNet given different defocus images? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Leaning to reject",
            "review": "The paper describes a method to learn a pair of networks to estimate depth: one which takes a single defocused image, and another which takes a focal stack (a series of images obtained by focusing at different distances). These networks are trained jointly, on a training set that only has an in-focus and defocused image as input. This is done by artificially creating a focal stack using the output of the defocus network to serve as input to the focus network, and various consistency losses between the depth maps and synhetically rendered defocus image.\n\n### Strengths\n\n- It is interesting to consider the case where monocular depth estimation (in this case, from defocused images) is learned using a pair of all-focus and defocused images, without an explicit depth map.\n\n### Concerns\n\n- The paper misses discussion of (and any comparison to) an extremely related paper (it discusses an older paper by the same first author, but not the one below):\n\nSrinivasan et al., Aperture Supervision for Monocular Depth Estimation, CVPR 2018.\n\nThe paper above is based on essentially the same idea in terms of supervision: train monocular networks with supervision only from pairs of images, one being all-focus and the other defocused (\"shallow depth of field\"). They however use this to train a network that takes an in-focus image as input, not the defocused (shallow depth of field) one.\n\n- The motivation for training the defocus network jointly with a focus network is not at all  clear. If one cared only about training a single (defocused) image-input network, one could take the approach of Srinivasan et al., 2018 above---which is to just use the all focus input + predicted depth map to create the defocused image. Why train this jointly with the focal stack depth estimation network? What additional information or supervision is the depth consistency loss between the two network outputs adding, given that the input for the focusnet is being generated synthetically from the output of the focus network?\n\n- One possible advantage of the method is that the framework also yields a focus network to use after training. But note that getting depth from a focal stack is a _far_ more well-posed and simpler problem than depth from defocus, simply because there is a lot more information in the inputs. But this focus network couldn't simply be trained after the defocus network is trained. There are no experiments or ablations indicating the benefits of joint training.\n\n- The experimental results do not show any comparisons to alternative methods for depth from focus (i.e., from a focal stack). All competing methods in Table 1 only take a single image as input. The paper should at the absolute very least compare to traditional methods for depth from defocus (essentially, those based on selecting the image for every pixel where that pixel has the highest gradient energy, and setting the corresponding focal distance as the depth, with additional smoothing to handle smooth regions).\n\n- The evaluation of the defocus network is also not substantial. Most methods compared to in Table 1 appear to be just simple monocular depth estimation methods (not targeted at depth from defocus per-se) that were fine-tuned. Unfortunately, the paper doesn't make this very clear in the description of experiments---at the top of Page 7, the paper says \"we compare with other recent methods on estimating depth map of a single defocus image\", but the methods were all mainly designed for standard monocular estimation, and then fine-tuned on defocus input data). It should be explicitly clarified from the start that these are all monocular networks trained with direct supervision, and then fine-tuned on defocus image inputs.\n\n- The paper should compare to deep learning-based approaches for depth from defocus, e.g.,\n\n Anwar et al., \"Depth Estimation and Blur Removal from a Single Out-of-focus Image\", BMVC 2017.\n\n It should also compare to a baseline where its own network architecture (for defocusnet) is trained with direct depth supervision. And it should compare to the Srinivasan et al. paper above (which reports results on the DSLR dataset).\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Hope the author provide the response to my concerns",
            "review": "1.    Summary\n\nThis paper proposes neural networks, called FocusNet and DefocusNet to estimate a depth map and all-in-focus image. Two networks do the role of depth-from-focus and depth-from-defocus and these networks are simultaneously trained in an unsupervised manner by constraining the depth map consistency and focal stack image consistency.\n\n2. Strength\n\na. This paper first proposes the unsupervised depth from focus and defocus algorithms and shows the state-of-the-art performance.\nb. For the diverse experiment, this paper captures the real-world defocused images and show the quantitative comparison to the conventional works.\n\n\n3. Weakness\n\na. I cannot get the practical utility of unsupervised Depth-from-Focus and Depth-from-Defocus algorithm. The author also trains the network with the synthesized blurred images from NYU dataset containing ground truth depth. As the author also synthetically generates the blurred images using a depth map, the depth map is simply utilized for the supervision of the training step. From the academic perspective, however, I see this paper’s contribution that first attempts to solve the depth from focus/defocus problem. So, I do not consider the weakness described in (a) as a weakness of this paper and have not reflected this factor on my rating of this paper. The followings are my major concerns.\n\nb. I am curious about the training schedule in detail. Both FocusNet and DefousNet are trained simultaneously from scratch? \n\nc. The comparison to the non-learning-based Depth-from-Defocus algorithms [1] and Depth-from-Focus [2,3] etc also should be reported. \n\nd. How much the performance drop exist as the proposed method is trained in an unsupervised manner?\n\ne. The details of the blur size is missing on the depth map evaluations on table 1 and table 2. The depth estimation performance will be highly dependent on the degree of blur, so the evaluation must be conducted with various blur images. It is hard to exactly judge how better the proposed method rather than the competitive methods. This evaluation can be simply conducted on synthetic datasets since the author can generate as many blurred images as they want. For the real-world images, the datasets can be captured with various camera aperture.\n\nf. It seems the thin lens model described in section 3.1., is never used in the proposed method described in the rest of the part, except for the synthetic blurred image generation. The model explanation is essential to present the proposed FocusNet and DefocusNet? There is nothing new in the model description compared to prior works [4] etc. The author can just state the blur model by referring to the paper [4]. \n\ng. The overall system does not seem novel enough, the contribution of this paper seems slightly marginal.\n\n[1] Zhuo, Shaojie, and Terence Sim. \"Defocus map estimation from a single image.\" Pattern Recognition 44.9 (2011): 1852-1858.\n[2] Suwajanakorn, Supasorn, Carlos Hernandez, and Steven M. Seitz. \"Depth from focus with your mobile phone.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n[3] Surh, Jaeheung, et al. \"Noise robust depth from focus using a ring difference filter.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n[4] Carvalho, Marcela, et al. \"Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n======After rebuttal======\nI keep my rating.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "novel pipeline but method requires more justification",
            "review": "**Summary**\nThis paper proposes an end-to-end learning-based method to estimate depth from an image with defocus. The authors propose a way to train the model in an unsupervised manner without requiring ground truth defocus or depth map to train the models. This is achieved by having two networks, one takes in a single image with defocus and estimates the defocus map, which is used for the second network that takes in synthesized focal stack and estimate a defocus map. The outputs from the two models are supervised to be consistent.\n\n**Strength**\nThe paper addresses a challenging problem with a novel training procedure that enforces consistency. The idea of using estimated defocus map to synthesize a focal stack, which is then used to estimate defocus 'again', is interesting. The paper also effectively leverages several datasets that were not used for depth from defocus to build the training dataset. Promising results such as one shown in fig 8 shows its robustness in estimating depth when varying the focusing distance.\n\n**Weaknesses**\nThough the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:\n    \n    - the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training? \n    \n    - related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?\n    \n    - According to Held et al \"Using Blur to Affect Perceived Distance and Size\", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?\n    \n    - the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?\n    \n    - in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?\n    \n    - the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?\n    \n    - fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?\n   \n    - what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?\n\n**Minor suggestions**\n\n    - figure text should be rasterized, and figures should maintain its aspect ratio.\n    \n    - figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.\n    \n    - the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.\n    \n    - the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.\n\n    - calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}