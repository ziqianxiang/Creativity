{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper propose a method to explain the contextualization of BERT by identifying a set of influence paths from the input to the output.  Although all reviewers give overall score 6, their comments are pointing to the negative direction.  The following excerpts summarize the general sentiment of the reviews:\nR2: Overall, I incline toward rejecting. This paper provides an instrument to explain BERT, but I have a hard time understanding the result itself (influence paths or patterns). I also have a major concern with the final analysis and its findings.  \nR4: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score.\nR1: Overall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication.\nR3: It looks like the proposed method is largely based on (Lu et al, 2020); the major difference is the introduction of Multi-partite patterns, which basically expands the objects of influence analysis from paths to patterns (partial paths). This doesn't look like a significant novelty.\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "### Summary:\n\nThis paper addresses the problem of how BERT contextualizes information. Specifically, this paper tackles a computational limitation of influence paths used to analyze LSTM models by introducing multi-partite patterns and guided pattern refinement (GPR). A multi-partite pattern, $\\pi$, defines a set of paths that follow a sequence of edges. The more edges it has in the sequence, the fewer paths it defines. Then, GPR is a greedy algorithm that iteratively adds an edge to $\\pi$ such that the influence of the pattern is maximized. For each iteration, the search is constrained to a set of edges within a single layer of BERT. As a result, an influence path is constructed (or rather approximated). The author validates the proposed method using concentration and model compression experiments for multiple agreement tasks. The concentration results show that influence paths discover by GPR have a very high influence. This is true for both embedding-level paths and attention-level paths. For model compression, the results show that most of the accuracy is maintained when retaining the identified influence paths and \"churning\" the others. Finally, the author explains how BERT contextualizes input words by visualizing the influence paths. The author discusses the four examples of the SVA across Obj task. The influence of the subject and the intervening noun is similar to that of LSTM (Lu et al., 2020). Insights from the visualization of the influence paths are discussed such as \"copy and transfer\" and a conflicting role of \"that\" (which might explain the common mistake of BERT).\n\n\n### Recommendation:\nOverall, I incline toward rejecting. This paper provides an instrument to explain BERT, but I have a hard time understanding the result itself (influence paths or patterns). I also have a major concern with the final analysis and its findings. \n\n#### Pros:\n- Novel method to compute influence paths that can help us interpret how BERT utilizes input words.\n- The quantitative experiments conducted on many agreement tasks validate the proposed methods.\n- Interesting findings that BERT likes to \"copy\".\n\n#### Cons:\n- Interpreting the influence paths to understand how BERT works is quite difficult and arguably subjective. It is also confusing why Figure 1 is so different from Figure 3. \n- Although the result influence paths include the skip connections, it is not quite clear how it is compared to the attention approach in the other aspects. For example, the author could use the model compression experiment on the high attention weight embedding (or hidden representation).\n- It is unclear how the main finding of section 4.2 came to be. For example, how many sentences that the author observed before concluding that \"copy and transfer\" are common. The speculation of \"that\" influence paths can somewhat explain the result discrepancies of SS and PS. Are there other words that behave similarly (such as \"who\")? I think the author should provide a more rigorous analysis on the interpretability of the proposed method, as well as an explanation of BERT contextualization.\n- It is quite common to study how the model works through this kind of task. But, most of the time, we BERT of encoding sentences without [MASK] word. I am not sure how this finding will generalize to other tasks.\n- This paper is very hard to read. The math expressions are clear, but the explanations and intuitions are often confusing.\n\n\n### Questions:\n\n1. I am not quite sure that \"churning\" means. Could you please provide more detail?\n1. What are the numbers on the \"Significant patterns\" figures?\n1. I am a bit confused with Figures 1 and 3. In Figure 1, there is a path from 1 word to [MASK], but in Figure 3 (and others), there are multiple paths. Could you explain why is there a difference?\n1. Would you have the same result using Lu et al., average embedding instead of [MASK] embedding as a base embedding?\n\n### Minor comments:\n- I think Section 4.2 onward is a bit rushed. There are many typos, such as \"... connections.  . We speculate ...\" or \"we how BERT\".\n- Definition 5, typo on \"$n_w,n_w$\"?\n- So much useful figures and tables are in Appendices.\n- I am not sure the point of the convergence analysis of the BERT base vs small. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Exciting work, I would appreciate a bit more extensive analysis and results section",
            "review": "*Summary*\n\nThis paper investigates how BERT uses attention to contextualise information. To find an answer to this question, the authors use abstractions of sets of paths through a neural network model, to quantify and localise the effect of specific inputs to the output. They describe these paths -- multi-partite patterns, as well as a guided pattern refinement that can be used to discover influential paths. To showcase their proposed techniques, the authors consider subject-verb agreement and reflexive anaphora in BERT.\n\n*Review*\n\nThe proposed method is exciting: investigating the flow of information information in neural models is a promising research direction that is receiving more and more attention. The authors spend sufficient time explaining several aspects of their method, although I find it at times a bit confusing to understand why some information goes in the background section and some instead in the section \"path abstraction\". \n\nIn terms of the result section, I think the paper could be improved. It is not trivial to parse the numbers in Table 1 and 2, and not much explanation is given. The paper furthermore only reports results on one of the tasks given in the abstract, all the rest of the results are in the appendix. I am missing also some explicit comparisons with previous studies with similar aims (see appendix). In general, I find that a bit too much of information that is relevant to understand the story is put in the appendix.\n\n*Questions for the authors:*\n- Much work has been done on investigating how LSTM language models model subject verb agreement and anaphora resolution, and in the literature also  several comparisons between BERT and LSTM models can be found. To validate your technique, perhaps it could be interesting to run it also for LSTM models, for which it is easier to establish if the results make sense? _**Update after author response:** I understand that space might not permit running this also for an LSTM model, and that there are substantial differences between LSTM models and BERT, in terms of depth. Still, I would appreciate to have some baseline results, which would help to better assess the contribution. In my opion, LSTM models, for which much more is already known about the paths through the network, would be the easiest way to go._\n- I think your work is very interesting, but I am really struggling to understand the results. Could you elaborate a bit more, and provide a bit more structured explanation and interpretation? _**Update after author response**: I appreciate that you took into consideration my request to add a bit more explanation. I still find the structure a bit difficult to understand, but it has definitely improved since the last version_\n\n*A few notes and suggestions*\n_**Update after author response:** Thanks for incorporating most of my suggestions_\n- Figure 1 comes a bit early for me, at the point where it is referenced I cannot really parse it yet, because much of the info to understand follows only later \n- The font size of the legend in Figure 1 is very small, could you increase it?\n- You generate your own dataset with NA-tasks with fixed patterns, this was previously also done by Lakretz et al 2018 -- The emergence of number and syntax units in LSTM language models -- a dataset which was used by several others afterwards. For comparability with others that have investigated this phenomenon in BERT and other models, it might be nice to run some experiments using that dataset.\n- A very relevant reference is Jumelet et al (2020) -- Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment -- who use contextual decomposition to find how information flows in a recurrent LM, looking also at SVA and RA. In a more recent paper Jumelet (2020) repeated this analysis also for BERT.  It would be nice to see some comparisons, and also some contextualisation on how the method here relates to their work, and to contextual decomposition (Murdoch et al 2018 -- Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs) in general.\n- In related work, I am missing Hupkes et al 2018 (Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure) as a reference for diagnostic classification\n- bibliography should be checked, there are quite a few arxiv references that are actually published papers\n\n_**Update after author response**: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score._",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially interesting, but not clear what the question is, why this method, or what answers we get",
            "review": "This paper analyzes functioning of BERT by identifying gradient-based influence paths to track flow of influence between model inputs and outputs. Such analysis using influence paths has been established in prior work, but the present paper expands on this work with definition of \"multi-partite\" patterns, and by introducing a method for identifying strongly influential paths in the model. The authors evaluate on existing datasets for studying subject-verb agreement and reflexive anaphora, and they report measures of concentration of the influence flows, and performance of the model after compression based on the identified influence paths.\n\nOverall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. \n\nThe reported results don't give us much to go on in terms of interpreting what we have learned from these analyses. The authors choose two measures: \"concentration\" of the influence paths, and drop in accuracy when compressing the model based on those paths. I'm not sure what we should be taking away from the concentration numbers, and the authors don't provide any substantive discussion of why this matters. There is also no meaningful baseline against which to compare these numbers. The second measure, accuracy of the compressed model, is a bit easier to interpret, in that it allows us to verify the extent to which critical information flow is indeed occurring within the identified paths, but while this verification serves to increase confidence in the method, it's not clear what it tells us about the functioning of the BERT model. (The authors also do not give a clear statement of what task these accuracies are in fact for, though I assume by default that they are referring to accuracy in assigning higher probability to the grammatical option over the ungrammatical option in the selected Marvin & Linzen datasets.) There is also no discussion of how we should interpret results from the attention-based versus the embedding-based influence paths.\n\nZooming out further, there isn't a clearly identifiable question being asked in this paper, and in particular there is no clear connection between the analysis method being used and the particular linguistic phenomena embodied by the chosen evaluation data. What question is being asked about the model's handling of these linguistic phenomena, and why is this method appropriate for asking it? What output of the analysis will be used as an answer to the key questions? These connections should be defined clearly and from the start. As it is, I'm unclear on what these results tell us about the chosen linguistic phenomena (or, conversely, why this set of phenomena was chosen to showcase utility of this method). The measures reported have some variation between sentence types -- but what does it mean that there is higher positive embedding influence concentration for sentences with subject relative clauses, or higher negative embedding influence concentration for within sentence complement sentences? What is the meaning of the accuracies dropping more for subject relative clauses and number agreement among the embedding influence paths, and all accuracies seemingly dropping for the attention influence paths?  \n\nIn Section 4.2 there are a couple of observations about the influence paths that make some concrete connections to potential questions about syntactic dependencies in the chosen datasets. However, because no formal connection between these things has been established prior to this point (no clear question, no clear linking hypothesis between the analysis and the phenomena) this set of observations comes a bit out of the blue, and leaves us still without clear takeaways. In sum, I think that there is potential for interesting insights about these phenomena to come out of this analysis method, but the paper would benefit from much more clearly defined questions and linking assumptions.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising explanation method; novelty is a bit limited and some parts are a bit confusing",
            "review": "#### Summary\n\nThis paper proposes a new explanation method of DNNs by applying Distributional Influence analysis on the proposed Multi-partite patterns. A Multi-partite pattern $\\pi$ is basically a partial path; it \"abstracts\" a set of paths $\\gamma(\\pi)$ which consists of the paths that include all edges in the pattern. The influence of $\\pi$ is thus defined as the sum of path influences among $\\gamma(\\pi)$. The authors also propose Guided Pattern Refinement (GPR) algorithm to find the most influential patterns for a certain input word. It is a greedy algorithm that adds an edge per step to maximize the current pattern influence.\n\nAlthough this method is in principle applicable to any DNNs, this paper is mostly focused on BERT. Experiments are done to evaluate and showcase the usefulness of the proposed method. The experiment results on Concentration (of influence) and Model Compression illustrated that the extracted patterns from BERT indeed capture our quantity of interest (QoI). The authors also utilize the extracted patterns to shed more light on BERT behaviors.\n\n\n#### Strength\n\n- This paper is targeted at BERT explanation, which is very important and valuable. Model explanation is gaining more and more attention, especially for BERT, given its dominant popularity and yet hardness in interpretations.\n- The proposed explanation method is reasonable and it proved useful in the experiments. The Concentration and Compression results are promising (except that comparisons to other methods are not provided, which is a weakness). Also, the behaviors discovered and discussed are also interesting.\n\n\n#### Weakness\n\n- It looks like the proposed method is largely based on (Lu et al, 2020); the major difference is the introduction of Multi-partite patterns, which basically expands the objects of influence analysis from paths to patterns (partial paths). This doesn't look like a significant novelty.\n- Several parts of the paper are very confusing to me, especially the background section (maybe because my primary research area is not model interpretation). I tried to read several references but still have many questions unsolved. I will put specific questions in the next section.\n\n\n#### Questions & Suggestions\n- Confusions\n\t- Section 2, Distribution Influence (paragraph): \"Influence can quantify a concept ...\" might be misleading. Influence is not quantifying a concept but quantifying the impact of an input concept (subject S/P) on an output concept (verb S/P), as stated in Definition 1.\n\t- Section 2, Definition 1: \"... a DoI D(x) parameterized by an input, ...\" Here \"parameterized by an input\" confused me a lot when I first read it. After checking references and the experiments, I guess you are referring to a distribution like the uniform distribution from original embedding to [MASK] embedding, where the original embedding is the \"parameter\". However, this sense is somewhat unusual... I always thought about \"parameterize\" like $\\mu,\\theta$ in Gaussian distributions and thus cannot understand. Also, I think the word \"input\" here is ambiguous: you have an \"analysis input\", which is $x$; you also have \"model input\", which are samples from $D(x)$. It might be better to explicitly distinguish them.\n\t- Section 2, Line 22-23: \"... in our use cases DoI are distributed along a one dimensional path $c:[0,1]\\to R^d$.\" This does not look valid mathematically; how can a distribution be over a function? I guess what you mean is that DoI are distributed over $c=\\\\{x_m+\\alpha(x-x_m),\\alpha\\in[0,1]\\\\}$.\n\t- Section 3, Guided Pattern Refinement(GPR): \"One could repeat the procedure until an individual path is produced though for reasons of interpretability, it might be preferable to reason about patterns instead of single paths.\" How are you actually extracting patterns? As I see in Figure 1 and 3, you are illustrating paths. Do you mean \"repeat the procedure until an individual path, but (mentally) reason about patterns in the path\"? It looks like so according to your experiments discussed in Table 1.\n\n- Others\n\t- Section 4: (a minor one) It is kind of inconvenient for readers that you frequently refer to Figure 1 in your discussion, which is many pages away. Would it be better if you move or copy Figure 1 and put it together with Figure 3?\n\n\n#### Typos\n- Figure 1, Line 1: \"GRP\" -> \"GPR\"\n- Page 3, Line 2: extra space\n- Page 4, Def 4: in the middle term, $x$ -> $z$\n- Page 4, Def 5, Line 4: $n_w,n_w$ -> $n_w,n_q$\n- Page 7, Line 2: \"retain a even\", \"a\" -> \"an\"\n- Page 7, Line -9: \"In ,\" -> (In all?)\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}