{
    "Decision": "",
    "Reviews": [
        {
            "title": "The review of carbon footprint of federated learning",
            "review": "This article mainly proposes an analysis of Federated Learning carbon footprint. For different federated learning design and carbon emissions, the author conducted carbon verification on different data sets. In response to optimizing the rate of CO2 emissions jointly to the accuracy of neural networks, the author proposes federated learning and algorithms efficiency to reduce its environmental impact.\n\nThe paper’s key strengths:\n\n1.\tI think the directions presented in this article are very novel and meaningful for the development of deep learning and machine learning. From the perspective of model accuracy, you may have more data and more machines to get better classification and regression results. But from the perspective of CO2, it will cause more pollution, which is an environmental problem that needs to be solved urgently.\n2.\tThis article proposes a formalization that integrates carbon emissions into the common neural network optimization process to lower the final Carbon Cost. I think this can be used to quantify the CO2 produced by the model, which is of great significance to the future development of AI and deep learning. It is very important to maintain the accuracy of the model while balancing CO2 emissions.\n\n\nThe paper’s key weaknesses:\n\n1.\tThe biggest problem of this article is that the method part lacks theoretical guarantee. The author proposes energy consumption and CO2 conversion methods based on federated learning mode. However, the author lacks the feasibility of such calculations and theoretical guarantees. I hope the author can make some supplements here.\n\n2.\tIn the experimental part, the author mainly analyzes the analysis and comparison chart of carbon emissions of NVIDIA GPU or other TPUs. However, the author mainly used Image net and CIFAR10 to verify the tradeoff of the model and carbon emissions, especially in Table 5, the author listed the accuracy of the model under different carbon emissions. But I think this is not generalizable, because there are many different models in the real world, such as embedding, xgboost or resnet, so I still hope that the author has some theoretical work to ensure the accuracy of different models and the tradeoff of carbon emissions.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good initial work in an area that deserves further investigation.",
            "review": "Summary:\n--------------\nThe paper studies the energy consumed/CO2 emitted from training of deep learning models and compares centralised versus federated learning approaches. An early optimisation formulation is presented to reduce the carbon footprint of federated learning approaches. The results are interesting, although not too surprising. \nSimilar to the sigcomm 2009 paper (https://www.akamai.com/us/en/multimedia/documents/technical-publication/cutting-the-electric-bill-for-internet-scale-systems-technical-publication.pdf), which leverages geographical diversity to reduce energy costs of CDNs, this work seems to have the potential to make a good impact in the area of energy efficient federated learning. \n\n+ves: \n-------\n1) A simple but valid model based on solid assumptions on energy parameters (energy costs of GPUs/processors, PUE, etc. ) to compare carbon emissions of federated Vs centralised training. \n2) Good preliminary experiments based on NVIDIA/Tesla GPUs that compare the two types of training to get a view on how expensive the federated learning approaches are in comparison to centralised training. \n3) Good early optimisation formulation to reduce carbon footprint of federated learning approaches. \n\n-ves/missing: \n------------------\n\n1) In case of centralised training there is a cost associated with transfer and maintenance the data at a central location. On the other hand, in federated learning, there is a cost of transmitting partial models. In order to make a fair comparison of CO2 footprint it will be good to discuss/account for these costs as well. \n\n2) This reviewer would have liked to see some early results/benefits of the optimisation formulation. How does leveraging nodes located in low PUE geographical locations more than the ones in high PUE locations result in a tradeoff of accuracy / CO2 reduction (possibly by making assumptions of data redundancy across nodes). Is it possible to FL train a model that has a slightly lower performance but possibly meets business expectations, but has drastically lower CO2 emissions - any use cases/experiments that illustrate this point clearly. \n\n3) Another related line of investigation worth discussing is the area of reducing energy costs, not necessarily carbon foot print. For example, would it make sense to utilise a node at a later stage in federated learning when the energy costs at its location are lower versus earlier in the rounds. Does this impact model convergence or reduce overall costs or increase carbon foot print. These aspects are unknown. \n\n4) Although the authors have conducted a good set of experiments, the value of this work will improve if measurements arre taken from a larger/realistic testbed. For example, by considering measurements over time even from a small university datacenter where energy costs are compared when models are being trained and when they are not. Such realistic experiments will make this work much stronger. \n\n5) Its unclear if anything would change if the modality changes from images to text or tabular data. So if I am a business relying on FL training for image, tabular, and text data and an NGO complains about my carbon footprint, which of these 3 should I target to optimise?\n\nConclusion: \n----------------\nOverall, this is a nice piece of work and a good direction to pursue. However in order to vow your readers with more insights, I think there is potential to investigate this space more thoroughly. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"A first look into the carbon footprint of federated learning\"",
            "review": "This paper proposes to present two contributions to the field of federated learning:\n\n1. A proper estimate of the CO2 emissions for federated learning, and\n2. A way to include CO2 costs in the decisions made regarding optimization hyperparameters.\n\nBoth of these goals are laudable. It is important to understand the climate impact of the work we do; strategies to mitigate such costs would be quite useful.\n\nThings that potentially make federated learning more carbon costly than centralized learning are: communications overhead, computation on edge-devices that is less power-efficient, training inefficiencies due to not being able to run fully centralized training epochs (data disparity, and non-centralized, parallel updating of the model parameters results in different convergence behaviour in FL compared to centralized training).\n\nBut, there are a few tunable hyperparameters to a federated learning setup: number of clients used, the time alloted for each round of training, and the number of local epochs. There is perhaps a sweet-spot where the training efficiency (including communications overhead) results in a low amount of carbon cost for a given performance (in terms of accuracy) threshold.\n\n----------------------\n\nThis paper I think nicely lays out the variables that are relevant for determining the carbon cost of a federated learning system: number of clients, time spent on each client, energy-rate on each client, and communications overhead. This is all of course multiplied the region-specific carbon cost per kWh.\n\nThe authors estimate the carbon costs of FL by using NVIDIA Tegra X2 devices and assume a 10W power consumption. They train two models to pre-specified accuracy targets and record how much compute time was used. They include an estimated power consumption of communication. This is how they come up with their total kWH for the federated learning. (Again, this can be multiplied by region-specific carbon carbon costs per kWh.)\n\nThis is all pretty straightforward and useful for us to know about. This would be a helpful contribution to the literature.\n\nHowever, I have significant doubts about the absolute values and relative comparison to centralized learning and across various setups for FL (largely presented in Table 1). We can ignore the rows, since those are just multiplying kWh by a different country-specific carbon cost. As far as I can tell, are not actually additional runs of the algorithm. What matters in this table for the purposes of this paper is the relative values between the columns. It isn't clear in this paper how many times this training was run to obtain this observed behaviour. Due to the non-centralized data, convergence behaviour of the FL approach could be quite dependent on initial conditions and training distribution. What if these were particularly quick (or slow) convergences compared to typical?\n\n-----------------------\n\nThe second claim of this paper is that it presents a joint-optimization formulation for pollution and learning objectives.\n\nI do not believe it presents such a formulation. It seems to demonstrate experimentally that there is a tradeoff between choices of hyperparameters and CO2 cost required to achieve certain performance threshold (see Figure 1). (Although, this could also just be variation between training runs. Looking at some of the more detailed data in Table 5 also makes me wonder whether there is significant random variation between training runs for identical setups.) But, even taking that at face value, this demonstrated tradeoff is not put together into a formulation that one can use to jointly optimize these hyperparameters and model weights.\n\nI may have misunderstood equation (4) and its incorporation into equation (7), but it seems to me that equations (4) and (7) could be minimized by setting either r, n, or t to zero. Minimum carbon cost would come by simply not training. Sure, the model accuracy would be very poor, but that doesn't matter when the numerator of (7) is zero. This seems to be missing a constraint on G(w). There is nothing in equation (7) that penalizes a very poorly performing model after training for zero seconds.\n\nI also do not see how Figure 2 supports the claim at the end of section 4: \"the linear growth of CO2 with respect to the training time could be turned into a non-linear function with a lower delta (or gradient) by simply dynamically adapting the number of clients selected in regard to the CO2 released at prior rounds.\" Does this envision all the training hyperparameters being fixed except for number of clients chosen per round? What would trigger the FL algorithm to select fewer or more clients during a subsequent round? And how would this predictably affect the overall carbon cost until achieving the desired performance threshold (i.e. choosing fewer clients might result in cheaper rounds, but more rounds may be required).\n\n--------------------\n\nI am sorry to recommend rejection. I think this paper fails to pay off what it promises at the outset. The empirical/estimated carbon costs of FL compared to centralized learning have not been demonstrated to be robust across training runs. Second, I don't see how it presents a formulation to allow optimization of carbon costs within federated learning optimization. See above for how I came to these viewpoints.\n\n--------------------\n\nQ: What is happening in equation (7)? Can you explain how I am misunderstanding how this would be minimized?\n\nQ: How many times have you run each training scenario? What convinces you that you aren't observing expected variation between training runs?\n\nQ: How do you envision dynamic adapation of number of clients to work in practice?\n\nQ: Very interesting that in Table 1, under IID training data, using 1 local epoch per round reaches 60% accuracy faster (or at least at much lower carbon cost) than 5 local epochs per round, but this relationship flips when using non-IID data. Does this reflect previously known FL behaviour?",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A promising direction, but insufficient/incomplete",
            "review": "Summary: this paper presents what seems to be the first analysis of the carbon footprint of federated learning. The authors present a formalization of the task of estimating the carbon footprint of FL, conduct experiments on realistic hardware for centralized and federated training, and use the results to compare the carbon footprints of the two training regimes.\n\nIt is encouraging to see research in this direction. However, I find that there are too many simplifying assumptions and estimates made in the experimental design for the work to stand as anything but a preliminary investigation. It certainly does not close the book on the climate impact of FL, or provide reliable empirical insight into the impact of large-scale federated training in the wild. This is not simply due to lack of access to real-world data, hardware, or infrastructure: there are modifications to the experimental design, formal analysis, and discussion which could significantly increase the paper's contribution. One of the key contributions, a joint optimization framework, is only trivially investigated.\n\n#### Major comments:\n\n* Due to the complexity of FL, this paper is forced to estimate many different parameters, any of which could have a considerable impact on overall carbon cost. These include: geographic distribution of users; size of clients sampled at each round (n); device hardware for clients and its power consumption; size of model and energy cost of transmitting gradient updates; local CO2 emissions in client regions. While I agree with some of the choices the authors make, others simply don't make sense to me, and corrupt the analysis: for example, using only France, USA and China as representative of the user population and local CO2 emission rates (What about India, a populous and carbon-emitting country with many smartphone users? Why France, which ranks 22 in global population and has very clean energy? Is there any data regarding smart phone usage to justify this choice?). Similarly, the choice of only maximum of 10 clients per round of FL is at least three orders of magnitude smaller than anything which would match real-world use (it would still be possible to simulate having more clients, with the limited hardware available to the authors). To me, these choices alone make the experimental results unreliable and too far from any realistic benchmark for acceptance or for evaluation of other FL deployments.\n\n* The authors propose an optimization-based approach to the problem in Sec 4.3, and list this as a main contribution in Sec 1, but they do not actually even attempt to solve it. Instead, they conduct a basic grid search, varying only the number of clients, from 1 to 10 (an unrealistic and uninformative range; better to use a log scale of 10^{1, 2, 3, 4}). The equality used for their formulation in Equation (5) is not correct. Maximization is equivalent to minimizing the negative, not minimizing the reciprocal.\n\n* It would be useful if the authors could somehow account for potential variability in their estimates (for example, of geographic distribution of users, or of the hardware available to users) and show the impact of these changes on their estimated carbon budget. Since, as the authors acknowledge, they seem to have no reliable information on this, a sensitivity analysis would be useful.\n\n#### Minor comments:\n\n* The authors' claim that \"this is the first time that ImageNet-scale experiments are performed using FL\" is dubious. Perhaps this is the first time ImageNet itself has been used for FL, I cannot verify, but it is certainly not the first large-scale image experiment with FL. Please clarify this claim or remove it.\n\n* Many other tunable hyperparameters are ignored in the experimental results (batch size, learning rate, number of local epochs, regularization, model size); I would be curious to know the impact of these, in particular on obtaining a more carbon-efficient model with equivalent accuracy to centralized model.\n\n\n#### Typos etc.\n\n* P1 \"each year they use\" - who/what is \"they\"? Deep learning? Data centers?\n\n* Many expressions missing hyphenation, e.g. P2 \"center based\" --> \"center-based\" and \"user owned and controlled\" --> \"user-owned and -controlled\".\n\n* P2 \"we have current zero understanding\" --> we currently have zero understanding\n\n* p6 \"variate\" --> vary\n\n* All of the numbered equations should be given a \"variable name\" for readability and reference later. For example, (1) could read TE = n(te_clients + 5s) [where TE is \"Total Energy\"]. (3) should be broken into two equations on different lines, with separate variable names.\n\n* The 10 symbols in Fig. 1 should be replaced by a continuous attribute (color or size).",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}