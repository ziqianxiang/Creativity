{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The proposed approach seems to have elements of novelty, it is well presented  and  reasonably motivated by the authors. In addition, empirical results seem to be promising. However, although rebuttal helped to clarify some of the pending issues, there are concerns on the fact that the raised issue about \"resolution dilemmas\" does not find in the paper a quantitative treatment. Without that, it is difficult to fully understand how to drive the learning of useful structural landmarks. Thus, notwithstanding the paper seems to contribute in a significant way to the advancement of the GNN field, it still needs additional work to better develop  the proposed concepts in a quantitative theory. "
    },
    "Reviews": [
        {
            "title": "Interesting exploration of  spatial resolution and structural resolution in Graph Neural Networks",
            "review": "Summary:\nAuthors introduces two new concepts spatial resolution and structural resolution in regards to understanding the graph structured data which are quite interesting and enlightening. Idea about projecting graph information \ninto structural landmarks is intriguing. To help make stronger case, I would suggest to do proper ablation study as it is not clear how much gain is coming unsupervised learning.\n\nPros:\nOverall I like intuition and the method about capturing the spatial resolution and structural resolution in a strategic manner. Author have some strong empirical performance especially on Protein, PTC and IMDB-M dataset.\n\nCons:\nAuthors argue that the classic graph neural networks which employ graph pooling operations are the bottleneck in identifying necessary substructures (or their interactions) for yielding high discriminative performance.  However, such statements are quite loose and need further theoretical justification given the fact graph pooling operations such as deepsets or sum-pooling are universal/injective functions in nature and thus can reflect any changes in the graph sub-structure (however their function smoothness or amount of representative power captured is entirely different issue).\n\nIt not clear why right hand side spectrum in Figure 2 will lead to lower generalization performance or over-fitting. Most of the time motifs/graphlets act as an atomic structure of a graph and their frequency distribution drives the discriminative performance.  As such identifying all such atomic structures should be helpful rather than harmful. It would be great if authors can expand on their explanation here and provide a real world example that would be more convincing to support their hypothesis. \n \nThere are certain paragraphs which are hard to read. For instance, Figure 1 lacks detail description and it is not clear what \"all the nodes are mixed into one\" means in the context. Also, a general suggestion would be to add more descriptive caption for each Figures in the paper.  \n\nI would suggest to provide compelling real-world examples (or do more qualitative analysis) besides the strong empirical performance in the main paper (there is some discussion in appendix but highlights can be included in the main context). \n\nAblation study is missing and thus hard to answer questions such as , is unsupervised learning (i.e., learning in Equation 2) even needed for getting strong results? I would really like to see the performance gains due to unsupervised learning.\n\nCan authors discuss the computation complexity of their method?\n\nTypos:\n\nVariables $T$, $b$ in equation (1) are not defined.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Addressing dual \"resolution dilemmas\" by graph pooling with landmarks",
            "review": "### Summary\n\nThe proposed SLIM algorithm organizes graph neural networks around substructures surrounding \"landmarks\" in the graph.  In addition to presenting the three steps of the SLIM algorithm (sub-structure embedding, sub-structure landmarking, and \"identity-preserving\" graph pooling), the authors compare to other approaches on a graph classification problem.  A large part of the paper is also given over to a high-level discussion of \"resolution dilemmas.\"\n\nThe idea of organizing around landmarks (and making the landmark choice a part of the optimization) is a nice one, and the new approach certainly seems to improve performance in the graph classification problem.  But I would have loved to see a more quantitative treatment of the \"resolution dilemmas.\"  All the ideas in this space involve lossy compression of the graph, and the authors characterize this in terms of loss of spatial resolution (ability to understand the \"high-level\" topology of the graph) or loss of structural resolution (ability to understand the prevalence of different types of local structures or motifs).  The attempt to balance these types of resolution concerns is the main motivation for the current paper.  But the discussion of these types of resolution is purely qualitative; the only place where there is really a quantitative discussion is in the experimental section in Figure 4.  Given the amount of space given over to discussing these different types of resolution, and the connections made (however briefly) to regularization ideas, I would have loved so see more of the discussion focus on how spatial and structural resolution can be measured, and how those measures might be used to guide the selection of algorithm hyper-parameters like the number of landmarks.\n\nThere is also a repeated statement that using substructure landmarks makes the representation easier to interpret.  This seems intuitive; but there are no examples illustrating such interpretability.\n\n### Typos\n\n- Abstract: \"iterpretable\"\n- Sec 2.1: \"tow key blocks\", \"capture lager substructures\"\n- Sec 2.2: \"Milo1, et al\"\n- Sec 3: \"sub-sturcture embedding\"\n- Fig 4: \"stuctutral resolution\"\n\n### Update\n\nThanks to the authors for their response to the reviews, and to the other reviewers for their comments as well.  It seems that we were all confused about many of the same things.\n\nThe authors have clarified some of the points raised, and I appreciate that.  I also continue to appreciate that the empirical results are promising.  At the same time, I personally remain confused about how to reason in a quantitative way -- suitable for diagnosing problems, determining hyperparameters, etc -- about the resolution tradeoffs that are described.  In other settings involving graph coarsenings, wavelets, graph Fourier transforms, etc, there is usually a more quantitative way of expressing what information is lost and retained by a given type of compression.  The discussion of coherence in the appendices says something about this, but not in a way that I would understand how to operationalize.\n\nI also appreciate that the chemistry example in the appendix exists, but I do not understand how to visually interpret the picture.\n\nMy recommendation remains a weak accept, as I think it is likely worthwhile to put the empirical results out in the world, and the theory may follow.  But I remain wary of my own lack of understanding of the theory in a quantitatively meaningful way, and would also welcome the chance to read a future version that had some of these aspects more clearly worked out and explained.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "Strengths\n\n1. This paper studied an important problem of graph mining on graph classification tasks by investigating the challenges that previous graph models encountered and solving them with the proposed method.\n\n2. The developed method is novel and interesting by proposing an inspiring concept called structural landmarking and capturing inherent interactions with it.\n\n3. The analysis and survey of related work with respect to the two mentioned resolution types is comprehensive.\n\n4. The paper is well written and easy to follow.\n\nWeakness\n\n1. The paper lacks enough analyses of behaviors of learned structural landmarks, with only an analysis of choices of their numbers.\n\n2. The time complexity of the developed method is not analyzed and its running time comparison with other baseline methods are also missing.\n\n3. The analysis of how the choices of each part of features learned in graph pooling would affect the results is missing.\n\n4. The experimental part lacks the analysis of why the method performs very well on some datasets while not performing well on others.\n\nSummary: This paper studies the problem of graph classification on chemical and social datasets. Existing graph classification methods with graph neural networks learn node embeddings via aggregation of neighbors and combine all node features into a final graph feature for classification, while such operations usually lack the ability for identifying and modelling the inner interactions of substructures. To remedy the information loss in graph pooling, the authors leverage the learned substructure landmarks to project graphs onto them for modelling the interacting relations between component parts of a graph. In this regard, an inductive neural network model for structural landmarking and interaction modelling is developed to resolve potential resolution dilemmas in graph classification and capture inherent interactions in graph-structured systems. Empirical experiments on both chemical and social datasets validate the effectiveness of the method. Generally speaking, the paper is well written and easy to follow, with clear motivation and organization. However, I have concerns about the lack of analysis for learned structural landmarks, since in the paper only the choice of its number is well discussed. Also, the time complexity of the developed method is not well studied. The detailed comments and suggestions of this paper are as follows.\n\n1.The paper proposes to learn structural landmarks and obtain representations of graphs by projecting them. Therefore, the quality of learned landmarks is crucial while the paper lacks enough analyses for them. I suggest providing a more comprehensive analysis for them.\n\n2.The proposed method generates various kinds of graph-level features while lacking enough analyses of their impacts on results. I suggest conducting more experiments of ablation study for this part.\n\n3.The detailed statistics of benchmark datasets are not mentioned such as the distribution of number of nodes in graphs.\n\n4.Although the results are competitive compared with other baselines, the authors didn’t explain why the method performs well on some datasets while not performing well on others. I suggest analyzing the reasons comprehensively.\n\n5.Only one evaluation metric is used in experiments, which is the accuracy. Since it’s a classification task, I suggest using various metrics to show the effectiveness.\n\n6.There are some typos in the paper that requires double checking: For example, \"breath-first search \" -> \"breadth-first search\", \"an molecule\" -> \"a molecule\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review comments to Paper 1160",
            "review": "==========Summary==========\n\nIn this paper, the authors investigate how to learn graph representations from structural landmarks. In particular, SLIM, an end-to-end trainable method, is proposed to simultaneously learn landmark and graph representations guided by downstream graph classification signals. Empirical results demonstrate the effectiveness of the proposed method on public benchmark datasets.\n\n==========Reason for the rating==========\n\nAt this moment, I am standing between 4 and 5. Overall, I feel the technique proposed in this paper is potentially interesting. However, its current presentation could also deliver quite confusing information. Hopefully, the authors could address my concerns in the rebuttal.   \n\n==========Strong points==========\n1. Structural landmark is an interesting idea to approach graph representation learning. This study goes beyond the assumption of \"bag of structures\", and investigates the angle of modeling interaction between discovered structures.\n\n2. The authors propose the SLIM framework that implements the proposed idea well.\n\n3. The provided empirical evidences suggest the SLIM technique is promising in graph classification tasks.\n\n==========Weak points==========\n1. The key insight \"resolution dilemma\" could be confusing. While one expects graph pooling causes the struggling between spatial and structural resolution, the two resolutions seem to be two orthogonal dimensions.\n    - The discussion on spatial resolution seems to be around the assumption for graph classification: bag of structures (BOS) or some more complex model that considers relations between discovered structures. Existing graph pooling methods mostly assume the BOS model, and develop different \"counting methods\" to generate one fixed-size vector for each graph. From the perspective of counting, it is difficult to see how existing pooling reduces spatial resolution, while indeed such pooling methods cannot consider relations between structures. Could the authors clarify what exactly \"diminish\" means in the context of \"spatial resolution\"?\n    - The discussion on structural resolution seems to be around model selection in terms of expressive power from learned representations: if we really can make a choice, which one could result in the most promising generalization performance. This problem is indeed meaningful, but it is not caused by pooling layers and is not necessary to address it at pooling layers.\n    - Overall, it is difficult to see the connection between section 2 and 3. After zooming into the two dimensions in section 2, they seem orthogonal, with weak coupling with pooling layers. The authors may need to provide stronger reasoning or evidences to motivate the proposed technique from the perspective of \"resolution dilemma\".\n\n2. The expressive power of substructure embedding could be a potential concern. Using $A^{(k)}$, different layouts within k-hops (k > 1) may not be able to be differentiated anymore. Does this imply different structures within k-hops do not matter?\n\n3. The presentation in section 3 could be further improved. \n    - The details on how to end-to-end train SLIM is missing in section 3, while one may find some relevant information in the experiment section. To make each section self-contained and the technique clearly described, the authors may put down the relevant contents in section 3.\n    - Section 3.3 is definitely critical in the whole framework; however, the current presentation could unnecessarily increase the difficulty in understanding the idea behind.\n\n==========Questions during rebuttal period==========\n\nPlease address and clarify the weak points above. \n\nIn addition, the current draft could use more proofreading to clear typos and grammar errors. The idea of landmark has also been discussed in a recent work [1], in the context of unsupervised learning. The authors may need to highlight their unique perspective.\n\n==========Reference==========\n[1] DDGK: Learning Graph Representations for Deep Divergence Graph Kernels, WWW 2019\n\n==========Post rebuttal comments==========\n\nThe rebuttal has addressed my main concerns. In general, I believe the core idea of modeling substructure interactions in GNNs should be shared in front of more audience. Therefore, I have increased the rating accordingly.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}