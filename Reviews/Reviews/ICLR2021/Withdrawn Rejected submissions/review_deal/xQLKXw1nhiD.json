{
    "Decision": "",
    "Reviews": [
        {
            "title": "A cross-modality super resolution method with weak experiments",
            "review": "The authors present a single-pair method for cross-modality super-resolution. The method relies on input pair of images. Similar idea has been published in previous works, limiting the novelty of the paper.\n\nThe experiments are not extensive or convincing. The authors should have compared with methods, which don’t use RGB images as guidance. Such comparisons would show how much the introduction of another modality data could improve.\n\nIn abstract, the authors claim that ‘We show that CMSR produces state-of-the-art super resolved images, yet without introducing artifacts or irrelevant details that originate from the RGB image only.’. However, there lacks corresponding visual results to support this claim.\n\nThe quality of most figures are relatively low and should be improved heavily. There appears obvious compression artifacts in the figures.\n\nThe writing and paper organization should also be improved largely. For example, Table 1 even exceeds the paper width, which is not a good shape for top conferences.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "My biggest concern is technical novelty. If the author’s can convincingly explain their technical novelty in the rebuttal, it would be helpful. ",
            "review": "Imaging hardware that is not rgb is more expensive than rgb sensors while the resolution is lower.\nIt is natural to think of a way to super resolve the low res sensor leveraging the rgb sensor which has the higher resolution information. \nThis paper is about performing SR on low res non-rgb images, using the knowledge from rgb image. \nFusing images from different modalities is difficult because of different innate properties. \nThe authors propose a single pair approach to cross modality SR. \nthe network is trained on a single two image pair. \n\nThis work is a nice work, which addresses a practical need and a simple solution to solve it. The solution is not impractical nor requires much supervised data. \nThe approach is understandable and the design choices are understandable as well. It is pretty much straight forward. \nThis work deserves a good publication. However, I’m not sure it meets the bar of ICLR. \nThe novelty of this paper in terms of learning representations, is limited in my opinion. \nThis work has a sound problem definition and solution. The architecture however does not provide additional insights into learning representation, but rather uses known and popular network components. \nThis work is application prone and provides a working solution, which might be more suited to other conference venues. \n\nMinor comment: please align Table 1.\n\nIn short, my biggest concern is technical novelty. If the author’s can convincingly explain their technical novelty in the rebuttal, it would be helpful. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weak Reject",
            "review": "In this paper, the authors propose a single-pair approach for Cross-Modality Super-Resolution, which uses an RGB image of a high-resolution to increase the resolution of a low-resolution modality. The network is internally trained in a self-supervised manner and contains a deformation model to align details from RGB image to the target modality in a coarse-to-fine manner. Experiment results indicate the proposed method produces state-of-the-art super-resolved images, which also allows the two modalities to be moderately misaligned.\n\n----Strengths:\n1.\tThe authors focus on the cross-modality super-resolution problem. The research direction of it is meaningful. The proposed network could allow the two modalities to be moderately misaligned, which is more effective in processing the cross-modality super-resolution problem in real-world scenarios. \n\n----Weaknesses: \n1.\tGenerally, this paper focus on addressing the cross-modality super-resolution problem in real-world scenarios. To achieve this, the proposed method contains a deformation model to align details from the two input images. Besides, the network is trained in a self-supervised manner. However, the methods (deformation convolution, self-supervised) are standard. In its current form, the contribution of this paper is not significant enough for ICLR. In fact, an application conference would be a more suitable venue\n2.\tFrom the abstract, “traditional state-of-the-arts which are trained on external datasets often struggle with yielding an artifact-free result that is still loyal to the target modality characteristics.” From the introduction, “they tend to fuse redundant details from the guiding modality anyway, resulting in the addition of textures and artifacts to the lower resolution modality” However, on the algorithm side, it is not clear why the proposed method could alleviate this problem. The justification needs to be articulated more clearly in the paper \n3.\tIn the experiment part, can you provide more implementation and training details about this work?  For example, what is the “Feature-Extractor I” and the “Feature-Extractor 2”?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Thorough experiments, while weak technical novelty",
            "review": "This paper aims to solve the problem of sensor image super-resolution with references from other modality. It uses a deformation network to handle the misalignment between the LR image and the RGB input, and then integrate the feature of the deformation-corrected RGB image with the feature map of the upsampled input to generate the final output.\n\n## Pros\n- This paper addresses an interesting while less-explored question in super-resolution.\n- The experiment is quite thorough. It shows results on NIR, depth images, and also the single modality experiments.\n- The joint training of deformation network and the main network is interesting. Thinking about the whole framework is only trained with SR reconstruction loss, the alignment result in Fig.9 is inspiring too. \n- Good quantitative and qualitative results.\n\n## Cons\n- The technical novelty of network design is marginal. \n- The part of self-supervised training could be better explained. For example, the appendix mentioned the runtime changes depending on the required iterations --- What's the stopping criteria? How about comparing this with ZSSR? \n- Besides, there are some minor issues: most of the visual results come without LR inputs; some misuse of \"x\" and \"$\\times$\", etc.\n\n## Additional questions\n1. The inference time is rather long, probably due to most of the operations are in the high resolution domain. I am curious about whether the author try to to the align / feature aggregation in LR domain? How would the result be like?\n2. I am also curious about the robustness and functionality of the RGB reference. Has the author tried to use non-related images, or even blank image? To what extent will the result be influenced?\n\nOverall, this paper is interesting to read for me. Thanks for the author to illustrate this question with rich experimental results. Still, I have some concerns on the method design, robustness and other details. Thus, I would give this score for now. Feedback from the author is greatly appreciated.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}