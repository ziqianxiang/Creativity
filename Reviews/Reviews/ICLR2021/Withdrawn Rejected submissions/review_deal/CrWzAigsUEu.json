{
    "Decision": "",
    "Reviews": [
        {
            "title": "Multivariate factorization",
            "review": "The paper proposes an extension of Sum Product Networks (SPNs), named factorize-sum-split-product networks (FSPNs), in order to achieve high expressiveness and tractability.\n\nThe proposed idea should be interesting, however the paper could be improved in many ways. \n\nIt is not true that SPNs are recursively defined as weighted sums of smaller SPNs on data partitions, or products of smaller SPNs on independent variable subsets. This corresponds to the process of a specific algorithm learning the structure of a SPN. The definition of a SPN does not depend on the data neither on the variable independence. Even the description of SPNs at page 3 is strongly correlated to training data, that is not the case. Indeed, a sum node in an SPN does not split the data.\n\nFurthermore, leaf node in an SPN could not represent a distribution of a single variable. It is possible to model a leaf node with a multivariate distribution.\n\nUsing a BN in an SPN does not increase the inference complexity if we have a BN with a low tree-width (1 for chow-liu trees).\n\nIt should be explained how the partitions are computed in order to obtain the ranges. Furthermore, how is chosen the split point? The use of EM for the split operation is unclear. \n\nThe adopted split operation is not new. See for instance the Cutset-Networks, a kind of deterministic probabilistic circuit representing conditioning.\n\nThe authors say that in the experiment on synthetic data they adopted the pomegranate package for learning BNs. Currently, pomegranate only supports discrete Bayesian networks. The domain of the variables has not been defined in the paper. \n\nAs regards the experimental evaluation on real-world dataset, the proposed approach seems to have the same performances on average when compared to SPN-BTB. Furthermore, the results for SPN-BTB reported in the original paper sometimes seem to be better than those reported in this paper.\n\nConcluding, the paper address an interesting point, but in the current version is difficult to appreciate its contribution and novelty.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper deals a very important topic related to improving  inference. In fact, inference is a NP-hard problem, because it depends on the complexity of the model and the number of variables. Improving accuracy and speed of the inference are two important goals in graphical models. ",
            "review": "To answer the challenge of inference in graphical models, the authors propose a new method called FSPN: factorize-sum-product-method. the proposed method FSPN combines conditional factorizations from Bayesian networks framework and independent factorization applied in SPN. FSPN proposes to outperform both BN\n\nOverall, I vote for accepting. I like the idea of combining conditional of BN and Independent for SPN in proposing this new method in handling inference in graphical models. The need for such methods in improving inference in graphical models is well presented and explained. Also, the paper is well written and the FSPN method is well presented and explained in details even for non-experts in inference in graphical models.\n  \n  \n1.\tThe paper takes one of the most important issue of inference in graphical models.\n2.\tThe proposed FSPN is original in the sense that it combines conditional and independence factorization approaches to decompose the joint probability distribution.\n3.\tThis paper provides extensive experiments to show the effectiveness of the new proposed method FSPN. The method improves both accuracy and speed of the inference process even in complex graphical models.\nThe paper is well written, and I read it with great interest. I just suggest adding spell out PDF first time introduced in the introduction, then keep use the abbreviation for the rest of the paper. (Page 1, Introduction-Challenges of PGMs: third line from the bottom).\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "FSPN: A New Class of Probabilistic Graphical Model",
            "review": "### Summary\nThe authors propose a framework to infer probabilistic graphical models in a general, fast and scalable framework, able to deal with highly correlated variables. The analysis on synthetic datasets show that the proposed method has high accuracy and fast inference.\n\n### Reasons for score\nI generally like the paper and the proposed approach. I found difficult to read the paper though, because of the formatting and in particular section , which is very mathematically dense.\nAlso, the authors in the analysis mention that \"The performance SPGM, SPN-BTB and ID-SPN is slightly worse than FSPN\" which goes against the results they showcase in the same page. It seems that especially SPGM and SPN-BTB are better than FSPN in at least 5 + 6 data sets (vs 7 of FSPN). The results suggest that such models are actually performing similarly. Also, 3 of the 6 data sets in which SPN-BTB is outperforming FSPN are the ones with the highest number of variables (with WebKB a close second, so almost 5). Also, is there a reason why SPN-BTB has not been used on 20 Newsgroups?\n\n### Pros\n- Generalises the PGM inference, with a scalable and fast approach\n- Comprehensive analysis with state-of-the-art models on a wide range of data sets.\n\n### Cons\n- Paper presentation and clarity could be improved.\n- Analysis is not always clear. Some claims do not seem to be supported by evidence.\n\n### Minors\n- Citations should be done using `\\citep` command (they get confused with the text).\n- \"while overcomes\" --> \"while overcoming\" page 2\n- I don't believe the formatting is standard. The underlined paragraphs look wrong.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A potentially good inductive bias for learning SPNs, but whose significance and effectiveness is not clear",
            "review": "##UPDATE \nI thank the authors for their answers and efforts in trying to improve the paper.\nI believe the uploaded refactoring is becoming closer to being publishable and I encourage authors to iterate one more time over it. The points that still needs to be taken into account concern:\n- removing references of FSPNs as 'new model' (starting from the title) and instead clearly say that they are proposing a new inductive bias that would be useful for learning\n- amend the statements about the relationship of FSPNs and Bayesian Networks and SPNs \n- linking and contrasting FSPNs to other circuit models in a deeper way. In essence, what the authors are proposing is a k-way split operator which can still be represented with sum and product units (or decision nodes).\n- report learning times and sizes of the learned circuits and competitors. It is not necessary to re-learn the best model for competitors but it is enough to report an average statistics over a resonable amount of trials/values in a grid search. \n\n## SUMMARY\nThe paper proposes a way to model and learn sum-product networks (SPNs) via multivariate split operations, which they call `factorize nodes`.\nThe authors claim that the resulting architecture -- called factorized SPN (FSPN) -- to be an hybrid of Bayesian networks and SPNs and being more tractable of the first while more expressive than the latter.\nIt seems to me that these, and numerous other claims, are not properly substantiated and this, jointly with a weak experimental evaluation, makes the paper in its current form immature for publication. Detailed comments below.\n\n\n## PROS\n+ Learning multivariate split nodes might be useful in practice\n\n## CONS\n- Novelty is limited, but more crucially, the true contribution is somehow hidden under several claims that  are unsubstantiated or imprecise \n- presentation will benefit from a major revision\n- Relevant SPN variants are not cited (CSPNs, SPQNs) nor discussed in detail (SPGM)\n- Experimental setting does not corroborate the empirical advantage of FSPNs\n\n\n## NOVELTY and CORRECTNESS\nWhile the underlying idea of introducing multivariate split nodes might provide an interesting inductive bias for learning SPNs, it value cannot be truly assessed from the current version of the manuscript.\nFirst, claims about architectural novelty and increase expressiveness are not substantiated. Second, relevant literature implementing similar ideas is left out of the scope of the paper, either not properly discussed or experimented with.\n\nIn fact, FSPNs are still computational graphs that can be represented with sum and product node only, in essence, SPNs. This is evident from split nodes, which are just deterministic sum nodes -- ie only one sum node child is activated at a time [1]. The factorize node is indeed just another variant of split nodes with conditioning over multivariate events, and as such still representable with a deterministic sum, realizing the computations in page 5.\n\nNote that multivariate split nodes have already been used in other kinds of circuits like PSDDs [2]. Conditioning in its more general form, has been introduced in SPNs with conditional SPNs (CSPNs) [3] and sum-product-quotient networks (SPQNs) [4].\nThese models should be compared with FSPNs, if not theoretically, at least empirically in some form.\n\nSince this re-writing of split and factor nodes only increase the size of the underlying SPN polynomially (linearly) there is no gain in expressive efficiency. \nMoreover, this definitely does not increase expressiveness of SPNs, which already are universal representations. I suggest authors clarify the several claims about increased expressiveness by adopting the distinction between expressivenss and expressive efficiency by Martens and Melibani (more comments below on this).\n\nIt is true that FSPNs (and SPNs) are tractable whereas BNs might not, in general. However this holds from a representational perspective, and when it comes to learning, one can learn a BN that is tractable by design: e.g., either by compiling it into a circuit [5] or by bounding its treewidth [6].\n\n\n\nA list of additional claims to be substantiated follows.\n\n- \"Sum-product network (SPN)’s performance significantly degrades in presence of highly correlated variables\"\nAll in all, I believe one general confusion that permeates the paper raises when considering SPNs to be only/always learned with LearnSPN-like algorithms. As such they are deemed to be trees (not DAGs, hence less expressive efficient) and subject to the issues of a greedy algorithm that does not properly model correlations.\n\nThe job of sum nodes in SPNs is exactly to properly recover the correlations between sub-distributions encoded in the sub-circuits. Posing the whole circuit as a DAG clearly helps modeling more correlations in a more compact form than a tree.\n\n- \"PGMs tend to be much more interpretable and faster in inference.\" \nThis is not true or at least misleading: PGMs include deep latent variable models and are in general intractable. Non-latent variable models might be deemed more interpretable in some cases (arguable).\n  \n-  \"However, marginal probability inference on MRF is more difficult\"\nThe hardness still comes from an integration/summation over an exponential number of states, as in BNs.\n\n-  \"SPNs can only accurately characterize the joint PDF of weakly correlated variables\"\nIn Martens and Melibamini (2014), the argument is about expressive efficiency,  not expressiveness. Specifically they construct a precise distribution that cannot be compactly represented by any SPN. This distribution is not characterized by correlations alone, and moreover this does not imply that all fully connected models cannot be captured by SPNs.  \n\n-  \"As a result, the learned SPN becomes very deep and heavily overfitting, which may greatly degrade estimation accuracy\"\nOverfitting depends on the relationship of model and data complexity, expressive efficiency is more linked to a model class expressiveness per se. Again, I believe the focus here is on the limitations of LearnSPNs and not on SPNs as models per se.\n\n-  \"To the best of our knowledge, no existing class of PGM has so far addressed the trade-off between tractability and expressiveness in a truly satisfactory manner\"\nI agree with the authors that this is an open and interesting problem, however this problem has been addressed numerous time in the vast literature of tractable probabilistic models [7]. See also the references listed above.\n\n-  \"BN applies conditional factorization, which is accurate but difficult for inference.  SPN uses local independent factorization, which makes the model tractable but causes large errors in the presence of strongly correlated variables.\"\nThis claim is misleading, SPNs and other circuit representations exploit a finer form of conditional independence, context-specific independence [8], and if the distribution of a BN includes these independence statements, it might be compactly compiled in a circuit.\n\n\n- \"FSPN can compactly and accurately model the joint PDF of variables with any degree of dependence\"\nThis statements need to be backed by some theory. To my understanding, since a factorization node is just a multivariate split node with $t$ children in an SPN, tractability depends on the value of $t$. Ultimately, this should be bounded by the treewidth of the distribution.\n\n\n-  \"Specifically, a sum node generates its children by splitting the data\"\nThis sounds like a description of how learnSPN proceeds, I guess the authors want to talk about the generative model behind mixture models, which are encoded as sum nodes\n\n- \"However, when there exist strongly correlated variables in X,the product operation is unable to split them and SPN would repeatedly use the sum operation to splitD′into very small volumes, i.e.,|D′|=1in extreme\"\nSame as above\n\n\n- \"Without loss of generality, we can represent any event E in the probability space as a hyper-rectangle\"\n\nThis is with loss of generality, events can be non-axis aligned polytopes. If that is the case computing their probabilities becomes hard for SPNs and therefore for FSPNs.\n\n\nSome minor rewritings:\n\n- \"SPNs are PGMs\"\nTo be pedantic they are not, the semantics behind these two graphical formalisms is quite different!\n\n-  \"While the probability inference time on an SPN is linear w.r.t. its node size\"\n\"Circuit size\", or \"model size\", or \"number of nodes\" are more appropriate\n\n- \"Therefore, MRFs are not suitable for probability inference tasks and thus mainly used for data generation and pattern recognition\"\nThe application mentioned do use probabilistic inference underneath!\n\n- \"However,marginal probability inference for BN has a high time complexity and is sometimes even intractable.\"\nBetter to say is intractable in general, but sometimes tractable (e.g. for bounded treewidth)\n\n-  \"However, the learned BN structure maybe inaccurate, which in turn heavily degrades the probability estimation accuracy.\"\nSounds like a tautology!\n\n\n## PRESENTATION\nThe paper is understandable, contains some typos and presentation might benefit a thorough proof-reading.\n\nAmong the typos, many involve calling model families with singular names, whereas plurals would be better. \nSome examples:\n+ Probabilistic graphical model (PGM), a rich framework [...] aims -> Probabilistic graphical models (PGMs), a rich framework [...] aim\n+ The most well-known PGM—Bayesian network (BN), -> The most well-known PGM—Bayesian networks (BNs),\n+ BN applies [...] SPN uses -> BNs apply [...] SPNs use\n\nConcerning notation, adopting a single way to denote splits and subsets of random variables for factorization nodes would improve readability (I have seen authors using S-H, X-H, W etc for the same case). \n\n- each value x of X−Hs\nA bit confusing, why not i) using set difference operators (/) and ii) introducing a new set of variables `S = X \\ H` with values `s`?\n\n## EXPERIMENTS\nExperimental results are not astonishing and fail to convince the reader on the adoption of FSPNs instead of other learners in the burgeoning literature of structure learning for SPNs. Moreover a comparison with relevant literature is missing.\n\nSpecifically, results reported in Table 2 show very minor gains and on 6 (on the seventh they are on par) datasets only wrt sota learners. Moreover, for these gains I suspect that a statistical test would deem the difference not that significant.\nConcerning other competitors outperforming FSPNs, the authors claim \n\n\"This is because they (SPN-BTB, ID-SPNs) use embedded BNs or MRFs in their structure to overcome thedrawbacks of SPN. However, such embedded components would result in slow inference speed.\"\nThis claim is misleading. First, the named competitors use BNs or MRFs only as leaf multivariate distributions over limited subsets of variables. These are compiled as circuits of the polysize and as such inference times are not supposed to degrade. \n\nTo make a point in this regard, authors should report times for learning and inference of all models and show the sizes of the learned circuits (they do this in the Appendix for the synth exps only). \n\nConcerning synthetic experiments, authors use the RDC as a multivariate test for independence. This very same test is used for mixed SPNs (MSPNs) [9] for learning product and sum nodes, however MSPNs are left out of the comparison. Is there a particular reason why?\nOverall, the effect of the different clustering and independence test algorithms used by the different competitors is not taken into account in this evaluation.\n\nThere is no clear ablation test on the use of different values for $t$ which clearly governs the size of the circuit.\n\nLastly, hyperparameters for the grid searches of all competitors are missing and reproducibility of the work is unclear.\n\n\n\n### REFERENCES\n[1] Darwiche, Adnan. Modeling and reasoning with Bayesian networks. Cambridge university press, 2009.\n\n[2] Kisa, Doga, et al. \"Probabilistic sentential decision diagrams.\" Proceedings of the 14th international conference on principles of \nknowledge representation and reasoning (KR). 2014.\n\n[3] Shao, Xiaoting, et al. \"Conditional sum-product networks: Imposing structure on deep probabilistic architectures.\" PGM 2020\n\n[4] Sharir, Or, and Amnon Shashua. \"Sum-product-quotient networks.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2018.\n\n[5] Lowd, Daniel, and Pedro Domingos. \"Learning arithmetic circuits.\" (2012).\n\n[6] Scanagatta, Mauro, et al. \"Learning treewidth-bounded Bayesian networks with thousands of variables.\" Advances in neural information processing systems. 2016.\n\n[7] Choi et al. \"Probabilistic circuits: a unifying framework for tractable probabilistic models\" 2020\n\n[8] Boutillier et al., \"Context-Specific Independence in Bayesian Networks\" 2005\n\n[9] Molina, Alejandro, et al. \"Mixed sum-product networks: A deep architecture for hybrid domains.\" Thirty-second AAAI conference on artificial intelligence. 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}