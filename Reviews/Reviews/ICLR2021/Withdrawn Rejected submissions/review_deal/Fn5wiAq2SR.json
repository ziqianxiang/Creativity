{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose adversarial training using contrastive divergence based on ideas from Hybrid Monte Carlo methods.\nOn the positive side the shown experimental results are promising both in terms of robustness and efficiency. On the negative side the paper seems to be written in a hurry. At several places terms are not defined, not explained or important details (e.g. parameters of the attack algorithms) are missing. \n\nThus the paper is not ready for publication yet and below the bar for ICLR but I encourage the authors to submit a significantly revised version to another conference.\n\nDetails:\n- in (7) N(x) is nowhere explained or defined, here also several threat models are introduced but later on only l_infty seems\nto be used e.g in Algorithm 1 (should be clarified)\n- as noted in the reviews the definition in (13) is based on quantities nowhere introduced\n- the potential U is only defined in the Algorithm (but then the arguments do not match with the RHS)\n- the kinetic energy in the algorithm is different from what has been used before\n- the parameters for the attacks used are not reported\n- why is AutoAttack not used for all datasets? They report 28% robust accuracy for the model trained by the Madry group\n (https://github.com/MadryLab/robustness) whereas in the present paper it is 35%.\n- the scales of the plots should be chosen such that the curves can be distinguished"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #1",
            "review": "Summary: \nIn this paper, the authors reformulated the generation of adversarial examples as an MCMC process and present a new adversarial learning method called ATCD, which approaches the equilibrium distribution of adversarial examples with only a few iterations by building from small modifications of the standard Contrastive Divergence. Extensive results with comparisons on various datasets show that ATCD achieves a trade-off between efficiency and accuracy in adversarial training.\n\nComments:\n\n1 . The proposed algorithm ATCD views the generation of adversarial examples as a sampling procedure. Specifically, it can be seen as performing HMC sampling. Also, it modifies the adversarial example generation objectives using a modified contrastive divergence objective. Compared with free-adversarial-training, which also utilize mini-batch replay and use the last iterate’s result as initialization, the difference only lies in the adversarial example generation objective and the extra noise brought by the HMC sampling procedure. However, it is still not clear to me what actually causes the performance improvement. The different objectives or the sampling noise? I would suggest the authors conduct ablation studies to find out this answer and it would be clearer for the readers to understand the true driving force for the proposed method.\n\n2 . In Eq (13) what is Q0 and Q1? It is better to formally define them. The intuition for this objective is a bit confusing to me. For standard contrastive divergence, it is the same as measuring the difference between the output probability between init point and K-step updated point. Here since the equilibrium distribution is unknown (fixed but unknown right?), how to compute the objective here with rho and lambda parameters? And why using different rho and lambda helps?\n\n3 . Notice that even adversarial training based algorithms could cause obfuscated gradient problem, therefore, it might be a good idea to further evaluate model robustness via totally gradient-free methods, such as hard-label attacks. I would suggest the authors to also evaluate using the following method: \n\n“RayS: A Ray Searching Method for Hard-label Adversarial Attack” KDD (2020)\n\nIn order to make the experimental results more convincing.\n\n4 . There are some other recent adversarial training methods that the authors might want to comment on\n\n\"Improving adversarial robustness requires revisiting misclassified examples.\" ICLR (2019).\n\"On the Convergence and Robustness of Adversarial Training.\" ICML (2019).\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting topic with weak experimental results",
            "review": "Summary:\nThis paper proposed a new adversarial attack method by using Markov chain Monte Carlo.  Based on this attack method, a new adversarial learning method called adversarial training by using Contrastive Divergence (ATCD) which approaches equilibrium distribution of adversarial examples with only a few iterations is performed. The experimental results demonstrated the effectiveness of ATCD.\n\nPros:\n1. The method of generating adversarial examples is new and efficient.\n2. Experimental results with comparisons on ImageNet, CIFAR and MNIST datasets show that ATCD achieves a good trade-off between efficiency and accuracy in adversarial training.\n\nCons:\n1. There is a notable natural accuracy gap between ATCD and Madry's PGD method and Free-m.\nMany papers were aware of overfitting in adversarial training such as 'Overfitting in adversarially robust deep learning'. Nature accuracy and robust accuracy are sometimes conflicted in the latter epochs. It seems like some plots like Figure 3 may answer the question, but Figure 3 only plots the first 40 iterations (I guess it means 'epochs' here), while the total epoch is 105.\nSo only observe the results of the last epoch may not fair since maybe different methods have different convergence rates.\n2. Some hyperparameter settings and sensitive analyses like $\\rho$ and $\\lambda$ are missing.\n3. Although, the experimental results show the efficiency of ATCD, the formal time complexity analysis of ATCD should be performed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Contrastive divergence for adversarial sample generation. Theory formulation and results reporting",
            "review": "Adversarial examples are time-consuming to generate. In this paper, the adversarial training is reformulated as a combination of stationary distribution exploring, sampling, and training. A Hamiltonian system is proposed to model data samples from their initial states, and is shown as the general form of FGSM. The sample generation method is proposed via contrastive divergence with few training iterations. Experiments have been validated on datasets. \n\nThe formulation of HMC sounds interesting and its relationship towards FGSM/PGD is discussed. There are few issues towards the theoretical discovery and experimental validations:\n\n1. The FGSM/PGD based attacks are formulated as the degeneration of HMC, as explained in Sec. 4.2. However, the experiments on the benchmarks consistently indicate PGD performs better than the proposed method ATCD. As ATCD utilizes a more advanced HMC, there lacks explanations of why the results do not correspond to the theory.  \n\n2. The proposed ATCD is claimed to efficiently generate adversarial examples while the performance seems to suffer from limited iterations. Is it possible to increase the iterations of ATCD for performance improvement?\n\n3. As the efficiency is claimed as a major contribution of ATCD, the computational complexity and time cost compared to other methods (e.g., FGSM, PGD) shall be reported as well.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}