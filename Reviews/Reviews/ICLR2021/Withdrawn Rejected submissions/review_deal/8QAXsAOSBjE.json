{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The initial reviews for this paper were very borderline. The authors provided detailed responses as well as a few additional results and observations. The authors' responses answered the reviewers' questions and addressed their main comments (including in the discussion of related works as well as with more in-depth analysis in a new Section 5.1). Unfortunately, the reviewers did not come to a consensus.\n\nOverall, this paper extends some current methodology for emotional classification, is well-executed, and provides a reasonably thorough study. The results are somewhat in line with previous results from other fields (and notably NLP), but the authors demonstrate the efficacy of using primary multi-task learning for multimodal conversational analysis. \n\nUnfortunately, this paper also has some flaws as highlighted by the initial reviews. As stated above, the authors did provide a strong rebuttal, but given the different comments raised by the reviewers that spanned many aspects of the paper including motivation, possibly limited contribution and novelty, missing related work, somewhat shallow analysis of the results, I find that another full round of reviewing would be useful to assess the paper.\n\nAs a result, this remains a very borderline paper, and given the strong competition at this year's conference, I cannot recommend acceptance at this stage.\n\nI suggest that the authors incorporate some of the discussions from this forum (and especially with respect to related work, new findings, and clearly defining the motivation and contribution of this work) into the next version of their paper."
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "This paper studies how the preprocessed data can be reused as auxiliary tasks in primary multi-task learning (MTL) for the multimodal emotion detection task. The authors propose and test three hypotheses for primary MTL. Two different hierarchical-level models, FLAT-MTL hierarchical attention model and HAN-Rock model, are proposed to improve the performance of the primary MTL.     \n\nOverall, the definition of the task and the overall architecture of this paper are both clear and straightforward, making this paper easy to understand. To explore how to use the preprocessed data as auxiliary tasks in primary MTL, the authors present a nicely executed study and test three hypotheses. Besides, the proposed model achieves impressive results on two datasets in terms of the multimodal emotion detection task. \n\nHere are some of my questions and concerns for the paper:\n1) The contribution of this paper seems to be limited since the idea of using visual features, audio features, and context as auxiliary information is similar to lots of emotional classification models such as ICON, CMN. In addition, the proposed model for MTL is also straightforward.\n2) More related works about multi-task learning and multimodal emotion detection tasks should be included.\n3) More in-depth experimental analysis should be carried out. For example, what is the individual performance of models on four different emotions under the three hypotheses?\n4) Some symbols in formulas lack explanations. For instance, X_t,X_(t-1),X_(t-2) on page 2 and b_w,W_w,u_w,u_s on page 3 are all not clearly defined. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work on multi-task learning but novelty is limited",
            "review": "This paper addresses challenges faced in the multi-task learning (MTL) models used in analyzing multimodal conversational data. The main challenge paper is trying to solve is on how to select relevant auxiliary tasks that avoid negative transfer. The authors explore how the preprocessed data used for feature engineering can be re-used as auxiliary tasks in the model. The authors identified sixteen relevant auxiliary tasks, identified a method to distribute learning capacity between primary and auxiliary tasks and proposed a relative supervision hierarchy between primary and auxiliary tasks. An extensive set of experiments are conducted to show the effectiveness of the approach. \n\nThe paper is trying to tackle an important problem faced by multi-task learning (MTL) and the way different aspects of the problem are explained in paper is valuable. The paper takes a systematic approach to address various aspects and conduct extensive experiments to show that having auxiliary task in the model significantly improved performance of the primary task, assigning higher learning capacity (number of parameters) to primary tasks helps in the performance and placing auxiliary supervision at the lower hierarchy significantly improves the performance on primary task. The paper is well written and easy to follow.\n\nAlthough the paper is trying to tackle an important challenge, the novelty or contribution of the paper is limited. The useful ideas concluded by the paper has been previously identified in the community, for example, NLP community has been using auxiliary tasks like NER to improve primary taskâ€™s performance. Also, hierarchy in which to structure primary and auxiliary tasks in a model is also somewhat discussed previously. Another shortcoming of the paper is that the proposed solutions are evaluated on a specific domain and it is not clear if these findings are general enough to be applied to other domains. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper addresses multi-task learning for multimodal emotion recognition on two existing datasets (IEMOCAP and SEMAINE). ",
            "review": "The paper addresses multi-task learning for multimodal emotion recognition on two existing datasets (IEMOCAP and SEMAINE). \nStrengths:\n*The issues addressed in this paper are very relevant. The use of a multi-task learning framework to tackle the lack of labeled data and the noisy labels for Affective computing research is a very interesting and still unexplored research line. \n\nWeaknesses:\n*This work is motivated by the analysis of video-conferencing videos which is indeed a crucial and topical issue. However, this motivation is a little bit heavy-handed as the processed data are very different from video-conferencing videos (face-to-face human interactions for IEMOCAP and human-agent/Woz interactions for SEMAINE database). \n*Contrarily to what is claimed in the abstract, the relevance of the auxiliary task is not investigated in the paper. The first type of auxiliary tasks is predicting outputs using external predictors (Open face in order to obtain action units activations from computer vision and Vokaturi in order to extract emotion categories from the prosody). The primary tasks (4-classes emotion classification using the same 4 emotions, and dimensions prediction) are very close to these auxiliary tasks.  The second type of auxiliary tasks is predicting contexts such as done in language models and could be considered unsupervised pre-training objectives used for learning representations (but it is not described like that). Thus, if we push the reflection a little bit further, H1  seems to validate that using external off-the-shelf predictors -that are predicting the same outputs as the ones of the primary tasks- improves the results of the proposed predictor, which is not so interesting.\n*The emotion categories that are considered are four of the Big-six of Ekman and I am wondering how relevant it is. For example, how often fear is observed in the SEMAINE and IEMOCAP datasets? \n*The method used to provide a multimodal representation of the data for the inputs seems interesting (as I understand it: extracting multimodal features and generating an augmented text containing multimodal narratives using MONAH, a previous system proposed by the authors) but it's difficult to understand what this method brings compared to the state of the art of learning multimodal representations. Besides, it will be interesting to discuss shortly the performance of MONAH on the two datasets of human-human interactions (maybe showing some outputs).\n*The proposed MTL framework relies on a combination of existing models (HAN and ROCK). Thus the contribution is rather experimental than methodological.\n*Considering the problem of multi-task learning requires identifying and differentiating tasks and domains and the paper fails in doing this. In Section 5, 8 tasks are identified while 2 datasets and 5 or 6 tasks are actually considered (emotion classif, valence prediction,activation prediction, Dominance/Power prediction) \n\n \nTypos :\n*abstract: it may generalizes\n*Section 3.2: indicateS\n*Section 4.1: valence\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Overall okay paper with some technical drawbacks",
            "review": "This paper tackles conversational analysis problem and more specifically the Primary Multi-Task Learning.\n\nThe three hypotheses the paper tries to address are interesting and important. \n\n- Hypothesis H1: The introduced set of auxiliary supervision features improves primary MTL.\n\nIn this section, I am not quite sure if using future labels would cause information leakage to the primary task. If so, this would make the conclusion questionable.\n\n- Hypothesis H2: When the primary branch is given maximum learning capacity, it would not be outperformed by models with primary branch having less than the maximum learning capacity.\n\nIn this section, I am not sure what is the fairest way to define learning capacity. Is increasing number of GRUs the best way? What if simpler model architecture can be used for auxiliary targets?\n\n- Hypothesis H3: Auxiliary supervision at the lower hierarchy yields better primary MTL as compared to flat-MTL.\nFor this, where is the experimental result for using them as features instead of using as targets?\n\nOverall, the paper adopts an experiments-driven approach to test the three hypotheses, but the main issue is that this approach adopts a specific neural network method. So how can we make sure that the conclusions always hold then?\n\nSome minor observation: it seems the multi-modal approach in the CVPR 2020 paper \"Multimodal Categorization of Crisis Events in Social Media\" can be used here so that in Figure 3, concat can be used with better attention mechanisms.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}