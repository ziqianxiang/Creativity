{
    "Decision": "",
    "Reviews": [
        {
            "title": "Result is reasonable but not strong enough given state-of-art understanding of PPO",
            "review": "This paper studied the asymptotic convergence of PPO and RUDDER. The result in this paper is reasonable given the asssumptions the author made, but this paper has the following issues:\n\n(1) The author make several mistakes in citing the papers. Some articale are even not cited in corrected place. For example the 4th paragraph about the cition of LQR... \n\n(2) Some claims about state-of-art works are not accurate. For example, the author claim that they study the policy class that become greedy which has not be studied before. However the neural soft-max policy studied in Wang et al. would become greedy during the training.\n\n(3) Some assumptions are too strong and should be verified. For example, in the second item of A6, the uniqueness of stationary point for nonlinear SA is too strong. In the last item of A6, although similar assumption has been verified in the linear case, since the author focus on general neural network work setting which is nonlinear, such an assumption may not hold in general and thus requires more discussion. A7 may also not hold in general, as the iteration of two time-scale nonlinear SA is not guaranteed to be bounded.\n\n(4) The author did not explore the core propoty of PPO in their proof, thus only establish local convergence in the paper. PPO share similar updating sturcture of TRPO and NPG, which updates the policy in the steepest direction in the distributional space. Many recently papers has found that in the distributional space, the objective function of RL has convex-like property, thus algorithms like PPO/TRPO/NPG has the potential to converge to the global optimal. Even though the general neural network policy does not have enough expressive power, global convergence can still be established given an unvanishing function approximation gap (see Agarwal et al 2020). However in this paper the author did not explore such a property, thus the result in this paper is not strong enough given the state-of-art works (Wang et al 2019, Agarwal et al 2020).\n\nOverall, since the paper still need to be polished, some assumptions are too strong and the result is not strong enough, I think the current version is not ready to be published.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The results look interesting, but the presentation makes the paper too hard to follow",
            "review": "The paper presents convergence results for policy gradient methods, in particular for PPO and RUDDER.\n\nThe result seems interesting, but the presentation of the paper is is very poor: even the main results are so notation heavy, that they turn out to be too hard to understand. Additionally, the paper is not even self contained: without reading significant parts of some of the references, the reader is left without understanding what some of the relevant notions and notations are. Finally, the paper should be completely reorganized to improve the flow and readability. (See more details below.)\n\n\nDetailed comments\n-----------------\n\nFrom the main part of the paper, the reader gains no real insight about the analysis. Basically the whole first half of the paper consists of introducing some very technical formulations in order to present the main result, and then some further (comparably) heavy notations to introduce its implications for PPO and RUDDER. The second half of the paper is what the proof is supposed to be; however, this consists of just briefly justifying the assumptions of the theorem by Karmakar and Bhatnagar one by one. This is somewhat confusing. It suggests that the main work was done by the cited paper of Karmakar and Bhatnagar, and then this paper is a relatively straightforward corollary. I presume that there is more work underneath the hood though - but that is exactly what the authors should try to summarize in the main part of the paper.\n\nAnother confusing thing is that it is not clear how (6)-(9) is equivalent to PPO. It would be very helpful to present PPO in a more familiar version first, and then explain how (6)-(9) results in the same algorithm.\n\nAdditionally, referring to (6) as the objective function is rather messy. Using E[-G_0] as the objective function (i.e., the dependence on the step-parameter removed) would be much more intuitive.\n\nSection 3.1 is also hard to follow. It is full of informal and missing details:\n* the definition of two-timescale stochastic approximation is not self-contained. For example, what is an occupation measure? What is the transition kernel and how does it depend on a_n? What is the actual relation between V(\\theta,.) and \\lambda(\\theta)? \n* Some of the assumption would actually take up less space presented in a formal way then in the current informal form.\n* The first assumption in (A6) on the uniqueness of the occupation measure is completely unclear due to the level of formality.\n\nSimilarly, referring to z_n as an \"additional Markov process\" is just confusing. On the other hand, giving the example above (15) right at the beginning could help a lot. In fact, even (15) should be introduced much earlier; preferably at the same time as introducing z_1. Additionally, (15) would be much better with the \\psi substituted by \\hat{q}. \\psi itself is an unnecessary distraction. In fact, the whole paper could seriously gain from reducing the generality in favor of cleaner and more direct definitions and result.\n\nThe paper should be much easier to follow if it worked with a clean, simple, familiar version of the algorithms, introduced them completely before discussing the details, gave up on generality for the sake of reader-friendly presentation - maybe even at the price of moving the main result in the appendix.\n\n\nAdditional comments\n-------------------\nIt would be interesting to contrast the results to existing TRPO results and to understand the main differences in the approaches.\n\n\nMinor comments\n--------------\n\"Temporal difference (TD) convergences to a local optimum with smooth function approximation like by neural networks (Maei et al., 2009).\" - the sentence does not parse\n\nWhat do you mean by saying that \"huge negative rewards if not executing the optimal actions may avoid convergence to the optimal policy if the policy is not greedy enough\"?\n\nWhat do you mean by saying that \"(z1)n directly enters the policy according to Eq. (15).\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "relatively weak result and unclear presentation",
            "review": "This paper considers a new convergence analysis for actor critic algorithms with applications to PPO and RUDDER. The analysis seems to be an application of the classical theory of convergence of two timescale stochastic approximation (SA), and the asymptotic convergence to a stationary point is presented. \n\nOn the up-side, the paper has presented new analysis that works for general smooth (but possibly non-convex) policy optimization problems, therefore the convergence results hold for any smooth function approximation of the policy.  \n\nHowever, the reviewer believes that the down-side of the paper has outweighed its up-side, as listed below: \n\n- Incremental Contribution\n\nThe paper only managed to show the asymptotic convergence for the AC algorithms, where it is unclear what is the rate of convergence for different methods. Notice that recently the convergence rates of actor-critic methods based on two timescale SA with a controlled Markov process has been shown in a number of works, e.g.,\n\n-- Xu et al., Non-asymptotic Convergence Analysis of Two Time-scale (Natural) Actor-Critic Algorithms, NeurIPS 2020 (appeared on arxiv/2005.03557)\n\n-- Wu et al., A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods, NeurIPS 2020 (appeared on arxiv/2005.01350)\n\nAlthough the above works have not tackled the case of PPO/RUDDER, in comparison the main analysis results from this paper are considerably weak. Moreover, it appears that the analysis results are adopted from the classical work of Karmarker (2017) and Borkar (2008), it is unclear what are the technical innovation involved. \n\n- Local Convergence Analysis\n\nThis paper focuses on local convergence where it is assumed that the iterates shall always stay in a neighborhood of a local minima. This is a strong assumption which is challenging to verify. \n\nParticularly, the authors assumed (L2)-(L3) where the neighborhood has only 1 isolated local minima. It is unclear how these assumptions can be verified in most of the practical problems. Indeed, getting rid of such assumptions is one of the main technical challenges overcome by some of the recent works. \n\nAnother assumption is (A7) which the authors claimed to be achieved by \"weight decay terms in practice\". It is not clear how the assumption can be achieved since the step size has to be non-summable (due to (A4)). The authors may need to elaborate on this matter. \n\n- Readability of the Manuscript \n\nThe reviewer finds it difficult to follow the discussions and analysis in this paper. As a general comment, the authors discussed most of the technical content in a long list of point forms while the central logical flow is missing. In addition, the roles of the control variates $z_1, z_2$ are not explained in the paper - while it is mentioned that they will be discussed in Section A.4, the reviewer failed to find the corresponding discussions. \n\nThere are a number of unclear discussions throughout the paper, to list a few:\n-- on page 2, it is unclear what does it mean by \"time $t$ can be extracted from each state\"\n-- on page 6, in (A5), how is the continuity of the transition kernels of $z_n$ defined? i.e., what is it continuous with respect to?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting analysis of PPO and RUDDER, although theoretical arguments currently lacking some detail",
            "review": "In this paper, the authors consider versions of the PPO and RUDDER algorithms, and set out to prove convergence of these algorithms with non-linear function approximation through the use of two-timescale stochastic approximation theory. The analysis is an important step towards understanding asymptotic properties of methods such as these, but at present I think the paper falls short of presenting fully rigorous convergence proof. I have given detailed descriptions of what I believe are the current issues with the proofs below.\n\nSignificance\n\nThe authors do an excellent job of summarizing related literature on the topic. One notable omission is Jaakola, Jordan & Singh (1994), which the authors may wish to include in a future version of the paper. The paper draws heavily on earlier work for analytical tools, but applies these to the analysis of algorithms which, to my knowledge, have not been studied in this manner before. These algorithms, particularly PPO, are widely used by the RL community, so improved theoretical understanding is likely to be of interest. One notable limitation of the work is the lack of characterization of the convergence points for these algorithms (i.e. what is the quality of the policies that the algorithms converge to)?\n\nClarity\n\nOverall I felt the paper is written with reasonable mathematical clarity. It is difficult to write a theoretical paper like this in conference format, and overall I thought the authors did a good job of providing an 8-page summary of the details of the main proofs, and how their analysis builds off of earlier work. There is quite a lot of forward-referencing earlier in the paper however, and this makes it harder to understand the content of Section 2 without reading Section 3 simultaneously.\nIn addition, something that I think would improve the clarity of the paper a lot is to connect the assumptions that feed into e.g. Corollary 1 with a specific algorithm implementation. For example, exhibiting an agent architecture, optimizer, and environment under which the assumptions of Corollary 1 are satisfied. At present, there is little discussion of which practical instantiations of an algorithm like PPO are covered by the theory, and which are ruled out.\n\n\nTechnical points\n\n\nJust above Theorem 1, it is not clear to me why \\theta* notationally depends on \\theta_0, \\omega_0, when the concept itself (a local minimum of \\theta \\mapsto L_h(\\theta, \\lambda(\\theta))) does not depend on (\\theta_0, \\omega_0). Can the authors explain?\n\nAs far as I am aware, Equation (6) matches the expected loss for the KL-penalty version of PPO, but the gradient estimator will be different from what is commonly used in PPO, since the second term of the expected loss in this paper takes the expectation over the \"training\" parameters \\theta_n, rather than the data generating parameters \\theta_{n-1}. Can the authors comment?\n\nIf I understand correctly, Corollary 1 is specifically for the KL penalty version of PPO, rather than the importance weight clipping version. Can the authors confirm?\n\nAssumption A5: What is meant by \"the transition kernels of z_n\" are continuous? That the kernel is absolutely continuous with respect to some fixed dominating measure on space S? I don't think this will be the case in general, for example in the scenario where the MDP has deterministic rewards. Can the authors comment?\n\nAssumptions (L1)-(L3):\n\nIf I understand correctly, these assumptions rule out many commonly-used architectures, such as those using ReLU units (which do not possess the smoothness required by L1). Additionally, I wasn't convinced by the \"Comment on (L1)\" - this seems to stop short of a formal proof.\n\nI also think there are many architectures for which L2 does not hold too. If for example at a critical point \\omega a ReLU unit is inactive on all possible state inputs to the network, then small perturbations of the weights leading into this unit will not change the loss. Can the authors comment on whether their \"Comment on (L2)\" covers this case?\n\n(A3): The discussion here seems only to discuss states and actions encountered along a trajectory, but I think the rewards are necessary too for constructing estimators of the required gradients. Can the authors comment on whether this adjustment needs to be made to the text?\n\nThe authors mention at several points that using weight decay can ensure that network weights stay uniformly bounded almost surely.\nFirstly, I don't think this claim is generally true without further qualification. For example, considering Baird's counterexample with low levels of weight decay will not prevent divergence of the parameters, so some further argument is required to state why weight decay is sufficient in the cases the authors consider, and also what levels of weight decay are required (i.e. if using L^2 weight decay, what coefficient in front of the regularization term is required, and does this affect the usefulness of the local optima that the algorithm converges to?).\nSecondly, If the authors propose to use weight decay, then this should presumably feature in the definition of the losses in Equations (6)-(9), and therefore be present for the other aspects of the analysis.\nThirdly, as far as I am aware, algorithms such as PPO as described by Schulman et al. (2017) do not use weight decay.\n\n\nMinor comments\n\n\nSection 2.1\n\"Set of finite states\"-> \"finite state space\" etc.\n\nIt is unclear what exactly \\mathcal{P} is -- it is intended to describe a joint distribution over next state and reward, and is written as a probability mass function, but presumably the authors would like to allow for the case where the reward distribution has a density, for example?\n\nA minor comment is that the expectation on the right-hand side of the equation defining \\pi^* is undefined, as an initial state distribution is not specified. Of course, any initial state distribution putting mass on all states will yield the same set of \\pi^*'s, but it is worth being precise about this.\n\n\"We assume that the states are time-aware ... \" this should not be necessary, unless there is some non-Markovian stopping condition for the MDP (e.g. all episodes terminate after 100 steps) - can the authors comment?\n\nThe notation is slightly inconsistent: the gradient of L_h with respect to theta is h, but the gradient of L_g with respect to \\omega is f. Given that this is a notation-heavy paper, the authors could consider whether a more consistent notation scheme is possible.\n\nI felt that the discussion around Equation (15) comes too late in the paper, and should come in Sections 2.2 and 2.3. At present, the reader is left with partial definitions of the systems to be studied until this point. For example, in Equations (6)-(9), it is not clear what version of PPO the authors are studying until they have defined z_n, and its role in e.g. the function \\rho.\n\nSeveral typos to be corrected, e.g.\n\"Markov-policy\" -> \"Markov policy\"\n\"Per sample\" -> \"per-sample\"\n\"Martingale\" -> \"martingale\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}