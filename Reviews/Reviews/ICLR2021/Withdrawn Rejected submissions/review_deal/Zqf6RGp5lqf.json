{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #3",
            "review": "This paper proposes a model for predicting drug-target binding values. It is a GCN in which node representations are given by the concatenation of protein and SMILES embeddings given by a pair of pre-trained BERT models. The results indicate that using a GCN instead of an MLP improves the results and overall the model outperforms the competition.\n\nThe single and main contribution of this paper is using the GCN on top of precomputed node embedding. Basically, they consider the activity prediction as an edge prediction task, which comes with some problems (see below).\n\nOn quality: The idea is simple and well-executed. However, the motivation and the rationale for this particular model is not well explained. For an application paper, the investigations are rather limited. Exploring questions such as the capability to generalize outside of the molecular space and the protein classes considered during training could improve the quality. \n\nOn clarity: The paper is easy to read. However, some details such as the type of GCN, the structure of the graph considered (many scenarios came to my mind while reading), how the embeddings are fine-tuned (LR) are missing. There is also a lack of focus in the related work and at the beginning of section 3 to present results from the BERT papers.\n\nOn originality: IMO, the originality is limited to the use of GCN, but there are many concerns with that. However, the idea has been explored elsewhere for drug-drug interactions, protein-protein interactions. \n\nQuestions/concerns\n\nThe main concern is that considering binding prediction as an edge prediction task works well mainly because of the graph connectivity of the training set that can be leveraged by the GCN to improve predictions. So there is no surprise that the model outperforms other models not leveraging this kind of information. However, the model certainly works well only for proteins and drugs that are seen during training. For a new protein (drug), for which no connectivity information is available, the GCN may fail to improve predictions upon an MLP. \n\nAlso on the choice of pre-trained models, there is little to no justification. What special about those pre-trained models? \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The BERT based model seems to offer good performance, its unclear though how innovative is the model or the ability to support strong claims made within",
            "review": "1. Summary:\nThis paper proposes a deep learning architecture to model the drug-protein interactions. By utilizing the state-of-art BERT-style models, the proposed method improves the representation on both protein and compound sides. In addition, they show the interpretability analysis based on the attention mechanism of BERT. With the high-quality representation, their model out-performs the affinity prediction task over other methods. The main contribution of this paper is the combination of pre-trained BERT-style models and utilizing the graph neural network to process the representations. Results are shown on the task of 1) Drug-Protein interaction prediction 2) interpretability analysis\n\n2. Pros:\nThe authors nicely demonstrate improvements of representation in latent space using t-SNE plots. It shows the advantages of using the BERT models, which makes similar components/tokens have more similar embeddings.\n\nThey propose an interesting method to encode the SMILES by borrowing the idea from data compression. It helps to reduce the total length on the compound side and further decrease the model complexity.\n\nThe proposed model provides some impressive performance on the affinity prediction part, especially the Davis dataset results.\n\nThe paper analyzes the advantage of combining GCN by replacing it with MLP. It helps to better understand the model performance.\n\n3. Cons:\nThe results included in the paper do not support very well the strong claims made:\nThe authors first claim that their approach is able to encode any novel drug-protein. However, they don't have the results from a new protein/compound or new dataset. In addition, we were also concerned whether the model will have trouble encoding data not even in the pre-training dataset.\n\nThe second claim is about “analyse and determine the essential features”. In our understanding, it means the interpretability of the model. However, the paper doesn’t show either the correctness of their attention results or the usefulness of those results. Figure 3 and 4 take a big part of the paper to discuss the attention results but we don’t see what conclusion we can draw from those. In another word, the authors didn’t show if their attention results are correct compared to the ground truth binding sites and if those results are biologically meaningful. \n\nRelative lack of novelty. This paper borrows the BERT and RoBERTa from other papers to do the representations. Then they use the GCN followed by several fully connected layers to provide the final affinity output. The biggest novelty here to our understanding, ML wise, is treating the representations as graphs and utilizing GCN atop to process. \n\nSMILES format is a big problem for interpretability. As mentioned in the paper, the attention mechanism focuses on the special characters and occasionally some numbers in SMILES. It is interpreted as “progressively learns the overall structure of the drug”, which is problematic. A good attention should provide more meaningful interpretation instead of only helping to improve the affinity result. For the SMILES format, it is really hard to avoid the situation of focusing on meaningless parts.\n\nWeak results on KIBA dataset. The model performs very well on the Davis dataset but only improves slightly on KIBA. It will be interesting to set some experiments and understand the underlying reasons. Is that related to the sample size? To #proteins vs #drugs? These differences are not mentioned or discussed in the paper. \n\nMinor Comments:\n=================\nTypo: “respectfully” should be “respectively”\n\n“Deep learning is now a prominent application of machine learning,” p. 3 - the term “application” is awkward here.\n\nTerm/model “DeepDTA” p.3 not defined before using it.\n\n“Within the in SMILES” p. 5\n\n“Citetozturk2018deepdta” bottom p. 5\n\n“However, to capitalise on the pre-training that was conducted for both BERT networks.” p.7 - sentence seems cut.\n\nIn Figure 1, it’s not clear why we still get separate encodings for drug and protein after the “Node Embeddings”. More generally, it’s hard to follow Fig1 to get an understanding of the overall model structure. Better description of the process in the main text would have helped here.\n\nIn Figure 2, it isn't very clear that “carbon-based molecules cluster close to the carbon atom” for drug embeddings results. Instead, some carbon-based ones are closer to other characters, like “NC” is very close to “S” and “1”.\n\nIn Figure 5 and 6, should the y axis labels be (GCN-BERT) and (BERT) instead of the reverse? If the current label is correct, then it seems the BERT clusters much better than the GCN-BERT for Davis, which is contradictory to the discussion in the paper.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A framework leveraging transformer models to produce sequence embeddings and GCN to model drug-target binding affinity with application in drug discovery. Novelty of the approach is limited but good experimental results are obtained in two separate datasets.",
            "review": "Overall comments \n- Two independent BERT-style transformer models are trained to provide embeddings for protein and drug sequences respectively. These embeddings are then fed into a GCN-based network to predict the binding affinity for the pair.\n- The idea of using embeddings learned via large deep nets is very sensible and mitigates issues that were plaguing the approaches relying on hand-engineered features.\n- However that idea is not new. As noted by the authors, several papers have already leveraged very similar architectures (e.g., Oztuk et al, 2018 and Shin et al., 2019).\n- Compared with the latest of these works (Shin et al.), the main innovation introduced in this work is rather limited: the CNN model used to get protein embeddings is replaced with a Transformer model, and the MLP used to predict drug-target affinity is replaced with a GCN-based architecture.\n- The introduced changes are well motivated by the authors (e.g., ability of the transformer to capture longer range dependencies in the protein sequence) and lead to superior performance in the two distinct experimental setups covered.\n- Authors demonstrated good knowledge of existing literature and good care was taken in certain modeling aspects (e.g., using BPE tokenization to process the protein sequences).\n- Several modeling decisions along the way are not well justified (see detailed comments) and many implementation details are missing (e.g., architecture details, training parameters). As such the work is not reproducible and the submission would have benefited from an appendix listing all these elements.\n- The paper is overall easy to read and follow, but there are many typos, missing words, style issues and inconsistencies throughout (see detailed feedback below).\n\nDetailed comments / questions\n- Why did you select this particular architecture for the protein Transformer model? For example, the architecture seems closely related to the one in the TAPE paper (Rao et al.), but uses a different number of attention heads (8 for TAPE Vs 12 for yours). Did you perform an ablation study to compare the effect of different design decisions?  \n- Why did you use a BERT architecture for the protein embeddings Vs a Roberta architecture for the drug embeddings?\n- Why did you perform BPE tokenization for the drug Transformer but not for the protein Transformer? See for example Nambiar et al. (https://www.biorxiv.org/content/10.1101/2020.06.15.153643v1) for a use of BPE in protein transformer models.\n- Did you draw any conclusion from the rightmost plots in fig 2a. and fig. 2b (token position in sequence)?\n- In figures #3 and #4, how did you select the example drug-protein pair? Do these observations translate to other pairs as well?\n- What are the characteristics for your drug Roberta model (# of layers, heads, etc)?\n- Which specific operation is performed by your pooling layer?\n- What are the details of your learning procedure (e.g., batch size, learning rate schedule, # of training epochs or stopping criterion, etc), for the two transformers and the GNN?\n- Could you describe a bit more the “BERT-MLP” approach (e.g., how many layers, sizes, types of activations)? In particular, is it comparable to the GCN approach in terms of # of parameters?\n\nMinor points / typos\n- Section1, second to last paragraph: “multiply layer perceptron” should be “multiple layer perceptron”\n- Section 1 last paragraph: “End-to-end, our approach will be able to encode any drug-protein even it is not present within the original datasets” → missing a “if” and it is not clear which datasets you are referencing\n- Section 2 last paragraph: “(Duvenaud et al., 2015) implemented a GCN” → you should replace \\citep{} by \\citet{} here (assuming you are using natbib). You have this issue several times in the same paragraph, and should be fixed as per the ICLR style guide.\n- Section 2 last paragraph: “(Coley et al., 2019) use a GCN based approach to model the interactions between organic reactions to predict the products” → You seem to be a missing one word here (perhaps “model the interactions between reactants of organic reactions to predict the products”).\n- Section 3, bottom of p.4: “A set of additional tokens were also included within this vocabulary (i.e. to denote special tokens for unknown characters, padding, separation and masked characters) such as to avoid unknown tokens during our pre-training stage.” → need to remove redundancies in the sentence\n- Section 3, middle of p.5: “which is in line with the finds from” → “findings”\n- Section 3.1: “As in previously work” → “previous work”\n- Section 3.1 “for the drug and protein model respectfully” → “respectively” (you have the same error in the last paragraph of the conclusion)\n- Section 3.2: “Following citetozturk2018deepdta” → typo in latex\n- Figure 1 -- the “encoder / decoder” terminology is confusing here -- would advise calling that something else \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for: Modelling Drug-Target Binding Affinity using a BERT based Graph Neural network",
            "review": "The authors address the problem of predicting drug-(protein) target binding affinity, which is critical for drug discovery. This is framed as a regression problem where the goal is to predict a measure of experimentally obtained binding affinity (e.g. Kd). The approach is divided into two main tasks. The first task consists of pre-training unsupervised BERT-style models that aims to learn low-dimensional feature vectors (embeddings) for drugs and targets from their chemical and sequence string representations, respectively. The second task uses the pre-trained embeddings as node features in a graph representation learning framework (GCN) to train a supervised learning model for the regression task. The authors also perform interpretability analysis of the model based on the attention mechanism and the embeddings.\n\nThe manuscript does not seem to introduce a new machine learning model, but focuses on application of state-of-the-art architectures, such as BERT and GCN, to the problem of predicting drug target affinity -- I think it could be valuable for ICLR.\n\nMajor concerns\n- This paper is not reproducible. The architecture presented in Figure 1 lacks details in the explanation. The operations performed by each part of the architecture are not clearly described, e.g. what operation is performed in the GCN box? What are the dimensionalities of the different variables throughout the architecture? How is the input feature represented? It is also unclear which graph is used in the GCN architecture.\n- The interpretability analysis performed in this paper is unclear. The authors claim that their model is interpretable by showing visualizations of the embeddings and the attention mechanisms of the BERT-style models (Figs. 2, 3, 4, 5 and 6). However, it is unclear what the message of these figures is. Which datapoints are drugs and which ones are targets in Figs. 5-6?\n- The authors claim that their embedding is robust. However, this is not supported by any experiment. The authors should better clarify and support this claim.\n- The authors should motivate better the choice of the type of BERT architecture in their framework. Why is RoBERTa, and not BERT, used to encode chemical representations? Or vice versa, why is BERT, and not RoBERTa, used to encode protein sequences?\n\nI should also add that I found the paper a bit difficult to follow and it is not self-contained.\n\n\nMinor points\n- pKd is not defined anywhere in the paper, but used in the results section.\n- The terminology “drug sequence” is not common in computational pharmacology. I suggest the authors using drug SMILES notation instead.\n- The authors need to proofread the manuscript for clarity, grammar and correctness. Some typos that I found while reading: \n“respectfully” is used instead of “respectively”.\n- the citation at the beginning of section Tasks (“citeozturk2018deepdta”).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}