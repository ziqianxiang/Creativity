{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "I thank the authors both for going the extra mile in doing further experiments for their response, and making the efforts to synthesize the main comments and concerns of the reviewers.\n\nOverall, I'm pretty sympathetic to the idea that syntactic and semantic representations should be very helpful to learning sentence embeddings. They provide a form of scaffolding. But a reviewer notes and I think anyone will admit in 2020 that contextual language models like BERT also provide much of this scaffolding, and it falls to the paper author to provide convincing evidence that using external parsers is valuable and necessary in this quest. In this, the current paper seems to fall somewhat short.\n\nPros:\n - Clearly and honestly written paper\n - Good exploration of value of constituency & dependency parse representations\n - Exploits recent work in contrastive learning\n\nCons:\n - Insufficient novelty\n - Experimental comparisons not well controlled – too much apples and oranges. \n - No comparisons of inference speed tradeoffs\n - Value of exploiting explicit syntax is too much assumed rather than explored\n - It's not established that use of explicit syntax really delivers versus alternatives such as contextual language models\n\nSeveral of the reviewers felt that this paper was a fairly limited extension of L & L 2018, without any clearly novel contribution. The issue of comparability in results is complicated. There is a reason to move to a new standard corpus, rather than privileged people passing around archived copies of the old BooksCorpus, and I think your additional experiments show the results are \"near enough\" but there would still be much more archival value in a new paper having a set of comparable results on a new corpus. The big question of whether to do this or use BERT is better addressed in your additional experiments presenting a random projection of BERT to a comparable higher dimensional space. But unfortunately these results further weaken the clarity of the case for needing to head in the direction of this paper rather than just using a large pre-trained contextual LM."
    },
    "Reviews": [
        {
            "title": "lack of justification and not convincing results",
            "review": "The paper proposed an ensemble of sentence encoders (RNN, BoW, tree-based) that is trained with contrastive loss. The proposed approach is a simple extension of the work of Logeswaran & Lee (2018). Overall, I think the paper has several major weaknesses. \n\nFirst, the results on SentEval are not convincing in comparison to other general purpose methods (i.e., BERT or QuickThought) which do not require a parsed tree during training and inference. The results also do not justify the use of additional resources (dependency and constituency parsers) as it complicated the models and is not applicable for many other languages where parsers are not available or their performances are still far from usable. Note that, there are several pre-trained models after BERT (e.g., XLNet, XLM-R, ALBERT,...) which can be used to obtain sentence embeddings. I think the authors should also compare the results with those models.\n\nSecond, SentEval framework suffers from some drawbacks [1]. Using it alone is insufficient to draw any meaningful conclusion about sentence embeddings. I think that the authors could have used probing tasks [2] to evaluate their sentence embeddings with respect to linguistic properties.\n\nThird, while the paper claims posed a hypothesis that structure is crucial to build consistent representations, this is not shown in the paper. I think that the paper lacks some analysis/examples  to show in which cases modelling structure of the sentence explicitly is necessary. Other models that use Transformers or RNN exploit structure in language implicitly in one way or another. For example, Hewitt and Manning shows that syntax is embedded in deep models (i.e., BERT). Thus It’s not clear to me what is the advantage of the proposed approach in comparison to other models.\n\nComments for the authors:  I think that the comparison with previous work in table 1 is not meaningful since the authors used another dataset for training.\n\n\n\n[1] [Pitfalls in the Evaluation of Sentence Embeddings](https://www.aclweb.org/anthology/W19-4308/). Steffen Eger, Andreas Rücklé, Iryna Gurevych. RepL4NLP-2019\n\n[2] [What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties](https://www.aclweb.org/anthology/P18-1198/). Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni. ACL 2018\n\n[3] [A Structural Probe for Finding Syntax in Word Representations](https://www.aclweb.org/anthology/N19-1419/). John Hewitt Christopher D. Manning. NAACL 2019\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Is explicit syntax really necessary?",
            "review": "The authors introduce a pretrianing paradigm based on contrastive learning between multiple syntactic views of the same sentence. The method maximizes representations between different setence encoders when given the same sentence, and minimize the similarity to all other sentence repre sentations. The results on the infersent benchmark show competitive performance of the approach when compared to non-syntactic pretraining methods.\n\nThere are a couple of concerns I have with the paper and some questions I will elaborate on in detail. In particular, this paper doesn't convince me that incorporating explicit structure is i) desirable and ii) necessary to achieve good results on the chosen benchmarks. I also have doubts about the comparison and the bechmark in general, especially given the fact that random encoders (wo/ explicit syntax) on pretrained word embeddings can do a pretty good job on those [2]. \n\nTherefore, at the current state of the paper I cannot recommend acceptance.\n\nDetailed comments:\n\n1) Are explicit, human designed syntax frameworks really needed? The authors argue that explicit syntax can help generalization, which I could maybe agree with, if trained parsers were perfect. Since they themselves are far from that and regularly fail on complicated sentence structures, I don't believe that this will assist in generalization. Rather the errors in parsing can lead to more systematic failures that the models might have a harder time to correct downstream. Prior work shows that Transformers trained on a (masked) LM objective learn about sentence structure implicitly [1]. In fact, although the authors \"hypothesize\" that explicit structure can help -- which they do not properly ablate in my opinion (see 2-3) --, the results in the paper show no clear advantages on their benchmarks when comparing to BERT. The increased complexity of the method (ie., relying on other trained models) should be warrented by showing a clear advantage in doing so.\n\n2) Fair comparisons: The models in the paper are trained on another pretraining corpus. Other baselines should have been trained on this corpus to have an apples to apples comparison. Furthermore, BERT models use much lower dimensional sentence embeddings which typically hurts performance. A way to make comparison fairer would be to apply a random projection of the 768 features to 4800 features. Note that this can have a dramatic impact even for BoW models on these benchmarks [2]. I also wonder how well a randomly initialized model would perform, that is, how much does the pretraining actually help? [2] shows that it might not be required at all. I think given the inductive bias coming from the parser and the use of TreeLSTMs, this effect might even be greater, because much of the information about the structure sentence is given apriori, which might even help random models to achieve better performance.\n\n3) What can we expect the pretraining to learn? For instance, wouldn't a satisfactory solution to the pretraining task for the 2 encoders to simply learn a bag-of-words representation? I believe that the large majority of the setences could easily be distinguished by that. Just memorize the exact words that occured in the sentence. I am not convinced from the paper that the model learns anything more semantic than that. More rigorous ablations are required to show the benefit of the pretraining method.\n\n\nMy personal opinion on introducing explicit syntax into neural nets (note that this will not be part of my decision): \n\nGiven all the evidence we have so far on pretraining LMs using generic model such as transformers, I just don't see any reason why we should still try to explicitly fit our potentially faulty and biased syntactic frameworks into neural networks when more generic models (such as transformers) can learn structure directly from the data. Our linguistic frameworks can still be super useful for understanding and systematically testing our models, but I think we should refrain from  introducing our potentially limited understanding of language into those models. A random thought: How would pretrained parsers help a model on twitter like text or child speech? \n\n\n[1] Jawahar et al., What Does BERT Learn about the Structure of Language? ACL, 2019.\n\n[2] Wieting et al., NO TRAINING REQUIRED: EXPLORING RANDOM ENCODERS FOR SENTENCE CLASSIFICATION. ICLR 2019.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "\nThank the authors for showing their effort in revising the paper.\nSome of my concerns have been solved thanks to the rebuttal.\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n\n#### Summary\n\n- This paper proposes a self-supervised (multi-view learning) method that constructs sentence embeddings by exploiting different types of sentence encoders, including tree encoders based on constituency and dependency trees. The intuition is that the integration of respective representations from disparate (linguistically-informed) encoders is effective since each linguistic formalism can provide different linguistic views. In experiments, the proposed method was tested on the SentEval benchmark and demonstrated its effectiveness compared to other baselines. The authors also conducted some qualitative analyses to reveal the inner working of their method.\n\n#### Pros (Reasons to Accept)\n\n- The introduction of (constituency and dependency) tree encoders to sentence embedding construction from the perspective of multi-view learning.\n\n#### Cons (Reasons to Reject)\n\n- Weak novelty: This work attempts to improve upon Logeswaran & Lee (2018) (so-called 'Quickthoughts') by applying tree (and some other) encoders in addition to sequential RNN encoders. However, it is hard to find a novel technical contribution that only appears in this paper, except that the authors demonstrated that the simple combination of two well-known concepts (i.e., Quickthoughts and tree encoders) is empirically effective for building sentence embedding. Even though the results presented in the paper is interesting, in my opinion, there should be a stronger contribution that leads to this paper to be unique and informative.\n- Doubts on the practical usefulness of the proposed sentence embeddings: Compared against typical frameworks that accept just plain sentences as input, the proposed framework requires input sentences to be parsed by external parsers, which might be problematic: Specifically, (1) the reliance on external parsers may cause unexpected error propagations from the parsers, considering that existing parsers are not perfect (in spite of recent improvements in developing good parsers). Moreover, reliable parsers are only available for a few resource-rich languages such as English. Therefore, some concerns about how to parse input sentences appropriately in different (or harsh) conditions should be (at least, briefly) covered in the paper. (2) As tree encoders are hard to be optimized with batch computations, there must be a decrease in inference speed if a model includes tree encoders. Considering this problem, the paper would be much persuasive when the inference speed of each compared model (in addition to its training time) is provided in experiments.\n\n#### Comments\n\n- I find myself being hard to believe that the comparison shown in Table 1 is fair enough, considering the following concerns. First, there are two factors that may impact a model's performance---(A) the architecture of the model and (B) the data used for training the model. Even though (A) is controlled in the presented experiments, (B) is not. In other words, readers cannot infer whether the performacne of a model is due to its architecture or its training data. I understand it's difficult and time-consuming to re-train all baselines. Nonetheless, the proposed model should be at least compared with the Quickthoughts model, which is the foundation of this paper, in the same condition for a fair comparison.\n- To make the paper more convincing, I recommend that the authors test the BERT model in the condition where the dimensionality of the BERT embedding is expanded to larger numbers, making it comparable to those of other models.  \nOne possible solution is to follow the method proposed in John Wieting & Douwe Kiela (2019).\nFor more details, please refer to John Wieting & Douwe Kiela, No Training Required: Exploring Random Encoders for Sentence Classification (ICLR 2019).\n\n#### Misc.\n\n- (Typo) Section 3.1. encodes -> encoders \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}