{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper studies offline meta reinforcement learning. Overall the scope of this contribution seems limited. Reviewers have raised concerns about the significance of the presented results given the assumptions, and that the experimental environments are not extensive and do not fully support the claimed advances. "
    },
    "Reviews": [
        {
            "title": "I have concerns regarding assumptions made by the approach and experimental evidence",
            "review": "Summary:\nThis submission studies the meta-learning problem in RL under offline settings. A new algorithm is proposed to address this problem by extending the recent VariBAD algorithm designed for online meta-RL. The key modifications to adapt the original VariBAD to offline settings are the state re-labelling and reward re-labelling tricks, which aim at addressing the MDP ambiguity specifically showing up in offline meta-RL. Experiments are conducted on four sparse reward tasks under both offline and online settings, comparing with PEARL (a recent online approach) and the original VariBAD respectively.\n\nPros:\n1. The problem studied is interesting and important in RL, and largely open.\n\n2. The proposed tricks for adapting VariBAD to offline settings are simple and easy to implement.\n\n3. The ablation study on the Ant-Semi-circle task is helpful.\n\nCons:\n1. Regarding the reward re-labelling trick, the assumption that reward functions for all training environments are known limits the applicability of the proposed algorithm for real-world problems.\n\n2. Regarding the experiments comparing with online PEARL, the training curves of PEARL in Fig. 3 do not seem correct to me, it is strange that PEARL does not make any progress with training, which is very different from the behavior reported in the original PEARL paper.\n\n3. Regarding the soundness of comparison experiments, it would be much more convincing if the following things are taken care of:\n\n (a) All four tasks reported in the original VariBAD paper are included, rather than just picking one from them. \n\n (b) Averaging performance is reported over 5 runs rather than just 3, especially considering that SAC could perform quite differently between different random seeds.\n\n (c) Hyper parameters are set to be more consistent for the same RL backbone (say SAC) across different tasks. It is understandable that two sets of parameters might be needed for online and offline settings.\n\nBased on my main concerns regarding the assumptions on reward function and soundness/correctness of experiments, I lean towards a rejection.\n\nOther questions:\n\n1. Regarding the state re-labelling trick, I have a question regarding the proof of Proposition 1: the first equality of the proof in Appendix A does not seem straightforward to me. It would be helpful if more derivation details are provided.\n\n2. I feel that the claim of approximating Bayes-optimal policy in the offline setting is overly stated. Is there any explicit arguments showing that this is \"approximately\" true?\n\n====================================================\nPost rebuttal:\n\nMy concerns regarding the experiments are mostly addressed, though as pointed out by other reviewers, more convincing experiments under changing transition dynamics would be very helpful. I also stand by the authors explanation regarding limited resource in running RL experiments, especially for novel research directions.\n\nThat said, for the same reason (pioneering research vs large-scale application), I do not agree with the authors explanation regarding the limitation of the proposed approach in assuming a known ground truth reward function. The main contribution of this submission is not in solving a specific real-world RL application problem as the cited references. As one of the first efforts in addressing the meta-RL under offline setting, I feel that setting this constraint is a limitation and should be relaxed by means of estimating the reward function. This should be addressed in future work.\n\nI raise my rating to a weak acceptance conditioned on the wording regarding \"Bayes optimal\" being more precisely presented, I think it is currently over-stated throughout the paper, which could be misleading to the community if published as it is.\n\nIt is important to carefully reword in which sense the proposed algorithm \"approximate Bayes-optimal policy\", as explained in the authors' response, the algorithm is shown to qualitatively behave in a way that a Bayes optimal policy would do under this particular setup, this is far from sufficient to claim any approximation to the Bayes-optimality in a principled sense. I would like to also point out that while Bayes-optimality is generally intractable, it is possible and not uncommon for a method to start from an explicit pursue of Bayes optimal solution and specify where and how approximations are performed to overcome specific intractibilities, and further show quantitatively that such Bayes optimality is indeed achievable under well-controlled toy examples where the true Bayes optimal solution is known. The concept of Bayes optimality is in essence quantitative, rather than qualitative.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for OFFLINE META LEARNING OF EXPLORATION",
            "review": "Summary: \n\nThe paper proposes an extension of VariBAD to the offline setting. The main difficulty of such an application in an offline setting is what the paper termed \"MDP ambiguity\" while proposing reward relabeling as a solution. \n\n#########################################################\n\n Pros: \n\n1. The paper is clear and well-written. \n2. It proposes a simple but effective solution to adapt VariBAD to the offline setting for a type of environments  where transition probabilities and reward probabilities are independent. \n3. The paper demonstrates very interesting learned exploration behavior and shows its superior empirical performance compared to baselines (VariBAD and PEARL). \n\n#########################################################\n\nCons: \n\n1. The test environments are not extensive: all 4 test environments used in the paper are quite simple and 2 out of 4 are even similar. What about other openai gym control tasks (or Atari games)? Given the strong independence assumption between transition and reward probabilities, it would be nice to demonstrate how resilient this method is on environments that violate this assumption. \n\n2. There are few baselines being compared against in this paper so it's unclear given some offline data from an unknown environment this method is the sota. For example, one potential area of comparison is model-based RL agents where the prediction models (or even transition models if the environment is simple enough) are trained with supervised learning using offline data.\n\n#########################################################\n\n Questions during rebuttal period: \n\n1. Why is the top right slot of Table 1 blank? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Offline Meta-RL using a variational approach coupled with state and reward relabeling. Discusses an important setting, and some new algorithmic ideas therein, but whether these ideas will work in practice is inconclusive. ",
            "review": "The paper studies the problem of offline Meta-Reinforcement Learning (RL). In this problem, N separate RL environments are considered which are drawn from a specific underlying distribution. For each such environment, M trajectories each of horizon H are provided beforehand. The task is to train an RL agent that performs well in expectation on a new RL environment drawn from the same distribution.  The authors adapt the online VariBAD algorithm (Zintgraf et al., 2020) by the use of a techniques called state relabeling and reward relabeling. \n\nThe VariBAD algorithm formulates online Meta-RL as a Bayesian Adaptive MDP (BAMDP) and solves it approximately using a variational approach. The authors, due to lack of online data, use partial offline trajectories to create belief over RL environment. Next, this belief is used to map offline Meta-RL to the BAMDP setting -- a.k.a. state relabelling. Once mapped to BAMDP off-the-shelf offline RL techniques are used to solve the problem approximately. The authors further introduce reward relabelling that adds new trajectories by mixing rewards among the RL environments conditioned on the state and action. This is claimed to be useful if the reward distribution is independent of the transition matrix distribution of the RL. \n\nThe authors provide multiple experiments with sparse rewards to show the VariBAD with relabelling works in some established benchmark RL tasks (albeit in the new offline Meta-RL setting). \n\nPros:\n* The offline Meta-RL is a very relevant problem with a lot of applications in the coming years. Designing a methodology to solve this problem using off-the-shelf techniques is a novel pursuit, which is carried out in the paper. \n\n* The idea of state and reward relabeling is novel in the meta-RL setup, as per my understanding (as an outsider to the Meta-RL community but with a background in RL).\n\nCons:\n1. Methodology: \n\n* The paper does not explore properly the (in)-famous \"deadly triad\" which is known to make offline RL difficult. Given the state-space of BAMDP captures beliefs over (possibly) complicated RL environments, the effect of  \"deadly triad\" is conceivably even more prominent in the current formulation.  \n\n* I am not sure why the reward relabeling is able to solve the MDP (a.k.a. aliasing in the offline RL community). In particular, \"Note that our relabelling effectively samples data from an MDP with transitions Pi and reward Ri, which has non-zero prior probability mass under the decoupled prior assumption\" -- the above statement is very vague given finite trajectories. The MDP aliasing (a part of the deadly triad) is closely related to this issue. \n\n* The state relabelling is heavily reliant on the existing VariBAD algorithm, and off-the-shelf RL techniques. Therefore, the algorithmic contribution seems somewhat lacking in my opinion. \n\n2. Experiments:\n* The provided experiments lack any conclusive evidence of the effectiveness of reward-labeling. Why in the Half-Cheetah-Vel experiment reward relabeling performs worse -- is the reward independence assumption not true here?  Will reward relabeling always perform worse? \n\n* The effectiveness of the Thompson sampling baseline used is not clear to me. Does no other simple baseline exist? I am alright with not comparing it with the concurrent works. However, such a comparison would have been much more convincing. \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good submission, but central claim supported by evidence",
            "review": "Summary after discussion period:\n----------------------------------------------\nThe authors have done a good job in toning down their claims to match what the evidence supports. After reading the other review's comments and the updated version of the paper, I feel both my comments and most of the reivewer's comments have been addressed, allowing me to recommend this paper for acceptance.\n\nSummary\n-------------\n\nIn this paper, the authors tackle the problem of offline meta RL, where one observes trajectories from environments drawn from a distribution of environments, then using these past observations finds a policy which will work well on new environments drawn from the same distribution, optimizing both in the face of uncertainty about the environment and about the random transitions. The authors propose a method based on VariBAD which achieves good experimental results.\n\nPros:\n-------\n\nI find the paper to be novel, relevant and well written, a good contribution to the scientific community. I liked it’s introduction to the idea of offline Meta-RL, and thought the experimental results where an agent learns to walk the semi-circle were impressive. Further, I greatly enjoyed the discussion of MDP ambiguity, it’s a genuine problem that I’d never seen before and I think addressing exactly this issue will be important in advancing offline meta-RL.\n\nMajor Concern:\n--------------------\n\nMy major concern is that the evidence does not fully support the author’s central claim: “quickly maximize reward in a new, unseen task from the same distribution” where this distribution is described as varying over both the reward and transition functions. Although I do believe that the author’s method addresses varying reward structures, they have provided no evidence that the method addresses varying transition dynamics. In their experiments, they sample different MDPs, but each has the same transition dynamics, only the reward dynamics vary.\n\nPlease do not misunderstand me, I believe that science is incremental, small steps are valuable, and having a method which addresses only the smaller problem of fixed-transition dynamics with sampled reward dynamics is already an interesting and valuable contribution. Yet I don’t feel anyone is served by a paper which claims it solves the larger problem of offline Meta-RL, but doesn’t provide evidence for this larger claim.\n\nTherefore I would request the authors either provide evidence for the larger claim, or (it’s probably the easier route) simply make a smaller claim that is supported by the evidence, and save the real offline Meta-RL problem for a future submission. This would be that one is interested in the setting where MDPs with varying reward structure are sampled.\n\nI believe that the problem of MDP ambiguity can’t be so easily solved for ambiguous state transitions. In section 3.3, the authors nicely address the issue of MDP ambiguity, (it was a very insightful section, a valuable contribution) and that it’s impossible to know if two trajectories come from different MDPs with different rewards, or one MDP with rewards at both locations. It’s an inherent counterfactual problem, one doesn’t know what would have happened if one had done something else.\n\nThe authors fill in the gaps in this counterfactual knowledge by assuming knowledge of the true reward structure from each past MDP. In this way, for all past trajectories the authors can say what would have happened if the same actions had been taken in a setting with a different rewards structure.\n\nYet the problem of MDP ambiguity isn’t restricted to unknown reward structures. Assume a meta MDP setting where each MDP is a gridworld with a key and a door and the agent collects reward after getting the key then going to the door, thus the state is the agent’s position and a binary variable indicating whether the agent has collected the key. For different MDPs in the distribution, the key is placed in different places. In this way, the reward structure is the same for different MDPs in the distribution, but the transition structure (where the agent collects the key) changes. I don’t see how the author’s proposed MDP ambiguity fix would address ambiguity in the transition dynamics, or even provide a roadmap for addressing this issue.\n\nIndeed, it is telling that the experiments only deal with Meta-RL where the reward distribution changes, and never address changing transition dynamics.\n\nTherefore I would ask the authors to clarify the scope of their contribution, and perhaps address what challenges stand in the way of offline Meta-RL where the state transitions vary too. After that, I'd be happy to update my review.\n\nMinor Concerns:\n----------------------\nIn Prop. 1, why do we have $E_{\\mathcal R, \\mathcal P\\sim b_t}$, it seems one must only sample $\\mathcal P$ in the first line, and $\\mathcal R$ in the second.\n\n“Recall that the VariBAD VAE encoder….” perhaps drop the word ‘encoder’, encoder is already in the ‘E’ of VAE.\n\nIn section 3.3, when describing figure 2 (also in the caption of figure two) you say the reward is in the yellow circle, but the circle looks reddish on my PDF viewer.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}