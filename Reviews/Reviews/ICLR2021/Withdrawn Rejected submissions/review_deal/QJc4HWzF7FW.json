{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting directions but clarity and empirical results are not good enough ",
            "review": "The paper proposes a meta-continual learning method derived from dynamic programming. The paper frames the MCL as a PDE problem in theory and proposes discrete approximation as the methodology \nin practice.\n\nViewing MCL from dynamic programming is a novel perspective and it is nice to see previous methods focusing on different parts of the proposed objective. A unified framework would further help understand the meta-learning with sequential data. \n\nHowever, the current paper lacks clarity for a broad range of readers. Here are some suggestions to improve it.\n\n— What may be compressed:\n- The statement and the role of theorem 1 are not clear. For statement: the derivative of V is a scalar, why it is said to be negative semi-definite? What does the “Let … be …” mean? Is it an assumption, fact, or notation? \n\n— What needs to be elaborated:\n- The paper seems to skip an important step to explain the connection between minimizing the cumulative cost (as in Eq. (2)) and computing the partial derivative of the minimizer (as in Eq.(3)). Are they equivalent? \n- The methodology on page 5 needed more explanations. Is there a specific reason to update the representation learning parameter \\hat{\\theta}_1 and  prediction parameter \\hat{\\theta}_2 seperately? How does it work if  \\hat{\\theta} = [\\hat{\\theta}_1, \\hat{\\theta}_2] are updated jointly? The explanation by Markov chain is not clear. \n- Some heuristics in the algorithm such as user-defined parameters \\zeta and \\beta are not clear how to set.\n- The connection DPMCL to previous work is currently explained in a very vague way. \n\nOther comments:\n- A key issue of MCL by previous works mentioned in the introduction is that it requires knowing data for all tasks in advance. For the proposed method, does it also need future tasks to compute the generalization loss J_P? How does it differ from previous work?\n- The learning rate \\alpha(t) below Thm 1 seems can be negative.\n- Some curves are missing in the panels of Figure 2. And DPMCL is worse than Naive on NTE in certain cases? \n- Why in Lemma 1 it assumes the loss function l(\\tau) to be lower bounded by \\epsilon? \n\nIn general, the direction this paper explores is interesting but the clarity needs to be improved. The intuition and main contributions are submerged in technical details. The paper aims for a theoretical sound framework for MCL yet it is weakened by many heuristics applied in practice. In addition, the empirical results especially for new tasks are not strong. So in general I rate it as “Ok but not good enough”. \n\n=====POST-REBUTTAL COMMENTS======== \n\nSince the author did not submit a response, the original decision is retained. (Update: the author did submit the comments after this initial response.)\n\n=====POST-REBUTTAL COMMENTS 2======== \n\nI have read the author's updates. The readability of the paper has improved in the revised version but it is not clear yet why DPMCL unifies several previous works and which change contributes to the improvements in practice. Besides, there are multiple undirected references in the appendix. I think this work needs further revision and the experiments need some ablation study before getting ready to be published, therefore I maintain my rating. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "3 main concerns",
            "review": "## Post-discussion update\n\n### Concern 1 \nMost of the issues in concern#1 have been addressed adequately in the updated draft. I disagree with the following response:\n\n\"Dyna Architectures are one of the first approaches that integrated experience replay into the learning process. It was one of the first works that addressed continual learning in the reinforcement learning setting without calling it so. We the authors feel that it is relevant to observe how the problem of learning was incorporated into the model as one of the first memory/experience replay driven approaches. \" \n\nThis is simply incorrect. The fact that future research has made connections been replay buffers and Dyna (e.g., in the \"How to use parametric Models paper\") does not mean that the Dyna architecture had anything to do with continual learning or experience replay. The motivation of Dyna was to incorporate planning in learning in a way that allows the agent to follow a reactive policy. This is in-contrast to planning algorithms of that time that relied on decision time planning (e.g., optimal control). \n\n\n### Concern 3 \n\nI see what the authors mean by returning a different set of parameters at each time, and I agree that earlier examples are not being counted multiple times, as I mistakenly suggested. The clarified definition has its own issues, however. If we formalize the MCL problem as finding a new set of parameters at each time-step, then it makes sense to define the goal of $\\theta(t)$ to make a correct prediction for $x(t)$ and $x(t)$ only. Why should $\\theta(t)$ aim to minimize cost on previously seen data-points?  If $\\theta(t)$ only aimed to minimize cost at $x(t)$, the resulting objective would be similar to the good old online regret minimization objective. To me, the new objective in equation 2 is worse than the online regret minimization objective when it comes to online learning. After all, what we really care about is prediction error on future inputs. It's entirely possible that the authors have some good motivations for defining the goal as they do in equation 2, but it's not clear to me what those motivations are from the write-up. \n\n\"This is also a misunderstanding. For each t we define the gamma in such a way that it monotonically decreases from the current task to older tasks. In particular, the current task is provided a larger weight and the older task is given smaller weight. We have made this definition precise in Lemma 1 and corollary in the revised version. Due to space constraints, the lemma and corollary statements with their proof have been moved to Appendix A.\" \n\nI wouldn't say this was just a misunderstanding. The earlier draft of the paper said \"Let $\\gamma(t)$ be a monotonically decreasing sequence such that $\\gamma(t) \\to 0 $ as $t \\to \\infty$ whereas the updated draft says $\\gamma(t) \\to 1 $ as $t \\to \\infty$. Both would make the integral converge, but would result in very different solutions. Moreover, if $\\gamma$ is defined differently for each t, it should be indexed by t i.e., it should be $\\gamma_t$ and not just $\\gamma$. \n\nEven with the clarification, the utility of corollary 1 is unclear to me. As R2 asked, why does t have to go to $\\infty$? The author's response to R2 is following: \n\n\"It is important to analyze this problem when an infinite number of tasks are present and each task contributes a nonzero cost (lower bound on the loss) to the integral. \" \n\nI'm not sure why it's important to analyze the objective with an infinite number of tasks. New ideas should be presented as clearly as possible, and $\\gamma$ is only introducing additional complexity and theory with little payoff.\n\n### Verdict \nWhile it seems that there are good ideas in the paper, and unifying different meta-learning approaches in a theoretical framework is valuable, the current write-up does not elucidate this unification well. Many aspects of the paper --- such as introduction of $\\gamma$ --- require better motivation, and other aspects are just unclear. Perhaps the right approach to this paper would be to scale back the work, and focus on equation 2 and 3 more, justifying each aspect of the equation while providing more background. From the current draft, I cannot even unpack why the goal as written in equation 2 is reasonable, which makes it impossible for me to recommend an acceptance. I'll maintain my score of a 4 with a note that it's possible that I'm undervaluing the paper, and if another reviewer thinks the paper is clear, they should make a case for it. \n\n## Initial review\n### Summary \n\nThe paper models the meta-learning for continual learning as a dynamic programming problem and formulates an objective that can be minimized continually. The proposed objective aims to minimize forgetting on previous data while maximizing generalization on the current task. It also proposes an algorithm for approximately minimizing the objective by casting the problem as a dynamical system.\n\n\n### Overall review\n In its current form, the paper has a few major issues. First, it cites existing work incorrectly many times. Second, some text is ambiguous or factually correct. Lastly, it's not obvious to me that the optimization problem introduced in the paper is compatible with goals of continual learning.\n \n I elaborate on the three concerns in the rest of the review.  The first two concerns are fixable in a revision. The third concern is more problematic but might simply be due to a misunderstanding at my end; I look forward to the author's response for a clarification if that is the case. \n\n\n### Incorrect citations \nThe paper mischaracterizes existing work by citing many papers incorrectly. Some examples are as follows: \n\n1. The paper cites Sutton (1990) for proposing an MCL method when in-fact  Sutton (1990) introduces Dyna, an architecture for integrating learning and planning in TD methods. I fail to see how Dyna is even remotely relevant to MCL. In a different sentence, the paper cites Sutton (1990) as a 'regularization approach' for continual learning which is also incorrect.  \n2. The paper says Finn et. al., (2017) investigated Meta-continual learning which is not true. Finn et. al 2017 proposed a general algorithm for meta-learning for fast-adaptation to tasks sampled from a fixed task-distribution. \n3. The paper incorrectly calls the method introduced in Finn et.at., 2019 OML and the method introduced in Javed and Martha 2019 CML. Finn et.at., 2019 called their algorithm FTML whereas Javed and Martha 2019 called their method OML. \n4. The paper says \"An online meta-learning approach was introduced in Finn et.al., (2017)\" when it probably meant \"Finn et.al., (2019)\".\n5. The paper claims that  \"Finn et.al., (2019)\" highlighted the tradeoff between forgetting and memory which is not true. \"Finn et.al., (2019)\"  sidesteps the issue of forgetting by storing all prior data, ignoring any memory and computational constraints. Instead, it introduces an online meta-learning algorithm with regret bounds. \n6. The paper cites Finn et.al., (2017, 2019) for \"Incremental sine waves\" benchmark in the paper and the appendix when in-fact it was introduced in Javed and Martha (2019).\n\n### Unclear writing\nMany sentences in the paper are either not clear, or imply facts that are untrue. For instance, the paper says: \n\n\"The catastrophic forgetting cost measures the error of the model on all the previous tasks (typically known as the meta learning phase (Finn et al., 2017)); the generalization cost measures the error of the model on the new task (typically known as the meta testing phase (Finn et al., 2017))\" \n\nIt's not clear what this sentence mean. The \"Meta-training\" phase is more general than the \"catastrophic forgetting cost\", and the difference between meta-training and meta-testing has little to do with forgetting cost vs generalization cost. A meta-learner can choose to only minimize generalization error during meta-training (As done by Finn. el. at., (2017)).\n\nThe mischaracterization of meta-testing is also apparent from the sentence: \"Furthermore, in MAML, the optimization in the meta testing phase is performed with the second-order derivatives.\" This is incorrect. Meta-testing, in MAML, does not involve second-order derivatives (Meta-trainig phase does). During meta-testing, MAML uses SGD updates for updating the parameters. \n\n### The optimization problem: \nThe paper proposes to optimize the parameters $\\theta$ to minimize the quantity in equation 2. At time-step t, this quantity is given by integrating the forgetting cost from $t$ to $\\Gamma$, where the forgetting cost is defined in equation 1. \n\nLooking at the forgetting cost at time $t$, one can see that it is defined to be the integral from time 0 to $t$ of the weighted loss, where the weighting is given by $\\gamma(.)$ and is monotonically decreasing with time i.e., loss on initial data points contribute more to total loss than later data points. \n\nThe problem with this formulation is following: equation 2 is essentially a double integral that would count $l(0)\\gamma(0)$ a total of $\\Gamma$ times whereas $l(\\Gamma)\\gamma(\\Gamma)$ is only counted once. As a result, the resultant total loss would care about the earlier samples significantly more than later samples. The issue with equation 2 is further exacerbated by the fact that $\\gamma(t)$ is monotonically decreasing, making the role of later samples even smaller. As $t$ gets larger, the contribution of new data-points on the total loss would essentially diminish to zero. Continual learning, on the other hand, demands the opposite: we care about is performance on future data-points and performance on older data-points is not as important. \n\n### Verdict \nDue to the above mentioned issues, I am not recommending for an acceptance. It's possible that I misunderstood some aspects of the paper. In-case I'm misunderstanding the objective of equation 2, I look forward to authors response. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "great paper. experiments and text could be improved a bit",
            "review": "**Summary**\n\nThe authors propose a theoretical framework for meta-continual learning (MCL) both in continuous time (CT) and discrete time (DT). Specifically, they treat the MCL problem of minimizing a catastrophic forgetting cost and a generalization (to new tasks) cost as a dynamical system. This system is described by a PDE involving four terms: impact of the catastrophic forgetting cost (1), the impact of the generalization cost (2), the impact of change in parameters on the total cost (3), and the impact of the change in the input distribution on the total cost (4). Then the authors argued that methods like MAML, OML, and MCL focus on minimizing (1) and (2) via meta-training and meta-testing, respectively.\n\nNext, the authors derive a theoretically grounded MCL approach base on the introduced framework, namely Dynamic Programming-Based Meta Continual Learning DPMCL). Concretely, their approach uses gradient-based meta-learning to optimize term (1) and (2) in the outer-loop and an approximation of term (3) and (4) in the inner loop. \n\nThe empirical section shows that DPCML can offer some gains over previous methods like OML, CML, ANML as well as ER.\n\n**position**\n\nContinual learning (CL) definitely needs more theory and theoretically-grounded methods. This is why I believe this paper is great. The theory and the methodology seem sound to me (although I haven't had the time to proof-read it all the way). As far as I know, the terms (3) and (4) have never been studied within CL, so this paper has the potential to change how we think about CL. My only concerns are about the experiments and the writing, explained next.\n\n\n**concerns**\n\nSection 3 could be improved. I greatly appreciate the effort that the authors made to introduce more theory to CL. However, some CL readers might not be well-versed in PDEs. I think the authors could provide more intuitions and examples such that these readers can also enjoy the paper. Specifically, I think that terms (3) and (4), which are central to the paper, should be explained in more detail. Understanding how these terms evolve from eq. 3 to eq. 4 could wasn't all clear to me initially. I understand, however, that the authors need to work within the 8-page limit. \n\nMeta-Experience Replay (MER) [1] is an important and missing reference. DPCML is closest to MER than all aforementioned methods. I'm willing to increase my score if the authors can explain how both methods relate, as well as adding MER in the experimental section. \n\nI was also disappointed that code wasn't released, although the authors have provided lots of pseudo-code in the Appendix.\n\nIt is also unclear if the experiments are in the task-aware or task-agnostic setting, i.e. are the methods allowed test-time task identifiers.\n\nIn Figure 2, some methods are not reported or at least lie outside of the cropping. It seems strange to me however that they do not appear at task 0, i.e. when methods have incurred any forgetting yet. Specifically, why is Naive not in the MNIST and CIFAR10 figures on CME? \n\nFinally, throughout all the paper, the authors always refer to CL as MCL. Not all CL is MCL. E.g., why is the theoretical framework about MCL and not CL? Seems to me that CL is the problem and that meta-learning can be a solution to solving simultaneously term (1)-(4).\n\nSuperficially the paper could also be polished. A non-exhaustive list is provided next.\n\n**Minor concerns**\n\n- Figure 1 is too involved to be at the beginning of the paper\n\n- Section 2: MCL is not \"widely studied\"\n\n- Section 2: what does that mean: \"where Ω is the compact set that implies that the parameters are initialized appropriately\"\n\n- Theorem 1: typo here ||Jˆθ(t)||,>0\n\n- authors should explain how MCL and ANML are used w/o pretraining, and not in the Appendix cause this is critical.\n\n- shouldn't be explicitly repeating the results of Table 1 in the text (last paragraph of page 7)\n\n- figure 3, don't center the text\n\n- κ and k is confusing. choose another symbol\n\n- there are other typos, please use a grammar checking software\n\n- \"well-learned representation\"? maybe use better wording\n\n_____________\n**POST REBUTTAL**\n\nI appreciate the efforts and clarifications that were made. \nI think that the addition of MER solidifies the paper.\n\nR4 brought up an important issue (concern 3).\nI think the authors should look into it.\n\nI'm keeping my score constant as it still reflects my opinion of the paper.\n\n\n_______________________\n\n[1] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro.\nLearning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR,\n2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice theoretical angle, with things to be fixed",
            "review": "This paper would be a nice theoretical framework for meta/continual learning in the online setting if it is further polished.\nMy evaluation maybe limited as I only followed through the derivation and locate problems that I can see, but didn't try to verify every detail of the derivation.\n\nPros:\n1. Formulating the online learning process as a value function is new. Both meta learning and continual learning is considered in the proposed formulation.\n2. The derivation would look good to me if the problems raised in cons is resolved by the authors.\n3. Analyzing the the online learning process under this framework provides a new theoretical angle to meta-learning and continual learning. \n\nCons:\n1. It confuses me to write in the form of (3) with the lhs a partial derivative of t. I think the real goal is to perform the minimization. A suggestion would be to write it in an objective function centric way, namely keep the lhs on the rhs in Supp (7), and remove the terms that are not dependent on theta.\n2. Supp (4), the sentence before (4) doesn't convince me that $V(t+\\Delta t, \\hat{\\theta}(t+\\Delta t))$ can be replaced with $V^*(t+\\Delta t, \\hat{\\theta}(t+\\Delta t))$. As the minimization is over (J+V), it is not necessarily true that $\\arg\\min_\\theta(J+V)$ is equal to $\\arg\\min_\\theta(V)$ (I write (3) as J+V and (4) as J+V* in short.)\n3. Lemma 1 seems not useful, as I understand it, $\\gamma(\\tau)$ is used in the range $0<\\tau<t$, to penalize forgetting on more recent data, we need to have $\\gamma(\\tau)\\rightarrow 1$ for $\\tau\\rightarrow t$, and $\\gamma(\\tau)$ increasing in the range of $[0, t]$. I wonder why it is assumed in lemma 1 that $t$ goes to infinity for $\\gamma(t)$? A quick look at the hyperparameters in the supplementary and the algorithm pseudo code doesn't give me the answer how $\\gamma$ is actually set.\n4. In the discretized case, it is very different whether we make prediction before or after parameter update. If we predict before update, it measures the forward transfer / generalization. If we predict after update, it measures how well we learn on this new data. How would you distinguish this two cases in your continuous time derivation?\n5. In the paramgraph below theorem 1, $\\|J_x\\|=0$ means the learning process will stagnate, I don't agree on this. Because $J$ measures the integral from $0$ to $t$, thus it should increase when new $x$ comes. If its derivative wrt $x$ is $0$, it is even better. Even if we consider $\\gamma$, $\\|J_x\\|=0$ could also result from that all $l(\\tau); \\tau \\in [0, t]$ changes but cancels out. Am I misunderstanding at this point?\n6. The link with MAML isn't explained enough, i.e. what are support/query set mapped to in the time continuous setting? Maybe the answer to 4 would also address this question.\n7. The theory part doesn't separate $\\theta_1$ and $\\theta_2$, but in the implementation, it comes out a bit abruptly. The final paragraph at page 5 gives a reason, but I think it doesn't justify why the derivation can't be used as it is.\n\nTypos:\n- before section 3, optimal using tools -> optimize using tools\n- Supp B4, Eq. ??",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}