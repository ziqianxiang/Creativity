{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper applies and compares the performance of Deep DQN (DDQN) (Mnih et al., 2013; 2015), Categorical Distributional RL (C51) (Bellemare et al., 2017), and Implicit Quantile Networks (IQN) (Dabney et al., 2018) optimal stopping problems under Bermuda option pricing, for which the underlying dynamic of the asset either assumed to follow Black-Scholes model or is historical S&P 500. ",
            "review": "This paper sheds light on empirical performance of three deep reinforcement learning methods (DDQN, C51, and IQN) when applied to Bermuda option pricing. \nTo address the underlying asset’s time series property etc., small blocks (T_ batch-size), Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) recurrent neural network, and a dropout wrapper around the LSTM are adopted. \n•\tThe small block (batch) approach is commonly adopted in Financial data analyses where time series presents. \n•\tThe dropout is often considered as a regulation method in deep learning. \n\nFollowing Hessel et al., 2018), three techniques like Double Q-learning  (van Hasselt, 2010), dueling architecture (Wang et al., 2016), multi-step bootstrapping of targets (De Asis et al., 2018) are considered. \nThe presentation of the paper can be improved. For example, \n•\tMore tech/math details displaying the three adopted methods and comparing the pros and cons may be nice. \n•\tP3 presented the similar notation from Ciocan & Misic (2018). But in other places, no rigorous definition of Black-Scholes model, no arbitrage concepts, option pricing, etc. under the same probability space (under sigma-algebra). \n•\tP6, either not to mention American option, or show the limit approximation of Bermuda option to American option, and try the simulation also under American option.   \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "The paper evaluates DDQN, C51 and IQN on the option pricing / option exercise tasks. It first evaluates those RL algorithms in  Black-Schole environment where the performance of the algorithms can be verified. It then analyses these algorithms against few benchmarks on historical data of S&P 500.\n\nStrong points:\n- The authors first verify / analyse the algorithms in settings where the dynamics are known and the performance can be compared to the \"ground truth\".\n\n- Selected algorithms are (relatively) novel and achieve strong performance on many RL benchmarks.\n\n- Previous work seems to be well referenced.\n\nWeak points:\n- At the end of the day, the paper simply takes 3  existing q-learning based algorithms and runs them on an option task.\n\n- The evaluated algorithms are very similar, the only difference really being in the training targets/loss of the network.\n\n- Small contribution\n\nConclusion:\nThe paper is not bad, but I don't feel that the contribution is sufficient for an acceptance. Running \"off the shelf\" RL algorithms in new tasks can certainly be impactfull in some cases, but I don't believe that in this case the contribution is there. One can not simply \"run an RL algorithm X on yet another task Y\" if it brings little new insights or surprising results. In this case, I don't see anything novel.\n \nAdditional feedback:\nI am not sure if I understand the value in including the Algorithm 1. It looks like it is just relatively straightforward DDQN and just taking a lot of space? Am I missing something?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good application of reinforcement learning to finance but may be of limited interest to ICLR audience",
            "review": "Summary:\n\nThis paper applies reinforcement learning based techniques to the optimal stopping problem in simulated and real financial engineering environments. The authors implement Double-DQN, C51 and IQN algorithms and present a thorough comparative study on the option pricing and optimal option exercise applications.\n\n\nReview:\n\nThe paper is well-written and easy to understand. Adequate literature review has been provided however I have concerns regarding the significance and novelty of this work to ICLR audience (detailed later).\n\n\nStrengths:\n\n1. The paper is clearly written and the methods involved have been described in adequate detail.\n\n2. The study is quite thorough and the authors have taken adequate steps to tune all RL algorithms well on the two applications studied in the paper.\n\n3. The authors have provided all the tuning details and hyperparameters and have clearly highlighted the difficulties of employing RL in highly stochastic and non-stationary environments.\n\n\nWeaknesses:\n\n1. My primary concern is that this work is a nearly straightforward application of existing RL algortihms to the applications considered. While it makes a very interesting financial engineering study in its own right, it does make any novel machine learning, deep learning or deep reinforcement learning contribution. So the work might be of limited interest to ICLR audience.\n\n2. While it is true that applications to stock market are plagued by uncertainty and non-stationarity, the optimal stopping problem seems to have a very low dimensional discrete action space (2 actions - stop or continue) which makes the problem less challenging than other benchmark applications of RL.\n\n3. Lastly, while the action (stopping or continuing at time t) effects the total return for a given trajectory, it is somewhat unclear to me if the actions effect the transitions or not. If the transitions are unaffected by the actions, then this may further simplify the problem setup.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Puzzling paper",
            "review": "Paper summary:\nThe authors propose to use RL to solve the optimal stopping problem.\n\nReason to reject:\nLeaving aside novelty and contributions, I can't figure out what exactly were the authors doing. Other than an algorithm with very confusing notations (e.g. x_{k−l:k} || K −x_{k} || T) that don't match up with the problem definition, there is nothing left for the readers to understand the paper. What is r? What is Q? Why is there a primary network that chooses an action when the action space is discrete and how come it doesn't appear in the algorithm? I tried to look for answers in the paper but no luck. Throughout the whole paper I couldn't find the definition of the reward function and the Q-function for their problem setting, they seem to only appear only in the algorithm. Furthermore, in their problem definition the actions space is discrete (A={continue, stop}), but they later claim that they have \"a primary network to choose an action given the current state\" despite having \"a=argmax_a Q\" in their algorithm. I am not sure whether this is a writing mistake or the authors simply weren't clear about how those algorithms work.\n\nIt is either that I do not have enough background knowledge in financial engineering to understand how RL is used in the field or the authors did a very bad job in making their paper clear. Since I failed to understand the paper, I would not make any comments on novelty or contributions.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}