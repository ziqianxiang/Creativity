{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting idea, but not convinced approach is useful. Some correctness issues and missing references",
            "review": "### Summary\n\nThe authors propose a method to estimate the loss of a model trained with the differentially private objective perturbation method at many values of the privacy parameter epsilon, without retraining for each value of epsilon. The method is to train the model for one value of epsilon, and to use a first order Taylor approximation for nearby values of epsilon. \n\n### Evaluation\n\nThe authors’ claim that this is the first paper to propose choosing epsilon based on utility is incorrect. In fact this is the method of choice in deployed differentially private systems, as far as I know. Methods for choosing a value of epsilon based on utility without rerunning mechanisms from scratch have been proposed before as well, for example the noise reduction method from “Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM” by Ligett et al.\n\nHaving more ideas in this space is valuable. The method proposed, being just a first order Taylor expansion, unfortunately only applies to values of epsilon that are very close to the one the model was trained on. Usually there would be enough uncertainty about the performance of the model with different values of epsilon that one would want to try values that are spread out. Because of that, it does not seem to me that the method the authors propose will be useful in most typical applications.\n\nThere are also a few places where I am not convinced the mathematics are correct:\n\n* Equation (4) fails to specify the assumptions on the loss from the Chaudhuri et al paper. One needs the losses to satisfy some bounds on the first and second derivatives. Without this, it’s not clear what the parameter $c$ is, for example.\n\n* The derivative of $b_\\epsilon$ with respect to $\\epsilon$ is not generally defined. Later on it becomes clear that the authors fix a map from random samples in the unit interval to $b_{\\epsilon}$ and this map is differentiable for fixed randomness. This is reasonable, but should be clarified before the statement of Theorem 1.\n\n* The derivations in Section 4.2 are incorrect. The expression in (11) is the CDF of the Laplace distribution, so the distribution for $b_\\epsilon$ that the authors are sampling is IID Laplace random variables. But the density in (3), and in Chaudhuri et al, is not that of the Laplace distribution: rather, it’s exponential in the $\\ell_2$ norm. \n\nA final remark is to be more careful with references. Why is the reference for the Taylor expansion some 1976 paper rather than a standard textbook? Why is the Dwork and Roth book listed with “et al” in the author list in the references, when there is no other author? Why is the definition of differential privacy, due to Dwork, McSherry, Nissim, and Smith, attributed only to Dwork?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "The paper mainly study the problem that, for DP-ERM, how to choose the appropriate $\\epsilon$. By using the objective perturbation method and calculating the partial derivative of the perturbed function w.r.t $\\epsilon$, the authors get a closed form solution to approximate  $\\hart{\\theta}_{\\epsilon'}$, giving the parameter $\\hart{\\theta}_{\\epsilon}$ and $\\epsilon$. That is, we can predict the performance when the privacy budget is $\\epsilon'$ for any $\\epsilon'$ if we just know $\\hart{\\theta}_{\\epsilon}$ for a fixed $\\epsilon$. Thus, we can select the best privacy budget. \n\nHowever, I tend to reject due to the following reasons. \n\n1. The method of this paper heavily depends on the objective perturbation method. However, as we know, objective perturbation method only holds for some convex loss function (that is with additional assumptions), it does not hold for non-convex loss. Thus, I think the generalization of the method in this paper is limited. \n\n2. Theorem 3 needs too many assumptions so that I think it cannot be used to many loss functions. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for paper #235",
            "review": "The paper considers the problem of how to select the privacy parameter $\\epsilon$ for differentially private empirical risk minimization, which is of broad interest to the community. The paper approaches the problem by providing an approximation scheme for the empirical error under different $\\epsilon$ based on computations under a fixed $\\epsilon_0$. With the approximation, the trainer can select the privacy level based on the anticipated utility. The paper considers the objective perturbation scheme proposed in Chaudhuri et al 2011 and the approximation is based on the Taylor expansion of the empirical error with respect to $\\epsilon$ at point $\\epsilon = \\epsilon_0$. The paper also conducts experiments on various datasets to show that the approximation scheme is accurate with an okay error.\n\nMy main critique of the paper is that the approximation scheme is not private. To be exact, the approximation formula (equation 9 in the paper) is based on non-private statistics of the dataset (e.g. the gradient of the empirical loss with respect to $\\theta$), which makes the choice of the new $\\epsilon'$ not private and neither is the final output because of composition. This makes the theory of the paper unsolid. So the paper is a clear reject from my side.\n\nOther comments:\n1. The paper only considers pure DP while approximate DP is the more interesting case in practice. It is not clear how this can be generalized to the approximate DP case.\n2. In the paper, it is stated that the \"user\" can select the privacy parameter. It is worth pointing out that this user is not the data contributors whose privacy the scheme is protecting. The \"user\" in the paper is referring to the \"learner\", which makes the motivation of the method less convincing.\n\nMinor comments:\n1. Page 1: results in useless model ---> results in a useless model.\n2. Page 1: too small $\\epsilon$ may lose to much utility ---> too much utility\n3. Page 5: formular ---> formula.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Choice of privacy parameter using utility",
            "review": "The paper gives a new way to estimate the privacy parameter $\\epsilon$ given a utility guarantee that a user is looking for.  Choosing the privacy parameter is one of the basic problems in the implementation of differentially private algorithms and only a small number of work shed any light on it. The paper analyzes one of the earlier algorithm for privacy preserving learning that uses objective perturbation. \n\nThe assumptions made in the paper is reasonable: the loss function is twice differentiable. The paper claims that if the function is non-differentiable, we can approximate it with the one that is twice differentiable; however, that would incur some more utility loss. The authors do not give any rigorous reason as to why it is always possible. \n\nThere is a something else that I do not understand in the paper. Theorem 3 makes an assumption on the boundedness of gradient, Hessian, the high dimensional variant of torsion (third order derivative) is bounded. The norm is not clear to me. Is the spectral norm of the Hessian is bounded and what is the norm of torsion that the authors assume is bounded? Secondly, what is the equivalent of that in the geometry of function is not clear to me. This makes their claim on the approximate case a little hard to understand. \n\nIf I understand correctly, the technique would only work when $\\epsilon$ is small because of Taylor's approximation. Can we use Analytic methods to estimate $\\epsilon$ in other cases? I am guessing there would be some numerical stability issues in the naive algorithm that tries to estimate privacy parameter using Analytic method. I am raising this question because in practice, we often have $\\epsilon \\geq 1$ and not less than $1!$.\n\nLast point is a general question: why does the author consider unconstrained setting? We know there is a difference between utility guarantees achievable in constrained vs unconstained setting in the regime of PPML.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}