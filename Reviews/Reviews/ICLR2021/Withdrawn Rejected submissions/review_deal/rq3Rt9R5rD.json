{
    "Decision": "",
    "Reviews": [
        {
            "title": "An unjustified GAN method for image-to-image translation.",
            "review": "The experimental results look good, but I can barely find a motivation and justification of the proposed BalaGAN. More specifically, I wish that the authors can answer the questions below in details: (1) what issues does BalaGAN address compared with existing SOTA image-to-image translation models? (2) how do the authors define modality? Section 3.1 mentioned that the authors employ SimCLR (Chen et al., 2020) to find modalities, and also mentioned spherical K-means. Then how did the authors actually find the modalities?  (3) Section 3.2 is confusing. What is the exact pipeline (including training and inference) of the proposed BalaGAN? How do the two discriminators collaborate with the generator? Why the two discriminators share weights with each other in the initial layers? And can the authors justify that why D_cls is necessary theoretically or empirically?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper tackles the balanced cross-domain image-to-image translation. It proposes an interesting idea of explicitly investigating the multi-modal latent distribution of the rich source domain for helping the translation to the target domain which lacks of richness and diversity. However, the evaluation of the contributions is not satisfactory. ",
            "review": "This paper proposes to tackle the image-to-image translation problem under the setting that the two image domains are not balanced, i.e. the source domain is rich and full or diversity while the target domain lack of richness and diversity. The proposed method explicitly hypothesizes that the source domain tends to have multiple modes in its latent distribution. With unsupervisedly decomposing the source domains into multiple modes/classes via applying k-means on  the latent representation (learnt by contrastive learning), these modes/classes together with the target domain (itself is a single mode/class) will become balanced and the multi-domain image-to-image translation model is learnt to translate across them. The FUNIT model from Liu et al., ICCV2019 is directly used as the multi-domain image-to-image translation used in this proposed method. The experimental results show that the proposed method is able to provide improved quality and diversity of translated images in terms of FID comparing to several baselines, including CycleGAN, MUNIT, StarGAN2, and CUT (Park et al., ECCV 2020). \n\nPros:\n+ The idea of explicitly investigating investigating the multi-modal latent distribution of the rich source domain for turning the imbalanced translation into balanced one is interesting and experimentally proven to be help learning image translation between imbalanced domains. \n+ The proposed method is able to provide superior performance against several baselines on various datasets and experimental settings. \n\nCons:\n- The main contribution of decomposing the multiple modalities of the source domain is not well verified. As we can tell from the experiments, such decomposition is able to be integrated into other multi-domain image-to-image translation models, however, currently we only see its benefits applied on the StarGAN model (StarGAN2^1 versus StarGAN2^{30}). It is better to have the results of having the multiple modalities of  source domain in other models such as MUNIT and CUT in order to well verify the practicability and generality of the proposed decomposition.\n- Another proposed technique used in the proposed method is the additional D_{cls} discriminator. However, although there is an ablation study (removing the D_{cls} from the proposed model) related to it, its difference and connection with respect to another discriminator D_{adv} is not well described and  clearly motivated. \n- Currently the number of modes in the source domain to be decomposed is somehow determined by the number of image samples in the target domain, this is actually weird as the multi-modal data nature of source domain should be independent of the target domain.\n- As now the source domain is decomposed into multiple sub-domains/modes/classes, the translation is also learnt across them. There should be also example results and evaluation on having the translation across the modes stemmed from the source domain. \n- Some minor concerns: (1) The naming of network components in the Figure.2 is confusing, the E_{source} and E_{target} would lead to ambiguity with respect to the source and the target domains, perhaps something like E_{to_be_translated} and E_{reference} could be better? (2) It would be better to have discussion why simple CycleGAN baseline can provide better performance with respect to other more recent models (e.g. MUNIT, CUT, StarGAN2). \n\nIn brief, I think the idea of introducing the multi-modal decomposition of source domain for resolving the image translation between imbalanced domains is interesting and somehow novel. However, the evaluation now is not satisfactory enough to really demonstrate the wide practicality and generality of such idea on other multi-domain image-translation models, and the motivation/intuition of the additional discriminator D_{cls} are not well described. I would have a lower rating (marginally below acceptance threshold) now and look forward to seeing the response from the authors. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer",
            "review": "########################################\n\nSummary:\n\nThe imbalanced problem of Image-to-image is investigated in this paper. Specially, the paper explores the setting where domain A has much richer in quantity and variety samples, while domain B contains limited and low diverse  images.  To remedy this issue, authors translate this problem into multi-class and conditional image-to-image translation. Specially, the domain A which contains more samples is clustered into K groups (domains) by the optimized feature extractor by contrastive learning.  The K +1 domain containing target domain are taken as training data to model a conditional image-to-image translation (e.g. FUNIT). Various kind of datasets are utilized to evaluate the proposed method.\n\n ########################################\n\nPros:\n\n+The work provides an interesting problem for image-to-image translation. To my best knowledge, the impact of the imbalanced data for image-to-image translation have not been explored.\n\n+The proposed method looks useful and obtains satisfactory performance on various kind of datasets\n\n+The paper is well-written and easy to follow\n\n+Authors present the quantitative and qualitative result to support the proposed method.\n\n ########################################\n\nCons:\n\n-The proposed is simple, but not enough to achieve the level of ICLR. The proposed method fails to attract my attention, since the proposed method lacks of highlight and novel.\n\n-Does authors try traditional method to the cluster? since the paper uses the contrastive loss to get the cluster. \n\n-Only FID is selected to evaluate the proposed method,  maybe authors consider KID and LPIPS.  And the experiment looks simple.\n\n-The baseline lack of FUNIT, since the proposed method is based FUNIT framework. It would be convincing to leverage FUNIT to evaluate the same dataset.\n\n-The result (Table 1) from StarGAN2 is weird. From my point, StarGANv2 almost has similar architecture except for conditional case. If StarGAN2 is low, why other baselines get better performance.    \n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting work but lacking some convincing results",
            "review": "Summary: This paper proposes to solve the imbalanced image-to-image translation problem, and obtains some good results on both unconditioned and style-transfer-based image translation tasks.\n\n\nMajor issues: \n- It is still unclear about why decomposing one image domain to multi-modalities can benefit to the imbalanced image translation problem. In instead of the intuitive explanations, it will be better to give some theoretical analysis. \n- The quality of generated images is still not satisfactory with such rapid development of GANs, such as the results in Figure 3.\n- The ablation study and qualitative evaluation are too limited. Only FID is used, adding other metrics, such as LPIPS, NDB, and JSD, will be more convincing. \n\n\nMinor issues\n- In the extreme case, the target domain only has one image, how will your model perform? Can you compare the model OSIT (one-shot unsupervised image translation).\n- How do you see the impact of the suggested techniques on tackling more limited data, e.g. both source domain and target domain have several images?  \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}