{
    "Decision": "",
    "Reviews": [
        {
            "title": "an interesting paper; but several concerns",
            "review": "*Summary\n\nThis paper tries to provide a comprehensive study of black-box attacks on defense methods, and three contributions are claimed: (1) a new metric for evaluating model performance is provided; (2) a new black-box attack is proposed; and (3) a new method, Buffer Zone defense, is proposed for defending against black-box attacks, and empirical results (on Fashion-MNIST and CIFAR-10) are provided for demonstrating its effectiveness. \n\n\n*Pros\n\n(1) As stated in this paper, there are little efforts on comprehensively evaluating model performance under the black-box setting. The reviewer agrees that this is an important research direction, as the black-box setting is a more practical scenario in the real world (compared to white-box attacks). This paper takes the first attempt on exploring this direction and providing a very comprehensive analysis of model robustness (which could be useful for future works in this direction). \n\n(2) The proposed defense is simple and effective. Basically, the proposed BUFFER ZONE DEFENSE = MODEL ENSEMBLE + RANDOM IMAGE TRANSFORMATIONS. The empirical results in Table 1 suggest the proposed method works pretty well under the black-box setting, and outperforms other defenses by a large margin.\n\n\n*Cons\n\n(1) This paper claims the design of the mixed black-box attack as an important contribution of this paper, but neither detailed description of this attack nor careful ablations for demonstrating the effectiveness of this attack are provided! The only relevant description is in Section 2.1, but still very vague. For example, how the proposed attack generates \"synthetic data\" and how many queries are needed? what is the \"synthetic model\", should it be similar to the backbone network applied in the targeted defense? Without these detailed descriptions and analysis, the reviewer cannot properly justify this contribution. Also, based on the current paper version, it is hard for other people to reproduce the proposed attack.\n\n(2) The reviewer cannot understand the importance of the proposed evaluation metric. By reading Section 4, it seems the message delivered by this new metric is EXACTLY the same as the message delivered by the existing evaluation metric---we need to lower down the defenses' accuracy drop and the attackers' success rate, for obtaining stronger defenses. If the messages delivered by these two sets of evaluation metrics are the same/similar, why we need this new metric (e.g., will it help us to select models with better accuracy/robustness)? The authors should carefully justify it!\n\n(3) Though the reviewer understands the main purpose of this paper is to investigate defenses under the black-box setting, the reviewer still wants to see a brief paragraph on discussing the robustness of the proposed defense under the (adaptive) white-box attack (the weak white-box performance will not negatively affect the reviewer's rating). On the one hand, this white-box result helps us understand whether the proposed defense leads to severer gradient masking issues or indeed smooth the loss landscape for improving robustness. On the other hand,  this result may be helpful for future studies on investigating stronger black-box attacks.\n\n\nOverall, the reviewer thinks it is an important and interesting study. But several contributions of this paper are vaguely claimed, and the authors need to carefully address these concerns during the rebuttal period.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting evaluation but lacks theoretical analysis",
            "review": "Summary:\n\nThe authors propose a novel approach to defend against adversarial examples based on the combination of deep neural networks and different image transformations. The experiment results demonstrate the proposed method can beat 11 other defenses on CIFAR10 and Fashion-MNIST.\n\nQuestions:\n\n1. Cifar10 and Fashion-MNIST are two toy datasets; defending on large-scale benchmarks (ImageNet) is more valuable. Why not test the robustness of BUZz on ImageNet.\n2. I cannot agree with the authors that ‘score based black-box attacks can be neutralized by a type of gradient masking’. NATTACK [1] is a strong score-based attack that can attack such gradient masking-based defenses. I would like to see the evaluation results over NATTACK.\n3. The new metric provides a trade-off between the accuracy and robustness of defenses. But I cannot tell this is necessary since we can evaluate the clean accuracy and attacking success rate.\n4. The defense is based on the combination of different deep neural networks and image transformation. The authors provide some interesting empirical results, but there is a lack of theoretical analysis. \n\n[1] Li, Yandong, et al. \"Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks.\" arXiv preprint arXiv:1905.00441 (2019).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Contributions are hard to quantify",
            "review": "Summary: The authors proposed a quite intuitive black-box defense algorithm called Buzz defense. The main idea is to set up some buffer zones near the boundary of two classes, and they employ multiple independent CNNs to do majority voting to get the final prediction.\n\n\nThe idea to me is quite straightforward, and I am mostly concerned about the experiments:\n\n1. First, in table 1, the clean accuracy for several defense algorithms are surprisingly low (I ran the [Xie] and [Madry] algorithms before, and I believe you can get much higher accuracies)\n\n2. I also know that changing the maximum perturbation could also affect the ranking of these methods, so authors could perhaps run with more configurations. As oppose to just 0.05\n\n3. Black-box attack is strictly weaker than the white-box attack so I personally think it mitigating that is less valuable than mitigating white-box attack. That is probably the reason you see less black-box defense than the other.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Neither the attack nor the defense are properly evaluated",
            "review": "This paper claims three contributions: (a) a novel metric for measuring the performance of defenses, (b) a new black-box attack (similar to Papernot 2017) and (c) a new defense that uses random input augmentations and majority voting.\n\nThe first half of the paper is confusing to read for a specialist because many terms that are well established in the field of adversarial robustness are used or defined wrongly. For example, \"adaptive adversarials\" are loosely defined (if I understood that correctly) as meaning: black-box attacks that have query access to the model under attack, rather than the meaning adversaries that design an attack specifically tailored to a given defense mechanism. Also, white-box attacks are narrowly defined as meaning \"attacks that rely on gradients\". But that's not the definition used in the community: white-box attacks just generally mean attacks that have access to all components of the model, as well as weights and training data. Whether or not gradients are being used is not part of the definition. This confusion leads to wrong statements such as \"White-box attacks does not imply robustness to black-box attacks\" which is simply not true because white-box attackers have strictly more information about the defense as black-box attackers, and hence should be strictly more successful. Individual instantiations of white-box or black-box attacks might be more or less effective, but this confuses the threat scenario (how much information is available to an attacker) with specific instantions (which attack algorithm is being used). There are several other wrong claims (e.g. \"black-box attacks allows readers to compare defense results\" or \"adversarially trained models perform poor against black-box attacks\") on which, unfortunately, this work is heavily based on.\n\nRegaring the claims of the paper, the proposed metric is really just the difference between the clean accuracy of the undefended model and the adversarial accuracy of the defended model under attack. But that's basically what the community looks at anyhow: we compare different defenses based on the adversarial accuracy (in the same threat model and ideally also the same network architecture). The proposed metric might correct a bit for differences in clean accuracies should two defenses use different base network architectures, but the practical need is limited.\n\nIn the proposed black-box attack, the defended model is queried on synthetic images, and the responses are used as labels on which a helper network is trained in conjunction with the natural training data samples. This builds on top of Papernot et al. 2017, but no comparison is being made.\n\nFinally, the proposed defense is very similar to what has already been proposed in the literature, and it is only evaluated against weak black-box attacks which are basically not aware of the defense mechanism, especially not the randomization part. Basically none of the community guidelines are being followed in the evaluation [1]. For example, there are no adaptive white-box or black-box attacks tailored to the defense, which are crucial to estimate how well a defense works [2]. The authors might want to take a look at the Mixup Defense by Pang et al. 2020 that is broken in [2]. The authors claim that their defense works best in two out of three black-box attacks - but for practical applications the question should rather be: which defense can resist a more powerful attacker who will use the best attack he can find. The paper gives no answer to this question, and so there is little relevance of the results for practical applications.\n\nTaken together, none of the claims are sufficiently substantiated, and the work contains many misconceptions of adversarial attacks and defenses.\n\n---\n\nMinor points:\n- The paper repeatedly claims that Chen et al. were the first to propose decision-based or boundary attacks. But that's not true: all  cited works directly build on top of Rauber et al. [3].\n- Madry et al. reported with 75% clean accuracy but should be > 82%\n- Please clearly define the threat model\n\n[1] On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.06705\n[2] On adaptive attacks to adversarial example defenses, https://arxiv.org/abs/2002.08347\n[3] Decision-based adversarial attacks: Reliable attacks against black-box machine learning models, https://arxiv.org/abs/1712.04248",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}