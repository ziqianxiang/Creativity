{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This is a nice paper using contrastive learning for code representation. The idea is to generate variations on unlabeled source code (using domain knowledge) by creating equivalent version of code. Improvements over baselines on two multiple tasks are shown. While some of the reviewers liked the (and R4 should have responded), none of the reviewers found the paper exciting enough to strongly recommend its acceptance. "
    },
    "Reviews": [
        {
            "title": "Review for \"Contrastive Code Representation Learning\"",
            "review": "This paper studies the self-supervised code functional representation learning and proposes a method called ContraCode. ContraCode utilizes some code functionality invariant transformations to generate positive pairs from the same code and negative pairs from different codes. After that, these codes pairs will be used to do the contrastive pre-training. Experiment results based on two tasks are reported.\n\nPros:\n-\tThe task of  code functional representation learning is important and valuable.\n-\tThe transformations proposed in this paper may produce some vaviance to the code while maintaining the same functionality.\n\nCons:\n-\tThe superiority of the proposed method is unclear. Many self-supervised code representation learning methods are mentioned in the introduction, such as [Ben-Nun et al., 2018; Feng et al., 2020; Kanade et al., 2020]. However, this paper fail to discuss of the differences (especially the advantages) between ContraCode and other self-supervised methods empirically.\n-\tSince no addtional supervision is evolved, unsupervised feature leanring models are good competing baselines.. The authors are  strongly recommended to compare the performance of ContraCode with other unsupervised methods under the same training dataset (both augmented).\n-\tThe key question is the whether the self-supervision generated by such transformation really makes any difference. Some transformation only change the formatting, which usually resulting the same feature representation because the formatting information is usually not considered in most of the feature learning methods for code. It appears that by applying the set of transformation, the code would not differ from its previous appearance much. Consequently, the feature representations generated by some unsupervised method from the original code and its transformed counterpart could be very similar to each other EVEN IF no self-supervision is enforced, which means self-supervision is not necessary.  Please clarify this be providing empirical evidences such as the portion of the changed lines or tokens from the original code, the similarity between the original code and its transformed counterpart over any two different pieces of code based on the features learned in some unsupervised way (with the same scale of training data).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Contrastive Learning on Source Code Representation",
            "review": "This work proposes to combine contrastive learning with code representation. The different transformations of code snippets are inspried by static complier.\n\nMy mainly concern is about the novelty. I agree with the claim about programs with the same functionality should have the same underlying representation. However, it's unclear to me that why using it as contrastive loss is a better choice than MLM loss in code understanding downstream tasks, especially for type inference. Any theoretical or intuitive explaination is good.\n\nIt seems that the performance gain about the proposed method is overcliamed, especially for the 40\\% top-5 accuracy gain of TypeScript which is a deterministic method. The actual gain compared to SOTA learning-based method is less than 3\\%. Also, the experimental results are unconvincing to me, for example, pre-training with MLM loss (then finetune on the downstream task? The corresponding descriptions are not clear) get poor accuracy on type inference task. The authors intuitively explain it as that MLM loss is not  suitable for this kind of task. However, it performs better when combining with contrastive loss than only using contrastive pre-training.  Whether on earth MLM loss is good for this task? \n\nSome necessary baseline methods are missing. For example, [1] and [2] for code summerazation task . And some important downstream tasks are also missing to demonstrate the ability of proposed method in code understanding, e.g., code clone detection (which I personally think that is more suitable for contrastive code representation).\n\n[1] A Transformer-based Approach for Source Code Summarization, ACL 2020\n[2] https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper with some debatable claims",
            "review": "## Summary\n\nThe paper proposes Contrastive Code Representation Learning (ContraCode), a\nself-supervised algorithm to learn task-agnostic semantic representations of\nprograms via contrastive learning. The guiding principle for contrastive\nlearning is that programs with the same functionality should have the same\nrepresentation. The authors develop an automated source-to-source compiler to\ngenerate different (most of the time equivalent) variants of the same program.\nThe neural network, which is basically identical to the Momentum Contrast\narchitecture, is trained using the programs generated by this compiler.\n\n## Pros\n\n- A very nice and intuitive application of Momentum Contrast to code\n  representation learning.\n- The use of source-to-source compilers for contrastive learning.\n- The results are consistently better than previous state-of-the-art on two\n  different downstream tasks.\n- The authors have done extensive ablation studies which provide further\n  insight.\n\n## Concerns\n\n- I believe that the most emphasized result -- 40% higher top-5 accuracy than\n  the current state-of-the-art static type analyzer for TypeScript -- is\n  misleading. As far as I know, TypeScript's built-in type inference returns\n  only a single suggestion. This is reinforced by the fact that Acc@1 and Acc@5\n  is exactly the same for CheckJS in Table 2. So comparing Acc@5 between the two\n  algorithms is not fair and it is pointless.\n- In Section 3.2, the extent to which He et al. (Momentum Contrast) is followed\n  is not clear enough. Part of the method that's described in \"Pre-training\n  objective\" strictly follows/summarizes Momentum Contrast but that's not\n  apparent from the paper.\n- In 4.1., I find \"cross-language knowledge transfer\" a bit of an overstatement,\n  as TypeScript is a superset of JavaScript.\n- In 4.2., the difference in F1 score between Transformer and\n  Transformer+ContraCode+augmentation is very small. I would like the authors to\n  discuss this.\n- In 4.2., I find the statement \"showing that code token reconstruction is not\n  an effective pre-training strategy\" too general.\n\n## Questions\n\nI have questions about the transformations done with the source-to-source\ncompiler, which could also be made clearer in the paper.\n\nFigure 3 shows the uniqe transformed program variants after applying 20\nsequences of random transformations.\nHow many transformations were done in a sequence?\n\nIn 3.2, the authors write that each program is transformed twice (similarly to\nMomentum Contrast). However, in Section 4, Pre-training dataset, they write that\nthe augmented dataset is pre-computed by sampling up to 20 unique transformed\nvariants per program.\n\nDoes this mean that the two transformed programs are sampled from these 20?\nHow many transformations were done in a sequence? Is that the same as for Figure 3?\n\n## Reasons for Ranking\n\nMy main concern is the +40% Acc@5 claim compared to CheckJS, which I find\nmisleading. However, I think that this is an interesting paper and a valuable\ncontribution. If my concerns are addressed I'm willing to improve my score.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}