{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper investigates the average stability of kernel minimal norm interpolating predictors. The main result \nestablishes an upper bound on a particular notion of average stability for which it is well-known that it \ncan be used to bound the generalization error. This upper bound holds for all interpolating predictors \nfrom the RKHS, but it is minimized by the minimal norm predictor. \n\nWhile at first glance this result looks highly interesting, a closer look reveals that the significance of the results \ncrucially depends on the quality of the derived upper bound. Here two reviewers raised concerns, since it is \nby no means clear that even the optimized upper bound produces meaningful bounds on the generalization\nperformance. The authors tried to address these concerns in their response and promised to update their \npaper accordingly. As a result, they added a paragraph on page 8. Unfortunately, this paragraph remains extremely \nvague, in particular if it comes to the more interesting case of non-linear kernels. Here, the authors briefly refer to \na paper by El Karoui but no details are given. However, looking at El Karoui's paper it is anything but obvious whether \nthe results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class \nof distributions. \nAs a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion.\n\n  "
    },
    "Reviews": [
        {
            "title": "This paper investigates kernel ridge-less regression from a stability viewpoint.  ",
            "review": "This paper investigates kernel ridge-less regression from a stability viewpoint by deriving its risk bounds. Using stability arguments to derive risk bounds have been widely adopting in machine learning. However, related studies on kernel ridge-less regression are still sparse. The present study fills this gap, which, in my opinion, is also one of the main contributions of the present study. \n\nPros:\n1.  As mentioned above, this study presents some novel research into kernel ridge-less regression from a stability viewpoint.\n\n2. The study presented here brings some novel insights into the relationship between minimizing the norm of the ERM solution and minimizing a bound on stability and also reveals the role that the condition number of the kernel matrix plays in kernel ridge-less regression, see formula (6). \n\n3.  The paper is well presented and well polished. The analysis conducted in this paper seems to be sound.\n\nJust one minor concern: what would happen if the boundedness assumption on the output variable, which excludes the most common Gaussian noise, is not imposed? I understand that this condition is common in learning theory but are expecting more comments.    ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper establish the average leave-one-out stability bound for the interpolation solutions. The results are interesting and novel.   But I have some concerns which need authors' clarification. ",
            "review": "The paper establish the average leave-one-out stability bound for the interpolation solutions, and show the above bound depends on condition number and spectral norm of kernel matrices.  The authors establishes a nice connection between numerical and statistical stability. A nice property is that among all interpolation solutions, the upper bound on stability achieves the minimum at the solution with the minimal norm. The authors then comment that the interpolation solution with minimal norm may generalize better than other interpolation solutions. The paper is clearly and well written.\n\nComments:\n\n1. In Theorem 7, the authors show that the stability can be bounded by the spectral norm of $K,K^\\dag$, the condition number of $K$ and the norm of $y$. It seems that this upper bound would diverge as we increase the dimension or sample size. This means that the upper bound is vacuous and may not explain the true generalization behavior of the interpolation solutions. If the upper bound is loose, then even if the interpolation solution with the minimal norm achieves the minimal upper bound, this may not convincingly show that it outperforms other interpolation solutions.\n\n2. I have doubts on eq (10). I think the left-hand side and right-hand side of the third identity differ by the term $2K^\\dag Kv_i$. If $K^\\dag Kv_i\\neq 0$, then this identity would not hold. Since $v_i$ can be any vector, the deduction is not convincing. I would suggest the authors to take a close look at it.\n\n3. Lemma 5 is standard in the literature. I would suggest the authors to indicate its connection with existing results, e.g., Lemma 11 in \"Learnability, Stability and Uniform Convergence\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper makes a worthwhile contribution and is to the point. The authors might, however, have overlooked something crucial which leads to the paper having more complex results than necessary.",
            "review": "UPDATE:\nAs the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version. With this I think this is a good paper that should be accepted.\n\n##########################\n\nSummary:\nThe papers addresses the setting of overparametrized models that interpolate the training data, and the related double descent observation in a kernel setting. The overparametrized case of interpolating models is not yet that well understood, but of importance as the success of neural networks is closely related to that setting. This paper shows that the minimum norm interpolating solution is optimal (among all interpolating solutions) with respect to a derived bound on the expected leave-one-out stability, and thus also optimal in the same sense with respect to the excess risk. \n\n##########################\n\n##########################\n\nPros:\nThe paper is well written, to the point, and technically (mostly! see cons) sound. \nTo the best of my knowledge this particular stability analysis is novel, and thus warrants a publication, in particular as the overparametrized case is not that well understood as of yet.\n\n##########################\n\n##########################\n\nCons:\nI actually don't have many cons, I enjoyed the paper. There is however one thing that the authors seemed to have missed:\nThe term V(hat(f_S),z_i) is zero, as hat(f_S) interpolates the training data and z_i is part of it. That doesn't mean that any of the theory is wrong, but that creates two problems in my opinion:\n1. The story about leave-one-out stability does not make sense anymore. In fact the expected leave-one-out stability is just the expected risk for interpolating solutions.\n2. I would imagine that most of the results can be simplified because of that. I could imagine that all the results hold with essentially all terms regarding hat(f_s) being removed. I think the qualitative conclusions would remain the same though.\n\nMy suggestion would be to leave the paper as is (the results as far as I see also hold for not interpolating solutions), and then discuss the interpolating solutions as an extra case.\n\n##########################\n\n##########################\n\nScoring: \nFor now I will have to vote for a rejection, as I am not sure if the problem that I mention can be addressed in one rebuttal phase. But I am happily convinced otherwise, or convinced that I am wrong in any other way.\n\n##########################\n\n##########################\n\nAdditional feedback:\n- When I first read the title I thought that you wanted to show minimal norm solutions are NOT stable, as it has 'minimal' stability. I understand now that minimal refers to the numerical value of the stability definition, but still the wording was somewhat confusing. (Just to consider, no need to change for me if you think it is correct like that)\n\n- Equation (2) and also bit later. You use a comma to separate an index \"S,i\", fairly unusual I would say.\n\n- Remark 4, rework first sentence.\n\n- Equation (7), in the very basic property of RKHS the kappa would depend on x, for you that seems to follow with one of your assumptions + cauchy-schwarz. I would not consider that a basic property.\n\n##########################",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "For interpolating kernel machines, minimizing the norm of the ERM solution minimizes stability",
            "review": "In this paper, they provide the risk bounds of  kernel ridge-less regression (the regularization $\\lambda-\\rightarrow 0$) based on the CV_{loo} stability. They show  that the interpolating solution with minimum norm is the minimal bound of CV_{loo} stability, and can be controlled by the condition number of the empirical kernel matrix, which establishes an  elegant link between numerical and statistical stability. \n\nPros: \n-1:  The stability of Tikhonov regularization has been well studied, but the study for the unregularized regression probelms is lacking. This paper fills the gap for unregularized regression.\n\n-2:  They provide an upper bound on the stability of interpolating solutions, and show that among all interpolating solutions, the minimum norm solution has the best test error. They further provide emprical experiments to verify their theoretical findings.  \n\n-3: They show that CV_{loo} stability can be bounded by the condition number of the empirical kernel matrix, which establishes an  elegant link between numerical and statistical stability. \n\nCons:\nIn Theorem 7, the CV_{loo} stability of minimum norm solutions  can be bound by $C_1\\beta_1+C\\beta_2$.  Note that the kernel matrix $\\mathbf K$ converges to the integral operator $L_K:  L_K(f)(x)=\\int_{X}f(x)K(x,y)dy$ when $n\\rightarrow \\infty$, so $\\|{\\mathbf K}^{1/2}\\|$  and $\\|{\\mathbf K}^+\\|$  may converge to  two constants.  Note that the  condition number of the empirical kernel matrix $cond(\\mathbf K)\\geq 1$,  and $\\|y\\|$ grows with the increase of $n$, thus the $\\beta_1$ and $\\beta_2$ may grow  with the the increase of $n$. However, intuitively,  we think the CV_{loo} stability should decrease with the increase of $n$. So, we think the upper bound proposed in this paper may be too loose.\n\nIf you can answer my concern at this point, I will increase my score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}