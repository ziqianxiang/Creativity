{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Pruning is an important problem in practice. The angle of this study is also interesting. The key concept proposed by this submission is called the \"utility imbalance\" of the weights.  There are many concerns raised by the reviewers. Let us summarize some of them here: (1) hard to follow even for the domain experts; (2) the definition and motivation on \"utility imbalance\" are unclear ; (3) loss landscape visualizations are too much simplified to be informative.  There are also lots of concerns on writings. The rebuttal did help clarifying some details. However, most of the concerns still remain. We hope the detailed comments from the reviewers will be useful for the authors to polish this work. "
    },
    "Reviews": [
        {
            "title": "Inscrutable Claims and Inscrutable Methods",
            "review": "# After Rebuttal\n\nI have read the response to my review and the responses to the other reviews. The summaries of the paper in the other reviews helped to clarify my understanding of the research question the authors were aiming to answer and how they went about doing so. I have re-read the paper and the revisions have helped to make this story clearer.\n\nWith that said, I remain concerned with many technical aspects of the paper, for example:\n* The scale of the networks studied.\n* I still don't understand why this particular definition of utility imbalance is well-motivated.\n* I generally don't think that two dimensional loss landscape visualizations are informative since they discard an enormous amount of information from the full loss landscape. To show that minima are related, I think it is better to use interpolation (mode connectivity).\n\nI am also still concerned about the writing. The revisions, alongside the other reviews, were enough for me to get a sense for the research question and technical story, but I still struggled to make sense of the details.\n\nSince the other reviewers appear to have better understood the paper, I have raised my score to a 4 and decreased my certainty to a 2. I suggest that the AC weight my review much less heavily than the other reviewers, who seem to have better understood the technical details.\n\n# Overall\n\nThis paper is inscrutable. The research question is unclear and the hypothesis is vague and imprecise. The metric being examined (\"utility imbalance\") is never justified in name or in definition, and broad, sweeping, unsubstantiated claims are made that it has to do with how the network uses its weights or the sharpness of the minima (among other aspects of deep learning). Section 3 seems completely unrelated to the previous analysis, and I don't understand how Sections 2 and 3 relate to form a cohesive story. I can't make sense of this paper - its question, its claims, or its method - and I'm an expert reader on topics of pruning and lottery tickets. I can't imagine what a non-expert reader would be able to glean from this paper.\n\n# Score\n\nI therefore strongly recommend rejection (score of 2).\n\n# To Improve My Score\n\nTo Improve my score, the authors need to:\n* Explain what the research question is, precisely.\n* Explain what the hypothesis is, precisely.\n* Explain (in detail) what the justification is for \"utility imbalance\" and why it corresponds to meaningful behaviors of a neural network. The authors will need to include evidence that this corresponds to the claimed attributes of a neural network, which will include a precise definition of what it means for a network to \"utilize\" a weight and how this metric measures that.\n* Explain why this is called \"utility imbalance\"\n* Explain jhow the results in sections 2 and 3 related to each other and the larger narrative/takeaway that they provide.\n* Explain why the network under study changes mid-way through the paper. Is there a reason for this? When I see this in papers, it's usually because the authors tried this on both networks and are only showing the positive results and are hiding bad results; to assuage this concern, the authors should show all results for both networks.\n\nFrom there, I'll need to re-read the experiments to see if I can make sense of them. In its current state, I do not believe that this paper makes a contribution to the scientific literature.\n\n# Notes\n\n## Abstract\n\nWhat do the authors mean by \"the pruning mechanism\"?\n\n## Intro\n\nWhat prior work has claimed that \"the reason for training the large network [is] to obtain a good minimum\"? I don't recall having previously seen this specific claim, nor the implied negation of this claim: that training a pruned network finds a \"bad minimum.\" What makes a minimum \"good\" or \"bad\" and how can we measure this?\n\n\"Cannot achieve similar performance when trained from scratch\" - this only occurs with a new random initialization, as Frankle & Carbin 2018 show (at least for the small-scale networks they examine).\n\nWhat do you mean by a \"utility imbalance\"? Which networks aren't \"utilizing all their weights,\" the unpruned networks or the pruned networks?\n\nAfter reading the introduction, I really have no idea what the paper is about. I don't understand the research question, the main hypothesis, the methodology, or the findings. the authors should make efforts to clarify this story. I'm confused, and that means I'm going to have a hard time making sense of the rest of the paper.\n\nI also can't make sense of Figure 1. I don't know which network this refers to, and I still don't know what a \"utility imbalance\" is.\n\n## Section 2\n\nWhy is there an assumption that a large network \"does not utilize all its weights and thus can easily be compressed?\" There are many possible reasons that one can prune a network, and we often find in the literature that pruning reduces accuracy. The entire point of retraining is to recover this accuracy. But since accuracy went down, it seems that these weights did serve some purpose.\n\nIs N_small the same pruned architecture, or is it a different pruned architecture, a smaller dense network, etc.?\n\nWhat does it mean for a network to \"utilize all its weights\"?\n\nIn Definition 1, does the pruned network include retraining?\n\nWhy is this measure of utility imbalance meaningful or useful? Networks whose outputs have very different distributions may get similar accuracy, so KL divergence doesn't seem to be esxpecially meaningful here. I also don't see what this has to do with utility.\n\n#### Section 2.1.1\n\nI don't follow this logic, and I'm an exceedingly well-informed reader when it comes to lottery tickets. What is the purpose of this section? Is utility being used in a formal way or an informal way? If formal, I don't see a proof or derivation to support the claims here. This seems like an argument without any evidence.\n\n#### Section 2.1.2\n\nIt is unacceptable that the name and details of the network used for this paper are buried in an appendix. The network in question, a small convolutional network, appears to be similar to the small convolutional networks used by Frankle & Carbin. These networks are not representative of larger-scale settings for lottery ticket behavior see (\"Linear Mode Connectivity and the Lottery Ticket Hypothesis\" (Frankle et al., 2020)), and the results in this paper should not be taken to be general.\n\nThe first paragraph of 2.1.2 suggests that the \"utility imbalance\" metric has anything to do with the specified claim made by Frankle & Carbin, and I see no reason to believe that.\n\nI don't know how to interpret the \"utility imbalance.\" It has a fancy name and it has a fancy definition, but I have no idea what it means. Graphs of the utility imbalance therefore aren't very enlightening.\n\nWhy does the utility have anything to do with the network \"utilizing the weights more effectively\"?\n\nWhat does the utility imbalance have to do with the sharpness of the loss landscape?\n\nWhy are the weights and SGD being anthropomorphized? e.g., \"The weights is struggling to be utilized more, rather than SGD purposely differ the utility among the weights?\" \n\nI'm completely lost in this section, as the above comments hopefully convey. I have no idea what is being measured, why it is being measured, or why the results have anything to do with the claims. These experiments are also being conducted on a single, non-standard network, so I don't understand why these results should generally be true.\n\n## Section 2.2\n\nWhat does it mean to \"utilize a weight\"?\n\nWhy were the weights rescaled? I don't understand what \"This was to control the number of feature maps and thus avoid any architectural bias.\" means.\n\nWhy did the experiment suddenly switch networks from what was being used in 2.1?\n\nI made a good-faith effort to read the rest of the section from here, but I wasn't able to make much sense of why the evidence substantiated the claims. Please see my general comment at the beginning of the paper.\n\n## Section 3\n\n\"Empirically, the generalization gap is known to be closely related to the geometry of the loss basin.\" This is hotly contested in the literature. I'm not sure you can say this so strongly.\n\nFigure 6 needs a legend to explain what the networks are. The text never explains what W*, Wp, and Wr are.\n\nI have no idea how this section relates to the rest of the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The assumption does not always hold; imbalance is not surprising; the study on imbalance is quite limited",
            "review": "The paper proposes to answer the question why \"a network with the same number of weights as that of the pruned network cannot achieve similar performance when trained from scratch\". Then it proposes an hypothesis that the small model \"does not utilize all of its weights either\". To prove this hypothesis, it goes on to define and study the \"utility imbalance\" of the weights and its changing with the pretraining, pruning, etc. Some visualization analysis was provided too.\n\nI appreciate the detail empirical studies conducted in this paper, but I have some serious concerns:\n\n1. The paper cites [1] for the assumption that \"a network with the same number of weights as that of the pruned network cannot achieve similar performance when trained from scratch\". This is the basis for the whole study of the paper. This assumption seems to be in contradiction with conclusion in [2] for some cases, including the experimental setting this paper studies. In fact, it is well documented both in [1] and [2] that a network trained from scratch can reach a similar performance as a pruned-retrained network, under relatively large learning rates, in the case of unstructured pruning for CIFAR-10, or any structured pruning. The learning rate used is 0.1 for the CIFAR-10, which this paper also uses. This basically makes the assumption of this study (and thus the goal of proving the hypothesis) at least not valid in some cases. I am aware that this issue on learning rate was mentioned in section 2.1.1, but that still doesn't address the basic assumption and aimed goal of this study. \n\n2. The paper demonstrated there is imbalance in the network weights' \"utility\", but did not try to investigate which part of the weights are of higher utility. I think this is a more important problem than its relation with network size. For example, in [2], the authors showed the imbalance usage within 3x3 convolutional kernels.\n\n3. The utility imbalance is not really a surprising phenomenon. After or during training, only with these utility imbalance we can prune network according to various criteria, and beat random pruning. Thus this should not be surprising to the literature. For imbalance at initialization, the paper directly cites [1] without much further study.\n\n4. The paper only discusses the conclusion at [1] for the utility imbalance at initialization. I think some experiments could help better understand the cause of imbalance at initialization. For example, do shallower layer's weights on average has larger utility? or those weights with large magnitudes? I wouldn't be surprised if the imbalance can be explained by the differences with initialized weight magnitudes. So again the question is how much of the imbalance is explained by the lottery hypothesis, and how much is due to other reasons. This goes back to point 2, where my concern is the paper did not investigate which weights are of higher utility, either at initialization, during optimization or after.\n\n5. The paper title is a bit too vague. The conclusions are kind of vague too. The conclusion section mentioned a lot of content was studied/discussed/defined/investigated, but didn't give what the findings are. This is also a reflection that the paper/study itself didn't have a strong conclusion either.\n\nIn total, there are some interesting analysis on the changing process of the defined imbalance. However, the paper seems to lack a central conclusion that can help future practices, or help people gain deeper understanding. Network weight imbalance is expected, not surprising. The paper did not dig further with the imbalance by investigating which subsets of weights are more useful, and did not give much explanation with experiments on why the imbalance exists. Last but not least, the assumption and the attempted hypothesis does not always hold in the first place, in particular even in paper's experimental setting (large lr). \n\n[1] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks [2] Rethinking the Value of Network Pruning\n\n\nI appreciate the author response but unfortunately they are still a bit vague (1,3), or not supported with experiments (2,4). I still maintain my rating of 4.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction but conclusions are not supported by the selected measurements",
            "review": "Summary: This paper dives into why small pruned networks don’t train as well as large networks. They come up with a measure of weight utilization and claim that networks of all sizes only use a portion of their weights during training, and the imbalance increases during optimization. Additionally, they visualize the accuracy surface on the plane defined by the pretrained, pruned, and retrained networks and find that the retrained networks end up in the same basin as the pretrained networks.\n\n\nOverall: The paper is going in an interesting direction, but I find the paper to be unclear and I do not see how their chosen measurements lead to their claims. Thus, I do not recommend acceptance. \n\n\nPros:\n* It is important to understand the mechanisms of how pruning works, as well as the role of overparameterization in neural networks.\n* It is also interesting to see how the ratio of weights used change with respect to network size or sparsity.\n* Visualizing the loss/accuracy landscape between the three networks (pretrained, pruned, pruned+retrained) is interesting and I have not seen that before.\n\nCons:\n* The need for overparameterization to achieve good performance has been studied before (https://arxiv.org/abs/1805.12076, https://arxiv.org/abs/1804.08838), though I do see the benefit of analyzing this from a different angle.\n* There are also studies of how neural networks do not use all of their weights during training (https://arxiv.org/abs/1909.01440)\n* The given utility measure is not the best way to measure how useful certain weights were. Some weights could have been very important for moving around during training, even if they end up unimportant or at a low magnitude. For instance, overparameterization provides more degrees of freedom that may be necessary in optimization (e.g. as discussed in https://arxiv.org/abs/1906.10732). You could include additional measures such as LCA (from https://arxiv.org/abs/1909.01440) to measure how “useful” certain weights were throughout training.\n* Suggestion: rather than choosing random subsets of weights to prune/ablate in definition 3, why not prune based off of some existing measure? E.g. prune by magnitude, using the n% lowest for $W_i$ (what magnitude pruning would choose) and perhaps the n% highest for $W_j$ for comparison. Your current measure won’t be able to pinpoint if the network heavily relies on, say, a specific 10% of its weights, because pruning a random subset of weights means that you prune both the important weights and the unimportant weights. Currently, this measure only evaluates the network’s sensitivity to random pruning.\n* Section 3 results: while it is useful to visualize the accuracy surface in 2D, a 2D picture does not tell the full story for high dimensional spaces: the existence of a connector in one particular dimension does not mean that two points are necessarily in the same basin. Further, if the pretrained model and the pruned+retrained model are in the same basin, that would mean the pruned+retrained model achieves the exact same accuracy as the full model, which is not the case unless the sparsity is low. Perhaps it would be interesting to see these visualizations at different pruning levels, where accuracy begins to drop. Also, why use accuracy rather than loss?\n\nThere are also several points in the writing that are unclear. While I do not want to penalize the paper for writing style, these issues make it difficult to understand the ideas in the paper. If it is just my misunderstanding, I will gladly read any explanations that the authors can provide.\n* Definition 3: don’t you need a $\\delta$ for utility imbalance? How do you convert standard deviation to utility imbalance? How do you “show that the utility imbalance among the weights exist” if utility imbalance is not a binary measure?\n* Please give a more intuitive description of utility and utility imbalance. What does it mean for networks to have high utility imbalance given the definition you provided? I don’t agree that having a large standard deviation in utility of the random $W_i$ signifies that the neural network utilizes the weights unevenly; the implication I see is that the network’s output is very sensitive to which random subset of weights you prune.\n* Figure 1 left: why would a drop in accuracy after removing weights imply the utility imbalance? Right: what does “ablation regarding the magnitude of weights” mean? How do you choose which weights to ablate?\n* Section 2.1.1: this explains why magnitude pruning works well but does not explain how initialization causes utility imbalance.\n* Section 2.1.2: how is $|\\Delta\\delta|$ the “amount of ablation”? How can $\\delta$ be added to $W$ if $\\delta$ is used in $|U(W_i) - U(W_j)| \\geq \\delta$? Also, my guess would be that utility increases over training because at the beginning of training, the network is close to random, so removing some subset of weights won’t change it as much (can’t get much worse than random) as ablating a more trained network. Thus it is only natural that it increases - I don’t see how this supports the claim that the loss landscape became sharper. Also, what does “each of the weights is struggling to be utilized more” mean?\n* Section 3 paragraph 3: what are the “assumptions for the heuristic”?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Study of a pruning after training using weights' utilization measure",
            "review": "This paper addresses the following important question related to \"pruning after training\" framework: why, in general, smaller network trained from the scratch does not achieve the same accuracy as the pruned network (obtained from pretrained net)? For that matter, authors introduce a new mechanism (\"weights' utility measure and utility imbalance\") to measure a discrepancy between subnetworks (subset of weights) within each network and it seems that having a large value of that measure accounts for the networks not utilizing all of their weights.\n\nIn general, I liked the approach chosen by authors to study the effects of pretraining and weight utilization in pruning. For example, it is interesting to observe that the accuracy difference can be monotonically increasing during neural net training and that the neural networks utilization is proportion to their size. Moreover, such difference (described by utility imbalance measure) is observed independent of network size. As far as I know, such study is novel. However, I don't think that the utility imbalance measure is a unique mechanism for such analysis and one can choose other possible options. In fact, a huge variation of the neural net accuracy depending on which subnetwork (subset of weights) you choose is so obvious that there is no need to show it explicitly (fig. 1). Moreover, the core idea behind pruning is to find such best subset of weights. Therefore, I don't think that such variability can explain why smaller nets cannot achieve the same accuracy as that of pruned net. \n\nAs for the second part of the paper (loss function surface), the accuracy surface is drawn based on only THREE points, specifically the loss of the pretrained net, pruned net and retrained net after pruning. I don't think that this information is sufficient to obtain an accurate visualization. Moreover, again, I believe that the pruned-then-retrained and trained net are lying on similar surface is trivial to show since: 1) pruned-then-retrained almost always perform better than just pruning; 2) if we don't prune too much then we can expect similar performance as the original net. Therefore, I don't see much contribution here.\n\nMinor concerns: \\\n . some parts of the paper is relatively hard to follow (e.g. in Section 2, jumping from experiment setting to discussing results and vice versa);\\\n . colors in some figures (e.g. fig. 1 and fig. 2) are really hard to distinguish since they are very similar.\\\n . section 2.2.2 describes utility imbalance in a pruned network (before or after retraining). I'm just curious what would be that measure for iterative pruning schemes. For example, as in (Han et al., 2015) and [1,2].\n\n[1] Zhang et al. Adam-ADMM: A unified, systematic framework of structured weight pruning for DNNs. 2018.\\\n[2] Carreira-Perpin˜an, M. and Y. Idelbayev. Learning-compression algorithms for neural net pruning. 2018.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}