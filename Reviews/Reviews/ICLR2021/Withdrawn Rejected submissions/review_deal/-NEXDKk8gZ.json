{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper arose a number of questions and concerns among Reviewers that made it get below-average scores (unfortunately, Reviewers did not provide further feedback on the rebuttal). After discussion between the Program Chairs, calibrating decisions across all submissions and, given the drawbacks mentioned below, it is decided that this paper does not meet the bar for this year's ICLR. Therefore, the final decision is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.\n\nPros:\n- Further developing on a simplification of previous approaches (learning diffusion sigmas).\n- Proposal of a new noise schedule.\n- Improving the log-likelihood of diffusion-based generative models.\n- Improving generation time.\n\nCons:\n- Similar FIDs as non-improved approaches (in some cases).\n- Focus on log-likelihood may not be of paramount importance for a generative task.\n- Dichotomy between better FID and better NLL could be further discussed.\n- More comparison with other approaches and further data sets could be done.\n- A bit ad-hoc noise schedule.\n"
    },
    "Reviews": [
        {
            "title": "Interesting techniques for improving diffusion models",
            "review": "**Summary**\n\nThis paper presents rich discussions and various practical techniques to improve the training of probabilistic diffusion models, which include a hybrid objective to learn the variance for improving log-likelihood performance, a different noise schedule tailored for ImageNet 64x64 and importance sampling to reduce gradient noise. Experiments on ImageNet 64x64 and various ablation study provided interesting insights and empirically justified the claims.\n\n**Pros**\nThe paper is well-written and develops some useful practical techniques for improving a recently proposed deep generative model (a modified version of the diffusion probabilistic model in (Jascha, et al 2015)). Specifically, the paper managed to improve the log-likelihood performance by identifying the issue of the simplified objective and proposed to learn the variance using a hybrid objective. I think this technique along with others are useful practical techniques to improve the training of diffusion models.\n\n**Questions & Concerns**\n- No results on CIFAR: Most recent papers in this field considers CIFAR-10 as the standard benchmark to report generative performance, including the original DDPM paper [1] and the score matching paper [2]. Although ImageNet 64x64 is a larger dataset with more complicated structure and diversity, outperforming pervious strong baselines in CIFAR-10 is still challenging and non-trivial. Thus the empirical study will also be more convincing to demonstrate that the proposed method can indeed achieve much better log-likelihood without sacrificing sample quality too much. Otherwise, it's hard to get a sense of how much improvement has been actually achieved by directly looking at the numbers in this paper and the ones in previous papers.\n\n- As the major contribution, the variance parametrization (Eq 16) needs more insightful discussions. For example, why this is the case: \"Since Figure 1a shows that the reasonable range for\u0012$\\Sigma(x_t; t)$ is very small, it is clear that we should not use a neural network to predict\u0012$\\Sigma(x_t; t)$ directly.\". Can we predict the log of the variance with a neural network directly? The proposed one is only an interpolation between $\\beta_t$ and $\\tilde{\\beta}_t$ - is this expressive enough?\n\n- About the noise schedule: is this a generally better noise schedule, or it is only tailored for ImageNet 64x64. In the latter case, I think it is only a trick that overfits a specific dataset. To improve the training of DDPM generally, is there any advice on how to find a good noise schedule?\n\nI will consider raising my score if the above concerns can be addressed.\n\n[1] Denoising Diffusion Probabilistic Models\n\n[2] Generative modeling by estimating gradients of the data distribution",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The authors explore behavior of likelihoods for diffusion models. Some useful experiments. Need work.",
            "review": "The paper talks builds upon the recent work from Ho (2020) about generative models that use noise diffusion. The authors suggest that the proposal in Ho can not only be used in good quality sample generation (as already shown by Ho), but also leads to reasonable improvements in likelihood. Overall, some of the ideas presented in the paper are interesting and useful; but the paper overall needs work. \n\nOther questions/concerns:\n1. Firstly, from an application point of view, what does achieving a high log-likelihood mean, if the samples are already good enough or high quality? \n2. How do we interpret the bits/dim metric here? Its rather hard to rationalize that a change in 0.01 makes sense in this metric? And more generally, what are we aiming for in terms of a reasonable change?\n3. In section 3.1; how did we end up needing to tune big-sigma_theta (x_t, t) while arguing that fixing small-sig_t^2 is ok? This is in section 3.1 second paragraph; Either I am missing something of the argument here is that we need to tune noise variance and cannot fix it? \n4. What is the intuition behind expecting the (squared) cosine schedule to work? It is interesting to think that a periodic decay noising schedule is better than a linear one? \n5. And related to that, do not understand this weird value of 0.008 for s? The whole point here is some small non-zero s is ok; why specifically 0.008?! \n6. One of the main conclusions in section 3.4 is kind of confusing --- based on the summary, if we are not interested in sample quality but only interested in maxing of likelihood, then the proposal of this work is not good, and working with L_vlb suffices? Is this correct? Based on the motivation, it seems the opposite was being claimed i.e., L_hybrid is important for maxing of likelihood (third para in introduction)? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "Denoising diffusion probabilistic models have been proved to produce excellent samples in the image and audio domains. However, it has yet to be shown that they can achieve competitive log-likelihoods. This paper shows that with several small modifications, diffusion models can achieve competitive log-likelihoods in the image domain while maintaining high sample quality. This paper is well-written and good-organized. However, I have the following concerns.\n\n1.\tThe authors claim that the noise schedule used in Ho et al. (2020) was, experimentally, sub-optimal for ImageNet $64 \\times 64$, which lacks theoretical guarantees.\n2.\tThis manuscript is mainly based on the previous work Ho et al. (2020). The novelty seems to be too limited.\n3.\tI am not convinced that only one dataset (ImageNet $64 \\times 64$) is sufficient to demonstrate the performance of the proposed strategy.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Tricks to improve log likelihood of diffusion models while maintain their sample quality",
            "review": "The paper found several methods to improve log likelihood of diffusion models while maintain their sample quality, including cosine instead of linear noise schedule, using a hybrid objective to learn parameters of the covariance function, and using importance sampling to improve the gradient noise. The authors also explore how sample quality and log likelihood scale with the number of diffusion steps and model capacity. Experiments on 64x64 ImageNet dataset show competitive llh while keeping the sample quality.\n\nClarity: as the denoising diffusion model, especially its success in generating high-quality image samples, is still quite new, it would be beneficial if the paper could describe the technical background of it and the existing variational training methods in more details. Section 2 kind of serves this purpose, but it misses many important steps and focuses more on defining terms used in later sections.\n\nSignificance of this work: The necessity for having larger log likelihood for the diffusion model is not very well motivated. If the model is mostly used to generate high-quality samples and we have known how to train it to do so, why does the LLH values still matter?\n\nOriginality: using cosine noise schedule, new parameterization and hybrid objective seems effective for training, but doesn't seem to be very innovative. But I'm not very familiar with the denoising diffusion model and could be wrong. The results on how the model scales with computation seems trivial and may have been known already. Finally, it'll be beneficial if the authors can verify findings got in this paper can apply to other dataset or types of data more broadly.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}