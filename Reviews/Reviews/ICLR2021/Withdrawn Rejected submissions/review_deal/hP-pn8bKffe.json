{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper studies the problem of tuning a pretrained deep generative model p with an external criterion f, aiming to bias the model to produce samples that maximize f. To this end, a variational framework is presented, which formulates the problem as a constriant optimization problem. Specifically, a variational distribution q is introduced, and the the problem is to maximize the expectation of f under q, subject to q being close to the original model p. Some case studies are presented, such as tuning in latent space, assessing the different choices of the crieria f, and an application on sampling energy consumption aggregated curves.\n\n* The problem of refining pretrained DGMs with an criterion is interesting. However, the core formulation proposed in the work is indeed not new. The variational framework of incorporating a criterion f (or, a constraint) has been developed in the posterior regularization (PR) paper:\n\n     Ganchev et al., JMLR 2010, Posterior Regularization for Structured Latent Variable Models\n\n PR is then extened and applied to deep generative models, e.g., in \n\n     Hu et al., NeurIPS 2018, Deep Generative Models with Learnable Knowledge Constraints\n\n which further studies automatic tuning of the crierion (constraint) f.\n\n* This submission discussed a couple of intersting application scenarios of the framework, such as tuning in the latent space z while fixing the generative model p(x|z), and using a classifier h as the criterion. Yet, these scenarios are not completely novel either. Related work includes:\n  - Nguyen et al., 2016, Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space\n  - Dathathri et al., 2020, Plug and Play Language Models: A Simple Approach to Controlled Text Generation\n  - Qin et al., 2020, Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning (this one was published after the submission deadline, though..)\n\n* The case studies and experiments are mostly toyish for proof of concepts.\n\nThe paper needs significant revision regarding the claims of contributions.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "[Official Review] This paper is unclear and lacks a convincing demonstration of the approach's utility.",
            "review": "------------------------------\nSummary:\n-------------\nThis paper addresses the topic of \"biased sampling\" of a generative model.\nThis is a setting where we wish to preferentially sample realistic examples that meet some (e.g. soft) desirability criterion.\nThe extent to which a given example $x in \\mathcal{X}$ meets the criteria is expressed via some function $f : \\mathcal{X} \\rightarrow \\mathbb{R}$ , which is assumed to be provided.\n\nSpecifically, for a generative model with distribution $p$, the authors wish to sample from $q_\\beta \\doteq \\frac{p(x) \\exp{\\beta f(x)}}{Z(\\beta)}$, where $\\beta$ is set so as to bound the KL-divergence between this distribution and $p$, and $Z$ is the partition function.\nA generative model could be trained from scratch to match $q_\\beta$, but the authors' approach assumes that the generative model has already been trained.\nThey do not clearly motivate this restriction, and doing so would strengthen the work.\n\n------------------------------\nReview:\n-------------\nThis topic is interesting and significant.  \nAs motivation, GPT-3 cost ~$10M to train, so being able to bias the sampling of a generative model without retraining the model seems potentially very useful.\n\nHowever, the paper suffers from significant flaws: \n1) It is not clear and is not well written.\n2) It does not provide sufficient motivation for the proposed approach.\n2) It does not provide convincing demonstrations of the proposed approach.\n4) The authors state that their approach: \"is to our best knowledge the first and only alternative to rejection sampling in this context\", missing the important related work of \"Fine-Tuning Language Models from Human Preferences\" by Ziegler et al. (LfHP)\n\nElaborating on points 1-3:\n1)\nI found the quality of the writing to be insufficient in terms of organization, motivation, and clarity of individual sections or statements.\nOverall, I think the background on generative models and normalizing flows would be insufficient and confusing for someone lacking background in these areas. \nOne reason I found the paper a bit hard to follow is that sections do not explain how their content fits into the larger picture, and so there is an overall lack of organization to the presentation.\n\n- It wasn't clear to me when the experiments operated in the latent space (as described in 3.3).  Also, while the authors claim that \"optimization criterion remains well-defined\" in this setting because the KL-divergence in latent space upper bounds the divergence in the input space, they don't discuss the characteristics of this bound.  What we care about is not whether the criteria is \"well-defined\", but whether it is sensible; showing that it is requires more than a bound, we need to know how good the bound is liable to be.\n\n- It wasn't initially clear what sort of assessment of a criteria section 4.2 is meant to provide.  It remains unclear whether this assessment method was used in practice.  Figure 1's caption wasn't very clear (what gradient is this the norm of?), and didn't really match the legend.  Also, the rest of the paper seems to assume that the criterion is provided, but this section suggests that the designer can modify the criterion at will.\n \nHere are some more examples of issues with the writing:\n\n- The first paragraph of the intro says: \"the fine-tuning of the generative model is seamlessly integrated within the learning process\", but doesn't explain what this means (e.g. what is fine tuning, what fine-tuning are they referring to, etc.)\n\n- Statements such as \"Probability distribution learning is most generally tackled within the Variational Inference (VI) framework.\" are unclear and lacking justification.  This seems to claim that all probability distribution learning can be viewed as a special case of VI, which seems false.\n\n- \"distribution spaces\" is not a clear paragraph title.\n\n- The paragraph on \"Refining Generative Models.\" doesn't explain what it means to refine a generative model.\n\n- Undefined or poorly defined terms, e.g. \"lequi-distribution\"\n\n- It is not self contained: many terms are not defined (formally or informally), e.g.\"lequi-distribution\", and definitions or descriptions of key concepts are too vague or misleading.\n\n- \"the normalizing flow architecture\" doesn't make sense; normalizing flows are a family of models, not a specific architecture.).\n\n- Section 3.2 begins \"It is seen that Eq. 4 essentially defines a Variational Inference (VI) problem for each β value\".  Does this mean the the authors have shown this, are about to show it, consider it self-evident, or something else? \n\n- \"Therefore qβ will expectedly have its typical set (Nalisnick et al., 2019) roughly included in the typical set of p. \"\nThe reasoning here is not clear, and typical set should be defined. \n\n- The part about privacy was unclear and unconvincing.  This claim is unsupported: \"The eventual biased generative model will expectedly both inherit the privacy guarantees of the general model,\"\n\n2)\nThe authors do not make the motivation clear enough.\n- Why can't we just incorporate the desirability criterion into the original training process?  The authors distinguish their approach from this alternative in the first 2 paragraphs of the introduction, but don't explain why their approach is better.\n- Why is it important to bound the KL-divergence to be less than C_D, and derive beta from the bound?  We can just treat beta as a hyperparameter instead of C_D, which would make section 3.1 irrelevant.  \n\n3) \n- The experimental validation is just an unconvincing proof-of-concept, and the authors don't compare to alternatives such as i) LfHP, ii) training from scratch to match $$q_\\beta$, or iii) rejection sampling.  Especially since they argue for the superiority of their approach to (iii), this should be demonstrated empirically.\n- It would be useful to compare generated samples to real data, to see if these samples are actually realistic.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A potentially interesting theoretical framework for controlling generative models with preliminary experiments",
            "review": "Summary\n-------------\n\nThis paper deals with the problem of specializing a pre-trained generative model \"p\" on a downstream task or reward function \"f\". This problem is related to the objective of building \"controllable generative models\" post-hoc (i.e. once a generative model has been trained already) and has been received some attention in the literature (https://arxiv.org/pdf/1711.05772.pdf, citation missing).\n\nThe authors draw on the general framework of energy-based models (https://arxiv.org/pdf/1910.14265.pdf) and formalize this problem as learning a distribution \"q\" with the same support as \"p\" that maximizes the expectation of \"f\" under \"q\", weighted by a constant \"\\beta\", and minimizes the KL divergence term \"KL(q||p)\". The minimizer to this problem is an energy model q_\\beta(x) \\propto p(x) * e^{\\beta f(x)}. This seems to follow the derivations in the method of Maximum relative Entropy (https://arxiv.org/pdf/0710.1068.pdf or \"Updating probabilities with data and moments\" from Giffin and Caticha, but citation missing).\n\nAs the author note, this problem is also reminiscent of maximizing a \\beta-weighted ELBO where one learns a distribution \"q\" over samples to maximize a weighted ELBO: \\beta * E_q[f(x)] - KL(q(x)||p(x)), which is reminiscent of beta-VAEs (cited therein). The author also propose to \"operate in the latent space\" (section 3.3) which basically boils down to maximizing the ELBO wrt a new \"prior\" distribution q(z) that selects a subset of latent codes that lead to samples maximizing f(x). x ~ p(x|z)q(z). The authors report some proof-of-concept experiments on sampling top energy consumption and on biasing MNIST generation towards a specific class labels. \n\nIn general, I think this paper tackles an interesting problem and proposes an interesting interpretation within the overarching mathematical framing of energy-based models. However, I found it hard to really discern what's the contribution that is put forward by the paper with respect to what exists in the literature. I feel that that the paper lacks clarity on some fundamental aspects (clarity on core contributions, validity of Algorithm 1 and use cases) which affect my overall judgment. Finally, the paper has somewhat of a preliminary / proof-of-concept experiments illustrating the potential of the approach. This makes the experimental evaluation a bit weak in terms of tasks and in terms of tested competing approaches. This wouldn't be so much of a problem if there was more clarity about the contributions.\n\nPros\n-------\n- The paper uses the encompassing general mathematical formalism of energy based models for post-hoc tuning of generative models.\n- The paper is clearly written.\n\nCons\n--------\n- The main contributions are a bit hard to discern.\n- Experimental section is rather weak and the paper does not seem to have comparisons with relevant related approaches in the literature.\n\n\nQuestions\n--------------\n\nI found hard to discern what are actually the contributions and which solutions actually ended up being used in the experiments.\n\n- Eq. (1) - (4) follow closely the principle of maximum relative entropy (https://arxiv.org/pdf/0710.1068.pdf), it would be worth to make the connection?\n\n- Algorithm 1 bears the name of the paper \"BTGM\", thus this means that it is supposed to be one of the core contributions but I am not sure if it is used for any of the experiments? How complex is estimating Algorithm 1 ? How do you perform Monte-Carlo approximation of samples from q_\\beta if q_\\beta is an energy model and thus cannot be explicitly normalized ?\n\n- Maybe I missed this, but where are Eq. (8) and (9) used ? For which task ?\n\n- How does Section 3.3 relates to Section 3.2 ? Given that you operate in latent space, you won't need a flow based model g to transform input samples x ?\n\n- How do you obtain the results in Appendix D ? Are you using the optimization in latent space as described in Section 3.3 ? If yes, is this optimized by standard SGD on a ELBO-like objective ? If yes, how does this compare to explicitly applying Algorithm 1 ? \"use a GAN model p\", please provide details about this.\n\n- In order to make the experimental section a bit stronger / informative, could one maybe take a task from https://arxiv.org/pdf/1711.05772.pdf  and compare with that approach ? How can you come up with a relevant baseline ?\n\n\nGrammar\n-------------\n\n- typo in the title, should be \"Boltzmann\" ?\n- wlog -> without loss of generality (wlog)\n- Section 4.2 \"an criterion\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}