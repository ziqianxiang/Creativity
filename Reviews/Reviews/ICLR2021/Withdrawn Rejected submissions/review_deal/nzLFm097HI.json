{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers are in agreement that this paper could benefit further improvement. There are several areas: novelty of the proposed approach and evaluation on real-world datasets (beyond just CLEVR)."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper proposes a neuro-symbolic model for sample-efficient VQA, which turns each question into a probabilistic program which is then softly executed. The problem explored in the paper and its background and context presented clearly and it does a good job in motivating its importance and trade-offs between possible solutions. While the use of a probabilistic program to represent the questions might be too stiff / inflexible in my opinion and may not generalize well to less constrained natural language, this direction is still of course important and interesting. It also does a great job in presenting the existing approaches and comparing their properties. The writing is good and the model is presented clearly with a very useful diagram. \n\nHowever, the novelty of the paper seems limited to me, as it mainly combines together ideas that have been extensively explored in many prior works which are mentioned by the paper. Turning the question into a series of attentions over semantic factors appears in the NSM and partially in MAC models. The iterative memory updates appeared in MAC. Combining together small operations and functions defined by hand as dictated by programs as in page 6 of the model is the main idea of Module network. End-to-end differentiability for VQA models has also been extensively explored and multiple solutions have been proposed: relations networks, soft variants of NMN, and also MAC and NSM, etc. The use of stacks has been explored too in the stack-NMN model. I therefore feel the paper mostly recombines and tunes together these ideas rather than offering one particular new idea or insight. \n\nThe paper presents results on CLEVR only, which goes back into my concern about the inflexibility of the probabilistic programs. Especially for this type of models, it will be useful to explore it on tasks beyond CLEVR such as VQA/GQA to show whether it can work for natural or richer language. The use of Mask R-CNN on CLEVR is also quite unreasonable in my opinion: the task has meant to be visually simple, so using a very strong visual model on it nullifies the visual aspect of it completely, making the model working on perfect semantic scene graph inputs rather than on \"real/natural\" uncertain and more noisy inputs. It also gives unfair advantage to the model when comparing to baselines which didn’t use object detectors on CLEVR but rather work directly with the image, e.g. MAC and others (presented in the table in the experiments section).\n\nAt the same time, it is important to mention the model does get quantitative improvements in scores and especially sample efficiency, but the paper doesn’t make it clear what is the particular property or part of the model that allows for the improved numbers, and so the paper doesn’t leave the reader with a clear new takeaway message.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Missing critical comparison",
            "review": "**PAPER SUMMARY**\n\nThe paper presents a method for visual question answering (VQA) that makes use of a differentiable program executor that softly approximates the execution of neural modules that read and write to a stack. Experiments on CLEVR demonstrate that the model can achieve high performance using small amounts of supervision (varying the amount of question/answer pairs, image/object annotations, and question/program annotation used during training).\n\n**STRENGTHS**\n- The method achieves differentiable program execution without resorting to training with policy gradients\n- Experiments demonstrate high accuracy on CLEVR, even using very little training data\n\n**WEAKNESSES**\n- Critical missing citation / comparison: Prob-NMN (Vedantam et al, ICML 2018)\n- The model hardcodes a lot of task-specific knowledge; is this useful?\n- The writing is very unclear in some places\n\n**UNCLEAR WRITING**\n\nI find the paper to be quite unclear about exactly what type of supervision is used where. Section 3.4 states that “We pre-train our models with a very small percentage of visual and textual data” -- but what kind of data is this exactly? If I understood the following discussion, I think that “image data” refers to (image, object) annotations used to train the Mask R-CNN model, while “textual data” refers to (question, program) pairs used to supervise the program executor. Then in Table 3, “X% pre-train” refers to the amount of (image, object) and (question, program) annotations used to supervise the model, and “Y% of Training Data” refers to the number of (question, answer) pairs used to supervise the model. I think that the clarity of the paper would be much improved by making it more clear as to what types of supervision are used at different stages.\n\nI similarly found Section 4.2 and Tables 4 and 5 very unclear, and I’m not entirely sure what experiments are being performed; what does it mean to “fix vision and train text from 0.1% pretraining” in Table 4, or “fix text and train vision from scratch” in Table 5?\n\n**MISSING COMPARISON: PROB-NMN**\n\nOne of the main draws of the proposed method is that it is extremely data-efficient on CLEVR, e.g. achieving 99.1% accuracy when supervised with 10% of (question, answer) pairs and 1% of (image, object) and (question, program) pairs.\n\nThis sort of data-efficient learning is also a key feature of Prob-NMN [1]; this is a critical piece of prior work which is not cited; the proposed method be discussed in light of this prior work, and the method should be compared against it. From Table 2 of [1], Prob-NMN achieves 97.73% test-set accuracy on CLEVR when supervised with 100% of (question, answer) pairs and 0.143% of (question, program) pairs; this outperforms the 93.1% reported for DePe in Table 3 of the submission which uses 100% of (question, answer) pairs and 0.1% of (question, program) and (image, object) pairs. However this comparison is not fully valid, since Prob-NMN uses slightly more program annotations but uses no explicit object detector. The authors should set up a more controlled experiment to compare their method against Prob-NMN.\n\n[1] Vedantam et al, “Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering”, ICML 2018\n\n**HARDCODED REASONING**\n\nThe proposed model achieves data-efficiency by encoding a huge amount of task-specific prior knowledge into the model architecture:\n- The use of any ground-truth programs assumes that the set of primitive operators needed to answer questions are known\n- The module designs in Table 2 go a step further, and assume that the implementation of all of these primitive operators is known -- the operators in Table 2 are effectively a differentiable approximation to the ground-truth CLEVR program executor\n- The use of an object detector assumes that all questions can be answered in terms of object positions and properties; that the vocabulary of semantic concepts needed to answer questions is known; and that furthermore the mapping of images -> object locations and properties can be supervised during training\n\nTaken together, the model essentially hardcodes the data-generation process of the CLEVR dataset into the model. This is evident from Table 3: if I’ve read and understood the experimental setup, training with 1% of (question, program) annotations and (image, object) annotations and zero (question, answer) annotations achieves >90% test-set accuracy on CLEVR! To me this is not a positive result -- instead it signifies that the model architecture itself is overfit to the dataset.\n\nThe higher-level research goal of building neuro-symbolic models that elegantly blend prior knowledge with learning is extremely exciting. But testing this kind of model on CLEVR is not. CLEVR is a synthetic dataset, and its questions are not natural: they are generated programmatically, and there is a lossless and deterministic mapping between question text and semantics (encoded as functional programs). CLEVR images are likewise not natural, and the image semantics necessary for answering any CLEVR question can likewise be losslessly encoded into a scene graph. As such, the fact that this model achieves data-efficient learning on CLEVR is nearly as uninteresting as the fact that the ground-truth CLEVR program execution engine achieves perfect accuracy when fed with ground-truth scene graphs: it is nearly guaranteed by construction, and only works by exploiting the synthetic nature of the benchmark.\n\nMy biggest concern with this line of work is that I don’t see how it could possibly be used to achieve visual reasoning on real-world data, where none of the above can be exploited: for true natural language and images, we do not know a vocabulary of symbolic primitives that can losslessly represent the world and we certainly can’t hardcode the implementation of all symbolic primitives necessary for performing real-world reasoning.\n\nSo although this method may indeed achieve extremely data-efficient learning on CLEVR, this is not an interesting achievement on its own unless there is some hope that the method used to do so can also be used for non-synthetic, real-world data. And the submission presents no evidence, or even an argument, for how this might be done.\n\n**OVERALL**\n\nOn the whole, I think that this paper is not ready for publication due to unclear writing, and the fact that it is missing a critical comparison to Prob-NMN.\n\nI also take issue with the underlying approach, since I think the methods used to achieve data-efficiency on CLEVR cannot scale to real-world data. However I can accept that different people in the community feel differently from me on this issue (as evidenced by excitement over papers like the Neuro-Symbolic Concept Learner), so I would not necessarily consider this as a disqualifier if it were my only concern with the paper.\n\n\n**AFTER REBUTTAL**\n\nI appreciate the author's efforts to improve the clarity of the submission and to add a comparison with Prob-NMN. However I remain unconvinced that this style of model can possibly scale to more realistic data. I've upgraded my score from 4 to 5, but I still generally lead toward rejection.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting direction but the paper really needs more work",
            "review": "This paper studies visual question-answering (VQA). The authors proposed an end-to-end differentiable framework that performs \"soft\" logical reasoning based on object-centric scene representations and attention-based language parsing. The authors claimed that the new model is more data-efficient.\n\nWhile I think this direction is interesting and useful, I think the submission really needs more work before it may be published at a top-tier conference. The current manuscript is hard to understand and the results are not convincing.\n\nFirst, the method is not well-motivated. The intro mostly discussed related papers and their differences, so I was expecting an analysis paper, but it turns out the rest of the paper is about a new model. There is no justification for why we need a new model, what is new about it, and why we expect it to do better. \n\nApart from this, the writing is in general unclear. For example, the model has an acronym DePe, but the full name was never fully told. The experiment section is also poorly written. In 4.2, it's unclear what \"Train Ratio\" means in Table 4 or 5, and these tables were never referred to in the main text.  I also don't understand the setup in Section 4.3, either, as essential descriptions are completely missing.\n\nThis makes it hard to assess the results.  In addition, all results are on the CLEVR dataset. One selling point of methods such as MAC, NS-CL, and Stack-NMN is that they can naturally be applied to real images such as the GQA dataset. Without experiments on real images, the analyses presented in this paper are unlikely to convince researchers in this area.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}