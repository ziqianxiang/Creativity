{
    "Decision": "",
    "Reviews": [
        {
            "title": "Please explain why combining the two proposed mechanisms works.",
            "review": "Summary:\nThe authors propose two mechanisms: (1) example-driven training and (2) observers. In the experimental results, the authors show that the ConvBERT enhanced with these mechanisms outperforms SOTA in both the full data and few-shot settings.\n\nReasons for score:\nOverall, I would vote for weakly reject. The motivation is convincing, and the results are impressive. However, there are some concerns about the clarity of the relations between two proposed mechanisms and the comprehensiveness of the experiments. See the pros and cons below.\n\nPros:\n\n-- The motivation of the example-driven training is straightforward and convincing. Illustrating the procedure as in Figure 1 helps readers to understand the proposed method comprehensively.\n\n-- The paper focuses on transferring the pre-trained BERT to intent classifications, which is a fundamental problem in modern dialogue systems. \n\n-- The experimental results are impressive on both full data and few-shot settings.\n\n\nCons:\n\n-- The relations between example-driven training and observers should be further discussed. The authors have to explain why combining the two proposed mechanisms works. For now, the paper looks like the authors randomly combine two proposed methods, and it just works accidentally.\n\n-- The idea of using metric learning to intent classification is not a new idea. For example, [1] (not included or mentioned in the paper) proposes to do intention detection with the Siamese network by utilizing BERT as the encoders, which is very similar to the proposed approach in this paper. \n\n-- Modifying the attended candidates of BERT cannot be considered as a new idea [2]. The paper describes the motivation of adding [OBS], but whether [OBS] solves the issue is not verified. Adding extra experiments to show whether the dilution happens or not with and without [OBS] would be helpful. \n\n\nQuestions: \n\n-- The description of Observers is confusing. Given the fact that [OBS] has position embeddings, the placements of the [OBS] affect the results. Is the [OBS] token always concatenate with the input sentence, as shown in Figure 2?\n\n-- Is the extra [OBS] token necessary? Can we solve the dilution problem by modifying [CLS]? If we simply modify the [CLS] token as the way of [OBS] token (attend to other tokens but never attended to) instead of using an extra [OBS] token, what would happen? \n\n\n[1] Intention Detection Based on Siamese Neural Network With Triplet Loss\n\n[2] Attending to Future Tokens for Bidirectional Sequence Generation",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper employs example-driven training and observers in the task of intent prediction in dialogue and achieves good results on three datasets in both full and few-shot learning settings.",
            "review": "* Summary:\nThis paper proposes two technics to tackle the intent prediction task: 1) example-driven training; and 2) observers. Example-driven training utilizes the intent prediction task to learn similar latent representations for semantically similar sentences. Observers uses a set of tokens that attend to all the input tokens but are not attended to, to solve the representation dilution problem where the model attends too much to the special tokens (e.g. [CLS] and [SEP]). The experiments suggest the use of example-drive training and observe achieves SOTA results on three different datasets in both full and few-shot learning settings and can be easily adapted across datasets and to unseen intents. \n\n* Quality:\nThe motivation to learn similar representations with the intent classification task is clear. The model is straightforward but not super exciting. The experiments and analyzes are through. \n\n* Clarity: \nThe paper is in general written clearly and easy to follow, with a few points to improve listed below. \n1. In 2.3. It's not entirely clear to me how the intent is predicted. e.g. calculate the distance between the learned representations of different sentences, and use the label of the closest sentence? \n2. It seems to me the use of observers alone is not helping much... could you elaborate on the possible reasons? Also, in 4.3, \"Similarly to Conneau et al. (2018), we avoid using the entire vocabulary for this probing experiment and instead use only the most frequent 1000 words for each dataset\" Why the most frequent 1000 words? Maybe a good F-1 on the less frequent words would better show the observers' ability to learn representations?\n3. Can the model be applied to open-domain dialogue intent classification?\n\n\n* Significance:\nThe model could be potentially very useful for open-domain dialogue intent classification as well and for tasks where large-scale annotation is not available.\n\n* Pros:\n1. The model is easy to interpret and works well under few-shot setting.\n\n\n* Cons:\n1. The observer doesn't seem to help much. Maybe attending to the special tokens is still important? Instead of not attending at all, the authors could still try attending to them, but not attend so much.\n2. Besides, some qualitative measurements such visualizing some sentence embeddings or sentence similarity comparison are appreciated and can support the claim that the model is learning similar representations for similar sentences.\n\n \n\n\n* Typo: \n1. In 4.3 \"Similarly to Conneau et al. (2018)\" --> \"Similar to\"\n2. In 4, \"Finally, we carry out a probing experiment demonstrates that\" --> \"Finally, we carry out a probing experiment that demonstrates that\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not ready to be accepted",
            "review": "The research topic studied in this paper is intent prediction. To improve the prediction performance of BERT, this work introduces a new special token called [Observer] and claims that it can address an existing issue of BERT caused by the special token [CLS] token. Combining with example-driven prediction, the experiments on three intent prediction datasets show the benefit of adding this special token. \n\nUnfortunately, this paper raises more questions than it actually addresses. \n\n1) about example-driven training, my understanding is that this is basically example-based or prototype-based prediction methods in statistical machine learning. A typical example $k$-nearest neighbor, as mentioned in the related work section. However, one well-known limitation of example-based prediction methods is that they require a large amount of data to guarantee prediction performance. Based on the description of the three datasets used in this work, clearly, they are not big enough. Then, is there any strong motivation for using example-based methods? \n\n2) about the [observer] token, it is not clear how the model was trained after adding this special token. It is possible that I missed some details of this work, I did find a description of pre-training after adding this special token. In addition, the classification task was still based on the [CLS] token based on Figure 2. With these two pieces of information, I think this special [observer] token was not actually involved in any training if my understanding is correct. \n\nIn addition, two statements in section 2.4 need further clarification or evidence\n\n- \"we aim to disentangle the relationship between the representation of each word in the input and the final utterance level representation\" I am not sure how to understand this statement and why adding the [observer] token can help on this?\n- \"we hope to avoid the risk of diluting the representation ...\" I strongly recommend to give some analysis regarding this statement and show some evidence. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Experiments need to be improved",
            "review": "Summary:\n \nThe paper introduced two approaches to improve the generalizability of text classification models. The first is matching based classification which is similar in spirit to Matching Network [1],  the second is using several “observer” tokens to aggregate the word representation for extracting better utterance level representation. Empirical studies on three intent prediction datasets show that combining these two approaches yields the SOTA performance on both few shot and full training settings. However, there is no consistent improvement when the two approaches are applied independently.\n\nPros:\n \n1. Extending metric-based meta learning to the intent classification is an interesting application.\n2. The presentation of this paper is excellent. The visualizations of the two approaches are very clear.\n3. The combination of the two proposed approaches shows very strong empirical results on the three intent prediction datasets.\n\nCons:\n\n1. In general, this work obtain SOTA results with the combination of multiple approaches, e.g., MLM, matching based classification, observers, which make it hard to understand how each method can generalize cross tasks.\n\n2. The motivation of having multiple “observer” tokens is not very clear to me. If the “observer” token can extract better representation by attending to all the tokens of intermediate layers, there should be a fair comparison between representation of a single “observer” token and a [CLS] token.\n\n3. The comparison in Table two seems not fair. Zero-shot CONVBERT is not comparable to CONVBERT + MLM + Example. There should be a finetuning baseline for unseen intent setting.\n \nQuestions:\n\n1. Have authors tried to apply the idea of “observer” tokens to other text classification tasks? It will be helpful to understand the effectiveness of this approach.\n\n2. Will the performance get further improved by using the centroid of the support set as intent embedding, like in [2]?\n\nReference:\n\n[1] Matching Networks for One Shot Learning\n\n[2] Prototypical Networks for Few-shot Learning\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}