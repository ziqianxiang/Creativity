{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed a new measure of effective gradient flow (EGF), and also compared sparse vs. dense networks on CIFAR-10 and CIFAR-100. The notion of EGF would be interesting, but the paper did not present enough evidence to support this notion."
    },
    "Reviews": [
        {
            "title": "Interesting direction, though the particular approach may not be the most impactful",
            "review": "Summary: This paper looks at how choices in optimization affect how well you can train sparse networks. The authors come up with a new measure of effective gradient flow, which is important for good performance. They also compare sparse vs. dense networks across various optimizers, hyperparameters and activation functions, and find that batch norm and certain activation functions are beneficial for sparse networks\n\nPros\n* It is important to understand what’s better for training of sparse networks. There are previous studies of the effect of overparameterization, but much less about sparse networks vs. dense networks at the same size.\n* The authors did a multitude of experiments thoroughly to compare different training configurations\n\nCons\n* While it is interesting to compare sparse network to their same capacity dense networks, this comparison is less practical because small dense networks are rarely used in practice.\n* I’m unsure about the novelty or rationale of the EGF definition: shouldn’t masked weights already have a gradient of 0? What is the rationale for not weighing each layer’s gradient equally, even if some have more weights than others?\n* How are the sparse connections determined? If it is random, then the findings are less applicable because people would use sparse networks found by a good pruning method rather than a random sparse network in practice.\n* While table 1 shows that EGF is better than simple gradient norms, gradient flow measures in general do not seem that important to the rest of the paper: why do we need gradient flow when we can just look at accuracy? Is EGF used for anything besides as an observation? Also, it is unclear in figure 2 if high EGF can explain good performance or if they are just somewhat correlated.\n\nOther questions:\n* How do you specify the number of active connections if $a_D$ of a fully connected layer is determined by number of inputs * number of outputs? Do you adjust the number of inputs or outputs, or both? How does the procedure work with conv layers? \n* Table 1: when do you take EGF measures - throughout training or at the beginning or end? Also, for the experiments in figure 2, how does EGF change over time and is it averaging EGF over training the best way to use this measure?\n* Why use a Wide ResNet rather than a regular ResNet?\n* Doesn’t data augmentation hurt dense networks more for adam?\n* What sparsity level does table 3 use?\n\nAdditional suggestions\n* Perhaps it would also be useful to see the effect of adjusting more common/simple hyperparameters such as learning rate, batch size, and momentum.\n* I would like to see accuracies of the experiments in table 3: even though it is scientifically interesting to see where sparse networks do better relative to dense networks, ultimately the goal is to get the best performing sparse network in terms of absolute accuracy. Figure 2 shows that, but not for all the combinations.\n* Minor: table 4 might be easier to visualize as a chart, similar to figure 2.\n* Minor: “batch normalizationalization” typo on page 5, and figures 7 and 9 do not show up\n\nOverall: This paper examines many experiments to improve the optimization of sparse networks, but I am not convinced that the approaches are the most effective (how important is EGF and comparing sparse to same size dense). Thus, I recommend rejection but am open to changing my stance based on the authors’ response.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Sparse Networks and Gradient Flow",
            "review": "Summary:\n\nThis work performs a systematic study to understand how various design choices affect the performance and the training behavior of sparse neural networks. In order to do this, it proposes a simple framework to compare sparse and dense networks (SC-SDC) and a simple common-sense metric to measure the gradient flow in sparse neural networks (Effective Gradient Flow). It is the first time when I see such study, and I believe that it can contribute to the way how we perceive sparse neural networks as it brings a new perspective. \n\nStrong Points:\n\nS1) The paper can guide the researcher or the practitioner to make good design choices with respect to sparse networks in a fast manner.\n\nS2) The well-designed experiments show the counterintuitive fact that even static random generated sparse network can outperform dense networks at the same capacity (number of parameters) if proper settings which improve the gradient flow are made, e.g. PReLU or SReLU as activation function, batch normalization.\n\nS3) The results suggest that sparse networks do not necessarily falls under the same behavior of their dense counterparts and highlight the importance of having specialized algorithms and methods for them.\n\nS4) In general, the paper is clear and well written, with some exceptions which likely show that it has been developed until the very last moment before submission.\n\n\nWeak Points:\n\nW1) The paper address just static sparse networks. As static networks still have a limited capacity, I believe that it would be much more informative if (ideally) the same sparse random generated networks would be studied in the same conditions using also dynamic sparse training.\n\nW2) The claim that comparing sparse networks with dense networks having the same number of parameters (thus fewer neurons) is fair does not sound completely fair to me. I agree that it is a tricky situation overall, but I would consider also the fact that more neurons mean more representational power and, thus, sparse networks have an \"unfair\" advantage from this perspective.\n\nW3) The general appearance of the paper suffers from, probably, the last-minute submission.\n\n\nWhile the paper has clearly added value, overall, I found it a bit immature for publication. During the rebuttal phase, I would recommend to the authors to address the above weak points and the following (minor) comments:\n\nC1) Make it clear from the begin (not just in the appendix) that you are discussing about static sparse networks to avoid confusion. Auxiliary, perhaps you can add some details about the sparse networks initialization in Table 2.\n\nC2) It is very interesting in Table 4, first row, that for this particular case the networks fail to converge in the two “extreme” cases (Dense and 20% density), while the middle density levels work fine. Do you have any idea why this happens? Same reasons for the sparse and the dense networks? Perhaps, this particular case, can help in developing more robust training methods for neural networks in general?\n\nC3) The clarity of the Dense Width paragraph from page 5 can be improved\n\nC4) Perhaps you can add also the dense network performance in Fig. 2?\n\nC5) Try to perform a thorough proof-read of the paper. There are many typos (e.g. page 3, “layer in l”), and other sloppy written aspects (e.g. detail MAW, tables 11 and 12 have the same learning rate value, figures are missing from the appendix, mention somewhere in tables 9-12 the activation function, etc.) \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "claims not well supported",
            "review": "This paper proposes effective gradient flow (EGF), which is a layer-wise normalized gradient flow. Compared to (unnormalized) gradient flow, the paper shows that the proposed EGF is (slightly) better correlated with metrics like test loss and test accuracy (see Table 1). Given that this claim is supported with experimental results, the paper would become much stronger if a larger number and a more diverse set of data-sets were used (in addition to CIFAR-10 and CIFAR-100, which are two very similar image-data-sets) as to show that the claim holds more generally. Apart from that, given that the correlations are (only) about 0.4 in Table 1, it seems that only some aspects are explained by EGF. \n\nThis said, if we continue with the insight that EGF is helpful, how would EGF help to design better sparse models and optimization methods? Why is it easier to compute EGF instead of directly the relevant metrics (like test loss etc.) to determine what training methods and model architectures work better?  In fact, Figure 2, top row, exactly does this: test-accuracy is directly used to determine which training methods work best for sparse models. What is the additional insight/benefit of using EGF (like in the bottom row of Figure 2) instead of test-accuracy ?\n\nThe paper also proposes the SC-SDC framework, see also results in Table 3. The key idea is to compare  sparse and dense networks that have the same number of (non-zero) weights. While this is a good start, I am not sure that this is really a fair comparison, though. I would expect the sparse network to have a larger capacity than a dense network with the same number of parameters. While there are many ways to see this, the simplest  might be that  a weight in a sparse model requires two parameters, its value and its index if we wanted to encode the model. Another way to see it might be that a dense model can be pruned to become sparse(with fewer weights), but without losing much prediction accuracy. As a consequence of potentially comparing sparse and dense networks of different capacities, the results in Table 3 might be biased in favor of sparse models. From this perspective, it is remarkable that sparse models do not clearly outperform dense models in Table 3, which might indicate that the training of sparse models with the analyzed approaches still suffers from the problems outlined earlier in the paper.\n\nEquipped with the proposed methods EGF and SC-SDF, the paper then analyzes several (standard) approaches like batch normalization etc as to determine which of these approaches are useful for training sparse models.  Again, given that only two very similar data-sets were used (CIFAR-10 and 100), it is unclear if the found results would generalize to other data sets. Moreover, it is not clear why EGF and SC-SDF are needed to determine which training-methods work well for sparse models. In fact, Figure 2 (top row) directly shows the test-accuracy for the various approaches--i.e., without using the proposed EGF and SC-SDF. EGF is shown in the bottom row in Figure 2, as to illustrate that EGF has a similar behavior as test accuracy in the first row in Figure 2. What is the additional insight obtained from EGF compared to using only test-accuracy as to determine that  batch-normalization works well ? \n\nSome minor points:\n\nHow are “test loss“ and “test accuracy” defined in this paper, and what is the difference? I did not immediately find the answer in the referenced paper (Jiang 2019).\n\nAre really all 6 digits statistically significant in Table 1 ?\n\nFigures 7 and 9 in the appendix are missing the actual image.\n\nThe paper has several typos, like “and.”, or the first sentence in Appendix A\n\n+++ updates after authors' feedback +++\n\nI  thank the reviewers for their detailed response. I still feel that more work (experimental and theoretical, as outlined in my review) is needed. \n\nI also would like to make sure that my point is not misunderstood when I said that the sparse model requires twice as many parameters to be stored (the value and the index),  compared to the dense model. Using compression algorithms, the number of bits to store the model can obviously be reduced (below the factor of two). Anyways, the deeper point that I wanted to make  was the connection between minimum description length  (number of bits to store the model) in information theory and the model complexity /capacity in statistics/machine learning: see BIC in https://en.wikipedia.org/wiki/Minimum_description_length\nHence, the bits to store a model are directly related to the model-complexity/capacity in machine-learning/statistics.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Meaningful investigation on the impact of different factors in sparse training",
            "review": "Recently, initialization has been found critical to the model accuracy attained by sparse training [1]. In this paper, the authors studies the impact of factors other than initialization on the model accuracy attained by sparse training relative to dense training (under the same model parameter count). At the core of this paper, the authors argue that the effective gradient flow (grad norm from only activate model weight dimensions) is an effective indicator on the model accuracy attained by sparse training. Firstly, the authors show that the effective gradient flow attains higher correlation with model accuracy than the norm of full gradient (including gradients on sparsified weight dimensions) in sparse training. Secondly, the paper empirically demonstrate that in sparse training, 1) weight decay and data augmentation can hurt model accuracy, 2) batch normalization plays significantly role for model accuracy in sparse training and 3) non-saturating activations boost the magnitude of effective gradient flow and consequently improve model accuracy. \n\nI think the direction this paper worked on can have meaningful influence on designing model structure and general training algorithms for sparse training. Specifically, the effective gradient flow can potentially be an objective to improve for innovate methods in sparse training. Additionally, the investigation presented the impact of different model and training factors on model accuracy that has not been systematically studied before; this could bring up discussions for more broader techniques in sparse training.\n\n\nCurrently I give marginal reject because I have the following comments on technical aspect and clarity regarding meeting the acceptance criteria. I am happy to raise the rating if the authors can resolve them during the rebuttal phase.\n\n1. In Table 1. the authors show that effective gradient flow achieves higher correlation with model accuracy than full grad norm. However the difference in the correlation attained by effective gradient flow and full grad norm is not significant. I was wondering which correlation is used here? Is it the Pearson Coefficient for linear correlation or rank correlation? If the rank correlation is not used here, I would suggest the authors to present results on rank correlation. Given that the effective gradient flow and model accuracy might not follow a linear relationship, the rank correlation can help better evaluate whether better effective gradient flow tends to imply better model accuracy in sparse training.\n\n2.  In section 4.1, how are the training hyperparameter tuned for different methods such as using and without using weight decay? Some quick elaboration on the fairness for tuning each setting will be useful to help evaluation the conclusion drawn in section 4.1. Also is the model trained for enough number of optimizer steps to attain almost-saturating accuracy for each specific setting?\n\n3. In section 4.1, it is a bit surprising to me that data augmentation can hurt generalization performance in sparse training. Are there any intuitive reasons for this observation?\n\n\nBesides the above major comments, here are the minor comments for further improving the paper:\n\n1. The definition and use of symbol a_s is confusing. In Equ. (2), a_s is a deterministic scalar value while in Equ. (3) a_s becomes a random variable.\n\n2. It would be helpful to clarify why the authors choose to use 1000 epochs (or is it 1000 optimizer steps?) uniformly in the experiments; this can help readers to better evaluate the experiment protocols.\n\n3. Symbol m in Equ. (6) should be bold to be consistent with vector notations on gradient.\n\n4. Larger font in figures will be useful for smooth reading.\n\n5. I totally understand that in section 4.1 that MLP for CIFAR100 is a micro experiment for study and < 40% accuracy can be acceptable. But giving dense training accuracy numbers/curves under the same model parameter count can be helpful better evaluating the sparse training accuracy relative to dense training.\n\n\nReference \n[1] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. Jonathan Frankle, Michael Carbin. ICLR 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}