{
    "Decision": "",
    "Reviews": [
        {
            "title": "Needs a major update.",
            "review": "#### Summary\nThe paper proposes to learn disentangled representations of images via an interventional method. The idea is to intervene on a certain dimension of the latent representations by replacing it with the corresponding latent dimension of another image. The authors further propose a disentanglement metric which unlike other metrics does not require access to the ground truth factors.\n\n#### Strong Points\n + In general, I find the idea of an unsupervised disentanglement metric very appealing. This requirement of ground-truth labels for the existing metric is very limiting and can not be fulfilled in natural, real-world settings.\n\n+ The proposed method achieves good quantitative results on disentanglements when the generative factors are correlated.\n\n#### Weak Points\n- My biggest (concern) question: does the proposed interventional method use VAE? The method claims to be disentangling generative factors but seemingly does not use a generative model. If it is based on VAEs then why is there no mention of variational inference in the expectations (ELBO)? If it is not VAE based then is the comparison with all the VAE baselines, which can not compute exact likelihoods, fair?\n- My second biggest concern is mistakes in the paper. The paper needs a major update in terms of rewriting. There are a lot of grammatical mistakes, typos, inconsistent notations, typefaces that I stopped tracking them after the first 4 pages.\n- Moreover, I think the general motivation and intuition behind the proposed method are lacking. I will go through these details in the following.\n\n##### Other Concerns\n\n- The interventional method and the respective disentanglement metric presented in the paper is heavily inspired by the interventional robustness metric introduced by Suter et al. It questions the novelty of the presented work. I would appreciate it if the authors could highlight the differences explicitly.\n\n- I think w.r.t the method, the statement of Proposition 1 is very obvious. It is more like a method description and does not need to be written as a separate proposition.\n\n- The authors then describe that “Proposition 1 is inefficient to obtain the disentangled representations as we may learn a trivial optimal solution.” My question is: What’s that trivial solution? and how does it affect disentanglement? The explanation will improve the readability and will add insights to why further propositions are needed.\n\n- I have some concerns with Assumption 1. I could not follow the line of argument. Moreover, the proof needs to be rewritten as the whole proof is put into one Lemma. I will recommend the authors to rewrite it and explain the intuition clearly.\n\n- [in the paragraph following Assumption 1] In order to explain the difference between the confounding factors and the independent ones, the authors say that “when the generative factors are all independent, the image distribution p_x and its distribution after intervention dot(p_x) are the same. I think this is not true as it doesn’t hold in all cases.\n\n- The authors introduce proposition 2 which I believe (a guess) is to cater to the disentanglement in the confounded/correlated factors. Could the authors comment on the insights on what does it do and why is this necessary?\n\n- What’s is the motivation and intuition behind Definition 1?\n\n- What distribution is used to compute the expectations in Eq 1? Also, I would appreciate it if the objective is written in a clear mathematical format rather than E_{cross_entropy}.\n\n- I think definition 1 provides a very simplistic view of disentanglement. The proposed metric is learned by tuning a downstream network [M] to map one image to another, and the disentanglement is measured in terms of difficulty in mapping those images. My question is; what will happen if the network M learns to map the trivial changes in multiple factors pretty well? For e.g. for a small positional shift in the x and y-axis both, the network may just learn to map the images correctly, exploiting the translation invariance ability of CNNs. Did the authors explore this in more detail? Or could provide a better understanding of the role of network M in capturing these variations?\n\n- The use of the term “Image translations” is confusing. It took me some time to realize that this not about the positional translational of dsprite factors. I would appreciate it if authors could use another like “mapping” or at least explain what it beforehand.\n\n- In section 4, instead of using an items list, authors can use small headings or a bold typeface to specify headings.\n\n- Figure 2(a) is confusing, the two latent factors correspond to the top image. However, the second latent is drawn in front of the second image. I realized after some time that the dotted boundary indicates the separation. At least add this explanation in the caption.\n\n- L_{adv} is wrong, the expectation on the right-hand side is missing.\n\n- Why do we need this adversarial loss? Wouldn’t it encourage the model to make the dot(x) the same as hat(x)? Thus adding more ambiguity in the latent factorization? \n\n- A detailed ablation for all the objectives would make things much clear.\n\n- In algorithm 1, L_{trans} only updates theta_{M}, but in the total loss, it is shown that the objective is used in conjunction with other losses and update all the model parameters. I think there is a major misunderstanding in the use of parametric notation.\n\n- I would have appreciated if the authors showed the relative performance of their metric with respect to ground truth factors. All the existing metrics used in the paper are very fickle and can not be reliably used (Locatello 2018 et al.)\n\n- General assumptions and intuitions on selecting the interventions are not discussed in detail.\n\n- Tables 1 and 2 probably show the performance of the method rather than a metric. It needs to be corrected.\n\n- How are latent traversals generated in Fig. 4(b)?\n\n- Why just compare with Factor VAE (gamma = 40)? Is this the best performing model?\n\n- Missing Citations! The paper really needs to make its claims more specific and position itself better with respect to related work. Also, discuss the relevant disentanglement papers.",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising metric, but lack of clarity and lack of ablation study limit impact",
            "review": "Summary: This paper proposes a novel method for learning disentangled representations by generating new versions of an input via manipulations of single dimensions of the representation. A set of losses are proposed to ensure cycle-consistency between real and generated inputs. It also proposes a novel method for evaluating disentangled representations without requiring ground-truth labels. The model is trained and evaluated on several standard synthetic datasets used in disentangling.\n\nThe proposed image-translation loss seems like a promising metric for evaluating disentangled representations without direct supervision. However, the lack of clarity in the writing limits the potential impact of this work. In addition, the proposed model seems to have redundant terms in its objective function, and no ablation study is performed to help us disentangle which parts of the model are contributing most to performance.\n\nWeaknesses:\n* L_trans seems to be somewhat redundant with L_in:  they are both enforcing cycle-consistency. What is the motivation for the additional loss? The paper would be stronger if it had an ablation study that could  tell us how well the model performs with only one of these objectives.\n* Instead of using OxfordFlowers102 and having to invent an ad-hoc method for generating factor data, why not choose a naturalistic dataset with labeled ground-truth factors, such as CelebA?\n* Proposed disentanglement metric depends on training a complex neural net.\n\nStrengths:\n* Proposed metric seems to effectively measure disentanglement without the need for ground-truth labels.\n\nClarity:\n* page 1, paragraph  2, last sentence: not sure what this sentence means, please clarify\n* page 3, proposition 1: Notation: x is used as both a function and an image here, please disambiguate.\n* Please define what you mean by “image translation task”, obviously\n* Typo: Page 5, first line “orignial” (also line 3 of algorithm 1)\n* I’m confused about what the adversarial loss is doing. In a vanilla GAN, the inputs are a sample from the “real” distribution and a sample from the generated distribution. In this case, the GAN will bring the modified samples closer to the distribution of the reconstructions. Why not bring them both closer to the data distribution? This seems like a very complex and sub-optimal scheme. I’m also  unclear on what W is doing, and how W is learned in the adversarial loss.\n* Translation model: How is \\theta_M initialized? In Eq. 2, it is implied that \\theta_M is an input to the L_{trans} loss. However, in algorithm 1, \\theta_M is not listed as an input, and \\theta_M seems to be trained from scratch within the loop. Is \\theta_M shared between different factors, or is there a single \\theta_M for each factor? Please clarify.\n* “factors such as texture and color are confounded by the species of objects” - If they are not independent, then why are texture and color considered factors? I believe this is getting at the idea of mechanistic independence (Schölkopf et al., 2012). If so, please make that clear. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea addressing an important problem, but the presentation/explanation of the method and the rigour of the experiments can be improved significantly.",
            "review": "The paper presents an intervention-based method for unsupervised learning when factors of variation are correlated, a more realistic setup compared to the usual independent factor setting in the literature. Moreover a new unsupervised metric based on a downstream translation task is proposed, and is claimed to be correlated with existing supervised metrics.\n\nStrengths:\n1. The paper addresses an important limitation of current unsupervised disentangling methods, in that they assume independent factors of variation. I appreciate that the paper takes on the challenging task of unsupervised disentangling in datasets with correlated factors, and believe that this is an important open problem in the field.\n2. The core idea of the paper based on intervention appears to be a good idea, but I think it can definitely be presented in a clearer way (see weakeness 1 below)\n3. The paper shows fairly convincing qualitative results, on both 3D shapes modified to have correlations between its generative factors and the Oxford Flowers dataset. The latter is a challenging real world dataset where correlations are present in the factors of variation.\n\nWeaknesses:\n1. The link between the motivation (unsupervised disentangling of correlated factors) and the methodology is unclear. The way in which the proposed intervention scheme allows disentangling for correlated factors is not well described. I presume that the intervention used for the reconstruction loss described in Figure 2a allows you to break the correlations (to generate images \\dot{x} where the correlations can be broken), but this is just my guess and I can’t find a suitable explanation in sections 3 and 4. I think this is a good idea, especially because intervention in the actual generative factors requires supervision and also may not necessarily be possible if there is strong correlation between the factors on which you intervene, but the idea can definitely be presented in a way that is more convincing - as things are it’s not clear at all that intervention is useful for the correlated case. e.g. one might question why the reconstruction loss L_in needs to be derived from interventions, and why not instead use the reconstruction loss between x and \\doubledot{x} without any intervention.\n2. It’s also not clear why you would want to match the distribution \\dot{p}_X to p_X. Going back to the paragraph before assumption 1, it’s not clear what is the trivial optimal solution, and how assumption 1 (that enables proposition 2) allows you to help avoid the trivial optimal solution. Given that you learn W(x) to match \\dot{p}_X to p_X via L_adv, where is W(x) used elsewhere in the model? What’s the point of this loss?\n3. The image translation task is questionable. When you predict image v from image u via model M, how can M possibly predict u given that it doesn’t know about the changed factor k? Even if z is perfected disentangled, any factor can have changed from u to obtain v, so I imagine M will just learn the average over all possible changes of a single factor, giving a rather high value of L_trans. I’m not convinced that this error will be correlated with other supervised disentanglement metrics (Figure 3): see next point.\n4. Figure 3 lacks a lot of information about how the correlation coefficients were calculated. For which models were they calculated? Was it just a set of models that were trained with the optimal hyperparameter settings, or was it calculated for all models across all seeds and hyperparameter settings? The results could be very different depending on how you calculated these coefficients.\n5. It’s not clear how sensitive performance is to the lambda values in the loss (Equation 3). An ablation study showing the role of each loss term, and how the behaviour of the model changes when each term is emphasised (by choosing a larger value of lambda) seems necessary. \n6. The disentanglement metric results in Tables 1 and 2 also lack rigour. Since you show results for just two different gamma values for FactorVAE, it’s not clear whether the regularisation hyperparameters (e.g. beta, gamma) for the baseline models have been carefully tuned. It would also be much more informative to show how performance varies as you vary the level of correlation, to better show the extent to which the proposed method helps in the correlated factors scenario. Moreover it would be helpful to show the metrics calculated for just the two correlated factors to emphasise the differences between the methods.\n7. For your proposed method, why are the disentanglement metrics for correlated factors better than the independent factors setting for 3D shapes? This seems counter intuitive and makes one question the validity of the results.\n\nOverall, the idea proposed in the paper is interesting and addresses an important problem in the field. However I strongly think that the presentation/explanation of the method can be improved significantly, as well as the rigour of the experiments section.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}