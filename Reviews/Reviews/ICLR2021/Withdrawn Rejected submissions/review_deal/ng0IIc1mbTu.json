{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "I would like to thank the authors for the their time and effort on this work. The paper is proposing an activation function that combines RELU like piecewise activation functions and a primitive attention mechanism. Then, they show that their proposed method works better in transfer settings.\n\nI think the approach authors taking here is more akin to a gating mechanism rather than an attention. So I would recommend the authors to change the name perhaps Gated Rectified Linear Units. The paper is interesting, but I agree with AnonReviewer4, that the experiments are not very convincing focusing on small scaled experiments in the supervised learning setting only. I would recommend the authors to compare their approach against other results from the literature as well. As it is right now it is not clear how significant the results in this paper are. I don't think the transfer and meta-learning experiments are very well-motivated in this paper. I would recommend the authors to better motivate those results.\n\nAfter considering my suggestions above and the comments from the reviewers I would recommend the authors to consider resubmitting to another conference.\n\n\n"
    },
    "Reviews": [
        {
            "title": "A new type of ReLU unit inspired by attention mechanism",
            "review": "[Overview]\n\nIn this paper, the authors proposed a new activation function called AReLU which introduces an attention mechanism to the original ReLU function. Based on this new activation function, the output will be adaptively adjusted by the two learnable parameters \\alpha and \\beta. This kind of adaptive adjustment can be thought of as an attention mechanism undertaken over each element in the input feature map. It will in general amplify the positive elements while suppressing the negative ones, and the parameters \\alpha and \\beta will be adjusted adaptively based on the activation values. The experimental results showed that AReLU can achieve much better performance with small learning rates while comparable performance with fairly large learning rates. This inspires another set of transfer learning experiments that demonstrate the effectiveness of AReLU.\n\n[Strength]\n\n1. This paper proposed a simple element-aware activation function. It is built based on ReLU but differs from ReLU in that it has two learnable parameters and will adaptively augment the positive inputs while suppressing the negative ones.\n\n2. The mechanism behind the proposed AReLU seems reasonable. It increases the scale to later than one for positive responses while decreasing the scale for negative responses.\n\n3. The experiments showed that AReLU is more robust to the learning rates compared with other activation functions, either non-learnable or learnable ones. Also, for a lower learning rate, the convergence speed is much faster than other activation functions. This property facilitates the transfer learning scenarios as shown in Sec. 4.4.\n\n[Weakness]\n\n1. It seems that AReLU will hurt the performance when the learning rate is reasonably set, as shown in the first super-column (1e-2) in Table 1. Also, it is not clear about the final performance on CIFAR-100. Overall, it hard for me to determine whether the proposed AReLU can have generic benefits to the training under different settings.\n\n2. In Fig.2 (b),  the values of \\alpha are different for different learning rates, while (1+\\sigma(\\beta)) converges to similar value. This raises the question that whether it is necessary to set a learnable \\beta or not because we can simply fix the value of \\beta at the beginning. Even for \\alpha, it is not clear whether it is necessary to learn it because the authors did not show the final accuracies for different settings.\n\n3. In Table 3, the authors showed that AReLU is more suitable for transfer learning with a low learning rate. However, the comparison is a bit unfair, for other activation functions, we can simply increase the learning rate and it might be the case that they can achieve better performance with a larger learning rate. Another baseline is, as pointed above, we can simply set \\alpha less than 1 (say 0.5) while 1+\\sigma(\\beta) to around 1.5.\n\n4. The proposed AReLU is good for learning the parameters in an adaptive manner. However, does this will introduce another problem, i.e., overfitting? With less amount of data, this adaptiveness may hurt the generalization ability of the learned network. In this paper, the authors did not study this aspect. \n\n5. If the goal of AReLU is mainly to address the vanishing gradient, then what if we remove the sigmoid to have a (1+\\beta)? What will be the outcome in this case? I would like to hear from the authors about this.\n\n[Summary]\n\nOverall I think this paper is well-written and with a fluent flow to follow. The intuition behind the proposed AReLU is clear to understand and I like it. Experimental results demonstrated the effectiveness of AReLU, especially under training+finetuning settings. However, as pointed above, I am still concerned about the generalization of AReLU and the necessity of applying AReLU in the training. I would like to hear more from the authors in the rebuttal.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Borderline Reject",
            "review": "Pros:\nThis paper is well written and is easy to follow. It focuses on the basic component in neural network, i.e. neural activation function.  Extensive experiments have been conducted and the corresponding experimental results are provided.\n\nCons:\nWhile, in my view, the novelty of this paper is limited and needs more improvements. By changing the actual function performed on the input tensor X has been explored too much, such as the recent proposed FRELU: Funnel Activation for Visual Recognition. What's the difference between this paper and the proposed one? It seems the difference is limited, and they belong to the same family.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The tuning of the baseline methods are required for fair evaluations",
            "review": "Summary\nThis paper proposed an attention module in which each element of features is scaled based on the sign of its value.  The authors argue that the combination of this attention module and ReLU can be considered as an activation function.\nThis novel activation function amplifies its gradient comparing with ReLU.\nThe experimental results show the proposed method outperforms conventional activation functions in few-shot settings of transfer/meta learning.\n\nStrong points\n* The proposed method consistently improves the accuracy of transfer/meta learning tasks.\nWeak points\n* The technical novelty is limited since the derived activation function is an expansion of PReLU:\n  * PReLU: min(0, a * x_i) + max(0, x_i)\n  * Proposed: min(0, alpha * x_i) +  max(0, (1+sigmoid(beta))*x_i) where 0.01 < alpha < 0.99.\n* Although the very specific form of ELSA is used in Section 3.2, there is no explanation of why C and sigma are introduced for ELSA.\nThe learning rate is not tuned for each activation function in Section 4.4 and 4.5.\n\nDecision reason\nSince the learning rates of the baseline methods seem to be not optimized, the experimental results are not convincing.\nI recommend resubmitting the paper after the tuning of baseline methods.\n\nQuestions\n* Why did you use momentum SGD for alpha and beta instead of using Adam or SGD as other parameters?  Their learning rates are also different from these of other parameters?\n* How is the learning rate determined for the experiments in Section 4.4 and 4.5?   Even if the learning rate is tuned for baseline methods, the proposed method outperforms them?\n\nAdditional Feedback\n* Although it is interesting to formulate activation functions with an element-wise attention mechanism, the merit of this new perspective is not clear for me.\n* It might be easier to understand the proposed method as an expansion of PReLU since the proposed attention mechanism is much different from the attention mechanism in the community.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work on Learnable data-adaptive activation function (AReLU)",
            "review": "Description:\n\nThis work presents a novel learned activation function called Attention-based Rectified Linear Unit (AReLU). Element-wise attention module is developed that learns a sign-based attention (ELSA) which is the novel component of AReLU towards mitigating the gradient vanishing issue. Extensive experiments and analyses have been provided on MNIST and CIFAR100 datasets. Compared with other relevant activation functions, AReLU achieves faster convergence under small learning rate because of the amplification of positive elements and suppression of negative ones with two learnable data-adaptive parameters.   \n\nStrengths:\n- 13 learnable and 5 non-learnable activation function compared with the proposed AReLU. Outperforms most in convergence speed and final classification accuracy on MNIST.\n- More effective training with a small learning rate. \n- Impressive results for transfer learning and meta-learning \n- Grad-CAM on CIFAR100 show semantically more meaningful activations \n\nWeaknesses:\n- Results are reported on CIFAR100 and MNIST datasets. The results would be more compelling if provided on datasets such as ImageNet.  \n- Implementation details are not provided. It would be really important to confirm if the same setup is used when comparing different activations (seed, initialization, hardware, environment)\n- It is not intuitive why two different families of models were used for different datasets - MNIST-Conv (specially designed) for MNIST while Resnet-18 and VGG11 for CIFAR100\n\nQuestions:\n- Could the authors confirm if the code would be made publicly available for reproducibility? \n- It seems like a faster convergence and more improvement observed when SGD (compared to ADAM) is used as the optimizer (Table 1). Was a similar graph observed when using ADAM as the optim for Figure 3?  \n- Continuation of the above point, it would be interesting to see a similar table for CIFAR100 dataset when using ADAM / SGD as the optimizer.  \n- Could the authors clarify the motivation for two different learnable parameters for positive and negative elements compared to only using a single parameter? Also why sigmoid is used for positive elements. \n- Have the authors considered other viable techniques to combine ELSA with ReLU (apart from addition in Section 3.3)?   \n\nSuggestions/Comments:\n\n- Please color code (red) the learnable activation function in Table1\n- Section 3.1 input a data -> input data\n- Figure 1 An visualization -> Visualization\n- Please correct the equation in Section 3.4 (better if equations are numbered)\n- It would be helpful to provide the motivation for clamp function also in Section 3.2 apart from 3.4\n- It would be helpful to be consistent with the nomenclature - MNIST Conv vs ConvMNIST in A1.\n- Please confirm the dataset in A3 - CIFAR10 or CIFAR100\n- It would help to explain “detaching the gradient back-propagation” in Section 3.4, independent of the framework used. \n\nThe paper is clearly written in most parts, It would be interesting to see other comments and discussions on this paper. \n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nPost Rebuttal update:\n\nI would like to thank the authors for providing relevant details and a thorough rebuttal to all the issues raised by the fellow reviewers. Original rating is maintained.\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}