{
    "Decision": "",
    "Reviews": [
        {
            "title": "Presentation of the paper need much improvement ",
            "review": "The paper proposes a method named DISUSE for dynamically removing noisy information from the given context to make the reasoning process more precise.\nThe paper focuses on the passage-based QA task and visual QA task. It has two main components: the erasure sampler, and the supervisor.  The erasure sampler works as an attention module, which scores the correlation between each \"clue\" in the context (e.g., a word in the passage or a physical object in an image) and the given query. Then, the supervisor aims to keep the consistency (in terms of answering a query) between the new, erased sample and the original sample. It shows better performance comparing to the methods of simply fine-tuning pre-trained models or self-supervised learning methods on three datasets (ReClor, GQA, VQA).\n\nFirst, I personally don't think it's a good idea to use the term \"commonsense reasoning\" or \"commonsense question answering\" here in this paper. The problems that this paper focuses on are conventional reading comprehension and visual reasoning tasks, where the context (i.e., the passage and images) contains evident information clearly. The term commonsense question answering now usually refers to the cases where the queries indeed need external information from commonsense knowledge for reasoning due to the incomplete context. I would suggest just using the word \"(relational) reasoning\", or \"question answering\" over text/images is enough. Even the creators of these datasets do not argue them as commonsense reasoning task, after all. That being said, if the authors really want to connect this work to the commonsense reasoning community, please check the CommonsenseQA dataset and datasets that are in AI2 benchmark leaderboards (https://allenai.org/data), and also discuss the connections with the commonsense reasoning models designed for them such as KagNet (Lin et al. 2019). \n\nAlthough the motivation is intuitive and straightforward, the current presentation of this paper did not justify its key arguments well. First of all, the paper lacks a clear, formal definition of the task setting and many important concepts. Please make a section about task formulation and use math notations to clearly illustrate your task setting (at both abstract level and dataset level) and your concepts with illustrative examples. The word \"clue\", \"context\", \"question-clue pair\" are confusing to me sometimes.  Also, the argument is not justified that \"Since in absent of any guided information for the unsupervised pre-training, the similar clues, highly frequent question-clue pair patterns, and abundant clues in the complected context can easily confuse or bias the model to well extract the exact clues for a question.\" Can you discuss with deeper analysis by showing illustrative examples?  What do exacatly the \"artificial clues\" and \"redundant clues\" mean here?  What specific \"unsupervised pre-training\" method are you talking about here? If it is the pre-training of BERT with masked-word-prediction objectives, why does this lead to negative bias for downstream relational reasoning?  The key idea is to remove irrelevant objects (i.e., \"clues\") from the context, it sounds like yet another (hard) attention module to me.  What is the key difference between DISUSE and the compositional attention of MAC and the structured attention networks (Yoon Kim et al. 2017)? Why do you need to determine the number of clues that should be erased? Will this increase the risk of removing important clues?\n\nFor the experiments in ReClor, why do you use the word as the clue unit instead of a sentence? A sentence in a passage is more reasonable to me being a unit for relational reasoning. What makes you only choose ReClor, which is less popular, over other more popular reading comprehension datasets such as RACE (aclweb.org/anthology/D17-1082/)? Can DISUSE also work on non-multiple-choice QA settings? The experiments lack case studies to convince readers that the behaviors of the erasure sampler are reasonable.\n\nOverall, I think the writing can be much improved with more carefully proofreading and adding more illustrative examples. I would suggest a rejection unless the authors can fix them properly soon. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Potentially interesting idea with completely opaque presentation and minimal gains. ",
            "review": "Overall, the methodology in this paper is opaque. On a very high level, the paper claims to 'erase clues' in QA and VQA systems with the intention of removing some spurious elements of the input. For example, in VQA, this could mean removing a region proposal in LXMERT. The exact mechanism for this is unclear. Are entities removed and the model is run twice after this erasing takes place? Section 3.1 is very hard to make sense of: I don't understand if we are removing elements that have high overall attention on them, or low, or how all of this relates to the objective L_reg. The intuition behind the supervisor I can follow, but without being able to understand anything about how the erasing happens, it is hard to really put the pieces together. All of the methodological confusion aside, the gains are very small, .3 points on VQA and .5 points on GQA. I am unsure why ReCLOR was chosen as a QA benchmark, but the gains are higher there, but it is not a benchmark which has had a lot of work on it. Why not a BERT baseline on Squad2?\n\nTo improve the writing in the technical section, I would suggest not mixing the equations and text so liberally in paragraph form. A short paragraph describing the high level points followed by equations explaining the details would help.\n\nPositives:\n+ On a very high level the idea of trying to find pieces of input to erase is intriguing\n+ Experiments span two modalities : QA and VQA\n\nNegatives:\n+ The clarity in the paper is so low that it is challenging or impossible to make sense of the method\n+ Improvements on competitive datasets are low to the point that they are likely noise",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a training method called DISUSE to make a model (hopefully) more robust to unwanted dependence on artifacts connecting an input with a label. The idea is to extract attention scores between the context and question for a QA task, then, to mask out/erase 'non-essential' features that do not seem to be attended. The approach then learns to make the answer predictions similar between both the erased and the non-erased version.\n\nStrengths:\n* This approach leads to state-of-the-art results on several datasets, with the ReClor results perhaps being the most significant (2-3 point improvement overall). \n* The experiments on both language-only and vision-and-language datasets seem reasonable and it seems great to this reviewer that the method works in both domains.\n\nWeaknesses:\nThe main weakness to this reviewer is that it is not obvious to this reviewer why the method works -- e.g. what is being erased and what isn't, and whether that corresponds to reducing dependence on artifacts, or just a form of data augmentation. To that end, the paper might be much stronger with some of the following:\n* It would be great to have some kind of study (quantitative or qualitative) for what types of things get erased on one of the datasets, and whether those things correspond to superfluous patterns or meaningful signals (that humans use when trying one of these datasets).\n* It would be great also to evaluate on not just in-domain splits of these datasets but also out-of-distribution ones which might measure robustness, e.g. from Agrawal et al, \"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering\"\n* Last, the paper could be improved greatly by comparing it to some other baselines, such as randomly masking the inputs. To this reviewer, the concern is that the approach boils down to a form of dataset augmentation, so it would be good to compare with simpler methods for doing so. Dataset augmentation helps a lot in vision, and a paper from ACL 2020 suggests that using the mask-prediction objective helps on NLP tasks, e.g. as seen by Gururangan et al 2020 \"Don't Stop Pretraining: Adapt Language Models to Domains and Tasks.\"\n\nMinor nitpick: to this reviewer none of the datasets presented are for 'commonsense reasoning' tasks, though that label is admittedly a bit subjective.\n\nOverall, to this reviewer, this paper and approach seem promising, it just needs a bit more evidence to show what it does and why that helps.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New approach for eliminating data artifacts in training a model; Needs more experimentation and analysis before it can be ready for publication",
            "review": "This paper is proposing DISUSE, a new approach for models that are robust to spurious correlation in the data.  They use a self-supervised method of erasing features that are attended to above a threshold while minimizing the difference in predictions from the original network.  They choose to focus on three commonsense qa tasks to demonstrate the utility of their model. \n\nPros:\n - They present a new approach for eliminating unimportant features for qa tasks.  The method here is not really specific to commonsense tasks and could be easily generalized to a variety of other tasks where spurious correlations or data artifacts are often found. In fact, the proposed model could possibly be applied to a wide range of tasks.\n - They show quantitative improvements on three tasks\n\nCons:\n- While the proposed approach does improve results, it’s generally by a small margin.  It would be helpful to see significance metrics.\n- It seems like there should be more experimentation or analysis for evaluating whether DISUSE is actually erasing data artifacts effectively.  In order to demonstrate that DISUSE avoids memorizing spurious features, the authors should show that the learned model can generalize to out-of-distribution inputs.  In the paper’s current form, this is investigated a little with the ReClor dataset, which has an easy and hard test set.  Although DISUSE is improving the scores on both test sets, there’s still a very steep performance drop from easy to hard examples.  This seems to indicate that DISUSE may still be learning features based on data artifacts.  More analysis and experimentation should investigate more into this.  For example, further experiments could investigate how well the trained models generalize to hard or out-of-distribution examples.  Another way this can be shown is if the authors provide error analysis demonstrating the types of features (or \"clues\")  are erased by the model vs. which features are kept as important.\n- There are grammatical errors throughout the methods section (section 3) that made it difficult to understand some parts.  In particular, parts of section 3.1 were a bit too hard to follow.\n\nMinor typos:\n- Abstract: “requires to mine the clues” → “requires mining clues”\n- Section 3: “For reading comprehension task” → For reading comprehension tasks\n- Section 3: “For visual question answering (VQA) task” → For visual question answering (VQA) tasks\n- Section 3: “since manually labeled data are expensive and are not sufficient in most piratical setting” →   “since manually labeled data is expensive and is not sufficient in most practical setting” \n- Section 3: “Since in absent of” → “Given the absence of”\n- Section 3.1: “we erasure each clue” → “we erase each clue”?  It might be better to describe this as masking each feature.\n- Section 4.2: “The threshold for important clue” → “The threshold for important clues”\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}