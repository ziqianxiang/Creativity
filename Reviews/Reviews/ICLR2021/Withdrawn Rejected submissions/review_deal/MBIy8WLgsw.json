{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting method. More justification needed.",
            "review": "This work proposes a performance estimation strategy that uses the feature histories of early model training process to predict the final performance of the model architecture of hyper-parameter configuration. Empirical studies show that it leads to improvement when used with existing search algorithms. \n\nThe key observation is that during CNN training, features of most images keep moving back and forth between current classifier boundaries, but more and more features acquire correct positions. And, if the feature of one image lies on the correct side of the optimal linear boundaries most of the time, with high probability it will be correctly classified at the end of optimization. This suggests an ensemble approach to performance prediction, based on early deep features. \n\nIn general, the presentation is clear and the idea seems to be effective. However, it is based on a heuristic that is not very well understood. Some rigorous analysis and justification seem quite necessary. Besides, several hyper-parameters may affect the observation, including the epochs considered in the early training stage, the choice of checkpoints, the number of classes, etc. A more systematic evaluation on more datasets is desired.\n\nSome related works on architecture performance estimation should be discussed:\n\n1. Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS.  https://arxiv.org/abs/1911.09336\n\n2. Rethinking Performance Estimation in Neural Architecture Search.\nhttps://arxiv.org/abs/2005.09917",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ok but not good enough",
            "review": "Summary:\nThis paper proposes a simple and intuitive approach to estimate the final performance of a deep neural network, using an ensemble of linear classifiers trained on the extracted features from early training epochs. The method can be combined with existing HPO and NAS methods to improve their efficiency.\n\nStrong points:\n- The proposed approach is motivated by empirical observations (Figures 1 and 4), and is thus reasonable and intuitive.\n- The proposed approach is shown to be competitive for performance estimation (Figures 2, 3 and 5)\n\nWeak points/questions:\n- Although the proposed algorithm is intuitive, I feel it's not so novel and lacks technical depth/challenge.\n- If we take the proposed algorithm out of context, it's simply a new ensemble method which combines some weak classifiers obtained from early training epochs. So, I don't see any part of the algorithm that makes it natural to be used for performance estimation in NAS/HPO. Please clarify this.\n- In the NAS and HPO experiments (Sections 4.2 and 4.3), why didn't you compare with other performance estimation methods? I think at least some methods mentioned in the Related Works section should have been compared with.\n- When using your algorithm in NAS/HPO, how do you determine the number of epochs to run before your algorithm stops? It seems this is a rather heuristic choice, and I think the impact of this choice should be explored in the experiments.\n- The performances for NAS and HPO don't seem to show significant advantage of the proposed algorithm. For example, in Figure 6, the proposed algorithm is only significantly better (non-overlapping error bars) for Hyperband; in Table 1, the proposed method is outperformed by other advanced DARTS algorithms such as P-DARTS, PC-DARTS and UNAS; in Table 2, the performances of all methods are very close since the error bars overlap.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper proposed a novel performance estimation strategy based on feature history and combined it with various search algorithms. The experiments showed the improvement for general search algorithm.  ",
            "review": "This paper proposed a novel performance estimation strategy and the algorithm is simple to implement. It saves the intermediate features  at early stage of training and then optimize and ensemble the linear classifiers based on them, which output as performance estimation to the final performance. \n\nThe idea is straightforward but it is not quite clear to the reviewer about the robustness of this estimation. It would be better if the authors can discuss about the possible edge cases when the feature history fails to give a good estimation of final performance. By lacking theoretical guarantee, it is not obvious to the reviewers that this method can always reach an accuracy closer to the final one at the early stage of training.\n\nIn addition, we would like to know how to determine the number of epochs which is enough to stop the training at the early stage of training and other hyper-parameters in this performance estimation algorithm (such as window size K in algorithm 1) in practice.\n\nPlease answer the questions listed above during the rebuttal process. I would give an accept if the questions are reasonably answered.   ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Blind Review #2 ",
            "review": "**Summary:** \nThe authors focus on developing efficient methods that can accurately estimate a model’s best performance using only a small time budget. To this end, they propose a new performance estimation method which uses a history of model features observed during the early stages of training to estimate the final performance. The proposed method yields better Kendall’s Tau than the baseline methods. However, the experiments should be further improved.\n\n**Strengths:**\n1. The authors focus on a very important problem in NAS to accurately estimate a model’s best performance using only a small time budget.\n2. The authors propose a novel performance estimation method which uses a history of model features observed during the early stages of training to obtain an estimate of the final performance.\n\n**Weaknesses:**\n1. The authors only conduct experiments on small datasets, e.g., CIFAR. It would be stronger to report the comparison results on ImageNet.\n2. From Table 1, the architecture searched by the proposed method is worse than P-DARTS and PC-DARTS. \n3. The authors argue that the proposed method yields 80% reduction of the search time. However, from Table 1, the search cost of the proposed method is 0.8 GPU days, which is even slower than darts. \n4. The proposed method has to save the feature histories, which may incur unbearable memory consumption. How much memory does the proposed method take in practice?\n5. The authors argue that the proposed method can “accurately estimate a model’s best performance”. How accurate is the prediction? Besides Kendall’s Tau, the error between the predicted performance and true performance should be measured.\n6. The authors argue that “if the feature of one image lies on the correct side of the optimal linear boundaries most of the time, with high probability it will be correctly classified at the end of optimization”. However, it only holds for the correctly predicted images. For those images with wrong predictions at the early training stage, the final model may correctly predict the labels in the end. In this sense, the model at the early training stage cannot fully represent the final model and thus the estimated performance should be inaccurate. More discussions on this issue should be provided.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}