{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "While the reviewers find the experiments in the paper somewhat interesting, they find that the paper does not sufficiently address whether the limitations shown for models in this paper translate to larger models and other, more realistic, tasks, or an artifact of the setup considered in the paper.  Overall the takeaways seem unclear from the paper and I believe it is not ready for acceptance.  Addressing the issues raised by reviewers and having a more clear discussion on connections to existing results will help the paper."
    },
    "Reviews": [
        {
            "title": "A somewhat interesting set of experiments with unclear takeaways.",
            "review": "This paper studies the efficacy of transformers on a polynomial simplification tasks. \n\nThere are two main motivations for this work: Piotrowski et al and, Lample and Sarton (references in the paper). The paper is set out to explore the capability of transformer networks of creating muti-step proofs. \n\nOne of the contributions of the paper is the creation of dataset of polynomial simplifications. They use the method in Lample and Charleston to generate a large random dataset of polynomials represented as a sum of products. Each term in that product is a product of a small set of factors. So the basic question is: how do we represent this polynomial by a formula of minimum length, in which each operation is one of the +, - or *.  \n\nThis is a hard question for sure.\n\nBut is this question as hard as in Lample and Sarton?\n\nIs it hard as computing integrals of expressions?\n\nEverybofy knows that computing integrals is hard. In fact, it is much harder than computing partial derivatives.\n\nHow hard is it to give a representation of a sum of products? It might be hard. But is it as hard as the above?\n\nI can't tell, but this paper does not even specify what is a baseline. When do we believe that something is important.\n\nThis paper fails to specify what is an interesting message and fails to specify that message.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Many experiments but not sufficiently interesting new insights",
            "review": "The paper studies the capability of the transformer architecture to\nperform rewriting to normal form in a simplified polynomial setting.\n\nIt is a continuation of the research by Piotrowski et al (PUBK) in the\narea of using neural nets to do symbolic rewriting, followed later by\nLample&Charton (LC).\n\nSeveral datasets are generated, using various\nconstraints on the sizes of coefficients, etc. Using infix vs prefix\nnotation is also analyzed. The number of variables is either 1 or 2,\nwhich seems insufficient. Already PUBK shows that going from 2 to 3\n(poly 5 vs poly6) variables reduces the performance considerably.\n\nThe main difference to PUBK is that the unnormalized polynomials are\ngenerated in a simpler format (sum of products of factors) that makes\n(or should make) the normalization procedure very simple. And that the\nevaluation is done step-wise, similar e.g. to work of Gauthier [3].\n\nThe setting is a bit problematic when compared to the full setting\nalso by forcing the normalization to be done in a particular \"obvious\"\nway. A richer set of rewriting steps consisting e.g. of finding common\nfactors (as e.g. in 2*(2x+1)*(y+1)*(z+2) + 2*(2x+1)*(y+1)*(z+3)) could\nlead to shorter rewriting sequences. These two issues - very\nsimple polynomials and very constrained rewriting rules - imply that\nthe setting is insufficient to answer the question posed by the title.\n\nMy overall feeling is that the paper shows a lot of experimental data,\nbut it does not bring sufficiently interesting new insights.\n\nSome more comments:\n\nThe poor generalization (e.g. Table 16) of the transformer to symbolic\ndata generated differently is not very surprising. Still, I am missing\ninformation about testing transformers trained on fewer variables on\ndata with more variables.\n\nOn page 5 the authors say \"We make sure that the simplified versions\nof the input polynomial in the training batches, do not collide with\nany endpoints in the  test and validation set.\"  \n==> \nHow was this done? PUBK shows that one simple kind of replacement in data (CONST\ninstead of all digits) leads to very large train/test overlaps in\nLC. But PUBK says that this is initial and much more needs to be\ndone. A simple improvement suggested there is measuring the\nLevenshtein distance as in Wang et al., 2018. [1]. I would expect much\nmore on this topic given the previous work and their issues.\n\nCompared with Zombori et al. [2], none of the settings is ever\nshown to learn perfectly a usable (even if simple) algorithm.\n\np2: \"As a state-of-the-art model, we explore Transformers. While both Graph Neural Networks and\nTransformers have been used for single-step representation learning of symbolic theorems and single\nstep goal-theorem scoring, Transformer-based sequence-to-sequence networks have shown superior-\nity in end-to-end tasks in integration, differential equations\"\n==>\nVarious versions of tree neural nets have been used quite successfully by Gauthier for related symbolic tasks [3].  Similarly for guiding theorem provers, in particular in ENIGMA [4] .\n\nPrefix vs infix: see [5] for previous related work on this and more.\n\np2: symbolic re-write==> symbolic rewriting\n\np2: the facstep in the example is unclear/confusing - X^1 is replaced just by X.\n==> What is the underlying representation? Can you give a more illustrative example of facstep?\n\n\nReferences:\n\n[1] Qingxiang Wang, Cezary Kaliszyk, Josef Urban:\nFirst Experiments with Neural Translation of Informal to Formal Mathematics. CICM 2018: 255-270\n\n[2] Zsolt Zombori, Adrián Csiszárik, Henryk Michalewski, Cezary Kaliszyk, Josef Urban:\nTowards Finding Longer Proofs. CoRR abs/1905.13100 (2019)\n\n[3] Thibault Gauthier:\nDeep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic. LPAR 2020: 230-248\n\n[4] Karel Chvalovský, Jan Jakubuv, Martin Suda, Josef Urban:\nENIGMA-NG: Efficient Neural and Gradient-Boosted Inference Guidance for E. CADE 2019: 197-215\n\n[5] Bartosz Piotrowski, Josef Urban:\nStateful Premise Selection by Recurrent Neural Networks. LPAR 2020: 409-422\n\n========================\n\nUPDATE\n\nThe response says:\n\n\"With the straightforward use of Transformers, where the model has only seen a single variable in training, there’s no information for the model about what to do with the second variable and thus it will not generalize to the two variable case. Training on two variable-polynomials and testing on two variable-polynomials has relatively low accuracies in our experiment. This suggests that training on single variable-polynomials and testing on two variable-polynomials will result in even lower accuracies. With more work, one may be able to design a model with appropriate inductive bias that understands the concept of multiple variables. This is beyond our scope.\"\n\nI am afraid that this makes the study rather insufficient for me. The problem of representing variables, eigenvariables/skolems, and capturing structural similarity between different theories and signatures is ubiquitous in the ML-for-TP area. Practically all useful systems developed so far - both features-based and DL-based - have to address this. The authors' answer is \"our representation is unsuitable\". The observation that if you have no shared representation of variables, you will get little/no generalization is a no-brainer and there is hardly any need to publish negative papers about it. In particular, in a conference about *representations* and some 15 years after first useful systems dealing with such issues have been developed. There are many fixes to this - see e.g. Gauthier's representation of variables in his Tree NNs, etc.\n\nMy score will stand, but I would like to encourage the authors to dig deeper and follow the suggestions given in this and other reviews. The general topic of learnability of symbolic rewriting by various neural architectures is certainly interesting, potentially very useful, and far from well understood.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice analysis with some more possibilities",
            "review": "The authors analyze the performance of Transformer models on simplifying polynomials and - importantly - generating proofs at the same time. This is a very nice idea that allows to study the performance of Transformers in depth and at the same time in an important setting where verification is performed as part of running the model. And the authors show a strong baseline, with models performing very well in a number of settings. A few areas seem to have been neglected though. For one, the authors only train a 4-layer 4-head model, which is quite small as far as Transformers go. Maybe it's irrelevant for this problem - but having at least one bigger model as a point of comparison would be good. Next, the out-of-distribution question warrants more experiments. Can the Transformers simplify polynomials with way more factors than trained on? With a higher number of variables? Higher degrees? The authors also show that one main problem for Transformers is learning to multiply the coefficients. But - assuming this reviewer understood correctly - the authors do not apply the proof requirement to multiplication. E.g., for \"12*3\" the model has to immediately output \"36\" rather than \"10*3 + 2*3 = 30 + 6 = 36\". Maybe this could help the Transformer learn and be more resilient to coefficient size? So while the current version of the paper is ok, there are a few areas for improvement which prevent it from being a clear accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting study of the behavior of Transformers on symbolic tasks",
            "review": "The paper \"Do Transformers Understand Polynomial Simplification?\" introduces a new reasoning task (convert polynomials into a normal form) and studies the performance and errors of Transformers on this task. The task itself is quite simple: given a randomly generated term involving small constants, variables, additions, and multiplications, bring the term into a well-defined normal form. Each task has to be solved in a unique sequence of steps. The authors study the performance of Transformers to either simplify the expressions step by step or to predict the simplified version directly.\n\nContributions I highly appreciate:\n- Contrasting step-wise generation of proofs vs end-point prediction.\n- Contrasting different representations of formulas: prefix notation appears to be better than infix notation.\n- Observation that transformers struggle with multiplication.\n\nQuestions to the authors:\n- The task is pretty simple compared to other (mathematical) reasoning tasks studied in the literature. The task is also not very useful as a prediction target, because it can be solved with a simple algorithm, as far as I can see. The paper essentially claims that this allows us to study Transformers better. What insights were produced here that couldn't have been produced on more challenging benchmark from the literature?\n- Why is it important for your paper that proofs are unique? Do we simply train Transformers to execute a simple algorithm? If so, how does that relate to the existing theorem proving and reasoning approaches where we usually have many possible proofs for a task.\n- What is the point of \"establishing baselines\" if the task is essentially solved already with moderately-sized Transformers?\n- What can be learned from the curriculum learning experiments? The way the curriculum is defined here appears to essentially require a synthetic benchmark for which we can generate examples of different hardness levels. So how could this help for real problems?\n- Introduction and Conclusion: The paper claims that training on the two variable case leads to better performance on the one variable case than training on it directly. This sounds very much like an artifact that could stem from the lack of training data or so. How is this possible?\n\n\nMinor comments:\n\np.1, pargraph 1: I believe Hahn et al also propose an \"end-to-end\" task using Transformers instead of embedding their approach in an existing neuro-symbolic system.\n\np.1, paragraph 2: The reference to Lample & Charton is slightly off: It was published in ICLR 2020.\n\np.2, paragraph 1: \"we observe that the system can understand candidate sub-expressions ...\" I am always wary of the use of \"understand\" in the context of neural networks, as it is not very clear what it means.\n\np2. related work: I do not understand how you contrast your work to the existing theorem proving works: There are a number of neural theorem provers (HOList, GamePad, GPT-f, and probably some more) that also generate proofs step by step. They might employ more advanced search ideas, but I think it would be good to state why your paper does not want to go in this direction.\n\np.2, Section 3: How does Sympy \"ensure correctness\"? There could be bugs in the code even if you didn't write the code yourself?\n\np.2, footnote: \"an unique\" -> \"a unique\"\n\np.3, last paragraph: remove \"Hence\".\n\nAdditional details about the training setup would be appreciated. For example, how many training steps/epochs did you train for?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}