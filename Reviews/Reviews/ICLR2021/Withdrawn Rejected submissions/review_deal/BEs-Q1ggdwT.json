{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nIn this paper, the authors proposed the expected quadratic utility maximization (EQUM) to implement the risk-aware decision-making for mean-variance RL. The EQUM framework directly optimizes the weighted sum of first and second order moments, while ignores the square of first moments, and thus, largely reduces the difficulty in optimization. The authors tested the proposed policy gradient based algorithm empirically. \n\nHowever, the connection to classic mean-variance is not clearly by simply ignoring the square of first moments in the objective theoretically (R2, R3, R4). Meanwhile, the effect of tunable weights (\\psi) is not clear and consistent empirically (R2, R3, R4).  \n\nAs all reviewers agree this paper is interesting and promising, I encourage the authors to address these issues and consider to submit to next venue.  "
    },
    "Reviews": [
        {
            "title": "A more practical RL algorithm for mean-variance MDP problem, justified by detailed experimental results",
            "review": "\nIn this paper the author proposed a new mean-variance algorithm whose policy gradient algorithm is more simpler than other SOTA methods and it has an unbiased gradient. Instead of formulating the problem as an traditional mean variance constrained problem, the authors utilized quadratic utility theory and formulate the problem as variance minimization problem with a mean reward equality constraint. Then by reformulating the problem with the penalized problem and opening up the variance formulation, they showed that this mean-variance formulation does indeed have an unbiased policy gradient, that does not require advanced techniques such as double sampling or frenchel duality. To demonstrate the effectiveness of this method on balancing risk and return, they also evaluate their methods on several risk-sensitive RL benchmarks (such as portfolio optimization) and compared with a wide range of risk-sensitive RL methods.\n\nIn general, I find this paper well-written with ample discussions of state-of-the art mean-variance RL algorithms. I also like the flow of this paper which first enumerates the existing issues of mean-variance RL approaches (that requires double sampling or frenchel duality trick to get an unbiased gradient for actor critic), and then propose an alternative algorithm with unbiased gradient that is much simpler yet circumvents the aforementioned complexity. They also demonstrate the performance of this new method on sufficient number of  experiments, ranging from discrete action atari domains, and the portfolio optimization problem, including comparisons with most known mean-variance RL algorithms, and showed that the proposed method achieves some of the best results.\n\nHowever i do have several questions about this paper:\n1) How does the per-step variance discussion in Section 3.2 relate to the proposed method?\n\n2) Can the authors provide more motivations for the problem formulation (1)? It's different from the standard Markowitz mean-variance formulation. How does one set \\psi instead of treating it as a tunable parameter. Even if  one is convinced that (1) is the \"right\" RSRL formulation to solve, why is the penalty formulation in (3) equivalent to the original formulation of (1)? Is there any formal proof?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper but needs improvement",
            "review": "The paper proposes a policy gradient style RL algorithm that optimizes an expected quadratic utility, a commonly used objective of risk management in finance and economics. The key idea here is based on the observation that when using the quadratic utility function, the use of mean-variance RL methods can be shown to optimize the utility of the agent. To this effect, the paper considers the use of expected quadratic utility maximization in the policy gradient. The quadratic utility can be naturally modeled using mean and variance. The paper implements two variations -- policy gradient and actor-critic with EQUM framework. \n\nThe main contributions of the paper listed as follow:\n1 The paper shows that maximizing the expected utility by balancing the mean and variance term is equal to reward targeting optimization or constrained cumulative expected reward problem, and could also be interpreted as maximizing the expected reward with the variance as regularizer.\n2 It introduces two algorithms, standard policy gradient and actor-critic, with the EQUM framework.\n\n\nI quite like the paper. The problem is well motivated. I like the idea of using EQUM motivated by economics as the objective function. The paper addresses all relevant works and is written well. The theoretical justification is quite good. The algorithms appear to be well designed and as far as I can see, they are technically correct.  The proposed framework is easier to compute and could be extended to many policy gradient methods, so there is a lot of potential for future works and applications.\n\nWhile I like the paper, there is clearly some room for improvement.\n\nThe justification of \\psi as \\beta/2\\alpha is not well justified. I find the explanation that this is based on economic players and economic empirical studies quite vague. I would have preferred to see the citations and proof. This is crucial to make sure that the algorithm does converge to the correct values. The values are quite important and not being precise will make it hard to reproduce the results. \n\nThe choice of \\psi through cross-validation also appears vague. I understand the nice properties that you have in your framework (particularly avoiding double sampling) but replacing a costly sampling with a cross-validation search for a regularization parameter is not quite convincing. As shown in Figure 1, it is clear that the method is quite sensitive to the choice of \\psi and so this choice needs to be well defined. \n\nWhile the empirical evaluation appears nice and comprehensive, I was a bit disappointed at the choice of atari games for evaluation. I would have liked to see more domains from finance and economics that would provide a better idea of the efficacy of the proposed methods. \n\nIt is true that EQUM methods perform best in R/R, but there is no consistent story here. Different psi can have arbitrarily different performance and I cannot tease out any performance discussion clearly. What really is missing is a good analysis of the empirical performance.\n\nI think the paper has a lot of merit and can be of high impact. But more work is needed in the analysis and discussion part of the paper. With this, it can be a really good submission.\n\nTypos:\n\nPage 2 section 3.1: The formula of maximizing expected reward with regularization has a pair of redundant parenthesis.\n\nPage 3: “Our proposed EQUM framework more focus on this problem.” Reads awkward\n\nPage 4: “Here, we introduce three interpretations of EQUM.” And you present a list of 4.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An Expected Quadratic Utility Maximization (EQUM) framework for policy gradient Mean-Variance control: interesting idea, but both methods and experiments should be improved.",
            "review": "## Brief Summary\nThe paper proposes an Expected Quadratic Utility Maximization (EQUM) framework for policy grandient Mean-Variance control. The authors claim that current state-of-the-art methods suffer either suffer from computational issues or cannot control risk at desiderable level, hence they propose their approach as a possible solution. They provide different interpretation for the EQUM: standard objective with a regularization, variance minimization with a constraint on expected return and a return targeting optimization. A policy gradient algorithm for EQUM is proposed together with its Actor-Critic extension.\nIn the experimental session, three settings are provided: a sensitivity analyisis on the \\phi parameter on the CartPole environment, and two risk-averse experiments on Atari games and a Portfolio optimization environment.\n\n## Strong Points\n- The multiple interpretations of the EQUM framework makes it an interesting objective to be studied\n- The state-of-the-art is complete and citations are correctly reported.\n- The paper is readable and easy to follow in all of its parts.\n- The experiments are executed in a proper way, and the proposed baselines are correct. Details are reported to reproduce the results.\n\n## Weak Points\n\n- In the \"Existing MVRL Methods\" section the authors claim that Xie et al. (2018) approach does not allow to control risk at a desirable level, and they use this point to prove the need of an alternative framework. However, this is not true, since different penalty parameter values allow to control risk in different ways. \n\n- It is not clear why one should prefer the EQUM framework over the classic Mean-Variance one. In particular, the gradient they consider is equivalent to ignoring the gradient of the squared expected return term in the classic mean-variance policy gradient, hence it is clearly not equivalent. The EQUM formulation has the same problem of the Mean-Variance one, namely, it penalizes positive deviation as well as negative ones. Moreover, the interpretation as a variance minimization problem with a squared penalization over deviation from target return shows that EQUM is pejorative from this point of view, since positive deviation are penalized again in the soft constraint. Finally, it is not clear in which setting is is convenient to set a target return instead of trying to maximize it.\n\n- In section 4.3 the authors implement the EQUM Policy Gradient, directly extending the REINFORCE estimator. However, the actor-critic formulation they provide is not a correct one, but it is simply a value function baseline. An actor-critic framework should instead substitutes the actual return with a bootstrapped estimate in the gradient formula. Moreover, the authors claim that the extension to TRPO and other state-of-the-art algorithms is straightforward. This is not true, since recent safe policy search algorithms relies on theoretical basis, that are not guaranteed to be valid also in this new framework. Therefore, the authors should provide some analogue safe guarantees also for the framework they propose, otherwise the application of those techniques is not justyfied.\n\n- The first two experiments on CartPole and Atari games seem not to be very well suited for a risk-averse approach: they represent, indeed, two deterministic environments, in which there is not any clear trade-off between risk and expected return.\n\n- The Average Reward measure is used also to evaluate the actor-critic experiments, however, in those cases, there is a discound factor, hence, the expected return would have been the correct performance to report.\n\n- In the financial experiments, performance are not measured w.r.t. the real objective (minimizing variance with a penalty on return targeting). One would expect to see it among the reported measures, otherwise, it is not clear if the method is effective in optimizing its real objective. \n\n- Tamar baseline is tested with only one risk-aversion parameter, hence, the comparison is not fair. Moreover, it would have been more interested to see among the results the approximated Pareto Frontier, obtained by the proposed approach and the baseline with a sensible set of risk-aversion coefficients.\n\n- Some minor fixes:\n    + \"Alternative Objective functions of AC\" in the appendix: it should be specified that the second proposed approach is a n-step Temporal Difference.\n    + The value function notation is difficult to read: it would be better, for example, to use \"V\" for the classical value function, and \"M\" for the second moment one.\n    + Computational issues are different from sampling issues: they become \"computational\" issue only when we need a simulator.\n    + \"Tamer\" should be \"Tamar\".\n    + Plots lower parts in Figure 1 are hard to read.\n\n## Recommendation\nMy reccomendation is to reject this work: there are incorrect statements and derivation, which have been pointed out before, and the overall contribution is small. The proposed framework is interesting for the multiple interpretations that offers, however, it is unclear when it could be useful in a practical case. One of the proposed techiques is flawed and need to be revised, while the other one is a trivial extension of the standard REINFORCE algorithm. Experiments are also not convincing in showing either the effectiveness of the technique, or its superiority w.r.t. the state-of-the-art.\n\n## Questions for the authors\n- Why should one prefer this framework over the mean-variance one, a part from sampling issues?\n- Can the authors provide more examples in which it is convenient to minimize variance targeting a specific return?\n\n## Additional Feedback\n- Since the second moment gradient is equal to the Mean-Variance one, ignoring the gradient of the squared expected return, the authors could use the work by (Tamar, 2012) to correct their Actor-Critic part and include an analysis on compatible features too.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}