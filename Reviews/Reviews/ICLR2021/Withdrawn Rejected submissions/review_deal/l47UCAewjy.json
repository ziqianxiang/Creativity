{
    "Decision": "",
    "Reviews": [
        {
            "title": "Motivation is unclear for why they should be analyzed and why the methods should be chosen",
            "review": "Why do the authors look at the “gating state”? Its motivation is not clearly explained. For instance, how is it compared with analyzing layer activations employed in existing studies? What are its advantage and disadvantage? I think the gating states of a net represent how it works and behaves to a certain extent, but not to a full extent. It is unclear how small or large the loss (of using gating states instead of others) will be. \n\nWhat is the practical implication of Eq.(6)? As the complexity and disentanglement are quantified for gating states, this question also goes back to the first question.\n\nWhat can we read from Figs.3 and 4? It seems the authors’ summary (1)-(3) in the paper merely trace complex behavioral differences caused by differences in the net architecture and the task; they do not provide generalizable findings.\n\nIt is unclear what the “task complexity” means in Fig.5 and the related discussions. The authors seem to explain this in a brief statement, “In this way, since the task MLP contained n ReLU layers, we considered the complexity of using target MLPs to mimic task MLPs was Task-n.” It is unclear what the authors did in the experiments and, more importantly, why it can be considered “task complexity.” In their summary (1) and (2) in “Findings from stacked networks,” the authors state “for the task of low complexity” or “as the complexity of the task increased,” but I don’t think such generalization is possible.\n\nIn the experiment of Sec.4.3, the effects of a loss constraining the complexity are examined. Figure 7 only shows the difference between L_test and L_train. If L_train increases due to the new loss, then the decrease of the difference does not necessarily mean that L_test decreases. An answer might be found in Fig.8 of the Appendix, which shows a few instances that L_test decreases. However, the authors state L_test decreases “sometimes” or “in some cases.” What does this mean? “Sometimes” means specific combinations of a network and a task, e.g., Residual MLP on CIFAR-10 or ResNet-34 on the first 10 classes of Tiny ImageNet? In any case, the explanation is too vague and needs to be improved. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Complexity of the gating states for the gated layers allows to inroduce a new loss which prevents over-fitting.",
            "review": "In this paper authors introduce three metrics based on entropy and mutual information for the gating states of the gated layers (like ReLU, dropout and max pooling): complexity measures of feature transformations. Authors prove negative correlation between introduced feature transformations complexity and entanglement of DNNs and show this results also in experimental setting. Further authors study how complexity changes during training for MLP-based DNN and for DNNs with skip-connections. Finally, they provide estimation on the complexity measure and introduce complexity loss minimization in addition to the task loss. It is demonstrated with experiments on CIFAR-10 and Tiny ImageNet that the gap between train and test loss reduces if the weight of complexity loss is increasing. \n\nPros:\n- Idea how to measure complexity vie the entropy of binary vectors (gated states) of gated layers. Simplification with usage of binary states only is cool idea (we don't consider the feature transformations in the real space).\n- Proof with some assumptions about correlation between complexity and disentanglement and showing it with experiments on MNIST.\n- Analysis of complexity behaviour for different DNNs on different datasets.\n- Introducing the complexity loss as approximation of complexity and showing that it helps in generalization problem\n\nCons:\n- Narrowing transformations of a DNN layer $x'\\in \\mathbb{R}^d \\to \\mathbb{R}^d$ to binary states doesn't allow to analyze DNNs with other activation functions which has no gated nature.\n- In all experiments there is no info about models accuracy which makes harder to understand dependence between accuracy and complexity.\n- Absent of training / test loss and accuracy analysis in case of experiments with complexity loss + careful comparison with standard training without complexity loss.\n\nComments:\n- In the paper abstract it is \"prove the strong correlation between the complexity and the disentanglement of transformations\", while in the OpenReview abstract \"prove the negative correlation between the complexity and the disentanglement of transformations.\" This was a bit misleading.\n- In the page 1 \"Given a DNN, the complexity of transformations represents the diversity of transformations that map each input $x\\in X$ to the corresponding $y\\in Y$ through all images.\" Here it is better to reformulate a bit, it is not clear what authors mean with \"through all images\" (maybe \"through all images\" -> \"for all images $x\\in X$\").\n- Do we compute entropy $H(\\Sigma)$ based on the training images only (not for all image space)?\n- Typo in fig.8 caption \"with the increas\" -> \"with the increase\"\n- What about another relation for proposition 2 between $H(\\Sigma_1, .., \\Sigma_l)$ and $H(\\Sigma_1, .., \\Sigma_l,  \\Sigma_{l+1})$? Could authors comment why they consider complexity through layerwise propagation as the entropy of last several layers? (My point of view: it should be all relations between entropy of $H(\\Sigma_{i_1}, .., \\Sigma_{i_k})$ and entropy $H(\\Sigma_{j_1}, .., \\Sigma_{i_n})$ to answer the question what is the relation between complexities on different layers).\n- On which data are experiments for Fig.1 done?\n- For readability it is better to provide formula for the left side of eq. 4 which is used to get the right side (otherwise the first thing in mind is to use the definition introduced in the top of page 4 for the $I(X; \\Sigma; Y)$).\n- \"Equations (2)(3)(4) are given in\" this sentence makes to believe that all equations authors didn't get by themselves (with specific definition of complexity using gated states) but just took from another works. Could authors make this more clear?\n- Fig. 3 and 4 hard to read, the spaces between plots and captions should be larger.\n- Fig. 5 no info in the caption about difference between left and right plots.\n- Result \"Findings from stacked networks. (1) For the task of low complexity, deep MLPs learned more complex transformations than shallow MLPs.\" seems to be not obvious. Any comments on this? \n- All figures could be better formatted with the text (captions and axis labels).\n\nThe paper is well-written, however, there are several not clear places in the experimental part, especially for the complexity loss which provides one the main paper results on better generalization.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new tool to analyze the learning of DNNs, some observations are not unexpected and suffer from theoretical analysis.",
            "review": "This work proposes to use the gating state $\\Sigma$, rather than the latent representation Z, as an alternative tool to \nanalyze the dynamics of learning and the generalization of deep neural networks. Three measurements are also developed, namely the entropy of gating state H($\\Sigma$), the mutual information I(X;$\\Sigma$) and the multivariate mutual information I(X; $\\Sigma$; Y). According to the authors, the gating state encodes if the feature in each dimension can pass through the gating layer, and thus can be interpreted as partial of the information contained in Z. Some interesting observations are made. For example, the complexity of $\\Sigma$ decreases with layer depth,  which may suggest that deep features are more simpler to do interence than shallow features. However, there are some observations are not unexpected and suffer from theoretical analysis (especially the new loss function).\n\nI listed a few of my concerns:\n1. From Eq. (6), it seems that minimizing the complexity is at the risk of increasing the degree of entanglement (measured by a total correlation). This is an unfortunate fact, especially when the authors add an extra term on minimizing H($\\Sigma$) in the loss, which may sacrifice the \"disentanglement\". Note that, similar trade-off has been extensively investigated in VAE and its related disentangled representation learning. I understand that earlier works focus on Z, rather than $\\Sigma$. But I still prefer similar discussions and analysis are missing.  Otherwise, the negative correlation becomes unclear and practitioners still do not know its guiding role in training a DNN.\n\n2. Authors argue that for network without skip connections, the complexity decreases first and then increases. Here, I have tow concerns: (1) It seems from Figure 3 that the observations do not hold for all networks without skip connections, like the VGG-16. In this sense, the explanation seems a little vague. (2) The explanation to the monotonically increasing trend of complexity in residual networks seems not convincing. \n\n3. Regarding the new loss function, which is exactly a cross-entropy loss plus a new regularization on H($\\Sigma$) or I(X;$\\Sigma$) (in most cases). Note that, if I replace $\\Sigma$ with Z, the loss can be interpreted as a network parameterization of information bottleneck. This is just because I(Z;Y) can be implemented by a cross-entropy term. Given this perspective, it is not unexpected that the new loss encourage the small gap between training and testing loss. In the information bottleneck, there is a few theoretical attempt in this direction, e.g., [1]. Again, I would expect discussion and theoretical analysis on this observations. Otherwise, the observation itself has little guiding significance.  \n\nA typo: last line at page 7, L = L_{task} + $\\lambda$ L_\\{complexity}.\nMissing references:\n[1] Vera, Matias, Pablo Piantanida, and Leonardo Rey Vega. \"The role of the information bottleneck in representation learning.\" In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1580-1584. IEEE, 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper proposes an information theoretic metric (and two similar variants) to measure the complexity of feature transformations encoded in neural networks. The proposed metric is shown to have negativity correlation with the disentanglement metric. Empirical studies are performed to use the proposed metric to 1) analyze the complexity during DNN trainings; 2) explore the maximum complexity of DNNs; 3) as a regularization loss to reduce transformation complexity and alleviate overfitting. Overall, I don’t think the paper is good enough for an accept, since the proposed metric and some experimental setups are not very convincing and well explained, and the contributions of the paper is a bit trivial. I detail my major questions and concerns below:\n\n1.\tThe motivation and the intuition of using the entropy of the gating states of nonlinear operations as the complexity measure are unclear. It only considers a very rough measure (entropy) of the nonlinear activation distribution and also ignores the complexity of linear transformations. It’s not very convincing that the proposed metric would be a good measure of complexity and a clearer explanation is needed.\n2.\tAnother concern of the paper is its theoretical contributions. This paper directly uses the KDE estimator proposed in previous works (Kolchinsky et al., 2019; Saxe et al., 2019) as a metric for the transformation complexity. And the theoretical analysis about the metric, e.g., complexity decrease through layerwise propagation and the correlation with the disentanglement metric, is very trivial.\n3.\tThe discussion of the proposed metric is confined to nonlinear activations with discrete gating states. Can we extend the metric extend to activation functions with continuous gating states, like tanh and sigmoid? \n4.\tIn Figure 3 (a) & (b), why are the lines of different models trained on CIFAR collapse? And why does the metric exhibit totally different trends on different datasets?\n5.\tThe experimental setup for the “maximum complexity” part is not very convincing and rigorous and needs further explanation. It is not clearly pointed out why this setup (use target MLPs to mimic task MLPs) could estimate the ceiling of a DNN’s complexity. And it’s assuming that increasing MLP layers means increasing task complexity, which might be wrong.\n6.\tAre there any intuitions or theoretical justifications for the difference of the trend of complexity change between DNNs with and without skip-connections?\n7.\tWhat’s the computational overhead of training DNN with the complexity loss? And how does adding the complexity loss influence the model performance (i.e., train/test loss/accuracy)?\n\n\nAdditional Comments/Questions: \n1.\tIn the definition of $I(X;\\Sigma;Y)$, the equation of $I(X;Y|\\Sigma)$ should be $I(X;Y|\\Sigma)=H(X|\\Sigma)-H(X|\\Sigma,Y)$.\n2.\tProposition 2 relies on the assumption that the DNN does not introduce additional information that is not contained by the input, which should be clearly claimed in the main text of Proposition 2.\n3.\tThe notation of dimension index and training sample index is a bit confusing. It would be better to use superscript for training sample index and subscript for dimension index.\n4.\tAt the bottom of page 7, the loss function should include a $\\lambda$.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}