{
    "Decision": "",
    "Reviews": [
        {
            "title": "Pseudo-label fusion and distillation for semantic segmentation domain adaptation",
            "review": "**Summary**:\nThis paper introduces a new pipeline, called pseudo label fusion (PLF) to deal with the semantic segmentation domain adaptation. The main novelty is that it leverages multiple networks to improve the predictions’ robustness. Three different aggregation functions are designed to combine the results from different teacher networks. A student network is also used to distill out the information from the teacher model. Authors conduct experiments on the standard settings to verify the effectiveness of the proposed pipeline.\n\n**Pros**:\n+ The whole idea makes sense. A new framework is proposed for semantic segmentation domain adaptation. Several variants of aggregation functions are also designed.\n+ Authors conduct comprehensive experiments to show the effectiveness of the proposed pipeline. Parameters analysis is also performed for the design choice.\n+ Overall, the paper is well written and well organized.\n\n**Concerns**: \n- For figure 1, I wonder why the mIOU is very low for train class if using PLF? What is the potential reason behind it?\n- One concern is about novelty. If I understand the whole context correctly, the teacher-student distillation framework has been explored to reduce the model size and computational cost. Using multiple networks to do ensembles is also touched by the previous works. This paper applies these techniques to the application of domain adaptation. I feel the whole pipeline is not novel enough to reach the standard of this venue.\n- For the multiple teacher networks, do they use different backbones? If that, is it possible to have the experiments to show the results using the same architecture, but with different initializations or training procedures? I am curious about the performance of this experiment.\n- As we have multiple teacher networks, is it possible to have the ablation study to show how number of teacher network affects the final performance?\n\n**Minor Comments**:\n* the last line on page 5. There are repeated definitions of $R^{fused}$.\n\nOverall, I prefer the rating as below the threshold at the current stage. The main reason is the limited novelty. Hope the authors could address my concerns or questions in the rebuttal period.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Insufficient comparison with competing methods, unclear motivation, and incremental technical contribution.",
            "review": "The paper proposes a method for cross-domain semantic segmentation with pseudo-label fusion and aims to better capture the label distribution in the target domain. The proposed framework consists of training an ensemble of teacher networks, fusing the predictions of the teacher models and distilling the knowledge in the fused prediction with a student network. Through distillation, the student network is able to approximate the ground-truth label distribution for target images. Following the framework, the authors proposed different approaches for pseudo label fusion, namely certainty, priority and majority fusion and explored the effect of each on different backbones on UDA between several dataset pairs. \n\nThe paper is well-organised and the method is well-explained, I am also happy to see that the method works for several different architectures including deeplab v3+, and DRN 50. However I have several concerns regarding the paper:\n(1) Cross-domain semantic segmentation is a quite active research field. However, I found the recent work in this space (such as [A][B]) is not discussed/compared in experiments. Therefore it's not fully clear where the method stands compared to the SOTA.\n(2) The technical contribution also seems incremental, the main contribution lies in the approaches to fuse teacher models, which may have limited impact on the work for UDA. \n(3) The motivation is also not fully clear to me, it is somewhat expected that ensemble of different predictions can improve the performance. However, whether this is a key problem in cross-domain semantic segmentation is a bit arguable.\n(4) In Table 3, the proposed method achieves a clear improvement between Deeplab v3+ and v2 in Synthia to CS (51.76 vs 48.45), however the gap is much smaller in GTAV to CS (57.65 vs 57.15). I am curious what's the reason behind this?\n(5) The presentation can be improved, currently the tables are not easy to read. \n\n[A] FDA: Fourier Domain Adaptation for Semantic Segmentation\n[B] Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer\n\n--\nSome of my concerns are addressed in the response, I appreciate the provided comparisons and results. However, I am still not fully convinced that the technical contribution is sufficient. To reflect this, I slightly bumped up the rating to 5.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper tackles the problem of unsupervised domain adaptation (UDA) for semantic segmentation using a pseudo label fusion framework (PLF). The basic idea is that the PLF fuses the pseudo labels generated by an ensemble of teacher models. Also, this paper proposes multiple methods for the label fusion and further investigates how they influence the performance. Experimental results show the effectiveness of the proposed method on both domain adaptation and generalization setting.",
            "review": "The claims and the method are correct. The motivation of this paper is the performance can be further enhanced by incorporating knowledge from multiple models. However, the proposed framework is very similar to the multi-teacher distillation, such as [*],[#]. Meanwhile, the certainty fusion and priority fusion methods are widely used in self-training. So the novelty is limited.\n\nThe authors have extensive experiments in Table 1, which reports the number of trainable parameters, the inference speed, and the semantic segmentation performance of the PLF framework with different student network backbones. The PLF achieves good performance even with compact and fast backbones compared with the state-of-art methods. In Table 2, a baseline ensemble is needed. Since the PLF use DACS, R-MRNet, CBST, and MRKLD models as teachers, the traditional ensembles which obtained through averaging the final predictions across all teacher models should be compared with the proposed method.\n\nThe paper organization could be improved, especially in section 5.\n1.What’s the confidence map in the paper? Is it the same as the channel-wised attention?\n2.There is a typo “Table A5” in the first line of section5.2\n3.There are some questions with the analysis in section5.3. Why mobileNet V2 whose parameters is similar with the Resnet 101 have a significant decrease in inference time? Is there a data error in the number of parameters?\n[*] MEAL: Multi-Model Ensemble via Adversarial Learning from AAAI2019\n[#] Multiple Expert Brainstorming for Domain Adaptive Person Re-identification from ECCV2020\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"UDA semantic segmentation via pseudo-label fusion\"",
            "review": "This paper proposes a pseudo label fusion framework for unsupervised domain adaptation in the context of semantic segmentation. The proposed method generates fused pseudo labels by aggregating multiple teacher model’s pseudo labels. Then, the fused pseudo labels are used to train a student model to perform semantic segmentation in the target domain. Experiments have demonstrated that the proposed method could achieve better results than SOTA. Below are some comments.\n\nQuestion 1: The main contribution of this work is to combine the advantages of different UDA methods to produce more reliable pseudo-labels through ensemble learning. Although the proposed method achieves excellent performance on multiple semantic segmentation datasets, its innovation is insufficient. Regardless of model ensemble, the innovation proposed in this paper focuses only on how to fuse pseudo-labels generated by different teacher models, which seems not enough for a ICLR paper.\n\nQuestion 2: This paper describes that in UDA, the methods based on adversarial learning are more time-consuming and laborious than that based on self-learning methods. However, the method proposed in this paper requires the training of multiple teacher networks, which should be more time-consuming. It lacks timing results to support this claim.\n\nQuestion 3: In 3.3.2, the construction of H^fused is not clear. What does the U in H^fused=U_(k\\inK)H_i^c(k) represent?\n\nQuestion 4: This paper proposes that PLF can combine the advantages of different UDA models. However, according to the results in Table 3, the IOU of class \"train\" is 0, which is far lower than the IOU of class \"train\" in teacher models, such as CBST and MRKLD. What is the reason for this?\n\nQuestion 5: Why are the teacher models used in GTA5 to Cityscapes different from that used in SYNTHIA to Cityscapes? Is it because of the side effects of using more teacher models in the SYNTHIA to Cityscapes scene?\n\nQuestion 6: The ordinate in Figure 1 shouldn't be mIoU it should be IoU. Are the last two formulas on the penultimate line of page 5 wrong?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}