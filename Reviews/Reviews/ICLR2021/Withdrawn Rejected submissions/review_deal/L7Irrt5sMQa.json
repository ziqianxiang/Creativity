{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well-founded. The key concern was that the novelty over the Sato's paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer's comments and resubmit it to a future venue. \n"
    },
    "Reviews": [
        {
            "title": "Well written paper about random features in GNNs",
            "review": "The paper study the effects of adding random features (RF) to graph neural networks (GNN). First, it is shown that, quite surprisingly, adding random features makes GNN universal approximators of invariant functions. Next, a novel dataset is defined that is aimed at evaluating the performance of models that have high expressive power. Finally, several experiments show that adding RF performs well on the proposed dataset.\n\nI think the paper is well written and well organized. The theoretical aspect seems novel and quite surprising, especially since it shows that adding RF makes GNN more expressive than k-GCN (or k-IGN) for any k. The new proposed dataset is interesting, and the experimental result looks promising, especially since RF performs quite well compared to 3-GCN while having much fewer parameters. Also, the observation that only partial randomness can already be beneficial is interesting.\n\nI have a couple of concerns regarding the paper for which I would be happy to see the authors’ comment:\n1)\tThe phrasing of Theorem 4.1 is a bit vague because it is stated with an \\epsilon, \\delta approximation while there is no clear explanation of how these parameters affect the theorem. I suppose that smaller \\epsilon, \\delta would mean a wider network, but this dependence should be shown explicitly. For \\delta, this dependence is somewhat shown in the proof of Lemma A.5, but for \\epsilon it is very unclear and is probably related to Lemma A.2 which cited from another paper.\n2)\tIf I understand the proof correctly, the width of the network in Lemma A.1 (and also in Theorem 4.1) should be super-exponential, no matter what is the target function. That is because in Lemma A.4 the sentence takes into account every graph in G_{n,k}, the number of such graphs is super-exponential and I suppose that the width of the GNN that realizes the sentence depends on the length of the sentence (or at least the number of literals). If this is true, then I think it is important to point that out in the main text.\n3)\tContinuing the previous point, the paragraph after Remark 1 isn’t clear: why the construction is adaptive to the descriptive complexity of the target function if for any target function the construction requires memorization of all graphs with n nodes?\n4)\tRegarding the Experimental results, I think it is important to also test RF on real datasets. The reason is that without this experiment it is not clear if adding RF doesn’t actually harm the performance of GNN and thus are actually very impractical. As the authors stated, I don’t expect RF to significantly improve performance on real datasets because high expressivity might not be required, but I am concerned that it will hurt the performance. \n\nI also highly suggest uploading the EXP and CEXP datasets as supplementary material, since these are newly generated datasets, and this way other people could also experiment on them and compare performance.\n\nTo conclude, I think this is a very nice and well-written paper that adds a novel view on adding RF to GNN, both theoretically and empirically. With that said, there are some issues with the vague form in which the main theorem is stated, and experiments on real datasets would help clarify whether adding RF could actually prove helpful (or at least not harmful) for practical uses.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper and contribution. ",
            "review": "**Post Rebuttal**\nI thank the authors for the quick replies and updates to the paper. \nI keep my positive score.\n\n---\n\n\n**Summary of Contributions**\nThe paper analyzes the model of Random features in GNNs as suggested by Sato et.al., in the paper called RNI.\nA result proving the universality of the RNI framework is introduced, a first of its kind in low tensor degree GNNs. \nTo evaluate the expressiveness of RNI and other more expressive GNNs, the authors design two datasets wich require 2-WL distinguishing power (which is higher than the ones MPNNs have)\n\n**Strengths**\n- Novelty - The universality result on RNI is novel and further extends the hints of improved expressiveness explored by Sato et.al.\n- Dataset design - the design of new datasets for expressiveness discrimination are an important contribution to the community. \n\n**Weaknesses**\n\n- Regarding invariance of RNI - a more rigorous explanation would be fit there. Why does RNI preserve invariance? \n- The main theoretical result of the paper is just appearing in the paper without details and intuitions towards the proof, a proof sketch or some discussion of that flavor would make the result more clear. \n- The experimental setting is not clear enough as presented in the main body of the paper. A lot of important details has to be dug our from the appendix which is tedious for the reader. For example:\n1. The partial RNI is not well explained in the paper, and it was not clear whether the partial applies to the feature dimensions or the nodes. \n2.  The input features in the designed datasets are not mentioned in the body of the paper. \n3. An elaborate description of the 3-GCN variant is missing. \n\n- *Partial RNI* - can the authors provide an intuition as to why it works? In a way, the universality comes from the network not taking into account the node features due to the full randomness but more of a statistical behavior and the fact that the nodes are completely distinguishable. So why does partial randomness work?\n\n\n**Recommendation**\nThe paper states an important and surprising result which can contribute greatly to the graph learning community. \nA good paper, Accept.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review#3",
            "review": "This paper studies the power of message passing neural networks (MPNNs) with random node initialization (RNI). Although the power of standard MPNNs is limited to 1-WL, the main result of the paper is to prove that RNI makes MPNNs universal. The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability. The datasets have been created in a way that their graphs are 1-WL indistinguishable to serve as a test-bed for graph neural networks with power beyond 1-WL. The results on these two datasets show the merit of adding RNI to MPNNs.\n\nMy biggest reservation with this work is that the theoretical and empirical results do not seem to offer much more than what has already been provided in [1]. In fact, if [1] didn't exist, I would have given a strong acceptance to this paper. I’ll elaborate on each of these aspects below:\n\nTheoretical result: While Theorem 4.1 is quite strong as it imposes no restrictions on the function f, two things are not clear to me: 1- What classes of functions does the Theorem 1 of [1] not cover that Theorem 4.1 of this paper does?, 2- Is the Theorem 4.1 of this paper a straightforward extension of the Theorem 1 in [1]?\n\nEmpirical results: While I appreciate the two datasets developed in this work and I believe they can be useful for future research, I have a hard time understanding what intuition/insights the results provide that has not been already provided in [1]. It has been already shown in [1] that RNI increases the power of MPNNs and enables them to do well on graphs that are 1-WL indistinguishable, where existing MPNNs fail. If Theorem 4.1 covers a larger class of models than Theorem 1 of [1], then I would expect at least some experiments covering those classes of models. Right now, the only added insight of the empirical results seems to be that randomizing a subset of the nodes is better than randomizing all of them.\n\nAll mentioned about the connection between this work and [1] is “Indeed, RNI has enabled MPNNs to distinguish instances that 1-WL cannot distinguish, and is proven to enable better approximation of a class of combinatorial problems (Sato et al., 2020). However, the effect of RNI on the expressive power of GNNs has not yet been comprehensively studied, and its impact on the inductive capacity and learning ability of GNNs remains unclear.”, which does not seem satisfactory.\n\nOther comments/questions: 1- Some experiments on standard benchmarks can strengthen the paper. 2- For GCN-x%RNI, how do you initialize the other (1-x)% of the nodes? Do you initialize them as all 0s? 3- In [1], a new random feature is assigned every time the procedure is called. Is this what you do as well? 4- Any reason why the results of 1-GCN and GCN-x%RNI are not reported in Table 1?\n\n[1] Sato, Ryoma, Makoto Yamada, and Hisashi Kashima. \"Random Features Strengthen Graph Neural Networks.\" arXiv preprint arXiv:2002.03155 (2020).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Why does randomness help? ",
            "review": "The paper studies the how random initialization of node states can improve the expressivity of message passing graph neural networks. Theoretically the paper shows that RNI makes MPNNs universal approximators for invariant functions over graphs. To supplement this claim, the authors evaluate GNNs with RNI and higher-order GNNs over a carefully constructed synthetic dataset and show that RNI (even if only a fraction of the nodes are randomly initialized) are as expressive as higher order GNNs. They also highlight some drawbacks of this approach, such as slower training and sensitivity to hyperparameter tuning.\n\nOverall, I find the topic and the findings of the paper quite interesting. However, explanations as to why or how randomness helps is missing in the paper. There is also room for improvement in the presentation and writing. The statement of Theorem 4.1 is a little vague; it would help if its made more precise (e.g., how much randomness is required, what is the depth of the MPNN required, the state dimensions etc.). The notion of invariance and equivariance can also be defined much earlier in the paper. \n\nWhile it’s interesting that randomness can yield MPNNs to be universal approximators, could you please give some intuition as to what makes randomness so essential? In the proof of Theorem 4.1 (in Appendix A.2), the random initialization together with the linearized sigmoid activation function is used to show that the vectors $x_i^(1)$ are mutually distinct {0, 1} vectors with high probability. To my understanding, this is the only place where the randomness of the initialization is used. If that is the case, why can’t any deterministic initialization that ensures mutually distinct {0, 1} vectors for $x_i^(1)$ suffice?  \n\nThe datasets EXP and CEXP seem to be motivated as graph encoding of SAT problems. Are they bipartite graphs? Could you give a sense for how the graphs look like (number of nodes, edges etc.)? \n\nIn the partial random node initialization (GCN-x% RNI), are you initializing some nodes randomly and other deterministically? Or are you initializing some entries in the state vector randomly and remaining entries deterministically? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}