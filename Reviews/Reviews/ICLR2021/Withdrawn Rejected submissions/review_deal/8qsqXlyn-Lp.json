{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "A method is proposed for removing prior knowledge, presented as a\ndistance matrix, from low-dimensional embeddings, to focus them on\nwhat is new.\n\nThe task of visualizing novely in data is interesting and good\nsolutions would potentially be highly useful.\n\nThe proposed method essentially substracts a distance matrix from\nanother. While this is sensible, it is not completely clear in what\nsense this is _the_ right solution for what the embeddings will be\nused for.\n\nIn final discussions among the reviews, the main remaining concerns\nwere considered severe: comparisons to other methods being limited,\nand possible problems in one of the experiments.\n"
    },
    "Reviews": [
        {
            "title": "Factoring out prior knowledge in dimensionality reduction",
            "review": "The paper presents JEDI and CONFETTI, two approaches to the task of factoring prior knowledge when creating low dimensional embeddings for data exploration. The authors allow the prior knowledge to be specified as distances based on an information theoretic extension to tSNE and UMAP.\n\nThe task is interesting and paper is well-presented. The theory section is a bit weak, although it provides an intuition of why those specific extra terms were added to the optimization problems. \n\nI found the experiments interesting -- the three case studies are well thought out. However, I find the comparison with the competitors unfair in most cases, as the information that it is provided to those is quite different. In this case, I think the paper would benefit from not comparing to those methods but to use simple baselines of projections that de-bias the embeddings to illustrate the advantages of using either JEDI or CONFETTI.\n\nFinally, I would like to have seen a discussion on how realistic it is that the prior knowledge would come in the form expected by the authors in real-world problems. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good ideas but somewhat ad-hoc nature in proposed solution, and limited experiments",
            "review": "Description:\n\nThis paper aims to \"factor out\" existing prior knowledge from embeddings, by adapting a tSNE objective or from other methods by operating on input distances. The prior knowledge is assumed to be pairwise distances.\n\nIn the tSNE case, a neighborhood distribution is derived from the prior pairwise distances, and a \"parameterized Jensen-Shannon divergence\" is proposed to measure deviation of the output neighborhood from that. The objective is then a difference of the tSNE objective and that divergence, optimized by gradient descent.\n\nIn the general case, it is proposed to edit the pairwise distances of the data by, substracting out the pairwise distance given in the prior information (up to some constants), for all pairs of different points. A metric property of the definition is proven, and a proof that independent prior distances would not on expectation change the neighborhoods.\n\nExperiments compare the result to conditional tSNE and to a modification of supervised LLE, on synthetic gaussian-cluster data and on two real data (flowers and single cell sequencing data). \n- On synthetic data, results are better than conditional tSNE but difference to the modified supervised LLE seems small. \n- On flower data, there are no comparisons to others except unmodified tSNE, but the proposed methods may be able to show structure beyond the prior as claimed.\n- In the single cell sequencing data, the comparisons are to unmofidied UMAP and to ctSNE. The UMAP with modified distances seems to show some batch-d and tissue-type differences.\n\nEvaluation:\n\nThe idea of factoring out prior knowledge seems very meaningful, although not completely new. \n\nThe approaches here seem reasonable but are somewhat simplistic:\n- The tSNE approach seems reasonable but not very striking: essentially it is a weighted sum of two cost functions, and most of the detail is just to ensure the new divergence does not overwhelm the original one.\n- The distance editing seems rather ad-hoc; while the result may have a metric property, it is hard to say in what sense this is the \"right\" way to combine the prior information to the distances.\n\nThe experiments are rather limited, which is somewhat disappointing; in the flower case no comparison to ctSNE or SLLE-1 is done, or to SLLE-1 in the single cell sequencing data (too large matrix inversion issues are claimed - could those have been resolved e.g. by computing the result for a subset instead?). The results do seem to show ability to show information beyond the prior knowledge, but for a modern dimensionality reduction paper I would expect a more thorough evaluation.\n\nComments:\n\n- Neighbor embeddings that explore beyond known annotation have been proposed long before conditional tSNE: e.g. the method of [1] includes extraction of gene expression features unrelated to an ontological annotation, to embed structure not explained by that annotation.\n\n[1] Peltonen J, Aidos H, Gehlenborg N, Brazma A, Kaski S. An information retrieval perspective on visualization of gene expression data with ontological annotation. ICASSP 2010.\n\n- The equation in definition 1 seems to be wrong. In the second divergence term, \"P' + (1-beta)Q\" is not a proper distribution that sums to 1.\n\n- In the single cell sequencing data, you used marker gene expression for \"Z\" data, but what was the X data? Clarify.\n\n- Figure 1: marker shapes are far too small to be seen at a normal viewing factor or in a printed paper.\n\n- In experiments authors write that JEDI is \"not designed for labels as priors\" but in the introduction you write \"we consider background knowledge in the form of pairwise distances between samples. This formulation allows\nus to cover a plethora of practical instances including labels\". In what sense is JEDI not designed for labels as priors?\n\n- The flower experiment seems somewhat biased because the input distance is a linear sum of distances including color-distance: therefore the distance-editing by subtracting the color-distance is of course well matched to this construction.\n\n- In the single-cell sequencing case, to use ctSNE authors \"provide it the cluster labels from an agglomerative clustering\": why was this not done for the flower data too (and for SLLE-1 too)?\n\n- In the single-cell sequencing case, Figure 3(a) is hard to compare to (b) and (c) since the coloring means different things.\n\n- In Figure 3(c), why are there smooth changes of the batch-id colors? Are batches with close-by bacth numbers somehow expected to be more similar to each other?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper + Important problem but the method formulation is somewhat contrived ",
            "review": "Summary:\n=======\nThe distance metric learned by low-dimensional embeddings typically captures the knowledge that we already know. This paper proposes a principled way of factoring out prior knowledge (in the form of distance matrices) from tSNE and UMAP embeddings. Two algorithms are proposed for factoring out prior knowledge. JEDI (for tSNE embeds) uses a parameterized JS divergence-- the objective is to learn a low-dimensional distance metric that preserves high-dimensional distances but is orthogonal to the prior distance matrix. CONFETTI is the second algorithm which also optimizes a similar objective to JEDI but doesn't employ JS divergence and is algorithm independent, so one can use it for tSNE or UMAP. Results are shown on synthetic and real-world flower and cell-sequencing data and they highlight the superior ability of JEDI and CONFETTI algorithms in factoring-out prior knowledge compared to the baselines. \n\n\n\nComments:\n==========\nThe paper is well written and puts itself nicely in context of previous work. Given the ubiquity of low-dimensional embeddings these days, the paper addresses an important problem of factoring out prior information from the embeddings.\n\n\n1). The paper doesn't describe the details of optimizing the parameterized JS Divergence (pJSD) metric that they propose. Is it even convex?\n\n2). How is the beta parameter of pJSD chosen?\n\n3). I didn't fully understand why the paper makes a big deal about UMAP, when JEDI is based on tSNE and CONFETTI can work with any embedding formulation? How not just mention a \"general embedding.\"\n\n4). The formulation of the CONFETTI method seems a little arbitrary. Why are the prior distances factored out linearly? Again, no optimization details are provided regarding the CONFETTI method.\n\n5). It seems that, as an application, the proposed methods can also be used for factoring out demographic information from word embeddings. For such applications how would one define the prior distance matrix? The datasets used in the paper and the broader setup seems a little contrived. It would be nice to provide guidelines on how one can readily define such prior distance matrices for other applications. \n\n\nTypo:\n\nConclusion: \"This shows that both are able applicable to real world...\"  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review #4",
            "review": "Summary: \n\nThe author(s) provide two methods for factoring out specific covariates from tSNE, UMAP or other distance matrices. The first one is JEDI, an extension of tSNE that minimizes a parameterized divergence (that takes into account the information to be factored out) instead of the simple KL divergence between high dimensional data and low-dimensional embedding. Because tSNE has inherent limitations, the author(s) also propose CONFETTI, a simple approach to create a distance matrix based on the distance matrices of the covariates and the input data. This produces a proper distance metric and can be used upstream of any embedding procedure. On synthetic data, the proposed methods perform to the level of ctSNE as well as a second baseline sLLE-1. However, both methods are not directly meant to solve the original problem. Only JEDI and CONFETTI may factor out continuous variations (ctSNE is designed for discrete clusters). On real world data, the method seems to effectively reorganize embeddings (either of images, or of single cells) by factoring out variations of interest. \n\nStrong points:\n1. The paper is well-written and simple to understand\n2. The idea is conceptually interesting \n\nWeak points:\n1. The manuscript does not acknowledge foundational work. The idea of factoring nuisance variation out of representations is rather common, either in the setting of matrix factorization, Gaussian processes or variational autoencoders [1, 2]. This is especially true in the setting of high-dimensional genomics data, in which all the work around \"removing unwanted variations\"  (RUV) became a common theme (> 10 papers).  \n2. I am skeptical of the applicability of the method, especially regarding runtime. Much of the paper describes JEDI. However, JEDI takes 100 hours to run on the scRNA-seq data compared to tSNE (that should finish in less than 10 minutes, or at least be faster than ctSNE that runs in 20 minutes). \n3. The construction of the distance matrix for CONFETTI is interesting, but there is no effort in understanding in which sense such construction is efficient, optimal or even why it seems to be a reasonable choice. I do not think that the presented theory is helpful for understanding when the method would work well or not. \n4. In terms of experiments, a stronger point could be made by choosing a challenging setting (removing cell-cycles in single-cell RNA-sequencing data in order to better recover cell types as in [2]), and comparing against RUV models, to justify why corrections on the distance matrices is more efficient than linear corrections on the input space \n\n[1] Gagnon-Bartsch, J., Jacob, L. & Speed, T.P. Removing unwanted variation from high dimensional data with negative controls. Tech. Rep. 820, Department of Statistics, University of California, Berkeley (2013).\n[2] Buettner, F. et al. Computational analysis of cell-to-cell heterogeneity in single-cell RNA-sequencing data reveals hidden subpopulations of cells. Nat. Biotechnol. 33, 155–160 (2015).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}