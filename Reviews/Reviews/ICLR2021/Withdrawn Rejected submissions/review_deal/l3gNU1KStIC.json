{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper describes a method called 'stochastic' inverse reinforcement learning. It is somewhat unclear how this differs from other probabilistic approaches to IRL. In particular Bayesian approaches have been used in the past to obtain distributions over reward functions. However, SIRL tries to estimate a generative model over such distributions. All the reviewers foudn the paper suffering from lack of clarity, in particular with respect to how the model/algorithm is constructed. There are some possible technical problems with respect to claims about inferring demonstrations by different experts (cf. work on multi-task IRL). The experiments also seem to be insufficient."
    },
    "Reviews": [
        {
            "title": "Promising idea, but clarity lacking and evaluation weak",
            "review": "The paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. Prior work has either learned a point-estimate, notably maximum entropy IRL, or used Bayesian methods to learn a probability distribution over reward functions. Maximum entropy IRL has scaled to complex environments with unknown dynamics and non-linear rewards (with methods such as AIRL), but do not learn a probability distribution. By contrast, Bayesian IRL is more theoretically principled, but has not scaled to complex environments or non-linear rewards. This paper performs maximum likelihood estimation of a parameter for a *generative model* over probability distributions, using a Monte-Carlo expectation-maximization (MCEM) method. It therefore still outputs a probability distribution like Bayesian IRL, but is able to learn non-linear rewards unlike prior Bayesian methods.\n\nStrengths:\n  - The method is novel.\n  - The motivation of the work is good: you identify important problems on IRL in the first introductory paragraph. (It would benefit from more follow-through -- which of these problems have you solved?)\n  - Reporting examples of learned reward functions (or mean of the distribution learned) as well as summary statistics like EVD gives a more thorough understanding of the method.\n\nWeaknesses:\n  - The paper was difficult to understand.\n  - The contribution seems limited. The method still seems challenging to scale (and no attempt is made to evaluate this), which seems to be it's main advantage relative to Bayesian IRL. Moreover, it is unclear if the probability distribution learned is well-calibrated, unlike Bayesian IRL.\n  - Weak baselines. DeepMaxEnt is a fairly old approach -- why not try e.g. AIRL? There's also no information as to how you trained the baselines, so it's difficult to know whether e.g. hyperparameter tuning was performed appropriately (noting that this environment is fairly different to what DeepMaxEnt was originally trained in). You should report the results of GPIRL given that this algorithm was developed for exactly this environment.\n\nI find the approach intriguing and would encourage you to continue developing it, but the submission is too preliminary to accept at this stage. In particular, I would suggest the following modifications:\n  + Significant edits to improve clarity. For example, the abstract is quite terse, especially the second sentence. I understood it since I am very familiar with this area, but most readers would be lost. I had to read section 2.2 several times to understand what you were proposing -- here (and elsewhere) you would benefit from giving the reader some intuition before diving into the math. The basic idea is relatively simple: you are performing maximum likelihood estimation on parameters $\\Theta$ that define a probability distribution over weights. Then there are some details: how you sample the data when you perform MLE (effectively from a power set of the demonstrations, thresholded by some minimum size), \n  + Clarify what the benefits of your method relative to prior work are, and then rigorously justify this (whether theoretically or empirically). For example, is having a probability distribution over rewards actually a desiderata (in which case you should evaluate if they're well-calibrated), or is it an artifact of the method? Likewise, being candid about it's limitations would help the reader evaluate whether it is appropriate for their application or what novel research directions exist to improve it.\n  + Improved evaluation. Stronger baselines as discussed in \"Weakness\" above. More environments. Perhaps report runtimes -- this would help assess scalability. I found this line uncompelling: \"Note that since almost only objectworld provides a tool that allows analysis and display the evolution procedure of the SIRL problem in a 2D heat map, we skip the typical invisible physics-based control tasks for the evaluation of our approach\". First, this is wrong: objectworld is not that unique, you could visualize reward over e.g. a gridworld or tabular MDP, and many IRL papers have done so. Second, providing this visualization does not preclude also evaluating in more complex environments -- as the AIRL paper did, for example. I strongly suspect your algorithm simply won't scale to such environments (at least without considerable hand-designed feature engineering) -- if this is true then admit it and discuss how you can address it in future work, and if it's false then show that I'm wrong with results.\n  + You should discuss limitations of your work somewhere, e.g. conclusion. For example the convergence guarantee only holds in the limit of infinite data if I understand -- there is no finite-time bound? This is common for MCMC methods which your approach is related to, so not too surprising, but it's important to make the reader aware of.\n\nSome questions I would appreciate clarification on:\n  - How is $f_M$ actually defined? In particular, what do the weights $\\mathcal{W}$ really do? A naive reading would suggest that you take a linear combination of the \"feature basis functions\" -- but I think you must do something more complex since you evaluate in an environment with a highly non-linear reward?\n  - What is Algorithm 1: Generative Algorithm actually meant to be doing? It seems one could not use this in practice, since it requires computing the EVD, which is only computable if one knows the ground-truth reward -- in which case no need for IRL. So I assume it is meant solely for evaluation -- but I am unsure what this is evaluation. If this is meant to be focusing on the most robust samples, then you should report what probability mass these samples have. Also, is \"any W'\" meant to be a universal or existential quantifier?\n\nSome points to improve clarity. At a high-level:\n  1. Paper might benefit from separate related work section. Intro is currently serving as this. But it detracts from the story. You could just summarize the IRL problem, what the key deficiency in existing work is that you're trying to solve, and then dive into describing your method and contributions.\n  2. Many parentheses should be parenthetical, e.g. “are known Russel (1998)” -> “are known (Russel, 1998)” – change \\cite to \\citep in the source.\n  3. As mentioned before, would benefit from proofreading for grammar. Examples: “problem is ill-posed that the” -> “problem is ill posed: the”, “an inverse problem that a” -> “an inverse problem where a”; “is in a lack of an explanation” -> “does not explain”; “variability set” -> “variable set”.\n\nSome specific points:\n  - Abstract, “considerable performance” – weaselword, how good is it relative to expert, to a baseline, ...?\n  - Preliminary, description of transition dynamics: “being current state s, taking action a and yielding next state s'” – does not make the conditionality clear. Perhaps “transitioning to next state s' when taking action a in state s”.\n  - Preliminary, MDP\\R: I have normally seen this include the discount factor, which is important to understand expert behavior. I think you assume this too in your algorithm? If not, flag this, and describe how you recover rewards without knowing the discount.\n  - Typo: \"experimental trail\"->\"experimental trial\".",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Less theoretical innovation and toy experiments only",
            "review": "The authors propose to solve underspecified IRL problem  using an MCEM approach. They claim it is the first succint, robust and transferrable solution and have some results on a Gridworld-like environment. Assessment: The MCEM framework actually is a good fit to the IRL problem and I dont recall if this has been explicitly called out in the literature before. But to me, the connection and resulting algorithm don't seem enough it terms of innovation and usefulness. Arguably, both BIRL and MaxentIRL do essentially (sampled) EM with different parametric forms. The experiments are unacceptably small scale and inconclusive.\n\nThere were confusing parts in the exposition that I will outline below.\n\nGoing by section:\n\nIntro: A good overview of the IRL literature. \n\nsec 2: In the problem statement itself you need to say something about how the expert demonstrations are connected to R. Otherwise they seem like there is no relation at all.\n\n2.2:\nI'm trying to understand the motivation behind the definition of C^E_\\epsilon. \n\nIn the objective formulation you are setting \\Theta to maximize the likelihood of a randomly choose subset of expert trajectories. I have never seen such a thing done before and it is interesting and possibly a route to robustness, but I would have liked more explanation and discussion. In particular, the choice of a uniform measure across subsets of different cardinalities seems bad.\n\n Would it be simpler to just introduce a hidden variable for each trajectory, stating whether it is valid or not (i.e.  drawn from the true reward function and should therefore be used in likelihood computation). I agree then with the 3rd point in 2.2.1, this could be a different way to model expert sub-optimality than BIRL. \n\n3.1.2:\n    It took me a while to read this and I dont think I understand. You seem to set \\Theta_2 = W by the end. Then this seems to be very complicated and redundant. In particular, it is very confusing how the dimensionality of the parameter space can depend on the number of monte carlo samples and changes at each iteration ! Maybe I misunderstood something.\n\n3.1.3: \n  This seems like a straightforward EM derivation.\n\n3.2:\n\n What is the difference between the 2 stopping criteria? Just the fact that the 2nd has a patience of 3? if so, seems trivial.\n\n3.3:\n\n what is being formally claimed here? That given the condition on N_t, convergence will hold and proof is in one of the references?\n\n4 (Experiments):\n\n  Experiments are done on exactly 2 small instances of a variant of Gridworld. This is much smaller scale than the experiments you would expect to see in an ICLR paper (even granted that ICLR is very theory-focused). Previous papers have used IRL for real world problems like robotics, vehicle routing etc. \n\nSummation: A couple interesting starting point ideas in the theory part, but not completely fleshed out and ultimately the development of the EM approach is partly straightforward and part of it is very confusing to me (sec 3.1.2).  Experiments are very unconvincing in terms of scope.  \n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The authors propose an approach to model-based inverse reinforcement learning which estimates a Gaussian mixture model over reward-function parameters. The method uses MCEM and samples reward functions from a current estimate of the GMM, updates them via a gradient-descent based maximum likelihood approach and then updates the GMM to fit the updated parameters. The authors evaluate the approach on objectworld.\n\n* The paper is at times hard to follow and should be rewritten to be more clear. The contributions and assumptions could be stated more clearly and the paper would strongly benefit from proof-reading. It would also be helpful to disentangle the machinery of MCEM from the novel algorithmic contributions of this paper.\n* The choice of a GMM to represent the distribution of parameters is not motivated at all in the paper. Intuitively, the main benefit is to allow for k reward-function archetypes that represent the set of expert trajectories well; however, there are no examples nor any evaluation to show in which case this is beneficial.\n* DSIRL is used as an acronym for a variant of the method but is not defined in the paper as far as I can tell.\n* While the method can seemingly be applied to deep as well as linear representations, it is unclear what the chosen features and representation is in the experiments.\n* The method appears to draw a set of weights W_i as well as a corresponding set of expert trajectories O_i at random in each iteration. The motivation for the use of O_i is that it may correspond to different modes in the expert set, e.g. demonstrations by different experts; however, the assignment of weights to object sets is not consistent between iterations so it is unclear to me how this would be able to handle different experts.\n* In the beginning it is mentioned that IRL methods require knowledge of the transition model. While many methods do, modern IRL methods are model-free more often than not, so this claim is misleading.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review for Paper78",
            "review": "### Summary\n\nThe authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). In my understanding, the knowledge of the environment dynamics is assumed. The authors tried to validate the proposed idea on objectworld (Levine et al., 2011)\n\n### Quality\nThe quality needs to be improved in the sense that a clear theoretical link between the target problem and MC-EM cannot be found in the submission. For example, the main objective (3) is optimized through (4) and (5), but the relation is unclear. There are lots of such things in the submission. \n\n### Clarity\nThe readability of the submission is poor and needs to be improved. Lots of terms are unclear to me (e.g., succinctness, robustness, transferability of rewards). At some part of derivation, I couldn’t understand the motivation. Experiment settings are unclear, and the results are not confident and seem irreproducible with given information. \n\n### Originality\nExploiting the distribution of reward is considered in Bayesian IRL. I think the probabilistic view was originated from Bayesian IRL (e.g., uniform prior on rewards may cover the idea of this work). The submission only sets MaxEntIRL as its baseline, but I think Bayes IRL should have been considered. \n\n### Significance\nThere seems to be a minor contribution \n\n### Detailed comments\n(p.1, `Abstract`) `expert demonstrations may be optimal for many policies`\n- I feel this statement is weird since we haven’t defined the optimality of expert demonstrations.\n\n(p.1, `Abstract`) `we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions.`\n- SIRL tries to solve the inherent issue of IRL problem, not **generalize** IRL. Also, since Bayesian IRL also recovers the reward distribution, I couldn’t get the major advantage of the SIRL from this statement.\n\n(p.1, `Abstract`) `The solution is succinct, robust, and transferable`\n- Definitions of these expressions seem ambiguous to me. \n\n(p.1, `Abstract`) `a global viewpoint`\n- Again, ambiguous. \n\n(p.1, `Introduction`) \n- It would be better to write it in a more abstract way and separately write down the `Related Work` section.\n- References should be much clearer: LaTeX commands like `\\citet{}` and `\\citep{}` should both be used. \n\n(p.1, `Introduction`) `if the model dynamics are known`\n- Recent works on IRL such as adversarial IRL (Fu et al, 2017) didn’t require the knowledge of model dynamics.\n\n(p.1, `Introduction`) `The recovered reward function provides a succinct, robust, and transferable definition of the learning task`\n- `succinct, robust, and transferable`: Ambiguous\n\n(p.1, `Introduction`) First paragraph\n- Lots of words from `Abstract` seem to be repeated. \n\n(p.1, `Introduction`) `In a real-world scenario, experts always act sub-optimally or inconsistently, which is another challenge.`\n- The sentence seems abrupt. The terms like `sub-optimal` and `inconsistent` here are awkward. \n\n(p.1, `Introduction`) `imposes regular structures of reward functions in a combination of hand-selected features`\n- GAIL (Ho et al, 2016) doesn’t require a hand-crafted feature. \n\n(p.1, `Introduction`) `hand-selected by experts`\n- A word `experts` here seems to imply a reward designer, not an expert on target tasks. I’d rather use a different word here. \n\n(p.1, `Introduction`) `based on demonstrations respectively`\n- `respectively` seems inappropriate. \n\n(p.1, `Introduction`) `Influenced by the work of Finn et al. (2016a;b)`\n- How these references affected AIRL needs to be mentioned. \n\n(p.2, `Introduction`) `because the MaxEnt approach is equipped with the \"transferable\" regular structures over reward functions.`\n- In Ziebart et al., 2008, transferability wasn’t mentioned. \n- I believe the statement -- MaxEnt itself gives transferable reward feature -- is wrong but you should share the correct reference if this is true. \n\n(p.2, `Introduction`) `The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem`\n- This explanation seems insufficient to understand the meanings of “succinctness” and “robustness”. \n\n(p.2, `Introduction`) `Benefits of the class of the MaxEnt method,`\n- Thanks to the benefits of the class of the MaxEnt method?\n\n(p.2, `Introduction`) `Since of the intractable integration in our formulation,`\n- Due to the intractable integral in our formulation? \n- I think the intractability of the mathematical derivation didn’t need to be mentioned in `Introduction`. \n\n(p.2, `Introduction`) `in a model-based environment`\n- when model dynamics is known? \n\n(p.2, `Introduction`) `In general, the solutions to the IRL problem are not always best-fitting in the previous approaches because a highly nonlinear inverse problem with the limited information is very likely to get trapped in a secondary maximum in the recovery.`\n- I couldn’t understand what the authors wanted to emphasize. \n- It seems like they intended to emphasize the problem of local optima, but I don’t know if such a problem is exactly what’s happening in IRL. \n\n(p.2, `Introduction`) `global exhaustive search`\n- What does `global` imply? Knowledge of dynamics?\n\n(p.2, `Introduction`) `theoretically convergent demonstrated by pieces of literature`\n- is theoretically convergent?\n- How the theorem in the references (Caffo et al., 2005, Chan and Ledolter, 1995)  is applicable to the proposed idea should be much clearer since this is one main advantage that the authors argue. For example, what kind of assumptions are required to acquire global optimality? What is the algorithmic assumption of MC-EM for optimality? How are those assumptions linked with IRL setting?\n\n(p.2, `Introduction`) `is also quickly convergent`\n- converges quickly?\n- How can we guarantee the convergence speed? Empirically or theoretically?\n\n(p.2, `Introduction`) `the preset simple geometric configuration over weight space in which we approximate it with a Gaussian Mixture Model (GMM)`\n- preset -> predefined?\n- approximate it -> approximate\n\n(p.2, `Introduction`) `We generalize the IRL problem`\n- It seems the objective is not a generalization. \n\n(p.2, `Preliminary`) $\\mathcal{T}:=\\mathbb{P}(s_{t+1}=s’|s_t=s, a_t=a)$\n- $\\mathcal{T}(s’|s, a):=\\mathbb{P}(s_{t+1}=s’|s_t=s, a_t=a)$\n\n(p.2, `Preliminary`)  `a sequential of state-action pairs`\n- a sequence of state-action pairs?\n\n(p.2, `Preliminary`) `The estimated complete MDP yields an optimal policy that acts as closely as the expert demonstrations.`\n- The discount factor should be considered as well. \n\n(p.3, `Regular Structure of Reward Functions`) $\\mathcal{N}$\n- I’d rather use a different letter since $\\mathcal{N}$ is used to indicate Gaussian distribution in Section `Second Stage`. \n\n(p.3, `Regular Structure of Reward Functions`) $\\{\\phi_i(s, a)\\}_{i=1}$\n- $\\{\\phi_i(s, a)\\}_{i=1}^M$?\n\n(p.3, `Problem Statement`) $\\mathrm{MDP}\\backslash R:=(\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\gamma)$\n- The definition doesn’t match with one without the discount factor $\\gamma$ in `Preliminary`.\n\n(p.3, `Problem Statement`) $\\{\\phi_i(s)\\}_{i=1}^M$\n- $\\{\\phi_i(s, a)\\}_{i=1}^M$?\n\n(p.3, `Problem Statement`) weights $\\mathcal{W}$\n- The definition should be provided. \n- Either $\\mathcal{W}=(\\alpha_1, …, \\alpha_M)$ (for linear model) or the weights of neural network (for non-linear model)?\n\n(p.3, `Problem Statement`) `more likely generates weights to compose reward functions as the ones derived from expert demonstrations`\n- Is this only a special case of Bayesian IRL?\n\n(p.3, `Problem Statement`) `Suppose a representative trajectory class ~`\n- The explanation should be clarified. In my understanding, $\\mathcal{C}_\\epsilon^E$ is a class of sets of trajectories.\n- Why do we need to care such a class with $\\epslion$ threshold?\n\n(p.3, `Problem Statement`) Integrate out unobserved weights $\\mathcal{W}$\n- What does *unobserved* weights mean?\n- Integrate out -> Marginalizing out?\n\n(p.3, `Problem Statement`) trajectory element set $\\mathcal{O}$ assumes to be uniformly distributed for the sake of simplicity in this study\n- I don’t fully understand what’s the advantage of considering a representative trajectory class and why it is required. \n- The section `Note:` tries to explain it, but more explanation or theorems seems to be needed. How can we theoretically guarantee that using a representative trajectory class doesn’t affect our estimation? It seems to me that we cannot guarantee the optimality with this class is the same as the original optimality. \n\n (p.3, `Problem Statement`) $f_{\\mathcal{M}}$\n- How this quantity is related to reward weights is unclear to me. The relationship between weights and $f_\\mathcal{M}$ for both linear and non-linear models should be specified.\n\n(p.3, `Note:) \n- Instead of using a separate section, I’d rather put these statements in the middle of `Problem Statement` for a clearer explanation. \n\n(p.4, `Two-stage Hierarchical Method`)\n- Why do we need to use two-stage method instead of single-stage method (joint optimization over $\\Theta_1$ and $\\Theta_2$)? The advantage of two-stage methods should be briefly mentioned when it first appears for readability. \n- How does the iterative update rule (4), (5) guarantee the optimization of (3)? It’s unclear to me due to the expectation in (4) and (5). My guess is that direct optimization of RHS of (3) is not possible and (4) and (5) might be either lower or upper bound of (3) due to Jensen’s inequality. \n\n(p.4, `Initialization`) `~in each learning task`\n- Do we care about multi-task learning or multiple reward weights only? I believe the latter case.\n\n(p.6, `Experiments`) `since almost only objectworld provides a tool that allows analysis and display the evolution procedure of the SIRL problem in a 2D heat map, we skip the typical invisible physics-based control tasks for the evaluation of our approach, i.e. cartpole Barto et al. (1983), mountain car Moore (1990), MuJoCo Todorov et al. (2012), and etc.`\n- I think this makes the contribution weaker. At least a few classic control tasks should have been considered. One way of evaluating the quality of rewards is retraining the agent with acquired reward, which is already widely used in the literature. \n\n(p.6, `Objectworld`) \n- One figure for illustration will enhance readability. \n\n(p.7, `Evaluation Procedure and Analysis`) `DSIRL`\n- DSIRL abbreviates Deep SIRL but wasn’t mentioned. \n\n(p.7, `Recovery Experiments`)\n- How many runs were used? How’s the mean and confidence interval of the empirical result? \n\n(p.8, `Robustness Experiments`)\n- I couldn’t understand how the robustness of reward is related to the proposed experiments. How the robustness is defined and its relation to the experiment should be clarified.  \n\n(p.8, `Hyperparameter Experiments`)\n- How is the range of hyperparameter search for all methods? Currently, only the results for SIRL and DSIRL are given. \n\n(p.8, `Conclusion`)\n- It seems like both succinctness and transferability were not discussed in the main part of the submission. \n\n### References\n- Levine et al., 2011, “Nonlinear inverse reinforcement learning with gaussian processes“\n- Fu et al., 2017, “Learning robust rewards with adversarial inverse reinforcement learning”\n- Ho et al., 2016, “Generative adversarial imitation learning”\n- Ziebart et al., 2008, “Maximum Entropy Inverse Reinforcement Learning”\n- Caffo et al., 2005, “Ascent-based Monte Carlo expectation-maximization”\n- Chan and Ledolter, 1995, “Monte Carlo em estimation for time series models involving counts“\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Confusing",
            "review": "Summary\n--------\nThe paper describes a method for inverse reinforcement learning---called stochastic IRL---that learns a distribution over reward functions. In that sense, the method is similar to Bayesian approaches, however, the learned distribution doesn't seem to approximate the posterior for any given prior. It is hard to summarize the algorithm because it is highly unclear what the method actually does, but I will try my best by stating my most likely hypothesis.\nThe algorithm starts with an initial set of $N_0$ parameter vectors, each corresponding to the weights for a linear reward function. Each of these weights are improved by using a fixed number of gradient steps using MaxEnt-IRL (Ziebart et al. 2010), where different (overlapping) subsets of the demonstrations are used for the different weights. Subsequently, a GMM over weights is fitted to the $N_0$ improved weights using maximum likelihood (EM). This procedure is iterated, where the weights at each iteration are drawn from the current GMM. The number of weight vectors is doubled at every iteration t, i.e., $N_{t} = 2 N_{t-1}$.\nI'm not at all confident that I got this right. For example, I solely inferred the use of MaxEnt-IRL from a sentence in the introduction (\"In this paper, under the framework of the MaxEnt approach, [...]\").\nThe algorithm is evaluated on a 10x10 gridworld and compared with MaxEnt-IRL and DeepMaxEnt-IRL. The experiments compare two versions of the algorithm called SIRL and DSIRL. The paper doesn't explain how they differ, but a neural network parameterization is mentioned for DSIRL, so I guess D stands for deep and uses DeepMaxEnt-IRL instead of MaxEnt-IRL.\n\nStrong points\n-------------\nLearning a distribution over reward functions is a laudable goal for inverse reinforcement learning as it can be a principled way to deal with the uncertainty in modeling the expert.\n\n\nConcerns\n---------\n__Clarity__\n The paper is really hard to follow, even though I'm very familiar with related work, and the algorithm and the equations seem to be quite simple. There are several reasons for this lack of clarity.\n1) Lack of details. Actually not only details, some of the most fundamental aspects of the algorithm are not mentioned anywhere. For example, the likelihood of the demonstrations given the weights, $g(\\mathcal{O}|\\mathcal{W},\\Omega)$ is nowhere defined. $\\theta$ is also not defined. \n2) Some of the equations are very confusing. For example, the second to last equations at page 4 seems to overload the subscript \"i\" which seems to index both the sampled weight as well as the gradient step, depending on the variable. Even when disentangling these different meanings the equation looks weird to me. Based on the surrounding text, I guess it should mean m gradient steps on the likelihood are performed, but the equation says different. It is also strange that the weights W are updated based on the gradient w.r.t. $\\Omega_1$ and that $\\Omega_1$ is set to the updated weights in the last equation at page 4. It seems like Omega_1 is superfluous. Also, the mixture model $\\mathcal{D}(\\mathcal{W}|\\zeta, \\Omega_2)$ seems to be conditional independent of $\\zeta$ given $\\Omega_2$. What's the difference between $\\mathcal{D}(\\mathcal{W}|\\zeta, \\Omega_2)$ and $h(\\mathcal{W}|\\Omega_2)$?\n3) Some algorithmic choices are not motivated at all (for example, doubling the samples at every iteration), others are only (insufficiently) motivated in hindsight. For example, the use of \"representative trajectory classes\" is motivated after introducing them and based on the example of different drivers with different driving styles. However, the paper never mentioned a multi-expert scenario and also doesn't try to cluster the demonstrations but only randomly removes few demonstrations; thus, it is not even clear how the trajectory classes would tackle such problem. \n4) Bad structure. While it is in general a legitimate approach to introduce the algorithm step by step, I think you should always also provide a rough sketch of the overall algorithm already early on, so that the reader has some context when you introduce the details. The way of introducing the trajectory classes is also a good example for the bad structure of the paper. Before introducing them, there is no mentioning of them, there is no motivation for them, and for all I can tell the paper doesn't even discuss the problem that they are supposed to tackle. And then in Section 2, they are introduced by stating \"Suppose that we have trajectory classes [paraphrased]\" and moving on as if nothing happened. \n5) I must say that the writing is also quite bad. Some sentence are hard to follow due to grammar mistakes, especially if the reader doesn't have enough background to infer their meaning. For example \"[...] suffer from the problem that the true reward shaped the changing environment.\" should probably mean something like \"suffer from the problem that the learned reward function is shaped---that is, it is entangled with the dynamics---and, thus, does not transfer to different environments\". As another example \"However, GAIL is in a lack of an explanation of expert’s behavior and a portable representation for the knowledge transfer which are the merits of the class of the MaxEnt approach, because the MaxEnt approach is equipped with the \"transferable\" regular structures over reward functions.\"\n\n__Soundness__ \nIt doesn't make sense to talk a lot about the soundness of the approach before clarifying what it actually does. However, it seems to me that the learned distribution is only heuristic and does not approximate the posterior for any given prior. Also, for the linear reward function, the maximum likelihood objective is convex, so I think that the different weights should even converge to the same solution. \n\n__Evaluation__\nI don't demand MuJoCo experiments and the like, and in some cases evaluations on gridworlds can be sufficient. But I must say, that the excuse for not performing continuous control problems because the reward functions can not be visualized by 2d-heatmaps lured out a smile when reading the article. If I understand the algorithm correctly, it is significantly more expensive than standard MaxEnt-IRL which already doesn't scale to such problem settings as it requires iteratively solving the reinforcement learning problem. \nThe results on the objectworld are also not convincing. The expected value difference is for 80 demonstrations larger than for 40, and for 160 demonstrations larger than for 80. Similary, the performance initially degrades when increasing trajectory lengths and only improves again when taking trajectories of length 32 or more. \nThe sentence \"A notable point in Figure 3 is that very few expert demonstrations (less than 200) for our approach also yields a small EVDs, which manifests the merit of Monte Carlo mechanism in our approach.\" is highly misleading. 200 trajectiories are not \"few\" for a 10x10 gridworld and an EVD over 20 is not small. When introducing this environment, Levine et al. (2011) achieved an EVD of around 1 based on 8 demonstrations (using 8 steps instead of 5).\nThe robustness experiment seems to subsample the weights based on the EVD (which requires knowledge of the true reward) before evaluation. \n\nQuestions\n--------------\nTo better understand the proposed algorithm, I have the following questions.\n1) How does the algorithm relate to MCEM? Is it some sort of EM within MCEM, where the first stage corresponds to the Monte-Carlo E-Step and the EM in the second stage corresponds to M-step of MCEM?\n2) How are the densities g and h defined?\n3) How does $\\Omega_1$ affect the optimziation. How does it differ from $\\mathcal{W}$?\n4) How exactly are the m-step update steps performed? Do they indeed correspond to m gradient steps on the MaxEnt objective?\n5) Section 3.3.: Should the inequality hold for $\\lim_{t \\to \\infty}$?\n6) What's the difference between DSIRL and SIRL?\n7) How do the algorithm compare in terms of computational cost?\n\nAssessment\n----------\nUnfortunately, I don't see a chance for acceptance here. Even if the algorithm was sound and sufficiently novel, the paper would need to be almost completely rewritten in order to address severe problems of the current presentation.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}