{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces an architecture based on structured causal model for long-tailed IE tasks. It incorporates the dependency tree structure of the sentence using a GCN for learning the representations. The key idea is to use counterfactual reasoning to help with the inference in attempt to reduce the impact of spurious relations. There are some concerns about the presentation of this paper. While the high level idea is reasonably clear and well motivated, the paper is quite messy with the notations and technical details. \nHow to use the causal effect estimation for the final prediction is not explicitly explained except for in Figure 1. \nFor the experiments on ED and NER, it is unclear if they assume the trigger or span is given. The method seems to need the span information to make the prediction. If span is given, this is a different set up that is much simpler compared to traditional ED and NER where the span or trigger needs to be detected as well.\nThere are also some question regarding the difference between this work and the prior work on using causal reasoning for improving prediction (the TDE work). One difference is the additional term in equation 8(updated version), which appears to be useful empirically, but the motivation is rather hand wavy and needs more clarification.\nOverall, there are some useful ideas, but the overall novelty does not particularly stands out, and the presentation of the paper made somewhat straight forward ideas more convoluted than necessary. \n"
    },
    "Reviews": [
        {
            "title": "This paper first proposes an SCM-model based on masked contextualized word representation learning to capture the direct slide effect; Second, counterfactuals based on syntax structure is computed to invervene on the model and an IE classification model are proposed to solve the slide effect and counterfactual rebalancing problem for unbiased IE prediction. ",
            "review": "The novelty of the paper seems to be in application of the counterfactual analysis to address the long-tailed IE issues, which might be interesting to the IE researchers. Overall, more theory about the counterfactual generation for IE task should be added, for this is what the novelty of the paper; also, for the rebalancing learning for slide effect and counterfactual, the theory appears to be not enough. The weak of this work is the theoretical and conceptual underpinnings of the proposed methodology. \n\nHere are my major concerns for  the paper:\nQ1. The main investigation of the paper is the unstructured text; however, I can see no discussion in the methodology part about how author represent unstructured text in SCM model (representation learning). Is it unimportant to mention or the SCM model definition is always the same no matter what sources of the data?\nQ2. Section2.2. Counterfactual Generation, as this forms the main contribution of this work, still no useful information is given in this section, but only a do-calculus is given. Since this remains at the basic concept of causal inference, how can the paper combine this with IE task? \nQ3. Equation (3) u_i appears to be the representation of the i_th position of a sentence, which I think is important notion across the whole paper, however, in the later illustration after equation(3), u_i didn’t appears again even once. Why? I am doubtful about the reproducibility of the paper. \nQ5. How author design the optimization function? How equation (5) (3) been used?\nQ4. As for the experiment, author use MR and MF1 as the evaluation metrics, since this work can measure causal effect. More evaluation regarding the causal effect should be added. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Counterfactuals for Long-Tail IE with help of GNN and dependency tree",
            "review": "#### Update after author response and other reviewers comments:\nI think after the addition of new evaluation (Appendix A.5) on the aspect of counter-factual and other comments made by reviewers I'll stick to my score. Also, I liked the idea of applying counter-factual to long-tailed distribution IE problem.\n\n### Summary\nModel performance suffers because of spurious relations present in the data, this is particularly true for long-tailed scenarios. To overcome this challenge authors introduces counterfactual thinking to IE. To learn the main effect and ignore the spurious relations (side effects) the paper proposes structured causal model (SCM) with syntax structures using GNN on sentence dependency tree. Extensive experiments on multiple tasks with different datasets shows significant performance improvement.\n\n### Strong Points\n 1. Application of counterfactual along with utilization of syntax structures for long-tailed IE is novel and aptly motivating.\n 2. I found the idea of measuring the main effect and a way to ignore the \"side effects\" very interesting.\n 3. Extensive experiments provides ample evidence that reducing spurious relations leads to improved performance.\n\n### Weak Points\n 1. How incorporation of the 3rd component (W_{x}X^∗) in Eqn. 5 measures the effects of the entity is not clear to me. X^* is already taken into account when calculating Y_{x^∗}(S), is it not?\n\n### Other comments\n 1. The phrase \"**replace** the tokens along the shortest path of the two entities of the relations and generate a new sequence S^∗\" was not really clear to me from the paper. Could you please explain this part such as replaced with what?\n 2. What prompted the authors to consider ≤ 4000 as few scenario for \"OntoNotes5.0\" dataset whereas for all other scenarios it's either 30 or 100? I feel **4000** is too large for it to be considered \"Few\" for a class.\n 3. It's curious to find that GloVe based approaches outperformed BERT based approaches by a large margin in ATIS dataset in NER task (Table 1) and MAVEN dataset for ED task. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea with comprehensive experiments",
            "review": "This paper proposes a novel model integrating both causal inference and structure-aware counterfactual training to enhance the long-tail performances of information extraction. The causal mechanism considers a structured causal model that takes into account all possible cause-effect relations for the final predictions, including contexts, target representations, POS tags, NERs, etc. They also implement counterfactual training strategy by selecting the most important factors and wipe off the side effects to enhance the long-tail situations.\n\nThe strengths of the paper includes:\n1. In general, this paper is well-written and easy to follow. The motivation and the structure are clear. \n2. The ideas of both structured causal model and structure-aware counterfactual training are interesting. \n3. Extensive experiments are conducted to demonstrate the effect of the whole model and each component. It is interesting to see how different generation of counterfactual examples using dependency structure affect the final performance.\n\nSome improvements could be made:\n1. If structure is considered, why not try to mask on some dependency relations? It will be interesting to see the difference between masking words and relations.\n2. What is the effect of using (5) instead of (4) in terms of the experimental result? Have you conduct such comparison experiment? And how sensitive is $\\alpha$ to the final performance?\n3. It is also better to demonstrate some qualitative examples on which factors are most important for NER, RE and ED.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper propose a counterfactual approach to improve the performance in information extraction tasks and in particular on the rarer classes. While this is an important research problem, I think the paper has the following issues:\n\n- The overall  approach is best described as a form of a data augmentation based on the model predictions. Steps 1 to 3 is essentially finding out which part of the input had the most impact on the model prediction by comparing the impact of a token being masked at random, and then steps 4 and 5 use this info to improve the prediction. This is quite commonly done in the context of model interpretation, e.g. finding out which words matter the most in a prediction was done in the work on LIME: http://sameersingh.org/files/papers/lime-kdd16.pdf , and by many others that followed this up, e.g. finding out which words are the most important via removal: https://arxiv.org/abs/1804.07781. The latter is actually doing what the first three steps do of the proposed approach. Steps 4 and 5 essentially use this output which presumably captures the contextual information and combine it with the contextual word representation, which is a pretty standard thing to do. However this existing work in NLP has been ignored.\n\n- The causal framing in Figure 1 appears flawed. There is no good reason to assume that the NER tag \"causes\" the trigger representation and not the other way around. However this is a fundamental assumption here. Also, why is not the contextual word representation \"causing\" the NER tag? Furthermore, there is no causality inferred here; the model output is post-processed, and the causality is not assessed in any way.\n\n- Few-shot learning has been investigated in the context of information extraction. Here are some papers: https://www.aclweb.org/anthology/D19-1649/ https://www.aclweb.org/anthology/P19-1589/ . \n\n- The paper is hard to follow. What is v_r in equation 3? In section 2.2, it should be explicit that the tokens are not replaced, but removed. Note that replacement with semantically equivalent words has been explored: http://sameersingh.org/files/papers/sears-acl18.pdf\n\n- There are some vague statements about the novelty vs previous work by Tang et al, but no explicit statement made in the model description. Eventually it seems to be about considering the syntax (page 7), but this is only used to just select which words to remove as far as I can tell and the GNN over dependency trees which is based on previous work.\n\n- While it is stated that the results of previous work are reproduced, then it is stated that they were not really. While this is not necessarily the authors' fault (reproducibility can be difficult sometimes), combined with the uncommon choice of metrics and the lack of any comparison to previously reported results in the literature, means that it is impossible to understand where the presented method stands. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}