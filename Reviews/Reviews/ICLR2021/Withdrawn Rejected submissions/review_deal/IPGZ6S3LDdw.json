{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a compelling mechanism for reducing the neural architecture search process based on accumulated experience  that the reviewers found compelling with significant improvements in performance.  This is an intriguing idea. However, there were concerns about clarity that need to be addressed, and more concerning, the paper lacked technical depth or details in several aspects described in the reviews.  The authors subsequent response and revisions have somewhat addressed these issues.\n\nThe reviewer discussion had mixed opinions, with some for weak acceptance and others for weak rejection.  There were compelling points that the contribution is significant, but overall this paper would benefit from thoroughly addressing the shortcomings mentioned in the reviews before it is ready for publication. "
    },
    "Reviews": [
        {
            "title": "Practical approach based on real observation but less technical depth",
            "review": "Summary\n-------\n\nThis paper proposes a fast general framework (FNAS) for neural architecture search (NAS) problem to enhance the processing efficiency up to 10x times. Three interesting strategies (UAC, LKP, AEB) for reinforcement learning (RL) processing are introduced in the proposed FNAS and evaluated by extensive experiments to show their efficacy. In particular, the assumption that architecture knowledge is transferable has been verified by real observation.\n\nHowever, the authors paid more attention to introduce the fact based on observations and the thoughts of the framework design, thus neglected the technical depth for the key component (UAC) that has highest impact on the overall performance.\n   \n\nStrengths\n---------\n\n- The paper is well-organized and well-written thus easy to understand, including motivation, approach, and experiments.\n\n- The proposed framework (FNAS) is general, practical and convincing. The three strategies (UAC, LKP, AEB) for RL processing are based on real observations shown in Figure 4, which positively supports the motivation of the approach.\n\n- The evaluations are conducted on extensive experiments with solid results including ablation studies for the three different components LKP, UAC, and AEB (although it might be not sufficient; see next the weaknesses).\n\n\nWeaknesses\n----------\n\n- The technical depth was neglected. For example, the  Uncertainty-Aware Critic (UAC) should be considered as the key component of FNAS framework because it was shown that the UAC has highest impact on (biggest contribution to) the overall performance in terms of efficiency in Table 4. However, there is no any technical/mathematical introductions about how the uncertainty network $U$ is obtained/prepared, and how the $U$ contribute to the NAS in detail.\n\n- Discussions regarding possible over-fitting are not sufficient. For example, the constraint (threshold $\\delta$) of uncertainty is introduced into the UAC strategy (Sec. 4.1), but without any experiment results to show the impact of such hyperparameters in the FNAS framework that has to be considered as trade-off parameter against over-fitting effects.\n\n- Similarly, in the AEB strategy (Sec. 5), the buffer size ($N$), and the annealing term ($\\beta$) are hyperparameters that should have impact on the over-fitting effects during RL processing. However, the authors did not provide any testing results to confirm their impacts. For example, why did the authors determine the buffer size $N=10$ in the experiments?\n\n- The testing of FNAS on vision tasks are not sufficient. This paper provided results on classification (ImageNet) and face recognition tasks, but how about other tasks such as object detection, tracking, person re-identification, and segmentation?\n\n\nOther Questionable Points\n-------------------------\n\n- The loss functions shown in Eq. (1) and Eq. (2) seem too simple. Are they really sufficient to get high performance? Is there any potential loss functions or improvements that would get better performance?\n\n- In Table 1, why the numbers of \"GPU Hours\" for MBv3 and EfficientNetB0 are not shown? It is inconsistent with textual description in Sec. 6.2 (\"there is nearly 10x of acceleration ...\") that is not able to confirm.\n\n- In Table 1 and 2, regarding the numbers of \"GPU Hours\" like 20,000 and 2,000, do they indicate the real runtime in the experiments or only estimated values? As we know, 20,000 hours are roughly equal to 2.3 years, and 2,000 hours are similarly equal to 2.8 months. The reliability of the experiment results might be doubting.\n\n- In the references, there are too many informal publications cited from arXiv. Instead, they should be replaced by their formal publications at the corresponding conferences or journals.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "Summary: The paper propose a few improvements to the sampling-based NAS using RL: 1) an uncertainty-aware critic to decide whether the sample needs to be trained; 2) a life-long knowledge pool to initialize the sample that needs training; and 3) an architecture experience buffer to reuse old samples for RL training. The experiments are done on ImageNet, facial recognition and transferability on object detection. The proposed methods are compared with related works. Finally the paper finishes with ablation studies on both the effectiveness and transferability of the proposed modules. \n\nStrengths:\n- The paper is well-written with clear flow and structures.\n- The three proposed modules are novel and improve the search cost significantly while achieving better performance. \n- It's great to see the authors has done a comprehensive comparison with the related methods for multiple tasks. The ablation study also demonstrate the effectiveness of the three proposed modules. \n\nWeakness:\n- The improvement of Top1 Acc. on ImageNet is marginal (without the 1.3 scale up) and worse than some of the recently proposed differentiable NAS work which requires far less search cost (comparable to DARTS).\n- There could be more discussion on related work studying uncertainty in RL or in supervised learning, given that it is one of the core modules in the proposed pipeline and uncertainty an important topic in general.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting topic, lots of ideas, shallow integration of ideas, and unconvincing results",
            "review": "In this paper, the authors propose to use a sampling-based approach to neural architecture search, which combines a life-long knowledge pool, uncertainty aware critic, architecture experience buffer. This approach has been demonstrated with vision tasks involving days of TPU training. \n\nOverall, I rank 5, marginally below the acceptance threshold. NAS is an underexplored topic. But the papers seems like an engineering project that combines multiple existing ideas from the others' work, and there lacks theoretical depth about clear mathematical formulation of the approach, and reasoning on why the approach should work.\n\nPros:\n\n+ Having access to TPU\n+ the NAS topic\n+ Integration of lifelong learning, NAS, and several other ideas\n\nCons:\n\n- Why lifelong learning should work, considering that we are not in multi-task learning and nonstationary environment scenario? How do we know that the experimental results is not some coincidence? \n- Can we put down the whole framework mathematically? It seems that this paper has only two formulas about some loss functions.\n- Can we reason about the math? For example, any ideas to better organize knowledge pool and ideas architecture experience buffer for a large number of architectures and parameters encountered?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A decent work",
            "review": "This paper proposes an RL-based neural architecture search approach to decrease the searching cost by introducing three modules to estimate uncertainty, restore parameters, and store old models. Compared to MNAS, it can significantly reduce the search cost up to x10, while giving competitive accuracy. \n\nThis paper is generally well-written and well-motivated, except for some unclear sentences;\n-      Architecture knowledge is not well described. Compared to parameter knowledge, the authors should clarify what they are and the difference between them. \n-      In Figure 4, it is unclear what the operators are and which operators are similar and different. Moreover, details are missing on how to sample 100 optimal models. \n-      In Equation 1, the definition of the reward is missing.\n-      LKP (the acronym first introduced in page 5) is not described.\n\nEven if it accelerates the search process, it entails additional memory due to the proposed module; it stores learned networks. So, I think thereâ€™s a trade-off between search cost and the total memory we need to reserve. From this, I wonder reducing the search cost is more significant compared to increase the required memory.\n\nIn Table 3, why FNAS has higher FLOPs than MNAS? This should be properly elaborated.\n\nIn Table 4, the cases using two modules are missing. It would be great to see the results to see which component actually affects the performance.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}