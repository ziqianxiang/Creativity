{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper is concerned with learning transformation equivariant node representation of graph data in an unsupervised setting. The paper extends prior work in this topic by focusing on equivariance under topology transformations (adding/removing edges) and considering an information theoretic perspective. Reviewers highlighted the promising ideas of the approach, its relevance for the ICLR community, and the promising experimental results (although improvements over prior work are not necessarily significant on all benchmarks).\n\nHowever, reviewers raised concerns regarding the novelty of the method and the clarity of presentation with respect to key parts of the method. These aspects connect also to further concerns raised, e.g., related to mathematical correctness as well as the significance of the proposed loss function, the benefits of motivating it from MI, and the improvements over GraphTER. The rebuttal didn't fully clarify these points. While the paper is mostly solid, I agree with the reviewers' concerns and -- currently -- the paper doesn't clear the bar for acceptance; it would require another revision to improve upon these points. However, I'd encourage the authors to revise and resubmit their work with considering this feedback."
    },
    "Reviews": [
        {
            "title": "Interesting idea & theoretical questions",
            "review": "#### Goal\n\n- This work considers the graph task of learning node representations that are invariant to small edge perturbations. It achieves this through a data augmentation procedure that samples new “fake” edges and regularizes the GNN equivariant representations to be unable to predict these fake edges. Overall it is an interesting idea.\n\n#### Quality\n\nThe paper is reasonably well written, except for the introduction, which throws too much jargon around without being concrete about the work’s goals. \n\nMathematically, I have doubts about the correctness of the work (see Cons)\n\n\n#### Clarity\n\nIt is a little confusing how we can obtain the fake/deleted edge probabilities p. Earlier in the paragraph there is a connection with GAT but GAT has an attention mechanism to obtain \\p_{ij}. While the first paragraph of 3.2 states the fake “edges” are given by a random matrix \\Sigma obtained from a Bernoulli distribution with parameter p. I am not sure what that means. The elements of \\Sigma are i.i.d. samples from a Bernoulli distribution? I am not familiar with Bernoulli distributions that sample entire random matrices.\n\n\nThe graph is defined with an edge set, \\mathcal{E}, and an adjacency matrix A. It is unclear why we need these structures to define a single graph. Isn’t A fully determined by \\mathcal{E}? If not, why not? It would be good to clarify.\n\n- The transformation $t$ and its homomorphism \\rho are given in Def 1 but not given a formal definition later in the paper. In the proposed algorithm in Section 3.4, what are t and \\rho?\n \n#### Originality\n\nThere is some level of originality, although there are too many papers in GNNs doing similar things to know for sure. It claims to be an edge extension of a previous work for nodes.\n\n\n#### Significance\n\nOverall, the work does not really bring much to the table. Robustness to a certain type of edge noise could be interesting, but the choice of noise process is probably very important. The noise process is not discussed in depth. Data augmentation (just adding the new perturbed graph as a data example) is the main competitor to this approach. The difference between what the authors did and data augmentation should be front and center at the experimental evaluation.\n\n\n#### Pros\n\n- The paper central idea applies invariant representations to make the GNN representation more robust to noise\n\n#### Cons\n\n- Predicting edges with equivariant node representations is provably impossible (Srinivasan & Ribeiro, 2020). The Representation Encoder looks like an equivariant node representation to me. Hence, the “Transformation Decoder” is attempting a provably impossible task. Remark 1 in (Srinivasan & Ribeiro, 2020) gives an interesting comment to why GraphSAGE is successful even if it was trying the same impossible task. The authors need to give a solid theoretical reason for their approach, otherwise it is not believable.\n\nThe main competitor of this approach is data augmentation. GraphSAGE has an unsupervised version that also adds fake edges at random as a form of data augmentation. \n\nSrinivasan, Balasubramaniam, and Bruno Ribeiro. \"On the Equivalence between Positional Node Embeddings and Structural Graph Representations.\" In International Conference on Learning Representations. 2020.\n\n\n#### Other comments\n\n- In Franceschi et al., 2019,  the authors also want to learn the underlying graph (which edges are fake, which are true). It seems that their approach could be applied in the framework of this work, and would maybe give a better way to sample the fake edges. \n\nFranceschi, Luca, Mathias Niepert, Massimiliano Pontil, and Xiao He. \"Learning discrete structures for graph neural networks.\" ICML (2019).\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An incremental advance on unsupervised learning of node representations of graph data",
            "review": "This paper presents the Topology Transformation Equivariant Representation (TopoTER) as a learning method for the (uncommonly) unsupervised learning of node representations  in graphs, with applicability to Graph Convolutional Neural Networks.\nThe paper falls well within the scope of the conference.\nIts writing could be improved and would benefit from a thorough language revision.\nThe contribution is well grounded, the state-of-the-art is appropriately identified and even though its novelty is somehow incremental, its value seems beyond doubt.\nTo the best of my knowledge, and not being an expert on graph data analysis, the maths seem sound.\nThe experimental work, including comparison with alternative approaches is quite detailed (and well-informed of the competition).\nAdding extra experiments as an appendix ... I guess it verges on cheekiness, but given, the particularities of accepted formats, assume it is fine.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A method for for self-training GNN",
            "review": "The paper propose an unsupervised method for self-training of graph-neural-networks (GNNs). The authors provide information-theoretic justification to their method using maximization of the lower bound of the mutual information on their objective. Their approach is based on maximizing the mutual information between a perturbed graph topology and its node representation.\n\nStrong points:\n- very good results (some of the results are comparable to supervised method and the improvement achieved on the other unsupervised methods is significant)\n- simple approach with theoretical justification\n- paper is nicely written and easy to follow",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work, similar to GraphTER",
            "review": "This paper develops a framework for unsupervised learning of graphs. The goal is to build graph representation using an encoder that is useful for downstream tasks such as graph classification. The representation is computed with an encoder $E$ applied to a graph data $(X,A)$, containing vertex data $X$ and adjacency matrix $A$. Given a graph $(X,A)$ and a perturbed version of its adjacency matrix $(X,\\tilde{A)}$, the decoder $D$ is tasked with minimizing the conditional entropy, $h(\\Delta A \\vert H,\\tilde{H})$, of the perturbation $\\Delta A = A-\\tilde{A}$ when given the two representations $H=E(X,A)$ and $\\tilde{H}=E(X,\\tilde{A})$. \n\nThis feels as a solid paper, however it is very similar to GraphTER (Gao et al. 2020), which as far as I can tell is a previous work. Let me detail the similarities. The GraphTER paper already defines the same encoder-decoder structure and training the decoder to infer the perturbation in the graph. The GraphTER  considers perturbation of node data $X$, but also perturbation of adjacency matrix since they define the adjacency matrix as a function of the node data, that is $\\tilde{A}=f(\\tilde{X})$. The current paper focuses only on adjacency perturbations. A second difference is that GraphTER uses the decoder to predict the perturbation directly, while the current paper asks the conditional entropy of the perturbation given the representations of the graph or its perturbation to be as small as possible (or the mutual information formulation). I don't see the benefit in the new loss formulation - if there is such a benefit it should be highlighted and demonstrated. Anyway, these two differences between the papers seem rather minor. \n\nOther comments/questions:\n\n-- What can be said about the equivariance of the learned encoder? Is there a way to quantify how equivariant is it? Why, if the loss that is optimized is equation 6, one would even expect equivariance? I mean, low entropy could maybe imply some unknown transformation of the perturbation. This should be explained. \n\n-- Why not formulate the loss directly as in equation 6 (conditional entropy of the perturbation) if this is in practice what is optimized? What is the benefit in going through the mutual information formulation?\n\n-- In the experiments part: how is the linear classifier in section 4.1 trained (after fixing the encoder)? Same question about the SVM in section 4.2? If indeed these classifiers are trained with labeled data - why is this method is called unsupervised? I am confused by that part. For example, in Table 1 it is emphasized that TopoTER does not use labels $Y$.\n\n-- Is the encoder only one layer of GNN as in equation 11? Did you try with deeper architectures? \n\nUPDATE: I would like to thank the authors for their rebuttal. I have read it, however unfortunately, I am not convinced the indicated differences from previous work is sufficient to warrant publication at ICLR. I am also not completely clear about the equivariance point. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}