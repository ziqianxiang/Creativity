{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind review #4",
            "review": "Summary:\nThe paper proposes a new differential Neural Architecture Search algorithm which uses one-level optimization, simultaneously optimizing differential architecture parameters and shared weights. Most NAS methods utilize bi-level optimization, usually alternatively optimizing shared weights and architecture parameters. In addition, they relax the cell space to allow an arbitrary number of connections instead of fixing the connections to 2 inputs per node and allow all the cells to have different architectures. By not limiting the number of connections, they are also able to reduce the disconnect that occurs in DARTS and other work when connections are pruned. Instead they gradually prune the connections during training that have very low weights so that removing them causes very limited impact on training error of the supernet use modulating resource penalties in the loss to push the weights lower so they can be pruned. This modulating resource penalty allows them to in one search to generate a pareto optimal frontier of architectures as more operations are pruned. They demonstrate the algorithm by finding high performing architectures for CIFAR-10 across the pareto-optimal frontier for flops vs accuracy. The robustness of the architectures is demonstrated by transferring them to Imagenet.\n \n \nReasons for score: \nOverall, I would weakly recommend accepting this paper. The algorithm and concept is appealing and has strengths in simplicity, removing artificial biases, and efficiency. A single search is able to produce a pareto-optimal frontier. However, the work is confusing in it's handling of resource constraints. It would benefit from more careful presentation with respect to optimizing flops or parameter size and would greatly benefit from analyzing the Imagenet networks with respect to flops. It's not entirely clear how optimal networks the algorithm finds are. The majority of the results are in a search space with only two operations.\n \n \nPros:\n1. The proposed algorithm is able to generate a high-performing pareto-optimal frontier of architectures in a single search. The architectures also perform well when transferred to Imagenet demonstrating the robustness.\n2. The one-level NAS algorithm is conceptually simple and easier to implement compared to existing bi-level algorithms which is appealing. They demonstrate the capability of the one-level NAS algorithm when combined with augmentation.\n \nCons:\n1. Introduces additional complexity in the form of the dynamic schedule which controls the weighting of the resource penalty in the loss which is used to control the pace of the pruning schedule. The paper would benefit greatly from more analysis of the schedule and ablation of the controlling hyperparameters.\n2. The search space in which most of the results are demonstrated is quite limited to only seperable convolutions and skip connections. It's unclear how much this is biasing the search and if the algorithm worse with more allowed operations. This is discussed a little in the appendix, but merits more analysis into the robustness of the search algorithm.\n3. It is also quite confusing that the algorithm is optimizing for FLOPS, but figure 1 shows the pareto-optimal frontier for parameters and the Imagenet table does not list FLOPS at all.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "[Recommendation to Borderline] Carefully Designed Techniques to Improve DARTS",
            "review": "This paper proposed a method to progressively prune a  big supernet to a smaller one, and in this procedure, a collection of architectures with different FLOPs can be obtained. Overall, speaking the whole algorithms is somewhat sophisticated and the final performance is relatively lower SoTA.\n\nPros:\n- Using the proposed method, we can get multiple architectures with different FLOPs in a single search.\n- Competitive performance compared to cell-based NAS methods.\n- Good visualization for the discovered architectures. (May I ask what tool is used for visualization?)\n- There are many carefully designed components over the whole search pipeline, which could potentially be used for \n\nCons:\n- The presentation should be improved. The current presentation is quite bottom-up, it takes me one hour to go through the paper and get the whole points of the proposed method. If the authors could use a figure to explain the overall algorithm at some early points of the paper, it could help the reader to understand the proposed method easier. In addition, the paper is not self-contained. Some necessary information is not included in the main paper, such as what is the formal formulation of the loss functioiin?  Could the authors explain \"the expected and uniform versions of FLOP\" with more details in the main text. Could the authors introduce more details about the regularization in Sec.2.4.\n- The proposed method is a sophisticated framework, including multiple components (1) enlarge search space, (2) one-level search, (3) regularization to solve discretization error, (4) gradual pruning strategy.  These components (in four subsectionos) are similar to existing methods, for example, the one-level search is proposed in previous works, enlarge search space have also been done in many previous works. It is hard for me to catch up the core contributatioin of the proposed method.\n- The authors mainly compare with DARTS in INTRO, which limits the scope of the proposed method. There are many follow-uup works of DARTS, I would suggest the authors consider the general differentiable method as an example in INTRO. -- such that the introduced \"major drawbacks\" may no longer exists? Consequently, the authors may need to rethink the unique part of the proposed method.\n- Missing comparison:\n---- One advantage of the proposed method is obtaining a collection of architectures. It should be compared with similar approach -- \"Once for all\" in experiments.\n---- In Table 1, the authors use the search space S0, which is not the same as the compared methods. For a fair comparison, I would suggest the authors also include the results of using the same search space.\n---- Another benefits of GOLD-NAS is \"faster\" as claimed in the paper. If so, I would also suggest the authors compare with some NAS methods that targets on accelerations, such as Searching for A Robust Neural Architecture in Four GPU Hours; Can weight sharing outperform random architecture search? An investigation with TuNAS.\n---- The current SoTA models for mobile setting is above 77%, which is higher than the discovered architecture by 1%. Could the authors explain the reasons?\n- The authors claimed that existing NAS methods have a lot of heuristic rules, and they try to avoid it. While from my point of view, the introduce S0 search space only havinig skip-connection and sep-conv is a strong prior, and the introduced hyperparameters \\mu \\eta \\t \\n0, etc are another kind of heuristic rules. How do you think?\n\nMinor comments:\n- From my personal opinion, Figure 2 and Figure 3 do not provide much useful information and can be moved to Appendix.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Many claims in this paper are not properly justified.",
            "review": "\n\n# summary\n\nThe paper proposes to relax the limitation of the existing DARTS search space that all layers share the same architecture type and result in a super-large search space. It then proposes a new training pipeline that gradually eliminates the operation through a regularization approach. Some of the searched architecture are trained from scratch on CIFAR-10 and image-net.\n\n# Strength\nThe discretization error is a good catch, but it might be more convincing with corresponding experimental evidence. \nThe pruning idea in DARTS algorithms is novel and seems a reasonable idea\n\n# Weakness\n\n## Unclear contribution\nIs this paper about a new search space or a new DARTS method? If it is a new DARTS method, it should be at least evaluated over the same search space with the previous baselines, as done in the PC-DARTS paper. If it is about a new search space, the technical contribution is marginal. If this is a work showing the new method only works over the new search space, we should see the discussion of why it does not work on the smaller (maybe easier) search spaces. \n\n\n## Lacking the ablation experiments to demonstrate why the proposed algorithms work.\n\nNAS algorithms are heavily criticized for their reproducibility and stability issues. (Li and Talwalker 2019, Yang et al. 2020 NAS evaluation is frustratingly hard, Yu et al. 2020 Evaluating the search phase of architecture search.) Up to now, it is hard to believe a NAS method with only evaluating some point estimates on specific datasets but do not decipher how the search progresses and why this is better than the baselines. In addition, since the authors also mention that `random search can achieve satisfying performance', it is best to see GOLD-NAS comparing with RandomNAS on the new search spaces. \n\n\nMany claims are without concrete evidence\n\n- Why a larger search space is necessarily better? Although it seems intuitive, it is very difficult to have concrete proof of this. In addition, Radosavovic et al. 2019 (On network design spaces for visual recognition) show that the DARTS space is actually better compared to the larger NAS-Net one. Would this be a counterexample for this claim?  \n\n- This discretization error is discussed without any empirical evidence. It would be better to see the author compose some ablation on this topic, showing the proposed regularization can improve this result.\n\n- On page 5, blue text, `we emphasize that ... gradual pruning strategy that alleviates the discretization error ...`. Any proof? \n\n- On page 6, bold text, `GOLD-NAS is faster, easier to implement, and more stable...' Any proof of that? Especially about the stable part. We at least see a comparison with random baselines in DARTS original papers. If the authors would like to make this claim, there should be empirical evidence. Table 1 only shows GOLD-NAS in various settings without comparison with other methods on the same search space. BTW, the table 4 in the PC-Darts paper shows a stability test over five runs, and please do the same test under the original DARTS search space to have a fair comparison. \n\n- `In the new search space, most existing ...methods can fail dramatically'. I did not see that other baselines are applied to the new search space at all.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A clear and interesting method but needs improvement",
            "review": "\n+ A large search space of size 1e117 is designed by this paper, which is the paper's main contribution. (+)\n\n\n- The authors claim that DARTS can risk a large discretization error, especially when the pruned operators' weights are not guaranteed to be small, and the proposed method in this paper can reduce this risk. Specifically, this paper uses the power of regularization to suppress some operators' weight so that they can be pruned. However, for me, this claimed contribution is neither new nor novel. Reducing the gap between the search and re-training stage has been studied, e.g., in ProxylessNas and SNAS. Moreover, recent works in the NAS community have employed single-path one-shot methods to avoid the risk of discretization error. (-)\n\n\n- Although the proposed search space is large, the architecture searched in this search space is not good (compared to other search space like EfficientNet-like ones on ImageNet). As we know, NAS is to find an excellent architecture that is better than human design. It is not promising if the authors' search space does not contain good architectures that are better than human designed ones. In my option, the proposed search space is just a toy. Maybe the authors may argue that their searched architecture obtains good performance in CIFAR-10. However, good results on ImageNet is more desirable because CIFAR-10 is sometimes considered as a toy dataset by the community. (-)\n\n- This paper's improvement over previous works is incremental. For example, it merely changes the softmax function in DARTS to a sigmoid function. (-)\n\n\n- Only four operations are used as the candidate operation. As we know, in existing NAS methods, there are some observations that the combination of different operations plays a more critical role in the performance than the topology network connectivity. For me, I don't think the dense connectivity in the proposed search space is a good design. Although we know that the design of shortcut connections is essential, there is a lack of evidence showing that dense connectivity is more important than the combination of different operations. This may be the cause of the inefficiency of the proposed search space. (-)\n\n\n+ One exciting observation by this paper is that using regularizations (such as using CutOut, AutoAugment, and using a large dataset like ImageNet) can stabilize one-level optimization. I think this observation is important to the community. I encourage the authors to provide more insights into it, e.g., by providing more theoretical analysis. (+)\n\n\n+ The design of hardware cost function is elegant, e.g., in Equation (5). (+)\n\n\n- As we know, ineffective architecture rating is the drawback of existing NAS methods, with two ICLR 2020 papers and many ICLR 2021 submissions suggesting that most of the current NAS methods are not better than random architecture selection. There is no evidence showing that this paper has addressed this ineffective architecture rating problem. As suggestions for the authors, I recommend two ways to evaluate the proposed method's effectiveness in addressing the architecture rating problem. The first way is to show that the discarded architectures are actually worse than the preserved architectures during the searching. I believe at least tens of architectures should be compared. The second way is to prove that the preserved architectures have higher and higher performances, with the searching iteration increasing. Without these pieces of evidence, we cannot believe that the proposed method does work well. (-)\n\n\n- No source code is provided by the authors. (-)\n\n\n- More recent works aiming at improving architecture rating should be compared with, such as PCNAS, DNA, and CDAS. (-)\n\nPCNAS: Improving one-shot nas by suppressing the posterior fading\n\nDNA: Block-wisely Supervised Neural Architecture Search with Knowledge Distillation\n\nCDAS: Cyclic differentiable architecture search\n\n\nOverall, I think this paper is interesting but needs improvement.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}