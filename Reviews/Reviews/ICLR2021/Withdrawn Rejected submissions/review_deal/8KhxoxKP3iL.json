{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The three reviewers seem to reach a consensus that the assumptions made in the paper are too strong and hard to interpret. In particular, R1&R2 made the comments that the generative model for the data by itself uses attention, which seems to be make the comparison unfair. The authors seem to argue that the attention model is still expressive enough, which in the AC's opinion could be true but does not justify the use of the generative model when comparing the sample complexity of the two methods. The reviewers also pointed out a few other limitations of the paper. The AC mostly agrees with the reviewers' points (though perhaps with one or two exceptions.) In summary, I think the assumption of using an attention model itself seems to be a big enough issue that makes the paper not read for publication at ICLR. "
    },
    "Reviews": [
        {
            "title": "Very strong assumptions without surprising results",
            "review": "This paper provides theoretical analysis of the sample complexity and the loss landscape of attention mechanism. Under several assumptions, the paper claims that attention models have lower sample complexity than non-attention alternatives. Besides, it also shows every local minimum of the attention model has low prediction error. The theoretical findings of the paper can further provide guidelines for designing attention models. A few empirical studies are conducted to confirm the theoretical findings.\n\nThe theoretical results rely on a few assumptions, which I am afraid are quite strong.\n\n1. (A1) assumes the data is generated by the attention model plus sub-Gaussian noise, which seems very strong to me. Since your data is obtained from an attention model, it is natural that an attention model can fit the data better with less data than a non-attention alternative. In other words, the sample complexity comparison (Theorem 1 vs Corollary 1) is not fair to me.\n\n2. (A3) looks weird to me, and I don’t see any explanation in the paper. What does this complex assumption mean? Is it just for conquering a step in the proof?\n\n3. Why is the assumption (B1) hidden in the text but not listed in parallel with other assumptions?\n\n4. Again, I have difficulty in understanding the complex (A5). \n\nMy feeling is that the author makes strong and hard-to-interpret assumptions, in order to apply the classical learning theory to show the results. \u0010The findings are not surprising nor inspiring that the guidance are just common practices in real applications.\n\nWhen you talk about the loss landscape, is it possible to draw some figures to compare the loss surface of attention models with that of non-attention models? An example would be in this paper: https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf\n\nTypo: The Attention weights are mainly contribute —> contributed.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong assumptions",
            "review": "# Summary\n\nThe paper presents a theoretical analysis of self-attention mechanisms in neural networks by considering a two layer neural network with known and learned attention mechanisms. The paper shows that in the non-agnostic case, i.e. assuming that data is generated from a function in the function class of two-layered neural networks with self-attention, then every local minima of the squared loss has low expected generalization error. The theoretical analysis reveals that using neural networks with self-attention can have lower sample complexity than standard 2-layered neural networks when the data is in fact generated by a two-layered neural network with concentrated self-attention weights. The paper justifies some of the assumptions through experiments on IMDB reviews data set and also shows both theoretically and empirically that weight regularization can improve generalization.\n\n# Strength\n\n1. While attention mechanisms have had tremendous empirical success in NLP, little is known about their theoretical properties. Towards that end the paper takes a first stab at theoretically analyzing attention mechanisms by considering the expected generalization error of local minima of the loss functions.\n2. Through some toy experiments the paper reveals that weight regularization can improve generalization of two-layered NNs with self attention.\n\n# Weakness\n\n1. The paper goes for too many results at the cost of readability. Lots of results (for recurrent self-attention) are deferred to Appendix and even formal statements of Theorem are not in the main paper. \n2. While analyzing two-layered NNs is acceptable for making progress, the main problem is the non-agnostic setting where the authors assume that data is being generated by a two-layered NN with concentrated self-attention. Existing results for two-layered NNs (e.g. Allen-Zhu et al. 2017) are for the agnostic case. This is a significant shortcoming of the current work.\n3. Better sample complexity of attention model comes from the assumption that the true generative model has concentrated self-attention weights, i.e. most of the attention weights are concentrated in a few attention masks. So fitting a model that exploits this assumption, , i.e. a model that belongs to the same class as the true function, would have better sample complexity.\n4. Apart from the non-agnostic assumption, the paper also makes a number of very unrealistic assumptions on the weights (Assumption A3, A1.3 and A4) of the model class. All these assumptions make assumptions on not only the true weights (the weights of the NN generating data) but on the weights of all NNs in the model class. So it is not clear if the model class is even realizable.\n5. There are also many problems with experiments which are detailed below.\n\n# Questions to address in rebuttal\n\n1. For which w's does Assumption A3 (maximum eigenvalue of covariance matrix of residual), A1.3 (zero mean), and A4 (query, key, value) need to hold? If this needs to hold for all w's (and not just the true w's) then this is a very strong assumption. Is the model class realizable? \n2. \"Assumption A1 (non-agnostic setting) is supported by previous works\" which works? Needs reference. Experiment 1 shows that the learned self attention weights are concentrated not that the true data is being generated from such a model.\n3. What is k in Assumption A3 ?\n4. Seems like $1/\\sqrt{n}$ rate comes from the assumption that maximum eigenvalue is $o(\\gamma^2)$ for some $\\gamma > 0$ ? Why use $\\gamma^2$ ? Why not just $\\gamma ?$ \n5. Not clear where assumption A3 is used in the proof. Apparently A3 is used in proof of Theorem 3 but A3 is also needed in proof of Theorem 1. It is stated in proof of Theorem 3 that \"by Assumption A3.1 we know all input features and weights are bounded.\" but that is not what A3 states.\n6. The theoretical claim is that l2 norm of weights determines generalization error but experiments use l1 norm. Why?\n7. Analysis is for regression tasks but experiments are on classification.\n8. Why noisy MNIST ? Why not on the original MNIST ? It is obvious that paying attention on the digit blocks and ignoring the uniform noise blocks will help.\n\n# Minor comments\n\n1. we first introduced a two-layer non-attention model ⇒ we first introduce a two-layer non-attention model.\n2. we consider attention weights are determined ⇒ we consider attention weights that are determined.\n3. There are also a number of other grammatical mistakes that should be fixed to improve readability.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ICLR 2021 Conference Paper2164 AnonReviewer3",
            "review": "Summary:\nThis paper studies attention mechanisms.The main results of the paper are upper bounds on sample complexities of attention models and the baseline non-attention model. The paper also further investigates several additional properties and provides empirical studies.  \n\nConcerns:\n(1) Without going into the details, the main problem of the paper is that it only gives upper bounds on the sample complexity. When comparing the upper bound of two algorithms, the comparison does not show which one is better. To show one algorithm is better than the other, you need to prove that one’s upper bound is smaller than the other’s lower bound on sample complexity.\n(2) The paper proves the result only for a specific non-attention model. Without further justification, it is very unclear why this is enough to represent other non-attention models.\n\nReasons for score:\nI vote for rejection. The theoretical results are not sufficient to support the claims made in the paper about attention models. \n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but with too strong assumptions and some black boxes unopened",
            "review": "Summary:\n\nThis paper analyzes the sample complexity and properties of loss landscape for self-attention neural networks. Specifically, under the assumptions about teacher-student setting, \"sparsity\" of the attention mask, enough over-parametrized student model, and regularity assumptions (all the weights are bounded, etc.), in a regression task with square loss, the authors proved that a two-layer MLP with attention enjoys a smaller upper bound on the sample complexity compare to the baseline without attention. Using very similar techniques and proof procedures with stronger assumptions, this result can be extended to multi-layer or recurrent attention models. Experiments were done on NLP and CV tasks with shallow networks/small datasets to validate the theoretical results.\n\nMain assumptions:\n\nThe assumptions for different theorems are slightly different, but they are somewhat parallel and can be summarized into the following categories. Each theorem requires one assumption from each category.\n\n1. Teacher-student setting: There exists a two-layer teacher model with attention that the ground-truth labels are the outputs of the teacher network plus i.i.d. sub-gaussian noise.\n\n2. The largest few (less than input dimension) weights in attention mask has a constant share of the norm of the entire mask, i.e., the weights of the attention mask are sparse/highly concentrated.\n\n3. When the sample size is large enough (tend to infinity), the student model with learned weights is significantly close to the teacher model in terms of covariance matrix of residual w.r.t. linear regression.\n\n4. Regularity assumptions, e.g., the norms of all the weights and samples are bounded.\n\nMain results:\n\n1. For two-layer neural networks, under all the assumptions mentioned above, the authors proved that the model with a fixed attention mask has a smaller sample complexity upper bound compare to that without an attention mask. By using the same procedure and techniques with stronger assumptions, the result can be extended to attention with a normal self-attention mask, multi-layer self-attention models, and recurrent attention models. The proofs basically come from the same lemma (Lemma 1 in appendix) and can be considered as different corollaries of Lemma 1 under different sets of assumptions.\n\n2. Experiments were done to validate the theoretical results. Specifically, the authors did experiments on Portuguese to English translation, IMDB reviews sentiment classification, and noisy MNIST with small models, e.g., two-layer conv nets or fully-connected nets with/without attention. The experiments showed that the weights of the attention mask are concentrated, and attention in these tasks has the effect of reducing sample complexity and improving performance.\n\n3. The authors also showed that the sparse attention model has a smaller lower bound on the maximal number of linear regions and suffers from the scaling issue when considering the flatness of minima.\n\nPros:\n\n1. The problem analyzed in this paper, i.e., understanding the underlying reasons why attention works, is an important problem, and this paper provides some theoretical together with experimental evidence for this.\n\n2. This paper is written in a clear and well-organized way so it is easy to follow. The experimental settings are explained in detail, and the experiments are replicated multiple times.\n\n3. The theoretical proof and experimental methods appear to be correct and reasonable.\n\nCons:\n\n1. The authors seem to use attention somewhat like a black box. The weight matrices, e.g., queries, keys, values, are learned during the training process, but the authors just assume that they have some nice properties as stated in Assumption A3. This makes the attention mechanism in the proof essentially the same as the known fixed function f. However, I think one of the core parts of the attention mechanism and part of the reason why it works is the learning process of the query, key, value matrices, which is ignored by the authors in this paper.\n\n2. Similar to 1, the authors also hide other properties of the loss landscape in assumption A3. This assumption is not only about over-parametrization because it is a universal property of all possible weights along the training trajectory or on the entire landscape. I believe that over-parametrization can be shown to prove the existence of such weights, but this assumption is much stronger than the over-parametrization itself because another important thing is how the training algorithms find such points. This is also part of the reason why the authors can have strong results that hold for *any stationary point*.\n\n3. The comparisons in this paper are about the upper bounds or lower bounds, but there is a lack of evidence about the tightness of these bounds. For instance, for the comparison between the sample complexity for models with/without attention, the bounds seem to be very large and a bit loose since empirically the networks can achieve small error with much smaller sample sizes. Therefore, it is somewhat unfair to do a direct comparison between the upper bounds. Besides, the loss landscape of models with and without attention may have different $\\gamma$, which is not addressed by the authors in the paper.\n\n4. The logic for this flatness part is kind of confusing to me. The scaling issue is to show that flatness alone cannot fully explain the generalization performance, but not having that issue does not mean the model can generalize better. Besides, the bound on the model weights are the assumptions, not conclusions, of this paper. The authors did not rigorously prove that their model does not suffer from scaling issues. \n\n5. For the noisy-MNIST experiments, the authors were claiming that effect of attention is not mainly due to the model structure by showing that an equal weight attention model performs poorly. However, the authors did not decouple the performance gain achieved by attention in the concentration of the weights and the learning process of the mask. I wonder whether the model will perform well if a fixed concentrated instead of equal weight mask is applied. If the concentration of the mask weights is very important, it should improve the performance of the model by a lot.\n\nRecommendation:\n\nI tend to vote for rejecting this paper. My biggest concern about this paper is that this paper considered some parts of the problem as black boxes and directly have some assumption about them (in other words, the assumptions made by the authors, especially A3, are too strong and hide a lot of things like implicit dependencies on network width, attention mechanism and model structure in parameter $\\gamma$), but some of the things skipped by the authors may be the core part of the problem, making the analysis provided in this paper a bit superficial and the results somewhat limited.\n\nSupporting arguments for recommendation:\n\nSee Cons, especially points 1 and 2 there.\n\nQuestions for the authors:\n\n1. Please address the cons mentioned above.\n\nAdditional feedback:\n\nThere are some notation confusions. For instance, $\\tau$ is used as both the parameter for the softplus activation function (last paragraph of page 2) and in assumption A2. Besides, for the paragraph before assumption (A1) and assumptions (A1) - (A3), the authors are using both $x\\odot f(x)$ and $x_i\\odot f(x)$.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}