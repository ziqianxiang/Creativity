{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper introduces a framework for learning dynamical system models from observations consisting of discrete spatio-temporal series. It is composed of two components trained sequentially.  A first one learns embedding from observations using a seq2seq approach, where the embeddings are constrained to follow linear dynamics. This is inspired by approximation schemes for Koopman operators. These embeddings are then used as the spatio-temporal series representions and are fed to a transformer trained as an autoregressive predictor. Experiments are performed on different problems using data generated from PDEs through numerical schemes. Comparisons are performed with different ML baselines.\n\nThe paper is well written with experiments on problems with different complexities. The original contributions of the paper are 1) the combination of pretrained embeddings with a transformer auto-regressor, 2) a seq2seq architecture for learning time series representations constrained by linear dynamics.\n\nOn the cons side, the paper original contribution and significance are over claimed. Closely related ideas for learning approximate Koopman operators and observables have already been developed and used in similar contexts. Besides there is no discussion here on the properties or physical interpretability (which is often an argument for Koopman) of the learned representations. Then the baselines are mainly composed of simple regressors (LSTM, conv-LSTM, etc.) and this is not a surprise that they cannot learn dynamics over long term horizons. There is no comparison with dynamical models incorporating numerical integration schemes that could model the temporal dynamics of the system. There is now a large literature on this topic exploiting discrete (ResNet like) or continuous formulations (as started with Neural ODE)."
    },
    "Reviews": [
        {
            "title": "a nice proposition, but the paper could be improved",
            "review": "This paper proposes the adaptation of the recent and SOTA framework\nused in NLP to dynamical systems. The idea is nice and well\nmotivated. Two main steps are described: Transformer model for\nmarkovian prediction of time series, along with a Koopman inspired\nmethod to learn embeddings of the state space.  \n\nThe interaction between the main training steps (embedding and\ntransformer parts) is not clear in the paper: pre-training of the\nembeddings, followed by a fine tuning step (or not ?). This is an\nissue of the paper since this interaction is important for NLP\napplications. The reader should be able to fully understand what is\ngoing on here, only by reading the paper.\n\nMy second concern is about the experimental part. The dynamical\nsystems are also limited in complexity, but this is not crucial here.\nHowever, the difference in the results is not well documented and for\nexample, with the fluid flow behind a cylinder, the low results\nobtained with CNN is surprinsing (too bad ?).\n\nThese concerns are not individually prohibitive for such a article,\nand maybe the paper could be improved before\npublication. Nevertheless, the ideas are really interesting.\n\n\nIn the introduction, authors claim that : \"Standard deep neural\nnetwork architectures such as auto-regressive, recurrent and LSTM\nbased models have been largely demonstrated to be effective at\nmodeling various physical dynamics (...) However, the current literature is focused on systems of limited complexity and domain size.\"\n\nIt is maybe important here to better characterize the term complexity,\nwhy it is limited ? and why it is important ?  Moreover, the\ncorrelation between this complexity and the originality of using of\ntransformers is not straightforward. Complex architecture based on\nLSTM for instance could be also effective or should better explain why\ntransformers are different when faced with larger complexity and\ndomain size.\n\nThe notations in the section 2 are a bit heavy and maybe it could be\neasier to follow one of the experimental example, without loss of\ngenerality, to introduce the whole picture.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new application of transformer models (physical systems) with domain-specific embeddings",
            "review": "**Summary**:\nThe paper proposes applying transformer models to modeling physical systems. The state at each time step is embedded into a continuous vector using a pretrained encoder-decoder model based on Koopman’s theory. The experiments are performed on three physical systems and generally show that (1) a transformer model outperforms alternative machine learning methods, (2) a transformer model with the proposed embedding outperforms transformer models with alternative embeddings based on autoencoders or PCA, and (3) more transformer layers help (but only slightly).\n\n**General comments**:\nThis is, to my knowledge, a new application area for transformer models, and the fact that they outperform alternatives could be interesting to the ML community.\nFurthermore, the application of transformer models is not entirely standard, in that the paper proposes learning embeddings that are specialized to the application area.\nThe proposed approach is compared both with alternative (non-transformer) techniques and with ablations of the transformer model.\n\n**Questions**:\nThe proposed embeddings are based on a neural network-based encoder and decoder pair and these are trained either with an objective based on Koopman’s theory or a standard AE objective. Have you considered training the embeddings jointly with the transformer parameters? I think this baseline is needed to motivate the need for pretrained embeddings and it could alleviate the concern that the model limitations are due to the \"inaccuracies of the embedding model\".",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice contribution",
            "review": "Quality\n\n\n- The experiments are clear and the results are easily understandable.\n- In all three experiments, the paper achieves compelling results with low errors compared to baselines, especially for high number of time steps.\n\n\n\nClarity\n\n\n- I find the mathematical notations in Section 2.2 could be more clear. I'm a bit confused about the infinite dimensionality of the state observables and how the Koopman operator handles it, whereas in the Figure 2 shown the Koopman operator K looks like a banded diagonal linear operator. It is not apparent why equation (3) leads to the construction of the encoder-decoder model shown. More explanations on the Koopman operator would be appreciated to make the paper more self-contained.\n\n- The plots make the experiment results quite clear in terms of the model's capability to estimate the dynamics.\n\n- Some minor typos such as \"on going\" which should be \"ongoing\"\n\n\n\n\nOriginality\n\n- This work uses a Transformer model instead of recurrent networks to perform dynamical system simulation. The paper also proposes using an embedding model. Overall, the novelty is moderate. \n\n\nSignificance\n\n\n\nHigh-level pros and cons\n\nPros\n\n- Modeling dynamical systems is an interesting deep learning application.\n- Good results compared to baselines in all experiments shown.\n\nCons\n\nThe clarity can be improved in my opinion.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Clear and well executed paper that currently misses significant experimental evaluation",
            "review": "### Summary:\nThe paper proposes to use transformers for modelling dynamical systems. The transformer is combined with a linear dynamical system to enforce Koopman features and is trained using the reconstruction and prediction loss. Finally, the proposed algorithm is applied to the different tasks with 1, 2 & 3 dimensions. On each simulated task the proposed algorithm marginally outperforms sufficient baselines.\n\n### Clarity & Style:\nThe paper is clearly written and understandable. The figures could be drastically improved by optimizing the white space and axes spacing, e.g. figure 5. Furthermore, having videos of the solver and network predictions would be preferable. It's a bit surprising that the authors don't cite https://arxiv.org/pdf/2002.09405.pdf, which would also be an interesting but not necessarily required baseline.\n\n\n### Quality, Originality & Significance:\nWhile the paper is clearly written and well executed in terms of experiments and baselines, the paper is missing originality and significance. The paper does not introduce new perspective or demonstrate a previously unaccomplished task. It simply applies a straight-forward method and demonstrates that it works marginally better compared to existing methods. Especially the usefulness of the model is questionable. The Lorenz system is a **three** parameter dynamical system and is approximated with a transformer with **54,000** parameters! What is the advantage of the transformer network compared to the analytical solvers? Is it faster and if yes how much more energy does it consume compared to the solver? Why is it better compared to a look-up table with 54,000 values, which does not require tuning network hyper-parameters? Does an analytic version of the Koopman operator exist for the Lorenz system? There might be a good chance as the Lorenz system is a polynomial dynamic system. How many parameters does this have? If so how do the learned features compare to the analytic features?\n\nFurthermore, the paper lacks to address the real challenges of learning dynamical systems (*in my opinion*). For clean data (or data with clean Gaussian noise) and a single coherent timescale most methods perform reasonably well. IMO the main problems are:\n\n* Messy data from real systems including, non-Gaussian noise, discretization errors, time delays. Can such an over-parameterized transformer work for such data and recover the underlying structure or does it overfit?  \n* Dynamical systems with multiple space & time-scales. Is the transformer model able to learn a good dynamical system with multiple time-scales?\n\nTo increase the significance of the paper the authors would need to address one of these two questions.\n\n\n###  Conclusion:\nAll in all, I think the paper is well written and the performed experiments are well executed. However, right now the paper misses significance to be published at ICLR. To improve the papers the authors would either have to address messy real-world data or dynamical systems with multiple time-scales. I am open for discussion, why this model is a significant advancement over prior methods, but right now I don't see that.\n\n### **Post Discussion Comments:** \nThe authors provided videos of the task. While I still find that this paper misses clear benefits/use-case for the machine-learning community. I still don't clearly understand why one would need 54,000 parameters for a  3 parameter system. One could store a really big table of this one 1d system for this amount of parameters. However, the paper is well executed and clearly written in regards of technical aspects. The motivation remains doubtful for me. Due to the execution and clear writing, i increased my score to weak accept.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}