{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes several simple alternatives to generate adversarial examples for deep reinforcement learning algorithms based on image distortions such as lighting change, blurring and rotation, and show the performance of DRL agents degenerate as a result. Most reviewers appreciate the simplicity and computational efficiency of the proposed attacks. The results revealed by the work is however rather unsurprising, given similar attacks have been evaluated for DNNs. The authors did not offer much more insight on the presented results beyond that, for example, robustness of different DRL algorithms with regards to these attacks as mentioned by reviewer 2, sensitivities of the parameters for each attack proposed, effectiveness of different attacks on different environment and possible combination of attacks. "
    },
    "Reviews": [
        {
            "title": "More experimental results and better evaluation of generated samples are needed",
            "review": "Strong points:\n1. The authors propose a new framework for constructing adversarial data samples for deep reinforcement learning training. It contains six different concrete methods. Some of them can generate data samples fairly different from the original data samples.\n2. The authors do experiments with the proposed technique to check the generalizability on 10 atari games, and show that the generated data samples can fail the agent training to a different extent.\n\nWeak points:\n1. The biggest problem is that: although the authors have demonstrated that the generated data samples are fairly different from the original data samples, how to guarantee that there is still enough meaningful information contained in these training data samples? Definitely, we can look at the generated figures and it seems that they are still recognizable. However, we probably need some quantitative measures to show that these data samples can still be used to train good agents. To the extreme, we can enumerate some random samples, which will be extremely different from the original samples, and they will also fail the training of the rl agents.\n2. More quantitative experimental results are needed. It is more convincing to see how much score loss can the proposed framework cause, compared with clean data samples.\n3. One central part of the evaluation should aim to answer: whether this framework can differentiate the generalizability of algorithms better than other frameworks. This should be shown in experiments. For instance, say we have three algorithms, A, B and C, and we believe the groundtruth is their generalizability is A >> B ~ C.  Then, under this framework, we should see the score align with this conclusion better than under other frameworks.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting study on generalization of RL using realistic perturbations",
            "review": "Summary:\n\nThis work focuses on assessing the generalization of deep reinforcement learning by applying semantically meaningful perturbations to the RL agentâ€™s observation system, e.g., brightness, blurring, contrats, shifting etc. The authors demonstrate strong degradation in the performance of RL agents in various Atari games, despite the semantic perturbations having significantly lower perceptual distance as opposed to planned adversarial attacks.\n\n\nReview:\n\nThe paper is well-written and easy to understand. Related work has been adequately referenced and the paper is of significant interest to the ICLR community.\n\n\nStrengths:\n\n1. The evaluation is quite comprehensive and covers multiple Atari environments.\n\n2. The chosen perturbations are semantically meaningful and cover a broad range of frequencies in the fourier domain.\n\n2. The degradation impact from semantically meaningful perturbations has also been compared to that from adversarial attacks, while at the same time characterising their perceptual similarity to original observations.\n\n\nWeaknesses:\n\n1. My major concern is that the metric defined to capture Impact (eq 2 in section 2.5) is somewhat unclear. Normally, one defines a score and then normalizes it with constants \"max\" and \"min\", for instance, like (max - score)/(max - min). In such a case, \"max\" and \"min\" are generally constant upper and lower bounds on score. However, while at the surface eq 2 seems to be doing the same, it might actually be doing something different. This is because of how Score_{max} and Score_{min} are defined. Firstly, Score_{max} and Score_{min} are not constants but differ for an agent depending on its Q-function. Secondly, the Score_{min} term seems somewhat misleading. The authors state that \"Score_{min} is the score at the end of the episode achieved by the agent who takes the action that minimizes its Q(s, a) function in every state visited\". Since, Q(s,a) is the value of action a in state s given that the agent maximizes the value from the next state onwards, just choosing the minimizing action in the current state may not be akin to choosing the actual minimum return from an episode. Hence, this functional form used to capture Impact may be doing more than just a score normalization. Can the authors please comment on why they chose the definition of Impact as such and if this could be having potential impacts on the results shown throughout the paper?\n\n2. Additionally, since the authors have conditioned the definition of Impact on Q-values, it seems like the framework is only restricted to evaluation of agents which learn a Q-function. Direct policy-based agents may not be amenable to the Daylight framework. Is it possible to extend the framework to handle RL agents, which do not learn a Q-function?\n\nI am happy to reconsider my score if the above two major weaknesses can be addressed.\n\nMinor comments:\n\n1. In most experiments from section 3, BankHeist stands out as an exception in the sense that Carlini & Wagner (2017) seems to be competitive on it as opposed to the proposed perturbations. It would be interesting to analyze and understand what sets this game apart.\n\n2. The text and labels on the axes in figure 1 are unreadable. Please rectify the font size.\n\n3. I might have misunderstood this, but why does figure 1 have only one spatial frequency axis? Shouldn't it also have two spatial frequency axes like in figure 2?\n\n4. Minor punctuation error in the abstract: \"state-of-the-art adversarial attacks, .\"\n\n------ Post-rebuttal update ------\n\nThe reviewers have addressed my concerns for the most part and I am happy to update my score to recommend acceptance. I also hope to see the A3C experiments in the final draft of the paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple attacks for reinforcement learning with pixels based on image distortions",
            "review": "################################################\n\nSummary:\nPrevious work on crafting attacks for deep reinforcement learning has relied on computing adversarial examples using knowledge of the environment, policy, and optimizer. Using Atari games and DDQN, this paper shows that simple image distortions, such as brightness changes, blurring, and rotations, often has greater impact on the agent's performance and is perceptually more similar to the original images. \n\n################################################\n\nPros:\n1. The proposed attacks are simple, intuitively meaningful, and computationally cheap. They are black box, not requiring information about the environment and policy, which is a more realistic setting.\n2. The proposed attacks seem to be better than Carlini & Wagner for most games. I was a little surprised how much, especially for JamesBond.\n\nCons:\n1. The results are shown for only one algorithm, DDQN, only on Atari. It would be good to provide results for newer algorithms like Ape-X or other environments like DMControl.\n2. Deep RL algorithms usually have large variance. It would be good to provide standard errors of the results, to accurately compare Daylight to Carlini & Wagner.\n3. The writing has some weaknesses, please see below for details.\n\n################################################\n\nOverall: I would lean toward accepting this paper. Technically it is not very sophisticated and the proposed attacks have been considered for image classification [1], but I believe that the results have strong practical implications. That is, simple image distortions are surprisingly impactful for attacking deep RL agents.\n\n[1] Samuel Dodge and Lina Karam. \"Understanding How Image Quality Affects Deep Neural Networks\". ArXiv 1604.04004.\n\n################################################\n\nSuggestions for writing:\n1. I think it would be better if the experimental setup preceded the results in Section 3. In addition, more details should be given, such as: How were the specific games and algorithm chosen? \n2. I think the paper [1] is relevant for related work, as it shows similar distortions are good attacks for image classification.\n3. The notation D(s) at the bottom of page 2 is undefined.\n4. The differences in Figure 2 are not very apparent. It may be helpful to add bounding boxes.\n\nFurther comments and questions:\n1. What happens if we combine several of these attacks? Would that lead to even greater impact?\n2. Is there any intuition for why certain games are more robust to certain image distortions than others? For example, why are the results for compression artifacts more mixed?\n\n################################################\n\nUpdate after reading other reviews and author responses:\n\nI am happy to keep my score and remain positive about this paper; the authors have answered my questions and partially addressed my main concerns in the revised paper. Like Reviewer 3, I would hope to see complete results for A3C in the final paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}