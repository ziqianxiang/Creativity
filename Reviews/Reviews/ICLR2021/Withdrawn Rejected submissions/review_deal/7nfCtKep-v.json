{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents novel model stealing attacks against BERT API. The attacks are split in two phases. In the first phase, the black-box BERT model is recovered by submission of specially crafted data. In the second phase, the inferred model can be used for identifying sensitive attributes or to generate adversarial examples against the basic BERT model.\n\nDespite the novelty of presented attacks against BERT models, the current version of the paper has some problems with clarity and motivation. The presentation of attacks is very short, and some technical details are not adequately covered. The practical motivation of adversarial example transfer attacks is not very clear, and the authors' response on this issue did not provide a convincing clarifications. Furthermore, creation of surrogate models for generation of adversarial examples is a well-known technique and the difference of the proposed AET attack from this conceptual approach is not clear.\n\nOverall, the paper reveals a solid and interesting work but a substantial revision would be necessary to make it suitable for the ACLR audience.  "
    },
    "Reviews": [
        {
            "title": "Reviews and Comments",
            "review": "### Overview\n\nThe authors propose a pipeline to attack and steal sensitive information from a BERT-based API service, and can subsequently perform adversarial attack to the victim model by creating white-box adversarial samples on the stolen model.\n\nThe pipeline can be summarized as the followings:\n1. Using distillation to train (steal) a model from the API.\n2. Conduct model inversion attack to the stolen model in step 1 to expose sensitive information of the training data.\n3. Create adversarial samples for the stolen model in step 1 and use them to attack the original API.\n\nSome of the assumptions of the experiment settings are too strong and far from the real situation, but the idea of using this pipeline to conduct model inversion and adversarial transfer attack is very interesting.\n\n### Pros\n\nThe pipeline proposed by the authors is very insightful. The experiment results also show the effectiveness of model inversion and adversarial attack. \n\n### Cons\n\nThe assumption of the Model Extraction Attack part is too strong. The authors use the same pre-trained BERT parameters for both victim and stealer model. However in real practice we are not able to know which pre-trained BERT parameter set is to be used for fine-tuning, nevertheless to get the pre-trained model. What if we use different pre-trained BERT parameters? What if we use a pre-trained BERT with different size (num of layers, hidden dim, etc.)?\n\nBesides, the stealing method is just a conventional distillation. Although the authors claims three differences between their method and distillation: (1) the goal (2) the accessibility of original data (3) the accessibility of hard labels, only the first one is appropriately claimed. For (2), distillation is also broadly used in transfer learning w/o the access of original data. For (3), distillation w/ only soft labels is also very popular and useful, from conventional distillation for compression, to self-distillation. \n\nI'd like to hear the reason why the authors make the assumption that the stealer would have the same pre-trained BERT when attacking, and also curious about the results of using different pre-trained BERT model. I might change the rating if the authors may address these questions.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Expanded Discussion of Impact and Limitations",
            "review": "################################ \n\nSummary:\n\nThis paper presents a model extraction attack (MEA) for BERT-based models that are hosted behind an API. Using the model obtained in this step, the work aims to subsequently demonstrate attribute inference attacks (AIA) to expose sensitive information of the underlying data used during fine-tuning and adversarial example transfer (AET) that can be used to attack the hosted model.\n\n################################ \n\nReasons for score:\n\nOverall, I lean toward reject. The underlying idea is interesting and timely, and core to this interest is that \"the adversary can steal a BERT-based API (the victim model), without knowing the victim model's architecture, parameters or the training data distribution.\" As demonstrated, a substantial portion of the architecture (BERT) is known and the exploration of fine-tuning as the only mechanism for tailoring the model (rather than continual pretraining) limits potential impact.\n\n################################ \n\nStrengths:\n\n- Broad interest. The underlying ideas are of general interest, especially given recent examples of language models hosted behind APIs. The notion that they can be efficiently reproduced from that API and that they may in turn leak training information is an emerging concern.\n\n- Clear differentiation from prior work. In particular, the section on comparison to knowledge distillation is helpful in grounding the setting for experimentation.\n\n################################ \n\nWeaknesses:\n\n- Limitations. There is an implicit assumption that the models being hosted behind APIs are fine-tuned BERT models. Limitations of this should be more explicitly discussed. Many works in specific domains (e.g., legal, biomedical, etc.) appear to rely on continual pretraining to integrate sensitive data rather than fine-tuning toward a single task. Others even appear to train these models from scratch on data. It's unclear how common the case of fine-tuned BERT models behind APIs are from this paper.\n\n- Motivation of AET. The motivation of adversarial attacks against a pay-per-query API are unclear. Yes, it's possible to cause the API to create incorrect predictions, but why is that problematic for the owner of the model? It's clearly undesirable with respect to creating robust models, but as presented it's unclear why this is problematic.\n\n- Impact. Similar to the point above, the assertion that \"modern NLP systems typically leverage the fine-tuning methodology by adding a few task-specific layers on top of the publicly available BERT base\" is not substantiated by this work or by citation. While BERT has certainly become abundant, many recent advances are either not BERT-based (though perhaps the underlying transformer architecture) or do more than fine-tuning.\n\n- Knowledge of black-box model. While a stated goal is that a knowledge of the architecture and training data is not required, the experiments leverage a knowledge of the architecture (BERT) and appear to share an architecture for layers used during fine-tuning.\n\n################################ \n\nQuestions:\n\n- Given the positioning of \"stealing\" a model, how many queries are required to obtain an approximate model? How many are required if knowledge of the previously issued queries is known?\n\n- Can you provide pointers to models that are BERT-based and fine-tuned for a specific task only?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need some clarifications.",
            "review": "The paper is motivated by a challenging problem in deploying a neural network-based model for sensitive domain and research in this direction is essential for making such model usable for sensitive domains. The paper presents a model extraction attack, where the adversary can steal a BERT- based API (i.e. the victim model), without knowing the victim model’s architecture, parameters or the training data distribution. The model extraction attack, where the adversary queries the target model with the goal to steal it and turn it into a white-box model. They demonstrated using simulated experiments that how the extracted model can be exploited to develop effective attribute inference attack to expose sensitive information of the training data. They claimed that the extracted model can lead to highly transferable adversarial attacks against the original model (victim model). \n\nThe model extraction step of the proposed method is the main concern for me. Conclusions maid by simulated experiments on model extraction attack might not hold for a real experiment. The simulated experiments make both victim model and extracted model accessible and thus measuring functional similarity is fairly easy. However, without the knowledge of the victim model and with limited query budget, the simulated experiment might not resemble a real-scenario. Some explanations with real scenarios would make the claim more realistic. \n\n\nSome thoughts:\n\nRe: “Modern NLP systems are typically based on a pre-trained BERT. ”: provide references or evidence to support the statement.\n\nRe: “Model extraction attack aims to steal an intellectual model from cloud services.”: provide references or evidence to support the statement.\n\nRe: “Most existing adversarial attacks on BERT are white-box settings, requiring knowledge of either the model internals (e.g., model architecture, hyperparameters) or training data.”:  provide references or evidence to support the statement.\n\nRe: “The intuition lies in the fact that the similarity of our extracted model and the victim model allows for direct transfer of adversarial examples obtained via gradient-based attacks.”  — BERT part is same for both victim and extracted model but rest is still unknown and how the complexity of the similarity measurement increases for a real scenario? \n\nRe: “We measure the accuracy (on the same held-out test set for evaluation purposes) between the outputs of the victim model and the extracted model to assess their functional similarity.” — Can this be arbitrarily true by accident? Is there a robust way that we can use to measure the similarity?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting results, but I have concerns about experimental setup",
            "review": "Summary: This paper is studying the vulnerabilities of modern BERT-based classifiers, which a service provider is hosting using a black-box inference API. Consistent with prior work [2], the authors succeed in extracting high performing copies of the APIs, by training models using the outputs of the API to queries (akin to distillation). The authors then study two attacks on the copy model --- private attribute identification of sentences in the API's training data & adversarial example transfer from the white-box copy model to the black-box API. The authors report high attack success rates, better than those from competitive baselines (which do not require constructing a copy model). A few defences are also explored but are ineffective to prevent these attacks.\n\n-------------------------------------------------\n\nStrengths of the Paper:\n\n1. While model extraction on BERT models has been studied previously [2], this paper goes beyond the setting of utility theft and explores information leakage and adversarial example transfer. These are extremely practical real-world settings. Moreover, the paper uses modern NLP techniques (finetuning BERT), which is ubiquitous in NLP systems these days.\n\n2. The reported attacks seem to significantly outperform some competitive baselines which didn't use an extracted model. While I have concerns about the experimental setup (below), these are very interesting results highlighting vulnerabilities of the models. This can encourage more research in defending against model extraction.\n\n-------------------------------------------------\n\nWeaknesses of the Paper:\n\n1. Query distribution: These distributions seem fairly similar to the downstream task for all datasets, for instance, \"reviews\" contains Yelp reviews, which is one of the datasets the victim model was trained on (I suspect some amount of overlap at the very least). The best MEA scores are observed when the domains are aligned, which might not be a practical setting for an attacker who has no knowledge of the victim model's training distribution. I suggest, at the very least, authors to provide n-gram overlap statistics between their preferred query distribution and downstream test set (the GPT2 paper [3] had similar statistics). The paper's story will be stronger if a corpus like Wikipedia is used for the query distribution, with the same set of downstream datasets.\n\n2. AIA Attacks: I have a few concerns here. First, isn't access to private attributes in half of the victim data (D_a) too strong an assumption? In a more practical setting, an attacker will have no access to D_a. It's even possible that the attacker doesn't know the output space of attributes. I think the more interesting setting is where the attacker is able to infer some information about the training data without supervising a classifier with gold data (D_a), perhaps using something like model inversion. This information need not be a private binary label, it could even be some canary string like a credit card number [4]. One more concern I had here was regarding the main baseline in this experiment, \"BERT (w/o fine-tuning)\". I find it quite strange that this is much worse than the majority class in two datasets. What happens when you fine-tune it on D_a? (using the standard practice of [CLS] vector for classification). This is a valid baseline if access to D_a is assumed, I think this will do quite well if it is possible to infer the private variable from the text.\n\n3. Adversarial example transfer: My main concern here is that \"transfer rate\" by itself is insufficient. You can make transfer rate 100% by retrieving examples from the target adversarial class. The more interesting evaluation is, what fraction of adversarial examples are both (1) transferred correctly; (2) not adversarial to a human (the changes are so minor that humans ignore them). Some kind of human evaluation for (2) will be helpful. Also, a good baseline here would be using adv-bert but with randomly chosen words (instead of white-box gradients), and an upper bound with adv-bert attacks on the victim model itself.\n\n-------------------------------------------------\n\nOverall Recommendation:\n\nWhile this is a very practically important setting, I'm not entirely convinced the proposed attacks work. My main concerns are regarding some of the experimental decisions and lack of baselines while comparing attacks. Overall I think the paper needs more work to be ready for publication.\n\n\n-------------------------------------------------\n\nOther Feedback:\n\nWhile these points are not a make or break for me, they will make the paper stronger. It will be nice to include some fine-grained qualitative analysis of the adversarial examples (along with samples), perhaps highlighting why generating that example would only be possible with access to an extracted model, and confirming the victim API model generates the same example. It will also be nice to see work beyond classification setting. Setups like question answering, machine translation, unconditional text generation are exciting testbeds which might be a lot more vulnerable to AIA style attacks than classifiers. With GPT3, black-box text generation APIs are probably going to get very common in the next 2-3 years!\n\n-------------------------------------------------\n\nErrors / Typos / Stylistic:\n\nI had some trouble understanding parts of the paper. I think with a bit more polishing and careful proof-reading, the paper will be easier to understand. There were also a few incorrect statements. I've pointed them below along with typos / stylistic suggestions,\n\n\"commercial NLP models such as Google’s BERT and OpenAI’s GPT-2 (Radford et al., 2019) are often made indirectly accessible through pay-per-query prediction APIs.\" --> This is not a correct statement, both pretrained models are freely available\n\n\"and NLP tasks (Chandrasekaran et al., 2020).\" is a mis-citation, you probably wanted to cite Pal et al. 2019 [1] or Krishna et al. 2020 [2] here?\n\nIn 3.2 and the Abstract / Intro I would remove the claim that \"architecture, hyperparameter is not known\", since both the victim / attacker are finetuning BERT.\n\nThere's some unnecessary mathiness in 3.2 (variables which are not referred to later on, like f_{bert}_theta*). I would suggest avoiding variables unless you plan to re-use them to reduce confusion.\n\nIn Table 3 I would suggest reporting attack success rather than privacy, to be consistent with other tables in paper (higher means more attack success)\n\nTable 4 caption, \"Transferability is the ratio\" --> \"Transferability is the percentage\"?\n\n-------------------------------------------------\n\n**After Author Response**: I really appreciate the author's efforts over the course of the rebuttal period for rigorously testing their method with several new baselines in such a short period of time. \n\nFor AIA attacks, the baseline numbers provided in the rebuttal are helpful but raise concerns about whether the proposed AIA attacks are working. I find it hard to believe that victim models have less private information than extracted models in 2 out of 3 datasets, and I suspect some other factors are contributing to this counterintuitive trend (like you said, maybe dark knowledge). I will stick with my stance that the AIA setting is broken since you are inferring private attributes using information from an identically distributed D_a (I think model inversion is a more valid setting to measure leakage).\n\nFor adversarial attack baselines, I agree with your argument that conducting black-box attacks directly on the victim models may need minimal difference queries which can be detected on the API side. However, you are going to need several orders of magnitude more queries to do extraction in the first place (which may or may not be easy to detect). I still encourage you to run this baseline in the next version of the paper, instead of only doing black-box attacks on extracted models. These minimal difference checks may not be in place, and directly doing black-box attacks on the victim model are much easier than extracting and then constructing adversarial examples. It is good to know what additional benefit you get by doing model extraction.\n\nOverall, I have decided to raise my score to 6 (more like ~5.5-6). This is conditional on the authors performing much more rigorous hypothesis-driven testing in the next version of the paper (just like they did in the rebuttal) to really validate the hypothesis \"extracting models make APIs more vulnerable to adversarial attacks\".\n\n-------------------------------------------------\n\nReferences\n\n[1] - https://arxiv.org/abs/1905.09165  \n[2] - https://arxiv.org/abs/1910.12366  \n[3] - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf  \n[4] - https://arxiv.org/abs/1802.08232",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}