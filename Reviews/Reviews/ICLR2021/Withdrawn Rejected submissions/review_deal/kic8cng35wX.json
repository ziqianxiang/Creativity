{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In line with recent work in the NAS literature, the authors consider a weak NAS performance strategy to filter out bad architectures and narrow down the exploration to the most promising region of the search space. The authors propose to estimate weak predictors progressively by learning a series of weak predictors that can connect towards the best architectures. The authors provided a number of additional experiments during rebuttal, addressing most of the reviewers' comments convincingly and further showing the strong performance of their method. However, the authors should relate their work to Bayesian optimization, which comes in many flavors, and black-box optimization techniques in general as their work shows a number of similarities, but is less principled."
    },
    "Reviews": [
        {
            "title": "Interesting approach",
            "review": "Summary of contribution: The authors propose an interesting approach to address the sample-efficiency issue in Neural Architecture Search (NAS). Compared to other existing predictor based methods, the approach distinguishes itself by progressive shrinking the search space. The paper correctly identifies the sampling is an important aspect in using a predictor based NAS method;\n\nThe writing is clear and easy to follow. The author provided an explanation of how their algorithm works, evaluate the algorithm on both NASBench-101 and NASBench-201, and show their methods work in practice on both\n\nSome critiques:\n\n1. Questions on predictor methods:\n\nI understand the NAS community has accepted many predictor based paper in major conferences, but it seems NAS is re-making the wheel in derivative-free optimizations, e.g. Bayesian Optimization, Evolutionary Algorithm, and MCTS. The predictor is essentially the surrogate model in Bayesian Optimizations, however, the current predictor methods simply ignore the acquisition function in BO that makes the trade-off between exploration and exploitation. These predictor paper in NAS basically suggest the acquisition functions are not necessary, and we can achieve great performance without that (e.g. on NASBench, I will come to this later). Apparently this predictor trend in NAS community is against the decades of development of Bayesian Optimization. If the authors believe the development in BO community is wrong, please justify that using acquisition is indeed not important, i.e. exploration is not necessary, with extensive experiments.\n\nIf the new experiment can persuade me, I believe this paper can be top 1% paper, and I will argue for accepting it.\n\nHere I'd like to list a set of predictor based paper that I have seen in the NAS community: \n\n[1] Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS, NeurIPS-2020 \n\n[2] Brp-nas: Prediction-based nas using gcns, NeurIPS-2020 \n\n[3] Neural predictor for neural architecture search, ECCV-2020\n\nI understand these paper have been accepted but do not necessarily mean these approaches expand the frontier of knowledge. \n\n2. Evaluations on NASBench. \n\nThe reason why predictor works well on NASBench simply because it can predict every architectures. The largest dataset has 4.2*10^5 architectures, it won't be hard to predict them all. However, I believe the original NASBench paper has set baselines for us to compare (though currently published paper fail to follow these baselines and setup). I strongly encourage the authors to take look at the following repository, to see how original NASBench-101 setup the comparisons. I admit the design of NASBench might have some glitch in evaluations, but it is important to follow the same standard.\nhttps://github.com/automl/nas_benchmarks\n\n3. Evaluations on ImageNet. \n\nGetting a good results on CIFAR10 and ImageNet can be tricky. Given the current ImageNet accuracy is 76.4@597 MFLOPS, while the SoTA top1 accuracy for a 600 MFLOPS model is 80.8. I understand these models use different search spaces, but the results on CIFAR-10 is missing. There are too many tricks to hack the network accuracy on ImageNet. Therefore, it will be better to judge NASNet search space on CIFAR-10.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper deals with a common bottleneck in neural search approches which is the prediction of performance of the potential sampled architectures. It proposes to learn weak predictors on a limited sample rather than model the performance over the whole architecture space. The experiments demonstrates that the proposed methods can reach a top performing architecture with fewer samples. ",
            "review": "Overall this work moves into the right direction of trying to improve the performance of predictors. \n\nPros:\n\n- The paper provides experiments on different datasets, and evaluates different predictors to validate the approach. \n\n- The approach seems to have a significant speedup on the search time, but I would also like to see the results in terms of GPU days which is a common metric. \n\nCons:\n\n- In a space where there is rapid progress and accuracy alone is not a practical metric for real-world applications since neural networks are used widely today from data centers to mobile devices. More effort should be towards this multi-objective problems considering also memory and computational cost. In this respect I find this a weakness of the paper in that it does not address these factors which would make the problem much more interesting, even though the proposed approach is appealing. \n\n- A broader comparison should be made with DARTs approaches that have been shown to be generally faster. Also how does the approach compare with DSNAS: Direct Neural Architecture Search without Parameter Retraining in terms of efficiency?\n\n- The setup and hardware used to evaluate the approach is not reported. \n\n- Please correct for typos and grammatical mistakes, e.g., end of page 2 \"the the loss\", page 5 \"our method can performs\"\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting and inspirational paper",
            "review": "The paper proposes an idea of jointly optimizing the sampling policy and predictor in NAS. With this method, we avoid to train a predictor which performs well on the whole search space and search a model with less queries.\n\n### advantages\nThe authors propose that training a good accuracy predictor for models in the whole search space is difficult. Thus, the paper focuses on a predictor that works in a small range, then optimizes the sampling policy as well as the predictor to yield a good structure.\n\nCompared with some evolution-based and RL-based methods, the algorithm observes a better model with less queries.\n\nTotally speaking, the idea is very interesting and it does ease the search process. \n\n### Weaknesses\nA weakness of the paper is that it does not compare with differentiable methods such as DARTS. It is well known that differentiable methods are nearly the fastest methods now and yield not bad results.\n\nAs the results show, the algorithm finds a good model on ImageNet with 1000 queries, which seems to take more time than differentiable methods.\n\n### Question\n1. For each query, do we need to train the model till convergence?\n2. How much time it takes to yield a good model (i.e., time for 1000 queries)?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This work is more like an incremental improvement to the neural predictor. It gets a small performance improvement by learning in a better subspace.",
            "review": "Pros:\n- A progressive method for neural predictor-based NAS is proposed, and it shows better performance than previous methods.\n- Experiments on the NAS-bench-101 and NAS-bench-201, as well as on the NASNet search space to validate the effectiveness of the method.\n- The proposed method is conceptually simple and efficient.\n\nCons:\n- This work is more like an incremental improvement to the neural predictor. It gets a small performance improvement by learning in a maybe better subspace.\n- Is it effective to simply choose TopK models? As the author said, the learning space is not convex thus TopK samples may contain many local-optima and make the learning of predictor difficult even failure (all the predicted Accs fall within a small range).\n- In Fig. 1, why is there a sudden increase for the strong predictor (at near 15625 samples.)\n- What if the search space is huge, like 10^20 in many SOTA NAS search space, rather than the small models in NAS-Benchs, the proposed method seems inefficient in this case. Will it still work?\n- What's the difference and advantages between the proposed model, and choosing the top 10% of the models and training a single predictor?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}