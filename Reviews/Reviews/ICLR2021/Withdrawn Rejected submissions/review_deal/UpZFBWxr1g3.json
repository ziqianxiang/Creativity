{
    "Decision": "",
    "Reviews": [
        {
            "title": "Searching skip connections efficiently",
            "review": "As Transformer model has a fixed skip connection architecture among different layers, this paper explores possible model architectures, specifically skip connection, to achieve better performance. The authors applied the idea of network morphism to add skip connections as a procedure of fine-tuning. Experimental results on several translation tasks show that the proposed approach successfully improves the translation accuracy.\n\n- Figure/Tables are too small to read. Please make them bigger in the camera-ready version.\n- The improvement reported in Tables seems to be limited. Did you conduct significance test between the proposed approach and the baseline? If you don't have the translation outputs of the previous work, what about reimplementing their work and directly compare them with your model?\n- Table 1 reproduces the decoding effects which is also reported in Edunov et al. (2018), and what about the results of the other combinations of decoding methods? \n\nMinor comments:\n- Abstract: \"in NLP (natural language processing)\" -> \"in natural language processing (NLP)\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Writing of technical parts needs improvement.",
            "review": "This work searches for better architectures with different kinds of skip connections for machine translation. \n\nI tried and failed to understand how the proposed method works.\n\n1. While the title is \"IMPROVING MACHINE TRANSLATION BY SEARCHING SKIP CONNECTIONS EFFICIENTLY\", I don't find anything related to search in Section 2. Algorithm 1 is just for parameter training for a given architecture. Where is the ``````''search'' ?\n\n2. \" In network morphism where skip connection is initialized with a small value a, the influence of an added skip connection is also small.\" This may be true at the beginning of training. When training goes on, the skip connection can play a key role if it has a large gradient. Why do we need to manually amplify the gradient? Note that a small \\alpha does not mean it has a small gradient. This faster trick should be better motivated. Besides, I'd like to see the ablation study of this trick. \n\n3. \" We extract the skip connection architectures of two fine-tuned models in two datasets, IWSLT’14 De-En and WMT’18 En-De with backtranslation. Then we apply these skip connection architectures to new transformer models trained in IWSLT’14 De-En dataset.\"     \nWhat does it mean by \"apply these ... architectures to new transformer models“？Directly train the architectures from scratch?\n\n4. \"We focus on applying network morphism to transformer model in machine translation tasks.\" There exist hundreds of NAS algorithms. What's the benefit of network morphism? Any experimental results to support network morphism?\n\n5. \"We find the architecture searched in IWSLT’14 De-En works better than the architecture searched in WMT’18 En-De for the models in IWSLT’14 De-En dataset.\" This is very normal. Why to emphasize it?\n\n6. \" the connection created by network morphism is dynamic.\" I don't understand this claim.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea of network search focusing on skip connections",
            "review": "This work proposes network architecture search focusing on skip connections. Basic idea is to employ pre-training idea so that a new network structure is fine-tuned after adding skip connection inspired by network morphism in Elsken et al. (2018). Since skip connection is light without incurring large number of parameters, the proposed method could easily train a network structure with new skip connections with the help of greedy algorithm to explore alternative connections. Experimental results show consistent gains after fine tuning.\n\n# Pros\n\n* An interesting idea of  exploring only skip connections to search better network structures. This work might have an impact to other research directions on searching a new network structure.\n\n* Many small tricks, e.g., faster gradient, for faster training with careful initialization of skip connection weights.\n\n* Very meaningful gains are observed after the fine tuning.\n\n# Cons\n\n* No analysis on experimental results.\n\n* No significant tests were carried out and thus, it is not clear whether the gains are meaningful or not.\n\n# Detail\n\nI think this work is good enough for acceptance given the well-focused idea with experiments. \n\nThe proposed idea of searching for skip connections sounds reasonable and empirically demonstrates its effectiveness in terms of the translation qualities and the speed of network search. However, I'd like to see more details in analysis, e.g. whether meaningful connection weights, i.e., a in Equation 4, are learned or not. I suspect some connections might have higher/smaller \"a\" values and it would be better to analyze more in details to see whether the connections are actually learned or not. In addition, it would be better to analyze how parameters are updated after the skip connections to see meaningful changes are performed on certain parameters or not.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "fair comparision and ablation studies are required.",
            "review": "In this paper, the authors argue that the fixed residual (skip) connection structure that Transformer follows might not be optimal. They propose to search better skip-connection architectures in a fine-tuning schema based on network morphism. To efficiently perform the searching process, they adopted hill climbing algorithm. They demonstrated the proposed method with experiments on WMT En-De and WMT En-Fr tasks, with a BLEU improvement of 0.3-1.0 BLEU.\n\nThe positive point of this method is that it is efficient, serving as a fine-tuning process. The method also achieves decent quality improvement, 0.3-1.0 BLEU.\n\nThe limitations of this paper:\n\n1)\tThe proposed approach only serves as a finetuning process. It can add new skip connections but not delete existing skip connections. This limits the search space, also upper bounding the methods’ capability.\n\n2)\tThe authors only experimented with post-norm Transformer, ignoring the pre-norm Transformer which has been proven effective particularly in deep settings [1, 2]. In pre-norm Transformer, each layer’s representation is a direct summation of all previous layers. Thus, adding new skip-connections might be less beneficial.\n\n3)\tThere are almost no ablation studies for the Algorithm 2, which lays out the foundation for the searching procedure. For example, what is the impact of those n_* hyperparamters on final translation quality? What if we only add skip connections to the encoder, or to the decoder, or to the encoder-decoder part? In addition, how stable is this algorithm? Since hill climbing is a random algorithm, can we get similar skip connection structure and model performance after each run?\n\n4)\tWhat’s the impact of F and S in Algo. 1 on translation quality?\n\n5)\tThe proposed method is deeply dataset dependent. Based on results in Table 5, the learned new architecture is not universal, hardy being used for other translation tasks and datasets. Specially, training new structured Transformer from scratch, with static mode, always delivers worse performance compared to the original baseline.\n\n6)\tThe largest quality improvement reported in this paper is ~1.0 BLEU, which can be observed in Table 1. Based on the description, baseline results in Table 1 are copied from (Edunov et al., 2018) which is trained with 24M back-translated pairs. However, the results for the proposed model are obtained on top of 226M back-translated pair. This large improvement is unfair! In a fair comparison in Table 2, the improvement is only 0.4 BLEU. Thus, overall, the proposed model can achieve at most 0.4 BLEU improvement, which is not very appealing.\n\n\n[1] Wang et al., Learning Deep Transformer Models for Machine Translation. ACL 2019\n\n[2] Zhang et al., Improving deep transformer with depth-scaled initialization and merged attention, EMNLP 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}