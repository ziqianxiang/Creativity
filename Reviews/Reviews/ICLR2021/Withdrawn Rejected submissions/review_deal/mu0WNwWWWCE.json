{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting controlled experiments but the main problem tackled requires further motivatio",
            "review": "\nThe authors aim to understand several aspects of a variation of the\nproblem of incremental learning.  In this incremental learning,\nnonstationarity is not a main issue (ie, the distribution of instances\nfor each class is not changing), except that the classes themselves\nmay not be presented all at once to the learner (class incremental\nlearning). In an extreme setting, instances for each class are shown\nseparately in a sequence with no intermixing.\n\n\nSome findings are interesting, for example, the importance of learning\ngood representations for not forgetting, which as the authors state,\nsomewhat intuitive or plausible.\n\nThe paper is written clearly, but certain important aspects seem to be\nmissing. See below.\n\n\nFundamentally, I found the problem setting and approach somewhat\nunrealistic: why not train on all instances, periodically? is time an\nissue or is it space, or both?  Why not train on a subsample (rather\nthan keeping exemplars), then? Comparisons to such sampling\napproaches? The motivation for class incremental learning appears weak\nto me (I looked at the citations briefly as well). (note: comparison/analogy to \nhuman learning is not so relevant in my opinion, since much of the approach such \nas the semi-batch training and keeping of exemplars may not be biologically supported   )\n\nRelated to above: what is the specific constraint (budget) on the\nnumber of exemplars (total or per class) in the experiments? This is\nnot clear from the paper.  I expect at minimum 100s to 1000s of\nexemplars per class allowed to perhaps make the problem relevant to\nreal-world settings, otherwise the empirical generalizations\n(findings) of this work may not hold.. (in smaller settings they may hold, but not\nin large-scale experiments, or vice versa )\n\nIf the practical domain of application requires, say, much fewer\nexemplars (training instances), please give examples of such\napplications and explain the constraints.\n\nOf course, the number of classes is also relevant to memory\nconstraints.  One has to go to the supplements or citations to figure\nthis out.. (except one indirect mention: there is a mention that\n100 classes are combined into 20 classes)\n\nIn summary, the premise of 'class incremental' learning appears\nweak to me. In practice (in vision applications, ..), most often labeling is\nmanual and highly time consuming, and that's the major bottleneck (getting\nsufficiently many accurate labels on many classes). As the labels\narrive, training or retraining (time and space, batch (re)training) is\nnot the issue with existing batch training methods and computational power.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review Essentials for Class IL",
            "review": "Paper investigates essential parts of class IL methods. Based on their analysis they propose a simple IL method that obtains remarkably good results. The separate decisions are well motivated and the paper shows a genuine intention (and success) to understand the reasons for catastrophic forgetting and the working of the techniques to mitigate them. I found the discussion of inter/intra-task learning and the proposed adapted cross-entropy losses insightful. Also, the proposed analysis of the effect of overfitting to incremental learning is insightful. The paper does not have a major contribution, but the combined improvements show excellent results. \n\nConclusions: I appreciate the effort of the authors to shed light on the mechanism involved in class-IL. I think this paper is a good contribution to improving our understanding. The novelty of the proposed method is rather small because there is no main contribution. However, overall I would still support the publication of this paper.\n\nPros:\n-\tPaper contributions to a better understanding of class incremental learning, its problems, and potential directions of solutions.\n-\tProposed CE losses show to be effective, simple, and are clearly motivated.\n-\tThe proposed analysis of the effects of overfitting and regularization is of interest (and also the newly proposed metric for forgetting of feature extractor). \n-\tResults are excellent and compared to relevant methods.\n\nNegs:\n-\tThere is no single main contribution, but rather a broad analysis with several minor improvements. \n-\tI would also like to see results with CIFAR-100 starting from 10 base classes (some conclusions could be different for this more challenging setting).\n\nOther remarks:\n\nPaper uses random exemplar selection, but do use a non-random mechanism to remove exemplars based on distance from the class mean. I would like to see this ablated. Did the authors found this to work better than herding (which normally works a bit better than random)?\n\nIf possible the results of Fig 3b with distillation loss would also be appreciated. \n\nRelated work: the paper should include a discussion of gDumb paper (ECCV2020) which in some ways is similar in that based on an analysis it proposes a new baseline method for the field. Comparing to this method would increase the impact paper. I do believe the analysis of the current paper to be complementary to gDumb. \n\nAny reasons why the performance gain is smaller on ImageNet ?  \n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good performance but little novelty",
            "review": "Summary:\nThis paper proposed a simple incremental-class learning method that consists of several training tricks. The paper also showed the correlation of the “secondary information” (the second largest logits), “feature quality” and incremental learning performance. Although good performances have been achieved, the proposed method lacks novelty and these tricks have been explored in previous works e.g. [a,b,c,d] (see Cons). \n\n\nPros:\n1. The paper tried several training tricks and achieved good performances.\n2. The proposed secondary information is interesting. Experiments show some correlation between the secondary information and incremental learning performance.\n\nCons:\n1. The proposed forgetting metric measures the gap between incremental learning model performance and the upper-bound (trained on whole dataset). It's not suitable to name it forgetting, because the incremental learning model maybe never learned some knowledge.\n\n2. Sec 4. “Intra-task Learning” claims that learning the classifier for new classes independently effectively eliminating the weight bias. It would be better to give an ablation study to show its effectiveness when the KD loss has been used. \n\n3. These “Intra-task Learning”, “Inter-task Learning” and “Transfer Learning” tricks are not novel. Some previous works [a,b,c,d] have already applied such tricks.\n[a] A Simple Class Decision Balancing for Incremental Learning (“Inter-task Learning”: train on new classes separately)\n[b] Continual Representation Learning for Biometric Identification (“Inter-task Learning”: train on new classes separately)\n[c] End-to-End Incremental Learning (“Inter-task Learning”: train the model on the down-sampled balanced exemplar set, “Transfer Learning”: reduce the learning rates for incremental steps)\n[d] GDumb: A Simple Approach that Questions Our Progress in Continual Learning (“Inter-task Learning”: train the model on the down-sampled balanced exemplar set)\n\n4. The claimed new direction that “has not yet been addressed in literature - aiming to further improve the feature representations for class-IL” has been explored in [b] which aims to learn better representations and improve model’s generalization ability incrementally. \n[b] Continual Representation Learning for Biometric Identification\n\n5. I am confused with Eq (1). f_k(x) is the k-th logit, so it is a scalar. How can the secondary-class softmax function be computed on the scalar?\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Overall the novelty of the paper is minor；Some of the findings depend on experimental settings, which might not always be correct.",
            "review": "Summary:\n\nThe paper tackles the catastrophic forgetting problem for class incremental learning. The paper proposes to change the softmax function span over network outputs for different loss terms. Additionally, the paper studies different common regularization techniques and finds the secondary class information helps on class incremental learning. The experimental results show the effectiveness of the proposed method.\n\n\nConcerns:\n\n- Figure 1 can be misleading. According to section Inter-task Learning, “we compile an exemplar set which contains equal numbers of samples from all classes including old and new classes”, however, it is not clear in Figure 1, since old data is denoted as $P$, which does not include new classes. \n\n- One point the paper made is to lower down the learning rate during incremental learning. However it relies on the assumption that the pretrained model is good enough. In the experimental setup, half of datasets are used as the first task, it makes sense that lower learning rate can reduce overfitting if you only consider overall accuracy, where the first task has the most weight on it. It would be interesting to see the performance in the other common setting where each task has an equal number of classes. In this setting, a lower learning rate could be harmful to learning the new task.\n\n- It is not well motivated and not well explained why secondary information is a good for evaluation of class incremental learning.\n\n- Overall the novelty of the paper is minor and some sections of secondary information are not easy to follow.\n\n\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}