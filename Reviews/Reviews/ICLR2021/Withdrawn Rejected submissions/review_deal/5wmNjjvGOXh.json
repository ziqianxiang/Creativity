{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors introduce an approach to train sparse RNNs with a fixed parameter count. During training, they allow RNN layers to have a non-uniform redistribution across cell weights for a better regularization.They also introduce a variant of the averaged stochastic gradient optimizer, which improves the performance of all sparse training methods for RNNs. They achieve state-of-the-art sparse training results on Penn Treebank and Wikitext-2.\n\nThe method achieves very good performance on sparse RNNs for challenging tasks. The paper is well written and provides solid analysis with new insights into sparse network models. Most reviewers believe it is a very solid paper.\n\nHowever, the technical novelty of the paper is limited. It can be seen as some tweaks and improvements of existing techniques, which seem to work very well. Since the number of papers that can be accepted is very limited, and since technical novelty is an essential criterion for published papers at ICLR, I propose rejection.\n\n"
    },
    "Reviews": [
        {
            "title": "minor technical novelities that lead to improved performance over state of the art",
            "review": "The paper claims that the previous sparse training methods mainly focus on MLP and CNN, and fail to perform very well in RNNs. Hence, the authors proposed an approach to train sparse RNNs with a fixed FLOPs budget.\nThe proposed technique is based on defining a mask matrix $M$ and refining it during training. It is initialized randomly to have the desired sparsity level $S$. After each training epoch, a fraction $p$ of the weights with the smallest magnitude is removed, i.e., those locations are zeroed in the mask M.\nNext, the same amount of parameters are randomly added to M again. \nMoreover, a variant of the averaged stochastic gradient optimizer (SNT-ASGD) is developed for the training of sparse RNN to account for the effect of weight masks during training.\nThey showed that in practice, the requirements for efficient sparse training of RNNs are different than CNN and MLP.\n\nStrengths:\nBy adding some refinements and tweaks to the existing techniques (masking for sparse training and adapting the NT-ASGD), the authors were able to achieve good performance to train sparse RNNs. The paper has a rather extensive set of simulations and experimental setups to analyze the best setup which yields good sparse training, e.g., comparing uniform vs ER distribution for masks, sensitivity to hyperparameters, ... Moreover, they have considered a fairly diverse set of RNN architectures to evaluate their method.\n\nWeaknesses and questions:\nCompared to the existing methods, the technical novelty of the paper is minor. It can be seen as some tweaks and improvements to the existing ones (although I admit that those changes are essential for the method to work for RNN.). \nWhat is special about the method that makes it specific to RNN? In other words, is it possible to use the same method for sparse training of MLP and CNN?\nA minor issue with the paper is the FLOPS analysis the authors used. Effectively, they use the sparsity of the parameters as a measure of FLOPS, not the actual FLOPS that might depend on the sparsity structure, HW, or software implementation. It would be a good idea to directly mention and use total sparsity, instead of FLOPS which can mislead the readers.\n\nSome parts of the method are not clear enough, e.g., \n1. In the paper, it is stated that \"magnitude weight removal\" is applied to non-RNN layers. Do the authors mean that for the parameters of RNN, this step is skipped?\n2. In \"cell weight redistribution\", it is suggested that the \"magnitude weight removal\" is applied to the whole set of RNN parameters $\\{\\theta_1, \\ldots, \\theta_t\\}$. However, in \"random weight growth\", it is mentioned that the same number of weights is grown immediately after weight removal, i.e., $R$ and $P$ have the same number of 1's. So, does it mean that the number of 1's in mask $M_i$ for each weight $\\theta_i$ ($1\\leq i \\leq t$) remains fixed S during training?\n3. Another aspect of training that is unclear for me is the parameters that are updated. Is $\\theta$ updated during training or only $\\theta_s$ is updated? As a result, if a weight is removed in one epoch and its value at the time of removal was $\\alpha$, and later regrown at another epoch, is its initial value set to 0 or started from its previous value before \"weight removal\", i.e. $\\alpha$?\n4. Did the authors add any regularizer (e.g., $\\ell_1$) to the training loss to improve sparsity in their experiments?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper that proposes an approach to train sparse recurrent models, and a sparse variant of the NT-ASGD.",
            "review": "In this paper, the authors propose an approach to train sparse recurrent models, and a sparse variant of the NT-ASGD. The proposed method mixes some interesting novel methodologies and achieves interesting empirical results on Penn Treebank and WikiText-2 language modeling tasks. \nIn general, the paper is well written and interesting, but in section 3 many explanations about the rationale behind some architectural choices of the selfish-RNN methodology are only partially explained, and sometimes they are just related to empirical results (e.g. in the cell weight redistribution). To me, a more theoretical explanation would significantly improve the manuscript readability.\nIn section 4 many different approaches were considered. But there are a few points that are not clear. The authors report the results of a “small\" dense network, but no information about this model is reported in the text.\nReading the results reported in table 5 of the appendix, I found it interesting that the performance of the DSR improves significantly by using SNT-ASGD instead of Adam  (it outperforms the Selfish-RNN). This table shows how much the optimizer influences model performance.  Even if the ablation study reported in appendix A highlights the benefits of the SNT-ASGD, the results reported in table 5 show that the impact of this component is even more important than the selfish-RNN. Honestly, I think that is fairer to compare all the methods using the same optimization algorithm, therefore my suggestion is to move this table in the main paper and extend the analysis of these results. \nReading the manuscript it is not clear how the hyper-parameters considered in the experimental campaigns have been chosen. By reading the first part of section 4.1 seems like parameters like the removing rate or the number of epochs are set without performing any validation on them. Even in appendix D, hyper-parameters (e.g. the learning rate, or the batch size) used to test the RHM are just listed. The authors should insert a more extensive explanation about how the hyper-parameters various models/approaches considered in the comparison have been validated. To perform a fair comparison the hyper-parameters of each model should be chosen according to its performance on the validation set.\nIn this regard, it is important also to highlight how the hyper-parameters are chosen because some SOTA models achieved better results. For instance on the Penn Treebank dataset in “On The State Of The Art Of Evaluation In Neural Language Models”, Melis et al. report perplexities on the test set of 59.7.\nexploiting better the research space. The reported results in the paper (and in Appendix L) show the benefits of using this approach, but honestly, to me, it is not clear if it helps in exploring the state space. In general, it is not clear what is the reason why the model benefits from using the random growth approach. Moreover, in “Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware” the gradient guided growth strategy outperforms the other sparse training approaches considered in the paper, even in the RNN case. Therefore a more extended evaluation/discussion of this point is required.\nAnother recently proposed approach that uses sparsity in recurrent models is defined in “Intrinsically Sparse Long Short-Term Memory Networks” by Liu et al. the author should compare this approach with the selfish-LSTM.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very solid paper on sparse training of RNNs which presents insights valuable for important questions on sparse training.",
            "review": "Summary: \nThe authors improve sparse for recurrent neural networks by developing a greedy redistribution rule for gates and adapting the ASGD optimizer for sparse networks. The work provides good results and a rich analysis of their and related methods.\n\nStrong points:\n- very rigorous experimental setup and analysis\n- Solid evidence for many new insights into some sparse training phenomena. The work provides broadens our understanding of sparse training.\n\nWeak points:\n- Some might complain that RNNs are outdated. I see this only as a minor weak point. Indeed, RNNs are not much used anymore, but many of the insights the paper provides are quite universal.\n- The fixed FLOPS only seems to be a by-product of the algorithm and particular network structure but not necessarily an algorithmic contribution. This makes the paper a bit confusing.\n\nRecommendation (short):\nThis is a very solid paper with exemplary experimentation and analysis. It provides many unique insights that are very valuable for anyone who wants to work in the field of sparse training. I recommend accepting this paper.\n\nRecommendation (long):\nI think this paper is one of these papers, which is a very solid all-around. The authors invested quite a bit of time in creating rigorous experimental setups that test hypotheses. In particular, I like the graph analysis of sparse connective between networks. Findings of different initialization schemes and performance of other sparse training methods are precious and make the overall literature on sparse training robust. I can see that this paper may seem a bit boring and less impactful to some reviewers, but good science like this is not about being exciting but about providing rigorous results for a small problem. This paper does exactly that. I think any good conference should encourage good science by accepting papers like this one.\n\nComments for authors:\nSolid work. Here some additional comments and questions.\n- Please feed your paper through a grammar/spellchecker. There are multiple errors which make the paper hard to read in some sections\n- It is not entirely clear why ASGD is needed for good performance. Can you elaborate, please?\n- Do you have any idea how does ER initialization relates to eigenvalues of recurrent matrices? If you can make a connection here, it would be a quite insightful addition to the paper since the top eigenvalue of the recurrent matrix determines the overall long-term behavior of the recurrent matrix and is known to influence behavior.\n- I would drop the fixed FLOPS contribution and focus on the other parts of the paper. You have more than enough contributions, and the space is better devoted to making the other contributions as clear as possible.\n- The cell weight redistribution algorithm description is unclear. A weight cannot have \"more parameters, I think you mean to say gate-neurons with large magnitude weights gain more parameters over time.\n- The sparse topology algorithm: Is the correlation between weights computed overall test set outputs between two networks/weights?\n- Figure 3, unclear. What does Figure 3 (left) show exactly? It is unclear what random initialization means: different sparsity patterns, different weight values, or both? What does the seed do here? Does it affect sparsity pattern, data order, weight values, etc.?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "lack of some important information",
            "review": "In this paper, the authors studied the possibility of sparsity exploration in Recurrent Neural Networks (RNNs) training. The main contributions include two parts: (1) Selfish-RNN training algorithm in Section 3.1 (2) SNT-ASGD optimizer in Section 3.2. The key idea of the Selfish-RNN training algorithm is a non-uniform redistribution across cell weights for better regularization. The authors mentioned previous sparse training techniques mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs) rather than RNNs. This claim seems to be doubtful because one-time SVD + fine-tuning usually works very well for most RNN training applications in the industry.\n\nOverall, this paper is carefully written and provides some interesting empirical results. However, due to the lack of some important information, it is hard to evaluate the contribution of this paper.\n\nHere are some of my questions.\n\nSNT-ASGD needs to save the weights w_i,t from iteration Ti to iteration K, will that cost additional memory?\n\nThe authors mentioned that they picked Adam optimizer for SET, DSR, SNFS, and RigL. Is Adam the best optimizer to build a strong baseline? I suspect Adam may not be the best optimizer for each of them.\n\nThe authors need to give more information on the hyper-parameters like the learning rate. The selection of hyper-parameters usually significantly affects the convergence/generalization performance of an RNN model. For example, the way of learning rate decay has a big impact on the performance of training Penn TreeBank dataset.\n\nCan the authors report the training epochs and wall-clock time (e.g. in Table 2)? The sparsity typically makes modern hardware like GPUs perform poorly. That may be a concern. That’s the reason why researchers are studying structure sparsity. For future work, an analysis of computation (flops) to communication (memory access frequency) ratio seems to be necessary.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}