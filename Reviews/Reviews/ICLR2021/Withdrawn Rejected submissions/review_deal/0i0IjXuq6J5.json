{
    "Decision": "",
    "Reviews": [
        {
            "title": "A theoretical extension for contrastive learning",
            "review": "Objective of the paper: The paper provides theoretical guarantee of self-supervised training.\n\nStrong points: The paper extends previous result to training with multiple negative samples and for multiway classification. They also develop an convergence guarantee of unsupervised training based on overparameterization.\n\nWeak points:\n1. The extension to multiple negative samples is incremental given the proof and techniques developed in Saunshi et al. 2019. The paper also does not well justify why this extension is important for either practice or theoretical development.\n2. The convergence proof is based on Theorem 6 of Allen-Zhu et al. 2019 and there is no substantial improvement. Moreover, it is believed that the neural tangent kernel does not reflect the true neural network training.\n3. The experiments are only verify the theoretical claims and provide few guidance for practice. \n\nI do not recommend the publication.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Comments ",
            "review": "This work provides a theoretical analysis of contrastive learning. Specifically, based on existing theory in (Saunshi et al. 2019), the authors first establish the relations between supervised loss and unsupervised loss. Then based on the results in (Zhu et al. 2019), the authors claim that the network with contrastive loss can converge. \n\nIn overall, this paper is a little incremental in terms of the results and proof framework. Specifically,   Saunshi et al. 2019 already established the relations between supervised loss and unsupervised loss when the loss only involves one positive and one negative sample. See their Lemma 4.3. This work extends this result to multiple negative samples and obtain very similar results. In this way, the results in this work actually do not reveal new insights. Moreover, the extension in proof actually uses almost the same proof framework. \n\nFor the convergence, the authors need to assume that the pair (x_i, x_i^+, x_i^-) is fixed. That is, for sample x_i, its augmented data are fixed as (x_i^+, x_i^-). However, contrastive learning actually uses random augmentations and obtains different augmented data for each sample per iteration. This is also the key to why it can works. If one fixed the augmentation, the algorithm will fail. The benefit of fixed augmented data (x_i^+, x_i^-) is only convenient for borrowing the convergence results in (Zhu et al. 2019), since it means that the target of sample x_i is fixed and thus can almost directly use the existing convergence results.\n\nFinally, for experimental results, the statement that supervised and unsupervised losses have similar behavior is a little overclaimed. For me, both losses just normally decrease. How to justify their similarity? \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper is not yet for a conference submission",
            "review": "First of all, I really appreciate the effort of the authors in understanding the convergence of contrastive learning towards classification and generalizing prior work. Nonetheless, the current presentation is below the bar for paper acceptance. \n\nSpecifically, the paper assumes the readers have a 100% understanding of prior work [1,2]. In the current presentation of the paper, we need to do back and forth between the manuscript and the prior work, which makes the paper lack readability. Specifically, a paper should be self-contained but not relied on the assumption that readers have the prior work side by side. I don't think the paper is ready for submission with the main complaint on the difficulty to fully understand its conveyed messages. It is a pity that even if the paper has a great contribution and impact, but because of its presentation, we can't distinguish the contribution.\n\nLast, I like to emphasize that I do not intend to be picky but would like to encourage the authors to take time polishing the paper.\n\nMore precisely, I would suggest the author:\n1. Have a separate section discuss the prior work [1,2]. Put the equations, lemmas, definitions and theorems from the prior work in this section. Then, discuss their limitations.\n2. The new section begins with the main theoretic results. Then, give an insight into what the new results tell us. Later comes with the details. \n\n[1] A theoretical analysis of contrastive unsupervised representation learning, Saunshi et al.\n[2] A convergence theory for deep learning via over-parametrization, Allen-Zhu et al.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice results",
            "review": "In this paper, the authors propose some theoretical analysis on contrastive learning. The results can be summarized in three-fold:\nThis paper provides an analysis of the number of negative samples: with the number of negative samples increases, the bound between unsupervised loss and supervised loss becomes tighter.\nThis paper provides a bound when the supervised loss is evaluated on a subset of the training data. This bound enables us to use out-of-distribution samples to improve self-supervised learning.\nWhen the encoder is a deep neural network, Theorem 1 guarantees that the encoder can find the parameters that achieve arbitrarily small errors.\n\nSince I’m not an expert in this field, I didn’t carefully check the correctness of the theoretical analysis. Assuming the correctness of the theorem, this paper provides an important explanation about the effect of the large number of negative samples, and the convergence proof. I personally enjoy the proposed results.\n\nFor some minor suggestions:\nMore details about the previous methods can be included in the main body. Both the background and the difference can be better illustrated.\nActually, I didn’t find the connection between Sec 3 and Sec 4. Indeed, both these two sections provide important results. But what’s the connection? It seems that the authors simply combine the two results into one paper.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}