{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work proposes to uses an energy-based objective combined with generative adversarial networks for imitation learning. While most reviewers find the work easy to follow and come with theoretical justifications, albeit mostly followed from previous works, and good coverage of experimental results, all of them raised questions regarding the limited novelty and added contribution of the work, and missing more recent baselines. Please consider address these feedback in your future submissions."
    },
    "Reviews": [
        {
            "title": "Lack of novelty and rigor",
            "review": "This paper proposes a learning framework for imitation learning (IL) that uses an energy-based objective for generative adversarial imitation learning (GAIL).\n\nPros\n+ Clarity. The paper is easy to understand and follow.\n+ Theoretical analysis is sound (not novel however -- see below).\n+ Benchmarking on sufficient number of environments that are standard in the literature. The baselines are however insufficient (see below).\n\nCons\n- The major concern with this work is a severe lack of novelty. There are thousands of follow-ups to GANs. In GAIL, it was shown that GANs can be applied to imitation learning. Repeating a GAN work in the IL context consequently should offer significantly more algorithmic/empirical/theoretical insights than the current work.\n- Relatedly, the paper lacks discussion and acknowledgement of related work. The most-closely related work besides GAIL is EB-GAN (Zhao et al.). This work has been cited but not discussed anywhere. Moreover, the references to this work are in the appendix. Theorem 1 essentially  follows directly from Lemma 1 and 2 (proved by Zhao et al) and Theorem 1 of the original GAN paper (Goodfellow et al.). It is unclear if there is anything novel in the analysis or a conclusion that is particularly novel to imitation learning. The subsequent analysis on occupancy measure matching also follows directly from GAIL.\n- On the empirical end, the baselines are very weak in the context of the current work. Just like the literature on GANs has come a long way for improving training stability and mode coverage, the benefits of these GAN++ frameworks have also been shown in the context of imitation learning. For example, the InfoGAIL work by Li et al. shows the benefits of using the WGAN objective for IL and would have been a stronger baseline for empirical comparisons.\n\nIn summary, the work falls in short justifying the merits of EB-GAIL in the context of our existing understanding of imitation learning due to insufficient experimentation and theoretical/algorithmic analysis.\n\nMinor\n- Please use \\citet and \\citep appropriately for improving readability of references.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "More experiments are needed",
            "review": "Strong points:\n1. The authors propose an auto-encoder network to replace the discriminator in the GAIL model. The authors also provide some theoretical analysis to show the proposed measure can match the occupancy measure of the expert policy.\n2. The authors conduct experiments on different tasks to show the proposed method can outperform GAIL, BC and GCL.\n\nWeak points:\n1. Some key baselines are missing. There are a few methods that have been proposed and proved to outperform GAIL and GCL. Here, we just show one example. For instance,\n\nFu, J., Luo, K., & Levine, S. (2017). Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248.\nThese methods should be compared.\n\n2. It is better if the authors can explain why using auto encoder as the discriminator can outperform the original GAIL. \n3. The experiments are conducted under relatively small data (usually a few trajectories). Is there a reason? If sufficient data is provided, will the proposed method still perform the best? Is the proposed method specially designed for this small data scenario? The authors may want to explain these questions in more detail.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting result suffering from unclear presentation",
            "review": "The authors propose a discriminator-based approach to inverse reinforcement learning (IRL). The discriminator function is trained to attain large values (\"energy\") on trajectories from the current policy and small values on trajectories from an expert policy. The current policy is then improved by using the negative discriminator as a reward signal. The specific discriminator suggested is an autoencoder loss. The authors continue to provide a proof that assuming their discriminator/generator attain a Nash equilibrium, the occupancy measure of the trained policy matches that of the expert policy. They follow up with demonstrating better performance of their approach compared to certain baselines when tested on a number of tasks on Physics simulators.\n\nI believe this paper has potential. Unfortunately I don't think it is publishable in its current form, mainly due to stylistic issues and small errors collectively grave enough to distract from understanding. The language is often unclear due to the unusual grammar, bibliographic references are inserted into the text without any distinction from the surrounding text, where clearer, the language is often a bit too casual for a scientific publication, and there's other small errors that hinder understanding. For example, I believe the above description (discriminator is trained to be small/negative on expert trajectories and large on trajectories from the current policy) is correct, but it's not entirely clear from the paper as e.g. the listing in Algorithm 1 seems to indicate the opposite (comparing the formula in point 4 there with equation (2))? On reading this, I first wondered what happened for negative values of $D$, but equation (4) seems to suggest $D$ is positive at all times? This should be spelled out more properly, and $D$ should be introduced before it is first used in equation (2). I also couldn't find any description of the neural network architecture used in the experiments (the functional form of the final layer would also have cleared up my understanding of what kind of values $D$ produces).\n\nI'm also unclear about the blanket statement \"One fatal weakness for behavioral cloning is that these methods can only learn the actions from the teacher rather than learn the motivation from teachersâ€™ behaviors.\" Recent breakthroughs such as \"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II\" employing a version of behavioral cloning (specifically, \"kickstarting\") seem to suggest otherwise, so this statement should be given some kind of supporting reference. The same goes for the statement \"These methods suffer from the problem of compounding errors (covariate shift)\"; I have heard the term \"covariate shift\" before but I am not truely certain what it _means_.\n\nIn addition to the above, I suspect that the comparison to known baselines is not necessarily the most interesting setting to evaluate the proposed method. However, that is a minor point given the current state of this paper.\n\nThat said, I believe the proposed method is likely to be a good one and will make a worthwhile contribution to scientific progress on IRL. I would encourage the authors to invest the time to polish this submission, ideally also getting it proofread by a diverse set of colleagues. As far as I could tell, the mathematics seems correct and the underlying idea is sufficiently substantial to be published in at a ML conference once it's clarified and the presentational issues are resolved.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review for Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning ",
            "review": "Summary\nThe paper proposes a new method, called EB-GAIL to deal with the Inverse Reinforcement Learning problem. EB-GAIL is an architecture for training generative adversarial imitation learning, where the discriminator is an autoencoder with the energy being the reconstruction error.\n\nStrengths \nThe experiments are promising since the algorithm performs well in almost all experiments.\n\nWeakness\n\nThe novelty of the article is unclear. Theoretical results are marginal as they are mostly from previous works.\nIt is hard to understand the proofs. \n- What is $R_{\\phi}$ in equation 16? I think that it is the same quantity as in the Generative Adversarial Imitation Learning paper.\n- What is d in equation 17?\n- I think there is an error in the second line of equation 21: a $\\le$ is $\\ge$\n\nThe paper overall is not well written. The Background section is confusing: it starts with an introduction to RL, then there is a subsection related to IRL, and at the end another subsection to imitation learning, where is another explanation of IRL. I would suggest changing this section into RL, Imitation Learning (IRL + BC). \n\nWhat are the main differences between this paper and some new works as Energy-Based Imitation Learning [1] or Strictly Batch Imitation Learning by Energy-based Distribution Matching[2]?\n\n\nMinor comments\n- the citations are all without parentheses\n- the reward is defined as a function R: S $\\rightarrow$ A\n- the discounted future reward has to be in expectation under the initial-state distribution, the transition model and the policy\n- it is missed a citation to behavioural cloning\n- it is missed a comparison with \n\n\n\nFinal comment\n\nThe document is unclear, some sections are confusing. The idea is interesting, but the paper must be edited before it can be published.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}