{
    "Decision": "",
    "Reviews": [
        {
            "title": "A decent work with a reasonable idea and solid experiments",
            "review": "This paper proposes to train NMTs with the self-supervision that the partial models with the shared structure should generate the same outputs. On the task of question answering, they explore three different ways to generate paired training examples with shared substructures and demonstrate all of them lead to improvement on both in- and out-of distribution test data on DROP dataset.\n\nThe main idea is reasonable and can be applied to many models in which the computation is dynamic and corresponds to some latent symbolic structures. The experiment results also validate the effectiveness of the proposed approach. The experiments of faithfulness and compositionally provide some intuition on how this pair loss could help the training of NMTs. \n\nIn general, I think this is decent work without apparent disadvantages. \n\n===================================================================\nAfter reading other reviewers' comments, I degrade my rating to 6 due to the concern of experiments and applicability. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Anonymous Review",
            "review": "This paper studies latent decision model for language understanding and text question answering. This work introduces a new training objective that maximizes the consistency between pairwise samples that have similar latent structure. This work also proposes ways to acquire training data for the pairwise consistency objective. The proposed model is evaluated on the recent DROP dataset, achieving superior performances over itself without the pairwise objective, and a recent baseline MTMSN. The paper is very well written and the results are clearly presented.\n\nThere are a few things on which I would suggest further improvement and clarification:\n\n1. The experiments impose strong condition on the dataset and model, and is only performed on a subset of the DROP dataset due to this reason. Given the claim made in this paper, it remains questionable how this model extends to boarder scenarios. \n\n2. It is unclear whether the improvement comes from more training data or a better objective. Are the baseline models leveraging the extra data that the authors gathered for consistency training? If not, then I will suggest further investigation under stricter benchmarking of the data usage. To my opinion, the current experiments are not sufficient enough for establishing the claims.\n\n3. On a higher level, adding the consistency loss will inevitably require extra labels (or efforts to generate these labels). What is the advantage of baking the consistency relation into a learning based model rather than directly applying the mechanism that gives you these labels in the first place? For example, why not directly compare to BERT ; or why not impose the heuristics to directly solving the task. Any justifications?\n\nBased on the above assessments, I suggest rejection of this paper. Even though I think this paper tackles a very important task from a promising direction, the current experimental evaluations need to be stronger to be more convincing. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns about the significance and applicability of the work",
            "review": "This paper presents a method to leverage paired examples that share internal substructure and enforces the consistency between the internal representations. The solution is straightforward: (1) If two examples share a sub-module, then the probability distributions of the output modules should be similar, hence an auxiliary loss of KL divergence is added to the original training objective. (2) They also explored several ways to generate paired examples, either by leveraging existing examples from the training set or constructing them using templates or a question-generation model.  The approach is applied to the textual neural modular network (text-NMN) on a subset of the DROP dataset. The results show that 1) it improves over its own baseline marginally in in-distribution evaluation; 2) the model can obtain better faithfulness scores and achieve better performance in compositional generalization settings.\n\nOverall, I am ambivalent about this paper. The paper is easy to read and well-written and the general direction (enforcing consistency of latent decisions leads to better compositional generalization) is appealing. On the other hand, I am concerned about the significance of this work: The approach is straightforward and not particularly new (this doesn’t have to be a major weakness though) and the results are evaluated on only one dataset with gold programs provided. I think it largely limits the applicability of the approach. It is also hard to evaluate the experimental results and put them in context (as there is really no prior work to compare to). I will elaborate on these points more below. \n\nAs the paper puts, one major difference of this approach compared to prior work (e.g., Li et al 2019, Asai and Hajishirzi, 2020) is that this approach is operated at intermediate latent decisions whereas previous work only operated at outputs. However, this paper used gold program annotations to identify paired examples and to train the model. Why can’t we just think of gold programs as outputs then? I would be much more convinced if the gold programs are not used and some consistency losses can be still enforced on latent decisions. Also, when finding naturally paired examples, it is constrained to programs that share a leaf *find* module (similarly for the templates used in generating paired examples). I think these constraints are all too specific and tailored to one single dataset. \n\nIt is also a bit difficult for me to interpret the experimental results.\n- Table 1: is it a fair comparison to (Hu et al, 2019)? I believe their work didn’t use gold programs.  PS. In the Dev + EM column, 62.4 is higher than 61.6 should be written in bold.\n- The evaluation of faithfulness scores is also somewhat surprising to me. The faithfulness score measures whether each module can be correctly aligned to the corresponding set of spans. Isn’t this information already provided as a training signal (in gold programs)? I am not sure why adding this pairwise auxiliary loss will help. Given that the gold program is used, how if you just add an auxiliary loss in addition to the final answer prediction so that it encourages the model to output correct sets of spans in every single module (no need to use paired examples)? I could have misunderstood something..\n\nFinally, it would be really great if the approach can be evaluated in more datasets, and in the setting that no gold program is used. Otherwise, I think the general applicability of the approach is limited or unclear. \n\nMinor: \n- It took me a while to understand the notations in Figure 1. I think they need to be clarified, e.g., What does “g(x2,[13:13])” refer to?\n- Similarly, explain what is “G.P.” in Table 3.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper using paired samples to improve performance, but it still has a lot to improve.",
            "review": "Summary:\nThis paper works on the compositional question answering task on the DROP dataset. The authors present a framework that exploits paired questions to improve the learning of textual neural module network (Text-NMN). Specifically, the authors propose three ways to obtain paired questions in DROP: discovering paired questions within the dataset and generate paired questions based on templates or a question-generation model. The proposed model leverages the paired questions by minimizing the KL-divergence of the output distributions of the shared module in each pair. Experiments are conducted on a subset of the DROP dataset that is covered by the modules in Text-NMN and results show that the proposed model can improve both the in- and out-of-distribution generalization and leads to more accurate intermediate predictions with better faithfulness scores.\n\nStrengths:\nThe proposed model leads to better answer accuracy and more accurate intermediate predictions compared to the baselines.\n\nQuestions:\n1. I'm confused about the notations of Equation 5 (the learning objective) . What are the \"g(q_i)\" and \"g(q_j)\" here? In the above text, it states that \"consider a question q_i that shares the substructure g(q) with a pair question q_j.\" Does g(q) denote a shared substructure or a probability distribution? In this case, isn't g(q_i) exactly equal to g(q_j) since they are shared by two questions? If so, why would the two distributions be different?\n\n2. In Section 5, the numbers of found paired questions are reported. I think it would be better also to report the numbers in terms of each module, i.e., how many paired questions share a \"find\" module or a \"filter\" module? I think the \"find\" module would dominate the numbers since it often appears in the leaf nodes of the program tree and thus is more probable to be shared by two questions.\n\n3. Are the ground-truth programs used for training? From the third paragraph of Section 3 (\"Given a question q, ... the gold program z*\"), it seems that the ground-truth programs are not used in training. However, in Section 4.1, the ground-truth programs are used to find paired questions and in Section 5, \"We only use these program annotations for training\"\n\n4. Important training details are missing. For example, how is the learning objective (Eq.5) combined with the original loss function in Text-NMN?\n\n5. The proposed method is dedicated to the compositional question answering task on the DROP dataset using neural module network and it is unclear how this proposed method can be generalized to other QA tasks. In this sense, it would be better to have a more accurate title than the current one, to avoid overclaiming too much.\n\n6. The proposed method is highly related to enforcing the consistency of question-answering models and it would be better to discuss the related works on model consistency in the \"Related Works\" section, such as [1]. I am not an expert on this topic and there might many other related papers on model consistency.\n\n[1] Are Red Roses Red? Evaluating Consistency of Question-Answering Models\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}