{
    "Decision": "",
    "Reviews": [
        {
            "title": "Unclear value of this work",
            "review": "This paper aims at introducing a metric to define the complexity of a representation of an intermediate layer in a deep network. \n\nThe paper makes a series of assumptions and definitions that are hard to follow and understand. I would suggest rewriting the paper to facilitate understanding the key parts of the paper.",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "It is an interesting work in a new view of the analysis of DNNs. I vote for acceptance.",
            "review": "Summary: \n\nThe paper provides an interesting view of visualization and analysis in DNNs. In particular, it uses the number of nonlinear layers (called disentangler net) to approximate the feature complexity by analyzing the complexity of nonlinear transformations. Then new metrics are defined to analyze the feature components in terms of the reliability, effectiveness, and significance of over-fitting. Further, the paper propagated the improvement of the performance of DNNs under these metrics by experiments. The experiments in the paper seemed to demonstrate the trustworthiness of the methods at least for these parts that agree with the common sense of the DNNs. But, the theoretical base of the work is not so sufficient. It seems that the definition of the l-order complexity of feature component $c^i(x)$ does not agree with the computation $\\Phi^(l)(x)$ in experiments.\n\n\nPros:\n\n1. The paper is well organized and written.\n\n2. The paper provides a new view of analyzing the DNNs and new metrics to help improve DNNs. \n\n3. The paper provides a lot of experiments to demonstrate the trustworthiness of the methods and also design experiments to show how the new metrics help improve the network of DNNs.\n\n\nCons:\n\n1. The idea of a disentangler net is interesting, but the theoretical derivation seems not so self-consist between the network design and its implementation. This point makes the work less convince especially when code is not available.\n\n2. Some notations are confusing and lack of clear definition.\n\nQuestions during the rebuttal period: \n\n1. Equation (2) defined in page 4 gives the computation of $\\Phi^{(l)}(x)$ and $c^{(l)}(x)$. But if we admit the definition of $c^{(l)}(x)$, the two sub equations in Equation (2) may not be consist. In other words, can you guarantee the $c^{(l)}(x)$ computed by Equation (2) satisfy its definition on the top of the same page? That is my major concern which determines the methods promising or not in the theoretical view. \n\n2. Also in page 4, the paper uses such equations $c^{(7)}(x)=\\Phi^{(7)}(x)-\\Phi^{(4)}(x)$ to compute $ c^{(7)}(x)$ which is not consist with Equation (2) and I was wondering if there was some explanations. \n\n3. The new metrics defined on page 6 seem not very relative to the Shapley value. As the most identical property in Shapley value listed in Appendix D  is that the Shapley value is the sum from all the contributions of the value differences for any subset of N with and without the i-th player. If the author claims they are relative and I am glad to see the explanation and if not I suggest the deletion of the Shapley value part for simplicity.  \n\n\nSome typos and misleading:\n\n1. The variable r repeats at different places for channel multiplier and iterations and may cause misleading. \n\n2. the operators g and h defined on page 3 and page 5 are different. On page 3, there is no explanation for g when g is introduced. On page 5, the definition of $h_k$ is misleading as different $h_k$ maps the same value $\\psi^{(l)}(x)$. From the formula above Equation (5) we can see that there is an average operation between k. It may be better if making it clear.\n\n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Ad hoc method, scores and not clear goals",
            "review": "The paper proposes a method to decompose activations of intermediate layers of feed-forward DNNs into sums of incremental contributions over their preceding layers with the main purpose of visualization as a model interpretability tool. Fitting another DNNs to reproduce the activations of the selected intermediate layer is used to define the significance and reliability scalar scores which summarize variability of the reproduced activations relative to the original activations. Score definitions, architectural and application decisions are introduced in a rather ad hoc manner, without sufficient justification. This makes the paper a confusing read, also due to missing clear statement of goals and the use of confounding terminology.\n\nThe paper could be improved along such directions:\n- more actionability in the method and experiments: e.g. what practical takeaways can a model developer have from Fig. 5? \n- more precise formulations:\n  - a statement of the practical task which is going to be solved here would be helpful. One possible way would be to be guided by the goals of experiments in sec 4; \n  - definitions would be easier to verify if they don't contain expressions like \"can be\" or \"cannot be\" (definition on p. 4). Otherwise, it has to be shown that $c^{(\\ell)}$ cannot be computed with any smaller than $\\ell$ number of layers.\n  - more justification for metrics: It doesn't follow from similar variance-based scores that the underlying distributions are similar as well (last paragraph on p. 4). Also, stability of a score is taken as evidence of (undefined) trustworthiness which could mean, ad absurdum, that an almost constant $c^{(\\ell)}$ is the most trustworthy feature component. In general, interpreting intermediate representations may be hard for multi-layered networks, since layers could, theoretically, \"conspire\" and communicate in an non-interpretable code between them (for a different application, http://arxiv.org/abs/2006.01067 shows an example construction).  \n- improved terminology use:\n  - \"disentanglement\" is used in the meaning that seems to be closer to an additive decomposition, rather than to the often used meaning of recovering  generative factors that produced data (see http://proceedings.mlr.press/v97/locatello19a.html inter alia). \"Significance score\" $\\rho$ could be conflated with the statistical significance.\n\nMinor remarks:\n- consider adding a superscript $\\ell$ on $f$ since $f$ is layer-dependent   \n- measures relative distribution -> measures relative dispersion?\n- consider renaming section 3. \"Algorithm\" is a pretty narrow technical term to encompass the section's content.\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting metric, but effectiveness is unclear",
            "review": "**Summary**\nThis paper presents a method to measure the complexity and significance of features (maps) produced by an intermediate CNN layer.\nThe proposed method applies a knowledge distillation approach to decompose the feature map in terms of complexity by using multiple CNNs of various depths which accordingly exhibit various complexity levels.\nBased on so-decomposed complexities, it also measures some characteristics such as contribution and reliability of the feature map by means of classification loss and variance of feature distribution.\nThe experimental results on Cifar-10 and several FGVC datasets demonstrate that the proposed method is capable of analyzing feature maps from those perspectives.\n\n**PROS**\n+ The authors present several measures to characterize feature maps of CNNs.\n+ The feature maps of VGG and ResNet models are empirically analyzed through the proposed measurement.\n\n**CONS**\n- The effectiveness of the method is unclear.\n\nThe reported results are neither so surprising nor novel.\nIt is unclear what the \"new\" perspective is. We demand a general way to actually analyze features such as for understanding how the deep CNNs work.\nOr, how does it effectively contribute to performance improvement? The results in Fig.9 are solely due to the knowledge distillation.\nIt lacks in showing generalizable metric to determine the depth of student network.\n\nThough the authors analyze relationship between the characteristics of features and classification performance through an empirical regression task, it is also unclear what is the qualitative conclusion/analysis about the relationship beyond the ad-hoc regression performance.\nJust for improving regression performance, it would be also possible to employ other statistics of feature maps.\nBut, it is necessary to provide a qualitative methodology/metric toward performance improvement which is applicable to various tasks and/or models.\n\nIt might be interesting to analyze the relationship between two complexities of features and task in Fig.3.\nHowever, the definition of task complexity shown in the paper is less convincing; it is trivial to control the complexity based on the CNN depth in the framework of feature matching.\nThe task complexity should be related to how the class categories are overlapped, which might be measured by quantitive ones of classification accuracies and qualitative ones regarding fine-granularity of class categories in FGVC.\n\n- Technical novelty is limited.\n\nFrom the technical viewpoint, the proposed method is simply based on the technique of knowledge distillation, exhibiting less novelty.\nIncorporating cycle consistency to extract common feature representation seems to be interesting, but the simpler approaches are also conceivable such as by multi-view approaches without the consistency which are not mentioned nor compared to the method.\n\n**Justification of rating**\nAlthough the idea to decompose feature maps via multiple CNNs for measuring complexity is interesting, it is hard to find out effectiveness and novelty of the reported analyses, making the paper lean toward weak rejection. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}