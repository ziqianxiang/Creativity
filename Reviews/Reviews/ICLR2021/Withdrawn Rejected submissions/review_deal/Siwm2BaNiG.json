{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces a conditional discrete VAE for uncertainty estimation on high-dimensional data. Reviewers found the paper borderline, and two of the three reviewers stated it doesn't meet the acceptance bar due to lack of clarity in several aspects and limited technical novelty."
    },
    "Reviews": [
        {
            "title": "Review 4",
            "review": "This manuscript proposes to measure the \"modal uncertainty\" in conditional generative models by forcing a discrete latent intermediate representation (here, C), between inputs X and outputs Y. By then manipulating the estimated categorical distribution likelihoods, an uncertainty estimate can be produced.\n\nThis is, in essence, the kludge of the VQ-VAE (van den Oord et al. 2017) categorical latent variable model into the Probabilistic U-Net (Kohl et al. 2018), which had previously used the Gaussian latent variable model. Empirically this difference is important, as the Gaussian VAE specifies a single mode latent distribution, which is incorrect in many cases.\n\nStrong points:  \n*Presents a clear argument for VQ-VAE style latents vs. Gaussians (...non-unimodal uncertainty representation).  \n*Constructs and executes examples from a subset of previous literature (LeCun & Cortez 2010, Kohl et al. 2018) that illustrate empirical effectiveness.  \nWeak points:  \n*Limited characterization of uncertainty (i.e. uncertainty of a label image is one-of-K, or combinations thereof, and not uncertainty w.r.t. model/training/etc).  \n*Possibly incorrect claims (prior-free, disentanglement, Prob. U-Net ordering, VQ-VAE sampling(?)).  \n*Possible limited scope from the chosen definition of \"uncertainty\" (no model uncertainty, no parameter uncertainty).  \n\nI think this paper is marginal for this venue. It improves on the Prob. U-Net by borrowing improvements to VAE from VQ-VAE. While it does seem to better characterize label uncertainty, it is better suited to a venue where such improvements have intrinsic importance (e.g. radiology), and moreover importance that can be measured (i.e. experiments asking whether radiologists will use Non-Top-1 segmentations?). However, it appears to be an improvement on the prior art in my opinion, and I believe it would have some interest to the community as a poster.\n\nOne overall question: are we learning uncertainty of segmentations/labels, or are we simply learning the biases of the varying raters/radiologists/human segmenters, or are these two concepts indistinguishable? Supposing the number of codes equals the number of raters, why should $p(c|x)$ differ from $p(r)$? In an imbalanced rater case, why would an ordering of $p(c|x)$ not simply reflect the likelihood of one rater or another.\n\nThis seems to speak less about uncertainty in the segmentation w.r.t. the image and model parameters, and instead speak about biases in the segmentation w.r.t. the raters. While this is clearly also important, does this capture notions of uncertainty beyond the uncertainty of who is rating a certain image? Or is that only and exactly the uncertainty that the authors are attempting to capture?\n\nDiscrete Representation and Associated Loss\n----\n\nAs expected, there is significant discussion and derivation devoted to the discrete representation. This essentially mirrors the short derivations in the probabilistic u-net paper, except using a discrete likelihood.\n\nFrom the loss in Eq. (4) we have 1) a distribution matching term between $p_\\theta(c|x)$ and $q_\\phi(c|x,y)$, 2) a reconstruction term $D_\\theta$ given the output of the E(x,y) encoder and $x$ (using $\\arg\\min || c' - E_\\phi (c|x,y)||$ for $c$), and then 3) the VQ-VAE \"stop gradient\" term.\n\nIn the derivation slightly higher on the page, we assumed that $q_\\phi(c|x,y)$ is deterministic (i.e. $q_\\phi(c|x,y)$ is $1$ for one value of $c$ and otherwise $0$). While this results in a nice cross entropy term, why is it necessary? We otherwise have a still tractable difference between $q_\\phi(c|x,y)$ and $p_\\theta(c|x)$, summed over $c$. Is there added benefit to the forced discretization here?\n\nPerhaps it is useful to sample $c \\sim q_\\phi(c|x,y)$ or use the mode $c = \\arg\\min || c' - E_\\phi (c|x,y)||$ (this inducing an implicit gaussian likelihood structure on the embedding space), but it's not clear from just the theory that this is true.\n\nWhy is it reasonable to always use the mode reconstruction when training a loss (again, the argmax from VQ-VAE's embedding scheme), and then look at the other categories? Why should the outputs from the non-mode categories be reasonable (up to their likelihood)? (this question is in contrast to the convergence of p(c|x) to a rater distribution p(r)).\n\nThe authors claim that as in van den Oord et al 2017 there is no shrinkage to a prior over $c$. Is the regularization term $\\beta || E(x,y) - sg[c]||$ not such a prior? Doesn't this enforce a structure on the co-domain of E(x,y)? Are the embedding vectors determining $c'$s learned? If not, isn't this a prior?\n\n[minor] The authors use the terms \"prior encoder\" to describe $p_{\\theta}(c|x)$, a learned network, and similarly \"posterior encoder\" to describe $q_\\phi(c|x,y)$, also a learned network. Reading the probabilistic U-Net paper, it appears that the authors of that paper also use this terminology. It is a matter of viewpoint, but I personally think these terms conflict with \"the usual terminology\", where $p(c)$ would be \"the prior\", to which we shrink $p(c|x)$ to. Further, this implies a causal order to $x$ and $y$; this is the case with the empirical examples (tumor labels on CT images, etc.), but may not always be the case.\n\nU-Net Comparison\n----\n\nThe authors claim that \"Probabilistic U-Net\" outputs cannot be ranked; is this actually the case? It is my understanding that the difference between the proposed method and the Prob. U-Net method in architecture is the replacement of their Gaussian latent C (in their paper, Z) with a discrete categorical C. This, alongside the different loss, contains the majority of the changes.\n\nCould the Prob. U-Net have outputs ranked by their point-likelihoods? The C output are conditionally Gaussian, so why not use the $p(c|x)$ likelihood? Understandably, generating diverse samples from this is not as simple as querying the different codes in the proposed method, but it seems incorrect to say that the samples from the Prob. U-Net are unordered.\n\nOther Questions\n----\n\nThere are several mentions of \"disentanglement\". In what way are these representations disentangled? They're certainly categorical by construction, but both the outputs and the actual labels are highly correlated between different categories.\n\nAt the bottom of page 3 there is a claim about sampling from a VQ-VAE being auto-regressive? Is the particular decoder architecture in VQ-VAE important w.r.t. the theory discussion directly adjacent to that statement? Surely VQ refers to the Vector-Quantization phase, which is agnostic of the decoder architecture, auto-regressive or otherwise? This seems incorrect. Similarly, there is a claim that VQ-VAE was introduced to avoid\n\n> noise sampling, which is a different cause than ours which usually results in blurriness  \n\nIs this the cause of blurriness (not, e.g. the L2 loss in the decoder?)? And is the introduction of VQ not the same issue the authors here are attempting to address (misspecification of a single-mode latent distribution), not the additive noise sampling?\n\nRecommendations for improvement\n----\n-Remove the disentanglement sentences.  \n-Resolve questionable claims.  \n-Include an experimental case where the number of raters is larger than the number of latent categories.  \n\nEdit after Rebuttal/Response Period:\n----\n\nFirst, an apology to the authors that a dialogue did not occur during the response period; the authors response was prompt, and my (R4) response was not, thus they were not given an opportunity to respond this response to their response to the review (...the number of recurrences may indicate why this was not possible, given limited reviewer time resources).\n\nI think the authors misunderstand my questions about the nature of the uncertainty they're capturing:  \nIs there intrinsic uncertainty in the observed phenomena (e.g. medical images of tissues), are we capturing mixture proportions of deterministic states which have been mixed due to quantization, OR is the uncertainty due to the raters, i.e. found in the labels ONLY due to differences in label generation?  OR, a third case, is this moot because it does not change the outcome?\n\nI understand that there is no explicit modelling of raters. However, my concern was that what we are capturing is intrinsically the uncertainty due to raters, even though no actual rater indicator variable was provided. This would be analogous to learning, unsupervised, the writers of the various MNISTs digits. While for MNIST this is surely difficult due to the number of writers (and their anonymity), for medical images we will likely have a limited number of raters. Having the posterior code collapse to a rater indicator appears problematic, not a desirable outcome, and likely if any one rater has correlated outputs across samples (which seems reasonable; some raters may be more or less conservative with their tissue labeling, boundaries, better/more careful at delineating curves etc). What prevents the capture of this signal, or is this the actual variation we intend to capture in the first place?\n\nR3 further included this interesting question in their initial review:\n> In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput [sic]. Can the authors comment on this potential issue?\nIf outputs are correlated i.e. overlapping in the original image domain, should the measured uncertainty be aggregated across codes?\n\nI disagree with the characterization of the Gaussian VAE calibration in the \"Ranking Probabilistic U-NET\" response section. Simply because it does not accurately fit the function (one mode vs. many) does not mean we can't evaluate the learned unimodal beliefs. Yes, it is misspecified. Does this mean the rankings are meaningless? Surely the discrete model is misspecified (there are more possible masks than codes), but the ranking is claimed to be meaningful. The discrete model may have a better fit, and make the argument that it is better specified, but this doesn't mean you _can't_ evaluate the Gaussian VAE.\n\nI stand by my initial rating and reasoning, though I note to the AC that, given space, this could make an acceptable poster. It is, in my opinion and in gross summation, an improvement on the Prob. U-Net by way of improving the Gaussian VAE sub-model of the Prob. U-Net to the VQ-VAE. This allows for sampling from a discrete set of codes which hopefully correspond with modes of the generating distribution in the data domain, instead of sampling from a parametric density, which, while continuous, has only one local maximum.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting way to apply discrete latent space VAEs, for multimodal outputs. ",
            "review": "This paper introduces a novel conditional generative model for high dimensional data with multimodal output distributions. The proposed method, called modal uncertainty estimation (MUE), is a conditional VAE but with discrete latent representations. This discrete latent space allows the model to better handle multimodal outputs and provide confidence scores for the different modes predicted by the model. These capabilities are applied to the task of segmenting lesions in medical scans.\n\n\n################################################\n\nStrong points:\n\n- The paper is clear and easy to follow. It is well-motivated and does a good job at highlighting the multi-modal posterior collapse problem.\n\n- The model outperforms the prior state-of-the-art on both a synthetic and realistic task.\n\n\nWeaknesses:\n\n- Although the application is very different, the proposed model is very similar, albeit lighter, to a prior work (cited by the authors).\n\n- In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput. Can the authors comment on this potential issue?\n\n\n################################################\n\nScore motivation:\n\nWhile the method is not particularly novel, the authors apply it in a way that could be of interest to the community.\nBesides, the behavior of VAEs with discrete latent space is a relevant topic that is little explored in the literature.\n\n\n################################################\n\nOther question:\n\nI am curious about the properties of the latent space succinctly mentioned at the end of section 4. What other properties have the authors observed? Also, do similar outputs tend to be represented with codes that are close in the latent space even in the case of discrete representations?\n\n\nMinor typo:\n\n3rd paragraph of the introduction: two consecutive commas",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Extension of conditional VAE for uncertainty estimation, but need polishing",
            "review": "The paper proposes a conditional VAE like framework to learn the one-to-many mappings between input and output, leading to an application of uncertainty estimation. Technically, the novel part is to utilize a deterministic (delta) distribution for approximate posterior.\n\nSome flaws may need future attention:\n\n1. In the introduction paragraph staring with \"Let us recall that one key ingredient of the VAE framework\", the main idea is understandable: discrete latent code has definitely advantages in coping with multimodal distributions than a Gaussian distribution which is in nature single mode.  However, its explanation is confusing as following: \n\tIn VAE, let's say we minimize KL divergence KL(P(c|x,y)|P(c|x)), where P(c|x,y) has two modes, P(c|x) has single mode. When minimization is successful, p(c|x) will spread out like figure 1(a), rather than 1(b). This is related to the difference between forward KL and reverse KL. However, it seems this paragraph suggests 1(b) as posterior collapse.\n\n2. Using discrete latent code is not new in VAE community. There are previous works (e.g. https://arxiv.org/pdf/1804.08069.pdf) noting that naively learned discrete code c for p(y|x,c) can not be interpreted alone, but need interpreted together with input x. Such statement argues this paper's novelty and contribution. \n\n3. Some typos and minor flaws, such as unclear references of figure rows in figure 3.\n\nOverall, the reviewer thinks this paper need some revisions for it to be more shining.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}