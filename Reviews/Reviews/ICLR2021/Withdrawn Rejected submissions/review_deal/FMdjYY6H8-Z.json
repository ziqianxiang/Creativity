{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "We thank the authors for detailing their answers to the reviewers and uploading a new version of the paper with more details and experiments. While the experimental section has improved in the revision, the fact that the method proposed is an ad hoc sequence of 6 heuristic steps, not supported by theoretical justification, and that the paper is hard to follow and not rigorous in its statements, remain.\n\nFor example, the authors explain in their response that \"[we] developed a new method to solve a fundamental mathematical problem\". If this is the case, then one would expect the mathematical problem to be rigorously formulated, and the fact that the method solves it supported by theoretical justifications.\n\nRegarding the problem statement (section 2.2), the authors write three equations, which however are ill-defined or ambiguous. For example, equation 2.1 has an expectation. Does that mean that X is a random matrix? This is nowhere stated, and there is no expectation in the following equations. Equations 2.2 and 2.3 are about a \"truncated SVD\" operator, which is also not rigorously defined. Literally, the authors state in Section 2.1 that for a matrix Z, tSVD¨*(Z)=USV, where S is the \"diagonal matrix Z's singular values, where all except for the top r singular values are forced to be zero\". But what is r? Is it a parameter of tSVD*? Or is it, as suggested in the last sentence, \"the numerical rank of Z\" that suggests that it depends on Z, but in that case the \"numerical rank\" should be defined if it is different from the rank. I take these examples to highlight that the authors should consider writing rigorous and correct equations to define the problem.\n\nRegarding the fact that the method proposed solves the problem, the authors add in the new versions some lemmas to support the claim. Alas, these lemmas also lack rigor (and therefore correctness) in their statement and proofs. For example, in Lemma one, the statement mentions a mysterious \"if the other r_{MS,i} are large enough\" (what is \"large enough\"), mention \"by expectation at least one matrix slice..\" (what does the \"by expectation at least\" mean?), and claim as main result \"at least one matrix slice [...] will enrich the matrix\" (what does \"enrich\" mean?). Looking at the proof of Lemma 1, it is just based on a probabilistic argument that if you randomly pick enough rows or columns in a matrix, they will hit a given subset with some probability. However, the authors seem to forget that randomly selecting rows and columns is only one step in their algorithm, and that the output of \"Matrix_Slicing\" is obtained by subsequent steps (maximizing inner products etc...). In conclusion, the statement of Lemma 1 is not rigorous, and its proof is also vague and not correct. More generally, this lemma and the following ones are far from providing evidence that the method proposed is likely to solve the problem.\n\nWhile the method may be a heuristic approach with some empirical merit, we therefore believe that the paper is currently not ready for publication.\n\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #6",
            "review": "Summary: The authors propose a new framing of the matrix local low rank representation technique, identifying 3 different subproblems and claiming most existing approaches sonly solve some, then proceed to propose a novel method that solves all three, evaluating on synthetic data.\n\nReview:\n* The paper is confused and hard to follow. The mathematical notation is inconsistent (e.g. what is $dist1$, $dist2$ in related works? what is $K_\\Omega^h$?). There are many grammatical errors. The proposed algorithm is complex and made of several different components that are not appropriately justified. No ablation study is performed. The experimental setup is not described appropriately(e.g. you mention a feed forward neural network in section 3.4 but don't report any parameter such as number of layers or size). \n* The evaluation is limited and not very supportive of the author claims, especially for real data more experiments would be necessary. The section on scalability lacks quantitative measures of the claimed good performances. \n\nSummary: While the idea could potentially be promising, a more clear explanation and more attentive evaluation are fundamental to assess the methods applicability and value.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review ",
            "review": "1- The section of mathematical formulation does not do a good job of providing a clear definition of the target problem. \n\n2- The presented algorithm is composed of many steps and the paper does not provide sufficient study/analysis to guarantee their performance. The paper also lacks a theoretical study of the proposed method. \n\n3- The experiments on real data are not convincing and I suggest the author to include further experiments with real data. \n\n4- I also suggest  to provide a discussion and elaborate the connection between MLLRR and the subspace clustering problem. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "The authors propose a multi-filter based neural network framework for local low-rank matrix approximation problems. They three sub-problems, namely, LLR-1C, LLR-1, and LLR-k. Based on extensive simulations, the authors claim that the proposed method is more general and outperforms previous methods on this problem.\n\n1. As the authors analyzed, this problem is tackling topically-coherent sub-matrix discovery in a given matrix. In Anchor-based methods, this problem is called anchor point selection. To tackle this problem, the proposed method is composed of 6 steps described in Figure 2. Some of these steps, however, are not fully justified to be present.\n\n 1-1) The first step slices the matrix into multiple P * P sub-matrices, but it is not justified why the submatrix must be a square matrix.\n\n 1-2) At the second step, multiple Q * Q matrices are sampled from the previous step. Again, squareness is not justified. Also, it is not justified why matrix slicing and random sampling should be in two separate steps, which are done at once in previous works. \n\n 1-3) In matrix slicing and sampling, it is an important criterion if the sampling allows overlapping between different submatrices. This is also not clear in this manuscript. \n\n 1-4) It is not clear how the matrix is sliced in the first step. In anchor-based methods, for example, a kernel is used to gather similar rows and columns. How does this work in this framework?\n\n 1-5) After the six steps, it seems the outputs are still P * P matrices. How do we recover X from these? This is not clearly described in the paper.\n\n\n2. The proposed algorithm is quite arbitrary, especially with the local low-rank filters. Previous anchor-based methods like LLORMA learn those filters (called kernels there) from the data. In this paper, however, the authors use somewhat arbitrary predefined filters like {3,2,1,-1,-2,-3,0,0}. Effectiveness of these filters should be different from dataset to dataset, so it is hard to justify generalizability.\n\n\n3. Experiments can be significantly improved in many aspects.\n\n 3-1) The authors mentioned they used two real-world data, but it is hard to find the details of those datasets. What datasets were used, how big/sparse are they, what pre-processing was done, what are the baseline performance on them? We expect reproducible details on these datasets for a successful ICLR submission.\n\n 3-2) Regarding the scalability, it is hard to say that the scale experimented in this paper justifies \"scalability\". The authors used a machine with 16GB ram, which even does not fit for a large-scale dataset to be loaded. LLORMA, for example, was evaluated on MovieLens 10M with a 70K * 10K matrix, or Bookcrossing dataset (100K * 300K). These are far larger than the scale used in this paper, such as 4000 * 2000. (Please see https://www.jmlr.org/papers/volume17/14-301/14-301.pdf for the journal version of LLORMA paper.)\n\n 3-3) In this sense, the authors' claim that \"LLORMA failed to converge under this data scale\" in Sec 4.1 is not trustworthy. The authors are encouraged to play with more hyperparameters on a machine with higher capacity. As LLORMA already has shown it is scalable to at least a few times larger matrices than what the authors tried, so the authors should be able to reproduce it.\n\n 3-4) The JMLR version of LLORMA paper describes some primitive anchor point selection methods. We encourage the authors to compare this against them, if applicable. (I am not sure if they are good enough to be a strong baseline though; those methods are really primitive, something like using k-means clustering.)\n\n\n4. Typos and mistakes:\n\n 4-1) At the beginning of Sec 3.1: {E_1, ... E_M_r} \\times  {F_1, ..., E_N_r} <-- the last E should be F. Same mistake in Algorithm 1.\n\n 4-2) In the first paragraph of Sec 3.1, (ii): \"R small between ...\" --> \"r small between\". Mathematical variables must not be capitalized.\n\n 4-3) At the end of page 5: {1000,100,10,1,1} --> 1 is repeated twice. Same for s_{MS}: 50 is repeated.\n\n 4-4) Third line of page 6:anda --> and a - The Conclusion section is on the 9th page, which violates the page limit.\n\n 4-5) In many places, mathematical variables are used without definition. E.g., what is E, u_k,  in Eq. (2.1)?\n\n\nOverall, this paper is tackling an important problem and we encourage the authors to keep working on this, but the current manuscript is not fully ready to be published in ICLR this year.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}