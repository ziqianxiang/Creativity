{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a robust formulation of Deep Subspace Clustering (DSC) based on the correntropy induced metric (CIM) of the error. All three reviewers recommend rejection. Their major critiques are limited novelty, insufficient experiments and similar performance to non-deep methods. The rebuttal highlights that the novelty is not DSC or CIM, but rather that the formulation does not require knowing the labels. I agree with the reviewers that the paper's novelty is very limited and didn't find the author's response compelling enough to overturn the reviewer's opinions. "
    },
    "Reviews": [
        {
            "title": "Paper537 review",
            "review": "This paper proposes the robust formulation of the self-supervised convolutional subspace clustering network. In order to handle the data corruptions, the correntropy induced metric (CIM) of the error is embedded into the loss function. I have some concerns about this paper:\n1. The major issue of this work is the limited novrlty. In its current form, the only novel point is the CIM loss of the representation error of Z in Eq.(4) and (5). Other loss terms in Eq.(10) are all used in previous works.\n2. There are too many parameters in Eq.(10). However, no ablation analysis for different loss terms are given.\n3. Experimental comparison is not sufficient. Only one method is used for comparison. However, there are many deep learning based clustering methods, they should be used for comparison.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A deep subspace clustering approach that aims to overcome some issues with existing methods but does not present compelling results.",
            "review": "This paper presents an approach to deep subspace clustering based on minimizing the correntropy induced metric (CIM), with the goal of establishing when training should be stopped and generalizing to unseen data. The main contribution over the existing S2ConfSCN method is a change from squared error loss to CIM when optimizing over the affinity matrix. A key benefit of CIM as a loss is that it does not decrease arbitrarily with training epochs, so it provides a means of estimating when training should cease without needing ground truth labels. The authors argue that CIM \"ensures a smooth decrease of the loss function that enables the use of label-free stopping criterion.\" However, this claim is only justified through a minimal empirical evaluation. The authors also include a means of enforcing block diagonal structure in the learned affinity matrix.\n\nWhile the ideas presented in the paper are important for the area of deep subspace clustering, the overall contribution of this paper is quite limited. Further, the results in Table 1 do not improve on the results of shallow subspace clustering methods such as SSC and EnSC. The authors argue that these methods are tuned using ground truth labels, but even tuning with the KSS Cost (as in Lipor and Balzano 2020) results in better performance on the given datasets. Given that shallow subspace clustering methods maintain strong theoretical guarantees, the empirical results presented here do not justify the use of deep methods.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Need a better presentation style and clarification",
            "review": "The authors propose to add a correntropy induced metric (CIM) loss term to improve the robustness of the self-supervised convolutional subspace clustering network (${\\rm S}^2$ConvSCN) to data corruption. The authors show that in a truly unsupervised training environment, the proposed robust ${\\rm S}^2$ConvSCN method outperforms the original ${\\rm S}^2$ConvSCN in [1].\n\nPros:\n1. The authors point out a very interesting fact that many deep subspace clustering methods often require access to the ground-truth labels to decide the stopping of the learning process and optimize hyperparameters using external clustering validation methodology, whereas the same labels are used for tuning hyperparameters and evaluating the final clustering performance. \n\nCons:\n1. The presentation style of this paper needs improvement. For example:\n    1.1 As this paper is based on the comparison of [1], the differences in Figure 1 should be clearly stated. Currently, this comparison is mixed in the contribution part, which is really confusing. The reviewer would recommend that before showing the results, the authors elaborator the differences between the proposed method and the method in [1] point by point. Then clearly state which part of the method in [1] is modified for \"fair\" comparison. This is very important for evaluating the contributions of this paper. \n    1.2 Define $\\bf{E}$ in (1).\n    1.3 Please define CIM near (4) and (5), or at least explicitly give the reference to your Appendix. Same for $||\\bf{C}||_{[k]}$. The reviewer believes that this paper is modified from [2], but the presentation flow is not carefully addressed. The Arxiv version is more readable.\n    1.4 Define $\\bf{Q}$ shown between (6) and (7).\n    1.5 The reviewer recommends that the authors include their model structure.\n\n2. The authors mention \"unfair\" comparison in the paper. I think a possible fair way to compare the performance of ${\\rm S}^2$ConvSCN method and robust ${\\rm S}^2$ConvSCN method on an \"unseen\" dataset. \n\n3. Please give your hyper-parameters in the paper.\n\n4. Please define the permutation space of $[k]$ in (11).\n\n5. For \"fair\" comparison, the author modified the method in [1]. However, in 4.1, only \"fixed number of epochs, learning rate, decay, and early stopping based on loss decrease only were used\". These should be explicitly stated. Are they the same as those in Table 3?\n\n6. The authors also mention the training time in the paper. However, for the device, only the word \"GPU\" is mentioned in the last paragraph without any details.\n\nIn summary, I believe this paper can be interesting if the presenting issue is well-addressed, and the comparison is well-stated, but the current version is not really good for acceptance.\n\n\nReference:\n[`1] Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, and Zhouchen Lin. Self-supervised convolutional subspace clustering network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5473-5482, 2019a.\n[2] https://arxiv.org/abs/2004.03375\n\n----\nAfter rebuttal:\n\nThank the authors for providing a revised version. The paper is more readable, however, I still hesitate to give 6 for the current version.  I would say that the fact pointed in this paper is really interesting, but this paper may need further improvement in the presentation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}