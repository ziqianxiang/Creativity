{
    "Decision": "",
    "Reviews": [
        {
            "title": "Reviews and Comments",
            "review": "### Overview\n\nThe authors propose a new training scheme to obtain a Transformer model with flexible hidden size for machine translation task. They can obtain smaller Transformers by clipping a slice of each parameter weight tensor from the largest model. Besides, the performance of largest model can also be enhanced with this training method.\n\n### Pros\n\nThe authors propose a new and interesting way to train Transformers with different size based on parameter sharing. The thorough ablation study clearly demonstrate the effectiveness the self-distillation method used in the paper.\n\nThe performance of the largest model can also be enhanced the proposed method. This might inspire the audience a new way to make further improvement on conventional scenario in NMT with Transformers.\n\n### Cons\n\nI am not sure if I understand correctly: it seems that all the combination of stage 1, 2, 3 in the ablation study are using parameter sharing. I am curious about the results using the same way of self-distillation but without sharing parameters. That might be useful to show how much parameter sharing contributes to the improvement and how much the training scheme does.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "The authors propose scalable Transformers (ST) that contains sub-Transformers with shared parameters and different model sizes. A three-step training strategy is introduced for ST, where the widest Transformer and sub ones are trained independently in stage-1, and word-level and sequence-level self-distillation are used in stage 2-3. ST achieves comparable performance on WMT En-De and En-Fr tasks with more flexible model scale. \n\nThe paper is, for the most part clearly written and easy to follow. The proposed method is simple and can be readily reproduced. The method has certain level of novelty but the contribution and impact is limited.\n\nMisc questions/comments: \n* Training: how do you determine when to terminate training in each stage? \n* In Stage 3, is ground-truth data completely removed and train all Transformers with distillation only, or do you train the largest one on the combination of ground-truth and self-distillation?\n* What is the overall training and convergence speed (in clocktime and number of examples) for ST compared with standard Transformer training?\n* Have you tried sub-Transformers with different number of encoder-decoder layers as well?\n* It would be more convincing and practical if the authors can also evaluate ST with large-scale monolingual data via BT/noisy BT or initialized with pre-trained model (e.g. MASS, BART) that has the state-of-the-art performance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "many mistakes in writing",
            "review": "This paper proposed a method to train transformer for MT with different scales. Different sizes of the transformer can be used in practice based on the computation and inferent time constraints. \n\nMy questions are as follows:\n1) Why NMT is short for Machine Translation? NMT usually refers to Neural Machine Translation. For the related work part, SMT has a much longer history before Forcada et al 2011's work. If you want to talk about SMT, there are tons of work before 2011 that need to be cited. For Sutskever's paper in 2014, it did not surpass SMT at that time. In the abstract of Sutskever's paper in 2014, they clearly stated that \"...which is close to the previous best result...\". I think both intro and related work need re-writing.\n2) math notations: Section 3.1 has a lot of math problems. How could you multiply W_e and S together? S is N by L and W_e is N by C. How could you add a vector b_k to W_kE which is a matrix to form K? This is a math equation, not pytorch code. What is X? Where does it come from?  What do you mean by softmax(OW_e^T)?\nO is N by C and W_e^T is C by N. What did you get if you apply a softmax directly to an N by N matrix? For the fourth line of section 3.2, I think you probably have a typo for \"W_e\". Do you mean W_e^T?\n\nDid I miss anything from your paper? Or you just totally messed up the math notations? \n\n3) There are many different ways of shrinking the model size by distillation. I think you should also compare your model with different sizes with them in your experiments. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Elegant idea with cumbersome implementation",
            "review": "This paper proposes to train a single Transformer-big for NMT such that it can be easily adapted to satisfy given computational/memory constraints by parameter matrices pruning. The motivation of the paper is clear. If there are many deployment scenarios with different computational/memory constraints, we would like to have a single model to use, instead of training many models from scratch for each of these cases. However, the implementation which introduces knowledge distillation, additional hyper parameters and breaks the end-to-end training cycle obstructs the usability of the proposed approach.\n\nPros:\n1. The idea of having a single set of parameter matrices which can be pruned to achieve given computational/memory constraints is elegant and (to the best of my knowledge) novel.\n2. Experimental evaluation shows good compression/accuracy trade-off.\n\nCons:\n1. The proposed approach introduces additional hyper parameters (to an already extensive list of Transformer hyper parameters) such as the coefficients for different loss terms and the durations of different training stages. The resulting pipeline is too complicated and training several vanilla Transformers from scratch seems easier to do in practice.\n2. Authors claim that the improvement of their approach does not come from simple knowledge distillation. However, the difference between independent training + separate widest teaches and the proposed 3 stages training is within 0.3 BLEU score (see table 1). The standard deviation of BLEU score in Transformers is usually around 0.2 which casts a doubt on the authors' claim.\n3. The comparison of training times of the proposed method and much simpler baseline of training all Transformers independently is too vague and short (4 lines at the end of section 4.2). I believe that this comparison requires more detailed exposition as it is crucial to the overall assessment of the proposed approach. Also, I am wondering whether the scenario of 12 different Transformers is practical. Having 3 or 4 Transformers for different deployment constraints seems more realistic. In such case, the huge difference in overall training time and especially total parameter count will diminish.\n4. Competitors in table 3 (except for solid Scaling NMT baseline) are outdated. It would be more informative to compare with recent approaches, especially those which use knowledge distillation. Also, it is important to compare with LayerDrop (https://arxiv.org/abs/1909.11556) which also allows to prune model on inference to satisfy lower computational/memory constraints. Note, the training regime of LayerDrop is end-to-end and straightforward.\n5. (minor) Text contains too many mistakes and misspellings. It should be proofread by native speakers or automatic engine such as Grammarly.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}