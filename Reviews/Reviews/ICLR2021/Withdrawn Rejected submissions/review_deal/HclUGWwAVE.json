{
    "Decision": "",
    "Reviews": [
        {
            "title": "Sample Balancing for Improving Generalization under Distribution Shifts ",
            "review": "In this paper, the authors focus on reducing the impact of distribution shifts between training (source domain) and testing (target domain) data on deep learning models' accuracy.  They assume no knowledge of the test data distributions, nor on the training data heterogeneity. The authors propose a new Sample Reweighted Distribution Balancing (SRDB) method for reweighting training samples to remove the complex dependencies and entanglement between features. It allows balancing the training distribution by globally reweighting the training samples,  and can be integrated into off-the-shelf deep learning models. Experimental results on object recognition benchmarks indicate that the proposed method can outperform related SOTA methods.  \n- The paper is clearly written and well organized, and all the key concepts and motivations are described in enough detail to understand the paper.  The authors could clarify the connection of their proposed approach to methods in the literature for disentanglement (to remove “nuisance” features). \n- The supplementary material provides additional information -- SRB algorithm, saliency maps, and training setup -- that should be useful to the reader. It however seems like their code is not made available, so there is a concern that the results in this paper would be very difficult for a reader to reproduce.\n- The experimental validation should include an analysis of the impact on the performance of the amount of training data, and capture conditions (nuisance factors).\n- The proposed approach is claimed to limit the computational complexity linked to reweighting training samples. However, there is no analysis and comparison of time and memory computational complexity to support this claim. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "# Summary:\nThe paper proposed to use Random Fourier Features (RFF) and the independence test proposed in [1] to learn independent features for improving domain generalization. However, the writing is not sufficiently descriptive which majorly hinders the potential contributions of the project.  The paper does not provide a complete description of how RFF regularization is incorporated during network training.  The paper did not have a related work section.  \n\n# Strength:\nThe method seems interesting and novel, but due to inclarity, it’s hard to say if it’s justified. \nSome empirical evidence shows that the method is  slightly better than the simplest  baseline and a few selected references. \n\n# Concerns:\n## Please improve the writing \nThere are just too many problems with the writing.  Here I name a few that really prevents me from a full understanding - \nEqn 7 inherits the  notation of A, B for 2 variables.  When you learn the network, what  are the 2 different variables, or they are actually the same variable (i.e., the features of one network)?\nIn Eqn6, 7, what prevents the degenerate solution of a one-hot vector, in which case the weighted cross-correlation is simply zero?\nEverything in Section 2.1 is for a 1-D variable, where is the multivariate description?\nThe paper hinges on removing correlation between robust features and non-robust features.  Suppose SRDB really decorrelates such features, how do we make sure we are using the robust features during inference? \n\n## Empirical results improves very slightly over baselines\nE.g., in Table 1, for PACS Cartoon, we are looking at 78.11 vs 77.02 against the most basic baseline. A one point improvement  is  very small, also it makes interpreting these results hard when the authors  don’t provide a confidence interval.  \nPlease provide confidence intervals for the results. \n\n## Missing related work\nThis makes it difficult for me to gauge how the authors position their contribution.  E.g., missing discussion of the DomainNet dataset and the method in [2] is confusing.  Is there a reason why the authors feel this is justified?  \n\nI can’t really say if the technical contributions are valuable due to difficulty in understanding the writing. The proposed method seems novel and interesting, but at this stage this paper needs more work. \n\n\nReference:\n[1] Eric V Strobl, Kun Zhang, and Shyam Visweswaran. Approximate kernel-based conditional in- dependence tests for fast non-parametric causal discovery. Journal of Causal Inference, 7(1), 2019.\n[2] Peng, Xingchao, et al. \"Moment matching for multi-source domain adaptation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting research, missing a significant amount of prior work",
            "review": "This paper introduces a novel approach for domain generalization (without domain labels). The central idea is to reweight the samples so that the dependency between spurious features (confounding factors, dataset bias) can be removed along the reweighting. The idea is built upon established theoretical work, with the extension to enable the method for larger datasets that need to be learned through batch-wise optimization. \n\nFirst of all, I find multiple places that the paper does not accurately describe its contributions, e.g., \n -  The abstract says \"object detection\", but the paper is about \"image classification\". \n -  The paper says \"we consider a more challenging case where neither of the above information is available during the training phase\", the above information refers to test data and domain IDs. Such a setup has been explored previously and not appropriately referred to. See e.g., [1]. \n - \"Recently, there are several methods which also focus on the correlation between features, but they are all developed under linear models with raw input features ... \", this claim, unfortunately, denies the contribution of many recent papers in the deep learning regime. Please see (e.g., [2-4]) and the references within. \n - I also find the paragraph starting with \"take a synthetic case as (an) example\" unnecessarily complicated, as the author can see from the missing references, such problems are very well established. \n\nMethod-wise, the central idea is to use an established definition of independence and optimize the criterion of dependence as it is, and also introduce an intuitive method to enable the idea for large-scale datasets. I do not consider the novelty ready for this conference. If the authors made other technical contributions prior to the paragraph starting with \"Sample reweighting for independence\" (middle, Page 4), please clarify the contributions. Also, technical writings can be improved, the notation paragraph does not fulfill the goal of introducing notations, as it only discusses the standard notations regarding the data and space. The authors still have to write sentences such as \"we use ... to denote sample weights\" long after the notation paragraph, and I do not see why these cannot be introduced at the same place. I will also recommend the authors to use the notation paragraph to explain what the subscripts mean, especially when there are two indices. \n\nExperiment-wise, it's exciting to see that the authors conduct a comprehensive set of experiments to validate their methods, but again, some references to the prior work are necessary, e.g., \n - The ResNet-18 performances on PACS are not SOTA, see e.g., [5]\n - The experimental protocol, especially the one in 3.4, can be improved by the way how [2] create synthetic experiments. Currently, I found that the authors only discuss experiments in MNIST-M limited. \n\nOther minor points:\n - I will also suggest the authors clarify computational loads as the method seems to be significantly more complicated. \n - I also suggest the authors define the problems more clearly to avoid discussing motivations repeatedly in 3.2-3.4, although the motivations are different. \n\nReferences: \n- [1]. Learning Robust Representations by Projecting Superficial Statistics Out \n- [2]. Learning De-biased Representations with Biased Representations\n- [3]. Informative Dropout for Robust Representation Learning: A Shape-bias Perspective\n- [4]. Learning Robust Global Representations by Penalizing Local Predictive Power\n- [5]. Self-Challenging Improves Cross-Domain Generalization \n- (Here is only a list of relevant new papers except [1] which starts the problem of DG without domain IDs; the actual missing reference list is longer than this list. Thus, I suggest authors briefly go over the papers and find other missing referred works.)",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Important details of methods are missing, along with in-depth analysis of experiments",
            "review": "This paper proposes a Sample Reweighting Distribution Balancing (SRDB) method to tackle the imbalanced settings occurred very often in machine learning. To that end, SRDB uses the Random Fourier features to maximize the independence of all features in each sample. Then to learn global weights, authors proposes an \"online\" manner of adding batch-wise weights online. Experiments on several domain generalization datasets show that this SRDB method can work well under four settings where imbalanced classes exist.\n\nPros:\n\n- The problem of imbalanced learning in distribution matching is important in real-world machine learning.\n- SRDB remains a simple and effective method.\n- Experimental results are good.\n\nCons:\n\n- The motivation is not clear, resulting in low novelty. If the main goal is to decorrelate all the features, then why do you use Random Fourier Features specifically? There are several alternatives such as Mutual Information (MI) and Hilbert-Schmidt Independence Criterion (HSIC) as you mentioned in the paper. Therefore, if \"decorrelating features\" magic works for RFF, it should also work for MI and HSIC and other metrics. If it only works for RFF, then maybe the idea of feature decorrelating features is not working in general.\n- The problem definition is unclear to me. Bringing domain shift and imbalance learning together is not a new setting. You should formally define your problem and clearly describe your objective regarding this problem. For instance, what metric should be used to evaluate the learning results in this distribution shift-class imbalance learning setting? Is accuracy enough?\n- Distribution shift occurs in imbalanced learning, while I fail to see the strong connection between distribution matching and the sample reweighting. It seems that the reweighting can work for all situations including distribution shift. If so, authors cannot say that they can tackle the distribution shift problem in this paper and maybe they should propose another experiment where there's only in-distribution data, to see if SRDB still works; if not, then it is clear that this paper does not solve the distribution shift problem explicitly since feature correlation does not mean distribution matching.\n- The concatenation of local features is technically an application of existing online learning algorithm such as online passive-aggregation. However, these works are not cited and discussed. Therefore, I do not consider this as a technical contribution.\n- The idea of decorrelating features are extremely like feature disentanglement and causal inference. However, neither of these two types of works are cited, discussed, and experimented on. The same goes to imbalanced learning in domain adaptation. I strongly insist that authors add a related work section.\n- Experimental results seem solid but I have several questions:\n  - Are there any connections between these datasets and your problem definition? Several DG datasets and DG methods are compared in this paper, but PACS and VLCS are very small in sizes, NICO is a new dataset on which the results of existing methods are not available hence must be run by the authors: but the details of tunning existing methods are not available, which may raise the unfairness problem that authors should pay attention to.\n  - Office-Home dataset is also widely used in DG problems and is larger and more difficult to benchmark. Why not using this dataset?\n  - Since you are dealing with an imbalanced learning problem, you should consider more metrics in addition to accuracy. For instance, Precision, Recall, and F-scores are typically used in imbalanced learning problems. Plus, there should be a confusion matrix to quantitively show your results against other methods.\n  - Regarding distribution shift, did you solve it? Then it should be discussed in the experiments better with some quantitively results.\n\n\nReferences:\n\nFor class imbalance learning:\n\n[1] Jiang J, Zhai C X. Instance weighting for domain adaptation in NLP[C]//Proceedings of the 45th annual meeting of the association of computational linguistics. 2007: 264-271.\n\n[2] Wang J, Chen Y, Hao S, et al. Balanced distribution adaptation for transfer learning[C]//2017 IEEE International Conference on Data Mining (ICDM). IEEE, 2017: 1129-1134.\n\n[3] Jamal M A, Brown M, Yang M H, et al. Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspective[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7610-7619.\n\nFor causality and disentanglement:\n\n[1] Rojas-Carulla M, Schölkopf B, Turner R, et al. Invariant models for causal transfer learning[J]. The Journal of Machine Learning Research, 2018, 19(1): 1309-1342.\n\n[2] Zhang J, Lv F, Yang G, et al. Learning Cross-domain Semantic-Visual Relation for Transductive Zero-Shot Learning[J]. arXiv preprint arXiv:2003.14105, 2020.\n\n[3] Luo Y, Zheng L, Guan T, et al. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 2507-2516.\n\n[4] Dou Q, de Castro D C, Kamnitsas K, et al. Domain generalization via model-agnostic learning of semantic features[C]//Advances in Neural Information Processing Systems. 2019: 6450-6461.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}