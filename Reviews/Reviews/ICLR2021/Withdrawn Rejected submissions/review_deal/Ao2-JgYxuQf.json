{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces a method to estimate dynamics parameters in recurrent structured models during the learning process. All three reviewers agreed that the idea is interesting and the proposed method could be potentially useful. However, two of the three reviewers have a serious concern about the lack of comparison with other approaches. I agree with these two reviewers; due to the lack of discussion and comparison with existing studies, I cannot recommend accepting this submission in its current form. "
    },
    "Reviews": [
        {
            "title": "paper is well-written and clear. There are no related work discussed.",
            "review": "This paper introduces a propagation method to estimate RNN dynamic parameters during the learning process. The algorithm is introduced well and the paper is clearly written.\nThe paper misses a related work section on other tuning methods or absence there of under special circumstances. \nFor the same reason, I am not convinced on the extent of comparisons in the simulation results. A large amount of the focus of the experiments is on the robustness to noise which is fine if there was an equal amount of comparisons against other tuning methods. Otherwise, if the focus of the paper is supposed to be only on noise robustness, I think the motivation in abstract and introduction needs to be clearer.\nWhile the motivation of the paper can be to some extent taken from the results, the introduction does not substantially motivate the problem. \nLastly, I think that majority of details on pages 4 and 5 are unnecessary. Instead, I think a more detailed discussion on comparing additional computational cost of active tuning to other traditional methods would be very useful.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel method to tune autoregressive model via hidden state optimization",
            "review": "Paper proposes a way to adapt an autoregressive model (RNN in examples) to the incoming noisy signal to generate noise-free data output. The approach is interesting due to applying updates to the hidden state of the past observation. The proposed approached is named Active Tuning and evaluated on 3 toy tasks. The idea sounds interesting, however the lack of comparisons with other approaches and theoretical justification of why this approach is superior makes it hard to convince reader. \n\nQuality: Paper is well written and most of the concepts is clear. However, paper will benefit from a better explanation of the method, simpler diagram and equations to remove uncertainty on implementation. \n\nOriginality: I believe the idea is novel and interesting for community. It has a potential to outperform meta-learning and sequence-to-sequence models on the task of model adaptation to noisy samples. \n\nPros:\n- Idea is interesting and has potential. \n- Explanation is clear, but still can be improved. \n- Provided experiments show benefits of the proposed method with respect to direct regression task (same model trained with less or more noise amount)\n\nCons:\n- Comparison with other techniques such as meta-learning, sequence-to-sequence models is required to understand the potential of the method.\n- Same comparisons might be interesting for tuning weights instead of hidden states. Or having only a small part of the model to be tuned (like the last layer). \n- Application to more practical problems could benefit the paper. For example image denosing task could be relevant (works like Noise2Noise, Noise2Self etc)",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting approach in optimizing the internal dynamics of recurrent neural networks",
            "review": "This is an interesting paper on an idea introduced by the authors as active tuning. This paper is well-written and clearly explains the proposed active tuning scheme. I read the paper carefully multiple times, and feel that a few inclusions will help the readers better understand the proposed method.\n\nAt the base level, this paper builds on optimizing the internal dynamics of a recurrent neural network unlike optimizing internal weights in traditional sequence-to-sequence mapping. This is achieved by decoupling the recurrent neural activities from the input temporal signal and propagating the error (the difference between the estimated input value and the observed input value of the input signal) to tune the internal dynamics of the network. To demonstrate the effectiveness of active tuning the authors trained a distributed graph recurrent neural network (DISTANA) on three datasets with increasing complexity. Datasets included: multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics.  On average ten independent experiments were performed and the effectiveness of active tuning was evaluated using root mean square (RMS).  Samples for the experiments were generated using five different noise ratios between 0 and 1 to measure the effectiveness of the proposed method for noisy data scenarios. The network was also trained on no noise to 0.05 noise induced into training data to see if it would help the models better generalize. The results as depicted in graphs show that active tuning is not only robust but generalizes well on noisy data. \n\nRecommendations:\n1. The active Tuning algorithm itself is missing from this paper. Even though the explanation is clear, it would help the readers to see the algorithm itself for better understanding. The reviewer referred to Hidden Latent State Inference in a Spatio-Temporal Generative by karlbauer et. al. 2020 (arXiv:2009.09823) for the algorithm. \n\n2. The authors confirm that 10000 and 1000 samples have been generated for all the problem domains tested. However, it is not clear if steps were in place to make sure that no bias was introduced during this sample generation. \n\n3. While the tuning length and tuning cycles were fixed for all three datasets, it is important to see how these values can be optimized based on the complexity of the time series data. Experimental results using a range of values for tunning length and tuning cycles would be beneficial.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}