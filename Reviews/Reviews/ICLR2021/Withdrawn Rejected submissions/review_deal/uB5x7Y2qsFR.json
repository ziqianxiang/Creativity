{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting results, needs more experiments and better exposition",
            "review": "This paper proposes Contrast to Divide (C2D), a method for learning with noisy labels (LNL) that utilizes contrastive self-supervised learning. Specifically the method modifies DivideMix, a recently proposed method for LNL that has three components: 1) a warm-up phase, where a network is trained using the entire noisily-labeled dataset in a supervised manner, 2) a division stage, where clean and noisy samples are split based on a mixture of Gaussians fitted to the loss distribution; and 3) a semi-supervised stage, where the noisy labels are discarded and MixMatch is applied on partially labeled data (MixMatch is a semi-supervised learning method that trains network with augmented labeled data and pseudo-labeled unlabeled data). C2D modifies DivideMix by replacing the warm-up phase with a contrastive learning (SimCLR) based self-supervised learning step with the motivation that self-supervised feature representations will be better at separating the clean and noisy samples, since they are agnostic to the amount of label noise. Using experiments on CIFAR-10 and CIFAR-100 at varying amounts of noise levels, the authors show that C2D is superior to DivideMix and obtains particularly significant improvements at very high noise levels.  On a much larger Clothing1M dataset, C2D performance is quite similar to DivideMix and other baselines, indicating that the gains obtained from self-supervised warm-up may  not be as significant on larger datasets. \n\n\nStrengths: Experiments with CIFAR-10 and CIFAR-100 show decent to substantial gains on strong baselines with increasing noise levels.\n\nWeaknesses and suggestions:  1. It is not clear to me if the warm-up phase makes a difference in performance on larger, more realistic datasets like Clothing1M. More careful analysis of how the warm-up phase affects the sample separation in SSL versus a fully supervised setting would  have been useful, including experiments on CIFAR-10. 2. Additional experiments on realistic noisy datasets like WebVision would have provided more support for C2D.  3. The paper is not clearly written. Important components like MixMatch are not explained. For instance, the Method section contains discussion on various design decisions, rather than a step-by-step description of the method itself. An algorithm figure detailing C2D method would be useful for  exposition. In sum, the paper definitely has a good idea and interesting results, but it is not well-structured, which makes it harder to parse the method and results. \n\nQuestions and suggestions: 1. Do you have any additional insights into  modest performance gains on Clothing1M  2. How does the algorithm perform on other real-world datasets like WebVision, evaluated by DivideMix? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments do not clearly demonstrate the benefit over the baseline methods for the real noisy dataset. Method section can be improved.",
            "review": "Summary:\n\nThis paper proposes a method to address the problem of “Learning with Noisy Labels”. The proposed technique is to first do self-supervised learning, then do a \"warm-up\" stage of supervised learning, use DivideMix's technique to decide which examples are noisy, and then do semi-supervised learning where the estimated-noisy examples are treated as if they were unlabeled.  \n\nReasons for score: \n\n1. The technical novelty is limited. The paper proposes to do self-supervised pretraining before the warm-up phase of DivideMix.\n\n2. Although the results of C2D are strong on the CIFAR10 and CIFAR100 datasets, the results on the “real” noisy dataset Clothing 1M are worse than the DivideMix. Thus in my opinion it does not sufficiently demonstrate that using self-supervised pretraining before the warm-up has any added benefit.  I would encourage the authors to demonstrate results on atleast one more large scale dataset. \n\n3.The method section is poorly written. It constantly brings up Dividemix and compares the proposed algorithm with dividemix.  The comparison belongs to the related work section.  The method section should really give a clear explanation of what \"Contrast to Divide\" is, in a complete sense, without assuming that the reader has complete knowledge of DivideMix, especially since DivideMix is far from common knowledge in the ML community.  \n\n\nPros:\nThe paper is well written for the most part, including related work and introduction section (except the method section)\nImpressive results on CIFAR10/100 in very high noise settings.\n\nAdditional comment:\n\nThe authors should add a baseline where they just do self-supervised learning and then fit a linear predictor on the labeled (potentially noisily labeled) data.  It's possible that this could recover and overlap with the strength of the proposed technique.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work propose to replace noisy pre-training with self-supervised learning in DivideMix approach. The experiments are sufficient, but there is no novel contribution in this work, it increments DivideMix with self-supervised learning.",
            "review": "In this work the authors propose to utilize self-supervised learning instead of noisy pre-training to improve the semi-supervised stage of DivideMix Li et. al. 2020. The authors demonstrate that a self-training instead of noisy pre-training avoids noisy labels and also speed up the process. The proposed method is evaluated using CIFAR-10, CIFAR-100, and Clothing1M dataset.\n\nPros:\n\nThe idea of replacing the noisy pre-training with self-supervised learning is interesting as the learned features are more effective and this way we also avoid the label noise. \n\nThe use of self-supervised learning is beneficial when the noise ratio is too high as a pre-training on such noisy labels leads to very poor performance.\n\nThe experiments are comprehensive and sufficient analysis of the proposed method is provided.\n\nCons:\n\nThe idea of replacing noisy pre-training with self-supervised is interesting, but this work builds upon DivideMix and there is no technical contribution in this work.\n\nThis work utilizes existing methods and provides no novel contribution. It is purely incremental combining self-supervised learning with DivideMix.\n\nThe authors are proposing to improve DivideMix by introducing self-supervised learning but the performance of the proposed method is even lower than DivideMix on real-world Clothing1M dataset. How will the authors explain this?\n\nMinor comments\n- Why the entry to 95% in the last row not shown in Table 2?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Self-Supervised Learning pre-training helps for learning with Noisy Labels",
            "review": "Summary of the work:\nThis paper aims at advancing the task of learning with noisy labels (LNL). Recent methods have shown that supervised pre-training on noisy data could be a way of identifying noisy samples, but it is highly sensitive. On the other hand, self-supervised pre-training was shown to be highly beneficial for semi-supervised learning (Semi-SL) and was also shown to produce strong transferable representations. The authors here propose to use self-supervised learning (Self-SL) for the task of LNL. They show that it is highly beneficial for experiments with (synthetic) label noise of up to 90% as well as for a real dataset with noisy labels.\n\nStrong points:\n* This submission deals with an interesting and important problem for representation learning: learning with noisy labels.\n* The authors verify the hypothesis that Self-SL pre-training on the target dataset is better than supervised pre-training with noisy labels. Gains are high in synthetic experiments where the noisy labels are more than the real ones.\n* The authors show that using a state-of-the-art Self-SL models like [SimCLR] trained the target dataset (Clothing-1M), they can practically match the performance of ImageNet supervised pre-training for a real dataset with noisy labels, without using external data (ImageNet)\n\nWeak points:\n* The authors transfer findings from semi-supervised learning eg in Chen et al 2020b to LNL. The two tasks are highly related, as shown by the [DivideMix] method that the authors adapt. Although interesting to experimentally verify, it is not really surprising that Self-SL pre-training also helps for the LNL task and outperforms supervised pre-training, especially when the majority of the labels are wrong. We know from the extensive Self-SL literature that pre-training without labels can learn representations that are almost as good as their supervised counterparts for linear classification on the dataset they were trained on (eg Imagenet for SimCLR). Therefore one can assume that the same would hold for other datasets, eg CIFAR.\n* The paper is not self contained and not clearly written, eg parameters like \"unlabeled loss weight $\\lambda_U$\" are not defined. The \"method\" section is not presenting the method, but rather a discussion on the differences of the proposed methodology to other p. The experimental protocol and the metrics are not presented (ROC-AUC, \"peak\" etc). \n\nQuestions and notes:\n* What would happen if you would use an out-of-the-box, ImageNet pre-trained SimCLR model for Clothing-1M? I.e. does the gains come from Self-SL pre-training on the target dataset, or any good starting representation would also work? \n*  It would be nice and fair to also show a performance upper bound for each method/backbone (ie a column for 0% noise and Supervised Learning), especially for the tables that contain results for different backbones (Tables 1,2). \n* Why would label-noise 90% make sense in single-label classification? I can understand that this could happen with missing labels, but in a single-class, low-res dataset like CIFAR it doesn't seem to me realistic to say that we are interested in the case where 9/10 labels are \"wrong\". Did the authors consider YFCC10M and multi-class classification as a more fitting test bed for this approach?\n* \"increasing λU from 150 to 500 improved DivideMix accuracy from 60.2% to 61.3%\" -- given that there are discrepancies, it would be fair to fully reproduce the numbers of DivideMix for all noise levels, and use the exact same hyperparameters for the two methods (since they only differ in the first stage). \n* It seems that some interesting results are only mentioned inline in the text: the ROC-AUC curves for Clothing-1M and comparisons vs (supervised) ImageNet pre-trained models for CIFAR (an interesting comparison using external data from a different domain)\n* It is unclear what interesting insights we get from Fig2, ie observing the curves during Self-SL vs supervised pre-training. In fact there seems to be some learning rate mismatch: the DivideMix methods seem to be trained with a step LR schedule (and not cosine like the \n* \"Using a small subset of the training set, we tuned the [hyperparmeters]\" is this common practice? How big is this subset, and is this a subset with noisy labels or an \"oracle\" subset with clean labels? Do you do separate hyperameter tuning for both your approach with Self-SL pre-training and Supervised (ie DivideMix)?  \n* In order to understand better why Self-SL pre-trainin helps over supervised, it would be interesting to see some results for the (real) noisy dataset; eg a study on examples misclassified from one vs the other method.\n\n\nOverall recommendation: \nThe authors show an interesting result, that they can match the performance of ImageNet pre-training on Clothing-1M with Self-SL pre-training on the noisy dataset itself. But that seems more of an application of SSL pre-training for LNL, rather than a contribution for the LNL task. Given that the paper also lacks in other ways and doesn't really give insights on why/how Self-SL pre-training helps for this task , I would be currently leaning towards rejection from ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}