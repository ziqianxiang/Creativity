{
    "Decision": "",
    "Reviews": [
        {
            "title": "review ",
            "review": "They propose a representative learning framework to mitigate task-irrelevant information. \n\n\nComputational costs:\n- It would be nice to add a table of results on comparing the performance of the methods. In CIM, since the method needs to construct \\Sigma_XX, the method can be slower than the baselines, could you comment on it? \n\n\nQuestions to the authors:\n- could the authors comment in table 3, why larger models are obtaining worst-group accuracy? \n- The method has two hyper-parameters of lambda, alpha => could the authors add a discussion on their impact?\n- In table 1, adding VIB seems to be the basic baseline nice to have, could you add VIB in table 1? I wonder why VIB is omitted in some of the tables.  \n- numbers reported in the original GDRO paper (Sagawa et al, 2019) are higher than reported in table 3, could you specify which setup in their work you report and explain why the results differ from Sagawa et al, 2019? (Sagawa et al, 2019 had multiple setups in  their work.)\n- Sagawa et al, 2019 which is a strong baseline for comparison for your work, also had MNLI as one of the experiments, could you explain why you did not test on this dataset? \n- in table 2 JiGen is based on alexnet, not resnet-18 and direct comparison is not meaningful, you need to use the same encoder to provide a fair comparison between methods. Again could the authors comment on why VIB is omitted in table 2?\n- In page 13, A.2, what is \\epsilon? could you confirm which models you have used for the baseline? are they all use the same encoder? Have you tuned \\epsilon for your method? Overall, I think the authors have tuned several hyper-parameters like batch_size, lr, alpha, \\epsilon, \\lambda for their method, but this is not done for the baselines like VIB.\n- On Appendix A.2, page 14 above table 4, what is m? what is re-weighting tuning? is this finetuning also done for the baselines?\n- In  figure 3, could the author comment why they have not used the same setup as 5.2 in Arjisky paper where they have multiple digits and decided to use the simpler setup with two digits? Also Kim, 2019 used multiple digits.\n\n\nReasons to reject: \n- I have doubts about the way VIB is tuned in this work, looking into appendix, I do not see that parameter \\beta in VIB paper, which specify the weights between the cross-entropy loss, and the compression loss is tuned. Given that the proposed method has two hyper-parameters, for a fair comparison well tuning the baselines' hyper-parameters are needed. Also, looking in table 4, it seems that the authors have tuned learning rate, batchsize, .. for CIM but not  the other baseline methods. \n\n- In section 3.2, the task is to reconstruct a random digit from the input class, which I cannot make much sense of it, could the authors comment why reconstructing a random digit from the input? Since the digit needs to be constructed is random, how would you evaluate the performance of the method?  \nThe other shortcoming with this experiment is that the authors are showing a handful of figures which does not even match between the first and second row, so one can compare them, and they do not report any evaluation metric.\nAlso to me black background and red square are both spurious since you want to reconstruct the digit, and it is not clear why authors think one is a relevant signal while the other is spurious in section 3.1. Still, they need to report the proper evaluation metric on how the reconstructed images are correlated with the spurious red box and background. \n\n\n\n\n\n\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A good idea, but with somewhat weak evaluations.",
            "review": "#### Summary\nThe paper proposes a contrastive approach to reduce the effects of spurious correlations on a downstream prediction task.\nThe method employs triplet loss on the gram-matrices of image features. This objective minimizes the\ndistance between the gram matrices of the positive samples and maximizes this distance for negative samples. \nConsequently, this contrastive setting helps to extract features that are invariant in the positive examples. The paper further investigates the robustness of learning such invariant features.\n\n#### Strong Points\n+ I find the approach interesting and technically sound. \n+ The method also shows improvements in downstream classification, OOD generalization, and subgroup performances.\n\n#### Concerns and Comments:\n\n1. The paper does not provide enough details on how positive and negative examples are sampled. This is very important for the relevant downstream classification task, as shown in the table in Appendix B.3. I would be keen to know the details of selecting positive and negative samples for each experiment.\n\n2. Also, the assumption of having access to the information regarding positive and negative examples is too strong. I understand why it helpful for the method, but can the authors provide motivation or some realistic setting where this assumption may hold.\n\n3. I would like the authors to comment on how this method differs from other invariant representations learning methods. Self-supervised contrastive methods also learn certain invariances, does the supervised contrastive setting provide us some benefit? I am probably confused about this as I didn’t find the explicit explanation of selecting positive and negative samples.\n\n4. In addition to the proposed supervised-contrastive regularization, I am curious if the authors tried a self-supervised contrasting scheme.\n\n5. The triplet objective is defined on the gram matrices which correspond to capturing textures of images (in style transfer literature). Would it be right to say that the proposed method enforces similarity in textures of the positive examples? This would imply that the classifier is forced to be more biased towards the texture information [1].\n\n6. I am curious if the authors tried the triplet objective directly on the perceptual features, instead of their gram matrices. It would be nice to see the difference.\n\n7. What’s the role of $\\alpha$ in negative examples distance in Eq. 4? Is it to determine the distance for negative hard-mining?\n\n8. I think the experiment in Fig 2(bottom) shows that the model is creating some variance in the background region. To me, this resultant image (m o x) seems rather difficult to classify than the input black image.\n\n9. Are the red squares at the bottom of MNIST digits correlated with the digits i.e. square of different colors (or at different positions) appear with different digits? If not then how does the presence of the red square at the bottom of MNIST images makes the task challenging? I think it would make more sense to show the behavior of a model trained on red squares (spuriously correlated with digits) and tested with blue or some other color squares.\n\n10. The experimental description in Sec. 4.1 is hard to understand. Also, the details do not seem consistent with Fig. 3(a) for e.g. in the text, \"at test time the digits 2 are colored blue but in Fig. they are colored yellow. \n\n11. Other experiments lack the details on the datasets and the experimental setup such as how positive and negative samples are selected. For e.g. it is unclear how the subgroup tasks are defined.\n \n12. Some of the text is misplaced and focuses on some details which are not relevant to the method. For e.g.\n    a. The objective function $\\mathcal{L}_{con}$ is referenced in the caption of figure 1, without defining or referencing it in Fig 1.\n    b. $\\alpha$ is used in Eq. 4 as a margin for distance with negative examples, however, the term $\\alpha$ in Fig 4 confused me which refers to the experiment Sec. 4.1.\n\n\n\n[1] Geirhos et al. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting but writing needs work",
            "review": "This paper presents a contrastive base approach to try and disentangle spurious ('task irrelevant') features from task-relevant ones. This is an important problem to tackle. The key idea of the paper is that using a contrastive approach to pull together positives and push apart negatives in a transformed (Gram matrix) feature space will lead to the effect of forcing task relevant features to arise. A number of empirical results are provided showing the benefit of their new loss function.\n\nHowever, the paper's motivating example and overall writing is somewhat confusing. Firstly, I really did not understand the motivating example in Figure 2 at all. It is unclear. A more intuitive explanation of why the contrastive loss along with a Gram matrix should lead to the desired task relevance property is missing. Secondly, the loss \"CIM + VIB\" is used in a number of places but it is unclear what form this loss takes. Since this loss seems to achieve the best results, a more thorough discussion of this loss is warranted. Finally, I also did not quite follow what the connection between the transformation network and autoencoder in \n\nTo decouple the effect of the transformation from the contrastive loss, a baseline would have been to run the contrastive loss on the output space similar to Khosla et. al. 2020. This seems like an important experiment to run. \n\nI think the paper needs more clarity in it's motivation, writing and experiments to really drive home the message which is interesting but still a bit incomplete.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good, but is contrastive loss required?",
            "review": "The paper proposes an approach to mask parts of the input samples, so as to train NN models that are more robust to spurious background information and/or domain changes.\n\nThe proposed architecture is quite similar to the ones implementing the attention mechanisms in many fields of computer vision (e.g. segmentation, or re-ID). The architecture presented in Figure 1 indeed involves a side NN branch to predict a single channel (soft) mask, in charge of weighting the input sample features. Here, the input channels are directly weighted by the mask (while weighting might affect arbitrary intermediate features in the more general attention-based mechanisms). \n\nThe main originality of the paper lies in the contrastive loss adopted to constraint the representation \\Phi used to derive the mask (see Fig1).\n\nThe experimental results are convincing. However, it is not clear that the benefit demonstrated in these experiments are due to  the contrastive input morphing, as claimed in the paper title. This is because no result is presented to discuss how the model behaves when \\lambda is set to zero, i.e. when the contrastive is not exploited and only the attention mechanism is implemented. Hence, it is not possible to conclude that the benefit observed in the experimental results is related to the contrastive nature of the transformation learnt by the side branch. Those additional results are definitely needed to support the claim made about contrastive learning, and would change the score from 'marginally below' to 'marginally above'. (or even to 'Good paper, accept' if some of the clarity issues pointed below are solved).\n\nA number of additional issues, whilst less critical, probably also deserve a deeper investigation:\n•\tWhy does the representation \\Phi considers the same number of channels as the inputs? Do the performance of the method change when changing the number of channels C in \\Phi ?\n•\tThe training procedure description lacks of clarity. Defining the dimension of involved variables would certainly help. Moreover, it is not clear how the terms in Eq(4) should be interpreted: what does || M , M* ||^2 denote, when M and M* are two matrices ?\n•\tIs there a link between the first line in Figure 2, and the procedure described in Eq(4) ? or does this line just correspond to the training of an autoencoder without being connected to the contrastive loss ?\n•\tIn Section 4.2, the experiment demonstrates that training a model with the proposed attention-based mechanism improves performance compared to other models in the particular case of a dataset merging multiple domains. However, the methodology adopted in this section does not demonstrate that the proposed method helps in improving the generalization when learning on a domain and testing on other domains. This second question would be more relevant to address in the experiments.\n•\tIn Section 4.3, end of first paragraph, should ‘group’ be replaced by ‘subgroup’ in ‘…well the subgroup level even without explicit group label information’?  In Table 3, worst group accuracy is significantly improved. What about best-group accuracies?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}