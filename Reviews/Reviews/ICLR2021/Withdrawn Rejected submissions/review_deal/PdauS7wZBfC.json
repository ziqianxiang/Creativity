{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper extends recent work (Whittington & Bogacz, 2017, Neural computation, 29(5), 1229-1262) by showing that predictive coding (Rao & Ballard, 1999, Nature neuroscience 2(1), 79-87) as an implementation of backpropagation can be extended to arbitrary network structures. Specifically, the original paper by Whittington & Bogacz (2017) demonstrated that for MLPs, predictive coding converges to backpropagation using local learning rules. These results were important/interesting as predictive coding has been shown to match a number of experimental results in neuroscience and locality is an important feature of biologically plausible learning algorithms.\n\nThe reviews were mixed. Three out of four reviews were above threshold for acceptance, but two of those were just above. Meanwhile, the fourth review gave a score of clear reject. There was general agreement that the paper was interesting and technically valid. But, the central criticisms of the paper were:\n\n1) Lack of biological plausibility\nThe reviewers pointed to a few biologically implausible components to this work. For example, the algorithm uses local learning rules in the same sense that backpropagation does, i.e., if we assume that there exist feedback pathways with symmetric weights to feedforward pathways then the algorithm is local. Similarly, it is assumed that there paired error neurons, which is biologically questionable.\n\n2) Speed of convergence\nThe reviewers noted that this model requires many more iterations to converge on the correct errors, and questioned the utility of a model that involves this much additional computational overhead.\n\nThe authors included some new text regarding biological plausibility and speed of convergence. They also included some new results to address some of the other concerns. However, there is still a core concern about the importance of this work relative to the original Whittington & Bogacz (2017) paper. It is nice to see those original results extended to arbitrary graphs, but is that enough of a major contribution for acceptance at ICLR? Given that there are still major issues related to (1) in the model, it is not clear that this extension to arbitrary graphs is a major contribution for neuroscience. And, given the issues related to (2) above, it is not clear that this contribution is important for ML. Altogether, given these considerations, and the high bar for acceptance at ICLR, a \"reject\" decision was recommended. However, the AC notes that this was a borderline case."
    },
    "Reviews": [
        {
            "title": "Prective coding shown to converge to backprop gradients for abritrary computational graphs",
            "review": "The paper extends prior work on equivalence between predictive coding and backprop in layered neural networks to arbitrary computation graphs. This is empirically tested first on a simple nonlinear scalar function, and then on a few commonly used architectures (CNNs, RNNs, LSTMs), confirming the theoretical results. The importance of this advance is highlighted by noting that the demonstrated equivalence shows how in principle modern architectures could be implemented in biological neural systems, and that the highly parallel nature of predictive coding could lead to efficient implementations in neuromorphic hardware.\n\nThe paper is very well written, easy to read, and includes a nice introduction section with a fairly comprehensive overview of backprop, and the problems related to its potential implementations in biological systems. I also appreciated the \"tension in a chain\" metaphor illustrating the dynamics of backprop and predictive coding. That the exact backprop gradients are computable in a fully local system with Hebbian plasticity for an arbitrary graph is an interesting and promising result.\n\nThroughout the text, predictive coding is quoted as biologically plausible. This isn't strictly true as noted already in (Whittington & Bogacz, 2017), as e.g. dedicated error nodes are not known to exist for every cell in the brain. I'd suggest calling this \"potentially biologically plausible\", and including a short discussion on how these plausibility concerns could be addressed.\n\nAll in all, the results are interesting, open up interesting directions for future work, and I recommend the acceptance of the paper.\n\nAdditional questions/suggestions:\n- Is the fixed-prediction assumption a limitation to biological plausibility?\n- Fig. 2 shows the model converging at high inference learning rates for the case of the scalar function. Is the 0.1 rate used for CNNs the max that was stable, or could higher values be used to reduce the computational overhead?\n- What convergence condition was used?\n- How does convergence speed depend on the size/diameter of the computational graph? This is similar to Fig. 9, but asking a slightly different question -- i.e. how many iterations are needed to reach convergence as a function of graph size.\n\nTypos:\nFig. 1 caption: \"backwawrds\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Generalisation of predictive coding approach to arbitrary computational graphs could be quite powerful",
            "review": "### Update after author responses: \nWhile the author address some of my comments, I would have still liked to see a more detailed discussion of how the algorithm compares in terms of algorithmic scaling, which I think is relevant because it is a fundamental property of the algorithm, even if it is targeted towards understanding biology. So my score remains the same.\n\nSummary:\n\nThe authors extend recent work on MLPs to show that predictive coding converges asymptotically to exact backprop gradients on arbitrary computation graphs. They construct predictive coding networks for common architectures and show that it works well.\n\nOverall, I vote for an accept because I think the generalisation is quite useful and interesting for training deep networks with local learning rules. The authors demonstrate that this method works, but haven't demonstrated its computational advantages clearly enough. There are some issues of clarity that I have also outlined below.\n\nStrengths:\n+ The generalisation of the earlier MLP results to arbitrary computational graphs is quite powerful esp. since it can be applied to most deep learning architectures.\n+ The experimental evaluation includes all popular deep learning architecture, and it's impressive that this works on all of them. The experimental evaluation is also extensive.\n  \nWeaknesses:\n- The increase in computational cost (of 100x) is mentioned quite late and seems to be glossed over a bit.\n- Due to the potential for parallelisation in the predictive coding network, a comparison of wall-clock time for training on highly parallel setups might have been very interesting.\n- For RNNs and LSTMs, the equivalent predictive coding network is generated after unrolling the network, which means that the predictive coding network has no memory advantage over BPTT and a huge performance penalty. This is related to the previous point, where the utility of the predictive coding network is not demonstrated sufficiently.\n\nClarity:\n- Many of the figures are almost unreadable on paper. E.g. Fig. 3.\n- Algorithm 1 does't seem to be referenced anywhere.\n- In fig. 1 bottom, $\\delta$ missing in the denominators\n- Eqn. 3 is a bit sloppy, where derivative w.r.t $\\theta$ is suddenly equated to a derivative w.r.t $\\theta_i$.\n- If $\\epsilon_i = v_i - \\hat{v}_i$ then eqn. 7 is inconsistent with this, since $\\frac{d\\epsilon^*_i}{d\\theta}$ would be $-\\frac{d\\hat{v}_i}{d\\theta_i}$ (missing -ve sign). Unless I misunderstood something.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clear paper - marginal originality",
            "review": "##########################################################################\n\nSummary:\n \nAuthors propose that predictive coding gives similar convergence as backprop algorithms by extending the work of (Whittington &Bogacz 2017,  https://www.mrcbndu.ox.ac.uk/sites/default/files/pdf_files/Whittington%20Bogacz%202017_Neural%20Comput.pdf) to arbitrary graphs. This is an important topic both for the application of classical deep nets to neuromorphic hardware, but also for our understanding of computations in biological tissues.\n\n1. The work presented in this paper follows similar results by Amit (2019) or Lilicrap, and provides with numerical simulations comforting the theoretical predictions. \n2. Authors present an extension of the previous work to arbitrary graphs, and they apply their claims to LSTM and RNN models.\n3. It provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. This makes the point of the paper more convincing and complementary to similar works.  \n\n##########################################################################\n\nConcern:\n \nA major issue of this paper is to not identify its originality. The extension to « arbitrary graphs » or to CNNs is straightforward in theory, While it is not explicitly stated in  (Whittington & Bogacz, 2017), an unwrapped RNN is by definition a feed-forward graph and is therefore a direct application of their work. \n\nConcerning novelty, the actual derivations (eg for the LSTM) are original and the simulations clearly support that original contributions. However this material is at the end of the paper or in appendices. Recentered on this original contributions and how this makes a suitable contribution to the community would make the paper acceptable to be accepted at ICLR.\n\n##########################################################################\nminor:\np.2 « backwawrds »\np7 « dataest » & p 14",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper is not clear enough",
            "review": "#### Summary of the paper\nIn their paper, the authors demonstrate that Predictive Coding (PC) is a local approximation of back-propagation and could then be interpreted with Hebbian learning rule (a neuro-plausible learning rule). This result has been first demonstrated by [1] with MLP network (on the MNIST dataset) and the presented paper extend this finding to CNNs (on CIFAR10, CIFAR100 and SVHN), RNN and LSTM. \n\n#### Pros\n* The authors provided experimental evidences on a wide variety of networks' type and databases.\n* The link between neuro-plausible learning rule and back propagation is interesting.\n* The paper is well situated in the literature.\n* The authors are providing the code for clean reproducibility\n\n#### Cons\n* The mathematical definition and notation of the paper are not rigorous enough. It makes the paper unclear and hard to follow.\n* Some crucial points would have deserved in-depth discussion and are just ignored (see below)\n* The paper is not well enough motivated: what’s the point of such a local approximation beside the neuro-plausibility (faster ? Consume less resources ? …)\n* The demonstration seems to include only one kind of loss function, which does not match the claim of the paper\n\n#### Recommendation\nGiven the limited impact and the lack of clarity of the paper, I would tend to reject the article.\n\n#### Detailed comments:\n* The gaussian parametrization used by the authors constrains the comparison between PC and backprop to networks with L2 loss function. One cannot claim to approximate arbitrary computational graph if one demonstrates the approximation on a specific loss function (which is known to poorly perform on classification problem). So could your framework be generalized to more effective loss function like cross entropy ? If yes, what would be the underlying probabilistic hypothesis ? This should be included in the paper, as it will strongly strengthen your claim.\n\n* The PC framework proposed by the authors propose a solution to the ’non locality’ of the back propagation (to be a bio-plausible mechanism). However the authors also raised the weight transport problem. On my understanding the proposed framework is still suffering from weight transport as the backward connection weights are the transpose of the feedforward one (due to the derivation of the forward operator). The paper would deserve an in-depth discussion concerning this point.\n\n* What is the computational advantage of local approximations ? Is it saving computational resources (computational time, memory…) ? A comparison of the algorithmic complexity between PC and back-propagation would be valuable to support your claim. In the discussion, the authors mention that their framework, being substantially more expensive than back-prop network, could be deeply parallelized across layer. The authors should provide experimental or theoretical evidences that such parallelization is enough to mitigate the higher number of inference steps (i.e. 100-200) needed by their PC framework.\n\n* The concept of ‘fixed-prediction assumption’ introduced by authors in the paper considers that each (v_i) are fixed to their feedforward value. Then what is the point of the Eq. 2, as you already know the value of the activity vector? On my mind this is here a crucial point, as this is dealing with the core principle of PC : an inner-loop (i.e. the expectation step) that find the most likely v_i, and an outer loop (i.e. the maximization step) that update the parameters. I have the intuition that this problem arises because the authors are tackling a discriminative problem (i.e. finding a mapping between inputs and labels) using a generative model (PC is a generative generative model as described by [2, 3]). Can you please clarify this point ?\n\n* What is the testing procedure of your network ? Is it a simple feedforward pass (which I suspect) or is it an Expectation-Maximization scheme. Your algorithm 1 shows only the training procedure (as you need the label information to perform the computation). If this is a simple feedforward pass, what would be the advantages of the inference process (more robustness ? Better prediction ?)\n\n## Typos and suggestions to improve the paper :\n* The authors state (3rd paragraph page 3) that they are considering a generative model. If it is the case, the formula p(v_i) = product(p(v_i | parent(v_i)) is inaccurate as the authors forgot the prior p(v_N).\n* Eq 1 : The derivation between the first and the second line of Eq.1 has to be demonstrated or referenced (at least in annex)…\n* The authors consider the posterior is a marginal probability (see eq1, and subsequent paragraph). In general the posterior is a conditional probability (this specific point makes your equation 1 hard to grasp because readers are not making the link with the classical ELBO, i.e. the negative free energy). In general, the probabilistic notations are not rigorous enough, and it makes the rest of the mathematical derivation complicated to follow. \n* The authors should reorganize the Annex to make sure it follows the reading order (Appendix D is cited first)\n* Caption of Figure 1 : backwawrds —> backwards\n* Page 3, 2 lines below eq 1 : as as —> as \n* Page 4, 4 lines below eq 3 : forwards —> forward\n* Figure 3, which is on my mind the most import one, is shown but not cited in the core text.\n\n[1] Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5), 1229-1262.\n\n[2] Rao, Rajesh PN, and Dana H. Ballard. \"Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.\" Nature neuroscience 2.1 (1999): 79-87.\n\n[3]Friston, Karl. \"A theory of cortical responses.\" Philosophical transactions of the Royal Society B: Biological sciences360.1456 (2005): 815-836.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}