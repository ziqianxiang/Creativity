{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper the authors propose an approach to improving the accuracy of the classification problem based on deep neural networks by detecting the in-domain data from background/noise.  The strategy is designed in such a way that the detector and the classifier share the bottom layers of the network.  Theoretical proof is given and experiments are conducted on a variety of datasets.  The novelty of the work is to come up with a better estimate the pdf of the data and use it to help the classification based on the deep neural networks.   There are concerns raised by the reviewers regarding the related work, the exposition and the experimental design.  After the rebuttal from the authors, which is meticulous, some of the issues unfortunately still stand.  The paper needs to make a stronger case in order to be accepted, especially, for instance, the theoretical and empirical comparison with the existing techniques sharing the similar idea. "
    },
    "Reviews": [
        {
            "title": "experiments are not sufficient",
            "review": "This paper proposes a training method for classification, with the goal of training with less data. The proposal is to train an auxiliary classifier at the same time. The auxiliary classifier and the main classifier share the early layers. The auxiliary classifier is a binary classifier that discriminates training data versus background/noise data. The proposed method is evaluated on image and speech classification tasks.\n\nOn the positive side, the experimental results show consistent benefit across tasks. The facial recognition comparisons should be taken with a grain of salt, because of how the background data were collected (see page 7 and appendix). Other than that, the results seem quite consistent.\n\nHowever, I have substantial concerns. The experiments are not sufficient to support the claims: there is little results on training with less data; certain critical experiments are missing; some comparisons are not fair. The theoretical content is detached from the proposed method and hence not important. I'll elaborate in the following.\nThere are three angles to look at the proposed method:\n\n1. Data augmentation. Part of the proposed method is building an extra class of training data that are background or noise. One example is given in Figure 5. It seems feasible for image and speech tasks.\n\n   A critical point, that is missing from this paper, is how would this data augmentation work by itself without the auxiliary classifier. In other words, what if we just train a classifier with n+1 labels, where the extra label's training data are the background or noise data points? At inference time, one simply uses the first n logits. How much of the reported benefits would this simplified method achieve? This would show how important is the data augmentation versus how important is the auxiliary classifier.\n\n   The appendix stated that the background/noise data include animal faces collected from open source images. Therefore, the discussion around Figure 4 and Table 2, which claims robustness advantage over ArcFace, is invalid. It's not a fair comparison when the training data is expanded with a particular emphasis.\n\n2. Semi-supervised learning with less labeled data. It seems that the most natural and meaningful task is where the auxiliary classifier is trained on all in-domain data, both labeled and unlabeled, while the main classifier is trained on a small number of labeled data. It's important to compare with existing methods in this setup.\n\n   Unfortunately this paper does not include such experiments. Table 8 includes reduced-training-set experiments on CIFAR, however 3/5 of the training set is still way too big, and the remaining 2/5 are not utilized as unlabeled in-domain data.\n\n3. Regularization of the early layers. This is the stated rationale for the proposed method. The auxiliary classifier is designed to encourage the early layers to learn more meaningful features, and Section 3 supports this relation.\n\n   Section 3 is very difficult to read. The conclusion is that the cross entropy loss of the auxiliary classifier is positively correlated with the quality of the estimation of unknown parameters of the data distribution, and therefore training with the joint loss of equation (2) is implicitly related to a good estimation of parameters, which in turn is related to better feature representations in the early layers (an implicit assumption here is that good features for the auxiliary binary classification are also good features for the multi-class classification, which could be a question by itself).\n\n   This relation, however, is qualitative rather than quantitative. In other words, neither training nor inference of the proposed method utilizes the content of Section 3. Section 3 mostly serves as a motivational argument. The auxiliary classifier idea is intuitive by itself, and it seems that Section 3 can be removed without affecting the core content of this paper.\n\n   The stated goal of regularization is to learn with less data. However the only results on this subject seems in Table 8 where 3/5 of CIFAR data are used. Table 8 is not the best setup, as mentioned earlier in the second point. Even not considering semi-supervised learning, 3/5 of the CIFAR training data are too many for a training-with-less-data experiment.\n\nMinor points:\n-- Figure 3 is not a fair comparison. With the joint training, the roles of ResNet blocks are likely shuffled. Some information that gets processed in later layers during normal training may get moved to early layers in joint training, and vice versa. Hence comparing at a fixed layer is not meaningful. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "improving robustness of DNN classification w/ auxiliary classifiers for distractors",
            "review": "To improve multi-class classification problem using DNNs, authors propose to do multi-task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. It is common to have some data which is non-representative of good training samples but they can be used (under DBT) to make first few layers (and thus all) of network more robust.\nResults are satisfactory but:\n1. Can this idea not be done simply by having one more classification unit which detects if x is in-domain or noise? I believe that experiment should be done. That way, whole network can be trained for this. I believe my suggestion is different from \"DBT(ABC-last)\" of appendix.\n2. The multi-task formulation should have a hyper-parameter lambda in Equation 2. Is there a reason it is set to 1? I think it is standard to experiment with that. Perhaps equal weightage to two tasks is not optimal.\n3. It is not clear if I use a regularization technique like MixUp or label smoothing, will I still see improvement?\n\nI am not much aware of related work BTW.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review by AnonReviewer1",
            "review": "This submission proposes a training strategy that leverages background/noise data to learn robust representations. Experimental results show that the use of this strategy generally leads to improved performance. \n\nQuality: The quality of this submission is lacking in several respects. \n\n1) You abstract states that there has been not much work done on using background/noise data whilst estimating model parameters. This is not true. What about NCE and all the follow up work?\n2) What you are describing is a feature extractor that is using one objective and classifier that uses another objective. There is prior work in this area. However, your related work section suggests something entirely different. \n3) You are cramming way too much into 8 pages. \n4) Section 3, 4 and 5 do not work together. Section 3 talks about Cramer-Rao bounds and Lebesgue measures while section 4 shows something that looks like a multi-task training objective. Section 5 does not have baselines other than the trivial one of not using your approach. \n\nClarity: The clarity in this submission is lacking. Given Section 4, the point of sections 2 and 3 is very unclear. \n\nOriginality: It is hard to judge originality of this submission given the way it is presented. Which theorems are your contribution? Which of many statements you make in sections 2, 3 and 4 are your contributions? \n\nSignificance: It is hard to judge the significance of this submission given the way it is presented. \n\nPros: This submission contains evaluation across many modalities. \n\nCons: One-sided presentation with links to existing works missing. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}