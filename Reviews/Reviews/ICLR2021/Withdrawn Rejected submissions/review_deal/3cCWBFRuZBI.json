{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for the paper: TRAINING-FREE UNCERTAINTY ESTIMATION FOR DENSE REGRESSION: SENSITIVITY AS A SURROGATE",
            "review": "The paper is proposing a very simple method to obtain uncertainty a posteriori on any network that was not trained with the intent to obtain uncertainty estimates.\n\n#### Idea and justification\nThe idea proposed in the paper -- perturbing input and feature maps --, sounds extremely simple and basic. The method is explained clearly, however, the paper clearly lacks intuition or justification about why this approach would work. It is not clearly explained why the variance of the perturbed outputs should relate to uncertainty of the pointwise estimate of the network. The variance seems related to the local sensitivity of the network, which could, intuitively, be caused by higher uncertainty, but there are many other aspects that would then have to be taken into account. It is also not well explained or justified why the proposed theorem supports the claim of using this variance as an estimate for uncertainties.\n\n#### Technicality\nThe technical part and the theorem are sound. The architecture and the method are well described.\nHowever, the authors do not say if they are estimating aleatoric or epistemic uncertainties and how these types or uncertainty are treated by their methods. Maybe that clarifying that would help the reader get a better understanding and intuition about the approach.\nAlso, since the paper is working on regression problems, I think that the theory would require some stability assumptions.\n\n#### Baselines\nThe baselines used are relevant. However, the baselines used are usually capturing epistemic uncertainty, where it is not clear what type of uncertainty the model is going after here.\nIt is surprising that MC-dropout and LLM give better results than Deep-Ensembles, which is normally the best method for uncertainty (though very expensive), and usually performs better than drop-out. Therefore, it I think that the specificities of the tasks used for evaluation should be considered and discussed more deeply.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Important problem, but the theoretical motivation of the method is not convincing, while the similar methods can be found in the literature.",
            "review": "The paper considers training-free methods for quantifying uncertainties in deep learning. The problem considered is of great importance nowadays. The majority of existing uncertainty estimation methods require NN models to have a very specific structure and thus limit the applicability of these methods. Moreover, many of these methods have computational overhead which limits their practical applicability. \nThe paper addresses both these requirements: 1) it can be used for any model \"off the shelf\" as it does not require retraining of the model 2) its inference time is of the same order as a forward pass of a model. \n\nThe authors focus on \"regression\" problems, Super Resolution and Depth Estimation, where considered models plus proposed methods performed comparable (or even better) than competitors.\nThe authors suggested two setups for uncertainty quantification: Black Box Uncertainty Estimation (BBUE) and Gray Box Uncertainty Estimation (GBUE), depending on the model's availability. They suggested using a tolerant transformation of input for both these options, which does not change output much. For BBUE, the transformation was \"infer-transformation\" -- similar to data augmentation (rotations, flips). For GBUE -- \"infer-noise\" and \"infer-dropout\". The former is an injection of Gaussian noise to a particular feature map. The latter is the same, but using Bernoulli masks.\n\nI have three main concerns about the paper:\n1. I don't think that the reasoning provided in Section 3.3 gives a good basis for the method suggested. The authors argue that the variance of the perturbed network gives a good proxy for the uncertainty. However, the arguments provided do not support that. Given the arbitrary nature of perturbation, the variance has absolutely no connection with the error of the neural network. For example, this approach allows perturbing networks for different inputs in a different way (say, different magnitude), which apparently will lead to meaningless uncertainty estimates. I don't mean that the approach does not work completely, but Section 3.3 can't explain it.\n\n2. The proposed approach partially has been considered in many papers that are not cited in the text. For example, [1] discusses the power of test-time augmentation, which is exactly the \"infer-transformation\" approach from the paper. At the same time, the injection of dropout in fully-connected layers of pre-trained networks for uncertainty estimation seems to be a relatively standard approach. See, for example, section 3.3 in [2] (though, I am not aware of any systematic study in this direction).\n\n3. The resulting numbers are reported without analysis of their significance (standard deviations of scores, significance tests, ...), while the difference in scores between the methods in many cases is very small. Thus, the significance of the observed benefits of the proposed methods is not clear.\n\nAs a minor comment, I would like also to mention that the authors argue that NLL is not an ideal metric for evaluating uncertainty estimation for regression. Unfortunately, no support of this claim is found in the paper.\n\nOverall, I think that the paper targets important research directions, the current findings don't seem to be well-motivated and supported by experiments well enough.\n\nLiterature\n----------\n\n[1] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. A Ashukha, A Lyzhov, D Molchanov, D Vetrov. arXiv preprint arXiv:2002.06470\n\n[2] Deeper Connections between Neural Networks and Gaussian Processes Speed-up Active Learning\nE Tsymbalov, S Makarychev, A Shapeev, M Panov. International Joint Conference on Artificial Intelligence (IJCAI-19)",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of paper #96",
            "review": "The paper proposes to estimate the uncertainty of deep regression models by a sensitivity surrogate.  The latter measures the variance of the network response w.r.t. different types of perturbations. The authors consider perturbations of the input and two types of perturbations in intermediate layers: additive Gaussian noise and multiplicative dropout noise. They compare their approach experimentally with other methods, by correlating the estimated uncertainty to the respective L1/2 loss available for inputs with known ground truth. These experiments include regression networks for super resolution and for monocular depth estimation. \n\nThe paper is clearly written. The experiments seem to confirm that the proposed method is at least on par with other methods and has the advantage that it does not require probabilistic models and thus can be applied to networks learned in a purely discriminative way.\n\nMy main concern about this paper is conceptual. In my understanding, any notion of prediction uncertainty should rely on the posterior distribution $p(y|x)$, where $y$ is the predicted quantity (e.g. depth map). This distribution is however unknown, especially if we learn a deterministic predictor $h(x; \\theta)$ in a purely discriminative way. The approach proposed in the paper claims, in a nutshell, that the uncertainty of $p(y|x)$ can be estimated by perturbing $x$ and/or $\\theta$ and collecting a statistic of the predictor $h(x; \\theta)$ outputs. It remains unclear to me, what conditions on the (unknown) distribution $p(x, y)$ and on the predictor model class $h(x; \\theta)$, $\\theta\\in\\Theta$ ensure that the proposed sensitivity measure is a good surrogate for prediction uncertainty.\n\n*Further comments/questions*\nPlease explain in more detail how you choose the respective perturbation strength in order to guarantee that they remain \"tolerable\". How are these strengths chosen for the comparisons in Figs 2 and 3?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}