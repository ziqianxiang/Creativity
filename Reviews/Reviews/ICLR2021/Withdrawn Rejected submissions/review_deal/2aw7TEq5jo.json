{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting, but not good enough",
            "review": "** Summary\n\nThe paper mainly aims to study and design efficient proxy tasks to speed up NAS-based channel search algorithms (called “width optimization” in the paper). The methodology is, first to run search procedures on a simplified proxy task (e.g. using fewer channels or depths, smaller input resolutions, or smaller scales of dataset), then apply a simple extrapolation rule to transfer the searched width configurations to the original search space if needed. The paper benchmarks three search methods (AutoSlim, DMCP and MorphNet) on two datasets (ImageNet and Cifar-100) with a variety of proxy tasks. Experiments shows that many of the proxy configurations are good enough to obtain improved search performances, while the search cost is reduced by orders of magnitude. \n\n** Contribution and significance\n\nThe empirical findings of the paper are interesting: it is good to know that channel search can be efficiently done in much smaller proxy tasks, whose performances keep consistent with that of searching in the original space.  \n\nHowever, I still feel that the technical contribution is limited. The significance of the empirical results is relatively low. First, introducing small proxy task for speedup is a convention and widely used in many general NAS frameworks (e.g. NASNet). While in the paper, only a very restricted search space (i.e. width) is considered, which seems to be neither novel nor general. Second, though the paper reports considerable speedup when searching with the proxy tasks, to my knowledge it may be because channel search is not that difficult – even though the search space of width is huge, the “pattern” of optimal solutions seems to be simple (Fig A1). Fig 3, 4 also imply that results of different search methods is very similar. Finally, as mentioned by EfficientNet, adjusting network widths only may result in relatively limited improvements; it is important to search width, depth and resolution jointly. So, I think the authors may study the joint search space on the proxy tasks to improve the significance. \n\n** Experiments\n\nI am not satisfied with the diversity of the experiments especially in Sec 4.1~4.5. The underlying optimal (i.e. ground-truth) widths for ResNet18 and MobileNetv2 are similar under different configurations respectively. It is unclear to distinguish whether the search methods capture some architecture bias so that they are insensitive to different proxy configurations, which makes the conclusion less convincing. So, I think at least experiments under multiple target FLOPs budgets are required in each subsections respectively. More architectures and benchmarks on other tasks, e.g. object detection, are also encouraged here.\n\n** Presentation\n\nThe writing and organization of the paper is fair. The presentation of the experiments needs to improve. For example, for each chart in Fig 3~6 the baseline configurations (i.e. overhead=0) are recommended to describe in the caption respectively. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review with comments",
            "review": "This paper proposes a method that projects networks and datasets to smaller sizes of them in order to reduce the computational complexity of width optimization methods. \n\nPros)\n- The idea of reducing the pruning space to lowered space to reduce the computational costs seem to make sense and but not that impressive\n- Experiments are done multiple times, and reporting means and std make the results more convincing.\n\n\nCons)\n- Though the idea looks sound, but the materials that support the idea are not clearly stated. Moreover, the paper needs to be refined all the sections and sub-parts, and all the figures are not clear, so one may not readily grasp all the materials. \n- Invariant to various aspects of network architectures as the authors stated in the paper does not seem to have sufficient evidence. The authors only tried with two architecture ResNet18 and MobileNetV2. \n- In section 4.2 about depth projection, ResNet18 in figure 4.(a) does not hold the claim. Why ResNet18's accuracy fluctuate much compared to that of MobileNetV2? \n\nComments)\n- This paper uses a popular github's training code with some training settings which include complicated data augmentation methods such as RandAug that leads to much better results. However, it has not been studied well with the performances of pruning techniques under additional augmentation methods. I think the goal of this paper may not heading towards maximizing the accuracy so recommend the authors to use fundamental settings to see the results more easily and fairly.\n- Width transfer may seem to work well, but any guides of how to use this method is not clearly stated. Any suggested rule-of-thumb?\n- Need more experiments to support the claim",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper needs improvement",
            "review": "This paper has conducted many experiments about reducing optimization overhead. Authors have researched on the view of width, depth, resolution and dataset size to study the issue of width pruning while preserving the top-1 accuracy. It has also conducted experiments on many datasets. However, my concerns are as follows.\n\n =====   The writing of this paper should be improved.  =======\n\nFor example:\n1. In the first line of 3.1 Notation, “We use [n] to represent a set {1, 2, …, n}”. Then, I don’t see anything related to [n]. Why define this [n]? In my view,  do you want to see the selection of channels in a layer?\n\n2. Missing  \"to\" in \"One of the key reasons why width optimization is so costly is due “to” its limited understanding by the community. \"\n\n3. The first sentence of abstract and introduction almost say the same thing. And can you divide it into several pieces?\n\n=====   Some missing justification in assumptions. ========\n\n1. In the 4-6 lines of section 3.1, the paper says that better test accuracy can be obtained by just multiple width multipliers. However, it needs justification. \n\n2. In the 3rd line of section 3.2, I do not really follow your attention that attempts to understand \"them further\". \n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "I think the paper lacks technical novelty and theoretical support. No methods are newly proposed.",
            "review": "This paper conducts experiments to investigate the transferability of optimized widths obtained by width optimization algorithms across different predefined projection dimensions, including width, depth, resolution and dataset size. Experimental results show that the optimized widths are similar across different configuration settings, suggesting width optimization can be firstly conducted on networks with more convenient configurations (such as smaller width, depth, resolution and dataset size) and then transferred into the target networks.\n\nHowever, I have some concerns as follows:\n1.\tThe authors simply conduct some experiments to analyze the effect of different configuration settings. No novel methods or ideas are newly proposed in this paper.\n2.\tThe experimental results cannot apparently support the conclusions. In figure 3, 4, 5 and 6, the fluctuation in accuracy is significant but the authors conclude that the optimized widths are similar and highly transferable. Moreover, at each projection setting, the experiment is not thorough enough since there are only 4-5 points for each line, which cannot rigorously reflect the trend.\n3.\tToo many words like “practically” and “empirically” are used, suggesting that many parts of the paper are lack of theoretical support. For instance, some settings like the source width multipliers are selected totally by human experience.\n4.\tSub-section 3.4 “Experimental Setup” should be put in Section 4 “Experiments” instead of Section 3 “Approach”.\nAll in all, this paper is lack of novelty and theoretical support.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}