{
    "Decision": "",
    "Reviews": [
        {
            "title": "A UNIFIED FRAMEWORK TO ANALYZE AND DESIGN THE NONLOCAL BLOCKS FOR NEURAL NETWORKS",
            "review": "This paper proposes a framework to unify the non-local blocks methods for neural networks. By using a graph convolutional neural network (GCNN) method, in particular, the spectral approach for GCNN proposed by Defferrard et al (2016 a, b), this paper shows that 5 methods for non-local blocks are special cases. Some experiments are also conducted to show that this unified view  may provide some advantage for computer vision tasks such as image recognition. \n\nThis paper still has a lot to improve. \n\n1.\tThe main problem is the presentation. It is problematic. The language usage has too many issues. Many grammatical errors and awkward sentences still exist, which makes reading of this paper not easy. For example, \n\nPage 2, 3rd paragraph: “… to better concerns the position wise affinity.”\nSage parapgrah: “… (2) We unifying well-known nonlocal-based blocks such as NL, NS, A2, CC, CGNL and anaylize them”\n\nSection 2.2, beginning sentence: “To unifying other …”\n\nSection 2.3, 2nd paragraph: “we can that existing non-local…”\n\nAnd many others. \n\nThe notations and quantifies are not sufficiently defined or consistently used. For example,\n\nSection 2.2, first paragraph: “… k-th order Chebyshev …”, then in the following equation (3), the order becomes K-1. \n\nBelow Eq. (4): $W_k \\in R^{C_s x C_1}$. What is C_1? No explanation at all.  \n\nPage 4, second paragraph: M_{ij} = f(X_{i, :}, …  What is $f()$? No explanation. \n\n2.\tThe experiments need more details. In Section 3.1, the experimental setup is specified. Because no explanation is given regarding what experimental setup is used for the baseline methods, it is assumed that the same setup is used for them. For the baseline methods, it is not clear if the parameters have been sufficiently tuned.    \n\n3.\tThe following is minor compared to the above – maybe the expectation is a little high -  the novelty of this paper appears somewhat not so impressive. The spectral approach of GCNN was first proposed by Bruna et al. (2013) by defining the convolution operation in the Fourier domain. (Henaff et al. 2015) considered localized filtering, and then (Defferrard et al. 2016 a, b) used Chebyshev expansion of the graph Laplacian for further improvement of efficiency. This paper has no novelty in terms of GCNN; the only novelty appears that it simply provides a GCNN view to the existing non-local block methods. Indeed, Theorem 1 points out the re-formulations or re-interpretation of the non-local methods from this view point. \n\nThough the re-interpretation of the existing non-local methods from this GCNN view is interesting, the main concern about the novelty is that, ** this view does not appear to bring about very new method or methodological improvement. ** – Overall, it appears that the main insight is that, in Section 2.3, it is pointed out that some existing methods cannot guarantee the symmetry of non-local based operators or non-normalized affinity matrix. This point is actually not so convincing because for GCNN graph attention networks (GAT) does not have to use symmetric operations as done in (Defferrard et al. 2016) or GCN but can still achieve competitive performance. Another point from this spectral view point is that some existing non-local methods used “the 1st-order approximation with sharing weight (NS) rather than the complete form of the 1st-order approximation,…” This contribution does not appear so significant, since it uses an extra additive term rather than the pure first-order term. The real advantage of taking this GCNN view, if existing, is still not so clear. This dampens the enthusiasm about this paper. \n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.\n\nMikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper first provides a unified framework for the nonlocal-like extensions. Based on this, an improved version of non-local block is proposed, named ChebyNL, which encodes the graph structure and guarantees the existence of the graph spetral domain for improving the robustness and accuracy. The effectiveness of the proposed block is tested in several image/video recognition tasks.\n\nConcerns:\n1. The most concerns lie in the experiments part. As the nonlocal-like blocks could help to model the long-range dependency, the extensions of non-local block is proven to be effective in numerous large-scale dense prediction tasks, such as object detection in COCO (NLNet, DNL [a]), semantic segmentation in Cityscapes (CCNet, DNL [a], ANN [b]), and action recognition in Kinetics (NLNet, DNL [a]). \n\nBut in this paper, the proposed block is mostly proven to be effective in recognition tasks (CIFAR-10/100, CUB, UCF101), which are not widely used in previous papers. Also, the main challenge for recognizing tiny images in CIFAR datasets may not be the long-range dependency modeling.\n\nThis would largely downgrade the impact of the proposed paper. It is important to verify the effectiveness for modeling long-range dependency of the proposed ChebyNL block in the widely used dense prediction tasks.\n\n2. Typo:\n    P2: \"Based on these analysis\", \"ChabySNL\"\n\n[a] Disentangled non-local networks, ECCV 2020\n\n[b] Asymmetric Non-local Neural Networks for Semantic Segmentation, ICCV 2019",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper proposes a framework for designing nonlocal blocks in neural networks (NNs), which unifies existing works on nonlocal-based computational blocks. Although the contribution is not extremely significant, the spectral view point is interesting.",
            "review": "This paper proposes a framework for designing nonlocal blocks in neural networks (NNs), which unifies and interprets existing works on nonlocal-based computational blocks. By viewing the nonlocal blocks as graph convolutional filters parametrized by Chebyshev polynomials, the existing works can be view as 1st order polynomial parametrization. The authors further propose a new nonlocal block by analyzing the \"irrationality\" of the existing works under the framework. Experiments are conducted to illustrate the improved performance of the proposed nonlocal block.\n\n** Strength and weakness\n\n+ The paper is generally well-organized. It is fairly easy to understand.\n+ Unifying some of the existing works on nonlocal blocks under the same framework of graph convolutional filters approximated by 1st order polynomial of graph laplacian is interesting.\n+ Using the new framework, more nonlocal blocks can be derived with greater flexibility.\n+ Numerical experiments have been conducted to illustrate the newly proposed nonlocal block under the framework\n\n- There are quite a few linguistic issues throughout the paper -- I would recommend the authors to proofread the manuscript more thoroughly.\n- I could be wrong on this, but normalizing and making the affinity matrix symmetric is not necessarily a significant contribution of the paper.\n- The are some questions on the paper that warrant more elaboration from the authors.\n\n** Questions for the authors\n\n1. In Property 1, why does the affinity matrix A need to be non-negative. Even if it is not, the graph laplacian is still non-negative.\n\n2. Page 3, the last paragraph in section 2.1: Can you please elaborate on why the original nonlocal block by Wang et al. cannot sufficiently suppress the noise?\n\n3. Equation (3) and (4): The authors propose to use Chebyshev graph CNN (4) as a framework for nonlocal blocks. In order for the expansion (3) to be simplified to equation (4), the affinity matrix needs to be normalized (property 2.) However, the prior works on nonlocal blocks (all can be formulated by equation 4 as noticed by the author) can be viewed simply as an ansatz for graph filter approximation. I am wondering why does it need to be related to the Chebyshev graph CNN?  In other words, even it they do not fit into the framework proposed by the author, they are not necessarily \"irrational\" as proclaimed by the authors.\n\n4. The authors claimed (in section 2.3 paragraph two) that the non-existence of the graph spectral domain weakens the robustness and flexibility of the prior works. Why is that the case?\n\n5. The proposed nonlocal block indeed marginally improves the performance of the NN. However, the authors also mentioned that more parameter matrix need to be learned during the process. I am wondering whether there is a significant increase in the model parameters that makes the performance comparison unfair.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review - AnonReviewer3",
            "review": "**Summary**:\n\nThe paper presents multiple non-local models from the spectral perspective of graph filtering. Essentially a parameterized filtering operation could be implemented in the spectral domain or equivalently it could be applied in the spatial domain. The paper presents a general formulation that approximates the spectral filtering using polynomials of degree K in the spatial domain. Using this formulation, a new non-local block is proposed and compared with the existing non-local methods that are shown to also fit this formulation. The proposed method achieves superior results on multiple datasets.\n\n**Strong Points**:\n\nIt is very important to see the graph filtering from the spectral perspective. Processing visual data with different variants of non-local / transformer / graph based methods is gaining a lot of track but the spectral perspective is mainly ignored. This paper could start a trend of analysing methods from this perspective.\n\nIt is a good point that this work makes the connection between the different non-local methods and the unifying framework is sound.\n\nThe proposed variant of non-local method achieves superior results on multiple datasets.\n\n**Weak Points**:\n\nWhile I like interpreting non-local from the spectral view, I think the paper has some limited theoretical contributions and it is unclear in some parts. \n\nConnection to the graph neural networks(GNNs) literature should be made. One of the most popular forms of graph networks is GCN [A] that is essentially the same first-order approximation of the spectral filtering as given by Eq. 4.\n\nThe analysis from the spectral view seems limited. The ChebySNL is obtained by the same analysis of spectral filtering as GCN (apart from keeping separate W1 and W2) and the only insight from the spectral view seems to be that A should be symmetric.  \n\n\nThe terminology should be more clear. First of all, the operation $A Z W$ is in the spatial domain / vertex domain not in the spectral domain. It is a first-order approximation of spectral filtering $ U g(Λ) U^T Z$., but since the eigendecomposition of $L$ is avoided this is not a spectral operation.\n\nAblation studies should be made. First of all, since the symmetry of the affinity matrix is essential for the existence of the spectral decomposition, its practical value should be tested. All the elements introduced in ChebySNL compared to the original NL should be verified in ablation studies. Starting from the NL, every single new element should be verified by an additional experiment. The key elements are the symmetric matrix and the usage of the additional ZW term. \n\n[A] Thomas N. Kipf and Max Welling.  Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017\n\n**Conclusion**:\n\nAlthough the paper has limitations, I think that analysing the spectral view of non-local operation is a good direction and I tend to give a *6: marginally above* rating.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}