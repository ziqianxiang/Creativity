{
    "Decision": "",
    "Reviews": [
        {
            "title": "Further development needed?",
            "review": "This paper is concerned with the assessment of the vulnerability of neural networks to adversarial attack, according to its ability to interpret new classes using previously-learned features. The underlying assumption made is that combinations of features from a sufficiently rich learned collection should be able to help identify members of previously unseen classes. For cases where the representational quality is poor, and the learned features do not suffice for new classes, the authors expect adversarial perturbation to be more successful. Conversely, they aim to show that when adversarial defense is added to a learning framework, it facilitates the identification of new classes from previously-learned features.\n\nIn order to assess the generalizability of learned features to new classes, the authors propose the use of zero-shot learning. Using a leave-one-out strategy (\"Raw Zero-Shot\"), features are learned over all but one class, and then tested on the class left out. The results are then combined over all possible choices of class to exclude from the training set. For the evaluation, the authors consider an internal clustering validation measure (Davies-Bouldin metric), and a measure of average L1 deviation between normalized probability vectors (soft label vectors) of the zero-shot classifier (where the new class is excluded from the training set) and standard classifier (new class included).\n\nThe authors then proceed to use these measures to evaluate the performances of classifiers in two main scenarios: generalizability of features when adversarial defences are applied vs when not applied; and the correlation of success / failure of adversarial attack vs the generalizability of features.\n\nPros:\n-----\n\n1) The experimental validation does provide evidence that the vulnerability of adversarial attack does correlate with the quality of the features. However, this connection is not unexpected, as adversarial perturbation techniques generally seek to discover nearby regions in which the features are less discriminative.\n\n2) The proposed method is easy to implement, and could be beneficial in practice when some indication is needed as to whether the training data is robust to attack, or can be expected to generalize well to new data classes.\n\n3) The paper is generally readable. However, more intuition could be given in the introduction to support the author's contention that feature generalizability could be related to the vulnerability to adversarial attack.\n\nCons:\n-----\n\n1) The technical contribution of the paper is not very novel. It essentially boils down to the use of zero-shot learning with cross-validation techniques.\n\n2) As proposed in the paper, Raw Zero-Shot takes a rather simplistic view of the nature of classes or their relative contribution to the training process; for example, it makes no allowance for differences among the class sizes in the training set. Also, Raw Zero-Shot seeks to characterize the overall generalizability of features over the entire training collection. Can any insight be provided at the level of individual test objects, so as to help give a more useful characterization of adversarial examples?\n\n3) This work could be better situated with respect to the relevant literature on the characterization of adversarial attack and of the generalizability / transferability of learned features. For example, other work on adversarial attack has taken the view that adversarial examples can be easier to detect when they lie far from the underlying data manifold. It seems that this work is implicitly making a related assumption. Discuss?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting premise, but needs further investigation",
            "review": "Summary: This paper proposes a new method to understand the vulnerability (or robustness) of networks to adversarial examples through the lens of zero shot learning. The authors introduce two metrics to measure the ability of models to generalize to unseen classes---which they suggest is tied to learning better feature representations. They then perform experiments to measure the effect of adversarial defences on the representation quality. Overall, the paper argues that robust models learn more general representations, and that representation quality can serve as a proxy for model robustness.\n\nComments: Understanding the representations learned by robust models is an important problem and has received extensive attention in the robustness community (see related work comment below). While this paper is studying an important problem from an interesting perspective (ZSL), I do not find the motivation for the proposed metrics and the experimental findings convincing. \n\nCrucially, the authors do not provide sufficient justification for why the proposed metrics are a good measure of feature quality (apart from some high-level intuition). This makes it hard to draw any conclusions from the experimental results in Section 5. In particular, I see some trivial failure modes of both the proposed metrics. The DBM score could be low if the representations for the unseen class are densely clustered in a neighborhood that is arbitrarily far from other related classes. For the AM score, the authors assume that \"â€˜Standard Classifiers should output a good approximation of the Amalgam Proportion since\nthe class is known to the Standard Classifier in the training phase\". However, they do not provide any evidence that this is actually the case. \n\nIf this assumption were to fail, the AM score would be low if both models (trained with and without the target class) have a uniform probability distribution over the remaining N-1 classes for samples of the target class---i.e., even if they do not learn general features. Also, the AM score could be high even if the model learns general features, if the other model trained without the target class predicts incorrect soft labels. \n\nOne way to demonstrate that these metrics are meaningful would be to compare them with performance evaluations performed in traditional ZSL, i.e., assuming some access to ground-truth attribute vectors or based on similarity in some embedding space (such as word vectors). If the proposed metrics are indeed good measures of feature quality (when supplementary information is absent), then these should correlate with traditional performance measures (when supplementary information is present).\n\nThe authors suggest that representation quality (as measured by DBM and AM) is correlated with the strength of adversarial attacks. However, as the authors admit, the numbers in Table 3 show no clear trend.\n\nOverall, as it currently stands, I think this paper could be significantly improved in terms of justification of the proposed methodology and experimental evaluation.\n\nOther comments:\n\n[Related work] There has been a line of work on showing that representations learned by robust models which the authors fail to discuss. For instance, recent work shows that robust representations are more perceptually-aligned (e.g., https://arxiv.org/abs/1805.12152; https://arxiv.org/abs/1906.00945) and better for downstream tasks such as transfer learning (e.g., https://arxiv.org/abs/2007.05869, https://arxiv.org/abs/2007.08489). Another directly related paper is https://arxiv.org/abs/2005.01499, where the authors demonstrate that robust models are also better for zero-shot learning. \n\n[Evaluation] In Section 4, the authors mention that both the train and test samples for the unseen class are used for evaluation. However, from what I understand, the train samples are used to train the baseline model based on which the AM scores are computed. This discrepancy could skew the results. The authors should repeat the evaluation on the test samples alone.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Key claims not fully established",
            "review": "This paper proposed two new metrics evaluating the quality of the learned features of neural networks, and used these metrics to investigate the correlation between feature quality and adversarial attacks and defenses. While this paper provides some interesting insights into how deep neural networks work, these conclusions are not well-established because the authors did not provide enough evidence to support the key assumption in the paper.\n\nMore specifically, one of the key assumptions in this paper is that the metrics, DBM and AM, characterize the feature quality. However, this assumption is unwarranted. The authors provide an intuitive explanation that projection of the learned features provides a good measure of the feature generalizability, but it is not clear why AM can capture such relevance in feature space. AM simply computes the difference in logits of the raw zero-shot classifier and the standard classifier. It is entirely possible that the logits of the seen classes do not capture the degree of projection of learned features onto these classes. This is particularly likely for the standard classifier, where the logits of the seen classes can be so small for the target-class images that their relative scales do not make much sense or suffer from large errors. It would make the argument much stronger if the authors can conduct experiments where AM is evaluated for a pair of neural classifiers whose feature generalizability is known. For example, one neural classifier can be trained to focus on generalizable features using techniques such as invariant risk minimization (IRM) or distributional robust optimization (DRO), and the other trained with regular empirical risk minimization. Similar experiments are also needed to prove the validity of DBM as a metric of feature quality.\n\nAs a related problem, there seems to be lacking a consistent result for the adversarial defense experiment. DBM can increase or decrease for different defenses, and so can AM. It looks like neither metric has a definite relationship with adversarial robustness. It would be useful to investigate whether such a loose connection is due to the imperfect metric, as I mentioned above, or because feature quality does not correlate definitely with adversarial robustness.\n\nFinally, some parts of the paper are not clearly-written. For example, it is unclear to me how the Pearson correlation is computed in the adversarial attack experiment is computed. Is it the correlation between the metric of the benign sample and the adversarial sample? How does this new metric inform the correlation between feature quality and adversarial attack? What is the conclusion of this experiment (there seems to be no conclusion at all in the main paper)? The paper mentioned that there are anomalies in the coefficient of AM with BIM and PGD for ConvNet & VGG, DeepFool for CapsNet, but I do not see the anomalies in Table 3. What are the anomalies? The visualization of the feature is unclear to me as well. It would be helpful if the authors can provide mathematical expressions for a clearer definition.\n\nIn short, without clear evidence of the validity of these metrics, the conclusions and claims in this paper are unwarranted. It would greatly improve the paper with further experiments and a better explanation of the details.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not discussed thoroughly enough",
            "review": "I am not sure if I appreciate this work enough. In general, there could be something there, but the writing is unpolished and severely lacks discussions and context. I would vote solidly for reject.\n\nFirst of all, I have issues with the phrase \"representation quality\". When the word \"quality\" crosses people's minds, the first thing they think is whether it's good or bad. In this sense, the results of LeNet having better quality than ResNet is confusing at least. I think what the authors wanted to mean is more similar to \"representation robustness\".\n\nSecond, the discussions are too few and far between, and it stays at an intuitive level which is probably not good enough for 2020 standards. For example, in terms of clustering tightness there could be some arguments made from information-theoretic terms that e.g. links with the bottleneck theory from (Shwartz-Ziv and Tishby 2017). No such attempt was done at all for this paper.\nAs another example, why would one prefer a low AM feature value? How does it relate to robustness? The evidence is not at all enough. Showing adversarial training reduces AM is an interesting anecdote, but it doesn't show a reduced AM indicates a more robust classifier, e.g. there is no evidence that LeNet cannot be attacked, despite having a lower AM than other methods. What is a good AM? Does increasing AM really means lack of robustness or what is a zone that is more sensitive? None of these discussions were made and we are only presented with a lot of numbers that are hard to de-cipher.\n\nAM is also on conceptual shaky grounds since it relies significantly on the correlation among categories, which is an assumption and not necessarily true. Besides, the variance among different runs are very large (Table 2). I have not been convinced that this would be any useful metric that indicates robustness.\n\nMy general sense for improving this paper to a publishable level (it might never be publishable in ICLR but could be publishable in other places) is to at least correlate results in AM with results on some other metrics -- e.g. does it give us any guaranteed robustness at a certain level? Things like this can be tested on the MNIST and CIFAR dataset and may give us better insights on how these metrics correlate with robustness. Looking into the line of work on certified robust classifiers could also be helpful to place the metrics on more concrete ground.\n\nShwartz-Ziv and Tishby. Opening the Black Box of Deep Neural Networks via Information. arXiv 2017\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}