{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a channel pruning method to compress and accelerate pre-trained CNNs.\nThe reviewers suggest further analysis of the experimental results to help explain the gains in performance, as well as point out some errors in the formulation. The paper is also found similar to meta-pruning method. The authors are encouraged to re-submit the paper after adding the analysis and improving the related work section. "
    },
    "Reviews": [
        {
            "title": "Review - Hyper-structure Network",
            "review": "This paper proposes a channel pruning method to compress and accelerate pre-trained CNNs. This paper proposes to use gated recurrent unit (GRU)-based hyper-structure networks (HSN) to generate channel importance, and Gumbel-rounding operations are subsequently applied to the importance values to generate stochastic masks to prune a CNN. The authors further introduced layer-wise trainable scaling with a modified update rule to accelerate convergence. While the tricks employed above are not original on their own, the combined method is still a novel amalgam, which learns inter-channel and inter-layer relationships in a unified package. Experiments on CIFAR-10 and ImageNet with ResNet and MobileNet-V2 showed that this method is competitive with existing state-of-the-art (SOTA).\n \n### Advantages \n* The combined method is novel, as it:\n\t* uses HSN to learn to prune without retraining models; and\n\t* learns inter-channel and inter-layer relationships in a unified HSN.\n* Good results that are competitive with the SOTA. \n \n### Issues\n1. Section 3.2, “This process can be summarized as using Gumbel-Softmax distribution (Jang et al., 2016) to approximate Bernoulli distribution.” This is not true, as equation (2) clearly uses the “round” operation which gives discrete outputs, while the Gumbel-softmax approximation by Jang et al. is continuous with temperature annealing.\n2. Section 3.2, the authors mentioned that $a_i$ is drawn from a uniform distribution but remains constant during training. What is the purpose of having only constant values that are not even hyperparameters as the inputs to the HSN? This seems unnecessary and introduces unwanted variance.\n3. Section 3.4, “When generating the vector multiple times, most parts of vectors are the same, the different parts are trivial and do not have impacts on the final performance.” Why? More results are necessary to verify this.\n4. The experiments are not run multiple times to give mean and standard deviation results.\n \n### Other minor issues\n* Section 2.1, “A straight forward way to”, should be “straightforward”.\n* Figure 1 is not mentioned in text.\n* In Section 4.4, “In Fig. 4 (c,d),”, should be Fig.3 (c,d).\n* It is not apparent if layer-wise scaling was used for ImageNet models.\n\n### Summary\nWhile this method proposed in this paper is novel and the results are competitive, the reviewer believes that in its current state, this paper lacks additional experiments to address issues 2, 3 and 4 mentioned above.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Some parameters need further investigations. ",
            "review": "This work explores the model compression problem with a hyper-network to adaptively determine the preservation of inter-channel and inter-layers. To this end, they use a shared-GRU layer to explore the relations in consecutive layers and then FCs to generate the coefficient vectors which indicate the preserved rates within each layer. As a result, the inter-layer relation can be given from GRU and inter-channel relation can be given from FCs. In this paper, a learnable factor in each layer is also involved in the network optimization to better balance the classification performance and FLOPs regularization. Experimental results also validate the performance of the proposed method.\n\nOverall, I think the paper is easy to follow and the derivation is also clear to understand. Nonetheless, there are still some problems that need to be further explored, given as follows:\n\n1. This paper only investigates the channel pruning, but the title uses the \"model compression\". This might be not very specific since model compression serves as a general topic and consists of many approaches, such as channel pruning, quantization, knowledge distillation and tensor decomposition. \n\n2. This paper also uses FCs to generate the coefficient vectors (ideally 0-1 code) to indicate the importance for each channel. However, this practice has been investigated in other channel pruning papers and is thus not new. Just to name a few,\n\n[NeurIPS2019] AutoPrune - Automatic Network Pruning by Regularizing Auxiliary Parameters  \n[Pattern Recognition] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference\n\n3. In subsection 4.4, the experiments on Figure (a)-(b) reveal that “changing $\\lambda$ does not have a large impact on the final performance of a sub-network, and our method is not sensitive to it”. I think the reason why the method is not sensitive toward $\\lambda$ should be further analyzed. As in the appendix (E) shows, in Eq.(8), the update of $\\alpha$ contains $\\lambda$, so $\\alpha$ can actually be reviewed as the adaptive variable, and for different $\\lambda$ during the network training, the value of will gradually converge to the corresponding scale which considers $\\lambda$.\n\n4. I think there are some mistakes in analyzing the bias of FLOPs regularization. As stated in appendix (B), the relative magnitude of gradients w.r.t  $\\theta_k$ and $\\theta_j$ can be approximately estimated. However, since the ratio value of $v_k$ and $v_i$ is 8, the conclusion about \"the gradient in the early layers are larger than that of the latter layers\" does not hold. I wonder whether it is a typo here. Furthermore, can you explain why the assumption “the magnitude of  $\\partial v_i \\partial \\theta_i$ is similar given different layers” holds herein? \n\n5. In subsection 4.4, “In Fig.4(c,d) should be changed into “In Fig.3(c,d)” instead.\n\n6. How to choose the hyper-parameter p?  \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "#Official Review 4",
            "review": "This paper propose to utilize a hyper-structure network to generate the architecture of the main network, which can help to capture inter-channel and inter-layer relationships. The learnable layer-wise scaling factors are introduced to balance the gradients from different terms. BasisNet can be applied to any network architectures. MCH shows state-of-the-art ImageNet performance in pruning setting.\n\nMy main concern is about the novelty of the paper. The idea to use a hypernet to help pruning is not new. MetaPruning uses a hypernet for automatic channel pruning [1]. GaterNet use a gater network to generate pruning mask for each input image [2]. This paper use a hypernet to generate mask to prune the main network, which can be viewed as a weak version of GaterNet where the mask for each sample is the same. \n\nIn addition, Eq.5 involves 2 hyperparameters, i.e. $\\lambda$ and $\\alpha_i$. $\\lambda$ is set by hand and $\\alpha_i$ is learnable. The two hyperparameters may be deprecated, that is, you only need to preserve $\\alpha_i$ and $\\lambda$ can be merged into $\\alpha_i$?\n\nOverall, I think this paper below the bar of ICLR due to the above weakness.\n\n[1] MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning. ICCV 2019.\n[2] You Look Twice: GaterNet for Dynamic Filter Selection in CNNs. CVPR 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "## Summary\nThis paper proposes hyper-structure network for model compression (channel pruning). The idea is to have a hyper-network generate the *architecture* of the network to be pruned. To do so, the proposed approach use Gumbel softmax together with STE to get around the non-differentiability issue of such design. Additionally, the proposed approach dynamically adjust the gradient for each layer so that earlier doesn't get over-regularized due to the FLOPs regularizer. Empirical results are presented showing better performance compared to alternatives with ablation study on the proposed components (hyper-network and layer-wise scaling).\n\n## Reasons for score\nI'm leaning toward a rejection. I like the idea of both layer-wise scaling and using GRU to model inter-layer dependencies. However, I find them under-studied as the novel components of the paper. While this paper provides seemingly good performance compared to prior methods, they are not really apple-to-apple comparisons, which makes them relative weak signals. Moreover, some of the recent papers that are closely related to this submission are missing. I'm willing to raise my score if the weaknesses I listed below are properly addressed during the rebuttal period.\n\n## Strengths\n\n- The paper proposed a novel formulation towards the channel pruning problem. Specifically, the novelty lies in using GRU with the proposed layer-wise scaling.\n- The results seem good compared to prior literature (with a caveat of having longer training iterations)\n- Ablation of the proposed method ($\\lambda$ and LWS)\n\n## Weaknesses\n\n- The novel aspect of the work, namely the layer-wise scaling, can be discussed in more detail. For example, why do we expect modifying the scaling of layer-wise gradient to make a better gradient than analytical gradient? In my understanding, analytical gradient gives you the steepest descent direction. In this case, why do we expect we can do better by tuning $\\alpha_i$? It seems this is suggesting that we need layer-wise learning rate and such learning rates can be optimized via gradient descent by meta-gradient. It is not clear to me why such a formulation is specific to pruning. Can it benefit training vanilla network by setting $\\lambda=0$? Overall, it is a bit mysterious to me why such a formulation *accelerates* training for equation (4), which is empirically shown in Figure 3 (c)(d). A more in-depth theoretical analysis would be very helpful. Without theoretical analysis, I'd be curious to see more settings empirically. If we sweep multiple learning rates, can LWS stop being better? If we use a different optimizer (say SGD), is LWS still better? Most importantly, if we use LARS optimizer [1], is LWS still better?\n\n- Missing AutoML-based related methods that have strong performances [2-5]. Discussion with these prior methods are needed to better position the proposed method. Comparing with these methods, the proposed method is only comparable. Specifically, DMCP [4] has 47% reduction with top-1 of 76.2 for ResNet-50 and 30% reduction with top-1 of 72.2 for MobileNetV2. Putting these methods into the table and discussion is necessary.\n\n- Comparison with AMC is limited to numbers from the previous paper. From the formulation of this paper, AutoML-based methods are highly relevant. It would be better if AMC is compared with this paper by using the same empirical setup. This is in fact doable as AMC has open source code. Without such a fair comparison, it is hard to understand what are the benefits of the proposed approach over AMC that solves the exact same problem. The paper has argued in the related work that policy gradient is noisy without really providing the quantitative evidences. The numbers from the paper is a really weak signal as AMC only fine-tunes for 30 epochs while this paper fine-tunes for 100 epochs for ImageNet.\n\n- The other novel aspect of the paper is using GRU for designing the network. However, it is not clear if GRU is necessary and why it is a good design choice. The argument for GRU is cross-layer dependences. I'm wondering what the results would look like if we simply use FC for each layer independently. This can better motivate the so-called cross layer dependencies and better motivate the adoption of GRU.\n\n\n[1] You, Yang, Igor Gitman, and Boris Ginsburg. \"Large batch training of convolutional networks.\" arXiv preprint arXiv:1708.03888 (2017).\n\n[2] Yu, Jiahui, and Thomas Huang. \"AutoSlim: Towards One-Shot Architecture Search for Channel Numbers.\" arXiv preprint arXiv:1903.11728 (2019).\n\n[3] Berman, Maxim, et al. \"AOWS: Adaptive and optimal network width search with latency constraints.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[4] Guo, Shaopeng, et al. \"DMCP: Differentiable Markov Channel Pruning for Neural Networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[5] Chin, Ting-Wu, et al. \"Towards Efficient Model Compression via Learned Global Ranking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n\n--------- Post rebuttal ---------------\n\nI've read the rebuttal and I appreciate the additional efforts by the authors.\n\nSpecifically, the authors have addressed my concerns comparing LWS (the proposed method) and LARS. Additionally, the authors have addressed my concerns regarding LWS by conducting more experiments. With another detailed read, I figured LWS updates $\\alpha$ in a lookahead fashion. Specifically, $\\frac{\\partial \\mathcal{J}}{\\partial \\mu}$ in Eq. 6 essentially requires one to compute the loss after the gradient update is being made, which gives it the potential to outperform the analytical gradient.\n\nMoreover, the authors have run additional experiments to demonstrate the usefulness of GRU, which makes the proposed method more convincing.\n\nWhile the authors argued that it is not fair to compare to AutoSlim, AOWS, and DMCP, I disagree. They are all relevant and strong channel pruning methods and the authors should have cited them and discuss the main differences (can be used to prune a pre-trained model or not) in the related work as opposed to omit them entirely.\n\nOverall, I find the paper interesting and it provides several novel aspects: GRU and LWS. Both are empirically verified to be useful in the channel pruning setting. However, the related work section can be further improved. As a result, I raised my score to weak accept.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}