{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an algorithm for training flow models by minimizing the KL divergence in the latent space Z. The paper addresses an important problem in training flow models. However, some major concerns remain after the discussion among the reviewers. The scale of the experiments and the scalability of the approach appear limited in the current version of the paper. Moreover, the applicability of the current theoretical analysis to general distributions is quite limited. "
    },
    "Reviews": [
        {
            "title": "Some interesting ideas, but lacks sound justification. ",
            "review": "This paper proposes a new method of training flow models, instead of minimizing KL divergence in the data space X, the paper proposes to minimize the KL divergence in the latent space Z. However, the problem of this is dimension mismatch, so the KL divergence is ill-defined. The proposed solution is to add noise, such that the KL divergence can be well defined again. In addition, this paper proposes learning a low dimensional prior p(Z) by a Gaussian distribution with low rank covariance matrix. \n\nPro\n\nThe proposed method addresses a very important concern about flow models, which is that the latent space dimension has to be identical to the data space dimension. \n\nThe paper proposes learning the dimensionality of the latent space (prior) distribution p(Z) by parameterizing it as a Gaussian with low rank covariance. This seems like a reasonable design choice. \n\nThe experiments show that for simple synthetic data, the proposed approach is able to recover the true dimensionality of the data. \n\nCon:\n\nThe major caveat is that the method seems to only work for volume preserving flow models (since the term 1 of Eq (12) can only be approximately neglected if the flow is volume preserving); most modern flow architectures are not volume preserving. I think this is quite a major limitation that needs to be explicitly stated when introducing the scope of the work. \n\nIt is unclear why minimizing the KL divergence in latent space Z is better than minimizing the KL divergence in data space X (which is current standard practice) \n\nI think the severity of the dimension mismatch problem is somewhat overstated. If one applies standard training procedure for flow models (i.e. maximum likelihood on data space X) instead of the alternative proposed in the paper (minimizing KL divergence in latent space Z), then the problem in Figure 1 will not occur? I think this is because the dimension of the model distribution is greater than the true data distribution (which is conjectured to lie on a manifold), so log likelihood is well defined. \n\nThe experiments are somewhat weak. Only synthetic datasets are used. If the claimed benefit is better sample generation quality, then at least readers would expect standard benchmarks such as CIFAR. \n\nMinor issues: \n\nThe title is not very informative\n\nThe major contribution of the paper is not stated early on (for example, the introduction seems like a background section). \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good work on training flow models on low-dimensional manifolds but lack some theoretical guarantees",
            "review": "This paper proposes a new method to train flow models on data from low dimensional manifolds embedded in high dimensional ambient spaces. The basic idea is based on minimizing the KL divergence in the latent space, which is equivalent to maximizing expected log-likelihood over the data distribution. Since the KL between two low-dimensional distributions are often undefined, authors propose to use spread divergence as a surrogate. Experiments demonstrate that the authors' technique can successfully model the distributions on low-dimensional manifolds without knowing the manifold beforehand. In particular, the proposed method can recover the intrinsic dimensionality of data manifolds.\n\n#### Pros\n\n* The proposed method simultaneously learns the manifold of a data distribution and the density on it, without the need of manually specifying the manifold.\n\n* The method can give the intrinsic dimensionality of the data manifold. Authors have carried out extensive experimental study to verify this claim using both toy and natural datasets. The method can also be used as an approach for dimensionality reduction.\n\n#### Cons\n\n* The theoretical part is weak. As admitted by the authors, their analysis is not applicable to data distributions that are not absolutely continuous w.r.t. the Lebesgue measure in the ambient space. A stronger analysis should prove that $H(Z_\\mathbb{Q} + K)$ is a constant for fixed $\\sigma_Z$ and volume-preserving $g$. \n\n* Experiments only show that the flow model can learn the manifold correctly. However, data points on the manifold can distribute differently. It seems all experiments in this paper assume that data points lie uniformly on the manifold. It is necessary to see experimental results where data have different densities in different regions of the manifold to check whether the proposed flow model can fit them accurately.\n\n* The proposed method requires using a volume-preserving flow model. This is a big limitation to the expressivity of model architectures.\n\n* In Figure 1, $p_d$ and $p_X$ should be swapped in the legend.\n\n--------------\nPost-rebuttal\n\nI would like to thank the authors for the response. However, my concern on the theoretical part remains. For densities within the manifold, I suggest reviewers consider experiments similar to those in [1][2]. I also agree with R3, R4 on the reinvention of the connection between KL in the data space and KL in the latent space. In addition, I agree with R1, R3, R4 on the scale of their experiments and scalability of the approach. As such, I will lower my score from 6 to 5.\n\n\n[1] Mathieu, Emile, and Maximilian Nickel. 2020. “Riemannian Continuous Normalizing Flows.” arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10605.\n[2] Lou, Aaron, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser-Nam Lim, and Christopher De Sa. 2020. “Neural Manifold Ordinary Differential Equations.” arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10254.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a modification to the latent distribution of a flow model, replacing the commonly used full-rank Normal with a low-rank one which has the form N(0, AA^T). To train this degenerate model, the spread divergence from Zhang et al. (2020) is used and some approximation is made (e.g., ignoring the first entropy term). Experiments on toy data (with 1d intrinsic dimension) and mnist digits generated from a GAN (with 5-10 intrinsic dimension) demonstrated that the proposed model can identify the intrinsic dimension through the rank of AA^T.\n\n##### Originality & Significance\nAs far as I know, the proposal of modeling the latent distribution of a flow model with a low-rank Normal is original .\nOther parts of the paper are more like combination of existing methods and known results (e.g., the invariance of KL under invertible transformation, spread divergence). \n\n##### Clarity\nThe clarity is generally good, although I don't feel the result in equation (4) is worth highlighting in the introduction. It can be a bit misleading as this is a well-known result.\n\n##### Strengths\n* Figure 1(c) is very illustrative as it shows how the distribution is curling up in the latent space to behave more like that it has two intrinsic dimensions, and the conclusion that neural network capacity is wasted by doing this.\n* The experiment results are good and it is clever to use the data generated from a GAN to control the intrinsic dimension of image data, although the data is still very toy.\n\n##### Weaknesses\n* The authors mentioned several times that they \"establish a connection between MLE and KL minimization in Z space\". I guess they are not aware of this result thus trying to make it a contribution of this work. However, the invariance of KL under reparameterization is a well-known result, and it immediately follows that doing MLE = minimizing KL in X space = minimizing KL in Z space.\n* The entropy term in equation (12) can only be ignored given two conditions\n * g is volume preserving, so that the log jacobian term disappears.\n * The variance of K is close to zero.\ndid the authors all use volume preserving flows in experiments? And the second condition can never be true in practice. In the paper an experiment that approximates this term has been conducted to justify the choice. But the approximation is very rough and I'd encourage the authors to try more accurate gradient estimators for the entropy term. For example, there is nonparametric score estimators (https://github.com/miskcoo/kscore) which can be easily plugged in here if you want to make a fast attempt, or you could try parametric estimators like sliced score matching (which requires training a neural network).\n* All experiments are sort of toy and has only <10 intrinsic dimensions. Among them the most complex data is MNIST. Therefore, it is  a question if there exist other easier methods that could also find out the intrinsic dimension given the problem is very simple (e.g., nonlinear PCA).\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A clean approach but lack of empirical evidence. ",
            "review": "\n### Summary:\n\nThis paper uses the idea of spread KL divergence to learn densities for data that lies on a lower-dimensional manifold. The proposed method does not require the knowledge of this dimension and even offers a mechanism to estimate this intrinsic dimensionality. The authors validate their ideas on several toy-experiments.  \n\n### Strength: \n\nThe paper's idea is elegant; it is extremely well written for most parts, and the visualizations are informative. Authors use several compelling examples and make their case favorably.  The notation is pinpoint, and the paper is overall an easy read. \n\n### Concerns:\n\nOne primary problem with the paper is the reinvention of the connection between MLE and KL divergence minimization in Z-space. This is well-documented (see [Papamakarios et al. 2017, Appendix A]; [Papamakorios et al. 2019, Appendix A])--not only authors skipped these citations, but the paper in its current form depicts this as a novel contribution (see Section 1, paragraph 2.)  \n\nI think the extension of this method to large-scale problems is not as trivial. The use parameters of the lower-triangular A scale as $O(D^2)$ can quickly become prohibitive.  Similarly, posthoc computation of eigenvalues of $AA^T$ can become expensive. Further, as noted by the authors in their experiments with MNIST, the real-world data may lie on a noisy lower-dimensional manifold--a modeling challenge that the current theory does not account for. I believe the easiest way to convince me would be to do experiments similar to Brehmer and Cranmer, 2020 on Style-GAN images (see Brehmer and Cranmer, 2020, Section 4, subsection E.) Alternatively, authors can explain why scalability is not an issue and provide other evidence for real-data performance.\n\n#### Minor concern:\n\nIn Cunnigham et al. 2020, the authors consider learning densities for data that lie on lower-dimensional manifolds using normalizing flows. Their approach starts with a low-dimensional latent variable and uses affine transformations to change dimensions; they also use additive Gaussian noise such that the final density is in the ambient space. I believe there are interesting parallels in this work and their work--the authors should consider drawing out this comparison in the revision. \n\n### References:\n\n- Edmond Cunningham, Renos Zabounidis, Abhinav Agrawal, Ina Fiterau, Daniel Sheldon. Normalizing Flows Across Dimensions. ICML workshop, 2020.\n- George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. NeurIPS, 2017.\n- George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. http://arxiv.org/abs/1912.02762. 2019\n\n### Updates after the rebuttal\n\nI found the approach clean and believe it has some merit among competing approaches. However, as detailed in discussions, I am skeptical of the scalability of the approach. I understand that an approach with scale limitations is acceptable and believe authors can benefit from an honest discussion about tradeoffs--which is currently missing. Further, I would suggest adding non-trivial experiments in the same vein as the ones in $\\mathcal{M}-$flow paper. I think with these revisions, the paper can make for a welcome addition to the flows-for-density-on-manifold-literature.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "Pros:\n1. this work propose to learn a manifold prior, by doing so, it can be used to improve the generation and representation quality.\nIntrinsic dimension is applied in the method.\n2. to fix the ill-defined KL, the authors proposed to use a \"bridge\" distribution, so that Q and P can have overlap on their support.\n\n\nCons:\neven though I love the idea of this work, in the experiment section, the authors fail to compare their method with other flow-based method with quantitative results.\nSince the authors claim that their method can improve the generation and representation quality, without any comparison, it lacks of evidence.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}