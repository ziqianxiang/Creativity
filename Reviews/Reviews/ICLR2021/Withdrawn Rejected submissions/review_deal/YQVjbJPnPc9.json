{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer.\nHowever, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept.\n"
    },
    "Reviews": [
        {
            "title": "Predictive Attention Transformer: Improving Transformer with Attention Map Prediction",
            "review": "This paper proposes a novel approach to improve self-attention through by bridging the attention maps from different layers via a chain of convolution-based prediction modules.  In particular, it proposes to augment the existing works on Transformer through supplementary prediction modules by CNN-based attention prediction layers.  The main contribution of this paper is the introduction of CNN-based attention prediction to enhance model predictions. Empirical studies are performed to show the superiority of the proposed model PA-Transformer over several SOTA approaches on NLP and image classification tasks. \n\nReasons for score: \n \n I like the idea of a chain of attention prediction to learn attention dependencies from the previous block. My major concern is about the clarity of the paper. Hopefully, the authors can address my concern in the rebuttal period. \n\nPros: \n \n1. The paper addresses one of the most important issue of transformer: attention dependencies cross blocks or layers. For me, the problem itself is real and practical. \n2. The proposed predictive attention transformer (PA-Transformer) is novel for capturing the attention dependencies transformer layers and address the problem of the self-attention maps learned independently for each layer. The design for using the PA-Transformer to tasks of NLP and image classification is reasonable and interesting. \n3. This paper provides comprehensive experiments, including both NLP and image classification results, to show the effectiveness of the proposed framework.  \n\nCons: \n \n1. The paper claims that Multi-channel is one of the first works to take attention maps as multi-channel images for explicit modelling in the section of Introduction. It is better to clarity this point and give the difference between Multi-channel and multi-branch on method section.\n2. Why does the paper call the proposed model as predictive attention? From my understanding, it is a type of residual connection for attention. Are the attention results used to predict some kind of tasks in intermediate layers?\n3. Is the source codes available to reproduce the work? \n4. It would be more convincing if the authors can provide a set of experiments about a baseline just using residual connection to bridge the layers, instead of CNN-based attention map prediction module, in the rebuttal period. \n\nQuestions during rebuttal period: \n \nPlease address and clarify the cons above \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "unclear why the proposed method improves performance",
            "review": "\n\nThis paper's main topic is the enhancement of Transformer models for improving performance in a task-independent way.\n\n\nThis paper first points out that the attention maps are trained independently among layers in the Transformer models.\nThe authors then hypothesize that the performance could be improved by integrating an additional module for estimating attention maps.\nTherefore, they propose a method that yields the attention maps based on the lower layer's attention maps.\n\nThe experimental results on several different datasets from different domains show that the proposed method consistently improves the performance.\nThey are somewhat surprising results.\n\n\nThe following are the questions and concerns of this paper.\n\n\n1,\nThe proposed method seems rather strange; why can the proposed method yield better attention map predictions?\n\nI understand such a phenomenon if we provide the correct attention maps for model training.\nHowever, it seems that the proposed method does not require any additional information on correct attention maps. \nThis is the largest mystery for me about this method.\nPlease elaborate on what architecture or mechanism enables the proposed model to provide better attention maps theoretically or empirically to support the authors' claim?\nIf I did not miss something, there are no clear explanations about it.\n\n\n\n2, \nthe source of the effectiveness:\nThis additional module seems to also work as a sort of skip or short cut connections between layers.\nIn my feeling, the performance gain could be just the direct linking between attention mechanisms in each layer and not be caused by a better attention map prediction.\nAs a recent common knowledge in the community, the correct attention map can be, but not necessarily, a strong correlation to performance.\nPlease reveal the actual source of the performance gain to prove the correctness of the authors' claim.\nOtherwise, there may be a risk of providing the wrong knowledge to the community.\n\n\n\n3, \nRelated to the above two questions, this paper's main concern is that this paper does not provide more in-depth analyses of the proposed method that tell model behaviors or characteristics.\nThere are no intuitive and motivational examples of what kind of situation the proposed method successfully works.\n\n\n\n\n4,\nThe current version does not have the \"Conclusion\" section, which most scientific papers have.\nOf course, there is no rule that the paper always needs to have a Conclusion section.\nHowever, I would like to know why the authors decided not to provide the Conclusion section.\n\n\nI am willing to change my score if I got reasonable answers for all the questions and concerns written in the above reviews.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Adding convolution-based attention prediction module to Transformer to capture cross-layer dependencies",
            "review": "\n**Summary:**\n \nThis paper proposed a modification to the classical transformer architecture and demonstrated significant performance gain on multiple benchmark tasks in both natural language processing and computer vision. Specifically, the authors propose to introduce a convolution-based attention map prediction module, so the dependencies of attention maps across different layers can be captured. With the extensive experiments, the proposed modification is quite effective on improving the model's performance.\n\n\n**Reasons for score:**\n\nThe idea of bridging attention maps across layers in Transformers is intuitive, as later layers can benefit from dependency structures learned from earlier layers. The model is validated on several benchmarks in both NLP and CV, and show some consistent improvements over baselines. However there are many unclear expressions and claims in the paper, and some ablations are missing which might be critical to better understand the module. Besides the paper definitely needs more proof-reading.\n\n\n**Pros:**\n\n1. The idea of treating multi-headed attention maps as multi-channel images is interesting, and the proposed convolution-based attention prediction module is a natural choice under such settings.\n2. The proposed methods are validated with extensive experiments, and the performance gain is consistent and quite significant. This shows the effectiveness of the proposed approach.\n \n**Cons:**\n\n1. When applying transformer architecture to images, the image of shape $H \\times W \\times C$ is flattened as $X \\in R^{N\\times C}$ where $N=H \\times W$. However for regular images the resolution is quite large, e.g., ImageNet is usually used in 224x224, then the N would be ~50k. In Section 4.2.1 it seems even for CIFAR it will be OOM (out of memory), but this is not mentioned for experiments with ImageNet in Section 4.2.2.\n2. On Page 2 the authors claim that \"... experimental results demonstrate the superiority of PA-Transformer in terms of accuracy, memory cost and computational efficiency ...\" but I was only able to see the accuracy improvements; did I miss the experiments for computational efficiency? If it refers to the #Params and #FLOPs in e.g. Table 2, I'm actually curious: according to Table 1, Transformer and PA-Transformer has <0.01K (if not identical) FLOPs. Could you explain how this is calculated? Because if we count multiplications and additions as FLOPs, I think the PA module will definitely introduce more than 10 (=0.01K) FLOPs. \n3. The authors claim several times (e.g. last sentence of Page 3) that self-attention module could \"dedicate itself to incorporate layer-specific knowledge into *residual* attention maps\". It seems arguable since in most cases the self-attention is dominating the generated attention map ($\\alpha$ is usually small). Also, I'm wondering if the 0-layer PA in Table 5 corresponds to a direct skip-connection, i.e. simply copying the $A_{\\text{pre-logits}}$ over to next layer. In fact I think it's a quite important ablation experiment, e.g. replacing \"predicted attention\" with \"attention from previous layer\", and may worth showing the results on other tasks too (e.g. on CIFAR and ImageNet).\n4. Just curious: the $\\alpha$ seems to have large variations across datasets, e.g. on CIFAR it is set to 0.01 but on SNLI and ImageNet it is set to 0.5. In addition to empirical validation results, is there any explanations for this?\n5. In Table 1 and Table 2 since each experiments are replicated for five times, it's better to show the standard deviation (confidence interval) together with the mean value.\n\n**Questions during rebuttal period:**\n\nI've listed my questions in the cons section, and hopefully can be addressed during the rebuttal.\n  \n\n**Some typos and minor issues:**\n\n-- The phrase \"except for\" is misspelled as \"expect for\" in several places (e.g., Page 5, 7th line of Section 4.1.1 \"Models\", Page 7, 2nd line of Section 4.2.1 Settings). \n-- Multiple typos: \"ResetNet-50\" -> \"ResNet-50\", \"Noe\" -> \"Note\", etc. This paper needs more polishing.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not convincing experiments and insufficient novelty.",
            "review": "The idea proposed in the paper is simple - \"predict\" future attention weights using past attention weights. A 2D CNN is used to mix previous N layer's attention maps. Strictly, this is also not \"predicting\" but instead generating\". There is no supervised loss here. \n\nThe authors introduce a PA-Transformer model. The idea is to use previous attention maps to augment future attention weights. A stack of N previous attention weights is modeled with 2D CNN to generate future attention maps. The idea of predicting attention weights (or generating them) is not new (see https://arxiv.org/abs/2005.00743). The difference here is that there is a 2D CNN to model relationships between N previous layers. This is somewhat a pretty incremental extension of the Synthesizer-Transformer model. \n\nThere is also insufficient convincing evidence that using previous layer's attention to generate future attention weights is beneficial. \n\nI think the experiments are lacking. The experiments on GLUE are only comparing against BERT (the least the authors could do is to compare side-by-side with at least a few other models). Machine translation datasets are tiny and ablation studies are unconvincingly run on SST and SNLI. The authors only run experiments using a preloaded checkpoint of BERT and do not apply their architecture to actually pretrain BERT which is also one weakness of this work. Hence, the paragraph beginning with \"pretraining\" is misleading\". The results on GLUE are also weak and could be a result of variance over the existing BERT model. \n\nThe authors should also discuss how this can be implemented in a decoder setting since the current setup will disable causal attention.\n\nOverall, I recommend a clear rejection. I think the key selling point and hypothesis behind this paper (using prev N layer attention) is not well supported. Experimental settings are also weak and there are insufficient convincing experiments to feel that this architecture is doing something useful. ",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}