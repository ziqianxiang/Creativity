{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work investigates the relationship between adversarial robustness and shape bias of neural networks. Reviewers pointed out that one of the primary questions being investigated \"(a) how adversarially-robust ImageNet classifiers (R classifiers) generalize to out-of-distribution examples;\" has already been a primary focus of several prior works, and that many of the findings are already well established, or expected given known connections between adversarial robustness and corruption robustness. I recommend the authors rework the paper to focus more on building upon these prior results. As a possible example, the work would be strengthened if the authors compared adversarial training to other data augmentation strategies known to directly improve shape bias and corruption robustness, does adversarial training provide any unique ood robustness properties distinct from these other methods?"
    },
    "Reviews": [
        {
            "title": "Review of \"The shape and simplicity biases of adversarially robust ImageNet-trained CNNs\"",
            "review": "The authors show that adversarially trained models have higher shape and simplicity biases compared to their vanilla counterparts.\nThe paper is well written and easy to follow. I think I understood most parts of the paper. The authors were explicit in stating which methods they used which is good.\nMy major concerns are:\n1.\tLacking novelty/ comparison to previous work: The authors did not compare to previous work properly. In my general comments, I suggest adding and discussing 11 additional publications. One of their major findings that adversarially trained models are invariant to additive high frequency noise has been shown and discussed in [8] which they did not cite. Other cited papers are cited in a wrong way, e.g. Geirhos et al 2018b did not perform experiments on adversarial examples; there are other examples in the General comments section below.\n2.\tNot relevant/ not surprising results: I am not sure how useful and interesting the finding is that adversarially robust models are more shape biased. It seems intuitive that given that adversarial perturbations are high-frequency, becoming invariant to them should make a model more attentive to low-frequency features such as shape.\n3.\tLacking discussion/ analysis of the results: In the first experiments section, the authors found that adversarially robust models are shape biased. With the NetDissect analysis, they showed that there are more filters in adversarially robust models compared to vanilla models that detect textures. These two results seem contradictory to me and should be discussed. If they are not contradictory, this should be explained.\n4.\tThe number of PGD steps used for the adversarial attack is 7 which is very low. I am not sure whether the results in this paper are meaningful in the sense that the models might not be adversarially robust at all if a higher number of PGD steps/random restarts were used. The authors should test bigger numbers of steps and make the step size dependent on the number of steps.\n\n\nGeneral comments:\n\nPage 2 “In contrast, there was also evidence that classifiers trained on one type of adversarial example often do not generalize well to other image types [Geirhos et al, 2018b, Nguyen et al., 2015]”. What is image types here? Both references do not provide any support for the claim made in this sentence. In Gheiros et al, the authors train on different types of image corruptions such as additive uniform noise or using high/loss pass filters. They do not show results for adversarial examples. Nguyen et al was published two years before adversarial training was proposed by Madry et al. There are other references that would be good to cite here though: [1] show that increasing robustness against adversarial attacks does not increase robustness against translations and rotations. [2] show that adversarial robustness does not transfer easily between attack classes. [3] also show that there is a performance trade-off between robustness types.\n\nPage 3: “Adversarial examples were generated using Projected Gradient Descent (PGD) (Madry et al., 2017) with an L2 norm constraint of 3, a step size of 0.5, and 7 PGD-attack steps.” A number of 7 PGD steps seems very very low. A more sensible number might be e.g. 1000 [4] or at least 200-400 steps. In [5], the authors urge to check the convergence of the attack and that white-box attacks generally converge in under 100-1000 steps. Additionally, Carlini et al [5] write that “the number of iterations necessary is generally inversely proportional to step size and proportional to distortion allowed.” -> Here, a step-size of 0.5 is chosen without providing any reasoning and the choice seems arbitrary.\n\nTable 1: On the torchvision website, the top1 accuracies for the S models are given as: AlexNet: 56.55%, GoogleNet: 69.78%, ResNet50: 76.15%. These numbers differ from what the authors show in Table 1. Working with the pretrained torchvision models myself every day, I know that a ResNet50 from torchvision will get a top1 accuracy of 76.13% (same to what is stated on the used “robustness” package repo of Engstrom et al). Where does the difference to the numbers in Table 1 come from?\n\nPage 4: “R models show no generalization boost on ImageNet-C i.e. they performed on-par or worse than the S counterparts (Table 3c). This is consistent with the findings in Table 4 & 5 in Geirhos et al. (2018a) that a stronger shape bias does not necessarily imply better generalizability.” -> This is completely opposite to what we see in Table 5 in Geirhos et al. 2018a. There, the authors attribute the higher robustness to common corruptions to the higher shape bias of their SIN model: “Again, none of these corruption types were explicitly part of the training data, reinforcing that incorporating SIN in the training regime improves model robustness in a very general way.” Due to the contradictory results, it rather seems that shape bias and corruption robustness might be more disconnected than previously thought. Also, the result that adversarially trained networks perform badly on ImageNet-C has been observed in [6] which might be good to cite here. \n\nPage 4: “However, this bias is orthogonal to the performance when only either texture or shape cues are present.” I don’t understand why it would be orthogonal? I would expect a model with a strong shape bias to care less if I remove texture cues from an image compared to a model with a strong texture bias. What exactly is new in this test compared to the cue conflict task?\n\nPage 6: “To confirm this hypothesis”, please write “to test this hypothesis”\n\nPage 6: “That is, the smooth filters in R models indeed can filter out pixel-wise Gaussian noise despite that R models were not explicitly trained on this image type!” The finding here seems very related to [7] and [8]. In [7], the authors show that adversarial training improves robustness to Gaussian noise and in [8], the authors study the Fourier properties of common corruptions and show that adversarial training improves accuracy for corruptions in the higher frequencies.\n\nPage 6: “Our result suggests that most of the de-noising effects take place at lower layers (which contain generic features) instead of higher layers.” I do not understand how this conclusion was made given the results, please explain.\n\nPage 6: “We found a consistent trend—adversarial training resulted in substantially more filters that detect colors and textures (i.e. in R models) in exchange for fewer object and part detectors”. This result seems surprising and not consistent with the finding that adversarially robust models are more shape biased. If shape bias is indeed correlated to the filter responses (which is not necessarily clear), I would expect filters of a shape-biased model to rather detect objects and parts instead of textures and colors. Please discuss this. It would also be nice to test the SIN trained model from Geirhos et al 2018a, since it has a high shape bias.\n\nPage 7: “As R hidden units fire for fewer concepts, i.e. significantly fewer inputs, the space for adversarial inputs to cause R models to misbehave is strictly smaller.” I believe this is meant to be an explanation for why adversarial training works. I don’t understand the argument well, but in [9], the authors show that adversarially robust models are better for transfer learning than their vanilla counterparts. Intuitively, a broad set of (unique) features must have been learned by the adversarially robust model for this task. I would like to see a discussion comparing these results to [9]. I also think that the analysis is not sufficient to claim that the space of adversarial examples is smaller for R models.\n\nPage 8: “For example, our preliminary results showed that encouraging S networks to have smoother kernels in early layers improves CIFAR-10 ResNet-18 network robustness to adversarial and noisy images.” This has not been shown in this paper/ reference missing.\n\nSuggestion: It would make the work stronger if the authors considered more adversarially robust models, e.g. [10], [11].\nPlease fix the references: I noticed that the authors cite the arxiv version for [Geirhos et al 2018a]. This paper has been presented at ICLR 2019 and this should be reflected in the references. Please check and fix all other references as well where this critique applies.\n\nReferences:\n\n[1] Kang et al: “Transfer of adversarial robustness between perturbation types”\n[2] Jordan et al: “Quantifying perceptual distortion of adversarial examples”\n[3] Tramèr et al: “Adversarial training and robustness for multiple perturbations”.\n[4] Tramèr et al: “On Adaptive Attacks to Adversarial Example Defenses”. \n[5] Carlini et al: On Evaluating Adversarial Robustness\n[6] Rusak et al. “A simple way to make neural networks robust against diverse image corruptions”.\n[7] Ford et al: “Adversarial Examples Are a Natural Consequence of Test Error in Noise”\n[8] Yin et al: “A Fourier Perspective on Model Robustness in Computer Vision”\n[9] Salman et al: “Do Adversarially Robust ImageNet Models Transfer Better?”\n[10] Xie et al: “Feature denoising for improving adversarial robustness”\n[11] Shafahi et al: “Adversarial training for free!”\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of The shape and simplicity biases of adversarially robust ImageNet-trained CNNs",
            "review": "In this paper, the authors show that adversarially robust versions of three popular CNN architectures trained for image classification on ImageNet rely on shape rather than on textures to perform recognition. They also show that adversarially robust networks do not outperform non-robust networks on corrupted data. Finally, they perform some analysis to determine whether intermediate features are more related to shape or texture, finding that these representations to intertwine both types of information.\n \n**Quality:** The paper is well written and the experiments are very interesting. However, the contributions may be somewhat incremental (see below).\n\n**Clarity:** The paper is written very clearly.\n\n**Significance and originality:** The paper is interesting, but the novelty is a bit limited. I am torn about this, because I do think that the experiments are interesting and well explained, and verifying results on different datasets (especially very large datasets like ImageNet) is important.  However,  given that the shape bias of adversarially robust networks was already known previously (as pointed out by the authors) and that there is no methodological contribution, I think that the contributions are somewhat incremental. \n\n**Pros:** Well written, interesting experiments.\n\n**Cons:** Incremental contribution.\n\nAfter reading the author feedback, I would like to thank the authors and I agree with them that it is critical to test hypotheses on large-scale datasets. However, I still think that the contribution is marginally below the acceptance threshold.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical studies on the adversarial trained CNNs and shape-based representation",
            "review": "This paper takes a step further to understand the relationships between the adversarial trained CNNs (R-CNNs) and shape-based representation, and delve deeper into the R-CNNs via studying the hidden units. First, it justifies that the R-CNNs prefer shape cues based on random-shuffled, Stylized-ImageNet, and silhouette experiments. Then, it tests R-CNNs on ImageNet-C to show the less connection between the shape-biased and the robustness against common corruptions. Finally, it studies the hidden unit via qualitative tools including NetDissect.\n\nThe studied direction is important for both representation learning and robustness communities. The methodology of this paper and all experiments are technically sound. Using Network Dissection is a good point to study here. Testing on three benchmarks, and evaluate three different network architectures can help to verify the conclusions are general. The results are sufficient and would support their arguments, though more analysis would add support to the claims. Overall the paper is easy to follow and the reviewer thinks the experiments are easy to replicate.\n\n---------------\nBelow are some concerns and suggestions:\n\nThe novelty is slightly limited. Many understanding is complementary to the findings in the literature, such as [1] that adversarially trained models are shape-biased, [2] that there're nor significant correlations between shape-biased and robustness against common corruptions, and [3] that low-frequency help the generalization. Many experimental methods are the same as previous work but only perform on adversarially trained models, like the scrambled (Sec. 3.2.1) is the same as the random shuffled in [4],  test on Stylized-ImageNet[2], ImageNet-C, and etc. Also, there're no new techniques proposed. Basically, all the used techniques are from literature, like Network Dissection, and etc. \n\n\nAccording to Table 3, compare to the Shape-Less column, the difference in the Texture-less column is not that significant. The reviewer wonder if the author would perform extra experiments to verify the R models significantly more prefer shape information, such as test on edge maps. The edge map is easy to drive from the silhouette and may be more suitable to be tested here. The value of the edge map and silhouette may be set to binary or greyscale [0, 255].\n\nCurrently, only qualitative results are provided to justify the R models would contain smoother filters than the standard ones. The reviewer wonders if any quantitative criterion can be designed and be reported thereon. \n\nIn the blocking pixel-wise noise section (Page 6), the author claims that R models are more robust against Gaussian additive noise based on Figure 3. However, the ImageNet-C contains such distorted images, and S models outperform than R models. The reviewer wonders if the author can test on Gaussian additive noise distorted images and report the results to further justify the claim.\n\n---------------------------------\nTo sum up, the reviewer would vote 5 currently -- due to its limited novelty but some understanding of the representation of adversarially trained models are proposed.\n\n[1]Interpreting Adversarially Trained Convolutional Neural Network\n\n[2]Imagenet-trained Cnns are Biased towards Texture; Increasing Shape Bias improves Accuracy and Robustness\n\n[3]High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks\n\n[4]Defective Convolutional Layers Learn Robust CNNs\n\n\n-------------------after rebuttal-----------------------\n\nI thank the authors for their rebuttal. Since the authors reply near the discussion phase end, I cannot ask follow-up questions. \n\nThe answers partially address my concerns and thus I would raise my score to 6. \n\nFor novelty , the author answers three points. For the first point, fuse two channels of information are not so convincing on the novelty aspects. Also, there needs an extra cost to collect process the images. For the second point, how would you formulate a regularize?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Fairly thorough experimental study, few new insights or consequences",
            "review": "Summary:\nThe submission concerns an experimental study of the behavior of networks trained with and without adversarial robustness criteria (using Madry et al., 2017). Given a set of such trained networks, a detailed look at the behavior and properties of adversarially robust networks and their non-robust counterparts is taken. This includes evaluation on the cue-conflict dataset (Geirhos et al.), and on scrambled, or texture-less (silhouette) variants of ImageNet images. Furthermore, visualizations of filter banks are analyzed and compared, as well as an analysis on the neuron level is carried out, using the NetDissect framework by Bau et al.\nInsights include that adversarially trained networks are more shape-biased (and reliant) than their counterparts which are known to be texture-biased. Furthermore, three simplicity biases are found that result in smoother filters, increased focus on low-level cues, and decreased diversity of inputs detected by neurons of adversarially trained networks.\n\nReview:\nThe authors have conducted a fairly thorough experimental study w.r.t. the level of detail considered; insights range from experiment to neuron level. The main limitations of the study are set by the types of networks considered (2012-2015 era networks only) and the types of adversarial training considered (only the work by Madry et al.). With regards to the latter, incorporation of newer work (e.g. (1), (2), or (3)) on adversarial training could have made the argument stronger, but this may simply have been subject due to scope and time limitations.\nDespite these limitations, the scope of evaluation is sufficient, and the level of supplied visualizations is exemplary. The work lacks any theoretical insight, but there is potential value in the type of conducted detailed empirical study. A large part of the results seems to agree with prior work (cf. mentions of “consistent with” and similar), such that part of the value of this study is additional confirmation of what prior work may have found or otherwise hypothesized.\nWhat I am missing a bit, however, is a discussion on what the mentioned findings may mean for future developments of adversarial training, adversarial attack design, or other mitigations of adversarial attacks. The discussion of possible consequences is limited to the last paragraph, which is meant to discuss future work.\n\nClarity of writing is generally good; however, I was not really a fan of the chosen nomenclature and the abundant definition of abbreviations which may clash with own notions of their meaning. For example, is it necessary to define “S-networks” and “R-networks”, where R-networks happen to be *s*hape-biased and S-networks *t*exture-biased? My preference would have been to simply spell out “adversarially trained” (“-adv”) vs. “not adversarially trained”. This is just a personal opinion and does not affect the rating. Similarly with the datasets; “ImageNet-C” is mentioned on page 3, but does not seem to be defined beyond “modified ImageNet images” and then shown in Figure 1.\n\nOverall, I believe the scope of experiments and evaluations goes beyond workshop-level work. But originality and significance remain limited, as mentioned above.\n\n—-\n(1) Shafahi et al., “Adversarial Training for Free!”, NeurIPS 2019.\n(2) Cohen et al., “Certified Adversarial Robustness via Randomized Smoothing”, ICML 2019.\n(3) Xie et al., “Smooth Adversarial Training”, arXiv preprint, 2006.14536",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}