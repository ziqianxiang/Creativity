{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes.  \n\nIn discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. Please see reviews and public discussion for further details."
    },
    "Reviews": [
        {
            "title": "An interesting approach, but there are major concerns",
            "review": "The authors study adversarially trained classifiers and observe that the accuracy discrepancy between classes is larger than that of standard models. They then propose a theoretical model where this phenomenon provably arises. Finally, they propose a method to reduce the (standard and robust) accuracy discrepancy between classes, by adapting existing methods from the non-adversarial setting.\n\nI found the paper interesting. The original observation is intriguing and rigorously reproduced on a synthetic dataset. Moreover, the method proposed does seem to improve the class disparity of these models.\n\nHowever, I have one major concern: it is not clear that the phenomenon observed is a property of adversarial training. A different explanation could be that robust models have lower accuracy than standard ones, with their robust accuracy being even lower. Should the increase in error be multiplicative (which is the most likely scenario) then it would potentially explain the main observation of the paper without taking into account adversarially training at all. Specifically:\n- Figure 1: Even the standard model accuracy fluctuates across classes. There are clearly classes that have at least 3 times the error rates of others. The situation for the robust model does not seem particularly different, there are also classes with similar multiplicative ratios in their error, the difference just looks larger because the error rate itself is larger. Measuring the ratio between different error rates might shed some light here.\n- Theoretical setting: Looking at equation (12), consider a similar case where the standard error of the robust model is, say, 4 times higher than that of the standard one. In this case, $\\Omega$ would be 4 purely based on the accuracy of the model.\n\nTo summarize, based on the existing arguments in the paper, we cannot tell apart the scenario where robust training causes class disparity due to an inherent property of the method or simply because it increases the model's error rate. Should the latter be the case, the contribution of the paper would be significantly smaller since the underlying phenomenon would have little to do with adversarial training (similar for the proposed method).\n\nAnother issue that was not clear to me is whether the test set is used during training to compute the weighting of the different loss terms (line 5 of algorithm 1). From what I understand, this is the same test set used to report performance, in which case the methodology would be fundamentally flawed and a separate test set (completely unseen) would be needed.\n\nOverall, I do not believe that the paper is ready for publication but I would be willing to update my review based on further discussion.\n\nOther comments (not affecting score):\n- Intro: \"tradition debiasing...\" -> \"traditional debiasing...\"\n- Figure 1: I assume models are trained against an 8/255 adversary, is this correct?\n- Eq 14: The fourth error rate should be R_rob, right?\n- Section 4.3: broken references\n- Section 5: \"fairly robust\" can be interpreted as \"somewhat robust\" which is not the intended meaning of \"fairly\" here\n\n====== POST-RESPONSE UPDATE ======\n\nI appreciate the authors' response and the additional experimental results provided. While I do think that they are a step in the right direction (I hence slightly increased my score to a 5), they still do not address my concerns. Specifically:\n- **Empirical results.** Taking a closer look at Table 1* of the response and the results on randomized smoothing (provided in other responses), I do agree that not all errors increase by the exact same multiplicative factor. At the same time, the discrepancy does not seem to be particularly large---i.e., the ratios have a mean of 2.5 with a standard deviation of <0.5. Clearly, given that this is a real-world dataset, it is natural to expect that the effect of adversarial training is not perfectly linear across all classes.\n- **Theoretical results.** After reading the response, reading the discussion with Reviewer4, and going through the paper again, I am still not convinced that the increase in class disparity cannot be attributed to a large extent to the gap in clean accuracy introduced by robust training. Perhaps there is a cleaner way of formulating there results or highlighting the key components of the analysis that resolves this. However, unfortunately, I still do not find the analysis convincing in its current state.\n\nOverall, I believe that this point is quite nuanced and the existing empirical and theoretical analysis is not sufficient to draw a confident conclusion. Given how this point is at the core of the paper's contribution, I still recommend rejection.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A fairness issue in adversarial training",
            "review": "Summary:\n\nThis paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training. It makes an observation that adversarial training algorithms (Madry et al. 2017, Zhang et al. 2019) yield biased performances on CIFAR 10. It also offers a theoretical study under a Gaussian mixture setting that respects Eq. (1). Three versions of fair robust algorithms are proposed and evaluated on CIFAR 10.\n\nStrength:\n\nThe paper makes an interesting observation on a fairness perspective in adversarial training.\nIt provides a theoretical insight that tries to explain the observation in a certain setting Eq. (1) in which low-dimensional robust features play a dominant role in adversarial training.\nWeakness:\n\nWhile the observation is new and interesting, this reviewer wonders whether it happens “fundamentally” in any adversarial training setting. The theoretical study definitely offers a deeper understanding, yet it is analyzed under a particular setting Eq. (1) which I am not sure if it represents most practically-relevant scenarios. Also, all the experimental results are provided for the two algorithms (Madry et al. 2017, Zhang et al. 2019) under only one dataset CIFAR 10. As the authors mentioned in Section 6.2, this may be due to particular algorithms and data distribution. If that is the case, it would be good to see that the observation may not happen in other settings. Otherwise (if the observation is fundamental), a thorough theoretical study together with extensive experiments that cover more diverse datasets are desired to be presented.\n\nTheoretical analysis: I can barely follow Theorems 1/2 and their implications stated in the paper. But it was not intuitively clear to me why the unfairness issue occurs in the considered setting. Any elaboration on intuition and insight might help.\n\nThere are many grammatical errors and typos.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "Thank you for your response, which cleared up some of my questions so I increased my score to a 5. I would have liked more analytical / empirical evidence to substantiate some of the claims made in response to the questions numbered 4, 5, 12, and 13 in the comments provided by the authors. My overall opinion remains unchanged but I encourage the authors to continue this line of work on build on these results and incorporate the feedback provided by the other reviewers as well.\n\n###\n\nThis work makes an interesting observation that techniques designed to maximize robustness to adversarial examples may have negative consequences on the model's performance. Typically, this has been studied primarily with an emphasis on accuracy. Here the authors look at fairness instead.\n\n- The first sentence of the introduction makes strong claim about the implications of adversarial examples without providing appropriate references or supporting the claim with evidence.\n- Adversarial examples do not have to be \"imperceptible\", the main requirement is that the perturbation introduced does not affect semantics of the input.\n- Adversarial training has been subsumed into verifiable approaches to defending against p-norm bounded adversaries (e.g., randomized smoothing, etc.). Could you comment on why these approaches were left out of the experiments here? They would be able to prove robustness is achieved uniformly for each training example.\n- If we know that adversarial training impacts accuracy (as recently re-demonstrated by Tsipras et al.), could you comment on why it would have been expected that fairness would not be impacted given that the training set is balanced among classes and certain pairs of classes are closer to one-another under the p-norm than other pairs of classes? \n- The presentation is somewhat confusing given that it entangles the notions of fairness and safety. This seems likely to lead to confusion from the readers. I would recommend splitting the work into two distinct studies: one of the worst-case performance of the classifiers in the absence of an adversary (i.e., safety) and the other being on fairness itself. Unless the authors wish to demonstrate explicitly the connection between safety and certain definitions of fairness, in which case this connection should be presented more upfront in the paper.\n- Typo/grammar in \"In the lower dimensional space, an optimal linear model is more sensitive to the inherent data distributional difference and be biased when making predictions.\"\n- Could you motivate the choice of dataset here? Why is CIFAR10 a good dataset to study fairness?\n- Figure 1: I would recommend using another type of graph. Typically series like this are used to represent data that is sequentially meaningful whereas here a bar plot or something along these lines would probably convey your message better.\n- The paper does not explicitly state which fairness definition is used (for instance in Section 2) while this is important here. In particular, the definition considered seems to be based on parity but this is not explicitly stated there. Would the approach considered here apply to other definitions?\n- It is not clear what is the applicability of the analysis based on robust/non-robust features is beyond the toy dataset considered here. Could you motivate better why this toy dataset matters in the context you are considering here?\n- Grammar error in the statement of Theorem 1\n- Grammar in \"should not be too larger\" (page 5)\n- Could you motivate the choice of different thresholds on the natural model error and adversarial model error?\n- Missing Section reference on page 6\n- Section 4.3 discusses the intricacies and possible tensions between natural and robust accuracies and fairness. The re-margin strategy proposed seems to be at odds with recent results demonstrating that improving robustness to p-norm perturbations conflicts with generalization (see Tramer et al. in ICML20). Have you thought about the suitability of the p-norm constraint placed on adversaries in the context of your approach? Does this create an artificial tension with fairness for the same reason that it creates an artificial tension with accuracy (because the p-norm is not aligned with class semantics).\n- Related to this, Table 1 suggests that there is increased tension with natural accuracy when employing FRL compared to the baselines considered. Is that due to the point mentioned above or to what is discussed in Section 4.2?\n- The experimental evaluation is limited. It is hard to draw conclusions from one dataset. In particular, I would encourage the authors to consider datasets which are not balanced, which are from other domains than images, and which have a natural motivation for achieving \"fairness\" due to the task being solved. \n- Experiments should also include multiple runs and report variance.\n- The discussion of related work is a bit \"thin\".",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some useful empirical results on class-wise accuracy imbalances",
            "review": "This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean (and robust) accuracies across different classes. This is an important observation, which I have not seen in published work (though I believe is generally understood by many practitioners with the robustness community), and is an important contribution.\n\nThe paper then proceeds to use a theoretical example (adapted from the model in Tsipras et al 2018) where adversarial training increases the accuracy difference across classes. More on this later. The paper proposes an algorithm, Fair Robust Learning (FRL), to address this issue. The starting point is a standard Lagrangian-based approach to approximately ensure constraints that the performance for each class should be close to the overall performance. The FRL algorithm then proposes 2 modifications. First, rather than applying the constraint on the robust errors directly, they decompose the robust error into clean error and a robustness term (similar to TRADES) and approximately maintain constraints on both terms. Second, for difficult classes, they propose increasing the adversarial radius, instead of/in addition to only increasing the corresponding Lagrange multiplier. The experiments demonstrate that their FRL approach improves the worst-class clean and robust errors, relative to both standard adversarial training and the baseline Lagrangian approach, without losing much in average-class errors.\n\nOverall, I believe the paper is fairly well-executed and investigates an important topic. The motivating empirical observations are an important contribution. The proposed approach is natural and well-motivated, and the experiments show improvements in the worst-class errors, as should be expected conceptually. The empirical explorations of different variations on the core idea are also valuable.\n\nThere were some points in the paper which I believe could be improved.\nFor me, the theoretical example does not add much insight regarding why this effect occurs. \n- Regarding Theorem 1, the comment says that \"when the term A is large, the model has close clean errors between the 2 classes, namely $R_{nat}(f, -1) ~ R_{nat}(f, +1)$. Assuming A^2 >> q(K), the terms simplify to:\n\n$$R_{nat}(f, -1) = P[N(0, 1) \\leq A(1- \\sqrt{K})]$$\n$$R_{nat}(f, +1) = P[N(0, 1) \\leq A(1- K)]$$\n\nwhich means we have a $\\sqrt{K}$ factor difference between the two errors. In this case, it seems that even before adversarial training, we see a difference in class errors between the classes.\n\n- Additionally, in Theorem 2, using a similar approximation B^2 >> q(K), we'd have\n\n$$R_{nat}(f_{adv}, -1) = P[N(0, 1) \\leq B(1- \\sqrt{K})]$$\n$$R_{nat}(f_{adv}, +1) = P[N(0, 1) \\leq B(1- K)]$$\n\nwhere the two terms differ by a $\\sqrt{K}$ factor again, only the leading constant is different.\n\n- The claim in Equation 12 that \"the ratio $\\Omega$ is large (e.g. >1)\" seems to be the result the section builds to, but it seems it is more naturally explained by the fact that if there is already a difference in clean errors, since adversarial training will increase these errors (Tsipras et al 2018), then adversarial training will also scale up the difference in these class-wise errors.\n\nIt's possible that this explanation is actually what's happening, and it doesn't seem clearly inconsistent with the empirical data, since in Figure 1 (left), there are already class-wise differences, which generally match the shape of the class-wise differences after adversarial training. But it does seem very different from the story the paper is trying to tell, and it would be good to figure out which one it is (or acknowledge both).\n\nMoving on to the experiments:\n\n- The TRADES baseline has robust accuracy almost 10% lower than the standard TRADES model - can you comment on this?\n- Section 4.3 comments \"we claim that only upweighting its cost (or Reweight) could not succeed to fulfill the cost-sensitive classification goal in adversarial setting\" - could you explain why this is? (after all, reweighting is the basic approach used here too)\n- How does the Re-Margin approach compare to the Reweight approach, when the reweighting also involves sweeping over the training $\\epsilon$?\n- It's nice to see the code. Hopefully this can also be released along with the paper if accepted.\n\nOther notes:\n\n- It would be nice to connect this to the broader fairness literature. This is unfortunately not my area of expertise, but difference in performances between classes (or demographic groups) is a very common metric, and it would be nice to relate this work more closely to work there. \"Distributionally Robust Neural Networks for Group Shifts\" (Sagawa et al 2020) is a (somewhat arbitrary) example of work which seems related, but I'm sure there are many more. It would be great to connect to this, especially papers which address why/when difference in performance across classes is expected.\n\nI give the paper a 6 overall, though could adjust my rating in either direction depending on the author feedback.\n\n\n________________\n\nEDIT: Score changed from 6 to 5 during discussion, see comments below.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}