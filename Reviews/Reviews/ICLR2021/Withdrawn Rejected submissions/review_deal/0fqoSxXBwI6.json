{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors present a method for self-supervised learning of representations of 2D projections of 3D objects. By performing known 3D transformations of an object of interest, a encoder/decoder network is trained to estimate the applied transformation from a series of 2D projections. The proposed method is used as a regularizer and experiments are performed on supervised 3D object classification and retrieval.   \n\nAfter seeing each othersâ€™ reviews, one of the main concerns from the reviewers was the relationship between the proposed method and Zhang et al., CVPR 2019 (i.e. AET). The two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different. \n\nIn their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition. However, there were still other concerns that the reviewers had e.g. R2 wanted to know why the model could not be applied directly to 3D shapes instead of 2D projections. \n\nGiven the above concerns (specifically the relationship to AET), there is currently not enough support for accepting the paper in its current form. The authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future.  \n"
    },
    "Reviews": [
        {
            "title": "The paper proposes to use MV-TER loss as a sub-task loss for 3D object classification and retrieval task and improves the performance of STOA methods. However, more detailed analysis and discussion are needed to show the real novelty and value of the loss design to the community. ",
            "review": "Summary:\nThis paper proposes a self-supervised learning framework for 3D object classification and retrieval based on multi-view representation, where a sub-task of transformation estimation is adopted as a regularizer. By adding the proposed MV-TER loss, the STOA approaches can gain notable improvement in performance.\n\nStrengths:\n-The paper is well written, and the idea of transformation equivariant representation of 3D objects is easy to understand. \n-The proposed method further improved the stoa methods on 3D object classification, which is already quite high.\n\nAreas for improvement:\n-Firstly, the representation of the transformation that the author used in the paper is not well specified.  I assumed that the Euler angle representation is used in this paper, but I think it is not proper to use the MSE loss in (5) or take the average in (8) for Euler angles. \n-Secondly, since it has been suggested by many previous work that the joint-training of multi-task is helpful for the network, it will be appreciated that the authors provide more analysis and discussion on how the MV-TER loss helps the network learn transformation equivariant representation than simply using rotation as data-augmentation or using pose-estimation as sub-task.\n-Thirdly, any quantitative results of the transformation estimation? From the images in Appendix B, the results of (average) are incredibly accurate.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comments for Self-Supervised Multi-View Learning via Auto-Encoding 3D Transformations",
            "review": "This paper proposed a self-supervised learning method of 3D shape descriptors for 3D recognition through multi-view 2D image representation learning. To represent the 3D shape, the authors first project the object to a group of 2D project images, which helps apply deep learning due to the image's matrix data format.  The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as \"self-supervised\" learning. The key idea of transformation equivariant representations is directly borrowed from existing works [1][2]. The method designed is almost the same as [1] except for the encoding network.\n\n[1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET)\n[2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT)\n\nBesides, there's no proof to verify transformation equivariant representations learning. The authors need to prove the properties for Transformation Equivariant directly for 3D objects. The current presentation of the paper is based on the 2D project image representation for 3D objects. If the authors wish to use Transformation Equivariant  in [1][2] above, the authors might want to consider adding additional 2D to the 3D reconstruction process, and  then consider the order of doing transformation and doing reconstruction etc.)\n\nMay some visualization results can better convince the audience. For example, the authors would like to add some visualization results for a 3D shape descriptor for objects from the same category and different categories and show how robust the proposed shape descriptor is.\n\nThe proposed approach had experimentally verified its effectiveness in 3D recognition.  However, for a paper being accepted to ICLR, I would like to see more technical novelties/merits beyond directly extending the existing image representation learning approach to 2D projection images of the 3D image. For instance, the authors could propose a method that can be developed based on the \"3D Transformation Equivariant\" to 3D objects directly instead of its 2D projections.\n\n---Additional comments after rebuttal--\nI have carefully reviewed the authors' feedback regarding their comments on how their proposed method differentiates the existing (AVT and AET). Unfortunately, it did not address my concerns about the novel technical contributions in the proposed paper. Obviously, the authors applied the \"Transformation Equivariant Representations by Autoencoding Variational Transformations\" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). I am not sure why in the rebuttal, the authors claimed, \"Their proposed method distinguishes from AET significantly in two aspects\". I would urge the authors to check both papers below, and it clearly defines the Transformation Equivariant Representations learning by Autoencoding Variational Transformations, which could be applied for various types of data. [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT)\n\nAnother concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the \"3D Transformation Equivariant\" to 3D objects directly instead of its 2D projections. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. I am not sure why the authors answered that \"3D objects are unavailable at the testing stage.\" The proof of Autoencoding Variational Transformations for 3D data directly should not depend on the availability of 3D data.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The manuscript presents a self-supervised learning scheme which improves multi-view object classification and recognition on the modelnet40 dataset. Therefore, I recommend to accept this submission. ",
            "review": "Summary of the Submission:\n\nThis submission proposes a self-supervised learning scheme for 3D object recognition. The basic idea is to predict a 3D transformation from 2D views. The features that facilitate 3D transformation prediction then generalize to other 3D object recognition tasks such as object classification and retrieval. The evaluations on modelnet40 and shapenetcore55 show significant improvement when adding the self-supervised training.\n\nStrengths: Self-supervised formulations are highly beneficial for tasks where no labels are available or where it is expensive to collect labels. The formulation is intuitive, the manuscript is well written and easy to follow.\n\nUpdate after Rebuttal:\n\nAn additional experiment on real data was added which I find a valuable addition to the submission. AR2 points out a similarity with AET which I did not notice in my initial review. I feel this does limit the contribution somewhat. As the rebuttal points out there are differences between the proposed method and AET but the core idea is very similar. In my opinion there is enough difference to still recommend acceptance.\n\nWeakness:\n\nThe evaluations are all on synthetic data. It would be great to see how the learned features generalize to real data such as e.g. pascal.\n\nThe paper title and the writing is in general talking about 3D transformations but it seems experiments have only been conducted with 3D rotations. Maybe it would be more adequate to change it to 3D rotations everywhere or demonstrate at least one additional type of transformation.\n\nThe means squared error as a loss on the 3D rotation is not well motivated. How is the rotation parametrized? Why not using a loss function that penalizes the amount of rotation?\n\nThe views are sampled in a regular spacing and not randomly. There is a worry that this induces a bias and recognition with multiple nearby views at test time might be affected as it will not have been present in the training data. But this mode of operation might be a common one in practice if e.g. a video of an object constitutes the multiple views.\n\nThis paper seems also related and might be an addition to the related work: Rhodin et al. Unsupervised geometry-aware representation for 3d human pose estimation, ECCV 2018\n\nExplanation of Rating:\n\nThis paper shows a self-supervised way to learn features which lead to improved results in classification and recognition on the modelnet40 dataset. The main weakness of the submission is that it was only evaluated on synthetic data. It would be interesting to see if the self-supervised learning helps with generalization to real images.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose a simple but effective self-supervised learning technique for multi-view learning.",
            "review": "The authors propose a self-supervised learning technique for multi-view learning based on a simple intuition that the transforms of the 2D views of a 3D object will be in an equivariant manner as the 3D object transforms. (Section 3.1). They show its effectiveness by clearly improvements under two frameworks MVCNN and GVCNN.\n\n+ clear presentation\n+ simple idea\n+ convincing experiments\n+ detailed ablation study\n\nOverall, most parts of this paper are satisfactory. They have successfully backed up their claims by applying the idea in two different frameworks and show clear improvements. While my main complaint is that the idea of this work is very simple and even can be called as common sense, and I have encountered this idea in many other papers, though in different fields, such as human pose estimation and 6d object pose estimation. I recommend the authors adding a full discussion of this simple idea based on transformation invariance.\n\nThe related works:\n3D Human Pose Machines with Self-Supervised Learning, PAMI 2019\nGeometry-Driven Self-Supervised Method for 3D Human Pose Estimation, AAAI 2020\nSelf-Supervised Learning of 3D Human Pose using Multi-view Geometry, CVPR 2019\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}