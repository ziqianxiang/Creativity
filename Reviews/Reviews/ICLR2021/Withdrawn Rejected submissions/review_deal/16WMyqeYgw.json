{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review and Questions",
            "review": "This paper proposes two new value-based methods to improve exploration. The two techniques improve exploration using the variance in the 1) return sequence and 2) weighted TD error. This work also contributes a two-head neural network to estimate the Q-value and the variance as well as optimization improvements. \n\nIt's interesting to drive the exploration directly from rewards, rather than observations or states as most exploration methods recommend. I believe the clarity of the work could be improved and I would increase my score upon improvement in clarity and satisfactory answers to my questions.\n\nQuestions to authors:\n* The largest improvements of your exploration method come for games that present no exploration difficulty, refer to Ali Taiga et al, (2019) for a list of hard/easy exploration games. We are finding little-to-no improvement on difficult exploration games like Montezuma’s Revenge, PrivateEye, Venture.\n* The premise is that variance of the value of a state-action pair can be used to estimate the convergence speed of the value function for that state-action. However, if the environment is simply very stochastic from that point, won’t this upweight these states? Does this fail the Schmidhuber noisy-TV test?\n* Can you describe how you ensured strong baselines? I’ve noticed that on many tasks that the performance is below that achieved by agents in Dopamine [https://google.github.io/dopamine/baselines/plots.html].\n\n\nNotes:\n* The clarity of the paper can be improved. As one instance, “During Monte Carlo policy evaluation… This series can start diverging due to the cyclic nature of value-based approaches; the evolving value function begets policy improvements which in turn modify the value function” -- under policy evaluation, the value function does not evolve. What is going on here?\n* Please make the separate sigma-stream more explicit earlier [Section 4.1]. Diagram in appendix perhaps?\n* I’d recommend logarithmic axes for Figure 2 and Figure 3. The dramatically different scores mask the improvements. A reader might look at this and conclude that while these methods improve, sometimes considerably, on several games, that they actually hurt performance on as much as 40% of games.\n* Nit:  It’s redundant to say you’re introducing a novel method -- just say you’re introducing a method.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Needs comparisons with other recent work in exploration",
            "review": "## Summary\n\nA challenge in reinforcement learning is how to explore effectively. In small tabular state and action spaces good solutions exist such as Upper Confidence Bound. However, it is non-trivial to determine how to scale such approaches to large state and action spaces.\n\nThe approach taken here is to add an additional ``head'' to a DQN network which tracks (using a TD estimator) the variance of returns for each state and action (as well as the standard predictions of the return for each action with the other head) (the empirical returns are weighted to emphasize recent returns when estimating variance) [this is what the paper states, as elaborated below I'm not sure this is quite correct]. Then the policy followed during training is $\\pi(s) = \\argmax_a \\hat{Q}(s, a) + c \\hat{\\sigma}(s, a)$ where $\\hat{Q}$ is the value prediction and $\\hat{\\sigma}$ is the variance prediction, $c$ is a hyper-parameter. This is a particular form of \"optimism in uncertainty\" in that actions which have had high variance outcomes are valued more highly.\n\nThey provide some simple experiments for the tabular case and then experiments on Atari showing significant improvements over Double DQN baselines.\n\n## Feedback\n\nDescribing the method and in the tabular experiments it seems the variance of returns is used for $\\sigma$. But in the actual scalable algorithm the \"variance\" of the return is estimated using the TD(0) error (algorithm 1 shows this most clearly). This seems a significant limitation because it means that it lacks what Bootstrap DQN refers to as ``deep'' exploration. If a state has high variance outcomes then good exploration may require navigating again to this state, but the TD(0) error may even be 0 on states leading to the unexplored state. Stated another way, the TD(0) estimate of returns has high bias, which will lead to substantially underestimating the true variance of the returns. This concern should be discussed.\n\nI was unable to follow how the variance estimation in section 4.1 is incorporated. It's not mentioned in the algorithm 1 or the losses in eq 8-11.\n\nThe \"noisy-TV\" (explained in [1]) problem for exploration, where an agent may repeatedly visit states with stochastic outcomes even when no further learning is possible is a challenge in exploration. This issue should be discussed (I believe the approach discussed here will fall for a noisy TV) and ideally experiments in a stochastic environment should be included.\n\nOne significant limitation of the empirical experiments on Atari is the lack of comparison with more challenging alternative approaches to exploration such as RND [2] which significant outperforms on some of the hard-exploration tasks such as Montezuma's revenge (here there is almost no gain on this over DDQN baseline), [3, 4] which also have substantial gains over many of the hard exploration Atari tasks here ([1] is a nice reference on recent work in exploration for Deep RL). Many of the Atari tasks typically considered to be exploration challenges are not improved over DDQN here and this should be discussed.\n\nI found it confusing in the abstract and elsewhere to refer to constructing an \"upper bound.\" This would seem to imply some theoretical guarantee or at least argument that an upper bound is being approximated somewhere. However, the method is heuristic and there is no particular guarantee that it is the upper bound of the true return, particularly given the variance is estimated using TD(0). There is even a hyperparameter for how much to scale the variance by when estimating value.\n\n# Conclusion\n\nOverall I think this work is interesting but not very theoretically well justified and although there are gains over DDQN baseline, as far as can easily be compared, there is not really gains over other recent work in exploration. For this reason I think this work may be of limited interest.\n\n[1] https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html#exploration-via-disagreement\n\n[2] Burda, Yuri, et al. \"Exploration by random network distillation.\" arXiv preprint arXiv:1810.12894 (2018).\n\n[3] Badia, Adrià Puigdomènech, et al. \"Never Give Up: Learning Directed Exploration Strategies.\" arXiv preprint arXiv:2002.06038 (2020).\n\n[4] Ecoffet, Adrien, et al. \"First return then explore.\" arXiv preprint arXiv:2004.12919 (2020).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Summary \nThis paper proposes to estimate the variance of the return using a neural network and use the estimated variance as an exploration bonus when acting in RL. Specifically, the paper presents two ways to measure uncertainty: 1) the exponentially weighted standard deviation of return or 2) the TD-error. The empirical results on Atari games show that the proposed exploration policy outperforms $\\epsilon$-greedy policy when applied to Double DQN (DDQN). \n\nPros\n* The performance is (slightly) better than the baseline. \n\nCons\n* Lack of theoretical analysis\n* Lack of comparison to other uncertainty measures\n* Lack of empirical analysis\n\n# Novelty\n* The idea of using TD-error as a measure for learning progress has been discussed in the context of experience replay, which is for learning. Using such information for generating exploratory behavior is new to my knowledge. \n\n# Quality\n* The results on Atari games seem to indicate that the proposed method works better than $\\epsilon$-greedy policy in practice, though the improvement seems a bit marginal. \n* My major concern is that this paper does not have any theoretical analysis of the proposed method unlike the other theoretically well-grounded exploration approaches (e.g., count-based method). For example, it is unclear whether the proposed method guarantees convergence.\n* The proposed method uncertainty measure does not seem to isolate the uncertainty of Q-estimation from the uncertainty of the environment dynamics. For example, if the environment has a highly stochastic reward function, the variance of return will be always large, which will be always encouraged according to the proposed method. On the other hand, many other exploration approaches (e.g., bootstrapped DQN, count-based exploration) won’t have such a problem.\n* Related to the above, it would be important to compare the proposed method to other uncertainty measures (e.g., bootstrapped DQN, model uncertainty, etc) and show the unique benefit of the proposed method.\n* It would be good to show some empirical analysis showing when the proposed method helps or hurts. The current set of empirical results is not so comprehensive. \n\n# Clarity\nThe paper is easy to follow.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Leveraging the Variance of Return Sequences for Exploration Policy",
            "review": "This paper is well-written in terms of the English, but unfortunately the authors make a simple error that invalidates the results of the paper.\n\nThere are two major notions of uncertainty that must be considered in statistics (and RL). So-called 'aleatoric' or irreducible, and 'epistemic' or reducible. Aleatoric is the intrinsic uncertainty / randomness in the system, for example when flipping a coin with Prob(heads) = p it will always have some randomness about the outcome (heads vs tails) - this uncertainty is irreducible. Epistemic uncertainty on the other hand is the agents own (possibly Bayesian) uncertainty about the parameters of the system, for example when flipping a coin the agent should be able to know p with greater certainty the more data the agent observes - this uncertainty is reducible, since more data should reduce it.\n\nThe type of uncertainty that can be used for exploration is epistemic. Aleatoric uncertainty yields almost no insight into the agent's epistemic uncertainty (I say almost because the aleatoric uncertainty influences the epistemic uncertainty, in that systems with higher intrinsic randomness are harder to learn about, eg, the posterior will concentrate slower, but to first order of approximation this separation is correct). As a concrete example consider a two-armed multi-armed bandit, where arm A has a lower average reward than arm B but significantly higher reward *variance*. In this case the aleatoric uncertainty of arm A is higher, but that tells us nothing about the agent's uncertainty about the average reward. The algorithm of this paper would pull arm A significantly more than it should, since it is prioritizing aleatoric uncertainty.  I think this case is made clearly in the Uncertainty Bellman Equation work: https://arxiv.org/abs/1709.05380, which seems like prior work that should be cited. In that paper the authors explicitly consider epistemic uncertainty, and show that the 'variance' of this uncertainty satisfies a Bellman equation. This is the correct notion of variance that can be used for exploration.\n\nIn this paper the authors confuse the two uncertainties. They use aleatoric to drive exploration, which isn't correct. As a thought exercise - how would the algorithm in this paper solve the DeepSea environment in https://arxiv.org/pdf/1908.03568.pdf ? This very simple environment can be thought of as an exploration 'unit test', and this algorithm would fail. The numerical experiments also drive home the fact that this is not exploring efficiently. In the hard exploration problems from table 1 here: https://arxiv.org/pdf/1703.01310.pdf, this algorithm tends to do poorly (eg Montezuma's revenge, pitfall, venture etc.).\n\nUltimately this paper should not be published in its current form.\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}