{
    "Decision": "",
    "Reviews": [
        {
            "title": "Ok but not good enough - rejection",
            "review": "This paper proposed a human eye-based synthesized image evaluation system that tries to address the difficulties in measuring the image completion results. This work is based on pre-existing work HYPE for complete image generation tasks and makes several modifications to adapt the methods to image completion. Empirically, the authors tested existing image completion methods on the metrics, and compare the metrics against existing ones, showing the limitations of existing methods measuring image quality.\n\nThe paper has several advantages.\nFirst, the evaluation results are comprehensive. The authors tested several groups of image completion methods, including 1) unconditional GAN-based latent space searching methods, 3) conditional image generation methods and 3) autoregressive methods. The authors also verify the correlation with previous methods, showing the potential failure in existing metrics.\nSecond, the general motivation for this paper is good. It has been widely recognized that current metrics measuring image completion methods do not comprehensively reflect human perception. \n\nThe disadvantages:\nThe proposed method is highly based on existing works HYPE, and the contribution is incremental. There are two major differences between previous work and the proposed one, which are 1) the standardized test set and 2) the use of the mask. Although these contributions might be useful, I believe they are in general engineering-oriented, and relatively straightforward. Besides, I am not sure if the use of a mask makes a great difference because even without the mask, the HYPE method can still be used for judging the realness of the synthesized contents in an image, i.e. the use of a mask is not a hard requirement.\nThere is no comparison against the previous method HYPE. Because this paper is highly related to the HYPE method, I believe it’s necessary to directly compare against the method, so that the difference between these methods can be investigated in detail. Based on my understanding, the direct use of HYPE on image completion tasks should be feasible.\nThe target of the paper is to measure the image completion quality. However, it seems there is only one method, Deepfill, that is originally designed for image completion. Other methods, although can complete the task, are not the mainstream. Thus I believe the authors would include more image completion/inpainting methods into the experiments to make the paper more convincing.\n\nBased on the points above, I consider that more improvements are needed before the publication of this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A psychophysical experiment is conducted to compare a set of image completion methods",
            "review": "This paper conducts a comprehensive psychophysical experiment to compare a set of image completion methods.\n\nStrengths:\n1. The scale of the experiments in terms of the number of images and the number of evaluated methods is a big plus. \n\nWeaknesses:\n\n1. The experimental procedure used in the paper is well and widely practiced in the field of Psychophysics. Therefore, the term STANDARDIZED is a bit strong. \n2.Due to the routine design of the psychophysical experiment, the observations drawn from the experiment are as expected and therefore not that interesting. For example, the researchers working in this field are aware of the superiority of autoregressive methods over GAN-based methods, the extreme difficulty of high-quality and high-resolution image completion, and the poor correlation of automated metrics to human judgments. This further undermines the contribution of this paper.\n3. HYPE-C and its precursor HYPE (and the majority of subjective assessment strategies) may not consider the diversity aspect of the models, which, to the reviewer's biased view, is equally interesting to investigate.\n4. As human evaluation is a time-consuming and costly endeavor, how to select a minimum set of images (and where to mask) that can reach the same conclusions is of practical importance, but is superficially treated in the paper.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very limited contributions",
            "review": "Paper summary:\n\nThe paper proposes a human evaluation protocol called HYPE-C for image completion. Adapted from HYPE [1], the proposed protocol selects 100 test images and 100 comparison images randomly from the dataset, then the image completion model is used to complete the masked bottom half of the test images. The HYPE-C score is simply the percentage of the images (both real and completed) being misclassified by the human evaluators. A perfect model should then achieve a HYPE-C score of 50% or more. The HYPE-C score is claimed to be sensitive for both image quality and diversity. Experimental evaluation is conducted using unconditional GAN models adapted to perform image completion, autoregressive models (such as PixelCNN) and DeepFill, designed for image completion task, across different datasets.\n\nPros:\n\n1.\tThe paper addresses an important task of proposing a protocol for evaluation of image completion models. \n2.\tThe proposed modification makes it possible to apply HYPE [1] to image completion models.\n\nCons:\n\n1.\tThe paper has very limited contributions. HYPE-C is mainly based-on the HYPE method and the adaption from evaluating image synthesis model to evaluating image completion model only needs a standardized test set and standardized masks, which in my opinion can’t be regarded as a substantial contribution.\n \n2.\tThe comparison between automatic methods and the proposed HYPE-C protocol does not provide any interesting findings and is lacking a more comprehensive analysis than correlation. It is not clear if the proposed metric is better than others. The paper only shows that the proposed metric is different than other metrics, see Table 4 and Figure 4. Being different from the automatic metrics does not imply that HYPE-C is a more reliable score. The proposed HYPE-C metric can be also empirically evaluated based on, among other factors, its consistency in applying the same score to the same method under multiple runs (here: consistency between human judges), their agreement with more expensive analysis (e.g., judgment by a panel of experts), or in using different masks for image completion (current setting is limited only to bottom half completion).\n\n3.\tMost of the methods evaluated are not originally designed for the image completion task, such as unconditional GAN models, which were designed for image synthesis task. The architecture of GANs needed to be modified (by adding an image encoder) in order to solve the image completion task. It is not unclear if those modifications may have obscured the quality of the original model in any way, of if the proposed modification is the best design choice. In this setting, it is not surprising that autoregressive methods perform well at image completion since autoregressive methods are trained precisely on sequence completion whereas the original GAN architectures were not designed for this purpose. Overall, the choice of the selected models for the evaluation is not justified. The evaluation of the metric should be done for the models designed for the task of interest, e.g. using different inpainting methods (besides DeepFill).\n\n4.\tThe HYPE-C protocol consists of only solving image completion task when the bottom half of the images is masked. However, in the real-life scenarios there are more challenging cases, so using diverse masks for completions would be beneficial for the model evaluation.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": " ",
            "review": "Summary and Contributions: The authors propose a modified version of HYPE for perceptual evaluation for the image completion task which is called HYPE-C.  In order to adapt HYPE to the task of image completion, they define a standardized set of test images, and also standardized mask or set of masks to apply to the test images denoting which regions to complete. They also ran several experiments using HYPE-C to benchmark a number of image completion models. \n\nCorrectness, clarity, and motivations: Overall, I enjoyed reading this paper and seeing the results provided as an appendix. This paper is, in general, well-written and organized, and the author extensively evaluated the method. However, still the motivation of the paper is not very clear for me. From the human perceptual evaluation point of view, one can present any generated or completed image to HYPE and get a score without having a mask.\n\nRelation to prior work and experiments:   While I enjoy seeing and also appreciate the extensive analysis of the method on several datasets, however, I strongly believe that authors have overlooked many relevant and recent advancements in image completion tasks and datasets that are used in these work. Per se,  the trend in image completion tasks is to produce larger and high quality images up to (8k) where in contrast we deal with the size of 32x32 to 128x128 pixels in the current paper. And I believe (as a human) having a human perceptual measure for 32x32 images is out of context. Towards this goal, many of recent and well-known works in image completion task (e.g. [Zeng et al., Learning pyramid-context encoder network for high quality image inpainting., CVPR 2019],  [Yi et al, Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting, CVPR 2020], [Yu et al., Free-Form Image Inpainting with Gated Convolution],  [Yu et al., DeepFillv1 and v2, ICCV 2019 and CVPR 2018]) used larger image sizes (256x256 - 8k) and performed image completion tasks on dataset such as  Places2 and CelebA-HQ, and etc. Based on this ground I believe the paper does not provide any relevant to community evidence for the performance of the proposed method.\n\nTo Conclude, the strong point of the paper is the great empirical study of the proposed method on several datasets. Unfortunately, it seems to me that this is not nearly enough work in relevant context to conclusively show the importance/utility of this measure for the community. There are many other datasets/benchmarks such as Places2 and CelebA-HQ with higher image quality that are considered essential for acceptance of such paper in image quality evaluation for image completion tasks .  Because of the above reasons I cannot recommend acceptance of the paper in its current state.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}