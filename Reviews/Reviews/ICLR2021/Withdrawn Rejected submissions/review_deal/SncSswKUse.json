{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper introduces a linear projection method, inspired by ANOVA,\nfor finding a supervised low-dimensional embedding.\n\nA positive aspect is that the method is straightforward, and it is\neven slightly surprising that in the family of linear models, there\nstill was an uncovered \"niche\".\n\nThe paper was considered useful for the purpose studied in the\npaper, single-cell RNA-seq data analysis. But to claim broader\nusefulness, more evidence should be presented.\n\nOne particular detail which was brought up by all reviewers was the\nPCA preprocessing. For ICA it is a sensible choice, as linear ICA is\nessentially \"just\" a rotation of the PCA components. But the\njustification is not as good for a supervised method. PCA may be\nnecessary in practice, but may lose important category-relevant\ninformation.\n\nThe paper still needs a significant revision before publication.  Even\nthough the method is straightforward method, a lot of time and\ndiscussion was required for expert reviewers to understand it.\n"
    },
    "Reviews": [
        {
            "title": "The paper presents a nice method in gene expression data analysis which has scientific implications, but probably lacks significance from a machine learning perspective",
            "review": "Pros:\nThe paper studies a very important problem in gene data analysis. The proposed method is technically sound. The method is intuitive in its idea and easy to implement. The results are interpretable. And according to the experimental evaluations, the proposed method is consistent to existing biological observations and could further identify unknown genetic targets. Therefore, it, potentially, has insightful scientific implications. \n\nCons: \n- Relevance and Generalizability are unclear: The paper is relevant to researchers in subareas only and it is best-suited to a bioinformatics or neurobiology community. The paper requires background in neurobiological data analysis to evaluate whether the proposed method brings meaningful scientific insights.  In terms of novelties in machine learning field, the improvement over existing works, algorithmically or computationally, seems relatively limited. It is also unclear whether the proposed method generalizes well outside the subdomain of gene expression. It seems FLDA could possibly be applied to any such tensor data but a discussion on its general applicability to other data will be nice. \n\n- Relation to prior work is insufficient: Related work has not been discussed adequately, thus making the significance of this paper unclear. The authors discusses CCA and autoencoders in introduction, and also mentions the differences between FLDA and LDA (2LDAs), which are compared with the proposed method in experiments. I find such discussions are inadequate for the readers to understand the baseline or state of the art in this field. Therefore, It is somewhat unclear how the work improves from existing works. \n\n- Experimental evaluation is incomprehensive: As a largely application work, the comprehensiveness or tricks in experiments should be explained more clearly. For example, one part of the experiment, the sparsity-based regularization of FLDA seems like the application of Rifle (Kean Ming Tan et al) on the data. Readers are unclear what are the unique challenges in current setting. Also, any computational details (convergence, scalability, etc.) or any guidance on the hyperparameter selection? And does this method also generate meaningful results on another gene data?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper, which describes a straightforward linear factorization method to try to explain single-cell RNA-seq data, solves a niche problem",
            "review": "This manuscript describes a generalization of ANOVA that is intended to be used in the interpretation of single-cell RNA-seq data. The method requires specification of an orthogonal, discrete categorization of cells, nominally by phenotype.  The method then linearly factorizes the observed gene expression values into features and their interactions, relative to the phenotypic categories.  The factors can then be used to help interpret the categories, especially in conjunction with a regularizer to reduce the number of genes involved in the factors.\n\nI found this paper frustrating to read.  The second paragraph does not make clear exactly what problem is being addressed.  It says that we are \"given phenotypic descriptions of neuronal types,\" but not where those descriptions come from.  So I skipped ahead to the methods section to try to figure it out.  But even after reading that section, I could not understand where the phenotype values come from.  It was only when I made it to Section 2 that I verified that, indeed, the phenotypes are not observed but only inferred.\n\nHaving understood the problem, it seems to me that the use case for this approach is quite specific.  We need to have RNA-seq data that can be clustered in such a way that we can assign clusters to pre-defined phenotypic categories.  The claim, in the discussion section, that \"our approach can be easily generalized to .. additional characteristics such as electrophysiology and connectivity\" was not clear to me, but I assume this still refers to phenotypes that are inferred from the scRNA-seq data.\n\nThe critique leveled against CCA is that \"this approach cannot factorize gene expressions according to individual features, making the result hard to interpret.\"  But this seems like it must also be true of the proposed method, since prior to any analysis the data is transformed via PCA (Section 5.2).  I am confused, therefore, about how the method can be used to select genes in Section 6.\n\nThe comparison to LDA is done using two metrics, based on signal-to-noise ratio and mutual information.  I would have liked to hear more about why these particular metrics are appropriate, and in particular how they relate to whatever use case the authors have in mind. Overall, I am still not convinced that this is a problem that needs to be solved.  \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "FLDA is a promising method for scRNA-seq analysis, which misses some deeper evaluation yet.",
            "review": "## Summary\nThe paper provides and ANOVA-inspired method, called FLDA, for creating a low dimensional embedding of scRNA-seq data. Additionally they propose a sparsity based method to find gene signatures, which can be used for further biological validation. The authors extensively evaluated their method on a data set of real expression values in Drosophila neurons.\nThey compared FLDA to two simpler and similar approaches, namely linear discriminant analysis (LDA) and a more feature-aligned version 2LDA.\nOn all benchmarks and metrics the FLDA shows clear advantage over the other methods.\n​\n## Reasons for score\nOverall, I vote for accepting. I can imagine several use cases of this approach, which looks like it is computational very feasible and easy to apply.\nAdditionally it is theoretically well founded on existing statistical methods.\nHowever, my main concern is FLDAs dependency on correct feature annotation, which is not mentioned by the authors.\nI hope authors can address my concern and the other cons in the rebuttal period.\n​\n## Pros\n- The authors clearly justify their design principle of FLDA based on ANOVA.\n- Benchmarks using different scores show clear advantage of the FLDA method.\n- The gene signature found for the T4/T5 data set show known marker genes for neuronal development.\n- Extension to the >2 feature case and other possible extensions are clearly outlined.\n- Extensive appendix showing theoretical validation for most obstacles in scRNA-seq data.\n\n## Cons\n- No evaluation on how dependent the model is on correct feature annotation.\n- No comparison to other methods for estimating gene signatures.\n- Validation on just one biological/real data set.​\n\n## Questions during rebuttal period\nPlease address and clarify the cons above.\nMainly what happens if one level is incorrectly defined, e.g. the ground truth are two or more levels for this singular level. Or in other words how robust is the method to the annotation.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting real world application",
            "review": "*Summary*\nThis manuscript presents a novel dimensionality reduction method, called Factorized Linear Discriminant Analysis. The method starts from a real problem in neurobiology, and tries to link expression levels of neural genes to phenotypes. In particular, the main goal of the proposed technique is to find linear projections of the genes expression which vary maximally with one phenotypical aspect and minimally with the others. The approach is evaluated using a synthetic example and a real case study involving Drosophila T4/T5 cells.\n\n\n*Positive points:*\n-> The paper starts from a real and challenging bioinformatics problem, i.e. the analysis of single-cell RNA sequencing data for neurobiology; data analysis tools for these data are nowadays fundamental to unravel the high complexity of this bio-medical field.\n-> The idea is interesting, well motivated and well explained.\n-> The proposed approach is simple and understandable: interpretability of solutions and results is currently fundamental when dealing with bio-medical data.\n-> The method is tested using a real world application\n\n\n*Negative points / questions:*\n\nCOMMENT 1. \nThe main problem of this manuscript is that the proposed method is not well inserted into the state of the art. Actually the discussion of authors of the related works is very limited, with only one paragraph at the bottom of page 1 plus the description of Linear Discriminant Analysis in Section 4. Many different linear dimensionality reduction techniques have been proposed in the past, each one with different characteristics, goals and optimization techniques. Authors should discuss them, especially in relation with their approach. A good entry point is the survey of Cunningham and Ghahramani:\n\nJohn P. Cunningham, Zoubin Ghahramani: Linear Dimensionality Reduction: Survey, Insights, and Generalizations, Journal of Machine Learning Research 16 (2015) 2859-2900\n\nAlso for what strictly concerns Linear discriminant Analysis, I think that a deeper discussion is needed. Many different extensions of LDA have been proposed, some of them strictly related to the goals/methods of this paper, like 2D-LDA:\n\nMing Li, Baozong Yuan, 2D-LDA: A statistical linear discriminant analysis for image matrix,\nPattern Recognition Letters, Volume 26, Issue 5, 2005, Pages 527-532, \n\nThis contextualization with respect to the state of the art is fundamental: without this, it is very difficult to get the true contribution of the proposed approach. In the same spirit, I suggest the authors to include some more recent techniques in the experimental comparison. \n\n\n\nCOMMENT 2.\nThe sparisification approach has not been fully discussed and justified. Many different methods for sparsification have been proposed, why do authors choose this particular one? How does this method relate to alternatives? Some sparse algorithms have been introduced also for discriminant analysis (not strictly related to the medical field), such as:\n\nN. H. Ly, Q. Du and J. E. Fowler, \"Sparse Graph-Based Discriminant Analysis for Hyperspectral Imagery,\" in IEEE Transactions on Geoscience and Remote Sensing, vol. 52, no. 7, pp. 3872-3884, July 2014, \n\n\nCOMMENT 3 .\nIf I correctly understand, in the experiments authors apply a PCA to reduce the gene expressions before applying the proposed method (last two lines of page 5). Is this a reasonable choice? I know that this is commonly done in other scenarios (like in Fisher-faces for face recognition), but in these experiments genes are fundamental for the knowledge extraction. I guess this is not done for the sparsified version (otherwise genes would have not been extracted), can you comment on this?\n\n\n\n\nCOMMENT 4.\nSome authors argued that formulating the optimization of linear dimensionality reduction techniques as eigenvalue or generalized eigenvalue problems is not always an adequate choice:\n\nJohn P. Cunningham, Zoubin Ghahramani: Linear Dimensionality Reduction: Survey, Insights, and Generalizations, Journal of Machine Learning Research 16 (2015) 2859-2900\n\nIn such paper the authors also suggested an alternative. Can you provide a comment on this?\n\n\nCOMMENT  5.\nI found some difficulties in reading and understanding the presentation and the discussion of the results. One problem was definitely the fact that tables are put in the appendix, and that such tables contains many numbers. I suggest the authors to put a summarizing table inside the manuscript (if possible).\n\n \nCOMMENT 6.\nThe phenotypical features are assumed to be categorical. How strict is this assumption? How much information are we loosing in this context? And in other contexts? In other words, are there other scenarios/applications in which this method be applied?  Adding some other possible application scenarios would increase the value of the proposal.\n\n\nCOMMENT 7.\nThis comment is related to the previous one, but contains more an open suggestion rather than a comment. Did you consider to use non categorical features? I think that this would open the usage of approaches used in three-way data analysis. Even I’m not aware of dimensionality reduction techniques methods for three-way data, I think a relation exists; techniques to extract interesting relations between different directions of data have been proposed, especially in the context of Tri-clustering, see for example:\n\nHenriques, R., Madeira, S.C.: Triclustering algorithms for three-dimensional data analysis: a comprehensive survey. ACM Comput. Surv. 51(5), 95 (2019)\n\nI suggest the authors to take a look also at this field.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}