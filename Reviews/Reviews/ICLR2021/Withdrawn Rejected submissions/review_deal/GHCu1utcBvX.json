{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work considers an apparent problem with current approaches to compositional generalisation (CG) in neural networks. The problem seems to be roughly:\n1. prior work in CG aims to extract 'compositional representations' from the training distribution\n2. work on CG, the training set and the test set are drawn from different distributions\ntherefore\n3. we don't know whether these models can also extract compositional representations from the test distribution\n\nAll four expert reviewers were, to differing degrees, confused by this problem framing, largely because they consider the premise (1) to be false. \n\nI am also aware of a large body of recent work on CG in neural networks (see those papers listed by R2) and, as far as i know, none of it involves extracting 'compositional representations' from the training set. Rather, it involves learning something (from the training set) that enables strong performance on a test set that differs from the training set in a way that is informed by ideas of compositionaity. \n\nAs far as I know, there are very few  studies that try to identify compositionality by considering the internal representations of neural networks, so it feels incorrect to claim this is standard practice. Any work that goes down this route ought to have a very thorough treatement of the various thorny philosophical and theoretical treatments of compositionality in the literature. As pointed out by R4, the work in its current form does not do this. \n\nIn summary, this work attempts to solve a problem that none of the four expert reviewers consider to be in need of a solution. "
    },
    "Reviews": [
        {
            "title": "The submission claims to be the first to truly test compositional generalization (ignoring all prior work that does) and fails to motivate an algorithm (whose relation to prior methods is not discussed) applied to toyish datasets (whose relations to existing evaluations that investigate compositionality are not discussed).",
            "review": "##### Impact:\n\nThe submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization (\"because all combinations have positive joint probabilities in training\"). However, I disagree that this is the case in prior work; here are some examples of prior works that correctly hold out novel combinations (of underlying components) for test time:\n- https://openreview.net/forum?id=HJz05o0qK7\n- http://papers.nips.cc/paper/8825-learning-by-abstraction-the-neural-state-machine\n- https://arxiv.org/abs/1910.09113\n- https://arxiv.org/abs/1912.09713\n- https://arxiv.org/abs/1912.12179\n\nThere are many such examples; they are too numerous to list here.\n\n##### Quality: \n\nThe algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed.\n\n  The evaluation tasks are extremely simple (overlayed MNIST digits and conjoined word token) and are, as such, far from the complexity of existing work on compositionality (which can deal with, for example, naturalistic image data; see the references above for examples of such works).\n\n##### Clarity:\nThere are many points of ill clarity / inconsistencies; for example:\n- \"The main approach for compositional generalization is to learn compositional representations\" Is this really the \"main approach\"? Compositional generalization has been studied in many contexts outside of representation learning (e.g., see https://semanticsarchive.net/Archive/jcyZDc1Y/Goldberg.Compositionality.RoutledgeHandbook.pdf)\n\n- \"We find that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions\" Why is it assumed here that there is an extraction network? The \"extraction network\" is referred to several times in the introduction and methods section prior to its introduction/explanation.\n\n- \"compositional generalization is a type of out-of-distribution (o.o.d.) transferring or generalization, which is also called domain adaptation\" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation. I think the submission could do with a better job of dealing with the distinctions between OOD generalization / domain adaptation / concept drift.\n\n- \"We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network.\" At this point, neither the \"extractor\" nor the \"auxiliary network\" are defined.\n\n- \"These networks can be some existing networks for compositionality learning\" If so, what are examples of \"existing networks for compositionality learning\"?",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of Transferability of Compositionality",
            "review": "## Summary\n\nThis paper studies \"compositionality\" and in particular the way in which it \"transfers\" on test data. They run simple baselines on three experiments (overlapped MNIST, colored MNIST and concatenated month names) and find that the baselines do not learn compositional representation. They proposed the use of an *auxiliary reconstruction network and a regularized optimization* which improves on these baselines. \n\n## Analysis\n\nThe authors frequently say that *compositionality may not transfer to test distribution* but I have a hard time understanding exactly what they mean by this. As I understand it, \"compositionality\" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. They claim theirs is the *first work for the transferability problem of compositionality* which I find really hard to believe. I would have appreciated a thorough study of the \"compositionaly\" limitations of previous techniques.\n\nI found section 4 particularly hard to understand. A lot of symbols, equations and nomenclatures seem to be used with too little introduction. As a result, I cannot vouch for the correctness of this section.\n\nGiven their claim that this is the *first work for the transferability problem of compositionality* the experiments presented on section 5 are on a new dataset and are not compared to previous work. Moreover, the proposed experiments seem relatively simple (two overlapped MNIST digits, colored MNIST digits, concatenated month names) and their baseline seem trivial (*we use a standard neural network with two sub networks, each for an output*) \n\n## Conclusion\n\nOverall, I find the claim of this paper substantial, while the experiments are relatively simple with trivial baselines and an absence of comparison to related work.\n\n## Typos\n\nI find the text difficult to read, it would benefit from a thorough revision. Ex: *This work is orthogonal to many efforts of learning compositionality in training distribution.*\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "the paper considers an interesting general problem, but the concrete supervised learning instantiation is problematic",
            "review": "The paper introduces a “transferability of compositionality” problem and proposes an approach to alleviate it. The said problem may arise when one trains neural models to produce “compositional” representations of the input. In the paper “compositional representations” consist of multiple vectors which are supposed to correspond to semantically meaningful aspects of the input, for example different objects in the case of images or different parts of compound words in the case of linguistic inputs. The transferability problem arises when there is a difference between training and test distributions, namely when certain combinations of objects have different probabilities in training & testing. The proposed solution at inference time is to project object representations to the manifold of individual object representations. The manifold is estimated by saving representations of individual object representations from the training time. \n\nThe problem that the paper considers is an interesting one. There have been a lot of papers on learning object-oriented representations recently [1, 2], and an implicit assumption in all these works is that there is no statistical dependency between which objects that occur in the scenes. There is also the literature on disentangled representations that the paper extensively cites, where the independence assumption is also common. \n\nMy concerns regarding the paper are as follows: \n- Positioning with the respect to the prior work. The literature on learning object-oriented representations is not cited. The work on disentangled representation is cited, but still new setups are created from scratch. \n- Related to the previous point, the use of full supervision (in the form of labels) in the proposed tasks strikes me as a deviation from most previously used setups. Previous work aimed to learn compositional representations without supervision, often positioning their efforts as a cog-sci-style inquiry in building human-like models. The use of labels makes this look less like a cog-sci and more like a machine learning paper. Viewing the work as an ML paper, one thing that stands out is the lack of connections to any applied ML problem. \n- I think the negative results in the paper would look stronger if pretrained image- and language- processing models were used in all experiments (e.g. Contrastive Predicting Coding & BERT) \n\nThe proposed method seems appropriate for the tasks that the paper considers. The experiments appear to be technically sound. My main concerns are thus focused on the motivation of the proposed tasks themselves and the positioning with respect to their prior work. I think a great direction to improve the paper would be to add experiments without supervised learning and using 3D-rendered images with multiple objects as it is done e.g. in [1] and [2]. \n\nFew comments on writing: \n- Algorithm 1 is very confusing because sample-level steps 1-4 are mixed with dataset-level steps 5 and 6. \n- A confusing sentence in the intro: “For a test sample, we regularize each hidden representation in its training manifold, and optimize them to recover the original input”\n- for colored digits experiments you might want to compare to and cite [3] \n\n- [1] “Multi-Object Representation Learning with Iterative Variational Inference” by Greff et al, 2020\n- [2] “MONet: Unsupervised Scene Decomposition and Representation” by Burgess et al, 2019\n- [3] “Invariant Risk Minimization” by Arjovsky et al, 2019",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Paper about a relevant topic, but with insufficient motivation and grounding in previous work",
            "review": "*Summary*\n\nThis paper proposes an architecture that addresses transferability of compositionality. The proposed architecture consists of three components: a network that transforms the input X into a series of hidden representations {H_1, H_2, ... H_K}, a network that reconstructs the input X from this series of hidden representations, and a prediction network that generates a prediction from the hidden representations. The authors propose several datasets meant to address transferability of compositional generalisation, and show that their architecture significantly improves standard DNN architectures as well as humans on these datasets.\n\n*Motivation for score*\n\nCompositional generalization in neural networks is a relevant and hot topic, with still many open questions. This paper aims to contribute on this topic and proposes some interesting datasets. However, However, despite citing several papers addressing compositionality in neural networks in the related work section, I am not convinced that the authors have properly understood the questions that are asked in this domain and were able to address them properly. Below, I outline my concerns.\n\n1. Definition of compositionality\n\nI do not find the definition of compositionality that the authors propose well motivated.\n- None of the three papers cited in the introduction to motivate the work actually has the word \"compositionality\" in the paper\n- The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits\n- The author's definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself. In addition to that, whether the mapping between input and output is compositional does not depend on what is in the train and what is in the test distribution. A model can understand the compositional structure of a dataset also if it has been trained on *all* examples of the dataset, only it will be impossible to behaviourally evaluate if it has. For this reason, much previous work on compositionality in neural networks has created datasets where the training and testing data were distributionally different (as also the authors of this paper do).\nI would recommend the authors to have a look at the paper _Compositionality decomposed: how do neural networks generalise?_ (Hupkes et al.; 2020), for a detailed account of compositionality in the context of neural networks. In particular, their section on _localism_ is particularly important for the author's definition of compositionality.\n\n2. Architecture\n\nI find the proposed architecture interesting, but it is not completely clear to me how it differs from an auto-encoder setup where the encoding is larger than the input instead of smaller (it is very well possible I misunderstood). Nevertheless, it can be interesting to see if auto-encoding based architectures behave better on datasets proposed to evaluate compositionality. One thing that is not clear to me is how the number of components _K_ is determined.\n\n3. Data\n\nI appreciate the effort of the authors to design new datasets that test out-of-distribution generalisation. I do have a few comments/questions:\n- If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn't it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?\n- What is the motivation for using a new dataset, rather than one of the previously proposed datasets for out-of-distribution generalisation?\n- It is nice that the authors try to include tests from different domains, but I think that calling a dataset mapping inputs like \"januarymarch\" to (0, 2) cannot really be called \"natural language processing\"\n\nOverall, I do not believe that this paper should be accepted for the conference.\n\n_**Update after author response:** I have read the author response, but do not find that the answers really address my concerns.  I have also not really seen any improvements in the paper itself._ ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}