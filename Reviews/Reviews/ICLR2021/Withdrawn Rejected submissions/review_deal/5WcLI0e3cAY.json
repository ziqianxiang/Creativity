{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new method for pre-training of language models in the e-commerce domain. It introduces five objectives for pre-training by incorporating domain knowledge into the model.\n\nPros • The paper is generally easy to follow. • Design of the pre-training objectives is reasonable. • Experimental results are solid and convincing. • A useful method is proposed, and its effectiveness has been verified in the e-commence domain.\n\nCons • Novelty of the work might not be enough. • Presentations can be improved.\n• It is not clear whether the proposed approach can be applied to other domains which may not have enough structured data.\n\nThe authors have made several things clearer in the rebuttal. They have also added new experimental results. However, the overall quality of the paper does not reach the level of ICLR from the viewpoint of novelty, significance, and clarity.\n\n"
    },
    "Reviews": [
        {
            "title": "Good practice on pretraining language models for specific domain",
            "review": "Summary: This paper proposes pretraining language model for e-commerce domain. Specifically, the authors design five pretraining objectives to incorporate various domain knowledge into the the models with an encoder-decoder architecture. When further finetuned on language understanding and generation tasks in the e-commerce domain, the proposed models named K-PLUG outperforms the existing baseline models including those pretrained on general domains. The paper is generally easy to follow. Designs of the pretraining objectives are reasonable and empirically effective. Experiments are solid and convincing.\n\nPros:\nThe paper demonstrates a good practice on how to pretrain a language model for a specific domain by injecting a variety of structured and unstructured domain knowledge. Experiments verify that the knowledge-aware pretraining is indeed effective when you want to boost the performances of the tasks in specific domain. The experiments are well designed and quite comprehensive.\n\nConcerns:\n1. The paper is generally lack of novelty and inspiration, however. Most of the pretraining objectives have been tackled more or less in previous work. It is not surprising that when pretraining on the large amount of domain-specific data, the performance on the downstream tasks will improve. It will be more inspiring if the authors can give some high-level principles. Is it possible to design a more general framework that can work well when shifting to a new domain? What are the principles of designing pretrianing objectives with regard to domain knowledge and is there any way to evaluate them without need of expensive pretraining and finetuning?\n2. The presentation of paper could be improved, especially for section 3. For example, from the notation it is unclear which training objectives are defined on encoder and which are on both encoder and decoder. Also in figure 1, there seems no clear distinction between the encoder and the decoder. \n\nAdditional questions: \n1. How the five objectives are weighted in the pretraining? \n2. What are the “knowledge-tokens” and “non-knowledge” tokens in KMLM and KMS2S? Are the results sensitive to them?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work should be described better",
            "review": "The paper proposes K-PLUG, a procedure for pre-training an encoder-decoder transformer with domain knowledge.\nA main claimed contribution that domain-specific knowledge is incorporated both in the encoder and the decoder (as opposed to focusing on the encoder side).\nThe case study is e-commerce, where the authors incorporate information from a knowledge base about products.\n\nMore concretely, the authors propose to apply multiple pre-training objectives adapted to the knowledge source:\n1. Token masking - similar to BERT, where the masked tokens correspond to `\"knowledge tokens\" rather than randomly selected tokens.\n2. Encoder-decoder masking - Again, it is proposed to mask segments of text that cover `\"knowledge tokens\".\n3. Product Entity Aspect Boundary Detection: tag mentions of `product aspects' in the text. \n4. Product Entity Category Classification: Associating a given text with the product category\n5. Product Entity Aspect Summary Generation: adapting previous work, this objective generates a summary from the description of a `\"product entity aspect\".\n\nThe approach is evaluated on several language understanding and generation tasks that pertain to the product knowledge source, including product knowledge base completion, abstractive product summarization, and multi-turn dialogue. \n\nPROS:\n- Integrating knowledge in transformer-based model is a topic of interest\n- The experimental evaluation is solid. The authors compare with previous works, and present sensible ablation results.\n\nCONS:\n- Some details of the approach are under-specified/formulized. The results may therefore not be reproducible. (see comments)\n- The paper is not self-contained (see comments)\n- It is not clear if and how the approach extends to other types of knowledge, outside of the e-commerce and products domain.\n\nDetailed comments:\n\n- The major difference from BERT is that our KMLM prioritizes knowledge tokens - what are the `\"knowledge tokens\"? is it based on lexical match? does it only include `\"content\" words (with no prepositions, for example)? is ambiguity accounted for? is there a preliminary step of entity linking?\n\n- What is a product `\"aspect\" exactly? how are `\"attributes\" and `\"features\" different?\n\n- The baseline method JAVE is missing a citation\n\n- Table 1: why not report also precision and recall, in addition to F1? there seems to be enough space.\n\n- Table 2: note that the bold-faced results are not the best for all columns.\n\n- The evaluation tasks are very briefly described. Perhaps examples can be given? with some explanation of how the proposed approach helps?\n\n- Some terms are under-specified: e.g., `\"intents\" in Sec. 4.2.3. \n\n- 'For the retrieval-based approach, we concatenate the dialogue context and use [SEP] token to separate context and response. \nThe [CLS] representation is fed into the output layer for classification.' - what is the context? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "K-PLUG Review",
            "review": "\nOverview:\n\nThe authors introduce K-PLUG, a knowledge-injected language model with domain-specific knowledge for both NLU and NLG tasks. They evaluate their method in e-commerce scenarios. They use five pre-training objectives, including domain-specific knowledge-bases, entities aspects, entities categories, and entities selling propositions. \n\n\nReasons to accept:\n* The authors present K-PLUG that pre-trained on Chinese e-commerce data, which contains approximately 25 million textual product descriptions.\n\n* The authors evaluate three different downstream tasks, e-commerce KB completion, abstractive product summarization, and multi-turn dialogue, and show good results compared to its baselines. \n\nReasons to reject:\n* The paper is not easy-to-follow with too many abbreviations.\n\n* The main concern of this work is: the comparison to existing works is too weak. As mentioned in Section 2.2, there are many related works also working on injecting knowledge or retrieving knowledge to improve language model pretraining. For example, KEPLER, SKEP, SenseBERT, and K-ADAPTER. However, the authors do not compare their training objectives/strategies to theirs, it is very hard for us to make a conclusion that which is better. It is not an apple-to-apple comparison. The only baselines to compare within this paper is C-PLUG and E-PLUG.\n\n* The dataset used to pre-train the model contains expensive human annotation, which is not scalable. For example, the category labels (used for Product Entity Category Classification) and the summary (used for Product Entity Aspect Summary Generation). In other words, this kind of makes the pre-training process like a \"multi-task\" learning approach with human annotations, which in my opinion is hard to collect more data. \n\n* The ablation study results that are shown in Table 5 basically suggest that many of the used objective functions are not important and could be ignored as the difference with and without is really marginal (I am sure some of them are not significantly better and could be the same if the authors tune a bit the hyper-parameters).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A well-designed and useful PLM for e-commerce; less clear if applicable to other fields",
            "review": "\n#### Summary\n\nThis paper introduces K-PLUG, an in-domain transformer-style pretrained language model. The domain on focus is e-commerce.\n\nTo enforce the PLM to be domain-aware, the authors identify several key types of knowledge for e-commerce: knowledge-bases, product aspects, product categories and product unique selling propositions (USP). They design specific pretraining objectives based on each type of knowledge.\n\nExperiments on several downstream tasks exhibited the usefulness of the proposed model.\n\n#### Strength\n\n- The idea of domain-specific PLM is likely to be useful in practice but not widely explored. Specifically, in the e-commerce field people often have heavy needs for automated text processing and also a large amount of data, making this line of work more useful in practice.\n- The pretraining objectives are properly designed, as they capture domain knowledge and have supervisions from the large-scale pretraining dataset. The experiment results also justified the design.\n\n#### Weakness\n\n- The proposed method (objectives) is hard to transfer to other domains. More importantly, it requires highly-structured and large-scale in-domain data for pretraining, which might not be available in other domains.\n- Some downstream tasks are too similar to pretraining tasks. For instance, Abstractive Product Summarization looks very similar to Product Entity Aspect Summary Generation (PEASG) in pretraining, given that the image modality is ignored. I understand that if pretraining tasks are similar to downstream tasks, the performance on downstream tasks will be better. However, such similarity limits the ability of the experiments on downstream tasks to show the usefulness of the pretrained model. In other words, when the tasks are too similar, it becomes less clear whether the performance gain comes from the pretraining method or merely a larger training set. Also, have you tried directly using the pretraining dataset in downstream tasks (it looks possible for E-commerce KB Completion and Abstractive Product Summarization)?\n\n#### Questions & Suggestions\n\n- In Figure 1a, in the pretraining data aspect descriptions, the knowledge-base values and USPs are highlighted. How are these labeled? Are they directly available from the platform?\n- For the pretraining dataset and downstream task datasets, each of them is said to be collected from \"a Chinese e-commerce platform\". Are they from the same or different platforms? It might be more clear to explicitly provide the platform names.\n- In the formulation of KMLM and KMS2S (page 4) you mentioned \"knowledge tokens\", what are \"knowledge tokens\"? Are they knowledge-base tokens and/or USP tokens?\n- For multi-turn dialog, since it is highly user-oriented, it would be better to have human evaluations.\n\n#### Typos\n\n- Page 5, PEASG, Line 6: \"a aspect\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}