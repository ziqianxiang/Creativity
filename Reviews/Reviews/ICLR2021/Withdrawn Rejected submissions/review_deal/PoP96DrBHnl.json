{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces an simple but potentially effective off-policy TD algorithm. \n\nOverall, the reviewers felt the work was incomplete and not yet ready for publication. The all recognized the authors made significant updates to the paper, but serious issues remain with the empirical work: studying the impact of the proposed extension on other algorithms, missing baselines (e.g., TDRC), scope of environments limited similar chain-like domains, significant questions about how best parameter settings where chosen for comparison, etc.\n\nThis is clearly an interesting direction. If the authors can improve the experiments and better situate their method if the literature (connecting to the lit in off-policy RL about accelerating and improving off-policy TD methods this will become a solid contribution."
    },
    "Reviews": [
        {
            "title": "Interesting approach, but needs more work",
            "review": "EDIT: After reading the other reviews, the author's responses, and thinking more about the concerns raised, I have increased my score. However, I still recommend rejection because of questions around the hyperparameters used in the experiments.\n\n---\n\n### Summary:\nThe paper introduces a regularized mean squared projected Bellman error objective function where the regularizer penalizes large changes to the estimated value function. This regularized objective function is used to derive a GTD2-like algorithm where updates to the value function weights are penalized. The paper claims an improved rate of convergence, and empirically investigates the proposed algorithm on tabular random walks, the Boyan Chain environment, and Baird’s counterexample.\n\n### Pros:\n+ paper proposes interesting new method\n+ paper includes theoretical argument for proposed method\n+ paper empirically investigates proposed method\n\n### Cons:\n- concerns about soundness of method\n- concerns about originality, clarity, and quality\n\n### Decision:\nAt the present time I recommend rejecting the paper until the following concerns can be addressed.\n\n### Soundness:\nDoes the proposed modification to the MSPBE change the underlying problem being solved? Is the solution to the regularized MSPBE the same as the solution to the original MSPBE, even with function approximation? The fact that Gradient-DD(4) did not converge on the Boyan chain is very concerning. The motivation for GTD2 is to converge when used off-policy with function approximation. If the proposed modifications lose the convergence guarantee then why not just use conventional TD off-policy?\n\n### Originality:\n- There are no references to prior work on convergence rates of GTD2 in section 4. The analysis seems like it was based on an existing analysis, but nothing is cited.\n- There is no explicit related work section, which would help clarify the novelty of contributions and would help position the paper within the existing literature.\n\n### Clarity:\n- Section 4 (improved convergence rate) is poorly explained, and very difficult to follow.\n- Section 5 doesn't mention beta—the step size for the auxiliary weights. Earlier in the paper kappa is referred to as a regularization parameter, but in section 5 it's called a step size parameter and annealed?\n- There are several statements that don’t make sense to me:\n\n    - “the regularization term uses the previous value function estimate to avoid large biases in the updating process.”\nThe use of the word “biases” here is confusing and conflicts with the statistical notion of bias. Updates to weights would generally not be considered “biases” in the statistical sense. However, the regularization term can be thought of as biasing the optimization towards solutions with certain qualities.\n\n    - \"[importance sampling] is useful for decreasing the variance of parameter updates\"\nUsing importance sampling to correct the difference between the target and behaviour policies usually increases the variance of parameter updates. IS shrinks updates that occur more often than they would when following the target policy, and enlarges updates that occur less often than they would when following the target policy. The average distance from the mean update can be larger than without importance sampling.\n\n    - \"In effect, the regularization term encourages re-experience around the estimate at previous time step, especially when the state space is large.\"\nWhat does “re-experience” mean?\n\n    - “accelerate the GTD2 algorithm” The word “accelerate” is used several times in the paper to describe the Gradient-DD update, but the idea of penalizing large updates to the value function weights conflicts with the conventional meaning of acceleration in optimization (using past information to make larger changes to weights as is done with Nesterov acceleration, momentum, ADAM, etc.), which is confusing. Penalizing updates to the value function weights would actually slow the changing of the value function weights, not accelerate it. This might allow the second set of weights to learn better estimates of the expected TD error (because the expected TD error is changing as the value function weights change), which could account for the performance increase over GTD2.\n\n### Quality:\n- Best performance in the final episode is not an appropriate way to determine the \"best-performing\" parameter settings when the paper makes claims about the speed of learning of various methods. The parameter settings that result in the lowest error at the end of training will not in general be the parameter settings that result in the fastest learning (i.e., smallest area under the curve). If the paper is going to make claims about learning speed, then the parameter settings should be selected based on the smallest area under the curve. This might be why TDC performs so poorly in these experiments when it out-performs GTD2 in other papers (see Ghiassian et al. 2018; TDC is called GTD in that paper) and intuitively should perform similarly to conventional TD in early learning when the correction weights are near 0. This seems like a serious issue to me; the experiments may need to be re-run with different parameter settings that better match the claims the paper is making about learning speed.\n\n### Suggestions for improvement:\n- In addition to addressing the concerns mentioned above, consider adding a related work section that explicitly compares and contrasts the most relevant related methods.\n- Consider motivating Gradient-DD more along the lines of TRPO, REPS, and other algorithms that penalize large changes to the weights being learned instead of motivating it as accelerating GTD2.\n- Actually, it would be better to do some simple experiments to test why the regularization improves performance over GTD2. Does it result in the second set of weights learning the expected TD error with greater accuracy? Can the same effect be achieved by a two timescale approach where the value function weights are updated with a smaller step size than the second set of weights? If not, it would provide more support for the proposed method.\n- Despite the concerns listed in this review, I actually think this paper has a very interesting premise and deserves further study and investigation.\n\n### Misc. details:\n- A sentence trails off in the first paragraph of the introduction.\n- ”where this term originates from the squared bias term in the objective (6)” Equation 6 seems to be the GTD2 update rules, not the objective function.\n\n### References:\nGhiassian, S., Patterson, A., White, M., Sutton, R. S., & White, A. (2018). Online off-policy prediction. arXiv preprint arXiv:1811.02597.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "### Summary of Contributions\n\nThe paper proposes the gradient descent TD difference learning (GDD) algorithm which adds a term to the MSPBE objective to constrain how quickly a value function can change. They argue that their approach has a quicker convergence rate, and empirically demonstrate in several examples with linear function approximation that it substantially improves over existing gradient-based TD methods.\n\n### Review\n\nI like the simplicity of the proposed method, and its intuitive interpretation as a value-based trust region. However, I have the following questions and concerns:\n\n1) There doesn't seem to be any information regarding how many independent runs were performed in the empirical evaluation, and there was no no reported statistical significance testing. Can the authors clarify this information, and comment on the significance of the results?\n\n2) While it led to improvements over GTD2, it largely didn't improve over regular (semi-gradient) TD apart from Baird's counterexample, which was designed to make TD fail. As such, I don't think the addition of a new parameter was convincingly justified. Some of the results seemed to suggest that the improvement grew as the state space/complexity increased, that it may be the case that the evaluation falls a bit short on exploring more complex environments. While the breadth of the ablation studies is really nice, we observe similar trends in many neighbouring figures that the space in the main text from showcasing the many different configurations could be summarized with representative examples, and the additional space could have been used to provide some additional experiments/insights (like those suggested in the discussion).\n\n3) From how modular the addition of the term is to the objective, have the authors tried incorporating the regularization to semi-gradient TD? Is there anything about the semi-gradient update that bars its use? TD generally performed really well in the paper's evaluation (outside of Baird's counterexample) that it would make a stronger case if the extension was demonstrated to be more generally applicable, and that it consistently improved over the methods it was applied to. This sort of ties into what was described in 2), where what was presented seems to fall a bit short, and how the space could have showcased a bit more.\n\n4) While the paper's focus was on the case of linear function approximation, can the authors comment on how readily the approach can be extended to the non-linear case? GTD methods have not seen as much adoption as their approximate dynamic programming counterparts when combining TD methods with non-linear function approximation, that it can raise questions as to how the methods scale to more complicated settings.\n\nGiven the above, I am erring toward rejection at this time. I think 1) is a rather significant issue that needs to be addressed, and I'm willing to raise my score if that, and my other concerns, can be sufficiently addressed.\n\n----- Post Discussion -----\n\nTaking the other reviews and the authors' response into account, I still maintain my score. While I agree that it's good to be thorough in something clear and simple, it can still be done to a point of redundancy, and consequently seem less thorough in the overall picture and claims made. I'm still largely unsure on the choice to only apply the supposedly modular extension to GTD2, and not try it with TD which seemed like a clearer winner (apart from Baird's counterexample). As others suggested, there are additional methods which might be good to compare to, and other evaluation metrics might make more sense for the claims being made. Many of my concerns were largely brushed off as future work, that little got addressed- without having to carry out the experiments, high level comments/current thoughts could be provided regarding how readily the approach can extend to the scenarios suggested, or if there are nuances that need to be worked out, etc.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea! The analysis is not rigorous and convincing.",
            "review": "This paper proposes a variant of the GTD2 algorithm by adding an additional regularization term to the objective function, and the new algorithm is named as Gradient-DD (GDD). The regularization ensures that the value function does not change drastically between consecutive iterations. The authors show that the update rule of GDD can be written as a difference equation and aim to further show the convergence via Lyapunov based analysis. An simulation study is provided to compare the proposed GDD algorithm with TD, ETD, and GTD. \n\nThe paper is well written in general. The idea of extra regularization on the distance between two value functions sounds reasonable to me since it resembles the constraint in trust region optimization for policy gradient methods. However, the claimed improved convergence over GTD is not rigorously proved and thus not convincing. \n\nIn Section 4, the convergence analysis is not derived in a rigorous way. It would help the readers to understand the improved convergence if the authors could complete the analysis and show the convergence rate. \n\nWhy can the eigenvalues of matrix J_n be written as the block matrix before eq (14)? It seems to me that G and H are diagonal matrices with the diagonal elements being the eigenvalues of G_n and H_n. Ideally the eigenvalues of J_n, which is denoted as J in this paper, should also be a diagonal matrix. Furthermore, since G_n is not symmetric, G may have some complex values as its eigenvalues. This is ignored from the current analysis without any explanation. \n\nIn the experiment part, Figure 3 shows that the RMS error of the GDD algorithm will blow up when step size is large. It seems that the proposed algorithm may not be as robust as the conventional TD algorithm?\n\n#########Edits after the rebuttal#########\nThank you for the responses. After reading them and the discussion with other reviewers, I still think the current contribution of this paper is marginal and I keep my score as 5.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "## Summary of paper\nThis paper introduces a novel regularized mean-squared projected bellman objective and corresponding GTD2-like algorithm which minimizes the objective. The paper analytically investigates the convergence rate of the proposed algorithm, then empirically investigates the performance of the algorithm across several problems with linear function approximation.\n\n## Summary of review\nThis paper is a clear reject for me. There appear to be significant issues in both the analytical section and the empirical section; which in total bring into question the utility of the proposed algorithm. The literature review also appears to be lacking and there appear to be several minor incorrect statements throughout the paper. I feel quite confident in my evaluation of the empirical section and literature review. I feel confident that there is a bug/typo/incorrect result in the analytical section. I did not attempt to debug the proof to determine which of the three (bug, typo, or incorrect result) was true. Details follow roughly in order of greatest concern -> least concern.\n\n### Proof of convergence rate\nI follow the proof up to equation (12). This is a standard result from (e.g) Maei's 2011 thesis. After equation (12) I follow the transformation that solved for $\\rho_{n+1} - \\rho_n$ resulting in a matrix inverse. However, after distributing the matrix inverse, I remain confused why $\\sqrt{\\xi} g_{n+1}$ is not multiplied by the inverted matrix? This, however, may not change the resulting eigenvalues so I do not believe it will change the result. Continuing on to equation (14), I did not check for correctness solving the polynomial for the eigenvalues, so I will rely on the given result in the paper. The conclusion of which states: $\\lambda = - \\frac{1}{2} (\\alpha^{-1} + \\lambda_G) \\pm \\frac{1}{2} \\sqrt{\\text{thing}} < -\\lambda_G$. Ignoring a few details in the middle, the result ultimately states $\\lambda < -\\lambda_G$. Here lies the fundamental problem. Because GTD2 is known to be a convergent linear system, then we know $\\lambda_G$ are all strictly negative (proof of this in Maei's thesis) under some assumptions (notably the invertibility of $X X^\\top$). For such a linear system to be convergent, we **must** have the real part of the eigenvalues be strictly negative. The proof under equation (14) states $\\lambda < -\\lambda_G$, which because $\\lambda_G$ is strictly negative, means that $\\lambda < \\text{some positive number}$. This (a) tells us nothing of the comparative convergence rates since $\\lambda$ could be larger than $\\lambda_G$, and (b) also suggests that there could be cases where the proposed algorithm does not even converge in the first place because $\\lambda$ could be greater than or equal to 0. This could be a typo and the negative sign on the right shouldn't exist, but because there are few details from equation (14) to the end, I was unable to debug and decide if this is typo/bug/incorrect conclusion.\n\nThere a few other issues with the proof that concern me.\n* Do we know that the upper-bound on $\\kappa$ is reasonable? The quantity $\\alpha (\\lambda_G - \\alpha^{-1})^2 / 4$ is difficult to interpret, but the reliance on $\\alpha^{-1}$ seems to imply a preference toward much smaller stepsizes; which seems counter-intuitive towards the goal of improving convergence rates.\n* Likewise, the first part of the solution for $\\lambda$ we have $-\\frac{1}{2} (\\alpha^{-1} - \\lambda_G)$ which likewise implies that smaller stepsizes significantly improve convergence rate (small $\\alpha$ implies highly negative $\\lambda$). Considering the remainder of the solution for $\\lambda$ is under a square-root, this first factor appears to be dominant (though I could be wrong on this, more insight would be appreciated!). A stepsize approaching 0 would yield the fastest convergence rate (apparently) by having the smallest eigenvalue approach negative infinity. This simply does not pass the \"smell test\".\n* Further building on my previous point, for large enough values of $\\alpha$ we will be in a situation where $\\alpha^{-1} < |\\lambda_G|$ meaning that the first term will become positive. Considering that the second term is a plus-or-minus, then we could be adding a positive term to a positive term yielding a positive eigenvalue. This means that for large enough $\\alpha$ we don't have convergence any longer. I wonder how feasible the upper-bound on $\\alpha$ is to guarantee convergence. I would also like to see these assumptions explicitly stated in the proof.\n* A lot of little details were left out of the proof. Where are the assumptions on boundedness of the features and the rewards? Can one show that the noise sequence of the modified algorithm is actually a Martingale Difference Sequence and thus the result from Borkar and Meyn 2000 holds? Need there be an assumption of independent samples or are these samples coming from Markovian sampling?\n\n### Empirical section\n* The choice to set the initial value function $V(S) = 0.5 \\forall S$ for the random walk was odd. I suppose that because the left reward = 0 and the right reward = 1, and the policy is 50% chance to go left or right, and $\\gamma=1$, then the optimal value function $v_\\pi$ linearly interpolates from (0, 1) with the center-most state having value $V(n // 2) = 0.5$. This choice seems likely to disproportionately favor the proposed GDD algorithm; which encourages the value function estimate to change slowly. Because the initial estimates are so close to correct, only small changes will be necessary and the regularizer term will remain small. What happens if the value function is initialized to 0 everywhere, or even to -1 everywhere?\n* The exclusion of TDC from the stepsize sensitivity investigation makes little sense to me. The first experiment chose an aggressively large stepsize $\\alpha = 0.5$ for which TDC performed poorly. Then did not investigate the sensitivity of TDC to stepsize in later plots because of this choice. If you check [Giassian et al. 2020], they report that TDC in fact out-performs GTD2 on all of the same domains tested here for appropriately chosen $\\alpha$ and $\\beta$.\n* The choice of $\\beta$ is never discussed. How did you set $\\beta$?\n* How many runs? What is the variance? Are any results statistically significant?\n* The primary motivation of the paper was around off-policy learning, yet only one of the tested domains was off-policy (Baird's Counterexample star MDP). It would have been nice to see the random walks made into off-policy domains.\n\n### Literature review\n* This paper modifies the MSPBE by adding a regularizer term. There are a few other papers in the literature that do this and derive the corresponding GTD2/TDC algorithms. Liu et al. 2012 and Ghiassian et al. 2020 immediately come to mind. These should both be cited and discussed.\n* Are there any papers that add such a constraint as $\\| w_n - w_{n - 1} \\|^2$ to any known objective function? This seems like an odd choice of regularizer (penalizes making changes to the weights), so any prior literature from any field (supervised learning, online learning, optimization, etc.) would go a long way in convincing the reader that this is a good idea.\n* The paper mentions several times that GTD methods converge more slowly than TD. I know of a single proof that shows this in Maei's thesis for the GTD algorithm. I do not know of any such proof for TDC or GTD2. There exists empirical evidence of this in Ghiassian et al. 2020 or White and White 2016, but neither of these papers are cited.\n* GTD methods and importance sampling are not mutually exclusive methods for off-policy learning. In fact, GTD methods canonically use IS for their off-policy variants. Further importance sampling definitely does not decrease the variance of parameter updates (mentioned in the second paragraph of Section 1).\n* Sutton et al. 2009 is not really a breakthrough in the study of convergence properties of MDP systems. In fact, the proofs of Sutton et al. 2009 do not even assume samples are drawn from a distribution induced by an MDP. Perhaps Borkar and Meyn 2000 is a better reference as it fundamentally builds the proof structure used by Sutton et al. 2009?\n\n### Other minutiae\n* Eye-balling the modified objective function leads me to believe the objective shares the same fixed-point as the MSPBE (thus the new GTD2 algorithm converges to the same fixed-point as TD), but it would be nice to show this formally in the analytical section.\n* Is it possible to extend this objective to the non-linear setting?\n* The paper mentions that the proposed regularizer avoids large biases in the updating process. Does it not *add* bias to the updating process? Perhaps it was meant that the regularizer avoids high variance? Either way, a careful analytical discussion of the bias-variance properties would go a long way towards improving this paper.\n* In section 5.3 what is $\\eta$? I believe this is supposed to be $\\beta$ (i.e. the stepsize for the secondary weights) since $\\eta$ is your secondary weight vector?\n* Would it make more sense to consider $\\kappa$ to be a regularizer parameter instead of a stepsize and having it absorb $\\alpha$? It seems in the experiment section you split these anyways, so perhaps it makes the analytical section much more clear if the algorithm was instead $\\alpha\\kappa (x^\\top w_n - x^\\top w_{n-1})$. \n* The paper repeatedly defines off-policy learning as learning the optimal policy using an exploratory policy. This is a bit of a restrictive setting and is certainly not the setting that Sutton et al. 2009 considered (the work that this paper builds upon).\n* Why assume that the target policy is deterministic (mentioned in Section 2.1)? This is a strange choice that is not used in either the empirical or analytical section as far as I can tell.\n* It is mentioned that TD methods should seek to minimize the MSPBE (or perhaps the MSBE it isn't clear which is meant), but shouldn't instead the goal be to minimize the MSVE (e.g. $\\| \\hat{V}_w - v_\\pi \\|^2$)?\n\n### Papers mentioned in this review\nMaei, Hamid Reza. “Gradient Temporal-Difference Learning Algorithms.” University of Alberta, 2011.\n\nSina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White. “Gradient Temporal-Difference Learning with Regularized Corrections.” International Conference on Machine Learning, 2020. http://arxiv.org/abs/2007.00611.\n\nAdam White, and Martha White. “Investigating Practical Linear Temporal Difference Learning.” International Conference on Autonomous Agents and Multi-Agent Systems, 2016. http://arxiv.org/abs/1602.08771.\n\nBorkar, V. S., & Meyn, S. P. (2000). The O.D.E. Method for Convergence of Stochastic Approximation and Reinforcement Learning. SIAM Journal on Control and Optimization, 38(2), 447–469. https://doi.org/10.1137/S0363012997331639\n\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, C., & Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09, 1–8. https://doi.org/10.1145/1553374.1553501\n\nLiu, B., Mahadevan, S., & Liu, J. (2012). Regularized Off-Policy TD-Learning. Advances in Neural Information Processing Systems, 9.\n\n# After discussion and edits\n\nI acknowledge that I have read the other reviews and resulting discussions and I have read the relevant changes in the edited text. I have raised my score from 2->3 to reflect that several concerns were alleviated through the edits, but several new concerns (and old concerns) remain. I will summarize below.\n\nAfter the author edits, the issue with convergence and convergence rates appear to have been resolved. I additionally appreciate the much greater clarity in the analytical section. However, I still find the contribution to be borderline at best in terms of novelty of approach and I find that the evidence of applicability is still considerably lacking. The introduction of a regularizer to accelerate GTD methods is itself not novel. The form of the proposed regularizer is novel, however, I find its form to be unintuitive as it punishes making changes to the weights. There are some prior works that motivate this well (i.e. TRPO and other trust-region optimization techniques), but this paper does not appeal to prior works to motivate their regularizer. Instead, I must rely on the empirical study which does not investigate the learning speed of the proposed algorithm compared to baselines. In many cases, the proposed algorithm does not clearly outperform baselines.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}