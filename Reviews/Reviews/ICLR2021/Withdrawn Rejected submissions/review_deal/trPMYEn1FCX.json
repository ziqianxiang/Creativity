{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a method for out-of-distribution modeling and evaluation in  the human motion prediction task. Paper was reviewed by four expert reviewers who identified the following pros and cons.\n\n> Pros:\n- New benchmark for testing out of distribution performance [R1]\n- Compelling performance with respect to the baselines [R1,R4]\n- Paper is well written and easy to  follow  [R2]\n- Generative model in the context  of  out-of-distribution modeling of human motion is novel [R1,R2,R4]\n\n> Cons:\n- Lack of support for interpretability claim  [R1]\n- Validity and usefulness of the metric [R1]\n- Lack of \"effectiveness\" of the proposed approach [R2,R4]\n- Technical contributions are not significant [R3,R4]\n- Experimental validation lacks comparisons to other state-of-the-art in motion prediction  methods [R3] \n- Lack of evaluation on additional datasets and for the main task [R4]\n\nAuthors tried to address the comments in the rebuttal, but  largely unconvincingly to the  reviewers.  On balance, reviewers felt that negatives outweighed the positives and unanimously suggest rejection. AC concurs and sees no reason to overturn this consensus. \n"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary:\nThis paper proposes a method and benchmark for out-of-distribution modeling and evaluation of human motion. They evaluate against state-of-the-art human motion methods, and show favorable performance against them.\n\n\nPros\n+ Generative model formulation for human motion prediction\n+ Benchmark for testing out of distribution performance in Human 3.6M and CMU-Mocap\n+ Proposed generative model outperforms baselines\n\n\nComments / Suggestions:\n- Interpretability claim:\nThe authors talk about facilitating interpretability in the abstract, however, I fail to find any clear experiments suggesting this. For example, I cannot find analysis of the different dimensions in the learned latent space or anything of that nature. I see section B in the supplementary material discusses interpretability, but I fail to find any clear cut results about this. Can the authors clarify how this claim is reflected in the paper?\n\n- Evaluations:\nThe evaluations provided in this paper are based on euclidean distance measured with respect to the ground truth. While this metric is reasonable, it may also not be enough to evaluate a generative model of motion (e.g., there are multiple plausible futures given a single past). Given that there are clearly defined actions in the used datasets, I would suggest using a metric that measures the generated sequences as a whole. For example, one can train a motion recognition network which given a motion tells us what type of motion we are observing. The authors could train this type of network and test it on their generated motion to see if the predicted / generated motion is recognized as the right category. Another similar evaluation would be FID, where the authors can see if the predicted / generated motion distribution in feature space is close to the ground truth distribution.\n\n- Differences with Kipf & Welling, 2016:\nThe authors mention that they adopt VGAE from Kipf and Welling, but I fail to find where the authors mention what are the specific differences of their method in comparison to Kipf & Welling, 2016. Can the authors clarify this or point out where the specific differences are mentioned?\n\n\nConclusion:\nThe proposed benchmark is interesting and useful for out-of-distribution evaluations, however, some evaluations may be missing to make this more comprehensive. The differences between the method used by the authors and the related work need to be clarified. I am willing to change my score if the authors successfully address the issues mentioned above.\n\n###########################\n  Post Rebuttal Comments\n###########################\n\nAfter reading the rebuttal, I am keeping my original score. For my first concern, they authors mention space as being a limitation for not providing analysis on the \"surveyable\" latent space, but as far as I know, additional experiments addressing my concern could have been added to the supplementary material. For my second concern, the authors talk about excelling in synthetic fidelity, however, there are fidelity measures for generative models that were not used in this submission. MSE is not a fidelity metric. I suggest that the authors address the concerns raised by the reviewers in future submissions, and it's highly likely that the work will be more solid.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed approach is reasonalbe for dealing with Out-of-Distribution (OoD) problem of human motion prediction. But the experimental results are unconvinced. ",
            "review": "This paper presents a generative model to solve the OoD problem in human motion prediction. It extends the GCN and attention-GCN works with VGAE for predicting human motions that are  different from ones used in training. Experiments are performed on H36M and CMU benchmarks for illustrating the efficacy of the proposed approach.\n\nPros:\n1. The paper is good in writting and easy to follow the idea\n2. The perspective of using generative model to deal with OoD problem in human motion is novel.\n\nCons.\n1. My major concern is the effectiveness of the proposed approach. From the results shown in Table 3 to Table 5, we could find that the proposed approach fail to solve the OoD problem for some actions when comparing with the baseline attention-GCN. For example, in Table 5, the proposed generative model achieve poor performance than attention-GCN, such as Discussion, Posing, Purchases, Walking Dog and Walking Together (5 out of 14 acitions). These experiments could not provide convinced results to depict the efficacy of the proposed approach. I think the authors should provide more explanations on this which should make this paper stronger. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"GENERATIVE MODEL-ENHANCED HUMAN MOTION PREDICTION\"",
            "review": "The paper presents, firstly, a new benchmark (based on Human3.6M and CMU datasets) for human activity and motion with a high degree of out-of-distribution examples, and secondly a hybrid framework for human motion prediction which is more robust to out-of-distribution samples. \n\nOn a positive note, the presented view of human activity as highly compositional and without a clear ontology of actions and sub-actions is highly relevant, and the observed issues with the state of the art methods are completely correctly characterized. The proposed behchmark is also highly valuable to the community.\n\nHowever, the paper suffer from two flaws that renders it unfit for publication in ICLR in its current form. \n* Firstly, the contribution - to combine GCN with the approach of (Myronenko 2018) to regularize the training with a generative model that takes unlabeled samples into accound, and to replace their VAE framework with a corresponding VGAE one -  is not on its own significant enough to serve as a basis for an ICLR paper. \n* Secondly, the experiments are not adequate in that the method is only compared to a GCN without the generative model - and not with any of the other state-of-the-art in motion prediction. Moreover, results are presented without standard deviations which makes it hard to determine if the improvements are significant.\n\nThese two flaws leads to a Reject recommendation, but the authors are highly encouraged to expand the experiments to empirically verify that the proposed contribution is significant enough to warrant publication, and resubmit to a later conference. While addressing the second flaw, it would also help convince the reader of the significance of the method contribution.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review - Generative Model-Enhanced Human Motion Predicition",
            "review": "## Summary:\n---\nThis paper raises and studies concerns about the generalization of 3D human motion prediction approaches across unseen motion categories. The authors address this problem by augmenting existing architectures with a VAE framework. More precisely, an encoder network that is responsible for summarizing the seed sequence is shared by two decoders for the reconstruction of the seed motion and prediction of the future motion. Hence, the encoder is trained by using both the ELBO of a VAE and the objective of the original motion prediction task. \n\n## Pros:\n--- \nThe paper has a novel and interesting direction as robustness to distribution shifts has not been studied before in 3D human motion modeling. It is implemented around one of the SoTA models based on Graph Convolutional Networks (GCN) using discrete cosine transformation (DCT) features extracted from the motion sequence. To simulate the out-of-distribution scenario, the baselines and the proposed extension are trained on a single action category such as walking and tested on the remaining actions such as eating and sitting. Experiment results on the H3.6M and CMU datasets show that the proposed approach is useful on out-of-distribution (OoD) test cases. \n\n## Cons:\n--- \nI have two main concerns on the proposed benchmark and the models. \n \n-- OoD Benchmark--\n- It looks like there is a significant underfitting problem. The performance of GCN on the walking category is 0.56 at 400 ms while the in-distribution (ID) performance with OoD training is 0.66 (Table 1). The training split for the OoD setup proposed by the authors is possibly too small. I also can not grasp the motivation for selecting a training set “as small in quantity, and narrow in domain as possible” (Section 3). While there is not enough or barely enough training samples, the comparisons might be misleading. We do not know how the proposed extension behaves on the standard task. The authors should compare their models on the main task as well.\n \n- Motion samples from different categories (i.e., walking, eating, etc.) can still be useful for the models in learning the 3D human motion prior. In fact, it has been shown by Martinez et al. (2017) [4] that training motion models with _all_ available actions improves the performance significantly compared to a single-action models as done in this paper. While the proposed approach outperforms the baselines in average performance, it is not always or substantially better on the fine-grained actions. \n \n- It is a tedious setup, but a leave-one-action-out strategy can be more reliable. In my opinion a better option would be training on one dataset and testing on another one. This would allow for an evaluation of the existing (and even pre-trained) models directly where the proposed extension would remain as the only factor for evaluation. In the context of H3.6M and CMU datasets, this might not be straightforward due to different skeletal configurations. Yet there exists a much larger benchmark for 3D motion prediction: AMASS [1]. This would be a suitable candidate for this task as it is a collection of several diverse mocap datasets with different motion categories. It would be very easy to train on a subset of datasets and test on the remaining ones as all the datasets follow a unified skeletal configuration. Note that this is only a suggestion to improve the current work and I am not asking for running experiments on AMASS for the rebuttal as it would drastically change the submission.\n \n-- Quantifying the OoD--\n- The existing architectures are augmented with a VAE latent space and a decoder, which is not technically novel. However, regularization of the representation space and reconstruction of the inputs as auxiliary tasks seem like helpful to the motion prediction task and a good contribution under the “limited” training/evaluation protocol. At the same, time the proposed evaluation protocol is not orthogonal to the task but instead it just follows the existing task protocol. In other words, it is not an independent metric/method/framework for assessing the existing models’ OoD performance. \n\nI am asking the following questions as the proposed approach is presented as a “framework”:\n\n- The authors hypothesize that motion prediction in generative modeling frameworks can alleviate the OoD problems. I find it too broad as generative modelling can be applied in various frameworks. Although it is conceptually very different, [1] uses an auto-regressive model, which is a generative model by design, and trains the model by predicting both the seed (i.e., loosely reconstruction) and the future frames similar to the proposed approach. Can we say that they also deal with the OoD problems implicitly? How do the authors position their “framework” compared to this line of work? \n\n- The authors only focus on the Seq2seq-based methods for motion prediction and choose a baseline with an implicit temporal model (i.e., using DCT to encode the motion sequences). Hence, the proposed approach seems to be limited to this GCN-based architecture. How can the sequential deterministic models [1, 3, 4] be addressed? \n \n-- Additional comments --\n- Figures should be improved. Especially the text is hardly readable. \n\n- I find it very hard to follow Section 3. It was clear only after I read the section A in the appendix. It would be clearer if some of the findings are discussed in Section 3 already. \n\n- The losses in the tables are too high compared to the actual task. I am not sure if there is a qualitative difference between the models as the authors did not present any qualitative results. \n\n- Missing related work on 3D motion modelling. I list a s,all collection of SoA representatives below:\n \n[1] Aksan, Emre, Manuel Kaufmann, and Otmar Hilliges. \"Structured prediction helps 3d human motion modelling.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[2] Gui, L. Y., Wang, Y. X., Liang, X., & Moura, J. M. (2018). Adversarial geometry-aware human motion prediction. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 786-803).\n[3] Pavllo, D., Grangier, D., & Auli, M. (2018). Quaternet: A quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485.\n[4] Martinez, J., Black, M. J., & Romero, J. (2017). On human motion prediction using recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2891-2900).\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}