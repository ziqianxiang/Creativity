{
    "Decision": "",
    "Reviews": [
        {
            "title": "Improving Zero-Shot Neural Architecture Search with Parameters Scoring",
            "review": "Summary:\n\n \nThe paper provides an interesting monotonicity metric to evaluate the adequate relative scoring of the architectures. Their exploration shows that the use of noise improves the score, and they observed a clear improvement when the evaluation\nis done in the parameter space. The authors performed their experiments on CIFAR10, CIFAR100, and ImageNet datasets. \n\n##########################################################################\n\nReasons for score: \n\nOverall, my score is more or less on the order but tend to see this work as a valuable contribution despite that I find some limitations in the paper. My major concern is about the clarity of the paper and some missing ablations.  Hopefully, the authors can address my concern in the rebuttal period. \n \n\n \n##########################################################################Pros: \n\n \n1. The paper takes one of the most important issues of neural architecture search: zero-shot NAS measuring the ability of the performance of a network in a task before the network is trained. For me, I find this problem of high importance and may encourage applying NAS to more problems/ applications. \n\n \n2. The paper explores twelve scoring metrics including the previously proposed JS score showing a reasonable depth in the possible scoring mechanisms to compare with.\n \n3. This paper provides comprehensive experiments on these twelve metrics on three datasets (CIFAR10, CIFAR100, ImageNet). \n They showed both the connection with accuracy and monotonicity of each score with respect to the accuracy.\n\n \n##########################################################################\n\nCons: \n\n1. the explored metrics seem to need more discussion and more motivation. Some of them are reasonable but for others, more explanation is required why they are included. More discussion of why they perform well or bad is needed. \n\n2. I still suggest the authors conduct the following experiments studies to enhance the quality of the paper: \n\n(1) how does the metrics that use noise perform based on the scale of the noise $k=10^-6,10^-7,10^-8, 10^-5. 10^-4$\n\n \n(2) can this score be used to improve AutoML zero? if so, would be interesting to do an experiment to study  if this may accelerating the learning process. \n\nAutoML-Zero: Evolving Machine Learning Algorithms From Scratch\nhttps://arxiv.org/abs/2003.03384, ICML, 2020 \n\n \n3. How scalable is this approach to apply for full imageNet scale and perhaps multimodal tasks? What could go wrong?  \n\n \n4. \"An ideal slope is not necessarily 1 or -1, but at least should remain constant or high enough for different ranges of the\nscore, to ease its usability.\"\nplease elaborate more experimentally on the tradeoff and what it means.   \n\n\nminor\n--------------\nthere are big spaces between figures (e.g., Fig 3 and 4) and the remaining space can be used to enrich the paper (more discussions & insights). \n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary\n   This paper builds on top of a zero-shot NAS paper [1]. In [1] they developed a metric based on the correlation matrix of the Jacobian wrt every datapoint for a batch. This paper conceived few other scores the predominant one being Jacobian Cosine (JC), Parameter based Jacobian Score (PJS), Weighted Parameter Jacobian Score (WPJS) and Weighted Parameters Jacobian Score Inverse (WPJSI).\nJC: Rather than having to compute the computationally expensive correlation matrix, they recommended using cosine similarity for all pairs of Jacobians.\nPJS: computes the Jacobian in Parameter space $\\frac{\\partial f} {\\partial W}$ instead of wrt the input datapoint $\\frac{\\partial f} {\\partial x_{i}}$\nWPJS: While computing PJS, higher weights are assigned to the last few layers\nWPJSI: While computing PJS, higher weights are assigned to the first few layers\n\nQuestions:\n\n1. Are the images in a batch formed by repeatedly augmenting an image using cut-out ? The first-few ones which rely on Jacobian wrt datapoint at least require that? If so, then can you empirically show that the quality of ranking does not depend on the image being used\n2. For Jacobians wrt parameters, can you empirically show that the ranking is robust to various initializations?\n3.  Could you also provide the Kendall Tau scores for correlation between the rankings using each metric vs that using the validation accuracy in a table? Also the monotonicity for the ranking of architecture above the median score seem to have dropped drastically. Are these metrics effective enough to rank networks whose performance is close to each other?\n\nWhile the paper is a step in the right direction to reduce the computational budget, the effectiveness of these metrics is still questionable. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Improving Zero-Shot Neural Architecture Search with Parameters Scoring\"",
            "review": "The authors consider the case of zero-shot neural architecture search. In this task, a function of the network is computed before training, and the objective is to determine whether this score is a consistent predictor of good performance after training.\n\nThe strengths of the paper are:\n- The field of application is interesting, and there are very few other papers on that topic (to the best of my knowledge).\n- The paper is easy to follow.\n- Some of the scores introduced by the authors achieve good performance with the chosen metric, and a detailed comparison is performed.\n\nThe negative aspects:\n- Each score proposed is justified, but it feels as if there is no underlying unifying theory for the work. It's definitely interesting to see if different heuristics result in good downstream performance, but I feel a stronger paper would try to get a more important theoretical justification rather than a list of motivated scores, or at least an intuition.\n- Results on the small architectures in the NAS-benchmark set are nice, but the paper would benefit from a comparison with larger, more realistic architectures. I would insist on the fact that this point is not however a deal-breaker for me.\n\n(very) Minor point:\n- The format for references seems wrong. I'm not sure about that aspect (so it did not weigh in my evaluation of the paper) (\"(1)\" versus \"(name)\"). Please disregard if wrong, or correct if true.\n\nMy present recommendation is a borderline reject. I like the paper, but don't feel it is currently strong enough to be accepted. I'm open to changing my opinion but will wait for the author's response, and the discussion with other reviewers before considering doing so. To address my points, I would encourage the authors to discuss in more detail what is to be gained theoretically from the empirical results they found. I understand this point is open-ended and might be somewhat difficult to address.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": " Technical correctness and usefulness of the proposed add-ons need further justification. ",
            "review": "The paper addresses zero-shot neural architecture search (NAS), where the zero-shot NAS approach was first proposed in a closely related arXiv paper, cited as [4] and also under review for ICLR 2021. The main effort here is to extend the Jacobian score (JS) function in [4] to several variants and compare their NAS performance. While the improvements seem to be marginal, the presentation of the paper can be improved to better characterize the technical contributions of the proposed modifications over [4].\n\nMain concerns about the technical correctness and contributions of the proposed techniques are listed below.\n1. In JSC, page 3, it is claimed that the time complexity for calculating JSC is O(B \\log B) and should be significantly faster than O(B^3) as in the original JS. However, such speed-up has not been observed in the experimental results (Figure 3).\n2. In SNS, page 3, the noisy data are formed by adding noise in proportion to the variance rather than the standard deviation.  Could the authors explain the reasoning?\n3. It is hard to comprehend the explanations and the indexing notations about the parameters-related score functions (PJS, page 3). Even more confusing is that the authors did not explicitly explain how the various parameters tensors are computed under the setting of zero-shot NAS, i.e., without any network training.\n4. The two weighing schemes (WPJS and WPJSI) are opposite to each other but give similar performance. What are the reasonable explanations here?\n5. The experimental results are not convincing.  First, the claimed time efficiency over JS has not been observed. Second, the validity of the experimental results from those parameters-related score functions need to be justified. Third, the usefulness of the monotonicity criterion is not evident from Figure 4 and the minor improvements in NAS. \n6. More detailed quantitative results and comparisons about the achieved accuracies from the yielded NAS architectures with respect to those by [4] and SOTA NAS techniques should be provided.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}