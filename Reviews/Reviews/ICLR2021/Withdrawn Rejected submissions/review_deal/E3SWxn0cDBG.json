{
    "Decision": "",
    "Reviews": [
        {
            "title": "Reviewer 2",
            "review": " \nThe paper shows the combined benefits of Polyak’s momentum with overparameterization in single-layer ReLU networks with finite-sum squared-loss.  Notably, the paper shows that momentum achieves an accelerated rate with an error controlled by the number of neurons and samples.\n \n \nStrengths:\n \nIt was not clear why momentum helped with neural network training, and this paper helps to shed light on that.\n \nThe related work is well-done and places the paper in the literature.\n \nThe writing in Section 1,2, and 4 is clear.\n \n \nWeaknesses:\n \nThere is no experimental validation for the theory.\n \nSection 3.2 to the conclusion is dense and difficult to parse.\n \n \nMy recommendation is to accept the paper with a 6, but I am not too familiar with work on overparameterization and I was unable to check the details of the proofs.  As such, my confidence is a 2. The paper shows reasoning why momentum may help training deep neural networks in the NTK regime, which may prove useful. However, no experimental results were presented to validate their results.\n \n \nThe following comments do not affect my score but may help improve the paper:\n \n“With infinite number of neurons” -> “with an infinite number of neurons”\n \n“Does not explain faster conference of momentum” -> “does not explain the faster convergence of momentum”\n \n“Discrete time” -> “discrete-time”, “best known” -> “best-known”, “second order” -> “second-order”, “continuous time” -> “continuous-time”, “mean field” -> “mean-field”\n \n“Produce exact same iterates” -> “produce the exact same iterates”\n \n“An resemblance” -> “a resemblance”\n \n“While obtain the same convergence” -> “while obtaining the same convergence”\n\n“We plot function value of” -> “we plot the function value of”\n \n“Provided the weights of neural net” -> “provided the weights of the neural net”\n \nThere are also various other grammatical points that could be easily fixed by passing the document through an automatic tool.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review for 'Provable Acceleration of Neural Net Training via Polyak's Momentum'",
            "review": "This work studies Polyak’s momentum in neural network training. In detail, the authors show that with sufficient wide two-layer neural network, the use of Polyak’s momentum can accelerate the convergence for the loss to achieve zero loss from $(1-\\eta\\lambda)^t$ to $(1-\\sqrt{\\eta\\lambda})^t$. The main idea of the proof is to use the fact that the activation patterns during neural network training will not change dramatically from neural tangent kernel (NTK) literature. \n\nHere are my detailed comments.\n- The main contribution of this work is to show Polyak’s momentum accelerates neural network training in the NTK regime. Although the authors claim that such an accelerated convergence rate is nontrivial, I still feel that it is pretty straightforward to obtain this result, given our understanding of NTK. Basically, NTK theory suggests that under the initialization scheme adopted in this work, a wide enough two-layer neural network can be regarded nearly as an overparameterized linear function in terms of the network parameter. Therefore, by adapting the existing results (Eq 6 and (Liu and Belkin, 2020)) about acceleration over an overparameterized linear system brought by momentum-based algorithms, we can directly conclude that the acceleration happens over overparameterized neural networks. To highlight the novelty of this work, the authors may want to highlight the technique difficulties among their proof, and demonstrate the key difference between an overparameterized two-layer neural network and an overparameterized linear system. \n- The authors suggest that by setting $\\eta = O(\\lambda/n^2)$, the convergence rate of Polyak’s momentum is $(1-\\lambda/n)^t$, which is faster than that $(1-\\lambda^2/n^2)^t$ of vanilla GD proposed in (Du et al., 2019b). However, (Wu et al., 2019a) suggests that the convergence rate of vanilla GD actually can be improved to $(1-\\lambda/\\|H^\\infty\\|_2)^t$, where $H$ is the Gram matrix defined in (3). With the fact that $\\|H^\\infty\\|_2 \\leq n$, the convergence rate of vanilla GD is actually $(1-\\lambda/n)^t$, the same as that of Polyak’s momentum proposed in this work. Therefore, it seems that Polyak’s momentum does not offer any advantages compared with vanilla GD. \n- Currently, this work does not have any experiments, and it is only for two-layer neural networks. Both of them limit the significance of this work. The authors may want to add some experiments and theoretical analysis for multi-layer neural networks to strengthen their work. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, well written, but moderate the claim (convergence up to a neighborhood)",
            "review": "This paper proves theoretical guarantees for training a one hidden layer relu neural network with gradient descent+Polyak's momentum, a common practice among practitioners. This problem was only studied from an asymptotic point of view, i.e. without convergence rate. More precisely, the main result (Theorem 2) states that the outputs of the network converge linearly up to a neighborhood to the labels on the training set. This assumes that the step size, the momentum parameter, the number of nodes of the network and the number of features are set accordingly. Moreover, the linear rate is accelerated compared to that of gradient descent, in the sense that is it (1-\\sqrt{\\kappa}) instead of (1-\\kappa). To my knowledge, proving theoretical guarantees for Polyak's momentum over GD is a difficult topic. Even for minimizing strongly convex functions, Polyak's momentum has provable advantage over gradient descent only in some particular cases.\n\n\n\nOverall, it is a good paper that proves that for reaching a neighborhood of the solution, Polyak is faster than GD (based on the upper bounds provided by Th1 and Th2). The topic (optimization for neural nets) is technical and fits well with ICLR. The paper also provides nice intuitions, based on explaining how Polyak accelerates for quadratic problems, to explain why it is true. More generally, the paper is well written (some minor typos below) and easy to follow. The main theorem is proven in the main paper and some intuitions are given, which I believe is a good thing for a theoretical paper. Hence, there are no numerical experiments (not necessary in my opinion).\nHowever, Th2 has some flaws even if it is a good entry point to understand neural networks training via Polyak's momentum. \n\n\nTheorem 2: First, the values of the parameters could be better explained. For instance, it is written \\eta = O(\\lambda/n^2) and later on \\eta \\lambda < 0.5. Moreover, there is a small discussion after the Th2 to explain the value of the parameters, but it can be improved. For instance, the authors could discuss the role of overparametrization. I don't see why the error is controlled by the number of neurons.\nMore importantly, Th2 proves convergence **up to a neighborhood** at an accelerated rate. In particular, the algorithm is not proven to converge to the labels. More discussion should be provided w.r.t. this point and the main claim of the paper should be moderated. On the contrary, gradient descent does converge (linearly). One way to answer this is to provide a complexity result: If I want to have error \\le \\varepsilon and I can set the parameters as I wish, what is the minimum number of iterations needed to achieve \\varepsilon accuracy. I think that this number of iterations will be higher than the one of GD. \n\n\n\n\n\nMINOR:\n\nCheck third line of paragraph 2.1 and first sentence of page 5\nTwo lines before Eq 10: Notation = undefined\nSection 3.1. Recall that S_i is a random set\nLemma 2: A_ir and S_i already defined\nThree lines after Eq 34: sum symbol over r is missing several times.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Meaningful but straightforward results, lack of techinical novelty",
            "review": "This paper theoretically proves that gradient descent with Polyak’s momentum achieves a faster convergence rate than vanilla GD, on sufficiently wide one-hidden-layer ReLU networks.\n\nComments:\n\n>The theory developed in the paper partially confirms the common belief among practitioners that Polyak’s momentum helps neural network training, which is an encouraging result. I say “partially confirms” because the provable acceleration is only on very wide over-parameterized neural networks, while it is still unclear for relatively narrow networks. \n\n> Given the knowledge/results obtained in the NTK literature, this result is straightforward and not surprising. As shown by prior NTK works (e.g., [Jacot et al 2018, Lee et al 2019, Liu et al 2020]), when the network is sufficiently wide, it is approximately a linear model, which is recently termed as “transition to linearity” by [Liu et al 2020]). As is already clear, Polyak’s momentum accelerates in the linear model setting. Hence, it is also expected that it accelerates in this “approximate” linear setting. Note that the work [Du et al 2018] is essentially the same as the NTK analysis.\n\n> The techniques used in the paper is a straightforward combination of Polyak’s analysis and the analysis in [Du et al 2018]. Since the wide networks are approximately linear, there is no technical difficulty to apply the Polyak’s analysis for linear models to the very wide networks.\n\n> The paper does not show acceleration of Polyak’s momentum on networks with practical sizes (the result is on very wide ones). This is inconsistent with the claim in the paper title.\n\nOverall, although the result is meaningful and should be respected, the theory is straightforward and lacks technical novelty. So, I don’t think this paper meets the standard of this conference.\n\nReferences:\n\n[Jacot et al 2018] Neural tangent kernel: Convergence and generalization in neural networks. NeurIPS 2018\n\n[Lee et al 2019] Wide neural networks of any depth evolve as linear models under gradient descent. NeurIPS 2019\n\n[Liu et al 2020] On the linearity of large non-linear models: when and why the tangent kernel is constant. https://arxiv.org/abs/2010.01092\n\n[Du et al 2018] Gradient descent provably optimizes over-parameterized neural networks.  https://arxiv.org/abs/1810.02054\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}