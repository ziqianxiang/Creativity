{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper studies the idea that subsequent neural network layers are interpreting their layer input as coming from a perturbed version of the original input creating an effect similar to data augmentation. This is done by analyzing inverse problem solutions at different layers with networks of different iterations. The authors show that inverse problems can still be solved when using using outputs from previous iterations with parameters of the current iterations.\n\nExperiments:\nThe main observation seems to be studied largely visually and in a very specific setting. How does the phase of training, learning rate, etc affect the result?\n\nThe argument about it being an augmentation seems largely based on the visualizations. It seems more likely to me that the “augmentation” is just the effect of it being harder to “invert” more distant iterations.  \n\nThe optimization and specifically  learning rate seems like it would affect the results, representations being less likely to change with smaller learning rates. The learning rate used for imagenet for example seems to be lower than the normal one, it would be interesting to understand the effect of this. \n\nRelated work:\nThere are a large body of recent work on invertible neural networks (e.g. iRevNet and follow up work). It seems this should be discussed in this work.\n\nClarity:\nThe paper seems unpolished and is rather hard to read due to this. There is a lot of imprecise language mixed with math notations making it hard to follow.  There are multiple one sentence paragraphs, an empty appendix, etc. I think also a diagram would be helpful to convey the high level idea more quickly. \n\n\nOverall, the observation that one can still “invert” representations from previous iterations is interesting. However it’s not clear what the results really suggest or how they can be used, furthermore the presentation of the paper can be greatly improved.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need more clarification and empirical support",
            "review": "This paper studies the question that given a layer output $r^t(x_i)$ on image $x_i$, and the same layer $r^{t+p}(\\cdot)$ after $p$ more training iterations. whether there exists a \"reasonable\" image $x$, such that $r^{t+p}(x_i)\\approx r^{t}(x)$. It talked about the implications on implicit data augmentation if this holds. It also shows empirically that for relatively small $p$, this generally holds for lower layers, but not for higher layers.\n\nI find this paper not very convincing, and both the motivation and the experiments could be potentially improved.\n\nFirstly, this papers talks about implications on implicit data augmentation. I think this is a crucial piece of support that could make this paper interesting. But currently it is only vaguely mentioned. As a reader, after reading the paper, I did not get a clear impression of what implicit data augmentation is, why it could help me understand deep learning better, or how it could be used to improve training algorithms, etc. If  this part could be more sufficiently developed, then this paper will have a much stronger case to sell.\n\nAssuming this is an important question to study, the experiments also needs improvements and clarification:\n\n1. Can you describe your inversion procedure in more details? In particular, do you initialize $x$ from random or do you initialize it from $x_i$? If from random, what kind of random distribution? What kind of optimization do you run to solve Eq(2)? How many iterations do you run? \n\n2. Is there a definition of \"invertibility\"? In some places (e.g. the caption of Fig. 6) \"invertible\" is used, but it is not clear how this is quantified. Do you use some threshold in some metric and use it to quantify invertibility or do you look at the inverted image and decide by human eye whether or not it is a successful inversion?\n\n3. For those visualizations that did not produce meaningful inversions, is it because optimization of Eq(2) failed? Or the optimization actually converges to global minimizer but the minimizer is not visually meaningful?\n\n4. Intuitively it is clear that solving Eq(2) for a higher layer is more difficult than solving for a lower layer. How can you show that when you say the higher layers are not invertible, it is not due to the failure of the optimization? In other words, it could be that all the layers are invertible, but it takes more efforts (maybe very complicated optimization procedures) to invert for higher layers.\n\n5. One potential baseline experiment you can do to verify the difficulty of optimization due to layer depth (see 4. above) is to run the same experiment as in, for example, Fig.2, but with very small $p$. For example, for $p=1$ or even for $p=0$. In the case $p=0$, you know that the image is invertible by definition. Could your optimization set up find the inverted solution for layers of all the depths? (assuming you are not initializing $x$ with $x_i$) If not, then the optimization is probably a bigger issue than (mathematical) invertibility. If yes, then at lease you have some *partial* evidence that the optimization procedure is powerful enough to verify the invertibility at all layer depths.\n\nFinally, I believe there are many more recent studies on inverting hidden layer representations since Mahendran & Vedaldi (2015) cited in this paper. Maybe it is good to have a related work section to do a more thorough discussion with more recent work on this topic as well.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper considers the training of deep networks. It makes the observation that each layer (or a set of layers between an intermediate representation and the loss) is trained in a way that does not depend on how this intermediate representation has been generated. The paper then considers the inversion of a representation using the model at a different training iteration, arguing that the difference between the models at different iteration leads to a sort of data augmentation in the input space. They show examples of such reconstructed augmentations, for different architectures, datasets, model sizes, and layers.\n\nWhile one of the findings of the paper (that the sub-model between a representation and the loss does not depend in its training on how the representation was generated) is correct, it is well known. It was also proven in Sec. 2.3 in a way that could be simplified without invoking gradients, simply by observing that the loss is a function of $r_l$ and $\\\\{w_{t}\\\\}_{t>l}$. \n\nIn Sec. 3, the question is raised of whether the activations for an image $x_i$ at a given time could be produced by the perturbation of this image at an earlier time. This seems like the wrong question to ask. Given that we are at time $t+p$, would it not make more sense to ask whether some activations that the trainer has already seen at an earlier time $t$ could be interpreted as the activations produced at the current time $t+p$ for an augmented image? In other words, considering the model at time $t+p$ as frozen, we could think of all the activations seen up to that point as being generated by that model. In terms of the inversion being solved, this would change Eq. (2) to $||r_l^{t+p}(x) - r_l^t(x_i)||^2$.\n\nAll the results in the paper seem qualitative, with the exception of Fig. 6. For the latter, I could not figure out what it demonstrates -- perhaps partly because the scatter plots lack axis labels. The paper alludes to the invertibility in several places, but it is not defined what that means. I suspect that the invertibility refers to the reconstructed perturbed images looking like the original images. While this is interesting visually, it is not clear what should be inferred from this in terms of either theory or prescriptions.\n\nOne potential issue with relying on the reconstructed images, incidentally, is that it relies on the reconstruction being unique, and the optimizer converging to it. I do not know whether these conditions hold or not, and the paper does not discuss them.\n\nOne minor comment is that Eq. (3) and the allusion to the probability measure distances seems superfluous as it has no connection to what the paper does, which is computing the activation differences in Eq. (4). The latter seems like a reasonable thing to do (without Eq. (3) as justification) but as the paper has found is not enough to predict the visual invertibility, perhaps because it does not take into account $\\partial{r_l}/\\partial{x}$.\n\nThe paper asks an interesting question, which is whether the training of each layer on a sequence of representations produced by a changing model acts as data augmentation. However, the results, being purely qualitative, are of limited benefit.\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, needs more extensive verification",
            "review": "This paper explores the connection between the invertibility of image representations and generalization in the context of convolutional neural networks. In particular, authors investigate how the invertibility of certain layers change accross iterations and argue that models with stable representations benefit from implicit data augmentation, and thus improved generalization. They examine how network architecture and capacity impacts layer invertibility and support their findings with numerical experiments.\n\nPositives:\n+ The idea of implicit data augmentation impacted by network architecture is exciting and novel.\n+ The problem is well-motivated. Demystifying the success of over-parameterized neural networks is important both from a purely theoretical and an empirical standpoint.\n\nNegatives:\n- The paper seems a bit speculative without really backing up the claims with extensive experiments or some theoretical framework.\n- The impact of the paper is somewhat limited. It is not discussed in the paper how the findings can be utilized or what broader consequences they have.\n\nEven though the fundamental idea of the paper is interesting, the whole paper feels a bit speculative. Authors could better articulate what they claim and back it up more substantially. Therefore, I am leaning towards recommending rejecting the paper.\n\nMy main concerns are as follows.\nFirst, the experiments are insufficient to back the claims. Authors claim that longer invertibility corresponds to more implicit data augmentation resulting in better generalization. They observe that ResNet learns more stable representations in the bottom layers than AlexNet. It is questionable how comparable these two models are and how much we can conclude based on the single image presented in the paper. Furthermore, authors did not quantify layer invertibility, making the comparison more questionable (see next point).\n\nSecond, the notion of invertibility is not defined in a rigorous way in the paper. It is not clear based on what condition authors declare the inversion to be successful apart from the subjective idea of images look somewhat similar. Without a more rigorous formulation it is difficult to carefully compare the invertibility of models.\n\nThird, authors speculate that over-parametrization may help preserving invertibility for longer. This idea is interesting, however they only provide a single very simple experiment and therefore the verification is not too convincing.\n\nFinally, could the authors comment on what broader consequences the results have? Can these findings possibly be quantified in a way to describe how the effective training set size changes due to implicit data augmentation? Does it give us any novel clues in architectural choices?\n\nSome minor comments/questions:\n- Eq (2) should be properly written as an optimization problem.\n- There are some typos in Section 3 and Section 4. There is an empty Appendix at the end of the document.\n- Is there a particular reason why batch normalization is not used in the experiments?\n- Have the authors investigated how invertibility evolves later during training (e.g input activation function at epoch 10)?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}