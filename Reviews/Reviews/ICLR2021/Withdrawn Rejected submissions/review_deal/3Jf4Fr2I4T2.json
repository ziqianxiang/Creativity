{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official review AnonReviewer2",
            "review": "Summary:\nThe authors present a novel methodology to construct confidence regions for the outputs given by any Bayesian optimization algorithm with theoretical guarantees. As discussed in the paper, based on generic probability inequalities that may not be tight enough, the confidence regions may be somewhat conservative and the proposed method can be useful when a conservative approach is preferred, such as in reliability assessments. Finally, the authors also gives possible ideas for further improvement, including more accurate inequalities and better error bounds tailored to specific acquisition functions.\n\nMajor comments: \n- Overall, the paper is well written and the relationship to previous works is well described, I think it is a valuable contribution to the conference.\n- It is mentioned that the proposed method can be applied in some specific areas. I would recommend the authors to provide a concrete example in numerical experiments, so as to better demonstrate the practical value of the proposed method.\n\nMinor comments:\n- The size of Fig. 1 appears too small, which may hinder readability.\n\nAt the moment, I recommend a weak reject, but I could be open to increasing my score if my concerns are addressed.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Uncertainty Quantification for Bayesian Optimization",
            "review": "In my opinion, the paper is not well motivated. It is not clear to me under which conditions and/or for which applications this approach is useful and possibly superior to the final incumbent suggestion of traditional BayesOpt (e.g., global optimum of the GP posterior mean). A real-world application can add to the value of this paper.\n\nIn this work the budget is defined in terms of the number of iterations. However, according to the cubical scaling of the GPs and the challenges of estimating and globally optimizing the acquisition function, a majority of BayesOpt SOTA defines the budget in terms of wall-clock time. How does the proposed approach incorporate additional notions of budget such as wall time and parallel computing? Assuming a wall time budget, how does the proposed method extend to low-fidelity evaluations? the proposed confidence intervals seem to be affected by the fidelity of the observations.\n\nMoreover, the uncertainty estimation of the incumbent should be affected not only by the budget and the stopping criterion of each evaluation (while the early-stopping effect can be modeled through the observation noise in GPs), but also by the search space. Would the authors please comment on the connection. \n\nPlease elaborate on the significance parameter and how is it set in this work? \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Uncertainty quantification in Bayesian optimization using RKHS bounds",
            "review": "**(Summary)** \nThis paper studies uncertainty quantification of the location of the global optimum as well as the global optimal value. The authors give a brief introduction to Gaussian processes and Bayesian optimization and use the RKHS bounds (mostly from Wendland) to construct confidence intervals. They present a series of theorems and finally demonstrate their method on a toy example where the data is simulated from a GP.\n\n**(Strong points)**\n- The paper is well-written and generally reads well.\n- I believe the problem of uncertainty quantification on both f(x*) and x* is important and needs to be studied more carefully to allow us to for example build better stopping rules for Bayesian optimization.\n\n**(Weak points)**\n- My main concern is that the paper is mostly a combination of well-known results, mainly from Wendland.\n- The claim that C=1 is robust is neither demonstrated anywhere in the paper nor is it intuitive. Although not stated explicitly, C likely depends on the RKHS norm of f and will therefore be very function dependent and hard to estimate in general optimization settings.\n- The authors only provide one numerical experiment that uses toy data, which makes it impossible to judge if this method is practical or not. I would like to see a more thorough numerical experiment section that studies how well the method works. While the current result\n\n**(Motivation for score)**\n- As mentioned above, I think this is an interesting problem and this seems like a reasonable approach to solve the paper.  I think this paper still needs some more work and a more thorough investigation into its practical performance before being ready for publication. I encourage the authors to keep working on this and trying to improve the paper.\n\n**(Questions for the authors)**\n- Can you provide a thorough discussion of what the constant C depends on and show for popular test problems in Bayesian optimization (for example from here: http://www.sfu.ca/~ssurjano/optimization.html) how the performance depends on the value of C?\n- What would be the main challenges to extend these results to the setting where f(x) is observed with normally dependent noise?\n- What do the results look like for a very smooth kernel such as SE? The width of the confidence intervals shrinks nicely when the smoothness is increased (linearly), which is expected but still encouraging, and I wonder how much more they shrink when working with an infinitely smooth kernel with much nicer convergence properties.\n\n**(Additional feedback)**\n- Nit: The abstract should say “the underlying objective function is *often* modeled as a realization of a Gaussian process”.\n- I think that the paper could be structured better in order to emphasize the contributions of the authors. I would prefer removing the detailed description of different acquisition functions as it isn’t relevant to the paper (you only consider UCB in the experiment) and spend more time focusing on the theorems, which are the main contribution of the paper.\n- The plots are very hard to read in black and white as the markers are the same for each line. In addition, some lines overlap which makes it even harder to read the results. I suggest using different markers as well as making a comment in the caption about the lines overlapping. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of Uncertainty Quantification for Bayesian Optimization",
            "review": "In this paper the authors propose to consider uniform bounds for uncertainty quantification on the  maximum value and point of a Gaussian process. They are based on theoretical analysis, giving conservative bounds. A numerical approach to get tighter bounds is derived.\n\nWhile the paper is globally well written and clear, the comparison to the state of the art is missing very similar recent works, and a more balanced discussion of the limits would be beneficial.\n\nDetailed comments:\n\nThe approach is interesting but there are additional related works that should be discussed:\n- Lederer, A., Umlauft, J., & Hirche, S. (2019). Uniform error bounds for Gaussian process regression with application to safe control. In Advances in Neural Information Processing Systems (pp. 659-669).\n- González, Z. Dai, P. Hennig, and N. D. Lawrence, “Batch Bayesian Optimization via Local Penalization,” in Proceedings of the International Conference on Artificial Intelligence and Statistics, 2016, pp. 648–657.\n- Sjö, E., Confidence Regions for Local Maxima of Reconstructed Surfaces, Methodology and Computing in Applied Probability, Springer, 2001, 3, 145-159.\n- Haaland, B., Wang, W., & Maheshwari, V. (2018). A framework for controlling sources of inaccuracy in Gaussian process emulation of deterministic computer experiments. SIAM/ASA Journal on Uncertainty Quantification, 6(2), 497-521.\nIn particular, uniform bounds for GPs are considered in Lederer et al., hence this work is not the “first” to give results (even if here it is more focused on optimization). An extensive discussion and comparison of these existing bounds is thus needed.\n\nOn the state of the art on convergence analysis, note the recent work by: Vakili, S., Khezeli, K., & Picheny, V. (2020). On Information Gain and Regret Bounds in Gaussian Process Bandits. arXiv preprint arXiv:2009.06966.\n\nWhile on the estimation of the maximum (or its entropy) – hence more related to UQ, see e.g.,:\n- Wang, Z., & Jegelka, S. (2017, July). Max-value Entropy Search for Efficient Bayesian Optimization. In International Conference on Machine Learning (pp. 3627-3635).\n- Villemonteix, J.; Vazquez, E. & Walter, E. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, Springer, 2009, 44, 509-534.\n\nIt is important to present the limits of the GP assumption: it is convenient but limiting in practice due to the estimation of covariance hyperparameters and features hard to model by a GP (e.g., non stationarity). Other limits that are not discussed include the scalability of the proposed approach: only examples up to three variables are presented, with small budgets. In the experiments in Appendix F, even 500 designs in dimension 3 remains coarse – this could explain the surprising conclusion that maximum value are smaller in higher dimension. \n\nFinally, for the sake of presentation it would be interesting to illustrate the confidence intervals found (CI and CR) on one or two dimensional examples.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}