{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new hard attention model for the image classification as a way to achieve explainability. Two of the reviewers do not find the output of the system interpretable, which is a fatal weakness for a paper on XAI.\nR1: The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge.\nR2: However the output of the system is not so appealing either in performance or explainability.\nR3: Post-discussion note: For me, it's a bit hard to say the proposed methodology is novel. Authors needs to explain why the proposed model is different from pre-existing methodologies regarding attention mechanism.  \nR4: Due to the above, the recommendation is Reject - but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline.\n"
    },
    "Reviews": [
        {
            "title": "Review of \"Achieving Explainability in a Visual Hard Attention Model through Content Prediction\"",
            "review": "This paper presents a visual hard-attention image classification model. The difference to standard classification methods such as CNN is that the model provides an explainable inner structure by default, that can be inspected to see what the model focused on. The difference to other state-or-the-art hard-attention models is that this model is differentiable, allowing for more robust and stable optimization.\n\nOn a positive note, the presented method is sound and mathematically principled, and the description of it is complete and technically correct. The paper is also well written, well organized, and easy to read. The relevant related work is cited.\n\nHowever, the paper suffers from two major flaws. \nFirstly, the contribution of the proposed method with respect to other recent hard-attention models based on reinforcement learning it is not well motivated - other than that this model is differentiable. The last paragraph in the Related Work provide no statement whatsoever as to what the present method contributes over the latest methods in the literature. \nSecondly, the baseline hard-attention model in the experiments, (Mnih et al. 2014), is very old and it is not surprising that the proposed method outperforms it. A more interesting baseline would be a later hard-attention model such as (Elsayed et al. 2019). Moreover, the used datasets are all quite simplistic, and it would be more interesting with a more realistic one.\n\nDue to the above, the recommendation is Reject - but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An overly complicated system for explainable image classification with hard attentions",
            "review": "\n**Summary:**\n \nThis paper follows a less explored strategy for achieving explainability via hard attention. They proposed a recurrent architecture which sequentially observe regions (glimpse) from an image. To decide where to look next, the model maintains a hidden state and use it to estimate the full image (or features of the image). This \"content prediction\" module allows the model to look ahead and make a decision based on the expected information gain (EIG) over different locations. The objective function (i.e. partial VAE loss and classification loss) in this system is differentiable thus the system can be trained with gradient descent. The authors validated the system on several benchmarks and show comparable performance with baselines.\n\n**Reasons for score:**\n\nThe system seems extremely complicated to me, as it involves multiple components and each component by themselves is very complex. However the output of the system is not so appealing either in performance or explainability. Probably I missed something but I don't quite understand the advantages of the proposed system.\n\n**Pros:**\n\n1. The core idea for training the attention policy is intuitive, as the information gain is a natural choice for determining the location of next glimpse. It's also appealing that the predicted content can provide sufficient signals to estimate the information gain.\n2. The equation is clear and makes the paper easy to follow.\n3. The careful analysis of the experiments is very informative.\n\n**Cons:**\n\n1. In general, my biggest concern about this paper is the complexity of the EXPAC system with the moderate performance. It seems that all benchmark datasets used in the paper are not so difficult (e.g., 10-way classification with 32x32 images) and the performance of the proposed system is still far from satisfactory. Therefore, it's questionable whether the system could be scaled to even more challenging (but more practical) datasets like ImageNet. Also I'm curious about the robustness of training such an intricate system. \n2. Regarding explainability, there are quite a few methods (not limited to hard attention) that target the same goal, such as Grad-CAM. However this paper seems to only compare with RAM (and a gist-initialized variation of GRAM). Even if the core algorithm might be different, it's still good to compare with other designs. \n(Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" Proceedings of the IEEE international conference on computer vision. 2017.)\n3. It seems to me that the content prediction module ($S$ and $D$) is critical as the predicted $\\tilde{x}$ is used for \"lookahead\" to estimate the next $l$. However the predicted $\\tilde{x}$ is never shown in the paper. I think it would be more straightforward to  better understand the performance of the system by comparing the predicted content with the original.\n\n\n**Questions during rebuttal period:**\n\nPlease address my questions in the cons section.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Review on \"Achieving explainability in a visual hard attention model through content prediction\"",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposed a new hard attention model for the image classification. They designed hard attention mechanism as a bayesian optimal experimental setting. Compare to other hard attention model, the policies of proposed hard attention can be explainable and differentiable, which is non-parametric. They evaluated their model to four different image classification dataset and their model outperformed than other baseline models.\n\n##########################################################################\n\nReasons for score: \n\nEven though this is an interesting setting and the technical solutions presented in the paper look reasonable, the idea seems to be pretty incremental as it stacks multiple existing techniques without many innovations. \n\n##########################################################################\n\nPros: \n\nThe proposed hard attention model finds an optimal location using partial variational auto-encoder. Their attention policy is non-parametric and explainable. They validated their model on four different datasets with qualitatively analyzed results.\n\n##########################################################################\n\nCons: \n\n1. There’s needs for more throughly designed experimental settings with more datasets.\n2. Authors need to perform ablation studies with other form of attention mechanisms. \n\n##########################################################################\n\nQuestions during rebuttal period: \n \nFor me, it's a bit hard to say the proposed methodology is novel. \nAuthors needs to explain why the proposed model is different from pre-existing methodologies regarding attention mechanism.\n \n#########################################################################",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "paper can be better organized",
            "review": "This paper proposes an alternative way to conduct hard attention. Specifically, it estimates the expected information gain (EIG) of attending various regions and at each timestep chooses the region with the maximum EIG. The proposed method is tested on image classification.\n\nIssues:\n1. what do you mean \"achieving explainability\" in the title? I don't quite get it. The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge. \n2. why the proposed method needs three building blocks is not well explained, especially the partial VAE. \n3. what is the data used to train the  partial VAE?\n4. no ablation study is conducted on these building blocks, such as the normalizing flow module in the partial VAE.\n5. I think image classification is not the best task to demonstrate the effectiveness of hard attention method. As shown in the experiments, simply using a standard CNN that takes the whole image as the input obtains the best result.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}