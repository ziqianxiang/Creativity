{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work presents a new theoretically motivated data augmentation technique. Reviewers agreed that the theory was interesting and has value, however raised concerns regarding the experimental evaluation which was limited to the Cifar datasets. There was some discussion over whether or not a comparison with AutoAugment would be fair, the proposed method is theoretically motivated whereas AutoAugment takes significant compute to train. I agree with the authors that if the method doesn't outperform AutoAugment on CIFAR, this would not necessarily invalidate their results. Nonetheless the work would be significantly strengthened if it included results. on additional datasets to stress test the theory. I recommend the authors add additional supporting evidence and resubmit."
    },
    "Reviews": [
        {
            "title": "A theoretical paper with a few implementation suggestions ",
            "review": "##########################################################################\n\nSummary:\n\n\nIn this work, the authors first prove a deep model can benefit from augmented data when the data bias is small. Then they propose two methods, namely \"AugDrop\" that corrects  and \"MixLoss\", that correct data bias by \"constrained optimisation\" and \"modified loss function\" respectively. Finally they show that these two methods can be combined and further improve the performance.\n\n##########################################################################\n\nReasons for score:\n\n\nI'm not an expert in the area of data augmentation, and I didn't check all the proofs and derivations in this paper. If I understand it correctly, the realisation of \"AugDrop\" is to omit data augmentation and train the model on original data, and the realisation of \"MixLoss\" is to train the model on both augmented and original data. The combined method, namely, \"WeMix\" is to run \"MixLoss\" first and then run \"AugDrop\". Overall, this paper is hard for me to follow. Yet another concern is that, the method was tested on CIFAR-10 and CIFAR-100, with \"mixup\" as the only competitor.\n\n##########################################################################\n\nPros:\n\n1. The paper tried to answer an important question \"when will the deep models benefit from data augmentation?\" They proved (though I didn't check the proof) that \"a deep model can benefit from augmented data when the data bias is small\". Based on this, they developed two methods \"AugDrop\" and \"MixLoss\".\n\n2. The proposed method seems to be very easy to implement.\n\n##########################################################################\n\nCons:\n\n\n1. I'm not convinced by the current experiments: (i) it runs on CIFAR-10 and CIFAR-100 only (ii) the only competitor is \"mix-up\".\n\n2. It wasn't quite clear (at least for me) why \"AugDrop\" and \"MixLoss\" can correct the data bias. Actually I have been lost at how the authors quantify the data bias.\n\n##########################################################################",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "WeMix help train with augmented data ",
            "review": "This paper illustrated two algorithms that correct bias in data augmentation: AugDrop and MixLoss. The authors illustrate their algorithms from a theoretical perspective and have mathematical proof the correctness of their algorithms. \n\nThe paper tackled the common problem on how to train with augmented data. it also half-opened a black-box of the training schemes. Authors' initial motivation can help the field significantly.\n\nThe concern I have is that if their method could be applied to a large range of applications since their experiment only based on some typical benchmarks, which already shows a good result in this field.\n\nBut generally, this paper is interesting and acceptable.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors propose a method to better utilize data augmentations especially when the bias between the original and augmented data distribution is large.",
            "review": "This paper proposes a method to improve the generalization of deep networks when the input is applied strong augmentations, i.e. mixup, resulting in large data bias. The authors proposes theoretical foundation behind their two methods, AugDrop and MixLoss and show their effectiveness on CIFAR10/CIFAR100 datasets. Although the paper has technical novelty and it only provides incremental gains on relatively small-size dataset. Below, you can find some of my comments/questions about the paper.\n\n1. The paper has some typos and grammar mistakes. Some of them are \"By contrast, we do not need original data is unlimited.\", \"it is sufficiently to optimize Lc(w)\"\n\n2. I feel like the notations used in the introduction is repeated in the beginning of the section 3. There is no need to repeat the same explanations and notations in section 3.\n\n3. I am not sure about the definition of data bias mentioned in the paper. The bias is defined as the difference between the label of the augmented instance and original instance. In reality, there might be strong bias in the label-preserving augmentations, i.e. Jigsaw. I would like to get a clarification on this.\n\n4. The paper only shows experiments only on CIFAR10 and CIFAR100. Potential positive results on ImageNet would have made the paper much convincing and stronger. This is because, in the past, many augmentation methods, i.e. CutOut, has been shown to perform well on CIFAR10/100 but not on ImageNet.\n\n5. The improvements on CIFAR10/CIFAR100 are very marginal. There exists better data augmentation techniques, i.e. auto-augment, yielding better results on CIFAR10/100. It would be nice to include the results from different augmentation techniques into the experiments sections.\n\n6. Similar to my point at 3, do the authors think that their method would work on an another augmentation method, i.e. jigsaw, that preserves the label? Or is it only constrained for the ones that changes the label.\n\n7. What are the disadvantages of the proposed methods? What kind of complexity do they add, i.e. increased training complexity?\n\n8. I understand that the authors try to show that in the case of very strong augmentations (mixup with 10 images), they can improve the baseline mixup augmentation. However, their performance on the mixup with 10 images is worse than the mixup with 3 images. Then, what exactly is the point of using very strong augmentations if we can not benefit from them?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting algorithms + theoretical analysis but weak experimental validation",
            "review": "This paper theoretically analyzes the effect of bias in augmented data on the efficacy of data augmentation approaches, and proposes three new approaches to leveraging data augmentation in ways that purport to limit the negative effects of data bias in the augmented data distribution, and both theoretically and experimentally analyze them.  The algorithmic approaches and analyses are interesting, but (A) the theoretical results don't deliver a lot of specific actionable insight beyond the high level intuition they start with, and (B) the experimental section is somewhat weak.\n\nIn the main analysis (Sec. 4), the authors establish a bound (Lemma 1) on the generalization error due to training on augmented data (nit: unclear why $w_{t+1}$ is used to denote the final (?) params learned...?) and compare with a standard bound training on normal data.  However, the bound in Lemma 1 is not overly helpful (quadratic dependence on max augmented label divergence- unclear how this helps / is actionable), and the comparison with the normal bound doesn't reveal much but the original intuition that this approach *could* easily introduce bias, as we started with.\n\nThe paper then proposes two algorithms- \"AugDrop\", which uses a constrained optimization approach, and a corresponding theoretical analysis that shows potentially less biased and better performance than training on normal augmented data- and \"MixLoss\".  They also propose combining the two into \"WeMix\".\n\nAt this point, the main issue is with the experiments.  The authors could potentially have tried to fulfill two main objectives here: (A) make this mostly a theory paper, and seek to explore the quantities and tradeoffs of interest in the theoretical analysis in some experiments; and/or (B) show the practical efficacy of the proposed algorithms by running them with real data augmentation approaches, compared against modern approaches.  However, the experiments section accomplishes neither.\n- Re (A): There is no detailed experimental analysis beyond comparing end performance of models trained with the proposed approaches.\n- Re (B): The authors only use very rudimentary data augmentation techniques- mixup and contrast- and only compare to simple and internal baselines, even though there is a rich literature of recent data augmentation approaches that perform much better (e.g. even on the leaderboards recently: https://paperswithcode.com/sota/image-classification-on-cifar-100).\n- Again to be clear: this reviewer is *not* saying that all papers need to compare with SOTA.  But if the goal is not to show strong real world performance / applicability as in (B), then the experiments section should at least accomplish something about elucidating core theoretical tradeoffs, insights, etc. as in (A).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}