{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a method of risk-sensitive multi-agent reinforcement learning in cooperative settings.  The proposed method introduces several new ideas, but they are not theoretically well founded, which has caused many confusions among the reviewers.  Although some of the confusions are resolved through discussion, there remain major concerns about the validity of the method.  "
    },
    "Reviews": [
        {
            "title": "Downgraded distributional reinforcement learning for multi-agent systems",
            "review": "This paper proposes a new value-based method using risk measures in cooperative multi-agent reinforcement learning. The authors propose a new network structure that calculates global CVaR through individual distribution and learns risk-sensitized multi-agent policies. The authors also propose a new dynamic risk level prediction method that can dynamically adjust the risk level according to the agent’s observation and action. Applying risk-sensitive reinforcement learning in multi-agent reinforcement learning is interesting, but several points can be improved.\n\nThis paper has a fundamental difference compared to the distributional reinforcement learning recently studied in single-agent reinforcement learning like IQN. In single-agent reinforcement learning, the distribution of Q-function is first learned through the distributional Bellman operator, and the learned distribution can be utilized in additional applications such as risk-sensitive reinforcement learning. Even if the mean value of the distribution is used without considering the risk, it shows higher performance than the existing reinforcement learning algorithms without distribution. The distributional Bellman operator has a richer training signal than the Bellman operator for the existing scalar Q-function, which enables fast representation learning. Moreover, by changing the sampling distribution through the learned distribution, risk-sensitive reinforcement learning can be easily applied to multiple risk measures.\n\nHowever, in this paper, the authors do not use distribution as a direct learning objective but only aim to maximize CVaR. Unlike the distributional bellman operator, this learning method cannot be expected to improve training speed, and it is difficult to understand the meaning of the learned return distribution learned with the scalar loss function.\n\nAlso, this paper proposes a dynamic risk level prediction, but this part is confused. The authors argue that this method solves time-consistency issues and unstable learning. However, there is an insufficient explanation as to why this problem occurred and how to solve it. There is also a lack of explanation on why risk level is divided into K steps and why alpha is defined as in equation (3). Finally, detailed analyses are needed on how dynamic risk level prediction affects performance.\n\nThere is a paper, which the authors must cite and compare to. This is not the same as RMIX, but close enough that it has to be compared. \n\nHu et al. (2020).  QR-MIX: Distributional Value Function Factorisation for Cooperative Multi-Agent Reinforcement Learning. URL:https://arxiv.org/pdf/2009.04197.pdf ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theoretical aspects could be stronger, but clear demonstration of the practical merits in ambitious experiments.",
            "review": "\nStrengths:\n\n1) The paper is well-written, and versed in the pulse of related works on the topic. This makes it very easy to assess the points of differentiation of the formulation/focus from prior art. Especially the problem class of MARL with risk measures is salient.\n\n2)  The authors clearly demonstrate the practical merits of the proposed techniques in ambitious experiments. This is a major upshot of the work: one may clearly see across challenging domains the performance gains achieved by RMIX.\n\n3) Solving Dec-POMDPs is in general intractable, and one must often resort to particle filtering/belief representations of the state. Combining these technical challenges with risk sensitive utilities is nontrivial, and the authors have made a bold effort to obtain a tractable algorithm despite these issues. Importantly, their work has some conceptual justification.\n\n\n\nWeaknesses:\n\n\n1) While the conceptual setup and theoretical contributions are clear, the actual training mechanism is given very little explanation. There should at least be an iterative update scheme or pseudo-code presenting the key algorithm of this work. This would serve to make it easier to distinguish what are the unique attributes to the algorithm put forth in this work outside of a contextual discussion and at a more granular algorithmic level. By reading the paper it is very difficult to understand what information agents must exchange during training.\n\n2) What is the purpose of the generalized return PDF? How does the information required for its estimation get processed algorithmically, and can each agent estimate its component of it with local information only? This is not easy to discern.\n\n3) Theorem 1 must somehow assume that each agent's marginal utility is concave and that the joint utility is jointly concave in agents' local policies, but a discussion of this seems missing. Without concavity, then one can only ensure that agents' policies in the decentralized setting converge to stationarity, and that these stationary points belong to the set of extrema of the global utility. It may be the case that the fact that the risk-sensitive Bellman operator being a contraction is enough to mitigate these subtleties and ensure convergence to a global extrema, but this is not discussed. \n\n4) Just defining the TD loss in eqn. (4) is not enough because the presence of a risk measure means that a vanilla TD step in terms of approximating an expected value does not hold. How does the `````blocking to avoid\nchanging the weights of the agents’ network from the dynamic risk level predictor\" change the TD population objective? Under normal circumstances, this would be the Bellman optimality operator -- how does C^{tot} mitigate this issue, and what relationship does this hold to the notions of risk-sensitive Bellman equations in\n\nAndrzej Ruszczy´nski. Risk-averse dynamic programming for markov decision processes. Mathematical\nProgramming, 125(2):235–261, 2010.\n\nSee also:\n\nKose, U., & Ruszczynski, A. (2020). Risk-Averse Learning by Temporal Difference Methods. arXiv preprint arXiv:2003.00780.\n\n5) In general, a discussion of the technical innovations required to establish the theorems is absent. Are these theorems inherited from the algorithms appearing in earlier work? What is new?\n\nMinor Comments:\n\n1) References missing regarding risk-sensitive RL:\n\nZhang, Junyu, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. \"Cautious Reinforcement Learning via Distributional Risk in the Dual Domain.\" arXiv preprint arXiv:2002.12475 (2020).\n\n2) The link between equation (2) and estimating a conditional expectation should be made more explicit.\n\n3) The meaning and interpretation of IGM is ambiguous. More effort needs to be expended to properly explain it. The idea seems to be that individual agents' optimal policy is equivalent to an oracle/meta-agent that aggregates information over the network. This notion is then key to interpreting the importance of Theorem 1.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Important issue for MARL, yet lack of clarity",
            "review": "Overall\nThe paper considers the cooperative multiagent MARL setting where agents have partial observability and share a team reward. Instead of optimizing the expected cumulative reward, the paper considers optimizing the CVaR measure. The proposed method relies on CTDE and features a dynamic risk-level predictor. Although CVaR optimization is an important problem for MARL and the experimental results seem to be convincing, the formulation of the problem and the description of the proposed method are not written with enough clarity. Specifically, I have the following comments/questions. \n \n \nComments/Questions\n \n1. For CVaR optimization, is the risk level (i.e. alpha) given as part of the problem itself? If it is given, what’s the intuition behind the “dynamic risk level predictor” that handles “the temporal nature of stochastic outcomes”. What is “the temporal nature”?\n \n2. It’s known that, for a discrete random variable, its pdf is composed by Dirac functions weighted by the pmf. Is Definition 1 repeating this?\n \n3. I have difficulty understanding Equations (1) and (2). In (1), what’s the meaning of delta(tau_i, u_i)? Isn’t that the Dirac function takes a number as its argument? Why can the Dirac function take a trajectory as its argument? For (2), why does the estimate not depend on P_j that appears in (1)?\n \n4. In Theorem 1, what is the definition of IGM, and the definition of C^tot? If the reward is shared, why would the local CVaR be different from the global CVaR?\n \n \nAlthough the paper considers an important optimization objective for MARL, I don’t think the presentation is ready for publication yet.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Reviews",
            "review": "The authors propose RMIX to deal with the randomness of rewards and the uncertainty in environments. RMIX learns the individual value distributions of each agent and uses a predictor to calculate the dynamic risk level. Given the individual value distribution and the risk level, a CVaR operator outputs the C value for execution. For training, the $C$ values are mixed as $C^{tot}$ and updated by TD error end-to-end. RMIX outperforms a series of value decomposition baselines on many challenging StarCraft II tasks. The paper is very clear and well-structured. Expanding value decomposition methods to the risk-sensitive field is a novel idea, and it shows competitive performance in empirical studies. \n\nHowever, my main concern is the definition of the risk level $\\alpha$. More in-depth analysis is expected to interpret why the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions could reflect the risk level. Since the embedding parameters $femb$ and $\\phi$ are trained end-to-end by TD-error, the real meaning of $\\alpha$ is unknowable. Eq. 3 is a little confusing, and I get the left side of Eq. 3 should be $p_{\\alpha_i^k}$, not $\\alpha_i^k$. It is hard to understand how to get the final $\\alpha_i$ from the K-range probability. Figure 12 helps a lot in this understanding, and I expect it to be more detailed and put on the main pages.\n\nRMIX is built on QMIX. However, the individual value function in QMIX does not estimate a real expected return, and the value has no meaning. Is the theoretical analysis of the risk still valid in QMIX？\n\nRMIX is proposed for the randomness of rewards and the uncertainty in environments. However, I think they are not usually observed in the environment StarCraft II, since the policies of the enemy are fixed. Increasing the randomness and uncertainty could verify the advantages of RMIX.\n\n----------Update after author response----------\n\nI thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}