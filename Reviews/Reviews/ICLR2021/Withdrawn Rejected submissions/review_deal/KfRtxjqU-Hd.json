{
    "Decision": "",
    "Reviews": [
        {
            "title": "GNN propagation scheme, but difficult to read",
            "review": "This paper presents a modified graph neural network propagation scheme.  The scheme is intended to address known issues of over-fitting and over-smoothing.  The novelty of the approach lies primarily in introducing a non-differentiable discontinuous selection function to determine whether a particular neighbor may propagate information at some level of the network.  The experimental results of this technique appear to be solid.  However, I found the paper very difficult to read and I still don't have a full picture of the operation of the system, partly because of issues in the writing, and partly because the overall processing of the network was not presented as a single sequence of equations but was instead presented in pieces that don't connect clearly.  For example, the authors might introduce the selection function $s_i$ in Equation 4, but then don't use the output of this function in Equations 5-9, which show the remaining operation of the system.  Hence, I have a rough sense of the overall operation of the system, but I don't have good confidence in my understanding.\n\nMy assessment is as follows.  The novelty of the contribution lies mainly in the careful selection of neighbors to propagate, and the remaining attention structure of propagation of growing neighborhoods from the initial set of propagating nodes $\\cal V^*$.  This is a plausible contribution, but does not represent a significant advance in the area.  The experimental results seem quite strong on well-studied datasets.  However, the difficulties in presentation are significant enough that I feel the paper is not yet ready for acceptance.\n\nMy remaining comments contain a few observations and suggestions on the presentation.\n\nThe introduction here gave me an idea of graph NNs, and the issues of overfitting and oversmoothing, but among the papers I read it also gave me the least understanding of the contribution.  There were a number of claims that felt a little open-ended, like saying the paper proposes a novel architecture that reflects \"the functioning of the brain and the principles of social networks.\"\n\nIn equation 4, is there any constraint on $||W_1||$?  If not, I don't see why $T$ would matter, as scaling up $W_1$ would cause the input to the sigmoid to be scaled correspondingly, altering the effective value of $T$?\n\nSome concrete examples of things that were a little hard to follow:\n\nExample 1: \n\nIn order to do a classification on $\\cal G$, a given GNN needs to learn a state embedding $h$ which includes the neighborhood information of each node.  This resulting $F$-dimensional embedding can be expressed in terms of a function $f$ such that: $h_i = f(x_i, \\{x_j}_{j\\in $\\cal N$(i)\\}$, where $h)i$ and $\\cal N(i)$ denote the learned embedding\\ldots.  \n\nAt this point in the paper, I'm not sure if $h$ is a vector of node embeddings, or the embedding of a representative node.  I don't know if the \"classification on $\\cal G$\" means that the problem is to classify the entire graph, or based on the following, is supposed to classify a particular node $i$.  It's not terrible, just a little harder to follow than it should be.  I recommend that the authors do a pass for mathematical clarity.  \n\nExample 2:\n\nIt is worth nothing that this weight matrix $W_0$, along with all other learnable parameters defined below, pertain to a single layer.  for instance, the matrix $W_0$ refers to $W_0^{(l)}$.\" \n\n I'm not sure how to interpret this.  Is there a separate matrix for each $l$, and the superscript is omitted because it's clear from context?  Or are they shared?\n\nSmall typo:\n\nAfter equation 7: \"...do not need to aggregate their neighbors' information Chen et al..\"\n\nCommon issue:\n\nJust because it happens a lot, please ensure that punctuation is inside your displaymath delimiters to avoid things like $$x=4$$, where the comma is in the wrong place.\n\nFinally, if possible I think it would be very helpful to have a single sequence of equations in one place that trace the entire operation of the system from end to end in a single flow.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Technical contributions and experiments are insufficient, and the organization and writing are poor.",
            "review": "Summary:  \nIn this paper, the authors propose a new GNN variant termed NODE-SELECT. Specifically, the proposed method first selects a subset of important nodes and only passes messages on these nodes. Then, the neighbors of these nodes pass messages sequentially. The major claimed contribution of the proposed method seems to be alleviating the over-smoothing and over-fitting problem of GNNs. Experimental results and analyses are provided to compare the proposed method with baselines.\n\n(+) Over-smoothing and over-fitting is an important limitation of GNNs that attracts considerable research attention.   \n(+) The idea of passing messages sequentially rather than simultaneously is an interesting try (though not deeply investigated in the paper).  \n\nThe paper also has many major limitations.  \n1. The technical contribution of the proposed method is very limited. As implied in the paper, most components in the proposed method have been introduced in the literature. For example, aggregating different filters is similar to JK-Nets or multi-head attention in GAT; selecting nodes behaves similarly to attentions, DropEdge, and sampling (see [1-3]). The proposed method simply combines all these components, more or less, without explaining the rationale except the vague comparison with social science and neuroscience.  \n2. Following (1), it is unclear why the proposed method can help to alleviate over-smoothing and over-fitting besides the arguments already presented in DropEdge.  \n3. The experimental results are poor and unconvincing. Specifically, Table 1 does not show any clear advantage of the proposed method. I acknowledge that sometimes it is too harsh to ask a new method to outperform the existing methods on all datasets. However, it should be clearly analyzed under what circumstance the proposed method can indeed gain advantages. Without such analyses, it is unclear how the proposed method really contribute to SOTAs.  \n4. The experimental supports for alleviating over-smoothing and over-fitting are quite limited. For example, though some results using different numbers of layers are shown in Section 4, the main results in Section 3 only adopt 1 or 2 layers. Besides, no other baseline regarding over-smoothing and over-fitting is compared. I suggest the authors refer to related papers for proper experimental designs.  \n5. The writing and organization need to be greatly improved. For example, mixing introduction with related works makes it very difficult to capture what the paper actually tries to achieve. Some figures such as Fig. 4 are very unfriendly to readers. Most technical arguments are also hard to follow.   \n6. Many related works are missing. For example, besides DropEdge, many recent papers are tackling the over-smoothing problem [4-7]. The sample technical for training GNNs [1-3], which behaves similarly to selecting nodes, is also not mentioned in the paper.  \n\n[1] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling, ICLR 2018  \n[2] Stochastic Training of Graph Convolutional Networks with Variance Reduction, ICML 2018  \n[3] Adaptive Sampling Towards Fast Graph Representation Learning, NIPS 2018  \n[4] DeepGCNs: Can GCNs Go as Deep as CNNs? ICCV 2019  \n[5] PairNorm: Tackling Oversmoothing in GNNs, ICLR 2020  \n[6] Simple and Deep Graph Convolutional Networks, ICML 2020  \n[7] Towards Deeper Graph Neural Networks, KDD 2020  \n\nConsidering the above limitations, I am voting for rejection.  ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "my review",
            "review": "To address the over-smoothness issue as a consequence of deeply stacked GNN, an approach named NODE-SELECT is proposed to restrict a learnable proportion of the graph nodes from propagating their features, so as to force the learning process to be dependent only on the selected unrestricted nodes.\n\nIn the NODE-SELECT mechanism, a probability p_i  is defined in Eq (3),  by taking aggregation of its neighbors’ y_j.  This probability is claimed to be the filter’s confidence to include a node v_i in its subset of selected nodes. A strong justification is required for this statement. Why p_i is the filter’s confidence?  \n\nThe selection determined by a threshold T is quite tricky. When T is low to 0, the presented method is similar to GAT. By varying T, the presented method can select different nodes for participating the propagation. For a given graph, it is always tricky to find the best T. The experimental results in Table 1 showed that NSGNN does not always perform better than baselines. GAT performs also well on CiteSeer, Cora and Amz-C datasets, suggesting T should be a small value. \n\nComparing with the existing models e.g., GAT, the presented method doesn’t have substantial novelties and has tricky parameters to tune.   \nIn addition, the proposed method should be compared with the recent work (Chen et al 2020), cited in this paper. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}