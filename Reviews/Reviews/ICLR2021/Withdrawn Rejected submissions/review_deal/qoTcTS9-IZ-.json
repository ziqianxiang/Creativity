{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a general framework to study the limit behavior of neural models with respect to the scaling of hyperparameters in terms of network width, which covers existing mean-field (MF) and neural tangent kernel (NTK) limits, as well as other new limit models that were not discovered before. While the reviewers agree that the study of limiting behavior of neural network models is of great importance and could be a good addition to the current understanding of NTK and MF, there is a technical flaw in the proof (regarding Condition 1) pointed by the reviewer. After reviewer discussion, all reviewers agree that this is a serious issue, and needs to be addressed before publication. I believe it could be a strong paper if the technical flaw can be fixed. I encourage the authors to revise the paper and resubmit it to the next conference. "
    },
    "Reviews": [
        {
            "title": "Official Blind Review #1",
            "review": "Summary:\n\nMain topic of the paper is to study various infinite width limits. The paper notices different scaling used in NTK limit and MF limit and proposes a general framework for studying the limit behavior depending on the scaling. This allows the authors to define more general dynamically stable models in the infinite width limit. \n\nStated contribution by authors are:\n\n- Framework for reasoning about scaling that leads to ‘dynamically stable’ model evolution in the infinite width limit. \n- Categorization of 13 distinct dynamically stable models in the infinite width limit.\n- Characterization of “sym-default” model, which along with NTK and MF limit which shows properties most of finite-width network evolution. \n- Model modification “Initialization-corrected mean-field limit”(IC-MF) that satisfy all identified property of finite-width network evolution\n- Demonstration of IC-MF limit approximating finite-width network the best among all other possible models. \n \nReason for score:\n\nThe paper proposes an interesting theoretical framework to capture various infinite width limits. While settings are limited to make theory tractable, there is some empirical validation as well as capacity to broaden the study to different infinite width limits. I believe there are interesting novelties to be shared among ICLR participants who are interested in deep learning theory. \n\nPros:\nGeneral framework encapsulates widely studied infinite width limits and generalizes them which would be useful for theoretical study of large neural networks.\n\nThe paper makes predictions on scaling limits and finds a limit that could agree with finite networks better (IC-MF). For a variety of experiments the fact that this class matches with a reference finite width network is demonstrated well. \n\nWhile the proposed analysis is limited to a single hidden layer case, the authors have described thoughtful possibilities how the framework could potentially generalize to deeper networks. \nAuthors shared code to reproduce their experiments in the paper which is useful for reproduction and clearly understand different proposed scalings.\n\nCons:\n\nBeing single hidden layer analysis is a drawback (while discussion on extension is appreciated and stated in the “pros”). Often multi-layer infinite width is more interesting since the number of hidden layer weights scales quadratically while only the input and output layer weight count scales linearly with width. This can induce different training dynamics and scaling, so limitation to single hidden layer analysis is a drawback. \n\nTo simplify proof non-linearity of the choice is “leaky softplus” which is not widely used in practice. Having general comments on extension or limitation to well-known activations is advised. \n\nAssumption in p3 stating “gradients for a and W are estimated using independent data samples” may be too strong and not realistic. I cannot find no validation that the assumption is a reasonable one to make beyond making the theory simple. This may lead to inconsistencies raised in the “Questions” below. \n\nExperiments demonstrating similar effect for more realistic setting (e.g. full CIFAR-10 classification task achieving reasonable performance) would have been better to convince the validity or generalizability of the theory. While one can see that IC-MF and reference agrees well for the training curve, for test-set since every curve essentially behaves the same it is hard to determine which limit is superior or not. \n\nQuestions:\nIn [Liu et al., 2020], they show non-constancy of NTK for non-linear output including the soft-max layer in the outputs. I believe this fact may be in conflict with this paper’s analysis on binary cross-entropy loss and still having constant NTK. I do suspect it may be due to assumptions on independent gradients for `a` and `W` in page 3. Do you think in a realistic training setting with softmax-cross entropy, you can generalize the analysis and reconcile the fact that NTK is non-constant?\n\nIn general, does the generalization of “dynamical stability” to admit infinite logits valid for multi-class cases? In multi-class simple “sign” can’t be used for classification and I wonder if authors have ideas on generalization to multi-class settings. \n\nIn eq (26), why is Gaussian distribution over logit a good way to measure KL divergence? I understand at initialization prior is distributed over Gaussian, however after training with softmax I expect the logit distribution is no-longer Gaussian. \n\nNits and additional feedback:\n\nNit: use \\citep in places where appropriate.\n\nDivision (/) for denoting “or” is often confusing and would suggest other notation. Especially in the Figures and Condition 2 when sometimes it means “or” and sometimes it means division.\n\np4: ‘grows width’ -> ‘grows with’\n\n[Sohl-Dickstein et al., 2020] show different scaling of weight and bias scaling extending standard parameterization to work well with NTK limit. While this paper mostly works with the NTK parameterization, it would be interesting to discuss how the improved parameterization in [Sohl-Dickstein et al., 2020] can be utilized. I suspect this is quite related to (9) / (10) where definition of NTK deviates from [Jacot et al., 2018]. \n\nIn Section 3: When comparing performance on finite networks and NTK, it can be subtle depending on how one trains finite networks and there’s architecture dependence [Lee et al., 2020] \nIn Section 4: suggest citing [Chizat et al., 2019] for ‘lazy training’\n\nChizat et al., On Lazy Training in Differentiable Programming, NeurIPS 2019\nLee et al., Finite Versus Infinite Neural Networks: an Empirical Study, arXiv: 2007.15801\nLiu et al., On the linearity of large non-linear models: when and why the tangent kernel is constant, arXiv:2010.01092\nSohl-Dickstein et al., On the infinite width limit of neural networks with a standard parameterization, arXiv:2001.07301\n\n----\n[post rebuttal]\nI thank the authors for their revision and clarifying many of my questions and adding results based on that. I have read other reviewers concern and while I agree some room for improvement on clarity, I still think the paper brings in value. Unless there's technical flaws spotted by other reviewers that has not been resolved, I'm still leaning towards accepting (increased score from 6 to 7). ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An ambitions, potentially exciting paper, which is unfortunately impossible to follow",
            "review": "\nThis paper investigates the dynamics of the training of fully connected neural networks with one hidden layer in the infinite-width limit, for classification. Starting from the observation [Golikov, 2020] that the mean-field limit and the NTK regime are only two special cases of continuous families of scalings, it attempts at identifying important or desirable features that an infinite-width limit should display in order to give good insight into the dynamics of large but finite neural networks. \n\nThis is an exciting program, and it looks like the ideas are very good. If / when laid out, they will really bring insight into the training of large neural nets.\n\nUnfortunately, the way the paper is written makes it hard to accept in its present form. While the global structure is quite clear, the wording makes it very hard to extract information. For instance, it is not clear what is mean by \"finite\": does it mean there is a finite limit (what should be expected in principle), that it is bounded from above (what it seems to mean sometimes) or that it is strictly positive (what it appears to mean sometimes)? Also, there is a classification into 13 cases that are promised, but they are not clearly listed in the main, the fact that we are working with only one hidden layer is not clearly said in the abstract (and the discussion on how to extend to more hidden layers is not convincing). The grammar is somehow problematic (there is in particular a lot of missing \"the\" articles in front of nouns), and it sometimes makes it hard to follow. Also, it is not very clear what is proven, where assumptions are used (e.g. analyticity of the nonlinearity), the seemingly most interesting regimes are not defined in the main, etc.  \n\nI am tempted to think that the authors are very lucid about their understanding of what is happening and that they really have interesting something to convey, but the present version makes it very hard to get a reliable information. A (very significantly) revised version of this paper could, I believe, bring much insight to our understanding of neural nets. There is a lot of potential with this paper, just not realized in terms of exposition.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting story; but the stability conditions defined in this paper are not justified to be important ",
            "review": "This paper proposed a general framework to derive different stable limiting behaviors of the dynamics of two-layers neural networks, under different parameterization of the hyper-parameters. For certain choices of hyper-parameters, this recovers the mean-field limit and the NTK limit. This paper also proposed certain properties of the limiting dynamics and showed that using these properties as the classification criteria, there are only a finite number of distinct models in the limit. This paper also proposed a novel initialization-corrected mean-field limit that satisfies all properties.\nThis paper tells an interesting story. The question is whether the problem solved by the story is important.\nThe main consideration of this paper is to find regimes of hyper-parameters such that the limiting dynamics are stable in some sense. For this purpose, the authors advocate the IC-MF regime, such that the limiting dynamics are stable with respect to all the conditions that the authors proposed. This story seems to be self-contained on its own. However, I believe that two more important criteria for good training algorithms are optimization and generalization efficiency, which are not discussed by the authors.\nThe optimization and generalization efficiency are much more important than the stability condition in practice. There could be some regimes of hyper-parameters that do not satisfy the stability condition, but has good optimization and generalization efficiency. The IC-MF regime proposed in this paper, although seems to satisfy additional stability conditions, but intuitively, it seems that its generalization efficiency is not as good as that of MF regime: it added an additional noisy function $f_{ntk, \\infty}^{0)}$ to the mean-field prediction function, and this additional noise (intuitively will) hurt generalization.\nI believe some of these stability properties should have some connection to optimization and generalization. For example, if some simple stability property is violated, the algorithm cannot generalize well. I feel that the authors should try to build connections of stability properties to optimization and generalization, to justify the importance of condition 1 and condition 2 defined in the paper.\nAbove all, I feel that this paper is interesting in its own criteria. However, it didn't justify that its criteria are important. So I feel that this paper is on the borderline.\n\nMinor issues:\n\t1.\tSome notations are easy to get readers confused. Eq. (7), $\\sigma(d) = \\sigma^*(d / d^*)^{q_\\sigma}$. Here $\\sigma$ is a function of $d$ while $\\sigma^*$ is a scaler (not as a function of $d / d^*$). It takes me while to understand this.\n\t2.\tTypos: page 18: in this case $1 + \\tilde q + 2 q_\\sigma$.\n\t3.\tThe notations of this paper looks very complicated, especially the superscripts and subscripts.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting proposals, but the paper requires substantial revisions and additional analyses.",
            "review": "The paper analyzes joint scalings of the parameter initialization and the learning rate, with respect to the limit of infinite width, in the context of two-layer neural networks with stochastic gradient descent and binary logistic loss. It proposes some “dynamically stable” conditions and identifies a range of scalings that satisfy these conditions. This covers the neural tangent kernel (NTK) and mean field (MF) scalings, as well as others. The paper then proposes to add an extra correction function to the initialization of the MF limit and argues experimentally that this correction can give a proxy for standard neural networks.\n\nOverall there are interesting points in the paper, but I think the paper needs a lot more works to make the study rigorous, complete and easier to read. More analyses into why any point in the “dynamically stable model evolution” band leads to stable dynamics at all training time are needed.\n\n\n### Positive points:\nThe recent trend in analysis of large-width neural networks has drawn a lot of attention, so the paper is timely. \n\nThe idea of joint scalings between the initialization and the learning rate, w.r.t. the width d, has been explored before; but here the paper proposes to do so in relation with a new set of criteria. The results make an interesting view on the connection between two distinct regimes NTK and MF, and beyond.\n\n\n### Negative points:\n\n**Presentation:**\nThere are many unnecessary notations, for example, \\tilde{q}_a and \\tilde{q}_w which are equal to each other in the main analysis. It is better to keep all discussions around just the two key scaling exponents. \n\nThe main figure, Figure 1, is very hard to read, and I cannot see where the curve for MF is in this plot. \n\nThe statements also seem to miss several assumptions e.g. assumptions on the data (will things hold if |x| ~ exp(d)??).\n\nSome notations are not explained, for example, \\sigma^{(0)}(x) that appears in Eq (13).\n\n**Incomplete proofs:**\nAside from many derivations which are heuristic (which are fine as long as they are not stated as propositions/theorems), the proofs for the stated lemmas / propositions are unrigorous and may entirely omit the more difficult technical points. For example, in the proof of Lemma 1.4 in Appendix B.1, why is it that the collection of the weights \\hat{w}_r^{(k), r=1…,d, at any time k>1 allows one to apply the law of large numbers? The variables in this collection are not independent; if they are mildly dependent, in what sense are they so that one can apply the law of large numbers?\n\nIn fact, there are simple mathematical mistakes throughout the proofs. For example, to prove Lemma 1.2 in Appendix B.1, the paper only proves the statement for a single fixed x; why does it hold for almost every x?\n\n**Abstract missing key information:**\nThe abstract should be more precise: it should mention logistic loss for binary classification and that the initializations are zero-mean. These are important; without them, a number of key results will fail.\n\n**Dynamical stability criteria are insufficiently justified; most are limited to just initializations:**\nThe paper does not give sufficient justification of why the proposed criteria (Conditions 1 and 2) should be considered. In particular, Condition 2 only concerns with what happens at initialization; does it guarantee stable dynamics well beyond initialization and in what sense? The paper does not discuss this for most points in the “dynamically stable model evolution” band. In fact, compared to the well-studied NTK and MF scalings, the newly identified scaling sym-default is said to have some divergence behavior (in which its prediction function is always infinite, for infinite d, at any positive training time). If anything, the paper should justify why this divergence behavior does not lead to instability; by looking at Figure 1, I think it suffers from numerical instability.\n\nIn fact, scalings other than NTK and MF have been studied well beyond initialization in a rigorous fashion. In particular, [1] shows that with a wide range of different scalings, it’s entirely possible to have the same stable behavior: NTK behavior where weights do not move and the model tracks some random-feature models. Given that such rigorous and thorough study is possible, it is unclear how the proposal of Conditions 1 and 2 leads to novel and significant insights.\n\n**Condition 2.3 is unusual in terms of physical units and not motivated:**\nIn particular, why should we compare the magnitude of the kernel K with the magnitude of the prediction function f, while they should have very different physical interpretation? Why is this condition interesting? The second paragraph in Section 3 seems to give some arguments for Condition 2.1 and 2.2, but none for Condition 2.3.\n\nGiven that the role of Condition 2.3 is unclear, if one removes this condition from the diagram of Figure 1, the band picture disappears and we are effectively left with a half plane, separated by the “evolving kernels” boundary. To the left of this boundary, it is the NTK behavior as expected in [1]. So unless Condition 2.3 signifies some significant and meaningful change in network behaviors, the diagram would not show new significant insights compared to the literature.\n\n**The derivations for the dynamics in Section 3 are incomplete:**\nIn the regime of small learning rate (\\hat{\\eta}* goes to 0), what we should obtain is a continuous time evolution with an expectation over data, not a discrete-time one with stochastically drawn data. Evidently Eq (14) as given in the paper completely contradicts all previous published reports on the MF limit.\n\nIt does not make sense to claim, after Equation (19), that the prediction function diverges from iteration k=1 onwards. Firstly, as said, the analysis has to be done with respect to small learning rate and hence the iteration k has to be determined in relation with the learning rate. Secondly, that claim contradicts Figure 1: the prediction function does not seem to diverge at iteration 1.\n\n**The initialization-corrected mean field (IC-MF) limit lacks justification:**\nIts aim is to correct the MF limit w.r.t. Condition 2.1, but there are very simple alternatives to do this correction. The first way is to add any fixed function, whose magnitude are independent of d in suitable sense, to the MF limit. The second way is to initialize the MF limit with non-zero mean distributions; in fact, it is known that non-zero mean initializations are more typical in MF limit.\n\nThe paper argues that IC-MF limit can be good proxy for standard networks, giving only one simple experiment shown in Figure 1. There is no proof or mathematical heuristics provided. I think this IC-MF idea does not have anything to do with the theory in the previous sections, nor the binary classification problem with logistic loss. As such, it should be at least further tested on more complex experimental tasks, e.g. CIFAR-10. From a mathematical standpoint, I do not foresee a simple argument to show why it can be a good proxy for standard networks.\n\n\n[1] A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics, E, Ma and Wu, 2019.\n\n---------------------\n\nPost rebuttal: \n\nI thank the authors for their rebuttal. Let me focus my reply on a few important points. I first thank the authors for clarifying the meaning of Condition 2. In this sense, Condition 1 is the key main contribution; however the current proof does not look correct to me, and the revised argument is far from being sufficient. In particular:\n\n- Point 5 of the rebuttal: I think the revised argument here is incomplete. The given argument concerns trivial facts and does not imply the claim. For example, what if the distribution of the terms is symmetric, the expected sum is zero, and hence the quantity might be of order smaller than d? Note that this is an example problem; there are multiple problems with the proof of Lemma 1.4. For example, the paper claims this for all k; but if k is something like d^100, would things hold? What would stop the magnitude of the weights to grow with time?\n\n- Point 6 of the rebuttal: The CLT, when applied w.r.t. the randomness of the weights, says that for a fixed $x$, $\\sum_{r=1}^d \\hat{a}_r \\phi (\\hat{w}_r x) / \\sqrt{d} \\sim N(0,v_x)$ approximately. That is, there is a non-zero probability (w.r.t. the randomness of weights) that the claim in the paper for a fixed $x$ fails. As such, to reason the claim for many $x$, one requires doing probabilistic arguments very carefully.\n\nThe paper should execute the proof very carefully. It is not just a matter of technicality; I suspect some of the claims are actually wrong.\n\nMore importantly, Condition 1 alone is insufficient to claim dynamical stability at any time k. What should qualify for dynamical stability is rather the existence of a well-defined limiting dynamics exists (which is argued heuristically in Appendix C), and its proof. In the current writing, it’s unclear how Condition 2 is crucial; while it studies interesting properties, it is very restrictive.\n\nAs said in my last review, one thing that has been missing is really whether the insight here differs qualitatively from the known NTK and MF limits. Further looking at the limiting dynamics in Appendix C, one sees that they are qualitatively either NTK or MF. There are possible degeneracies due to scalings and the use of logistic loss, but these do not lead to much deviation from NTK or MF behaviors. If one is to use a squared loss for instance, what one would obtain in Figure 1 is just the line connecting NTK and MF; all other points in the band outside this line are degeneracies due to logistic loss. The behavior on this line, again, is qualitatively either NTK or MF, and this is shown (somewhat implicitly) already by a number of past works.\n\nI would imagine a rigorous derivation of the limiting dynamics for each point in the band revolves around the renormalized dynamics in Appendix C. When translating from the renormalized dynamics to the original one, the extra scaling factors will complicate the proof (for instance, they can blow up Lipschitz constants). Again this has to be done very carefully.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}