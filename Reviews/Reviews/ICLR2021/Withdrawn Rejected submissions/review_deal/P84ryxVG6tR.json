{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The is a borderline paper with the reviewers split in their recommendations.  The decision is therefore not easy.\n\nThe work is promising, but a key concern is that the contribution appears incremental: the paper proposes to alternate between kickstarting, which is itself not entirely new as an idea, with a simple instance transfer heuristic.  The resulting method is straightforward, which can be considered a strength, but there is no serious technical justification beyond intuitive motivation.  Rather than present technical analysis, the paper focuses more on intuitively delivering the proposal then evaluating it.  This would be acceptable if the empirical outcomes were undeniably impressive, but the outcomes, though positive, are not overwhelming.  The experimental evaluation is limited in scope, considering only the simplest MuJoCo environments and a benchmark racing simulator.\n\nThe authors responded to some of the criticisms forcefully, and were comprehensive in their rebuttal, but if the support for the proposed method is to be entirely intuitive and empirical, one would have expected a more comprehensive evaluation where transfer was used to solve more impressively difficult problems.  Overall, I think this work would be better served by adding a technical analysis that validates the significance of the instance transfer heuristic, combined with a broader empirical study that tackles more challenging domains.\n"
    },
    "Reviews": [
        {
            "title": "A well written paper suggesting a hybrid approach for transfer learning",
            "review": "This paper deals with transfer learning in RL. The problem broadly defined is to improve performance of an agent on a new task, given an agent (teacher) trained on a previous (different) task. There are multiple approaches to this from fine-tuning, distillation to instance transfer. A central question in this field is how to transfer learned useful behavior even when task similarity is low. While most methods would work well on similar tasks, naively applying, for example distillation, would hurt when training on a very different target task. \n\nThis paper suggests an approach that builds on two approaches: kickstarting and instance transfer. Kickstarting can be viewed as distillation of a policy with a dynamically tuned coefficient that control how much to weigh the distillation loss against the actor-critic loss. Intuitively this distillation is more helpful at the start of training and the original work uses population based training to automatically anneal this. Applying this as-is to tasks with very different reward functions would lead to a degradation in final performance on the new task. \n\nThe second approach considered here is instance transfer. Briefly, we consider trajectories from the teacher and train the agent on those that are \"good\" as filtered by the advantage under the current reward function. \n\nThe results are encouraging, especially showing that performance is not lost when tasks are dissimilar.  The writing overall is very clear. Below are few specific notes/questions.\n\n* One suggestion if for greater discussion into why this works. It was not clear to me on reading what the fundamental rationale for combining the approaches in this way is, as opposed to say combining kickstarting with some other method like successor features. \n\n* How sensitive is the method to the schedule for kickstarting? Could a well hand-tuned kickstarting schedule alone achieve all of the performance gain if we knew a priori how similar the tasks were? \n\n* The paper mentions automatic identification of task similarity as future work -- do the authors have ideas on how best to do this? \n\nOverall I found this a well-written paper that combines two existing approaches to transfer learning in RL to provide a unified algorithm that can do well in settings with low or high task similarity.  I think the results are promising although given the size of the error bars on some of the plots it's not clear it is completely worth the effort to use this method in all cases. Addressing the main point above on why this is necessarily a principled approach as opposed to just an ensemble of methods would help motivate this. Nonetheless in its current form I would recommend an accept. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "RL domain transfer method has limited novelty ",
            "review": "The proposed method relies on kickstarting, thus using policy distillation as an auxiliary loss for transferring from a source task to a target task, as a starting point. In addition, the authors add 'instance transfer', i.e., selecting some prioritized data from the source task to be used to train the target task. The combination of these two features produces fairly strong performance on various transfer experiments in four simulation environments. \n\nThe approach is clearly presented. The motivation is clear and related approaches are described. The experiments are described, although the descriptions and conclusions could be more precise. \n\nThe proposed approach has very little novelty, since kickstarting is already published and similar versions of instance transfer have been published. The instance transfer method, which is rather heuristic, requires tuning a threshold which may be a limitation. The results, moreover, do not convincingly show that putting these two features together makes a significant difference to performance. The improvements are minor for all the experiments when compared with the baseline or kickstarting.\n\nPros\n- Clearly described method\n- Well-designed experiments\n\nCons\n- Lack of novelty beyond the combination of 2 known methods\n- Results don't show that the method is substantially better than kickstarting alone",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper describes an algorithm for transfer from a teacher policy that combines an existing kickstarting approach with advantage reweighted ‘instance’ transfer of the teacher.",
            "review": "Below I list the strengths and weaknesses of the paper in my opinion. Overall I vote to accept the paper for now, but my final decision will depend on the authors' clarifications to my questions below.\n\nStrengths:\n - The algorithm is a simple combination of ideas each of which seems important according to the results.\n - Data efficiency and the ability to use previous knowledge to inform solutions to new tasks is an important problem in RL and I encourage the authors to continue to explore this space. \n - The appendix includes quite a good overview of experimental details and since the work is based on open source implementations it is potentially reproducible.\n - The appendix also includes a number of ablation studies including results on end-to-end wall clock time and additional results. As I describe below though, more can be done in terms of experimental rigor and clarification is needed for some of the presented results.\n\nQuestions/Clarifying points:\n- I am concerned with the strange drops in performance in Figure 2. This instability also holds for the Baseline which is trained from scratch. This indicates it does not have to do with the transfer of the teacher policy where ostensibly the drop corresponds to critic training. Could the authors clarify what is happening here? I think it is important to have a good explanation for this.\n\n- It is mentioned that the Beta_k parameter used in kickstarting is annealed so that it vanishes at later iterations. Could the authors clarify if this schedule is also maintained for the pure ‘kickstarting’ baseline? If it is, I’m uncertain why this baseline performs asymptotically worse on the ‘dissimilar’ tasks. Shouldn’t the influence of the teacher slowly disappear anyway?\n\n- Figure 2b) indicates that using the top 20% metric for advantages works as well (if not better) than the one used in REPAINT. Since this metric also has the property of being scale invariant, would it not be better to use this instead? \n\n- The authors do not really compare against any existing methods for transfer learning. For instance, it would be interesting to see how the proposed method compares to say DAGGER.\n\n - I’m not certain the specific task similarity measure described in the main text is a good one. For instance the ideas introduced by Carroll and Seppi and Lazaric (both referred to in the main text) attempt to quantify similarity in terms of how well an expert on the source task would perform in the target task. For instance Carroll and Seppi mention: ‘The task similarity measure should provide an approximation to the amount of learning improvement that we would get when using the source task to learn the target task under a given transfer technique’. In light of the current work, the ‘similarity’ under the proposed approach is 0 for the ‘dissimilar’ tasks. While kickstarting performs worse than REPAINT on these tasks, it still manages to do reasonably well. It would be interesting to note how dissimilar the tasks are under the other proposed metrics.\n\nApart from these, there are technical details which could help improve the work that did not affect my decision but list below:\n- The main text mentions that the clipped loss L_clip is used even for instance transfer in Algorithm 1. Strictly speaking the importance weight used for this should be the teacher policy. This may be worth mentioning somewhere although it is not too important.\n- For ease of parsing the experiments, it is often useful to have a consistent color schemes for the legend used across figures. Figure 2 and Figure 3 use the same palette but with different names attributed to each method. This isn’t a big issue but fixing it would make it easier to parse the figures and improve the overall presentation.\n- The related work could also include a discussion of recent work on offline reinforcement learning. Specifically recent methods proposed by Peng et al. , Siegel et al. and Wang et al. show advantages of using different forms of the advantage function when learning from offline data. This is similar to the instance transfer prescribed in the paper where the teacher generates the fixed distribution to learn from.\n\n\nReferences:\nSiegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. ICLR, 2020\n\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S. (2019). Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177.\n\nZ. Wang, A. Novikov, K. Żołna, J. T. Springenberg, S. Reed, B. Shahriari, N. Siegel, J. Merel, C. Gulcehre, N. Heess, and N. de Freitas. Critic regularized regression. arXiv preprint arXiv:2006.15134, 2020.\n\nS. Ross, G. Gordon, and A. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. Journal of Machine Learning Research, 15:627– 635, 2011.\n\n\n--- Edit: After rebuttal ---\nI thank the authors' for their detailed response to my questions and for clarifying so many aspects of the work! I also appreciate the time taken to make the minor corrections that help with the presentation. \n\nMy main concerns have been sufficiently addressed - specifically the references to the dips in performance have eased my concerns with the validity of the experiments. The ablation with the beta parameter is also quite important I think and at least helps clarify that the issue for kickstarting may indeed be one of reaching a local optima.  Given the substantial effort (including implementing a new baseline) that went into the rebuttal I am willing to increase my score and recommend the paper be accepted at this venue.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A transfer learning method that combines kickstarting policy distillation with experience selection",
            "review": "This paper proposes a new transfer learning approach to leverage the previously learned knowledge stored in a pre-trained teacher policy to facilitate the learning in a new task. The proposed method combines an established technique termed kickstarted training with a simple experience filtering method. The kickstarted training approach augments a policy distillation loss to the actor-critic training loss. The experience filtering simply performs thresholding to filter out transitions generated by teacher policy which have reward below some threshold, where the reward for thresholding is taken from the target task. \n\nI lean slightly towards rejection because the novelty of the method is somewhat too limited and the empirical evaluation does not give much new insights to the problem of policy transfer. The only difference between the proposed method and kickstarted training is that the method uses more data, which are the transitions generated by the teacher policy and could receive high reward at the student task domain. Such an add-on does not give much privilege in terms of return or training time.\n\nOther detailed comments :\n- The tasks for MuJoCo-Reacher are too similar. The authors only use an alternative weight on the control reward to create a distinct task. But essentially these can hardly be considered as alternative tasks for policy transfer scenario.\n\n- I wonder how the weights for policy distillation loss are specified for the baseline method kickstarting. Do you use the same value as REPAINT for each evaluation domain?\n\n- In MuJoCo-Reacher-similar, REPAINT and Kickstarting achieves the same standard for mean reward, and training time for the two also seem quite close to each other.\n\n- In MuJoCo-Ant, the improvement of average return for REPAINT over Kickstarting also seem not that significant. I wonder why the results are presented in table rather than plots.\n\n- The proposed method presents a way to quantitatively evaluate the task similarity (Eq 4.4). I'm expecting to see some finer-grained usage of such a formulation, e.g., creating tasks with positive but decreasing levels of similarity and show empirical results. However, the tasks used for the actual evaluation only have vaguely characterized similarity values, which leaves a feeling that the formulation for similarity is not necessary to have.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}