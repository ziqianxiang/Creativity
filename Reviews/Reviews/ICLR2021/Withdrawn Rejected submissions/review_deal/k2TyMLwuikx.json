{
    "Decision": "",
    "Reviews": [
        {
            "title": "The idea is interesting and the comparison with other related works demonstrate the improvement. However,  the details for measuring the difference between feature maps with model output part is not very clear. ",
            "review": "This paper presents one new framework for model compression. The comparison between the feature map of skip-connections and the model output is used as the criteria for model compression. However, the description of the approach is not very clear in details, it may help people understand better with figure diagram along with the workflow. Perhaps the author can consider showing one example of how D can be calculated. This approach belongs to structured model compression. It seems that it is only limited the network with skip-connection.  It would be good to see the extension of this idea to other general network without skip-connections.  There are several typos in the paper, e.g., \na. in Theorem, it should be 'if' instead of \"iff\". \nb. please see section ?? at the paragraph before Section 4. \nThe legends in Figure 1 and 2 are also not very easy to read. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Pruning with Energy distance",
            "review": "This paper proposes a method of finding an informative layer of a network by energy distance and performing pruning based on this. There are some new aspects to finding informative layers in a non-parametric way. \nHowever, this paper is not yet ready for publication for the following reasons.\n\n- Limited contribution of Energy Dependence to performance\\\nFigure 2 shows the difference in performance between random and the proposed method is limited, especially for reduction range 0% to 60%. In particular, the reduction rates of the experimental results in this paper are mostly within 60%. How many times were each experiment performed for the results in Figure 2?\n\n- Unfair comparison\\\nIt is difficult to understand why Cutout augmentation is used in Cifar experiments.\nRetraining is performed by repeating a large number of stages, and it is difficult to exclude the gain due to this training budget increase.\nFor example, in [1] , even if the author re-initializes [2] and uses the same training budget, the result was 73.41% with ResNet50. This is a higher accuracy than this paper.\\\n[1] Rethinking the value of network pruning, Z. Liu\\\n[2] Data-driven sparse structure selection for deep neural networks, Z. Huang\n\n- Omission of some high performance baselines\\\n[1] Convolutional Networks with Adaptive Inference Graphs, A. Veit\\\n[2] Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference, J. Luo\\\n[3] URNet : User-Resizable Residual Networks with Conditional Gating Module, S. Lee\\\n[4] Channel selection using Gumbel Softmax, C. Herrmann\n\n- Missing important experimental settings \\\nRe-train is performed during N stages, but the paper does not include essential hyper-parameters such as training epoch and learning rate.\n\n- Many writing errors \\\nPage 1, \"ReseNet\"\\\nPage 5, refers to the appendix, which is not included.\\\n\"Section ??\"\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Model-Free Energy Distance for Pruning DNNs ",
            "review": "This paper introduces a metric called \"energy distance\" in DNN pruning to evaluate the relationship between feature maps and the output. However, the rationale behind this metric is not clear. The definition and discussions of this metric in Section 3.2 are inconsistent with the way it is used in Section 3.3, making the validity of the former definition unconvincing. Besides, since the results in this paper show that the maximum value of this metric does not mean the best pruning effect, what changes have been made to this metric to get the best pruning effect need to be explained. Thus, it is hard to tell whether this metric is really effective and the improvement shown in the experiments seems marginal. Additionally, the organization of experimental results needs to be improved. As this paper aims at proposing a new pruning method, it is suggested to put results from the same model together when comparing algorithms. Besides, as the main contribution is a new metric for network pruning, it is necessary to compare different metrics and put their comparison results together.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Okay submission. Imformative and Sound Experiment Results",
            "review": "### Overall\nThis work presents a new DNNs pruning method, Pruning with Energy Distance (PED). This method works based on the energy distance between hidden-layer output and the final representation learned from the DNNs. Imformative experiments show that PED outperforms those existing SOTA compression methods on various benchmark datasets.\n\n### Pros\n1) This work supports the merits of PED with plenty of experiment results. They compare PED with several SOTA methods to show the outstanding performance. \n2) This work proposes the energy distance (ED) as the knowledge for the pruning. Such method does not rely on the pre-knowledge of feature distribution in the dataset, and all the pruning knowledge is learned automatically.\n\n### Cons\n1) The overall submission is okay. Readers can get the rough idea of PED. However, it seems not ready enough for a submission in the conference. Typos show that some parts of this work haven't went through proof reading. For example, in page 5, the last 10th line, \"Please see section ?? in the\". The reader could get the content in the appendix, but it should be fixed a more clear description. Another example is the number in tables. In table 4, the Top1 Acc of HRand Lin et a. (2020) should be \"0.7198\" not \"0.71.98\". More examples, \"Next PED uses \\hat{D}\" in page 4 could be \"Next, PED uses \\hat{D}\".\n2) The section of \"Proposed Algorithm\" is trying to explain the main idea of PED, but it is hard for readers to follow. Notations are mixed in the description how PED works. Since the organization of section 3.2 looks clear, I suggest the authors rewrite or at least re-organize the section 3.3 to make the section 3 more readable. For example, those notations K, A, S could be introduced before the formal description. Since the main idea will be shown in Algorithm1, readers would like to know more intuition of this algorithm. Authors would not need to show the steps in the content but leave the key steps only. Things like how to do the clustering can be moved to footnotes. The analysis of experiment results could be moved to section 4. \n3) Although the model compression method focuses on how to reduce the inference complexity of DNNs, an expensive pruning method is still painful in practice. I would suggest this work presenting time complexity analysis for PED compared to SOTA method. And since the algorithm mentions Train_err multiple time, the definition of such notation should be clear in advance. After following the description, we are still wondering how and when to compute such Train_err. Do the users need to compute the Train_err for the whole datasets? Does the Train_err require the user to train the pruned model until converge? And the algorithm requires to get the min candidate in centroids. Will the computation power be consumed for unused candidate if the method really needs long training process?\n4) Minor issues: Figure 2 could be re-draw to fit the page limit. The presented one is too flat to show the difference between curves and the legends are too flat as well. \n5) Missing reference: Dai, Bin, et al. \"Compressing neural networks using the variational information bottleneck.\" International Conference on Machine Learning. PMLR, 2018. This work also mention how to utilize mutual information in the compression scenario.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}