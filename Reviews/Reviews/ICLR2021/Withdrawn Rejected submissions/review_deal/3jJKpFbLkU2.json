{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper had four borderline reviews with none enthusiastic about championing the merits of the paper. While it was felt that the extension of an existing technique to deep learning via amortization is a useful procedure, it is also not very novel and the experiments didn't demonstrate a significant leap in performance."
    },
    "Reviews": [
        {
            "title": "paper 2062 Review",
            "review": "The authors propose the amortized conditional normalized maximum likelihood (ACNML) method for scalable, general-purpose uncertainty estimation, calibration and out-of-distribution robustness for deep neural networks. ACNML is a tractable approximation to CNML that leverages Bayesian inference by using an approximate posterior in place of optimizing over the dataset during inference.\n\nBased in the formulation provided CNML does not seem to be intractable but prohibitive.\n\nThe proposed approach is very simple and intuitive, even after the modification in (19). The analysis of the second-order approximation in Section 3.2 though interesting does not seem to impact the proposed method (as implemented) and the experimental results.\n\nIt is difficult to evaluate the advantage of the proposed method because from the results: 1) all compared methods perform about the same in terms of accuracy, 2) the predictions from the proposed models do not necessarily result in better NLL values (as a proxy for uncertainty estimation performance), or calibration on in-distribution datasets. It is only in terms of calibration of OOD calibration where the proposed approach seems to have an edge over existing methods.\n\nMinor:\n- $p_\\theta(x)$ is not defined.\n- there is a missing reference in the Appendix, above (20).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper overall; however the proposed method is straight-forward and not that convincing",
            "review": "The paper presents an approach based on conditional normalized maximum likelihood (CNML) for uncertainty estimation, calibration, and out-of-distribution robustness with deep networks. CNML is intractable to compute in general and therefore the authors propose a tractable approximation which uses approximate Bayesian inference techniques. Experimentally, the authors show that their new approach is competitive and sometimes better than existing approaches for uncertainty estimation and calibration on out-of-distribution test data points.\n\nThe paper is well written, easy to understand and the authors are well versed with the domain (the writing clearly demonstrates scholarship, knowledge and expertise). As far as I can tell, related work is appropriately cited and the authors have done an excellent job in describing how their current work advances and relates to prior work on this topic. My criticisms/suggestions which I outline next are purely constructive and my hope is that they will greatly improve the paper:\n\n1. The algorithmic advances (Algorithm 1) are not that novel. I am not sure why the equation for the regret and definitions of unconditional normalized maximum likelihood are presented in the paper. The authors can simply start with the non-regret based definition of CNML given in Equation 6 and derive their approximation (the minimax optimization is not relevant to the algorithm). \n2. It would be informative to include comparison of your method with the method of Bibas et al. 2019 in terms of accuracy, NLL and ECE scores (at least for the MNIST dataset). In a way, what you are proposing is a faster, more scalable approach and it would be informative to know how far its estimates are from a much slower (but potentially more accurate) approach by Bibas et al.\n3. Another important comparison missing in the paper is how various methods perform on synthetic datasets where CNML can be exactly/accurately computed. As such, it is unclear whether smoothing/averaging effects of CNML are responsible for performance improvements on out-of-distribution samples or whether it is highly dependent on the quality of the approximate Bayesian inference method used.\n4. Latex tricks are used to somehow squeeze Figure 1 in the paper. Perhaps, what you could have done is removed definitions of (unconditional) NML.\n5. Section 3.2, although important from a theoretical point of view does not advance the main thesis/point of the paper (a faster approach for uncertainty estimation/calibration that uses approximate Bayesian inference). Also, most of the results described in section 3.2 are already known (specifically, Theorem 3.1). \n\nOverall, a good paper which advances the state-of-the-art on an important problem (uncertainty estimation and calibration) in neural networks. However, the paper lacks novelty and its presentation and experimental evaluation can be improved. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting new direction for computing well-calibrated predictions",
            "review": "This paper explores the application of an alternative inference scheme for learning well-calibrated predictive models especially target to ut-of-distribution samples. The paper is based on the so-called conditional normalized maximum likelihood, which is an inference approach based on min-max regret principles. Experimental evaluation shows promising performance to robust predictions under distributional shifts. \n\nStrong points:\n\n- The paper explores the use of alternative and well-founded inference approaches. \n\n- The presented solution is well-motivated and scales to large data samples and large models. \n\n- The empirical evaluation shows promising results on ut-of-distribution samples. \n\nWeak points:\n- The adaptation of the conditional normalized maximum likelihood approach presented in this work lacks the min-max regret guarantees provided by the original framework. \n\n- Empirical evidence about the advantage of the method is limited. \n\n- The presented method is too cautious when making predictions which are not out-of-distribution samples, and its accuracy is not competitive in this regime. \n\nI support the recommendation for acceptance of this work because it explores the use of a well-founded inference framework, the conditional normalized maximum likelihood, which optimal min-max regret guarantees but which is computationally intractable in its exact form. This work proposes an approximate inference scheme which makes the application of this inference framework tractable for deep neural networks. \n\n\nQuestions:\n\n- Does  the provided analysis of second-order approximations provide any relevant insight? It is based on very strong assumptions which do not hold on reality.\n\n- What happens when there are multiple maximum-likelihood estimates like happens in deep ensembles? Every member of the ensemble achieves zero training error. \n\n- Why don't you compare with Bibas et al. (2019)?\n\nMinor comments:\n-  Space below Figure 1. \n\n- Y label Figure 1 b) and c) is not readable. \n\n- Missing Eq. reference in Pag. 11\n\n- Figure 2 is hard to read. Please consider another format (bar based plot?). \n\nPost-rebuttal: I thanks the authors for their answers to my concerns. After looking the reviews of the other reviewers and the ongoing discussions, I have decided to keep my score. I think this paper makes a novel proposal which deserves to be published. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "a paper that may need careful checking before submission",
            "review": "This paper proposes ACML method which provides a tractable approximation to the CNML distribution and reduces the computational complexity of CNML method.\n\nStrength:\n1. Provide a tractable approximation for CNML which makes training easier and faster.\n2. Theoretically prove the approximation error is bounded, but the proof part has too many typos and cannot be confirmed.\n \t\nWeakness:\n1. The author didn’t use the experiment to show the accuracy of CNML distribution and ACNML distribution. The author only compared the training time of these two methods. Since in equation 18, the given bound of the approximation error increases with k, we don’t know whether this approximation is well bounded.\n2. In Table 1, MAP is a better method than ACNML for the MNIST dataset.\n3. In Table 3, it misses the results of ACNML-KFAC for WideResNet28x10 dataset, and it seems that the proposed methods don’t outperform other existing methods.\n\nTypos:\n1. In Equation 34, there should be a θ ̃_z instead of θ ̂_z\n2. In Equation 36, the third and fourth terms sum over z’, but there is no such parameter, and the symbol of absolute value disappears, due to this reason, I cannot verify if this equation holds.\n3. Looks like Equation 39 misses a minus symbol.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}