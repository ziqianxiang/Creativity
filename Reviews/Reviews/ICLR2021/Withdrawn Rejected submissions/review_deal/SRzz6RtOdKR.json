{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The manuscript presents a deep network approach for heteroscedastic regression problem. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.\n\nThree reviewers agreed that the manuscript is not ready for publication. The major issue is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method. \n\nR2 and R4 confirmed that they have read author response. The rebuttals are useful to clarify some points, especially related to experimental settings and results. However, they are not convinced by the authors' argument on novelty and whether the assumption is realistic. \n"
    },
    "Reviews": [
        {
            "title": "Important References are Missing; Novelty is Unclear.",
            "review": "The paper propose to address the heteroscedastic regression problem using deep neural networks. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.\n\nThe major issue to me is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method. See Chapter 10 of \n\nhttp://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf\n\nThis paper failed to cite any relevant reference and clarify the novelty.\n\nTo apply the method to a deep learning setting, some interesting problem can be how to estimate the variance with deep network in a reliable way (this was done previously using classic models). However, this paper did not tackle this harder (and more interesting) problem. Instead, it assume the variance is simply given during training. This is not very realistic in real world setting. The experiments are all synthetic and are not particularly convincing. \n\nFinally, the paper claims a lot of connection with privileged information (LUPI). But I found it hard to consider this variance a similar concept as privileged information, which is realistic and interesting.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes a reweighed loss function for robust regression model training against label noise. The weight value of each training instance is determined by prior knowledge about the noise generation process. Results confirm empirically the merit of the algorithmic design",
            "review": "In this paper, a reweighting technique is proposed to suppress the impact of heteroscedastic label noise in regression model training. The objective function of the regression model training process is composed of a weighted combination of instance-wise training loss. The instance-wise weight is determined by the estimated noise variance based on prior information of the label generation process. The weighting formulation is inspired by the best possible estimator of noisy measurements reaching the Cramer-Rao bound. \n\nThe paper is clearly written. It explains well the problem definition and the methodological formulation. However, we think the innovation in this work is limited. The downside of this paper is as follows: \n1. It is a strong and usually impractical assumption to know a priori knowledge of label noise in the regression model training process. Especially, in the proposed method, the estimate of the noise variance needs to be accurate enough, so as to help suppress the noise impact accordingly. It is better to incorporate jointly the learning of noise distribution and the regression/classification \nmodel, so as to optimize the tolerance against the data-dependent noise. \n2. It only considers the noisy learning process for regression. However, in classification scenarios, noise (with respect to labels) is usually presented in the form of label flipping. The proposed reweighing technique is not directly applicable in that case. Please refer to the following work for further reading: \nLearning with Noisy Labels, Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari, NIPS 2013. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Very well written paper, but I didn't find the contribution strong enough",
            "review": "##########################################################################\n\nPaper Summary:\nThis work targets regression tasks with noisy labels, and proposes to incorporate knowledge about the variance of the gaussian noise corrupting the observed labels to weight the loss function, at training time. The proposed method is evaluated in a series of experiments involving deep networks trained according to the weighted loss function, and compared to a baseline method that omits training samples that have a label noise variance larger than a threshold.\nResults indicate the proposed method is more robust to noisy labels when compared to alternatives that do not exploit the information on the noise affecting labels.\n\n##########################################################################\n\nReasons for score: \nOn the one hand, the paper is extremely well written and somehow pedagogic, in that it provides compelling motivations for considering heteroscedasticity, and its possible sources, and general, intuitive descriptions of the proposed method before specialising them to the instance they evaluate. On the other hand, I think the prose lacks sufficient technical depth, on the model they use, its relation to “text book” material on heteroscedasticity, e.g. for Maximum Likelihood Estimation (MLE), and on the properties of the proposed method. The experimental evaluation, while representing a reasonable starting point, is not sufficient to fully understand the behaviour and the properties of the proposed method.\nFor these reasons, I think this work cannot be accepted as is.\n\n##########################################################################\n\nPositive points: \n\n1) The editorial quality of this paper is high, and the overall motivations given to support the problem statement are compelling and well discussed. This also relates to the fundamental assumption underlying this work: access to privileged information, taking the form of knowledge of the stochastic noise variance affecting observed labels, at training time.\n\n2) The proposed method appears to be well positioned w.r.t. the recent literature on statistical modelling with noisy labels, especially concerning neural network based methods. It is unfortunate though that the literature scan doesn’t cover well-known approaches to tackle heteroscedastic noise in simple linear models, or in general MLE frameworks, which may be considered text-book material.\n\n3) The experimental evaluation considers two regression tasks on their respective UCI/Bike sharing, and UTK Face datasets, considering several variants of noise generation processes, affecting labels in different and sufficiently realistic manner.\n\n \n##########################################################################\n\nNegative points:\n1) My main concern is the “thin” contribution of this paper. The technical details of the proposed method are not sufficiently developed. Drawing inspiration from Fisher information calls for an appropriate discussion on the likelihood model, its noise model, to begin with. Then, I think the relation of the proposed idea to simple linear models, for which heteroscedastic regression has been studied in great detail (e.g. [1], for a general reference), and for MLE (e.g., [2]), would become more clear and would give the opportunity for the authors to develop what are the merits of their proposed method. For example, the weighted least square method is very similar to what is proposed in this paper. \n[1] Econometric analysis, Greene, William H, 2003, Pearson Education India\n[2] Maximum Likelihood Estimators with Heteroscedastic Errors, G. R. Fisher, Review of the International Statistical Institute\nVol. 25,  1957\nSee also “Pattern Recognition and Machine Learning”, Bishop, 2006, (chapter 5 and 6)\n\n2) The experimental evaluation is not sufficient to appreciate the virtues of the proposed method. For several noise distributions (including the additional ones considered in the supplement), the proposed method BIV does not seem to behave much better than the proposed baseline, that uses a simple threshold. Additionally, the figures are  cropped and do not allow to get a sense of what happens for all competing methods (Fig1 and Fig2). In Fig3, the figures report test loss and as there seems to be overfitting kicking in.\nAlso, it is mentioned the baseline method requires to set a cutoff parameter, but the proposed method also depends on a hyper parameter to optimise (done in the appendix). As a consequence, it is difficult to appreciate the main advantage of BIV w.r.t. the baseline.\nFinally, in Fig.3, there are clear signs of overfitting: why did the authors suggest (end of Sec.3) that their work does not require regularisation?\n\n\n#########################################################################\n\nMain criticism:\nI think, overall, the main criticism I have for this work is that the contribution is not sufficient. The main idea proposed in the paper fits sec 4.2, and it is based on well known results from text books. In eq.(5), the summation term is MLE with heteroscedasticity. The loss is scaled by a coefficient that collects statistics on the sample batch: if this batch is very small, or its elements not sufficiently diverse, I am afraid it could have a negative impact on the optimisation process (this is why, in the experiments, the authors chose a batch size of 256).\nOne possible way to overcome this criticism is to clarify the likelihood model, and compare the proposed method to existing approaches to address heteroscedastic Gaussian noise.\nA possible advice would be to reduce (or move to the appendix) the discursive parts on heteroscedasticity, and the general formulations (e.g., sec. 2.2, sec. 4.1), and gain more space to explain how BIV is different from what is known.\n\nAdditional comments:\nA note on experiments using the UTKFace dataset. In this case, the MLP used with 4 layers may be a bit “too simple”, in light of the high test loss on GT labels. Did you try with convolutional architectures, even simple ones?\nJust to clarify, the test loss reported in the figures, as a function of training steps: what is the *test* batch size? Same as training batch size? The test loss is computer according to \\sum_{k \\in \\text{test batch}} \\mathcal{L}(f(\\mathbf{x}_k, \\theta), \\tilde{y}_k) right?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}