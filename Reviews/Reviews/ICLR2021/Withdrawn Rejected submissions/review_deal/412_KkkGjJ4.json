{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents work on scene graph grounding under weak supervision.  The reviewers appreciated the consideration of this task and formulation of a solution for it.  However, concerns were raised over the importance of this weakly-supervised grounding task, how it addresses challenges in previous methods, the empirical evaluation, insights obtained, motivation, and clarity of exposition.  After reading the authors' response, the subsequent discussion and reconsideration resulted in a sense that while the task is new, the overall contribution and remaining questions over empirical evaluation mean the paper is not yet ready for publication at ICLR."
    },
    "Reviews": [
        {
            "title": "Problem is new, but the experiment is weak",
            "review": "**Summary**:\nThis paper proposes a weakly supervised scene graph grounding method. The setting of this paper is new and interesting, and I think this task would be beneficial to downstream tasks. The main contribution of this method is the mapping between visual objects and nodes. To bridge the gap between them, the authors propose two metrics and learn their model on a weakly-supervised dataset. The ablation study shows the effectiveness of each contribution, and they further apply the method to weakly supervised scene graph parsing.\n\n**Pros**:\n- This paper proposes the weakly supervised scene graph grounding task and formulates the mapping between scene graph and objects as a minimum match problem on a bipartite graph instead of graph alignment. \n- They train their model with two metrics (object-class based and relation-based) and search the minimum match in an iterative manner.\n- The visualization shows that nodes and objects can be well aligned.\n\n**Cons**:\n- What's the design of pool-based multiple instance learning? Can you provide more details?\n- The updating algorithm of the scene graph is not new; it is close to the graph encoding process; it updates the representations according to its neighbor. \n- In fig. 3, the authors mentioned that the boxes are ground-truth, while they also mentioned that the visual objects are extracted by off-the-shelf Faster-RCNN.\n- For VG and VRD, in my opinion, the objects have been semantically aligned with scene graph nodes since the objects are detected by the pre-trained detector (OpenImage). Why not use a scene graph generated from captions as the input.\n- Why the $\\lambda$ for VG and VRD is different, how to get those values?\n- As for visualization, the authors only show the scene graphs with the same objects. They should also show the images with objects, not in the scene graph.\n- The writing needs to be improved; for example, Eq. 11 should be exp(-d(u,v)).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New problem, interesting findings, valuable insights.",
            "review": "This paper introduces a new problem called weakly supervised scene graph grounding, which is potentially very useful but barely studied in the literature. Given a dataset of images labeled with scene graphs, but without bounding box annotation, the task is to learn to align each node of a given scene graph to the corresponding object in the image. This is useful for weakly supervised training of scene graph generators, as well as other applications such as multimodal grounding. The authors clearly define the problem and propose an evaluation setting for it. They also propose a new method for this task and compare to existing baselines. Their findings are interesting, novel, and potentially influential for future work.\n\nNevertheless, there are some confusions/weaknesses that can be improved in the rebuttal:\n\n1. There is insufficient insight about the proposed method and whether/why it works. Particularly, the authors first train a weakly supervised object recognition model using a multiple instance learning framework (Eq. 3), then use that to align ground truth nodes to objects (Eq. 9), and then use this alignment as a ground truth for training the graph neural networks (Eq. 10). However, the ground truth alignment produced by the object recognition model only takes into account object classes and not their relations, while the goal of the graph neural networks is to incorporate relations to disambiguate between instance of the same class. Hence, the ground truth is inadequate to learn the task and the model may just learn to ignore relations and use objects alone to do the alignment. Experiment results (table 2) confirm this concern to some extent, as the ablation without relational metric (no graph neural net) achieves very close performance to the proposed model. Therefore, it is essential for the authors to justify their proposed model e.g. using extra experiments/analyses/visualizations, to prove that the graph neural networks indeed does learn to take relations into account for grounding objects.\n\n2. The writing of the paper can be improved. It is not very clear, except from Algorithm 2, that the training framework has 2 phases: training the object recognition model, and then training the graph comparison model based on that. The authors may add an overview to explain this. There are also notation issues and confusions in the equations. For instance, the definition of U, N, and Y below Eq 3 is confusing. Also it is not clear what is the input set to \\sigma (pooling). The definition of \\pi in Eq. 8 is also not precise enough. And in Eq. 10, the second equation is confusing, maybe the \\lambda inside the sum should be removed. Furthermore, Section 6 should be elaborated more. For instance, it should be made clear that this makes the training a 3-step process: learning object recognition, then learning the graph metric, and then using that to learn the scene graph generation model. \n\n3. Experiments provide new findings and useful insights, but they can be improved. For instance, it is not clear why, even though the proposed alignment method is far better than the VSPNet baseline in table 1, its effect on the scene graph generation performance is small (table 3). Moreover, the SGCls and PredCls evaluation metrics are both unrealistic metrics that assume ground truth bounding boxes are available during inference. The authors are encouraged to report SGDet, to show how effective the proposed method is in real-world settings.\n\nDespite the suggested improvements in experimentation, writing, and justification of the method, this paper carries a significant value for the community, as it calls attentions to a new task, that is barely studied, but has potential applications. The empirical findings of the paper provide new insights that do not exist in the literature, and can directly guide future work. Hence, my recommendation is to accept this paper. \n\nThe authors are strongly encouraged to address the concerns mentioned above. I also recommend resolving grammar mistakes throughout the paper, although that should not affect the final decision. Here are some more suggestions to improve the paper:\n\n1. The authors are encouraged to more extensively justify why this task is important and difficult. They did provide an example of a person riding a horse while another person is riding a bike, but that single example may not be convincing enough for others to pursue further research in this direction. The authors may provide some statistical analysis of how many difficult grounding cases exist in the dataset, what makes these cases difficult, and how improving the grounding can improve downstream tasks such as scene graph generation.\n\n2. There are some recent papers on visual grounding by incorporating relationships between objects, such as [1]. I recommend including a comprehensive review of such papers and discussing their connections and differences with this work.\n\n[1] He, Chuanzi, Haidong Zhu, Jiyang Gao, Kan Chen, and Ram Nevatia. \"CPARR: Category-based Proposal Analysis for Referring Relationships.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 948-949. 2020.\n\n\n######## Post-Rebuttal Updates:\n\nThe authors have addressed most of my concerns, and I appreciate their effort. I am not convinced that early-stopping alone can help the model learn to utilize relation information, when the target ground truth does not require the model to do so. However, I recommend accepting this paper as it introduces a new task and presents strong results. \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "see comments",
            "review": "Summary:\nThis paper presents a method on the task of Scene Graph Grounding, where the object and (object, object) relationship ground-truth labels are only at the image-level but not localized, i.e., instance-level. The core of the proposed method is the Wasserstein distance minimizer (aka, Earth Mover's Distance) for two object pair sets: one is from visual detection, and the other is from the textual nouns. The initial cost for each set is initialized by MIL probabilities. Experiments demonstrate a considerable performance boost over a SOTA (Zareian et al., 2020b)\n\nStrength:\n1. Strong performance in Table 1 (I do not mean that the experiments are comprehensive, see weakness below)\n\n\nWeakness:\n1. The motivation is not clearly justified in the Introduction. For example, for readers not familiar with this task, the authors may want to provide a clear illustrative example of this task, to show why scene graph grounding is challenging and significant. Moreover, compared to (Zareian et al., 2020b), whom the authors are targeting, in fact, I see no obvious weakness in their method and how you resolve it. \n\n2. I am not objecting to use the simple EMD to solve weakly supervised matching problems, as it is straightforward. However, I feel that the authors are just using the right tool to do the right job, without convincing the readers why this tool is sufficient and necessary, especially compared to many other possible tools, such as using GNN, (Zareian et al., 2020b), MIL (Zhang et al. 2017b), and graphical models (Justin et al, Image Retrieval with Scene Graphs, CVPR'15). \n\n3. The overall pipeline is not clearly illustrated. For example, after the EDM matching, how to obtain the final scene graph alignment?\n\n4. Experiments are not comprehensive. First, I'd like to see visual qualitative examples of the success and failures of grounding, especially for improving weakly supervised SGG. Second, ablations such as using different metrics for the matrices are missing.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting and crucial task, but the novelty of idea is limited.",
            "review": "###############################\nSummary:\nThis paper focuses on a new scene understanding task: Weakly-Supervised Scene Graph Grounding. Given the image and a paired scene graph, this task needs to link each scene graph node to an object in the image. Specifically, it regards this problem as a bipartite graph matching problem, and designs a cost function considering both object-class similarity and relation-based similarity. Experiments on two datasets (Visual Genome and Visual Relation Detection) have demonstrated the effectiveness.\n\n################################\nAdvantages:\n+ The paper is the first work to explore the task: Weakly-Supervised Scene Graph Grounding. This task can serve as a crucial preprocessing step for weakly-supervised scene graph generation: After solving this grounding task, the weakly-supervised scene graph generation task can be easily transferred to fully-supervised scene graph generation by regarding the grounding results as ground-truth annotations.\n\n+ The whole paper is well-written and all notations are quite clear.\n\n#################################\nWeaknesses:\n\n- The novelty of the idea for weakly-supervised scene graph grounding is limited. It regards this grounding task as a bipartite graph matching problem, which is very similar to the existing work for weakly-supervised scene graph parsing (ie, VSPNet). \n\n- Although this paper is the first work to explore weakly-supervised scene graph grounding task, the task is very similar to the existing weakly-supervised phrase grounding task.\n\n- The comparisons between the proposed method and baselines are not fair. For example, in Table 1, for the baseline WS-VRD [Baldassarre et al., 2020] model, it only utilizes predicates as input for weakly-supervised learning, while the proposed model uses the whole scene graph as input.\n\n- The details of some experiments are not clear. For example, for the compared baseline VSPNet [Zareian et al., 2020a], the authors use two variants of models (VSPNet-v1 and VSPNet-v2), but the details of these two variants are not clear, ie, how to change an scene graph parsing model (VSP) for the scene graph grounding task.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}