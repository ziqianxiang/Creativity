{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Being able to give confidence intervals or have a robust measure of uncertainty is very important for offline RL methods. In this work, they proposed a dropout based method to have a measure of uncertainty. The authors provide an significant empirical improvements over other baselines. Nevertheless, as it stands right now and as AnonReviewer5 have pointed out, this paper has some important shortcomings. I have noticed that the authors have updated the paper, but still the some of the important points made by AnonReviewer5 are unaddressed as it stands right now. Thus, I am suggesting to reject this paper hoping that the authors will address those issues and resubmit to a different venue.\n\nFirstly, I agree with AnonReviewer5, it is not clear if the dropout and the variance trick used in this paper actually represents epistemic uncertainty that we would like to have for an offline RL algorithm, because the variance do not necessarily need to shrink as you train it with more data, and as opposed to supervised learning setting it is not clear what type of uncertainty the proposed dropout method will induce in the offline RL setting. It would have been nice to have some results showing how calibrated the uncertainty estimates coming from the dropout is... I would recommend the authors not to include any claims regarding the epistemic uncertainty in the camera-ready version of the paper.\n\nAlso as AnonReviewer5 pointed out, having distributional baselines and/or ensemble methods like REM or bootstrapped DQN would be a more fair comparison. So, it would be nice to see some of those baselines in a future version of this paper."
    },
    "Reviews": [
        {
            "title": "Not clear where the benefits are coming from",
            "review": "This paper proposes to use an uncertainty-weighted objective for offline RL with BEAR (Kumar et al.) that penalizes the MMD distance between the learned policy and the previous policy. The uncertainty weighted objective weights the policy improvement objective with the variance in the Q-function, where this variance primarily represents aleatoric or intrinsic uncertainty, not the epistemic or belief uncertainty. They show that their method performs reasonably better than prior methods on the D4RL datasets and show that the learned Q-values are better than BEAR.\n\nWhile the work seems promising empirically, I have a number of concerns about the method:\n\n1. The division by variance comes across as a hack -- while standard deviation could be more appropriate, I wonder why there is no need to even center the Q-function before dividing by the variance. Moreover, what does the current weighting do? Does it penalize specific state-action pairs? Can it be compared to a more standard square root inverse-counts style weighting in a tabular setting? The lack of such ablation experiments make it impossible to interpret the method.\n\n2. Typically the notion of uncertainty in batch RL or exploration is epistemic uncertainty, where uncertainty stems from the fact that there is limited data available to the agent. The notion of uncertainty or variance considered here is not this notion. Since there are no time-consisent backups, it is unclear how epistemic uncertainty is being preserved termporally in a trajectory. The method, as it stands, seems to resemble aleatoric uncertainty more, and if this is the case, why not just use distributional critics? What is the specific beneift from dropout-based Q-functions?\n\n3. Ablations on uncertainty: the method uses both a constraint against the behavior policy and a weighting on the policy objective. Which of these helps more and when? The Q-value plots shown in the paper are reasonable, but it is still unclear when the uncertainty still helps and why, and more understanding is needed on this part in my opinion. In fact, the Q-values plots perhaps signify my belief about 2, that their approach works because it performs \"better\" Q-learning similar to Agarwal et al. 2020. \n\nOverall, I do not buy the claim that the real benefits are coming from uncertainty based Q-functions, but perhaps more so like a stronger critic function. The paper doesn't ablate over showing the benefits of uncertainty over a policy constraint or comparing to a method that actually learns Q-value estimates aware of the epistemic uncertainty making it hard to compare to these. Finally, the method does come across as adhoc. Is it possible to test the method on some Atari domains, which have some different properties than the continuous control D4RL benchmarks? This would validate if dividing by the variance is indeed transferable across domains.\n\n\n------------Post Rebuttal----------------\n\nI thank the authors for their response. I disagree about epistemic and aleatoric uncertainty. Bootstrapped DQN (Osband et al. 2016), Randomized prior functions (Osband et al. 2019), and several other works show that to get the variance of different possible Q-functions given the data, $p(Q|D)$ or differnet possible MDPs given the data, $p(M|D)$, you need backups consistent in time, i.e. the same dropout mask is to be used for both the main Q-network and the target Q-network for the backup. This is the uncertainty that we trypically need in offline RL and is also used in Theorem A.2 (for the high probability bound which is given the data), and that's why ensembles with Q-functions consistent with themelves are typically used. By merging the target values across different dropout masks, the uncertainty is not timewise consistent. While the paper that the authors point does actually do dropout masks with Q-function at each step, it is discussed in later works including Osband et al 2016, 2018 that not being consistent over time is leading to wide uncertainty estimates.  I would recommend the authors read the discussion on Posterior sampling for RL vs Optimism and Thmpson sampling for a discussion on this.\n\nI am a bit disappointed with the rebuttal. I expected a comparison to such metods that perform timewise consistent uncertainty estimation and also to distributional RL, since the algorithm that the authors use can also be drawn similar to a set of particles of Q-functions and performing a backup using target values computed using all of the possible particles, which is essentially what, for instance, QR-DQN does in a way or even IQN does in a way. Even REM would have been fine. Without this comparison, I unfortunately cannot increase my score and I am going to retain my score.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Proposes novel algorithm for incorporating uncertainty estimation in offline-RL",
            "review": "Summary :\nThis paper considers the problem of dealing with uncertainty for static datasets in offlineRL. The authors propose a novel algorithm ‘UWAC’, uncertainty weighted actor-critic. UWAC takes a Bayesian perspective of RL, and uses Monte Carlo dropout for detecting, down-weighting OOD samples. Building on BEAR, they estimate the epistemic uncertainty as the Var(Q(s, a)), and modify the update to downweight samples with higher variance. On D4RL datasets, UWAC seems to perform competitively against model-based baselines like MOPO, model-free baselines like BEAR/CQL.\n\nReasons for the score:\nI vote for accepting the paper, with some improvements. The paper suggests a strong improvement in the stability and performance of model-free algorithms for offlineRL. I would strongly encourage the authors to improve the writing and presentation of the algorithm in favour of clarity. \n\n\nStrengths:\n+ UWAC bridges the gap between the performance of model-based and model-free algorithms in offlineRL. \n+ The problem is well motivated, and uses dropout variational inference to estimate the epistemic uncertainty and imposing a soft-penalizing on OOD samples.\n+ Provides extensive experiments and empirical evidence on the D4RL benchmark, where it outperforms MOPO and appears to be more stable than BEAR.\n\nWeaknesses:\n- The baseline for MOPO is missing from the plots. As we are considering the stability of the algorithm as compared to other baselines (like BEAR) it would be critical to compare the same for MOPO (trained for a similar number of epochs).\n- Drop in performance seems to be significantly related to exponential increase of the Q-values. Taking this into account, I would consider an implementation with some variant of clipping (gradient or reward) to be a strong missing baseline. \n- Some sections were hard to follow, particularly Section 3, Section 4 could be substantially improved for clarity. Comparison among related work could probably be represented via a table of tradeoffs to better understand the strengths, nuances of the proposed algorithm. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple dropout method for offline RL with uncertainty estimation",
            "review": "##########################################################################\n\nSummary:\n\n\nThis paper shows that a easy-to-implement dropout to estimate uncertainty can address the value overestimation problem in offline RL for OOD action or states. The paper baselined against the results from D4RL and showed to outperform those more than half of the times.\n\n \n##########################################################################\nPros: \n\nUncertainty is an important yet underexplored domain in Offline RL. Having a simple method that can be applied to actor-critic based methods is attractive. \n\nAs far as I know, there hasn’t been literature that combines dropout and Offline RL uncertainty estimation in the loss function\n\nThe performance of the algorithm is decent on D4RL, a standard offline RL baseline.\n\n##########################################################################\n\nCons: \n\nSince offline RL is motivated by a potentially costly evaluation and/or data collection process, It is risky to add additional hyperparameters in offline RL. Thus it would be informative to mention the following in the paper: Is your model sensitive to the choice of beta? How many values of beta did you sweep over? \n\n\n##########################################################################\n\nQuestions during rebuttal period: \n1. I could be missing something. Since the beta hyperparameter is a constant in all losses. Can it be absorbed into the learning rate?\n\n2. In the appendix, I found theorem A.2 confusing. It assumes that Q is bounded for any (s,a). this assumption makes the proof almost trivial. The given justification is that Q can be bounded with spectral norm, yet in practice spectral norm performs much worse than no spectral norm. I thus find the theoretical backing shaky, especially you further assumed in the paper that Z can be absorbed into beta. Can you expand more on this?\n\n3. Could you address the Cons above?\n\n##########################################################################\n\nAdditional suggestions:\n\n1. Can you add a graph in the appendix where you zoom in a bit on the Q target? I want to see whether it still overestimates, and if so by how much. Having the y axis = 1e12 hides those information.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "The authors present a simple and efficient method for training offline RL agents. They estimate the epistemic uncertainty of their model through dropout  variational inference. While this method is not novel, it has never been applied to offline RL as far as I know. On the basis of this epistemic uncertainty estimate, they regularize the policy search to avoid sensitivity to overestimates in poorly known states. The way this regularization is performed is taken from [Kumar2019]. The authors then experimentally demonstrate the effectiveness of dropout uncertainty estimation for RL, and achieve the best performance on a classic offline RL benchmark, with the best offline RL algorithms for continuous state-action MDPs.\n\nThere is not much to say about the paper. It is well written and positioned. One could that the method is the straigthforward application of dropout variational inference to [Kumar2019] algorithm, but it remains that the empirical results are quite significantly improving the state of the art, and as such, it deserves to be known.\n\nI only have one minor remark to share: Section 4.1, the explanation of the variance decomposition is a bit misleading. Red minus blue is the dropout variational variance, and this is what measures the uncertainty of the model (not singlehandedly the red term). ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple, empirically working solution for offline RL",
            "review": "This paper proposes a simple offline RL algorithm, called UWAC, which down-weights the bellman errors for uncertain state and action pair while the uncertainty is measured by MC-dropout. The algorithm empirically shows competitive performance in various test domains in the D4RL benchmarks, including Adroit hand manipulation tasks. Overall, I agree with the main motivation for the paper that tries to estimate and use uncertainty, and it seems that the method empirically achieves the goal. Still, I have a concern about the theoretic validity of MC-dropout in the Q-learning setup and the clarity of the papers (especially typos and proofs in Appendix). Hopefully, the authors can address my concern in the rebuttal period.\n\nConcerns:\n* It is unclear whether MC-dropout measures epistemic uncertainty precisely even in Q-learning in which bootstrapping is in use; since the target Y for certain input X depends on its own estimation, MC-dropout could become unreliable. In the meantime, the same algorithm could leverage uncertainty measured for transition dynamics with proper scaling as it is done for $Q$. Why can measuring epistemic uncertainty for $Q$ via MC-dropout be valid (or effective)? What is the main advantage (or motivation) over other uncertainty measuring methods (such as deep ensemble) or other domains (such as transition dynamics learning)?\n* Is there a fundamental reason for using BEAR as a base algorithm to work upon? How well will the raw UWAC -- without MMD penalty of BEAR -- perform? As it is mentioned in Section 3.2, the support constraints via MMD is rather empirical and could be wrong in some cases. If raw UWAC fails, then what will be the main cause?\n\nMinor Comments:\n\n* Section 4.1, Variance equation: what is $\\sigma^2$, what is the theoretic impact of this term, and how is this value decided from the implementation perspective? Also, why $\\hat{Q}$ is in a vector notation (transpose)?\n* Algorithm 1: is $Z$ approximated with $\\{a_i\\}$, or is $Z$ simply ignored?\n* How to set a proper $\\beta$, and how sensitive is UWAC to the hyperparameter?\n* Section 5.1: what do you mean by the ‘final’ replay buffer? Does this mean that the last 100,000 transition tuples are saved for the offline RL dataset? Then, why do trajectories gathered by executing nearly expert policy have relatively complete coverage over the observation space? The expert policy won’t execute the action that is suboptimal, so the coverage would be limited. Also, it would be nice if you can include the uncertainty figure of Q which is trained without skewing.\n* Figure 5, pen-human-v0 results: what is the possible reason for the degeneration of UWAC? Wrong OOD detection?\n\nTypos:\n\n* Equations are the components of a sentence, and every sentence ends with proper punctuation; add proper punctuation before or after every equation.\n* Section 3.1 -- $\\gamma \\in (0,1)$ → $\\gamma \\in (0,1]$. \n* The second paragraph of page 5 -- erase parenthesis around $\\pi$.\n* For the completeness of the paper, please include all loss functions used in the algorithm; the exact MMD penalty term is missing in Algorithm 1.\n* Target network update should be $\\theta’ \\leftarrow (1-\\tau) \\theta’ + \\tau \\theta$, if $\\tau$ is a polyak coefficient used for slowly moving target network. Also, just use $\\theta_{1,2}$ instead of $i$. $i$ is used in another context, so this is confusing.\n* In Algorithm 1, $\\phi_i$ in line 12 is a typo, and $\\{ a_j \\sim D\\}_{i=1}^{n=10}$ should be $\\{a_j \\sim D\\}_j^n$.\n* In Tables 1 and 2, when your results are the best, the results that lie in some confidence interval of yours should also be bolded for a fair comparison.\n* The second paragraph from the bottom of page 7, “when Q Target” → “when Q target”, “and actions: As performance” → “and actions: as performance”.\n* It would be nice if Figures 7 and 8 in the appendix can be integrated into a single figure.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}