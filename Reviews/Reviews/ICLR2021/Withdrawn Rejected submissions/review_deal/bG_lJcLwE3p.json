{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviews are a bit mixed. While all the reviewers feel that the paper proposed an interesting mechanism to train conditional generators from a single image and demonstrated good image editing results in the experiments, there are also common concerns about the practicality of the proposed method for interactive image editing. All the reviewers asked for the computation time, and some expressed the concerns about technical contributions. While these concerns were (somewhat) addressed in the rebuttal, the AC feels that it’s a hard sell to bet on the dramatic increase of computational capacity to make the computing time from an hour to realtime. Concerns about novelty also remained. Given the drawbacks, the final decision was to not accept. However, this work is promising and can be made stronger for publication in a later venue."
    },
    "Reviews": [
        {
            "title": "The proposed method in this paper demonstrates its effectiveness in single image manipulation task through various results, including both quantitative and qualitative experiments. However, additional descriptions on the effectiveness of the objective function and a primitive representation are required to make this paper convincing. Therefore, I vote for ‘Marginally above acceptance threshold” for this submission, but I may reconsider my assessment if all the concerns are resolved.",
            "review": "This paper proposed a single image-based manipulation method (DeepSIM) using conditional a generative model. The authors addressed this problem by proposing to learn the mapping between a set of primitive representation, which consists of edges and segmentation masks, and an image. They also adopted a thin-plate-splines (TPS) transformation as augmentation which enables the model to robustly manipulate an image by editing primitives.\n\nPros\n-\tThis paper is clearly written and easy to follow.\n-\tThe authors proposed a novel conditional manipulation method based on a single image, which is new in this area.\n-\tDeepSIM is capable of generating the plausible results by manipulating its contents in both a low and a high-level manner, maintaining its realism and fidelity.\n\nCons\n-\tThe authors need to clarify why the VGGNet-based perceptual loss encourages the model to maintain the fidelity. As Johnson et al. [1] argued that this loss is defined as humans’ perceptual difference between the images, it would be helpful to further explain how the model with this loss between G(x) and y better reflects the primitive representations in the generated output than the model without it. Additional explanation and experiments, such as ablation study, would make the paper convincing. \n-\tCompared to the segmentation, the edge map seems to less contribute to the image manipulation. Most of the results are mainly attributed to the segmentation changes, and only slight modification is caused by the change of edge primitive, as shown in Appendix. A “Removing the teeth of the top lama.” Additional qualitative results for drastic manipulation in a low-level manner caused by the edge primitive would be necessary.\n-\tI think the technical novelty of TPS-based augmentation in the paper is not significant in that the TPS transformation has been widely used in existing literature [2][3] for learning correspondence between two images.\n-\tAs the authors mentioned in the conclusion, training a network for a single image manipulation would be a critical bottleneck in practical use. In this respect, the training time on a single image should be reported. Additionally, detailed descriptions how to obtain the primitives (edge and segmentation) for the input image would be required.\n\n[1] Johnson et al., “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.”, ECCV’16\n\n[2] Han et al., “VITON: An Image-based Virtual Try-on Network.”, CVPR’18\n\n[3] Lee et al., “Reference-Based Sketch Image Colorization using Augmented-Self Reference and Dense Semantic Correspondence.”, CVPR’20\n\n-------------------------------------\nAfter rebuttal:\n\nThank you for the dedicated consideration of my comments, but there are a few remaining concerns that are not clear.\n\n1. The editing effects of edge maps are not distinct from those of segmentation maps. More specifically, except for the background in Fig.3 and the second face in App.D, most of the examples demonstrate the same kinds of manipulation as shown in the samples of the segmentation map manipulations. This includes moving, stretching, and erasing the objects. I think that the qualitative results of edge modification are not sufficient to prove its effectiveness, compared to those of segmentation maps.\n\n2. As shown in the image segmentation video the authors provide, a user needs to segment every single object which are selected for the manipulation. Moreover, segmenting small and fine objects requires further elaborate and laborious annotations from the user, resulting in a critical bottleneck for practical use.\n\nDue to these concerns, I would keep my previous rating of “6. Marginally above acceptance threshold.”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns on super primitive generation, technical contribution and qualitative evaluation",
            "review": "This paper provides an augmentation method to enable single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. During manipulation, the generator allows for making general image changes by modifying the primitive input representation and mapping it through the network.\nOn the positive side:\n- The paper proposes an interesting mechanism to train conditional generators from a single image.\n- The proposed super primitive works well for single image manipulation tasks.\n- Some good image editing results are shown in the experiments.\nOn the negative side:\n- The method requires a professional editing ability for editing edges of a super primitive. The generation of primitives also highly depends on the accuracy of semantic segmentation. If the segmentation is done manually, the editing process maybe time-consuming.\n- The technical contribution is limited. It seems that the kernel of the framework is a direct use of cGAN without introducing many new ideas.\n- The training speed is not reported. The reviewer thinks that a fast training process is important for single image manipulation.\n- The qualitative evaluation is not very convincing. It’s better to conduct a user study.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting take on single-image generative models",
            "review": "<Paper Summary>\nThis work proposes a method to design conditional generative models based on a single image. In particular, while some recent models have enabled one to sample (unconditionally) images from a generative model learned from a single image (like SinGAN), this work explores a way of conditioning the generation on a primitive, which can be user-specified. As a result, one can produce realistic modifications to a given image by modifying - or sketching - some primitive.\n\n<Review Summary>\nI like the simplicity and the ingenuity of the approach. This reviewer is not aware of any method that can produce similar results, and as such it represents the state of the art for deep-learning based single-image manipulation. At the same time, clarity (of both writing and technical aspects) could be significantly improved. \n\n<Details>\nStrengths:\n* Novel formulation to train 'single' image generative models\n* Flexible framework\n* Compelling experimental results\n\nWeaknesses:\n* While it is true that this model trains a generative model from a single image, it does so by deploying traditional large-scale learning on many modified versions of a single input image. This is in contrast to other methods (DIP, SinGAN) that train completely on *a single image*. This distinction could be made clearer in the text.\n* Many comments and expressions are too vague, making it difficult to fully understand the approach: In GAN models, the discriminator, $D$, is typically a deep-network model parametrizing a function from the space of images to the reals, say $R^n \\rightarrow [0,1]$, representing the probability that the input comes from the distribution of real images as opposed to from a synthetic generator, $G$. In Eq. (2), however, the discriminator model seems to receive two outputs, $(x,G(x))$, which makes little sense to me. Did the authors perhaps meant to write simply $D(y)$ and $D(G(x))$ in Eq. 2? More broadly, this confusion comes from a lack of a clear definition of the employed functions.\n* Also in Eq. (2), GANs typically have a large collection of training samples and so writing the expectation over the distribution of images $p_{data}$, makes sense. In this case, however, one has only 1 sample. It's true that the authors are artificially creating a distribution around the given sample, so perhaps they could make this more precise and clarify what they refer to as $p_{data}$.\n* It would significantly help the presentation to define the different quantities and spaces used by the authors. For example, consider defining the domain and codomain of the warp $f$, which they employ to generate the augmentation. It is also not totally clear how these transformations are sampled (the $t(i,j)$'s). Later in Sec. 3.4 the authors mention that they randomly sample a new TPS wrap, but sampled how and from what distribution?\n*  There's no comment as to how computational intensive the method is: How many new samples (augmentations) are generated for every image? How long does it take to generate these, and how long does the subsequent training take?\n* \"The long tail of images\" doesn't mean anything to this reviewer. It is clear that the authors intend to refer to images that occur very infrequently in the distribution of real images, but the authors should make this more precise and avoid comments of the sort of \"primitive from the long-tail\", which make no sense.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}