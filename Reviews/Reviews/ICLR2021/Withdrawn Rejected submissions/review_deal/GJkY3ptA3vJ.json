{
    "Decision": "",
    "Reviews": [
        {
            "title": "Lacks comparison to similar approaches, low originality",
            "review": "Summary. The authors introduce the concept of robustness of representations in NLP, and propose an approach based on contrastive learning to achieve robust textual representations. Inspired by the work of Wang & Isola (2020), they propose to look at the \"alignment\" and \"uniformity\" components of the standard contrastive loss separately. Using an architecture inspired by BYOL (Grill et al., 2020), alignment is achieved by an MSE loss of the l2 normalized embeddings, while uniformity comes from the use of a variant of batch normalization. Synthetic positive instances are obtained via synonym replacement.\n\nStrengths:\n1. An interesting type of approach to unsupervised representation learning, which has seen a lot of success in computer vision recently. It's great to see people trying this in NLP.\n2. A nice analysis which clearly shows the effects of different types of normalization.\n\nWeaknesses:\n1. Low on original content. The proposed method is very similar to BYOL (Grill et al., 2020), with the only substantial changes being the use of different encoders (transformer-based rather than convolutional residual networks) and the use of PowerNorm instead of batch normalization. One area where I would have really liked to see more done is data augmentation, a traditionally tricky problem for NLP. The authors barely discuss this aspect, and adopt a very simple setup (synonym substitution) without trying other widespread approaches such as backtranslation.\n2. Poor presentation. A lot of the theoretical background is taken from the cited papers with minimal changes, resulting in Sections 3 and 4 looking like a bit of a patchwork. An example of this is Eqn. 9, about PowerNorm, which is identical to what's in the original paper but is placed there without defining a single variable. Similarly, Section 3 very closely follows the presentation of Wang & Isola (2020), but while it's very clear in the context of the original paper, it is hard to follow when transplanted out of context.\n3. Lack of comparison to related approaches. There are many alternative approaches to contrastive learning for NLP, many of which the authors cite in Section 3. These should have been benchmarked against the proposed model. Without this, it's hard to see what the proposed model brings to the table.\n\nMinor remarks:\n* Tables 1, 2, 3. Pet peeve – if bold is going to be used to highlight the best performance, then it should be used universally and not just when it is the authors' proposed method that achieves top performance.\n* Eqn. 3. Missing summation indices.\n* Section 4.2. BRET => BERT.\n* Sentence at the bottom of p2: I would not say this is a fair statement. There are many other approaches to textual contrastive learning which may very well prove to be more robust than the one proposed in this manuscript.\n\n**Post-discussion comments**: Having read my colleagues' reviews, and given the lack of response from the authors, my rating remains unchanged and I do not recommend that this paper be accepted",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting perspective towards learning robust language representations with contrastive training approach",
            "review": "The paper propose a method to improve the robustness of pre-trained NLP models. The key method is to design the contrastive training objective with two components. The first component is to enforce a sentence and its augmented sample close to each other in the embedding space. The second is to ensure the extracted feature follow the uniform distribution. The method is evaluated on a variety of benchmark tasks including GLUE, SQuAD, and SNLI. \n\nStrong points of the paper:\nS1. The goal of robust representation learning for text is a critical topic to study. \nS2. The paper propose to use power normalization to achieve the uniformity in the learned representation. \nS3. Experiments are comprehensive with a variety of benchmark tasks. \n\nWeak points of the paper:\nW1. The main method follows much the framework of Wang & Isola 2020 and Grill et al 2020. \nW2. Some details of the method are missing, which makes understanding and reproducing difficult. For example, what is g in Eq(8). Many symbols in Eq(9) are not explained. \nW3. Some claims are not supported. In the abstract, it states that \"our methods can obtain better results compared with the baselines\", while in the experiment it is not the case (see Table 1 on GLUE, Table 2 on SQuAD, Table 3 on CoLA and SNLI). There is not one setting that shows the proposed DCL is superior. \nW4. There are components in the objective. It would very interesting to analyze the effects of each. However, there is not much analysis regarding this aspect. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "The main focus of the current work is the introduction of a new model for learning robust text representations. In particular, the authors present two-folded disentangled contrastive learning method, which aims to shorten the distance between similar semantic features that contribute to feature alignment while aims to have features with uniform distribution on the unit hypersphere.\nThe authors used WordNet as a semantic resource for synonyms and antonyms for permutations.\nThe resulting model is evaluated in three standard NLU benchmarks, GLUE, SQuAD and SNLI and compared against BERT, a BERT trained with augmented data and DCL, and tested out in two configurations, normal and robust (with invariance tests and adversarial attacks). \n\nThe overall presentation is regular. Some of the claims made by the authors are arguable (see below), which hurts the overall presentation. The introduced idea, to my knowledge, is new, but seems a combination of existing approaches, (contrastive learning and feature alignment while keeping feature uniformity). A recap of approaches with some of those techniques (but not all) will strengthen the work.\nFinally, the results are inconclusive. It is not possible to to extract a clear trend from the experiments and the gain in performance, where is present, seems to be marginal.\n\nSome comments\n\nI find particularly problematic the claim that inspired the work \"a robust representation should be similar in the feature space with subtle permutations, while large variations occur with different semantic meanings\". That's not necessarily true. Also, significantly different lexicalizations of a sentence might share the exact same semantics. \n\n\n\"Definition 1.Robust textual representation indicates that the representation is  [not?] vulnerable to small and imperceptible permutations originating from legitimate inputs.\" I guess the definition is wrong. A robust representation should not be vulnerable to small changes.\n\nSome typos and formatting recommendations\n\n \"feature uniformity Wang & Isola (2020)\", put parentheses on the whole cite  \n\n\"in the original loss of BRET Devlin et al. (2019a)\"\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Vague problem formulation, unclear methods and weak experiments",
            "review": "This work proposes a method to learn robust text representations based on contrastive learning. Proposed method is shown to be better than BERT on some NLP benchmarks.\n\nPros\n- Learning representations that are robust to perturbations is an important problem. \n- Experiments show some improvements over a BERT baseline on NLP benchmarks.\n\nCons\n- Vague problem formulation\n- Modeling choices not motivated well\n- Presentation can be improved \n\nProblem formulation\n- It is unclear to me what problem this paper is trying to solve. The term ‘robust representations’ is quite broad. The paper talks about robustness to adversarial perturbations, robustness to ‘permutations’, robustness to the test distribution during fine-tuning, etc. so I found this quite confusing. Why is robustness to permutation desired? (Assuming permutation refers to permuting the words of text). It would help to give some examples to aid the discussion.\n- In section 3 the text says “g(.) takes input from x and outputs a valid probability distribution for tasks”. I don’t understand what this means. \n\nModels\n- My biggest concern is, it is unclear to me what’s new in this paper and how it compares to Wang & Isola. Is it a direct application of their method to NLP tasks? The authors should also do a better job at separating prior work from their contributions. Some parts of section 3 are directly from Wang & Isola, and this is not explicitly mentioned or made clear. \n- Why is momentum representation consistency used here? I couldn’t find enough motivation for this and felt like an arbitrary modeling choice. \n- Section 3 and 4 are quite confusing. I can’t find the definitions for some notation used (eg. equation 9). Description is also hard to follow. \n\nExperiments\n- What is the ‘robust’ setting here? I couldn’t find enough details in the paper.\n\nPresentation\n- Presentation lacks clarity and paper is hard to follow in general. \n\nIn summary, the paper doesn’t manage to clearly define the problem it is trying to solve. It is unclear what’s new here and how the paper compares to Wang & Isola, and many of the modeling choices are poorly motivated/described.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}