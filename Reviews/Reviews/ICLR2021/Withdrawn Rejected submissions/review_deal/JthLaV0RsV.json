{
    "Decision": "",
    "Reviews": [
        {
            "title": "Addresses important problem, but approach may not generalize to other dialogue tasks",
            "review": "Summary: \n\nThe paper addresses the problem of inconsistency and repetition with large-scale language models applied to dialogue. It uses an RL objective to refine MLE-based models and does so without any user simulator. It also proposes several reward functions to identify repetitions and inconsistencies. Experiments on the PersuasionForGood dataset show improvements over two previous approaches (MISSA and ARDM).\n\nPros:\n\n- Improvement over the state-of-the-art on one dataset, both in terms of automatic and human evaluation. However, the paper's approach relies on some heuristic components and relies on an ontology, so it isn't clear how applicable the paper would be to other dialog tasks or datasets.\n\n- A human evaluation that considers different qualitative aspects, such as non-repletion, consistency, and fluency. The paper shows improvements in the two dimensions it proposes to improve, i.e., consistency and non-repetition.\n\n- An ablation study. Unfortunately, that study shows that the model performing best in terms of overall experience is the one that disables RL training (i.e., the best system is similar to prior work – ARDM – with classifiers added to the pipeline).\n\n- The paper is generally well written and clear and provides numerous examples.\n\nCons:\n\n- Novelty: the paper does make several contributions, but each of them has relatively limited novelty. (1) RL finetuning essentially applies proximal policy optimization (PPO) (Shulman et al., 2017); (2) Some of the reward functions are quite simple, e.g., a Jaccard coefficient that ignores non-lexical repetitions. (3) The potentially more interesting reward functions rely on the Profile Builder, which is built on top of an ontology (which presumably would need to be adjusted or replaced for other tasks and domains). More generally, it seems the authors built a pipelined architecture with classifiers (response filter, response imitations) and heuristics and hand-coded attributes (e.g., \"For simplicity, we only track five attributes in the top grey table in Figure 1\") that don't seem to offer a great deal of generality.\n\n- Comparison with related work: A main claim of the paper is that it doesn't require user simulators and is, therefore, more practical than, e.g. (Li et al., 2016), as user simulators for dialogue can be intricate to design. I agree that user simulators are difficult to design and complicate the evaluation and experimental design. However, disabling interaction with user simulators (or real users) requires training on existing demonstrations (which they do in lines 5-10 of Algorithm 1), which effectively disables the multi-turn exploration aspect of RL (and specifically of Li et al.). That is fine to do, but that also makes the submission more comparable to a relatively large body of RL-based generation (e.g., (Ranzato et al., 2015; Rennie et al., 2016) to mention just a few), which the paper doesn't mention. The paper doesn't optimize over multi-turn simulations, which makes the setup similar to conditional generation for tasks such as MT (Ranzato et al., 2015) and image captioning (Rennie et al., 2016). Given that this lack of multi-turn exploration brings this form of RL much closer to supervised learning, this could explain the lack of a consistent winner between RFI and RFI-minus-RL.\n\n- The related work section is rather thin. This is not a submission to a dialogue (or NLP) conference, so I think the paper should provide a broader perspective and, in particular on text generation outside of dialogue (RL or non-RL). For example, there is work on improving and evaluating the consistency and coherence of general texts (Cho et al., 2019), e.g., summarization (Kryściński et al., 2019).  \n\nOverall:\n\nThe paper addresses an important problem (improving consistency), but the approach of the paper is fairly specific to the task and domain, doesn't bring much technical novelty, and the general approach (RL-based generation without simulator) is not so novel and is related to a body of prior work in generation that the paper mostly ignored.  \n\nMinor comments:\n\n- “Further, “ -> “Furthermore, “\n\n- \"RFI\" vs. \"RFI-RL\": the hyphen may easily be interpreted incorrectly, so some readers might understand the latter as \"w/ RL\" instead of \"w/o RL\", as I did initially. (For example, \"FC-LSTM\" doesn't mean FC w/o LSTM).\n\nReferences:\n\n- Ranzato et al., 2015: https://arxiv.org/abs/1511.06732\n\n- Rennie et al., 2016: https://arxiv.org/abs/1612.00563\n\n- Cho et al 2019: https://www.aclweb.org/anthology/W19-2401/\n\n- Kryściński et al., 2020: https://arxiv.org/abs/1910.12840\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review3",
            "review": "This paper presents an RL approach to refine an MLE-based language model with human demonstrations, to alleviate the repetition and inconsistency issues in response generation. It added two modules (repetition detector and inconsistency detector) in the pipeline to reduce the two issues, and it also has a response filter and response imitator to give the rewards to the response. On a human-human persuasion dataset (PERSUASIONFORGOOD), it shows the promising results over baselines on automatic evaluation and human evaluation. \n\nSome questions about the experiments:\n\n1. For the profile builder, how many users in this corpus? How do you handle Out of User issues? if a user is not in the vocab.\n\n2. And for human evaluation, how to build this user profile? since human evaluation is more random, you can not assume the human to simulate the existing user profiles (in the corpus).\n\n3. For the average length metric, is the longer the better? or ?  Do you get the similar results on human evaluation (Table 2)?\n\n4. Is there any other metric to measure the average turns of the dialog? For this kind of dialog setting, it the shorter the better?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "[Summary]\n\nThis paper proposes a reinforcement learning framework for dialogue systems. This framework tackles the repetition and inconsistency generation problem and uses a hard filter to select persuasion responses. This is done by sampling multiple responses from the policy and by ranking/rejecting them based on multiple criteria (Section 3.1.2,3.2,3.3). Furthermore, the authors proposed a profile builder for both user and systems to improve response consistency. Finally, they tested the framework with the PersuasiveForGood dialogue dataset (Wang et.al.2019) and conducted an in-depth automatic and human evaluation.\n\n[Cons]\n- Submission clarity: the paper is very hard to follow, it proposes many different things and do not provide any in-depth description/analysis. All the pieces are not described as a framework but like independent pieces specific to the datasets used for the evaluation. Indeed, methodology and results are often mixed, and for example, the persuasive filter is specific to the particular model. \n- Technical correctness and novelty: from the reinforcement learning point of view there is no novelty, response re-ranking and filtering are well-understood methods and commonly used by the dialogue community. \n- Experiments: the authors report results only on a very specific and small dataset, which is by far not a very standard benchmark for RL dialogue generation. \n- Response generation: From the examples shown in Table 3, the response seems very unrelated to the dialogue context. For example, in RFI model the \"persuasive\" response seems repetitive and out-of-context, and the RFI-RL, instead, seems very much a chit-chat session. \n\n[Pros]\n- The human and automatic evaluation is very well done. \n\n[Reason to Reject] \nThe paper has marginal novelty and it is very hard to follow.\n\n[Suggestions]\n- Dividing methodology and experiments, make the method explanation separate and general, instead of specific to particular model/classifier. \n- Make the methodology generic and instantiate the model to datasets, instead of creating a model for a dataset. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}