{
    "Decision": "",
    "Reviews": [
        {
            "title": "good results while with very limited technical novelty",
            "review": "This paper focuses on processing input features at multiple scales. The author proposes the pyramidal convolution operation which applies convolutions with different kernel sizes, to capture different levels of details in the scene. The proposed method achieves good results on different datasets.\n\nWhile the technical novelty of the proposed method is very limited. Processing features with different sized kernels are widely explored in lots of previous algorithms, e.g, the Inception architectures, ASPP structures, etc. I did not find anything interesting or new in the proposed paper. It is just an ensembling and engineering paper, and readers are not inspired by the claims. I think it is below the acceptance bar and not a good fit for ICLR.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Promising results but missing important comparisons to prior works",
            "review": "This paper proposes a multi-scale conv kernel PyConv, with the main idea of integrating multiple kernel size like {3x3, 5x5, 7x7} to a single conv layer. It shows PyConv can improve ResNet for imagenet classification and AED20K semantic segmentation.\n\n\n—————— Strengths ————\n\n(1) Paper is well written and easy to follow.\n(2) Comprehensive evaluation on both classification and segmentation, with promising results.\n(3) The studied problem (how to design more efficient Conv blocks) is important to this domain.\n\n————— Weaknesses————\n\n(1) A major issues of this paper is lack of comparisons to closely related work. Mixing different kernel sizes has been studied in several recent works. In particular, the following two works are closely related to this paper:\n\n  [1] (BMVC’19) MixConv: Mixed Depthwise Convolutional Kernels.\n  [2] (CVPR’20) MUXConv: Information Multiplexing in Convolutional Neural Networks.\n\nThe authors cited [1] in appendix and explained “Each kernel in [1] only applies to a subset of input features, while PyConv gets access to all input feature maps”. This seems to be a relatively straightforward change, and it is unclear how much additional gain this change brings (in addition of the existing multi-scale kernels in [1][2]). Unfortunately, this paper didn’t show any results comparison with [1], and it also didn’t discuss or compare with [2]. \n\nWithout these comparisons, it is difficult to justify the importance of the new ideas (e.g. using all inputs feature maps).\n\n(2) Another issue of mixing different kernels is that they may run slow on real hardware like GPU. I acknowledge the authors has Appendix Table 5 compares the latency for Inception, but there is no study or comparison for ResNet-50 in the main paper. I recommend the authors to add GPU latency comparison for ResNet-50 and PyConvResNet-50, and it is commonly expected that PyConvResNet-50 would be much slower despite they have similar FLOPs.\n\n(3) Lastly, I have a few minor comments to the writing:\n\nA) Section 3: FM looks like two variables. I recommend changing FM to D, as it actually means depth.\nB) Table 1: as explained by the authors, PyConvResNet moves the pooling to next stage, but it is unclear whether that would also benefit ResNet as well. Having an additional study of moving pooling to next stage for ResNet would be helpful to explain the gain is from PyConv.\nC) Figure 7: what’s the wall-clock training time difference between different models? For example, do ResNet-50 and PyConvResNet-50 finish the training with about the same time?\n\n—————— Suggestions ———————\n\n(1) Compare the proposed PyConv with closely related works like MixConv/MUXConv.\n(2) Add latency comparison on either CPU or GPU.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The work is incremental in both performance and technical novelty ",
            "review": "- The author proposes a pyramid convolutional layer with different kernel sizes and channel numbers. Experiments have been done to verify the effectiveness of the approach. \n\n- The technical novelty of this paper is low. The design spirit is similar to ResNeXt which uses several parallel branches to construct the convolution module. The major difference is that this paper uses multiscale kernels, and to reduce the computation, they reduce the number of channels. This seems to be very straightforward and trivial in comparison with the design of ResNetXt. Moreover, the improvements in comparison with ResNetXt is also incremental, especially when the layers of the network increase  (as shown in Table 3).\n\n- The results are incremental and missing comparisons with SOTA approaches. The author only compares with papers published several years ago in image classification and semantic segmentation, and the current SOTA approach is not well compared and analyzed. For instance, Tan, Mingxing, and Quoc V. Le. \"Efficientnet: Rethinking model scaling for convolutional neural networks.\" ICML (2019).  Zhang, Hang, et al. \"Resnest: Split-attention networks.\"\n\n- Serious overclaim. I think the writing needs to be significantly modified. For instance, \"PyConv has the potential\nto impact nearly every computer vision task\",  Does it really can?.  \"Our approach shows significant improvements over these core tasks in comparison with the baselines.\" with improvements by just a few points. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Multi-scale convolutions",
            "review": "The work proposes combining multiple-scales/kernel sizes into each conv layer. In order to avoid a computational and parameter blow-up the authors propose using group convolution with increasing number of groups per pyramid level to roughly keep the computational cost constant.\n\nPositive\n- The idea is quite interesting and seems to lend itself well to neural architecture search, since there are many additional hyper-parameters. Nonetheless, the authors propose \"hand-tuned\" suggestions that reduce the FLOPS and parameters significantly at comparable or slightly improved accuracy on ImageNet. On the segmentation task the improvements seem less convincing.\n\nNegative\n- I wish the authors discussed certain choices in more detail. Specifically, why is the pyramid setup the way it is, vs being inverted, or just a stacking. I could not find any motivation for the choices made; or how it compares to other variants, e.g. atrous conv. The choice of dense 9x9 kernels seems rather inefficient.\n- While the proposed work shows promising improvements, the interpretation of the results is lacking. Some arguments around the larger receptive fields and multiple scales are given, but no experiments that support these arguments are given.\n\nThe paper proposes a new promising direction that I can imagine to have potential, especially when combined with NAS, but also isn't a significant step beyond the already existing neural architecture search spaces. Without a more detailed analysis of why the proposed architecture provides benefits, the results are rather incremental despite the promising improvements on Imagenet. In order to significantly increase the value of this work for the community the choices made (dense vs sparse kernels, pyramid vs stack vs inverted pyramid) should be analyzed and discussed in more detail. I am on the fence here, but given the promising results slightly leaning towards accept.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}