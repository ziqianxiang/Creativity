{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This is a well written paper, outlining a class of assistive algorithms. Being more or less a survey paper, it could do a better job of discussing 'inverse reinforcement learning' and 'collaborative inverse reinforcement learning'. It could also be slightly more general: for example the human dewcision function need not be known if we model the interaction as a Bayesian game (then the human might have a latent type, which can be inferred together with the reward function). The active reward learning problem is sometimes referred to as 'preference elicitation'. In the end, it was not clear that the discussion in this paper had any actionable insights for future models or algorithms in this area.\n"
    },
    "Reviews": [
        {
            "title": "The paper provides a comparative analysis of reward learning and assistance. While the discussion in the paper is certainly interesting, I'm uncertain about what are the contributions of the paper.",
            "review": "### Overview\n\nThe paper addresses the problem of learning from human feedback. It provides an analysis of reward learning---where human feedback is used to extract a task description in the form of a reward---and assistance---where the learning agent and human co-exist in the environment and both perform actions; the agent seeks to select its actions to optimize the (unknown) that is implicit in the human's actions. \n\nThe paper shows that reward learning problems can be converted to assistance problems, turning queries from reward learning to communicative actions in a two-phase communicative assistance problem. Conversely, two-phase communicative assistance problems can be converted to active reward learning problems.\n\n### Comments\n\nIn my opinion, the paper reads very well, and the discussion in the paper is quite interesting. \n\nIn spite of an interesting discussion, I am not certain about the contribution of the paper. The derivations, although they seem technically sound, are not particularly surprising; similarly, the qualitative differences in behavior pointed out in the paper are hardly surprising---assistance learning, being a more general problem than reward learning (as follows from the discussion of the paper), will lead to more diverse behaviors. \n\nThe particular behaviors observed are also a consequence of the POMDP formulation, as the agent will act to strike an adequate tradeoff between information gathering and goal optimization.\n\n### Post-rebuttal update\n\nI thank the authors for the clarifications and discussion. However, and admitting that I may have missed something, I remain unconvinced regarding the contributions of the paper.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review | Reviewer #4",
            "review": "#### Summary\n\nThe submission provides a survey of two paradigms for ‘agents learning from human feedback.’ The two paradigms are unified under a new formalism (assistance games), which subsumes them as its special cases. Further, a taxonomy of different problems resulting from the formalism is provided (communicative games, two-phase games, etc.), along with illustrative examples of resulting agent behaviors. Based on the survey and taxonomy, the authors highlight that the assistance paradigm is more advantageous (in terms of possible behaviors that it can result in) than the reward learning paradigm.\n\n=======================================================\n#### Reasons for score\n\nStrengths\n  + The topic of ‘agent learning from human feedback’ is topical and of interest to the ICLR community.\n  + The proposed taxonomy (i.e., assistance games) can serve as a useful common ground for discussing the different paradigms, problems, and solutions of ‘agent learning from human feedback.’\n  + The paper is well written and organized.\n\nWeaknesses\n- While the submission does discuss related work (Section 2), the discussion omits several related research threads. Some of these threads have strong overlap with the setting proposed in the submission.\n- Similarly, the qualitative behaviors that emerge from assistance games (Section 4) have been demonstrated in prior research (including on larger problems and with human users), thereby making it difficult to assess the novelty of the formalism.\n- The key contribution of the submission is unclear (e.g., whether it is a survey, a model, a taxonomy, or all?).\n \nI am truly on the fence regarding this submission. A novel taxonomy is certainly needed to relate and compare the diverse and growing body of research in the area of ‘agent learning from human feedback.’ However, to arrive at this taxonomy a more complete consideration of existing formalisms and algorithms is necessary. Please see suggestions listed below on prior research that relates to and, in certain cases, extends the formalism of assistance / collaboration.\n\n=======================================================\n#### Key Comments\n\n1.\t(Introduction and Proposition 1) The insight of having a single control policy for both reward learning and control modules has been previously explored. This insight is identical to that of planning / control formulations in human-robot interaction literature that model the human’s preferences (which in turn influence the reward) as latent states. These planning methods use POMDPs to represent the interaction / collaboration problem, and solve the exploration-exploitation tradeoff associated with reward learning (exploration) and control (exploitation) using POMDP solvers or MPC. Please discuss the novelty of the proposed formalism in relation to these methods. For instance, \n    - (considers continuous states and actions spaces; assistance paradigm) Sadigh, Dorsa, et al. \"Information gathering actions over human internal state.\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2016.\n    - (does not require one agent, R or H, to act before the other; assistance paradigm) Chen, Min, et al. \"Planning with trust for human-robot collaboration.\" Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction. 2018.\n    - Gopalan, Nakul, and Stefanie Tellex. \"Modeling and solving human-robot collaborative tasks using pomdps.\" RSS Workshop on Model Learning for Human-Robot Communication. 2015.\n    - Nikolaidis, Stefanos, et al. \"Game-theoretic modeling of human adaptation in human-robot collaboration.\" Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction. 2017.\n\n2. (Section 2.3) Assistance games, as defined, assume parametric specification of the hypothesis space of the reward / preference of humans. However, nonparametric extensions (both for assistance and reward learning) have been proposed. Please consider relating the proposed formalism with these prior works. For instance, \n  - Michini, Bernard, and Jonathan P. How. \"Bayesian nonparametric inverse reinforcement learning.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.\n  - Panella, Alessandro, and Piotr Gmytrasiewicz. \"Bayesian learning of other agents' finite controllers for interactive POMDPs.\" Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. 2016.\n\n3. (Section 2.3) In assistance games, as defined, the reward (or human preferences over reward) does not change during the task. However, extensions exist which model the latent state corresponding to reward as being locally active and / or time varying. Please consider relating the proposed formalism with these related works. For example, \n  - (locally active reward; reward learning paradigm) Michini, Bernard, and Jonathan P. How. \"Bayesian nonparametric inverse reinforcement learning.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.\n  - (locally active reward; reward learning paradigm) Park, Daehyung, et al. \"Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning.\" Conference on Robot Learning. PMLR, 2020.\n  - (considers time varying preferences with learned dynamics; assistance paradigm) Nikolaidis, Stefanos, David Hsu, and Siddhartha Srinivasa. \"Human-robot mutual adaptation in collaborative tasks: Models and experiments.\" The International Journal of Robotics Research 36.5-7 (2017): 618-634.\n  - (considers time varying preferences with learned dynamics; assistance paradigm) Unhelkar, Vaibhav V., Shen Li, and Julie A. Shah. \"Semi-Supervised Learning of Decision-Making Models for Human-Robot Collaboration.\" Conference on Robot Learning. 2020.\n\n4.  The behaviors arising from solving assistance games (Section 4) have been previously formalized by multiple human-AI collaboration approaches and demonstrated with human users. For instance,\n  -\t(exhibits behaviors outlined in Section 4.2) Kamar, Ece, Ya’akov Gal, and Barbara J. Grosz. \"Incorporating helpful behavior into collaborative planning.\" Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS). Springer Verlag, 2009.\n  -\t(reasons about communicative actions) Whitney, David, et al. \"Reducing errors in object-fetching interactions through social feedback.\" 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017.\n  -\t(reasons about both physical actions and communications) Nikolaidis, Stefanos, et al. \"Planning with verbal communication for human-robot collaboration.\" ACM Transactions on Human-Robot Interaction (THRI) 7.3 (2018): 1-21.\n  -\t(reasons about both physical actions and communications) Unhelkar, Vaibhav V., Shen Li, and Julie A. Shah. \"Decision-Making for Bidirectional Communication in Sequential Human-Robot Collaborative Tasks.\" Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction. 2020.\n  -\t(communicative actions) Liang, Claire, et al. \"Implicit communication of actionable information in human-ai teams.\" Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 2019.\n\nPlease discuss the connection of the proposed formalism with these approaches.\n\n5.\t(Section 2.2) Please also consider discussing the following related works on active reward learning,\n  - Lopes, Manuel, Francisco Melo, and Luis Montesano. \"Active learning for reward estimation in inverse reinforcement learning.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2009.\n  - Brown, Daniel S., Yuchen Cui, and Scott Niekum. \"Risk-aware active inverse reinforcement learning.\" Conference on Robot Learning. 2018.\n  - Tschiatschek, Sebastian, et al. \"Learner-aware teaching: Inverse reinforcement learning with preferences and constraints.\" Advances in Neural Information Processing Systems. 2019.\n  - Cui, Yuchen, and Scott Niekum. \"Active reward learning from critiques.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.\n\n=======================================================\n#### Minor Comment\n\n- (Section 2.1) Please consider including a note explaining the asterisk notation (used to define the domain for POMDP policy).\n\n=======================================================",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "[UPDATED] [Official Review]: Well written, but a bit too one-sided",
            "review": "UPDATE:\nAfter extensive discussion with the authors, I'm raising my score to a 7.  \nI believe their revision will adequately address the concerns I've raised.\nI think this paper clearly identifies and illustrates the qualitative advantages of assistance, and that this is a novel and significant contribution.\n\nIn particular, I do *not* believe, as other reviewers seem to, that any of the following are sufficient reasons for rejecting this work:\n* The wealth of prior work on variants of reward learning and assistance \n* The lack of a comprehensive survey or categorization of such work in this submission\n* The lack of further results\n\nAfter reading the other reviews and responses, I am more confident that this paper makes a valuable contribution, although I stand ready to be challenged by other reviewers.  This is because the authors have argued that the qualitative benefits they describe in sections 4.1/2/3 have not been available to any of the many previous works reviewers mentioned, and no reviewers disputed this.  Furthermore, I did not find the reasons provided for rejection to be very relevant to the goals of this work.  So overall, I do not believe that other reviewers have made a strong case for rejecting this work.\n\nIn my mind, the best argument would seem to be simply that the contribution is insufficient.  I think this is a common criticism of papers that do not adhere to a conventional format or \"type\", but in this case, it seems unfair.  I believe the intellectual contribution of this paper is rather modest, but nonetheless novel and significant.  And I found this motivation for the work quite compelling (emphasis mine):\n> This existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. But (as we show) the assistance paradigm can enable significantly better behavior from the agents we train! **We are hoping to influence researchers to put more effort into algorithms for the assistance domain, in order to realize these qualitative benefits, instead of continuing to work in the reward learning paradigm as they have done so far.**\n\nI would encourage the authors to explain this goal in their revision, and make sure their claims about the superiority of assistance are appropriately modest.  Overall, I think the qualitative benefits of assistance presented provide a compelling argument for more work in the assistance paradigm, *given the paucity of such work*.  But I think the overall message of the paper should be: \"Given these advantages, and the lack of work on assistance, there should be more work on assistance, since it seems promising and neglected\", and not \"Assistance is better, so why would you do reward learning?\"  And my first impression of the paper was closer to the latter.  \n\nEND UPDATE\n\n-------------------------\nEvaluation:\n\nThis paper is well written and makes a nice point regarding qualitative advantages of assistance over reward learning.  Specifically, the authors show how assistance naturally leads an agent to \"agent to (1) choose questions based on their relevance, (2) create plans whose success depends on future feedback, and (3) learn from physical human actions in addition to communicative feedback.\"  However, the framing is quite one-sided, and the authors make no mention of potential advantages of reward learning.  And it is possible to achieve the same qualitative advantages by slightly modifying the (rather restrictive) formulation of reward learning that the authors use.  I think the work should either include such a discussion, or offer more convincing evidence regarding when assistance is in fact preferable in practice (e.g. experiments with some features that might advantage either approach).\n\nSome potential advantages of reward learning are:\n- reduced complexity of the learning problem\n- the human retains more control\n- it seems to require less modeling of human psychology\n- less opportunities to corrupt the reward signal\n\nRegarding the qualitative advantages mentioned in Sections 4.1/2, we can achieve the same benefits in many instances by incorporating regular feedback sessions where H and R can communicate.  I was confused by the claimed advantage in 4.3, since IRL already can learn from physical actions.\n\nOverall, I think this paper sets up a bit of a false dichotomy between (non-interactive) reward learning, and the full assistance game formulation.  We can instead view these methods as varying wrt 1) when and how we ask R to interpret H's actions as communicative, and 2) how interactive the learning process is.\nThis paper still makes important (corresponding) points: 1) there is a benefit to considering H's behavior as communicative whenever we understand its semantics, and 2) interaction is important.\n  \nThese are not very surprising, although very nicely and clearly argued for and demonstrated. However, regarding point (1): if we don't understand the semantics of H's behavior, it makes sense to restrict the communication to a more well-defined channel, as in reward learning; furthermore, even if we *do* know the semantics, H may not wish all of her behavior to be viewed as communicative, and allowing her to directly control when and how her behavior will be viewed as communicative grants H more agency over the behavior of R.\n\nAccepting points (1) and (2), it still seems like most effective methods are likely to lie somewhere between the two extremes described in this work.  For example, the authors state: \"Thus, for good performance we need to model π^H. This will require insights from a broad range of fields that study human behavior.\"  However, accurately modelling π^H  seems significantly less urgent when communication from H is more limited and literal (i.e. when we move closer towards current reward learning practice).  As another piece of loosely supporting evidence, in \"Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning\", Milli and Dragan show that more sophisticated assumptions about the semantics of human behaviors may also be more brittle.\n\nAnother contribution of the paper is to explicitly frame reward learning as a special case of assistance.  This seems like a straightforward, minor, contribution.\n\nI was also disappointed that the paper didn't discuss reward corruption. The need to more fully understand H in the assistance paradigm creates more opportunities for R to misinterpret H's behavior.  In practice, the tighter feedback loops favored by the assistance paradigm might also create more opportunities for R to (e.g. irreversibly) \"corrupt\" H's reward function and/or policy, since an initial misunderstanding could be self-reinforcing.  For instance, H might fail to tell R that its behavior was intimidating if doing so previously had led R to become more (instead of less) intimidating.  This could lead R to become confident that this behavior was *not* intimidating to H.  Such a scenario seems less likely when H provides feedback to R in an \"offline\" or \"purely communicative\" context.\n\n-------------------------\nDetailed Suggestions:\n- Figure 1 caption could be clearer (what is depicted on the right?)\n- \"These behaviors cannot be expressed by a reward learning agent.\" <-- not literally true; suggest a rephrase.\n- \"Since c0:k-1 only serves to provide information to R\" <-- It doesn't have to... e.g. people do IRL on data that doesn't fit this description.\n- Provide more of an informal introduction in section 2.3 before jumping into definitions\n- \"However, since we want to compare to reward learning, we follow its assumptions and so assume that the human policy πH is available.\" While I was eventually able to understand this reasoning, it sounds false out of context, and should be clarified.\n- a^H_noop is not defined\n- I consider the first paragraph of Section 4 to be the \"meat\" of the paper, and I think some of this content should be front-loaded more.  It's a shame to wait until page 6.\n- I would start each of 4.1/2/3 with a paragraph giving a non-mathematical, qualitative description of the result.\n- I like the paragraph at top of page 7, which gives an explanation of how reward learning would act/fail.  Can you include such a paragraph in 4.1/3 as well?  I think the more you mirror structure in these sections, the better. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review #1",
            "review": "Summary:\nThis work proposes learning a single control policy for human-in-the-loop learning rather than having a reward learning component and a control component. The key difference is that the action selection can use information from the reward learning module. The authors formulate an assistance game in this setting and show that it can reduce to an equivalent POMDP. The work then describes a communicative assistance problem and shows the equivalence of reward learning to assistance and visa versa. Results show qualitative improvements on variants of the kitchen domain.\n\nPros:\n- The paper was overall well-written (although some key differences with prior work were unclear, as described below).\n- The high-level area of human-in-the-learning is a useful and important space.\n\nCons:\n- There are many works in human-in-the-loop learning, and it wasn’t clear whether the ideas in this paper were novel enough at a high-level. Some relevant works are included below.\n- The paper was hard to follow with respect to the distinction between reward learning and assistance. Adding more examples or describing the difference from other angles could be useful.\n- The experimental domains were simple and there were no computational results shown in the main paper. This is important to show in order to support the claims of the paper.\n\nComments:\n- The qualitative results are useful for showing specific cases in which the proposed approach may be beneficial but this restricts how applicable the approach seems to be. It would be nice to see computational results over a variety of domains and comparisons with baseline approaches to see the benefit of the approach more generally. \n- Section 4.2 focuses on asking the right questions at the right time. What is the main novelty here with respect to more general active learning approaches?\n- A few minor notational confusions: for example, it’s confusing to have R be the robot as it’s often the reward function. And in Section 4.1, C is used for choices and for cherry.\n- Other work that might be relevant to the problem proposed here:\n\nGame-theoretic modeling of human adaptation in human-robot collaboration\nS Nikolaidis, S Nath, AD Procaccia, S Srinivasa\n\nMaximizing BCI Human Feedback using Active Learning\nZ Wang, J Shi, I Akinola, P Allen\n\nModeling humans as observation providers using pomdps\nS Rosenthal, M Veloso\n\nActive Learning for Risk-Sensitive Inverse Reinforcement Learning\nR Chen, W Wang, Z Zhao, D Zhao\n\nContact: Deciding to communicate during time-critical collaborative tasks in unknown, deterministic domains\nVV Unhelkar, JA Shah\n\nEfficient model learning from joint-action demonstrations for human-robot collaborative tasks\nS Nikolaidis, R Ramakrishnan, K Gu, J Shah\n\nRecommendation:\nOverall, I thought the work at a high-level was limited in its novelty compared with prior work in human-in-the-loop learning. The distinction between reward learning and assistance, a key part of the paper, was hard to fully understand so clarifying this description through examples or more clear text would be valuable. The evaluation was also simple and not very convincing with respect to supporting the claims. Adding computational experiments and appropriate baselines would be important to show the general applicability of the approach.\n\n--------------------\nResponse after rebuttal:\nThank you to the authors for their response. I appreciate the detailed answers to each of the prior works. I still do think the paper needs to be more clear in the problem and differentiation with prior work in the writing itself, which will require a non-trivial update to the paper.\n\nOn the computational results + baselines side, while the authors have run the experiment and have described these qualitative behaviors, this isn't a substitute for quantitative results, especially because this is important to show when comparing with baselines. The authors say \"We have updated Section 4 to be clearer about what baseline approaches would do in the environments we have tested\". It's important to show evidence that this is what the baselines actually did.\n\nBased on these points, I don't think the paper is quite ready for publication yet.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "It seems to me that the authors disregard a rich related literature",
            "review": "------------------------------------------------------------\nPost-rebuttal\n-----------------------------------------------------------\nGiven the effort of the authors of improving their manuscript, I am improving my original score. However, my evaluation is still \"weak reject\" for the reasons below:\n\n(1) I still fail to see clear differences between \"assistance\", as defined by the authors, and the other reinforcement learning-like approaches that assume that the reward function is unknown. I can see that they perhaps provide a more organized and methodological description of how that \"assistance\" can happen when compared to the previous works. However, the paper lacks practical advice, exactly how should I build an agent to leverage such \"assistance\"? I don't think their ideas are so novel that other methods couldn't be at least adapted to work in their scenario (to include some empirical evaluation in the manuscript).\n\n(2) The paper seems a little displaced to me in this conference. The paper neither provides practical and direct guidance on how to build algorithms to leverage \"assistance\", nor is a survey that focuses on organizing the area and discussing differences between works. Perhaps the paper would be better placed in a \"Blue Sky\" track. \n\n-----------------------------------------------------------\n\nThe authors propose two learning paradigms where the learning agent doesn't have access to a reward function but instead has to learn directly from the \"assistance\" from a trainer agent.\n\nWhile looking for ways to facilitate task specification and human integration in the learning process is a relevant and promising research goal, the authors don't explain the difference between their newly-proposed paradigms and the very rich literature on Inverse Reinforcement Learning and Learning from Demonstrations.\n\nLearning from human \"assistance\" instead of a reward function is not a new thing, and the surveys below (not cited by the authors) summarize a rich literature that does precisely that:\n\nSilva, Felipe Leno, and Anna Helena Reali Costa. \"A survey on transfer learning for multiagent reinforcement learning systems.\" Journal of Artificial Intelligence Research 64 (2019): 645-703. -> Surveys many categories of works where one agent provides guidance to others, including humans providing \"assistance\" to learning agents.\n\nArgall, Brenna D., et al. \"A survey of robot learning from demonstration.\" Robotics and autonomous systems 57.5 (2009): 469-483. -> Surveys learning from demonstration, where a human provides policy demonstrations to a learning agent, which usually doesn't have access to a reward function\n\nGao, Yang, et al. \"A survey of inverse reinforcement learning techniques.\" International Journal of Intelligent Computing and Cybernetics (2012). -> Inverse reinforcement learning, where the learning agent doesn't have access to a reward function and has to infer a policy from human assistance\n\nWithout a comprehensive discussion about the differences between the authors' proposal and those paradigms, I can't judge the paper contribution. To my eyes, all the descriptions gave in the paper look just like the same as one of the problems surveyed in the papers above.\n\nFor example, \"Non-active reward learning\" seems to me equivalent to inverse reinforcement learning.\n\nAlso, \"active reward learning\" seems to me a special case of the action advising problem surveyed in the first paper above.\n\nIn the case the authors deal with a different problem in this paper, I suggest that the manuscript is rewritten to thoughtfully explain the difference between all those scenarios. In case they indeed are correspondent scenarios, I suggest that the authors use the same notation as the previous works, and include comparisons with the state of the art methods in the experimental evaluation.\n\n-----------------------------------------------------\nOther suggestions\n\n- I don't get how the policy decision function will compute the expected reward if the reward function is unknown.\n\n- You assume access to a \"human decision function\", what exactly does that mean? do you need the probabilities for taking each action? If that's the case it is very unrealistic to expect that a human is able to provide probabilities for each action. Asking s/he to simply pick one action when requested is more realistic.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}