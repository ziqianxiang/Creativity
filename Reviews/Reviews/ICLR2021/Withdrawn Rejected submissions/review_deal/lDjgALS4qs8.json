{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes to explain the representation for layer-aware neural sequence encoders with multi-order-graph (MoG). Based on the MoG explanation, it further proposes Graph-Transformer as a graph-based self-attention network empowered Transformer. As commented by the authors, a main purpose of Graph-Transformer is to show an example application of the MoG explanation.\n\nDuring the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524-561 in the attached code: \"supplement/fairseq-0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py\". The AC had made the following comments to the referees: \"Whether the performance gain of Graph-Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high-level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x = x - beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output.\"\n\nReviewer 2 responded to the AC's concern: \"After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self-attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi-head attentions (line 539-541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539-541) is not very convincing. The link is weak. We agree with your comments.\"\n\nTo summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an ICLR publication. Therefore, the AC recommends Reject.\n\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "We carefully review the motivation, approach, and empirical results.",
            "review": "### Summary\nThe authors propose a new Transformer variant for neural machine translation. Compared with the standard Transformer framework, this work explains the representation generation process of the encoder via a multi-ordered-graph MoG and develops a novel Graph-Transformer method based on MoG, which is capable of capturing diverse relationships within the sequence. Empirical results over benchmark datasets validate the effectiveness of the proposed method. \n\n### Pros\nThis work appears originally in its new explanation of the representation generation process. Specifically, the main pros are summarized below.  \n1.\tIt provides a multi-ordered-graph MoG explanation for the representation generation of Transformer encoder. MoG is capable of capturing diverse relations of the sequence compared with standard simple directly graph explanation.  \n2.\tIt develops a novel method, dubbed Graph-Transformer, by combining MoG and Transformer, which generates layer-wise representation from the previous representation and incremental representation aspects explicitly. \n3.\tIt implements two fusion strategies, i.e., weight-gate and self-gate, for layer-wise information aggregation. Ablation study is also conducted to investigate the effectiveness of them. \n\n### Cons\nMy primary questions/concerns are listed below. \n1.\tThe experiments are limited with only two benchmark datasets. Since Transformer has been demonstrated to be effective in many NLP tasks, it would be better to include more datasets or even other tasks except neural machine translation for a comprehensive comparison. \n2.\tThe selection of the baseline seems unfair. As stated in Section A.5.1, position-encoding can be viewed as a specific MoG because it builds edges between nodes. Therefore, methods that focus on position-encoding should also be included for comparison. For your reference, Transformer-XL[1] and [2] are two tailored solutions for this purpose. \n3.\tIt is unclear about the model’s efficiency, i.e., memory-efficiency and learning efficiency. The idea to split the layer-wise full representation into previous representation and incremental representation is interesting. However, it also inevitably increases the model complexity. The authors should provide more discussions about the model complexity to improve the quality further.\n4.\tIn Figure 3, why position-encoding is needed? MoG is already capable of capturing the sequential information by merging subgraphs. More explanation is welcomed for this configuration. \n\n### Clarify\nThis work is well organized and easy to follow. However, the readability can be improved by addressing the following suggestions. \n1.\tThe layer-level iteration in Figure 2 is difficult to understand; it would be better to give a toy example with a real sentence for a better explanation. \n2.\tIn the second sentence of Section 2.4, r is not defined. In my understanding, each generated edge $e_j$ constructs a unique sub-graph, then what is r stand for?    \n\n[1] Dai, Zihang, et al. “Transformer-XL: Attentive language models beyond a fixed-length context”, ACL, 2019. \n[2] Wang Benyou, et al. “Encoding word order in complex embeddings”, ICLR, 2020. \n \n\n### Response to Rebuttal\nThank authors for taking the time to clarifications and considering my comments.\n\nWe appreciate authors' efforts to add additional experiments results in Table 1 and Table2. However, the performance improvements are marginal (more or less 0.7) and speed of Graph Transformer is slower than transformer.\n\nEven though additional explanations about positional encoding (Appendix 8.1) can resolve our concerns, layer iteration (Figure 6) are not still clear for us, e.g., what are orange blue, yellow nodes? how layer iterations are used in graph transformer? Authors should refine its main context to increase understanding instead of adding lengthy Appendix for us. This such paper presentation and organization are not clear to understand.\n\nConsidering the above points, we still remain our decision.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "idea is insightful, but some parts are not clear",
            "review": "The paper has propose the layer-aware graph transformer to enhance the ability of capturing heterogeneous information of the current models. The experimental results has justified the effectiveness of proposed model.\n\nThe multi-order graph structure is innovative and contribute to the expression of generating subgraphs. Such operations has benefited for questions the proposed in section 2.5, which I believe could bring up insightful idea to the community. However, there are still some parts that should be given an extended clear statement.\n\n1, it is not clear how the \"unified explanation\" is defined and delivered in the graph-transformer model? Does it contribute the the model performance? MoG seems doesn't make a well-defined explainability to reflect the quantitative model explanation.\n\n2, How MoG 'observe' every step of generation of embedding? Is there any model transparency defined?  \n\n3, Could the model be more generalized to other downstream task besides NMT?\n\n4, It is not cleared that in the 2.2, whether it could be overlaps between SN and TN? If so, the number of generation of kinds of subgraph could be very large.\n\n5, The experimental parts lack of detail procedure. How to extract the relations within each sentence? Also the performance of the proposed model may highly be dependent over the quality of MoG construction, which may limit the generalization ability of the model.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThe paper proposes a new multigraph architecture called Multi-Order-Graph to explain the representation generation process in neural sequence encoders (Self-Attention or SAN based models). The main contribution of this MoG is the introduction of n-order dependency which can model not only relationships between words but also high order relationships such as syntax and semantics between subgraphs. Taking inspiration from MoG explanation, a self-attention powered Graph Transformer is proposed which beats the Transformer baselines on NMT tasks (English-German and German-English).\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPros: \n\n+ The proposed idea of representing the encoding process as generation of a Multi-Order-Graph is novel and is able to provide good insights for SAN-based models.\n\n+ Proposed Graph Transformer uses self-attention and also attends over different order of subgraphs (low-order, middle-order and high-order). Since these subgraphs represent high order relationships (syntax, semantics etc.), the model is able to pay attention to salient subgraphs.\n\n+ This paper could fuel further research in the explainability of neural sequence encoders and SAN based models.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nCons: \n\n- The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Only two datasets are used for NMT task. There has been a lot of improvement on base Transformer models (cited in the paper), still there is no comparison with them.\n\n- This paper claims that with the proposed MoG explanation it is possible to model high order relationships such as Syntax and Semantics. However, there is no example provided for this. Even some cherry picked examples would have helped in visualising the behaviour.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nOverall, I would like to see the paper at the conference. The idea of modelling the relationships as a Multi-Order-Graph is certainly novel and can fuel further research. However, due to the current state of experimentation I am voting for **rejecting** the paper.  I am willing to increase the score if the concerns are addressed by authors in the rebuttal period. \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nQuestions and Suggestions:\n\n1. I would like to see comparisons with models like Transformer(big), [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), [He et al.](https://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf). I would also encourage usage of some more datasets for the experimentation, for example En-Fr, En-Ro. Datasets can be found in the papers linked above.\n\n2. It would be interesting to see how different components of the Graph Transformer affect perplexity.\n\n3. What is Transformer(small)? I am sorry but I am not aware of this variation. It would be good if you could provide a reference for this particular variation of the model.\n\n4. In Fig 4a, it is evident that self-gate works better at encoding longer sentences. It would be good to see some discussion on this in the paper.\n\n5. Which dataset is used for Fig 4?\n\n6. In fig 4b, we can notice a sudden decrease in subgraph weight at layer 4 for 12 layered model. In figure 5, similar trend can be seen for different layered model except for 9 and 10 layered model. What is the significance of this behaviour?\n\n7. I would encourage the authors to provide the implementation for the model proposed in this paper. \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nMinor Comments/Typos:\n+ Section 2.4: source node of edge Snj -> Snj, source node of edge Ej\n+ Section 2.4: target node of edge Tnj -> Tnj, target node of edge Ej\n+ Section 3.3: the order of subgraph is in the range of 2^(n-1) to 2^(n) -> the order of subgraph is in range 2^(i-1) to 2^(i)\n   Similar mistake is in Middle-Order and Low-Order section. \n+ Section 7: MoG connects only only words but also subgraphs -> MoG connects not only words but also subgraphs. \n+ A.5.3: Rlationship information generated -> Relationship information generated\n\nPost Rebuttal Comments: Authors have addressed most of my concerns and as a result I have increased the rating from 5 to 6. Thanks!",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}