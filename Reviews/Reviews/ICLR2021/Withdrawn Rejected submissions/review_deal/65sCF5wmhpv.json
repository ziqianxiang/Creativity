{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The setting and the problem addressed by this paper has been considered as important and interesting to tackle with reinforcement learning. Yet, the reviewers expressed several concerns about this paper. Especially, the lack of comparison to state-of-the-art methods and to the standard visualization methods was a shared concern. The empirical validation also appeared as not ambitious enough. Finally, the novelty in the field of machine learning was also questioned since the paper is mainly about applying existing algorithms to a known problem. "
    },
    "Reviews": [
        {
            "title": "nice visualization scheme, low scientific impact",
            "review": "The paper shows how to incorporate an observation cost into RL control problems to assess the inherent value of information in different domains.\n\nI found the paper fun, and well written/edited.\n\nHowever, I don't see much of a scientific contribution here. The paper says its aim work is to reveal the information structure in the observation space within a systematic framework. So, it's essentially a kind of \"ML for scientific visualization\" paper. The ML novelty appears small---standard algorithms and test problems are used. The paper isn't really evaluated from a scientific visualization perspective, so it's not clear that it is over the bar from that perspective.\n\nThe light shed on some standard test problems (\"decisions aren't that impactful when the pole is almost balanced\", etc.) are nice, but not really impactful.\n\nDetailed comments:\n\nRelated work: I think it would be appropriate to cite Valentina Bayer's \"cost sensitive learning\" work. I think there's also a \"cost observable MDP\" model that is very related. The earlier work isn't able to solve these problems as well as the current paper, but the model is very related.\n\n\"towards to\" -> \"towards\"\n\n\"maximize average\" -> \"maximize the average\"?\n\nTable 1: Use right justification for easier visual comparison.\n\nI'm confused about the state used in the experiments. It's a POMDP, so was there a recurrent network used? Were multiple steps available in the state representation? How were the RL algorithms able to represent and learn the strategy?\n\n\"For Mountain car environment all\" -> \"For the mountain car environment, all\"\n\n\"following rise\" -> \"following arise\"\n\n\"the agents performance\" -> \"the agents' performance\"?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper approaches an interesting problem and proposes a simple, yet reasonable, approach. Unfortunately, the evaluation fails to provide a clear perspective on the potential impact of the proposed approach.",
            "review": "= Overview = \n\nThe paper proposes a reinforcement learning algorithm that enables an agent to \"fine tune\" the quality/accuracy of its sensors to its current task. The paper considers a partially observable MDP setting where the agent, besides the control actions, is endowed with a set of \"tuning actions\" that control the noise in the perception of the different components of the state. Additional reward terms are introduced that discourage the use of \"tuning\". By enabling the agent to fine tune its perception to the current task, the paper seeks to also investigate the relative importance of different state features in terms of the task.\n\n= Positive points =\n\nThe paper is well written and the ideas clearly presented. The ideas seem vaguely related with recent work on \"tuning\" MDPs [a] and some older work on learning state representations in multiagent settings [b,c], where the agents are allowed to \"pay\" to have better models or perceptions. The paper proposes the use of similar ideas in a completely different context - to identify relevant information state information in POMDP settings. \n\n= Negative points =\n\nMy main criticism is concerned with the particular domains considered, which I believe are too structured to provide a clear understanding of the potential impact of the proposed approach.\n\n= Comments = \n\nI believe that the problem considered in the paper is interesting and follows some recent work on \"tuning\" MDPs (see ref[a] below). The approach explored is quite simple but that is not an inconvenient per se. My main criticism lies in the fact that -- in my understanding -- the domains selected are too structured to provide really interesting insights. \n\nIn particular, all domains considered are classical control problems with essentially deterministic dynamics and full observability. The approach in the paper injects artificial additive noise in the state as perceived by the agent (the paper only provides explicit information regarding the noise in the Mountain Car domain, but I'm assuming that is similar in the other domains). \n\nNow I may be missing something, but it seems to me that, from the agent's perspective, this is equivalent to adding noise to the dynamics of the environment, since the agent treats the observations as state. Therefore, from the agent's perspective, the practical effect of the \"sensor tuning\" is to actually attenuate the noise in the dynamics, which partly explains the results provided. This renders this work particularly close to those on MDP tuning referred above, and more discussion in this direction would be appreciated.  \n\nI think that the paper would greatly benefit from considering richer domains, either where partial observability is a central issue -- such as those from the POMDP literature -- or with richer perceptual inputs --- such as those from game domains.\n\n= References = \n\n[a] A. Metelli, M. Mutti, M. Restelli. \"Configurable Markov Decision Processes.\" Proc. 35th Int. Conf. Machine Learning, pp. 3491-3500, 2018.\n\n[b] F. Melo, M. Veloso. \"Learning of coordination: Exploiting sparse interactions in multiagent systems.\" Proc. 8th Int. Conf. Autonomous Agents and Multiagent Systems, pp. 773-780, 2009.\n\n[c] Y. De Hauwere, P. Vrancx, A. Nowé. \"Learning multi-agent state space representations.\" Proc. 9th Int. Conf. Autonomous Agents and Multiagent Systems, pp. 715-722, 2010.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting setup, but limited experiments",
            "review": "\nIn contrast to standard reinforcement learning (RL), the paper investigates the variant where the observation made by the agent about its state has a cost. The authors propose to model the problem as a POMDP with an augmented action space (normal action + observation accuracy) and a new reward function that is defined as the original one penalized by the observation cost. They solve the problem with TRPO in three control domains: mountain car, pendulum, and cart pole.\n\nPROS\n\nI find the research questions asked in the paper interesting. The proposed variation seems to be novel as far as I know. Besides, two scenarios are studied in the paper, which correspond to two extreme cases: continuous vs discrete accuracy.\n\n\nCONS\n\nThe conclusions of the experiments seems to depend on the specific values set notably in Equations (9a-b) and (10). I think a discussion is warranted about how they were chosen. Notably, can the same conclusions be drawn if those values are changed? \n\nI didn't find the information about the policy used in TRPO. Notably, how does it deal with the partial observability?\n\nThe paper should be proof-read.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Small contribution and absent comparisons with previous works",
            "review": "This paper aims at studying an optimized way of collecting samples from an environment, discarding the ones for which the accuracy of the observation is high. This way the agent focuses on collecting only the samples that improve the knowledge of the state space.\n\nThis paper could be presented better, as the motivations of the work and the description of the method lack clarity and effectiveness. First, the title is somewhat misleading, as we cannot say that the agent is \"learning to observe\", which can remind something more related to feature extraction in representation learning. Indeed, the agent is learning to explore states under a certain criterion, i.e. minimizing the accuracy of the observation, closely reminding all the literature about intrinsically motivated exploration, that in this paper is only cited in the related works. After all, it looks to me that this paper is exclusively proposing a form of intrinsic reward, but it fails to explain it thoroughly. In particular, only a small subsection, namely 3.4, is dedicated to this description, moreover referring to \"reward shaping\", which is not the same concept as intrinsic motivation. The experimental section is weak, as it only analyses two simple RL problems, more problematically not comparing with any method in the literature.\n\nPros\n------\n* The paper addresses an interesting problem that can potentially improve sample-efficiency in deep RL problems.\n\nCons\n-------\n* Poor description of the methodology, in particular explaining its connection with intrinsic motivation;\n* No deep RL problems considered;\n* No comparisons with methods in literature.\n\nI recommend the authors to substantially restructure the paper to include a better analysis of how their method compares with intrinsic motivation, include deep RL problems where the problem of exploration and accuracy of observations is more accentuated, and add comparisons with representative baselines, e.g. Pathak et al (2017), Bellemare et al (2016), etc..\n\nPost-rebuttal feedback\n-------------------------------\nI thank the authors for their reply.\n\n> In contrast, our paper focuses on the following question: “how can we reduce the number/accuracy of the samples the agent \ntakes during the test phase”? (Here, the test phase corresponds to the agent’s behaviour after the training is completed.)\n\nI agree with the authors that intrinsic motivation is different, and perhaps in my review I expressed this concern too strongly. So I thank the authors for their long and informative answer.\n\n> We believe that the reviewer refers to the problems where a possibly large multi-dimensional data such as images in games are used as input to the RL algorithm.\n\nExactly. Experiments on high-dimensional problems would make the contribution of this paper stronger, considering the rather limited  theoretical/methodological impact that it has now. I strongly suggest the authors to work in this direction, perhaps on robotic application if possible.\n\nAfter the rebuttal, I still argue for rejection, although I increase my score from 3 to 4.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}