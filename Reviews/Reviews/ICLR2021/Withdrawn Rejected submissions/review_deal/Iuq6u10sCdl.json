{
    "Decision": "",
    "Reviews": [
        {
            "title": "Mathematical grounds of graph embedding. A problematic presentation, and unclear contributions.",
            "review": "In this paper the authors address the mathematical grounds of graph embedding, and try to give results regarding some properties of the embedding.\n\nThere are two key aspects: the presentation and the content of the paper.\n\nThe presentation is far from acceptable. Extra spaces between punctuation and words, capital letters in the middle of sentences, repeated words in the same sentence, or unfinished concepts/definitions, like in 2.2, just to name a few. The bibliography is not acceptable. Missing years, inconsistent formatting, references 21 to 25 in the same paragraph. There are also mistakes that can be found by running the spell checking tool. Some of the other points may be avoided by using LaTeX.\n\nIn the middle, there is the decision of what content to include. To me, all the Section 2, and the Theorem 1 should not be included in an academic paper, since they can be found in any book used for undergrad courses in topology.\n\nThen, there are some mathematical mistakes. For instance, the loss function cannot have those domain and codomain sets. The letter V is used for a vectorial space and also for the vertex set, within half a page of difference. The technical aspects are not well defined. A graph is not defined (is it an undirected unweighted graph?, because there is a weird sentence: \"We also assume that graph is defined by the position of its Vertices\", from which I understand that the vertices have a position in some ambient space...)\n\nTheorem 2 seems to be the main contribution of the paper. However, the statement of the theorem is vague, and I could not infer the proper statement from the proof, which is also very unclear.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper might contain valuable work, but in my opinion it is not ready for publication, yet. My major concern regards the considered setup.",
            "review": "The paper claims to provide theoretical guarantees about the existence and uniqueness of optimal maps to embed the nodes of a graph into a vector space while preserving certain proximity properties.\n\nThe paper might contain valuable work, but in my opinion, it is not ready for publication, yet. My major concern regards the considered setup, which is not sufficiently motivated and appears to be quite strict. I also raise some concerns about the overall clarity of the presentation and the math developments.\n\nAssumptions and motivation.\n1. The linearity assumption of the embedding map seems quite restrictive. I agree that most of the methods include a linear step, but I argue that there is some preceding non-linear part that should be considered. Moreover, I think that literature for such types of embedding is available, and it is not clear to me how much improvement this work can bring to, eg, \"Duin, Robert PW, and Elzbieta Pekalska. Dissimilarity Representation For Pattern Recognition, The: Foundations And Applications. Vol. 64. World scientific, 2005\".\n2. I suggest expanding the discussion about the importance of having a unique solution here, once the existence is granted. After all, I think that we are more interested in retrieving one of the (possibly multiple) optimal solutions.\n3. Sec 3, \"all ... are ... functions of the coordinates in latent space\". This seems incorrect because in the preceding expression $S_G$ is function of the nodes themselves.\n4. Sec 3, \"decoders are all multivariate polynomials\". The claim seems incorrect to me. It is not unusual to consider squashing functions, like a sigmoid, to approximate an adjacency matrix.\n5. Sec 3, \"polynomial proximity measures\" seems to refer to $S_G$. It is unclear however with respect to what proximity measures $S_G$ are polynomial. For example, the adjacency matrix reported in table 1, is not polynomial in $v_i$ and $v_j$.\n6. In Sec 3 it is said that the goal is to construct a good decoder. I would rather say that it is sufficient that a good decoder exists, with the encoder part being more important.\n7. Sec 5 considers the two optimization problems (encoding and decoding) separately, however, I think they should be jointly addressed.\n8. It appears to me that the claim in the first sentence of Sec 6 is incorrect. The current work is not providing means to compare node embeddings, rather it provides existence results.\n\nPaper presentation:\n1. Par 4, Sec 1. If I understand correctly the setting, the conclusion that $MxVxV$ is a Hilbert space directly follows from the fact that, when equipped with appropriate norms, $M$ and $V$ are Hilbert spaces themselves. It doesn't look to be a contribution by the paper.\n2. Th 2, part 1. It seems impossible to me that the optimal embedding is unique because any rotation results in a different, yet equivalently optimal, embedding.\n3. Th 2, part 2. Please, clarify \"uniquely approximated\". I guess it means that there is a unique optimal approximation. If so, please specify the optimality criteria.\n4. Th 2, part 2.b is unclear to me. Non-squared matrices are included in statement (a). So it seems that (a) does subsume (b).\n5. Th 2, proof, part (i). Finite-dimensional vector space does not imply Hilbert space. Secondly, $E$ is a vector space, hence contains the null vector which has the smallest norm. So delta appears to be trivially 0.\n6. Sec 4, Remark, bullet 1. I suggest providing further insights into why homogeneity is not enough.\n7. Sec 4, Remark, bullet 2. I don't see why bounded-rank matrices cannot be used. Only the uniqueness may fail, but optimality seems still achievable.\n\nMinor comments\n- Sec 2.2. Definition of compact space not concluded.\n- Sec 2.3. Symmetric and reflexive terms are inverted.\n- Sec 2.3. The concatenation of homotopies in incorrect, t's in H and K has to be rescaled and shifted.\n- Sec 2.3. The definition of fundamental group seems unusual because it generally operates on loops.\n- Sec 2.5.1. The notion of completeness is used before being defined.\n- I suggest keeping a consistent numbering in the theorems and associated proofs.\n- The concept of decoder used here is not the most common. The paper would benefit from more discussion and motivation about it.\n- Th 2, proof. Several quantities are not introduced, such as X, Y and alpha.\n- Differently from what claimed in the text, Table 1 only reports information about the decoder, but not the encoder.\n- Several typos. I report here only some of them. Sec 1, par 2: emperical -> empirical. Sec 1, par 3: sinlge -> single. Sec 3: encode -> encoder, fuctions -> functions....\n- Reference [4] appears to be incorrectly reporting the paper title.\n- Reference [21] contains all subsequent ones.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Graph Embedding via Topology and Functional Analysis",
            "review": "The paper attempts a theoretical study of graph neural networks, respectively graph embeddings, by appealing to tools from functional analysis and topology.\n\nPro:\nThe author probably had an interesting idea in mind here.\n\nCons:\nUnfortunately, the paper is simply not finished, but seems to be a work in progress. There are a large number of inconsistencies, I am afraid, including many typos, grammatical mistakes, and ill-formated references.\n\nThe abstract states  that a number of important results are proven but the authors fail to say what these important results should be, what their implications would be etc. \nMost of the claims appear to be essentially trivial or not well supported\n\nThere is also a lack of motivation and context -- why does the author think this would be a useful study? what have other people done in terms of related work? All this would need to be articulated far more clearly.\n\nThe logical flow of the paper is not clear to me and accordingly I cannot really see what the author tries to achieve here.  For instance, a number of standard and sometimes trivial results are proven, but how these should help in advancing the goals of this paper is unclear. \n\nLet me provide a few concrete examples:\n- The Preliminaries list a number of definitions and tools from topology: topological spaces, compact space, homotopy and the fundamental group. However, none of this is really needed or used afterwards.\n- Section 3 \"encoder-decoder perspective\" initially provides some fairly generic description (though difficult to follow) description of a graph embedding problem, but then the authors restrict themselves in a half sentence to a) linear maps (encoder) and b) to a l_2 loss function. One may argue that l_2 loss is pretty common, it is nonetheless far from a general result as promised.\nMoreover, if we restrict ourselves to approximating a matrix in l_2 loss then the best approximation would simply be an SVD (Eckhard-Young).\n- In terms of section 4 -- the main results section: Theorem 1 is basically listing well known facts about convex sets and vector space. There is no contribution here and, moreover, there is actually hardly any relevance of the theorem for this specific paper.\n- Similarly the author fails to explain how theorem 2 is relevant to the problem he wants to study; and how the proofs are supposed to connect to the theorem is also not clearly explained.\n\nI thus cannot recommend anything but rejection here, as I also do not see how the paper could be revised to make it acceptable --- in my opinion this would essentially amount to a new submission.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper was poorly-written, felt incomplete, and was excessively mathy with no reason to be so.",
            "review": "# Quality\n\nThe quality of this paper is poor.\n\nThere was clearly very little effort put into presentation of the authors' work. Typos are present throughout, along with obvious mistakes including, but not limited to: not capitalizing beginnings of sentences, writing in fragments, not typesetting equations properly, and generally sloppy layout. In future endeavors, the author(s) should consult with a colleague who speaks English at a proficient level, or put more time in to writing with proper grammar.\n\n# Clarity\n\nThis paper is quite unclear, mostly stemming from the previously described issues with grammar. Not only is the discussion throughout this paper poorly written, it is lacking in depth.\n\nThe first three pages of \"Prerequisites\" string together elementary definitions from topology and functional analysis in a style reminiscent of a set of lecture notes. In fact, most of the definitions in this section are either not used or barely used: I would cite this as an example of \"excessive mathiness\" [1]. \n\nThe discussion relevant to graph embedding begins in Section 3: \"Encoder-Decoder Perspective of Graph Embedding.\" The authors, with very little motivation, list out simple definitions for encoders, decoders, loss functions, and proximity functions. They then refer to a popular existing paper on graph embedding [2] in an attempt to explain what they were just talking about, but this lacks real discussion to tie together their set of definitions/framework to existing methods. They also make dodgy claims in this section, such as \"decoders are all multivariate polynomials in their coordinates in latent space.\" Perhaps this is true in the few methods they examine, but this is barely explained.\n\nThe results are presented in Section 4: \"Mathematical Results Corresponding to Graph Embedding.\" Theorem 1 is simply a set of trivial statements regarding convex sets, polynomial vector spaces, and their products. This Theorem is not ever referenced in this paper, other than in its statement. Theorem 2 is stated imprecisely, and proven with excessively fancy terms. It reads as if the authors had some ideas on graph embedding, and then decided they wanted to incorporate a list of basic results in topology and functional analysis, rather than using appropriate mathematical tools to answer a standing question. Even if the authors' goal as they claim in the conclusion is to \"disclose the relation between two superficially unfamiliar fields\" and not necessarily to solve an engineering problem, they have failed to uncover any interesting underlying principles of graph embedding in this paper.\n\nBy Section 5: \"Algorithmic and Computational Aspects,\" the authors have moved on to making claims on their results' relevance to embedding algorithms, where they essentially state that embedding algorithms are convex and solvable. Again, this statement is poorly supported, and shrouded in excessive mathiness, rather than any solid principles. If the authors' claims are true here, they should have been able to demonstrate the solution uniqueness yielded by convexity in all of the described settings.\n\nFinally, although it is a tired cliche in the review of machine learning papers: there are no experiments.\n\n# Originality\n\nIt's hard to say if this paper is original or not. It certainly takes an approach to graph embedding that I have not seen before, but the link the authors claim with the fields of topology and functional analysis is tenuous at best. Moreover, a significant portion of the paper is devoted to repeating (largely uncited, I might add) elementary results in mathematics.\n\n# Significance\n\nThis paper's problems are too strong to glean any significance from it. Perhaps if there was some attention to detail in its writing, then it could have marginal impact. In this work's current state, however, I see no way in which this paper would have any positive impact on the ICLR community.\n\n# Overall review\n\nThis paper is deeply flawed, and attempts to cover up its lack of depth with excessive mathematical notation and faux-formalism. I recommend with overwhelming confidence that it is rejected from ICLR 2021.\n\n# References\n\n[1] Lipton, Zachary C., and Jacob Steinhardt. \"Troubling trends in machine learning scholarship.\" arXiv preprint arXiv:1807.03341 (2018).\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}