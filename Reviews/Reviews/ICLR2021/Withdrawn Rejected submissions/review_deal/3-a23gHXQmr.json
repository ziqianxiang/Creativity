{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers are in consensus that the manuscript is not ready for publication in its current form: more comprehensive evaluation, and careful analysis (either theoretical or empirical) of the simple-but-effective methodology would improve the quality further. The discussion was constructive and helped the authors to reason about their work better.\n\nThe AC recommends Reject and encourages the authors to take the constructive feedback into consideration . "
    },
    "Reviews": [
        {
            "title": "AnonReviewer1",
            "review": "* **Summary**: \n\nThis paper propose to improve parametric density estimation for unseen environment using uncertainty-aware neural networks, and proposed a practical, two-step algorithm based on DeepEnsemble. Given training data $(y, x)$ and a known density family $p(y|\\theta)$, the goal is to learn $\\theta$ in an unseen environment $\\{x', y'\\}$. \n\nThe basic method proceeds as follows: (step 1) at training time $(y, x)$, train a deep ensemble model $\\{ f_1, .., f_M \\}$ to learn a mapping $f: x \\rightarrow y$, (step 2) at testing time, estimate parameters of interest $\\theta$ by MLE w.r.t. $p(y'|\\theta)$, where $y'$ comes from ensemble predictions $f_1(x')), .., f_M(x'))$.  Based on this basic recipe, authors proposed two augmentations: (a) select only the top performing ensemble members at test time. (b) re-weight the training objective using a function of the predictive variance $w_{nm} \\propto \\sigma_{nm}^{-\\lambda}$, where $\\lambda$ is an application specific parameter.\n\n* **Strength and Weakness**\n   * (Strength) A novel method for density parameter estimation in physics problems that account for uncertainty. \n   * (Strength) An interesting application to X-ray polarization, showing clear advantage over existing approaches. \n   * (Weakness) There exists some theoretical concerns on the soundness of the approach, which should be addressed by adding additional discussion, please see Major Comments. \n   * (Weakness) Insufficient ablation study for the proposed modifications (ensemble member selection and re-weighting), as a result it is unclear the relative contribution of each components. \n\n* **Recommendation**: I recommend reject the manuscript in its current form. While acknowledging the novelty and significant of the application, I find the method a relatively straightforward combination of existing techniques (deep uncertainty and sample re-weighting), without sufficient in-depth analysis (either theoretical or empirical) on the merit the combination for the intended application. There are also some potential theoretical concerns that needs to be addressed. I'm open to adjust my recommendation, assuming these concerns are sufficiently discussed in the paper and additional ablation is conducted.\n\n* **Major Comments**:\n  * Why two-stage approach / use Gaussian likelihood: If I understand correctly, at test time, authors trained a deep model to generate uncertainty-aware predictions $y_{test}$ using Gaussian likelihood, and then conduct parameter estimation by performing MLE over a weighted likelihood constructed using the deep ensemble prediction. I have two concerns over this approach: (1) In the case that distribution of y is not Gaussian (e.g., Equation 4), how to justify learning y using a Gaussian likelihood? Would that lead to issues in uncertainty quantification, since model likelihood is mis-specified? (2) Even if it is admissible to learn y using Gaussian likelihood, are we risking under-estimating uncertainty by using MLE to estimate parameters in the second stage? In comparison, why can't we estimate the model parameters $\\theta=(\\phi, \\Pi)$ jointly with $y$ (e.g., jointly learn $y=f_y(x)$ and $\\theta=f_\\theta(x)$ using deep ensemble by minimizing the correct likelihood $p(y|\\theta)$)? Because by doing so you are learning with respect to the correct likelihood, and the uncertainties can be quantified end-to-end via deep ensemble. It would be good if author can provide discussion clarifying (1), and discuss / compare the method outlined in (2) as an intuitive baseline.\n\n  * Uncertainty under model selection: At test time, author used only the best-performing ensemble members to construct the model likelihood. There might be an concern regarding uncertainty quantification under model selection: since the model selection is not uniform, the predictive uncertainty from the select model is no longer a representative sample of the original deep ensemble. Would this cause issue in terms of uncertainty quantification? It might be good for author to justify this at least empirically by comparing against a baseline with no model selection.\n \n\n* **Minor Comments**:\n  * Notation: This is very minor: author used $k$ for total number of parameters, and $K$ for testing data points. This can be a bit confusing. It might be good to use consistently use lower case for index, and upper case for total number of parameter / samples. So it might be good to use $N_{train}$, $N_{test}$ to indicate sample size, and $K$ for the total number of parameters to estimate.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Appealingly simple method, interesting application; baselines may not be sufficient",
            "review": "Update: After reading the other reviews/responses, I'm keeping my score of 5, due to pervasive concerns about the narrow focus of the experiments and incremental novelty of the method.\n\nThis paper proposes a method that uses ensembles of deep neural networks for parametric feature density estimation and enables control of the bias-variance tradeoff, and demonstrates the method’s performance on an X-ray polarimetry task. The paper is well-presented, the method is straightforward to implement, and the choice of experiments in the natural and physical sciences is interesting and relevant. The case study on X-ray polarimetry shows convincing results (pending some concerns on baselines mentioned below), and the authors make a good argument for the relevance of the method to parametric density estimation in the natural/physical sciences more generally.\n\nThe method appears to be sound and principled, and involves combining epistemic uncertainty (captured by the NN ensemble) with aleatoric uncertainty (estimated directly in the loss function) using quadrature. A drawback of the method is the additional computational cost incurred through training multiple ensembles. The authors mention a connection to Bayesian methods, which I believe could use further elaboration.\n\nBaselines are the standard IXPE method (based on matching moments) and a simpler NN-based approach. Bayesian methods (MCMC or variational inference) for parameter estimation would seem to provide a natural basis for comparison as well -- why were these not considered? A full Bayesian neural network likely isn’t necessary, but perhaps incorporating Bayesian uncertainty over the last layer of weights, or a similar baseline the authors deem appropriate.\n\nWhy was IPOPT chosen, instead of more standard gradient descent methods? Constraints on parameters could also be achieved via transformations of an unconstrained space (e.g. a softplus function to transform the real line to R+).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Straightforward approach; the methodology lacks experimental justification.",
            "review": "# Paper Summary\nThis paper addresses the problem of estimating the distribution parameters of features extracted from a set of high dimensional observations, a problem that is common in the physical sciences. To solve this problem, the authors present a deep learning approach that utilises a combination of (i) deep ensemble training, (ii) post hoc model selection, and (iii) importance weighted parameter estimation. First, a deep ensemble is trained to solve a regression task (observation -> feature). During testing, this ensemble is frozen and used to generate feature samples from unseen observations. Using these feature samples, it is possible to estimate the distribution parameters using maximum likelihood estimation. The authors evaluate their method on X-ray polarimetry, and compare it with two other approaches, one of which is also a deep learning approach. On all tasks, the presented method outperforms both baseline approaches.\n\n# Assessment Summary\nThis paper presents a flexible, data agnostic, and easy to implement approach to *parametric density estimation*. The paper is clearly written, giving a good understanding of the different components of the approach. I would feel confident to implement this approach myself.\n\nHowever, I believe that the presented methodology, in particular steps (ii) and (iii) achieve the opposite of the authors intentions (see negatives below). Further, the use of an ensemble model is not properly motivated. The experimental section does not give justification to each of the model components. \n\nI therefor cannot support the acceptance of this paper.\n\nFor a future manuscript, I recommend the authors to add an ablation study (see below).\n\n# Positives\n- Flexible approach: The approach is separated into two stages: (1) Training a deep ensemble on a regression task, and (2) maximum likelihood estimation of distribution parameters under the ensemble and an unseen test set. This two stage process makes the approach very flexible. The ensemble is trained once on a data set, and then exploited on multiple test sets - even with different likelihoods - for ML estimation.\n- Data agnostic: The approach does not make any further assumptions about the data, apart from the fact that the ensemble can be trained in a regression task.\n- Easy to implement: The presented approach chains a number of simple components together (deep ensembles, sample reweighting, ML estimation). Each of these components can be found or easily implemented in most common learning frameworks. Advancements in each of these components can have a trickle down effect on this approach.\n- The paper is clearly written.\n\n# Negatives\n- Step (ii) of the approach is used to select a sub-set of models from the ensemble that was trained in step (i). Step 3 & 4 in algorithm 1 gives a hint of how this selection is implemented: On a new data set the features $y$ are estimated from each neural network of the ensemble. The density parameters are then fit to approximate the distribution of features from each network under a given likelihood (I believe it should say maximize instead of minimized in algorithm 1: 3). The models for which the parameters can be best fit will be selected for step (iii). The authors claim that this removes the models with highest bias from the ensemble. However, at least in the way I understood it, this approach will in fact select the most biased models. Imagine a network that always output the same feature values. It will be easy to fit these values using the density parameters.\n- Step (iii) will give highest importance to the most confident model prediction, disregarding whether that prediction is correct or not (we cannot know during test time). A confident but wrong predictor can therefore dominate the loss for a given sample, essentially eliminating the benefit of the ensemble.\n- The experimental evaluation does not give any insights into which components of the approach actually help in performance. A proper ablation study should be carried out (see recommendations below)\n\n# Minor Comments\n- Figure 2 does not follow the formatting guidelines. Figure and table should be separated.\n- Wrong citation command (citet <-> citep)\n  - Section 2.2, 2nd paragraph\n  - Section 2.2, 4th paragraph\n  - Section 2.3, 2nd paragraph\n  - Section 2.3, 4th paragraph\n  - Section 2.3, 5th paragraph\n  - Section 2.4, 2nd paragraph\n  - Section 3.1, 1st paragraph\n  - Section 3.1, 5th paragraph\n  - Section 3.4, 1st paragraph\n  - Section 4, 1st paragraph \n\n# Recommmendations\n- Ablation study: The authors present 3 components to their approach: (1) A deep ensemble, (2) model selection, and (3) importance weighting. Whether each of these components is necessary to achieve the performance as presented in the results section is not clear. To test that, the authors should carry out an ablation study by varying the size of the ensemble, varying the number of M top performing models, and by adding or removing the sample reweighting from step (iii).\n\n--------------------------------------------------------------------------------\n\n# Review Update\nI thank the authors for their thorough response and the additional experiments. Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). I would have liked to score the paper higher, but at this stage I believe the paper is still not ready to be published. The authors acknowledged in their update that the review process helped them to understand their own work better. As a result, some aspects of their approach have been changed (e.g. removing step(ii), changes to step (iii)). I believe changes to the method go beyond the scope of the discussion phase and instead justify resubmission. This would give the authors some more time to get an in depth understanding of their approach as well.\n\n## Author comments on step (ii)\nI thank the authors for clarifying. In their response, the authors claim that they will use a held-out data set with known density parameters. It is then possible to evaluate which models in the ensemble best estimate these density parameters. I have some issues with this claim:\n\n1. This is not made clear in the paper.\n2. The approach assumes that the density parameters are unknown. Adding this assumption will weaken the paper.\n3. The selected models will be biased toward the held-out set.\n\n## Author comments on step (iii), now step (ii)\nIn their update, the authors change the reweighting scheme. Instead of having a model-based weight, the reweighting is now done solely on a per-sample basis. I believe this looks like the right direction to take.\n\n## Ablation study\nThe ablation study is important. One possible addition would be to make a comparison for different ensemble sizes.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The authors propose a new method to learn unconditional generative models on the targets of a dataset via ensembling. ",
            "review": "The authors propose an approach to constructing an unconditional model on the targets (which they refer to as features) on a supervised learning dataset. As I understand it,  such models are useful in physics applications, for example. However, directly learning such a robustly model may be challenging. Thus, the authors proposed a two stage approach where, they first train an ensemble of probabilistic discriminative regression models, and use the predicted means and variances as important weights for max-likelihood estimation of the unconditional (marginal) distribution over the targets. The proposed approach works better on a range of physics-based applications of ML (X-RAY POLARIMETRY).\n\nThis work, however, raises some questions. Firstly, this work is difficult to read due to non-standard machine learning terminology, likely due to the authors background in physics? This makes it challenging to fully understand what the author's aim is (I only managed managed to put to together by the middle of paper on a second reading). I appreciate that writing cross-disciplinary work is challenging, however, I strongly encourage the authors to improve upon the clarity and structure, and to bring the terminology closer to the standard in the ML community. \n\nSecond, why was no comparison made to directly estimating the parameters of the generative model on the targets (predicted features) in the polarimetry dataset? It seems that a natural baseline for a Deep-Ensemble-based importance weighting likelihood loss would be a standard likelihood loss. If this comparison was made - can you please clarify where?\n\nMore generally, I would be keen to see an ablation study, first where a model is trained on targets (predicted features) in the dataset, then on predictions from a neural net, then on an ensemble of predictions (but no importance weighting), and then finally with importance weighting via predicted variance. This will allow the authors the piece apart which part of their contribution is useful and which isn't. In addition, comparison to use the full ensemble vs. the selected ensemble would also be informative.   \n\nFinally, given the the authors claim the method is useful for other application - it would be good to have a demonstration. As it currently stands, this method is only validated on the X-ray polarimetry task.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}