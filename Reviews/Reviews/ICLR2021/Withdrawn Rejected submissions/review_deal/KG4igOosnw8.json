{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper explores the connection between diversity of gradients and discriminativeness of representations. Based on the observations, authors propose Discriminative Representation Loss (DRL).\n\nThis paper resulted in a lot of discussions and specifically, R5's detailed comments helped the authors improve their paper. Authors did a good job in making significant improvements to the paper based on the reviews, including better connecting the theory and experiments. However, after much discussion it was felt that experiments and analysis still needed improvement, leading to a decision to reject. The authors are encouraged to use the reviewers' post-discussion updates to further improve and submit to a future venue."
    },
    "Reviews": [
        {
            "title": "Connecting representation learning to continual learning would be of great value, but the theoretical and empirical claims made in the paper are insufficiently substantiated",
            "review": "############## Summary ##############\n\nThis submission draws a connection between deep representation learning and continual learning. The authors include theoretical and empirical analyses that suggest that the learned model for continual learning should both separate the representations of instances of different classes and separate the representations of instances within a single class. This motivates a new representation-learning-based continual learning method called DRL, which the authors evaluate empirically against various baselines.\n\n############## Strengths ##############\n\n1. The problem studied in this work is interesting. A deeper understanding of what types of representations could enable continual learning could potentially be very impactful to the field.\n2. The paper includes both theoretical and empirical analyses.\n3. The experimental setting is described in detail and includes a comprehensive set of baselines.\n\n############## Weaknesses ##############\n\n1. The theoretical findings seem to be somewhat disconnected from the empirical study in Section 2.1, and the analysis connecting the two is unconvincing. Moreover, given this disconnect, the empirical study should have been much more complete, including various data sets.\n2. The writing and the structure of the paper is quite unclear, leaving the reader confused at times, having to jump back and forth to draw connections between different sections that are not explicitly described in the text.\n3. The experimental evaluation is inconsistent in showing the advantage of their method, and this is not analyzed in any depth.\n\n\n############## Recommendation ##############\n\nUnfortunately, I lean towards recommending the rejection of this paper. While the studied problem is of high relevance, whose answer could be very impactful, I find the theoretical and empirical analyses in this work to be unconvincing, and thus the claims made throughout the paper to be insufficiently founded. The authors use very simple toy experiments and theorems on linear models to motivate their claim that learned representations should be very different across different classes. While using theory from linear models is a common technique for motivating empirical study of deep learning models, a much more comprehensive empirical study would be needed to validate that the theoretical findings extend to the deep learning setting. Moreover, the paper should be substantially revised both to add clarity to the motivation of the problem, and to more carefully analyze the connections between the theoretical and empirical analyses in Section 2.1.\n\n\n############## Arguments ##############\n\nThe biggest reason I lean towards rejecting this paper is that I am unconvinced by the claims made throughout Section 2. \n\nFirst, the section begins by suggesting that instances with similar gradients are those most useful for generalization. This seems to contradict the findings of Aljundi et al., which chooses to keep samples with maximum gradient diversity in the episodic memory. This apparent contradiction should be carefully discussed and analyzed.\n\nThen, the experiment of Figure 1 suggests that: the gradients with the most diverse gradients are those close to the decision boundary, and in order to avoid forgetting previously seen tasks, samples with diverse representations help maintain good decision boundaries as new tasks are learned. The former claim is further studied in the rest of the section, but why does this experiment suggest that we would like to keep samples with diverse gradients? I keep seeing conflicting indications that we should store samples with diverse or similar gradients. On the other hand, the latter claim is not substantiated beyond these toy experiments. It seems to be specific to incremental-class learning, and would probably not hold for other variants of continual learning that don't require the same model to discern between the new class and the previous ones. \n\nIn Theorem 1, the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients. Figure 4 in the Appendix seeks to illustrate this, and could potentially be very helpful towards understanding this work better, but  1) the figure is missing a y-axis label and 2) the caption is confusing and contradictory between the sub-figure captions and the full figure caption, which makes it hard to take much away from it. Moreover, the claim hinges on the assumption that <x_n, x_m> is likely to be positive if classes are near the decision boundary. This doesn't seem to be addressed in Figure 4, and I don't believe it is necessarily the case in high-dimensional spaces, where points can be very different and still close to the decision boundary. Are the authors assuming that _all_ inner products of the representations are positive, since they use ReLU activations? This should be explicitly clarified.\n\nIn Theorem 2, the authors mention that a deep net can be viewed as a representation extractor, and that ReLU activation would lead to positive inner products for the representations.\n1. First, this seems vacuous, since all inner products (even across classes) would be positive.\n2. Second, the inner product of the gradients _of the final linear layer_ are the ones that are guaranteed to have positive inner products, but this claim says nothing about the gradients of remaining layers in the network. ~~Since recent work [1] has shown that forgetting primarily occurs at the shallow layers of the network, this does not seem to be a helpful theorem.~~ It is still useful, since recent work [1] showed that forgetting primarily occurs at the deepest layers of the network, but this should be stated clearly.\n\nThen, the authors move to a simple experiment using deep nets, which is supposed to help extend the findings of Theorems 1 and 2 to the deep learning case. However, the connections between these two appear to be very weak and poorly described.\n- It is unclear why representations are obtained by concatenating the outputs of all layers, or how this relates to Theorems 1 and 2. Why not use directly the input feature representation, or better yet use only the final hidden representation before the output layer as hinted at in Theorem 2? This choice is not justified.\n- Are the gradients also obtained by concatenating all layers' gradients? This _should_ certainly be the case, since we want to learn about the gradient of the entire model.\n- It seems that in Theorem 1 we care only about the sign of the gradient: positive representation similarity likely leads to negative gradient similarity. In Figure 2, _all_ points have positive representation similarity, so it's unclear how we can relate these two. It seems that we should now look for decreasing gradient similarity as representation similarity increases between different classes, but this is not stated or analyzed. Also, there is no mention of \\beta, \\delta, or \\alpha, all of which could be explicitly computed and used to analyze these results.\n\nOverall, the Theorems seem to give results only about signs and not distances, whereas the deep learning experiments try to extend the analysis with distances. This means that distances are actually far more useful for relating the findings to metric learning. Since the empirical findings are actually different from the theoretical ones, these experiments should be far more extensive to justify the proposed method, including analyses on more complex data sets, and a much clearer description of exactly how the experiments are carried out.\n\nThe take-away from Section 2.1 seems to be that similar representations across different classes lead to conflicting gradients. This is only hinted at in various ways, but I don't believe it is directly stated. Clarifying this would tremendously help lead into Section 2.2. Moreover, it is never explained what the intuition behind this is, which would also be very helpful to increase the reader's ability to follow the paper. I would expect that the intuition is that having similar representations across different tasks would lead the network to somehow confuse the different classes, but I don't believe this is explained.\n\nIn Section 2.2, I find the motivation for learning a diverse set of representations for each class somewhat unconvincing. The experiments of Figure 1 show that the learner suffers forgetting on the previous tasks because it only keeps points with similar representation. However, the only reason why this is a problem is that there exist other points with very dissimilar representations, which will then be ignored by the updated margin. If we learned a representation where all points within a class are similar, then that would not seem to be a problem. While this is somewhat analyzed in the experiments in Table 4 via the rho-spectrum, this analysis is quite incomplete. Moreover, Table 4 shows no correlation between forgetting and rho-spectrum, which is what Figure 1 suggests we should expect. This is not discussed in any way.\n\nTo add to the confusion in the analysis, the experiments in Section 5 use inconsistent definitions of what the representation is, with all layers being used as the representation in some cases (just like in the analysis of Figure 2), but only the final representation and the logits being used in other cases (which, as an aside, violates the claim that the inner products are necessarily positive).\n\nThe empirical results somewhat inconsistently show that the proposed method is better than baselines. However, considering that MNIST-based data sets are usually not very indicative of performance in more complex data sets, I would focus my analysis on the CIFAR-10 and CIFAR-100 experiments. Here, the proposed method was best in one case but not in another. This would require a much more in-depth analysis, but the whole results analysis is limited to a single paragraph. \n\n\n[1] Ramasesh, V. V., Dyer, E., & Raghu, M. (2020). Anatomy of catastrophic forgetting: Hidden representations and task semantics. arXiv preprint arXiv:2007.07400.\n\n############## Additional feedback ##############\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nAbstract\n- DRL is efficient w.r.t. what?\n    - The intro seems to suggest it's computational efficiency\n    - The first time we actually get to see what this means is at the end of the paper, buried in the last paragraph of Section 5. Since this seems to be such a relevant point (enough to point to it in the Appendix and Introduction), it would be relevant to bring it up earlier and give more details about how it compares to other methods.\n\nIntro\n- I wouldn't consider that methods for growing the model size fall in the same category as regularization-based methods. Their focus isn't preserving knowledge of past tasks' models, but instead adding the current task's knowledge into a separate model.\n- Paragraph 4: \"direction that closest\" --> \"direction that [is] closest\"\n- Why such a detailed summary of gradient-based methods? Push to separate Related Work section.\n- Again it's unclear what the metric for efficiency is. Context seems to suggest it's computation time, but one could also think it's data efficiency or memory capacity (since the former is commonly important in lifelong learning and the latter is mentioned in this section).\n\nSec 2.1\n- Why two different notations for x_n with and without boldface? They both seem to denote a single data point.\n- Theorem 1 is the first time the authors mention negative inner product of the gradients. Before, only the terms \"similar\" and \"diverse\" were used. I suggest making this clearer in the explanation throughout.\n- Deep learning experiments\n    - Is the correlation coefficient between the x axis (representation similarity) and the y axis (gradient similarity)?\n    - It would be helpful to clarify explicitly in the text that in Figure 2 the blue dots should be analyzed under Theorem 1 and the orange dots under Theorem 2.\n    - The final statement about discriminative representations of task 1 vs task 2 doesn't seem to add much value, so my suggestion would be to drop it or clarify how it's useful to the argument of the paper.\n\nSec 2.2\n- The first sentence in second paragraph seems disconnected from what follows. What's the point of it? Where will the unused information be leveraged?\n\nSec 3\n- Is the representation h_i the same as in the experiments of Figure 2, i.e., the concatenated output of all layers?\n    - It is made clear later that the definition of h_i is not consistent throughout the experiments. Here, the authors should clarify this and explain how this definition should be chosen, and why it is not possible to use a consistent definition.\n- Figure 3 is missing a y-axis label.\n- The effects on the similarities within classes seem to be very small in Figure 3. Is this because \\alpha is small?\n\nSec 4\n- The statement that in the incremental class setting the method can work without task boundaries seems vacuous: whenever the learner sees a new class, evidently it corresponds to a new task.\n- This is the first time that task-incremental vs class-incremental settings are introduced, without explaining exactly what they are. The manuscript should be self-contained and so it should give a clear description of these settings if they are relevant to the submission (as they clearly are based on the statements in this section).\n\nSec 5\n- Fashin-MNIST --> Fashion-MNIST\n- Details of the experimental setting are quite comprehensive (+).\n- In addition, We --> In addition, we\n- What is the reason the proposed method achieves a better trade-off between forgetting and intransigence? This is not mentioned at all at any point in the paper up to this point.\n- Why only compare computation time on MNIST and not on remaining benchmarks? Since computational complexity depends on the representation size, it is thus clearly dependent on the choice of representation, which inconsistently varies across benchmarks. Also, what is the theoretical complexity of the baselines? Are there settings where they would, at least theoretically, be faster? This comparison would strengthen the claim that the method is more efficient.\n- I would also like to see an ablative study removing each of the terms in the regularization function in turn. If ER is not implemented using the same sampling strategy as BER, then I'd also like to see the effect of BER training on its own. This would help tease apart the contributions of this work better.\n\nAppendices\n- How are the hyper-parameters of DRL chosen in Table 6?\n\n\n----------- Updates during discussion -----------\n\nI have updated my score from a 4 to a 5 based on the authors' response. My justification is below in my comment to the authors.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper analyzes the relationship of gradient diversity to the performance of continual learning systems, the analysis inspires a novel loss which shows some improvement in the continual learning setting.\n\nStrengths:\n\n-The empirical analysis is interesting making connections to Liu 2020 and some effective visualizations. The linear cases is studied in detail and provides intuition for the method\n\n-The proposed loss gives some improvement in larger memory settings\n\n\nWeaknesses:\n\n-Proposed method is too informally motivated by diversity analysis. It’s not clear to the reviewer how the theoretical observation regarding the linear case can be extended to the non-linear case. \n\n-Experimental results are promising but not completely convincing especially with the 2 additional hyperparameters:\n1. The authors use buffer settings much higher than prior works (e.g. [a,b] consider memories of 200,500,1000), this choice is not explained nor lack of comparison to any existing published result\n2. There are several works e.g. [a,b,c] which consider the same setting as this work and have better performance than the baselines shown. For example for CIFAR-10 [a] uses smaller memory size and has higher performance than shown here. \n\n-Cross entropy is already indirectly bounding something similar to the L_bt-L_wi ([e]), so the objective with 2 hyper-parameters seems overly cumbersome.\n\n-Related Work is a bit sparse. Besides the works mentioned above there is several considering metric learning based approaches for continual learning which should be discussed (e.g. [d])\n\nQuestion: \n-Are the hidden representations normalized? \n-The minibatches used are small and thus can have few same class pairs, how do the authors assure that L_wi can be optimized in this case, particularly with say 100 classes as in CIFAR-100. \n\nOverall, I find the work promising. The observations regarding gradient diversity might be the basis of future more effective methods. On the other hand the exact method proposed and results are not completely convincing in their current state. \n\n[a] Aljundi et al “Online Continual Learning with Maximally Interfered Retrieval” \n[b] Ji et al “Automatic recall machines: Internal replay, continual learning and the brain”\n[c] Caccia et al “Online Learned Continual Compression with Adaptive Quantization Modules” \n[d]  Li et al  “Better Knowledge Retention through Metric Learning”\n[e] Boudiaf \"A unifying mutual information view of\nmetric learning: cross-entropy vs. pairwise losses\"\n\n----------------\n\nPost-rebuttal:\nAfter reading the rebuttal I maintain my score. The observations seem promising, the method a bit cumbersome but also interesting, but neither of them are fully fleshed out. The authors should either greatly expand the empirical analysis (in the non-linear setting) of their claims on intra and inter class variability in CL  and/or make the experiments of the DRL method more convincing and varied in scope",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Insufficient experiments ",
            "review": "This paper presents a novel way of making full use of compact episodic memory to alleviate catastrophic forgetting in continual learning. This is done by adding the proposed discriminative representation loss to regularize the gradients produced by new samples. Authors gave insightful analysis on the influence of gradient diversity to the performance of continual learning, and proposed a regularization that connects metric learning and continual learning. However, there are still some issues to be addressed as below.\n\n1. Authors do not explain how to determine weights of the two terms in the proposed discriminative representation loss to achieve good balance between discrmination capability on the current task and generalization on future task.\n\n2. No experiment on the influence of memory size and batch size on the proposed method. These are important hyperparameters for the proposed method. So there should be experiment inspecting its influnece on the performance.\n\n3. There is also no experiment on the influence of different memory update rule on the proposed method. It is not clear how well the proposed method is robust to different memory update rules.\n\n4. Experiments on more diverse datasets should be done to prove its effectiveness, such as TinyImageNet, and MIT Scenes/Oxford Flowers/UCSD Birds/Stanford Cars.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}