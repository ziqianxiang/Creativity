{
    "Decision": "",
    "Reviews": [
        {
            "title": "The topic of this paper is interesting",
            "review": "Overall, the paper is well-organized and easy to understand. The authors proposed a new momentum decay strategy  for SGDM and Adam methods, experimental results shown the effectiveness of the propsoed methods. I have the following comments/questions.\n\n1. The motivation of using $\\beta_t$ is still not clear to me. Could you please explain more?\n\n2. The convergence analysis is in the convex setting, which does not satisfy neural network training. I do not think the proposed methods have theoretical guarantee in training neural network. It would be better if the convergence rate can be established in the non-convex setting. I am happy to see if the authors can provide the new analysis for non-convex setting.\n\n3. The proposed Demon methods require knowing the value of total iterations T before running it, which is different from SGD and its variants such as SGDM and Adam. This is quite similar to SGDM with cosine decay learning rate. Another question is that why don't you compare with SGDM with cosine decay learning rate in the experiments? I would like to see if the authors can show such results. \n\n4. The authors claimed that \"DEMON momentum SGD is observed to be significantly less sensitive to parameter tuning than momentum SGD with learning rate decay schedule\". Could you please show the evidence in experiments?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Well-written paper but is this just linear decay?",
            "review": "# Summary\n\n### Demon schedule\n\nThis paper proposes a momentum schedule for gradient-based optimization called \"Demon\". The functional form of the momentum schedule is:\n$$\n  \\beta_t = \\beta_0 \\frac{1 - t / T}{(1 - \\beta_0) + \\beta_0 (1 - t / T)}\n$$\n\nThe motivation is intuitive rather than theoretical (\"Demon is motivated by learning rate decay models which reduce the impact of a gradient to current and future updates\"). The authors' notation is a bit imprecise, but I believe that the specific functional form arises from the paper's implicit observation that for any value of $\\beta_t$, the total contribution of that momentum term is:\n$$\n  \\sum_{i = t}^T \\prod_{j = t}^{i - t} \\beta_{t + j}\n$$\nwhich the paper claims converges to $\\beta / (1 - \\beta)$ as $T \\rightarrow \\infty$. Thus, to simulate an effect similar to learning rate decay, the paper solves for:\n$$\n  \\frac{\\beta_t}{1 - \\beta_t} = (1 - t / T) \\frac{\\beta_0}{1 - \\beta_0}\n$$\n\nThe solution is the paper's proposed functional form.\n\n### Empirical analysis\n\nThe paper proposes to apply the above schedule to momentum SGD and Adam. The paper lists a wide range of evaluation scenarios, including CIFAR, Tiny Imagenet, STL10, PTB, and (F)MNIST. In the \"mainline\" experiments, the tuned hyperparameter is learning rate and all other hyperparameters are fixed.\n\nIn the context of the paper's experimental setup, the results show that (1) for a given value of $\\beta$, using Demon allows the user a wider range of admissible learning rates, and (2) the Demon schedule results in consistently lower error upon convergence than a fixed momentum.\n\n# Merits of the paper\n\nThe experimental results are striking and hold over many different environments.\n\nMoreover, the paper is clear that the proposed schedule comes from an intuitive motivation rather than some contrived pseudo-mathy derivation. I can't thank the authors enough for doing so. It is so refreshing to see a paper that plainly states \"we thought of something, we tried it, and it works.\" The paper shows benefits above and beyond linear momentum decay, which justifies the somewhat unconventional functional form that is derived.\n\nThe paper's convergence results are welcome but the authors wisely choose to elaborate in the appendix as the convergence of the algorithms should be immediately evident to a reader working in this field.\n\n# Drawbacks\n\nTwo unresolved questions jump to mind:\n\n- **To what extent is momentum decay interchangeable with learning rate decay?** That is, how much of the Demon decay schedule's benefits actually come from the momentum decay component, versus the indirect effect of reducing update magnitudes? To answer this question, I would like to see a set of experiments which hold momentum fixed at $\\beta = 0.9$ and instead reduce the learning rate to achieve the same decay in update magnitude. I haven't worked out the math to figure out what this would look like, but the authors are welcome to follow up with a proposed LR schedule and I will verify.\n- **Are the authors' PTB results rigorous and is the baseline configuration appropriate?** I will rely on other reviewers' assessment as I am not an NLP practitioner myself. I'm not looking for SOTA here, simply that the paper's configuration is commonly accepted as a baseline.\n\n# Overall Rating\n\nMy original rating was a 4 due to reviewer math error. After correcting for the error and examining the authors' updated results, my initial score is a 6. I may adjust depending on further discussion or new results.\n\n# Comments / points of clarification\n\n- \"Moreover, for an asymptotically large number of iterations, we know that $\\beta$ contributes on up to $t âˆ’ 1$ terms.\" This statement is unclear.\n- While I don't believe this is significant enough to discuss in detail in the Drawbacks section, the argument that the sum of $\\beta$ contributions converges to $\\beta / (1 - \\beta)$ is a bit of a stretch given that $\\beta_t$ is non-constant. It is probably fine in the limit but the authors should discuss and justify this approximation explicitly in the paper.\n- I gather that \"exploration\" refers to CIFAR and STL, while \"mainline\" refers to all other experiments. However, the paper does not actually define the delineation between \"exploration\" and \"mainline\", so it would be great to revise accordingly.\n- Where possible, the empirical analysis should be expanded to do head-to-head comparisons. For example, instead of comparing SGD-Demon and Adam-Demon with AdamW, AMSGrad, QHM, QHAdam, you could do \"AdamW-Demon\" vs. \"AdamW\", and so on.\n\n# Retracted comments\n\nThe comments below were in the original version of the review but are incorrect. I am preserving them here for transparency.\n\nMy suspicion is that the paper's proposed schedule is an overly complex form of the the following: $\\beta_t = \\beta_0 (1 - t / T)$. Here are some example plots: [0.999](https://www.wolframalpha.com/input/?i=graph+%281+-+x%29+%2F+%280.999+%2B+0.001+%281+-+x%29%29+from+0+to+1), [0.9](https://www.wolframalpha.com/input/?i=graph+%281+-+x%29+%2F+%280.9+%2B+0.1+%281+-+x%29%29+from+0+to+1), and [0.8](https://www.wolframalpha.com/input/?i=graph+%281+-+x%29+%2F+%280.8+%2B+0.2+%281+-+x%29%29+from+0+to+1). Unless there is a compelling reason (or the authors can show significantly better results of their schedule over linear decay), then this paper should instead just be an empirical analysis of linearly decayed momentum.\n\nIn this light, I do believe there are some overreach concerns with the paper's present claims. I would like to see one of the following:\n\n- Results across the experimental suite showing that the Demon schedule is better than linear momentum decay.\n- A re-scoping of the paper's stated contributions as follows: \"We perform the first large-scale empirical analysis of linear momentum decay and find ...\"\n\nMy overall rating is a 4 as written, due to the significance concerns. I don't think this paper can survive as a \"here's a novel momentum schedule\" paper given its likely similarity to linear decay. However, I'm of the opinion that a large-scale, well-executed empirical analysis of linear momentum decay is relevant and useful to the ICLR audience. I am willing to increase my score if the paper is re-scoped as such (or if the authors can demonstrate a real difference from linear decay).\n\nMy rating does not reflect my admiration of the authors' work. This is a well-written paper with results that, presented in the appropriate form, deserve an audience.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Decaying momentum helps training of deep networks",
            "review": "Authors propose a decaying schedule of momentum. Using the enchancement and applying to Adam, the authors see that better performance overall, less sensitive to hyperparameters.\n\nReason to score:\n\nThis idea is interesting on its on. It adds a dependency on momentum to total steps T to train on. Emperical evidence show a small improvement over baselines, hence this suggests that this of practical use. \n\nI have listed my concerns below and hopefully authors can address my concern during the rebuttal period.\n\nConcerns/Questions:\n\nComparisons with averaging and related methods can improve the paper.\n Is it an averaging effect that we see in models when reducing the learning rate to zero? This will answer where the improvements (2-5%) come from, the results indicate that this might be going on, as there is a sharp improvement during the end of training as momentum is decayed.\n\na) Could you compare the method with one in \"Reducing the variance in online optimization by transporting past gradients\", S Arnold et al https://papers.nips.cc/paper/8779-reducing-the-variance-in-online-optimization-by-transporting-past-gradients.pdf\n\nb) How well does baselines, experiment improvement when polyak averaging or stochastic weight averaging is enabled.\n\nClaims in Appendix: D LINEAR REGRESSION is problematic\n\na) would authors clarify what theory they refer to?\nb) experiments indicate that SGD, SGD-M, and SGD-Deamon all perform the same? Did the authors mean to do full-batch training there?\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The nonconvex analysis should be presented and baselines need to be improved.",
            "review": "This paper is a lack of theoritical guarantees. The authors do not present any  soild theoretical guarantee for the proposed algorithms and any solid insight. The convergence rersults directly use  previous convex analysis. The nonconvex results are missing.\nThe experiments  are all toy tasks and the baseline was deliberately lowered. For example, Resnet20 used SGD with momentum 300 epoches to achieve 93% accuracy.    For the PTB experiment, Adam train LSTM's Perplexity can easily reach 60, and their report is more than 88. The results in this paper do not match existing SOTAS.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}