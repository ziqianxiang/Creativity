{
    "Decision": "",
    "Reviews": [
        {
            "title": "Evaluation of GAN's training stability, image quality and mode diversity. But the proposed metrics are not convincing. ",
            "review": "This paper discuss the evaluation of GAN models and propose a new evaluation framework named SQD. \n\nPros:\n1. The framework considers both training stability, image quality, and mode diversity simultaneously.\n2. The metric is rather stable while using a small number of generated images.\n3. The author validates that the score predicted by MTCNN (Zhang et al, 2016) can be used to measure the quality of face images. \n\nCon:\n1. The stability metric measures the difference of parameters between two optimization updates, it is very weird to use such a metric. As it is not model-agnostic, when you compare models with different architectures it is not a valid metric. It also heavily depends on the optimizers, learning rate schedules. E.g., whenever you apply learning rate decay, you can have a very stable GAN. GP, of course, is very stable as each optimization step is very small. \n2. The visual quality metric is only designed for face image generation. The authors argue the proposed metric is better than IS/FID is because the score prediction network is trained using face images only. 1) The author should clearly state that the proposed method is only designed for face image generation in the introduction/abstract/title. 2) How would the proposed metric perform when comparing with IS/FID using a face recognition network instead of using the inception net.  \n3. The mode diversity metric is only applicable to face image generation. Basically, if we jointly consider (7) and (8), it is a face classifier. The proposed metric (9) is evaluating the entropy of the \"hard\" predicted labels over the generated sample. Suppose the generated samples are just random noise and the well-trained classifier should be able to output nearly uniform distribution over different classes. Does this case maximize the entropy (KL divergence to a uniform distribution)? On the other hand, IS measures the KL divergence between \"soft\" predicted probability (instead of \"hard label\") and the marginal distribution (instead of \"uniform distribution\"). I think IS makes more sense as it considers the confidence information. \n4. Lack of comparison with current methods, e.g., IS, FID. Maybe the authors need to conduct a human evaluation, in order to show the superiority of the proposed method.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Important problem but lacks novelty and a rigorous evaluation",
            "review": "The authors propose the SQD framework to evaluate GAN training in real time. Tracking is done via three metrics, one for each of training stability (S), visual quality (Q) and mode diversity (D). Stability is tracked via calculating angular differences, Q uses a face detection network and D associates the embeddings of a sample with its closest counterpart in the training data. These metrics require a relatively small amount of samples and can be computed fast so can be tracked in real time during training. The evaluation is carried out on 4 variants of GANs with and without gradient penalty.\n\nReview:\nOverall the paper is well written and is easy to understand.\nDesigning a good metric is a very important problem in the field and existing metrics fall short of providing a satisfactory answer. As Theis et al.[1] have suggested, I think separating metrics into a suite is a wise choice on the authors part.\nI like the idea of tracking training progress via angular differences rather than losses which, for GANs, are often not interpretable.\nUltimately, I do not think the paper represents a sufficient advance to warrant publication. I do not think the availability of these metrics gives the practitioner any particularly valuable insight which they might not have gotten by merely examining samples periodically. \n\nSome other observations:\n1. The training stability effect of GP is known. While more empirical validation is always welcome, I do not think it is required here. \n2. The authors mention the existence of a turning point in training beyond which training outcomes start to deteriorate. However, what characterizes a turning point is not mentioned. Is it simply a change in the loss? How big should this be? For example: In fig 3a, there are two peaks, but the second one is chosen as the turning point for some reason. Some explanation here would help clarify this choice (maybe in the appendix). \n3. Based on the samples in the appendix, visual quality seems to get better (from my subjective perspective) after epoch 40 (Fig 10) which is supposedly the turning point of training. The plots in Fig 4 (visual quality metric) seem to suggest image quality should get worse. I’m not sure how to reconcile these two observations. Are these samples only taken from the peaks shown in the visual quality plot? \n4. The main criticism I have (which also plagues many other GAN evaluation metrics) is that a model which merely memorizes the training data would achieve very good performance on these metrics, at least for visual quality and mode diversity. Having two separate metrics to track instead of one allows us to know whether we have low image quality or low mode diversity (or both) but, in my opinion, this is something which is of marginal importance when memorization fools your metric. Additionally, diversity of images for a given mode, which is something other metrics lack and very important, is not mentioned.  \n\nTypos:\n1. Mode collapse instead of model collapse in abstract\n2. In the conclusion “on the CelebA dataset, we would like to extent our methods” should be “on the CelebA dataset, we would like to extend our methods”\n\n[1] Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The metrics are interesting, but need more studies",
            "review": "– Summary –\n\nThis paper proposes three new metrics to evaluate the training stability, visual quality, and mode collapse of GAN training. The first metric is the measure of the angular difference of weights of generator and discriminator for accessing the training stability. The second metric is for visual quality that computes the average of output scores of an existing face detection framework over image inputs. The last one is to measure the mode diversity that measures the matching for each identity by using another face verification framework. The experiments are on CelebA dataset with standard GAN architectures. The study provides some quantitative understanding of the improvements of gradient penalty variants over some baseline GAN models via these new metrics.\n\nOverall, the paper is interesting and well-written, however, the paper looks a bit rushed in the submission. My main concern is the missing comparison to existing scores, e.g., IS and FID, to justify the advantages as claimed. Further, the quality and diversity scores are likely limited to the face domain dataset only, and the experiments are only one face dataset. There are minor mistakes of gradient penalty equations, yet not important. It would be more valuable if the paper also evaluates other batch sizes (as shown in SAGAN and BigGAN paper the batch size is important and highly affected to the overall performance), network architecture, … \n\n– Strength – \n\nS1 – The first metric for the training stability and its turning points are interesting to me. It could be useful to learn why the turning points happen in the future of this work. I notice that if the model is well-converged, the angular difference between G and D always keeps at a distance away, and that of G is mostly higher than of D. Maintaining this difference in the training may be a way to improve the stability of GAN training.\n\nS2 – The paper conducted the quantitative comparison of variants of gradient penalty based regularizations. I learned from this paper: the hinge loss + gradient looks an effective combination. \n\n– Weakness – \n\nW0 – There is no comparison to existing metrics, e.g., IS and FID to clearly show the advantages as the authors claimed in the paper, e.g., fewer samples and low variance. I think people now are using the combination of IS and FID in their experiments to measure quality and diversity. How are these two new metrics better than the existing combination? There is a bit disconnected between the concepts of the three proposed metrics, and I think it's still fair to compare the proposed metrics (visual quality and mode diversity) with the combination of IS and FID.\n\nW1 – The visual quality and mode diversity metrics use the face detection/verification frameworks which are quite specific to face datasets that have ground-truth labels. Can they apply to other domains rather than faces? If so, does it cause any bias?\n\nW3 – What is $|X|^-1$ in Eq. 6. Is it the inversion of matrix determination or number the division of the number of samples?\n\nW4 – Mistakes in Eqs. 3 and 10 and Table 3, should be gradient of $\\nabla_{\\hat{x}}$ instead of $\\nabla_{\\eta}$.\n\nW5 – It looks like the paper is a bit rushed in this submission, there are sufficient missing details of implementations, e.g., hyper-parameters, batch size, detail of architecture, … are not provided. These factors may also have effects on the training of GAN. It would be interesting to have studies on these in future work.\n\nW6 – The paper's experiments are limited to one low-resolution dataset with standard GAN architectures. It's important to evaluate various GAN architectures and datasets for GAN assessment papers. It would be interesting to conduct the experiments also on state-of-the-art GAN models, e.g., SN-GAN, BigGAN, ProGAN, and StyleGAN, and with high-resolution datasets as well.\n\nW7 - Some metrics are interesting and likely to be valuable in the future, but can be more polished on experiments and comparison. It would be great if the current measures can be developed to extend for other datasets, e.g., CIFAR, ImageNet.\n\nW8 – The paper has derived some mathematics of turning points, but what is its meaning and how is it useful to derive this equation?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Major concerns about the presentation and the approach",
            "review": "The paper proposes new methods to measure the training stability, visual quality, and mode diversity of GANs. It claims that the proposed methods require only a moderate number of samples so as to enable an efficient real-time monitoring. It conducts experiments with different variants of GANs on CelebA dataset, and show the turning-points issue with many GANs variants, and verify that the gradient penalty and some other Lipschitz regularization techniques can effectively alleviate this issue.\n\nI have some major concerns with this paper, regarding the presentation and the significance/usability/novelty of the proposed approaches.\n\n* The paper is misleading overclaims in several places.\n\n    * The proposed methods for evaluating visual quality and mode diversity only work for CelebA (or faces) datasets. I think this is a big limitation and should be mentioned early on in the paper. However, it is not clear to readers until reading the method section. (1) The abstract didn't mention it at all. (2) And then in the introduction, it says \"We propose a new framework... We tested the proposed SQD framework on popular variants of GAN that model the human faces in the CelebA dataset.\", which sounds like the proposed method is general for any datasets.\n\n    * In section 3, it says \"Effective evaluation metrics for mode collapse had been missing in the literature since the birth of GAN\". This is wrong, as there are already many methods for evaluating mode collapse (e.g. [1]). \n\n    * The paper lists the findings about the turning-point issues and effectiveness of gradient penalty/other Lipschitz regularizations as contributions (occupying 3 points out of the 4 points in the contribution list in the introduction). However, these are well-known facts. It is good to confirm these facts with your proposed metrics as a sanity check, but it is not your contribution.\n\n* The proposed method is limited.\n\n    * The proposed visual quality and mode diversity metrics are only applicable for CelebA (or faces) datasets. This limits the usability of the approaches significantly, as GANs are currently used for many other types of images, and even data beyond images.\n\n    * The proposed mode diversity metric only captures the diversity over identity. What if the generated images cover all the identities in the training dataset with the correct identity distribution, but all the images have the same background color? This is also an instance of mode collapse, but will get a perfect score under your metric.\n\n    * The mode diversity metric only considers the identities in the training set. What if the GAN generalizes well and all the generated identities are new?\n\n    * In fact, the mode diversity metric is very similar to the ones in prior work. The prior methods usually assume that the ground-truth modes in the data are known, and use a pre-trained classififier that maps a generated image to the mode. The metric proposed in this paper just assumes that the *identity* is the mode, and uses a special form of the classifier (nearest neighbor on the mean embedding of ArcFace network). Mathematically, the metric (equation 9) is exactly the same as reverse KL metric in [1] up to a constant shift, when the ground-truth distribution over the modes is a uniform distribution.\n\n    * The major claim regarding the benefit of the proposed approaches is that they only need a few samples. I think you should support your claims with more detailed experimental and/or theoretical results. For visual quality metric, I only see one sentence in Section 3 about the standard deviation. However, it is unclear at all whether this standard deviation is small enough without knowing its associated visual quality score. For mode diversity score, you only give the order of the sample needed for estimating the entropy from prior work, without providing the important details (e.g. how large is the constant? What is the variance of the estimation with this number of samples?). Experimental results are also needed.\n\n\n[1] Srivastava, Akash, et al. \"Veegan: Reducing mode collapse in gans using implicit variational learning.\" Advances in Neural Information Processing Systems. 2017.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}