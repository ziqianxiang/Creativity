{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper tackles an interesting problem (that of federated continual deep learning) and proposes an effective approach for it with good results.  This is a good contribution. However, there are presentation issues in several aspects of the paper that require improvement before publication. The authors' claims of the novelty of the federated continual learning problem is overstated (there was an AAMAS'18 paper they cited which IS applicable to the federated setting, although their method is certainly a substantial improvement, more flexible, and supports deep models), and there are aspects of the analysis and experiments that could be improved (the authors are somewhat dismissive of one reviewer's concerns about the insensitivity to the regularization parameters. While I agree with the authors that this aspect of the review is focusing on this one detail in lieu of the bigger picture, the author's insensitivity study in Figure 6 and subsequent analysis are lacking and could use improvement).  The authors' revisions did help clarify/address a number of issues raised in the initial reviews,  but it was felt that more improvement was needed.\nIn particular, there are still mistakes with characterizing this work with respect to existing work, especially in overstating the novelty of the federated continual learning problem.  I do believe this paper is the first to call it \"federated continual learning\" (a name I like), but the paper should be less dismissive of existing works on \"multi-agent lifelong learning\" that could also apply to this setting, albeit with shallow models.  Consequently, while this reduces the novelty of the federated continual learning problem, the authors still have a substantive contribution -- just one that needs improvement in presentation before publication."
    },
    "Reviews": [
        {
            "title": "This paper proposes a method to solve continual federated learning. It allows the models to leverage task specific information by other peers while preventing the negative interactions. The paper is tackling a cool problem but is borderline due to contributions and experiments. ",
            "review": "Combination of federated learning and continual learning is a timely problem. Given the trending popularity of either of them it's the time to tackle this problem. The problem that is tackled is nice but the proposed formulation looks incremental.\n\nThe paper is generally well written but has a few typos along the paper like:\nPage 2: \"... need to selective utilize ..\"\nPage 2: ... once when ...\"\nPage 7: \"dtaset\"\n\nIn section 3.1 the relation between tasks of the clients is not clear. Are the tasks at step i of all tasks related to each other? \n\nIn equation 1: In the third term --> Isn't there a case for transfer between the task of the same client to future tasks?\n\nSection 3.3 for training: Imagine we are at task t. Then, the minimization problem in equation 3 involves solving for A^i. Why do one need to find A^i for a previous task and find a new parameter for that? Isn't that task already gone?Although, does this mean that the parameters to be estimated at each step is growing with the number of tasks?\n\nThe NonIID data set is a cool combination of different smaller datasets.\n\n The experiments look overall good. However, the part that shows alleveriating catastrophic forgetting seems rather less elaborated. Why only 6th and 8th tasks? Why not demonstrate forgetting of task 1 over the time. Why not all tasks? And, Fig. 6 only shows up to 5 tasks. How many consecutive tasks does the proposed method handle?\n\nThis works look like an increment on the top of (Yoon et al, 2020). Like making their approach adapted to a federated learning case. Thus, the contribution of the works is rather limited. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper investigates a new problem – federated continual learning by Federated Weighted Inter-client Transfer. The key idea is to decompose the network weights into global federated parameters and sparse task-specific parameters such that each client can selectively receive knowledge from other clients by taking a weighted combination of their task-specific parameters. The experiment results in two contrived datasets demonstrate the effectiveness of the proposed method.\n\nStrength:\n+ This paper is well presented and organized.\n+ The proposed federated continual learning framework is innovative and technically sound.\n+  The experiment results are solid.\n\nWeakness:\n- The optimization procedure for Eq. (2) is not provided.\n- Some details are missing (as shown below).\n\nThe following are some questions that I concern.\n\n1.\tThe authors address the importance of federated continual learning from the aspect of continual learning, but is there any difference between federated continual learning and federated learning considering that most existing federated learning methods (fedavg and fedprox) are agnostic of client id?\n2.\tHow to train alpha in Eq. (1)? Is this a learnable parameter with sigmoid activation? If yes, how to set the parameters for testing on different tasks? Moreover, the whole testing process is confusing to me.\n3.\tThe detailed optimization procedure for Eq. (1) is not provided.\n4.\tFor the training objective Eq. 2, intuitively, authors propose decomposable parameters and want Base parameters B to be sparse with the help of task adaptive parameters A.  However, it is interesting to see that there is no constraint to encourage B and A to focus on different aspects. I thus doubt that the communication efficiency brought by the sparse parameters mainly benefits from the sparsity constraints, i.e., 2nd term in Eq (2), but not the decomposable parameters. The authors are encouraged to show that the base parameters (m*B) are more sparse than the model trained with existing federated methods with similar sparsity constraints.  Otherwise, the communication efficiency of the proposed method would not stand. \n5.\t For the third term in Eq. (2), whether it is necessary to impose the constraint on  A_c^i (i<|t|), as in Eq. (1)?  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "The authors propose a federated continual learning setting where each node has a non-iid stream and a different dataset. The authors address this by extending the parameter decomposition method of Yoon et al\n\nStrengths:\n\n- The authors introduced a relevant new task that introduces concepts from continual learning to federated learning. The task setting seems practical overall as different nodes will often be following a different distribution\n-The algorithm proposed is interesting. I would like further discussion on the differences to Yoon et al\n-The results are good and evaluate a lot of  relevant factors\n\nWeakness:\n\n- Although the setting proposed is interesting, certain aspects of it seem artificial: although it seems very relevant that each node is a different dataset, strong non-iid behavior per node seems not realistic for many settings \n-What happens when some nodes start learning much earlier than others\n- A discussion of challenges in these settings and other potential methods\n- Results are shown for very simple LeNet architecture only\n- The algorithm proposed is interesting, but more motivation and other possible alternatives in this setting would improve the paper\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel problem setting but writing is unclear and concerns in empirical results. ",
            "review": "In this paper, the authors present a federated continual learning framework. By decomposing the local client parameter, the method could alleviate the effect of negative transfer and improve efficiency. Empirical results partly show the effectiveness of the proposed algorithm. \n\n**Strength**\nTo my best knowledge, the problem setting is quite novel. Decomposing the parameter parts are also interesting. \n\n**Weakness**\nWriting is not good and many details are not clear. Some parts of the methodology lack explanation and the experiment results cannot support all claims of the authors. Below is my detailed opinion.\n\nEquation 1 describes the decomposition.  However, some notations are not clearly defined. For example, what is L, I, O£¿ I have to guess that L is the depth of the network, I represent the input dimension, O represents the output dimension after I reading it many times.  I am not sure whether the above guess is right. Also, A_c^{(t)} is sparse task-adaptive parameters, this is not clear. I am not sure the sparse means in task-level or for every matrix A_c^{t} is sparse. I have to guess from equation 2, the author tries to concatenate all A, and perform sparse constrain.  I am not sure whether the above guess is right since the author did not explain it well.\n\nIn this paper, it seems that the author assumes that when a new task comes is knew ahead. In practice, it is hard to get such prior knowledge. Also, in the experiment part, it seems that different clients receive different tasks simultaneously, which is also quite not realistic. In practice, the new tasks could come asynchronously.\n\nOn the 2nd line of page 6, the author claims the efficiency of the algorithm is boosted since it requires |C| * (R * |B| + A). Isn't B is the same shape as \\theta? I understand that the author assumes B is sparse, but we do not know how sparse it could be or can we recover the ground-truth sparse matrix or not. I can see that in the worst-case, without assumptions, this cannot improve efficiency. Empirical results only support that using the l1 constraint can output sparse results, while when this will work and correct is not clear in the methodology part. \n\nAlso, for this sparsity, lambda 1 is fixed across different tasks. This indicates different tasks have the same penalty. I do not see why this works since some tasks could be very similar to other tasks while some tasks are very distinct.  The same reason can be applied to lambda2 since the relationship between the current task and the previous task could be different. In figure 6, it seems that hyperparameters are very sensitive.\nThe plot of learned attention (Fig 5b).  I am not sure whether this is top-5 attention since there is no description (maybe I missed it). I assume that this is top-5 attention results. However, this cannot support the author that the proposed method can handle the negative transfer. First, empirically, why Traffic Sign has more weight than SVHN in the MNIST task? Moreover, This figure shows that empirically, using attention can focus on more important tasks/features and this should not be in the main contribution, and this is a well-known phenomenon. Has the ability to include the right task does not indicate excluding negative tasks.\n\nIn the middle plot of figure 6, I wish to see the whole task performance rather than selected tasks (this should be included in the appendix at least).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}