{
    "Decision": "",
    "Reviews": [
        {
            "title": "A more disciplined or fundamental pleasure reward model would be necessary.",
            "review": "This paper presents a pleasure based reward model which encourages explorations during the reinforcement learning procedures. The main contributions and principles are using probability distribution of stimulation. Based on the stimulation density, authors define adaptation level, reward curve and aversion curve. In principle, adaptation level is a truncated distribution with some small (in terms of simulation level) values are omitted. Thus, the distribution with different adaptation level shifted toward large values. The reward curve encourage exploration of higher simulation values which are not explored yet. The aversion curve discourage exploration of (too much) higher simulation values. Thus, exploration will be selected gradually toward new (less explored) high simulation values.\n\nStrong points\n- Psychologically well justified theory of pleasure and aversion in RL\n- A simple but reasonable solutions for exploration.\n\nWeak points\n- The proposed model is to simple (e.g., univariate, an approximately single mode and a single direction of exploration) to be used in several RL problems/models.\n- There is no strong connection between the psychological pleasure model and proposed computational models.\n- Empirical evaluations are not extensive in that it compares with one algorithm/method.\n\nI am leaning to reject the paper since (1) the contribution of this paper is not significant enough and (2) empirical evaluations are not rigorous.\n\nHere are my comments and questions to authors.\nThe presented method may intrinsically assume that (1) simulation is linear ordered (univariate); (2) simulations are approximately a single mode (of distribution); (3) exploration is toward a single (large stimulation) direction. \n(1) Thus, I am curious about whether the method can be applicable to multivariate simulations. \n(2) The proposed algorithm would work,  if simulations (of exploration) are generated multimodal as mixture of Gaussians. \n(3) The proposed algorithm can grow both directions. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Explore with pleasure - Official Blind Review #3",
            "review": "This paper proposes exploration with pleasure (EP) framework that is built upon two theories (the pacer principle and the Wundt curve) of optimal stimulation level from psychology. Actually, EP is able to identify the region of stimulations that trigger pleasure to the agent. The effectiveness of EP has been examined on two machine learning fields: reinforcement learning  (generate intrinsic rewards) and bayesian optimisation (specify the exploration parameters in two acquisition functions).\n\nPaper’s key strengths:\n- The usage of psychology theories to identify the region of stimulations that trigger pleasure to the learning agent is quite interesting and novel. \n- The EP framework is general and can be applied to different machine learning settings. \n- Empirical results in curiosity-driven RL and Bayesian Optimisation have been presented.\n\nPaper’s key weaknesses:\n- The performance of EP is not good in its application in curiosity-driven reinforcement learning.\n- One type of stimulation property is only considered. \n- The quantification of the stimulation property and its impact on the performance of EP framework is not clear. \n\nAs aforementioned, I found the idea of using psychology theories to identify regions that trigger pleasure to the learning agent quite novel. Despite the possible merits of EP framework, the performance of EP is not good in its application in curiosity-driven reinforcement learning. For instance, the performance of PE+EP is only -1.1 in the case of the Pong Atari environment. It is much lower compared to other RL algorithms that are able to achieve a score around 20. Also, authors should define PE in the 4.3 Experiments. Does it denote the PPO rl algorithm without EP? Furthermore, it is not totally clear why the state encodings are not trainable (A.3 section). Additionally, the performance of PE+EP algorithm should be also evaluated on other more challenging sparse-reward environments.\n\n Another point that should be discussed further is the usage of more than one stimulation properties. It is not clear how someone can consider more than one stimulations simultaneously. Finally, the selection of a proper quantification of the stimulation property is another open question. How does it affect the performance of EP framework? What will be the difference if we use the forward dynamic network to predict the next state instead of predicting the encoded next state used for the prediction-error-based surprise quantification method?\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Not enough theoretical or empirical payoff from the psych-inspired objective",
            "review": "The paper proposes a psychology-inspired intrinsic reward function to drive exploration in sequential decision making problems. The essential idea seems to be to empirically find a \"goldilocks region\" w.r.t. the distribution of some utility value and use that as an objective. The paper demonstrates the benefit of the approach in experiments including pure exploration in RL, and conventional BO, showing improvements relative to a narrow set of competitor benchmarks. \n\nI think it's always interesting to see bio-inspired ML, especially psychology which is relatively under-exploited relative to neuroscience or biology. I also think that the paper does well in seeking to demonstrate its advance in two different sequential decision making domains (RL and BO). What weakens the paper is that it doesn't quite do enough to exploit the importation of an old idea into a new domain. This can be done either theoretically or empirically, and neither is done strongly enough in the paper. I discuss these two key weaknesses next, and conclude with some minor points / nitpicks: \n\n# Theoretical weaknesses #\nFrom a theoretical perspective, there is little insight given about what the Wundt curve buys here, and what's really doing the work. It it the basic intuition above about a \"goldilocks region\" correct? Is it something deeper? Does it matter if we use the specific two-parameter form given by psychological theory, or do we just need to maximize while attenuating high values? \n\nMost importantly, it's not obvious to me that this method is doing the same thing in the RL and the BO experiments:\n- If the utility value is a pure-exploration one (as in the RL example), the proposed method will downweight highly surprising regions, presumably yielding more conservative exploratory behavior (and as such should possibly be compared to safe exploration algorithms). If the utility value is exploitation-focused (as in the BO example), the proposal will downweight highly rewarding regions, increasing exploration of other regions. So it seems like what it is doing to exploration is different in these settings, and it will in fact reduce exploration in the RL case (since high-uncertainty and therefore likely high-PE states will be visited less often). \n- In the BO case there are two sources of variance that lead to the distribution over rewards: the points at which the surrogate is evaluated, and the uncertainty in the surrogate. In the RL case the distribution is driven by the rewards presented by the environment. I'm assuming in the BO case the distribution over actual function evaluations wasn't used because \"episodes\" were too short, but still it seems like the density cutoffs are computed based on fairly different quantities in the two examples. \n\nIs this correct? If so, can we still interpret it in the same way, as helping drive exploration? \n\nPerhaps there are other insights that can be derived here that I am missing -- my point is just that I'm not sure what's \"doing the work\" in the contribution, and without additional insight it is hard to view the contribution as more than a coarse psychology-inspired heuristic. \n\n# Empirical weaknesses #\n\nIn the absence of stronger theoretical results or insight/intuition, we should expect empirical benefits relative to strong competitors. This is also missing. \n\n**RL setting:**\n- A comparison is given only to a prediction-error-based exploration algorithm, and only on four atari environments. It is clear that the proposed method provides benefit over the baseline (this is good). But the baseline paper, while recent, was intended as a broad-coverage study of exploration, so the narrow set of environments and the choice of a single feature representation from that paper feels like cherry-picking. If I were to cherry-pick baselines, I'd pick environments considered to be hard for exploration (e.g. Montezuma's revenge or Pitfall). As it is, it's hard to tell whether this is a generally useful heuristic. Will it help when using other directed exploration strategies? Does it actually yield state of the art performance, or does it just help a specific method perform better? These questions aren't convincingly answered. \n- There's no empirical exploration of what's driving the differences in performance: what kinds of states does the PE+EP agent avoid that the PE agent visits? \n\n**BO setting:**\n- A comparison is given on more test functions (this is good), though the baselines could potentially include additional options such as knowledge gradient, thompson sampling, and epsilon-greedy modifications to EI and PI. KG in particular seems useful to add here as it is more recent and often performs better than EI/PI, and TS, like the proposed method, exploits the ability to sample from the GP. \n- The comparison isn't fair since the competitors aren't given the same flexibility in hyperparameter search given to the proposed method (i.e. $\\tau$ for EI/PI and $\\kappa$ for LCB are fixed whereas $\\alpha_R$ and $\\alpha_A$ are tuned). \n- For a fair comparison, the paper should also report the function value where the surrogate is maximized, rather than reporting the best achieved value with exploration on (since different exploration heuristics will penalize the intermediate function values differently). \n- I think it's fairly conventional to initialize BO with some non-GP trials (e.g. from a Sobol/Halton sequence, latin squares, etc) to avoid early degenerate behavior like the lengthscales going very large/small. Was this done, and if so how? \n- I'm puzzled that the curves in the GP performance figures don't start in the same place: shouldn't the best function value show no difference between methods after 0 function evaluations, assuming enough repetitions of the experiment were run?\n- $h_t(\\tilde(y))$ doesn't have a unique maximizer if the two normal CDFs are far enough from each other (the max becomes a plane) -- how is this handled?\n\n# Minor comments #\n\nAL and EP are both somewhat overloaded acronyms (AL is often used to mean active learning and EP is often used to mean expectation propagation). \n\nFigures could use larger text in axis labels and legends. \n\nFigure 1 legend would be better placed adjacent to the figure, so that the full curves are visible. \n\nThe level of mathematical exposition is inconsistent and veers towards unnecessary \"mathiness\", e.g.: \n- Why say \"algebraic sum\" vs just saying that the aversive and rewarding functions are subtracted from each other? \n- $u$ is defined as a random variable multiple times in the text, and often referred to as \"the stimulation property $u$\" when just referring to it as $u$ after first definition is sufficient. \n- Is it important that $U$ is a double-ended buffer (and what does this mean? Is it the same as a deque?)? If not, why not just say \"buffer\"? \n- Why is it necessary to describe BO (algo 3) and define EI and PI (exprs 8-9) but not provide the same level of detail for RL generally or PPO specifically? It seems like the former could be omitted. \n- The proposed algorithm is defined 3 different times, once without mathematical detail (algo 1) and twice more in application to specific settings (algos 2 and 4). \n- Expressions 2 and 5 define $\\sigma_R^2$ and $\\sigma_A^2$ identically: is this correct, and if so why not just define as $\\sigma^2$? Expressions 3 and 6 are similarly defined identically. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A paper combining psychological knowledge into the decision making process, but more clarifications are needed. ",
            "review": "\nThis paper propose to use the Wundt curve, a reward function defined from a psychological perspective, as the new reward function to help explorations, and come up with a new algorithm called Exploration with Pleasure (EP). EP is applied on a subset of Atari games and Bayesian Optimization setting, the empirical results are encouraging, where EP based algorithms have a significant gain compared with the rest. While I appreciate that the paper tries to improve RL or BO methods from a psychological point of view, I have some major questions need clarifying. \n\nMajor questions:\n- Methodology part:\nThere are three points I don't fully understand. \n\nFirst, this paper discussed extensively about Reward and Aversion curves, and their psychological backgrounds. However, I cannot relate these on either curiosity-driven RL tasks or Bayesian Optimization tasks very well. More specifically, under the curiosity-driven RL setting, the stimulation is the same as curiosity, then why wouldn't we want to explore some states where we are more 'curious'? For Bayesian Optimization, it is also not very clear to me why we should avert the posterior points with higher values. \n\nSecond, I'm not very clear about the definitions of mu_A and mu_R in Def 3.3 and Def 3.4 (I cannot get free access to the paper cited here Berlyne 1966;1971, so I'm not sure if these definitions are from the literature.), can we just use percentiles to get mu_A and mu_R and what will be the pros and cons than the current definitions? Besides, from def on eq(2) and eq(5), it looks like sigma_R and sigma_A are the same, is that true? Lastly, is the Reward curve and Aversion curve well defined? What if u is a uniform random variable and eq(1)-eq(3) are not feasible (likewise for eq(4)-eq(6))?\n\nThird, I think the idea of applying recurrent state space model and build reward based on the state representation and latent variables have been successfully implemented in PlaNet and Dreamer[1,2]; More specifically, if we look at [1] eq(4), is EP a special case incorporated under this framework if we denote u as the hidden variable h and manually design the deterministic path as the calculation for mu_R and reward function as hedonic value? If so, I think it would be more appropriate to compare EP with its RNN variant because both of them take advantage of the representative knowledge from previous steps. \n\n- Experiment part:\nI have two concerns about the RL experiments. \n\nFirst, Atari games are known to be unstable, which means an algorithm's performances can vary quite significantly over the suite of 57 Atari games, and only 4 of them makes the conclusion not very strong. \n\nSecond, is it possible to use one set of parameter for EP across all the tasks? It's a little bit unfair when the tuning budgets are not the same for two algorithms. \n\n\nMinor questions:\nIn the paragraph above Algorithm 1, is the u in h(u) a vector or should it be h(u_i)? The notation of text and math mode of u is not clear to me, does one of them represent vector?\nIn Algorithm4, 2nd line below Input, should that be \\phi_{\\tilde{y}}.. instead of \\phi_{\\hat{y}}?\nWhere does the positivity of u used in Def 3.1?\n\n\n[1] Hafner, Danijar, et al. \"Learning latent dynamics for planning from pixels.\" International Conference on Machine Learning. PMLR, 2019.\n[2] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}