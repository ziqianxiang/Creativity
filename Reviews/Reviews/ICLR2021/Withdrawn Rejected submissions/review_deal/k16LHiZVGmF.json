{
    "Decision": "",
    "Reviews": [
        {
            "title": "An attempt to use deep learning into solving PDEs",
            "review": "This paper is attempting to model a Riemannian Manifold for which we don't have enough data points but we have prior knowledge in the form of a conservation low (or anything that can be expressed in a specific PDE format). It is not very clear to me why. the authors chose the hyperbolic dynamical system and what is the connection to real data modeling problems. They also introduce the function F as the Flux, which is also an abstract quantity that is not well-grounded to the problem.  On page 3 when the Roe solver is introduced and we learn that the variable u is a computational cell. I am not sure what this means, it might have a meaning in the world of PDEs but in the area of Representational Learning (which is the area of ICLR), it is not a known entity. From what I can infer in equations 3 and 4 in order to design a Roe Solver for equation (1) or (2) we need to design carefully the matrix \\tilde(A).  Instead of learning this matrix directly with a neural network, we learn with two neural networks its diagonal form. After this point, the paper floats into confusion and it is very difficult to understand what is going on. From figure 1 it looks like we are having a fully recursive neural network. It looks like a self-supervised architecture but I struggle to understand what is the input and what is the output. How do we generate training data? Is what we see in figure 1 a nonlinear dynamical system? I don't understand how we run backpropagation on this network.\n\nI am also very confused about the experiments. I see very little value as they refer to synthetic data, If I understood correctly the system is trained with some generated data from a system that behaves smoothly in the beginning but it is about to blow up in the future. The knowledge of the blow up is somehow encoded in the F function which is prior. Eventually, we predict this discontinuity. I am sure I am missing something here, as we predict something that we already encoded in the system.\n\nIn general, I think this paper is not very well written. It uses PDE nomenclature and is grounded on concepts that are very common in the area of PDE research, but it fails to make connections to the core of ICLR which is representation learning theory. It needs to better connect to the concepts of learning. I suggest the authors map it in one of the following categories, unsupervised, self-supervised, supervised, one-shot, zero-shot, few-shot learning. Then make it clear how the training and the test data are generated. Show a loss function. Show training and test statistics. Explain more clearly the role of the F function and what it actually represents in the physical world. How does it relate to what we know as prior knowledge?\n\nIn conclusion, I think the work has merit and the ideas are worth publishing, but the paper has to be rewritten, explained better, and be framed in the principles and concepts of representational learning so that it can be understood by this community.",
            "rating": "3: Clear rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Lots of missing details.",
            "review": "Review Summary: I think the paper targeted an interesting problem, and the proposed approach makes some sense. However I feel that overall the paper is poorly explained and misses various key details that makes it difficult to understand and/or reproduce. My assessment of this paper would be a 6, but minus 1 for the quality of the presentation as it misses key details.\n\nPaper Summary: The paper presents a data driven approach to fit a key model parameter \\tilde{A} (decomposed into L and Lambda) in hyperbolic dynamic systems as a function of u at each central and nearby grid locations. This can be used to produce the update equation for computing the values at the next time step. The authors showed stronger performance that the simple Roe Solver baseline.\n\nI find it a little difficult to grasp the main conceptual contribution for this paper and assess it fairly, partially because there are some crucial details that are missing which will render this work difficult to reproduce. My questions for the authors are the following.\n1. How does the baseline RoeSolver acquire the parameter \\tilde{A}? More background introduction is needed.\n2. How are the models trained? This is a key detail that the authors are missing out. The authors mentioned that they used the Mean Squared Error. However to train this, do they always take the ground truth for one time step, predict \\tilde{A}, supervise using the GT u at the next step, or unroll for multiple steps and supervise on the last step?\n3. Above and below Eqn. 6, the authors mentioned that \"Using neural networks to directly approximate L and Lambda is ineffective\", and the pseudoinverse would enable a hidden dimension Nh. It is not clear to me why the former statement is true, and after reading through the paper I still don't understand what this hidden dimension Nh is.\n4. It's unclear to me why this would generalize to unseen shocks by training only on the smooth portion. My understanding is that the network takes in u and predicts A. During shocks, the u distribution is *very* different than that of the smooth training part. Neural networks are generally poor at generalizing to different data distributions from the training distribution.\n\nNits\n* There's a line that reads: \"Roe Template  Substituting (18) and (6) into (15) along with the third Roe condition yields\". Eqn (18) and (15) are both outside of the main text.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Useful method, but imprecise framing, motivation, experiments",
            "review": "=quality=\nexposition/framing low quality (see below)\nmethod is interesting and novel\ndetails of experiments require clarification\n\n= clarity = \nimprecise set up for problem, related work, motivation.\nmissing key details for baseline method.\n\n= originality = \nPart of an important research thread over the last through years on learning PDE solvers end-to-end. It's thematically similar to prior work, but for a different type of PDE solver, with some problem-specific details to get things to work well.\n\n= significance= \nNot particularly exciting for the ICLR commnity, but may be well received for the numerical PDE community if discussion of related work, motivation, and experiments was improved.\n\n\n=motivation/framing=\n\nThe framing of this paper would need to be substantially changed before publication at ICLR. The technical contribution is sound, and well-placed with respect to recent related work. However, rather than discussing the precise details of the problem definition and the contribution, there is a lot of grandiose terminology:  \"predicting the invisible future\", \"unobservable pattern prediction\", and \"our learning paradigm facilitated with these prior-embedded modules can characterize dynamic evolution, [where] no accurate PDE models connecting the current and future.\"\n\n\nWhy not just state that you introduce a new data-driven method for improving the accuracy of PDE solvers for hyperbolic systems? You also need to formally introduce the setup for data-driven learning of such methods. What does it mean to have a train vs. test set, etc?\n\nI also found it confusing that you argue that you building prior beliefs into the solution, when in fact you are taking existing solvers that use hand-engineered parameters and replacing them with parameters that are predicted on the fly using deep learning. Isn't this using *fewer* priors than the baseline? I thought that was the whole point: why introduce prior knowledge when you can learn the necessary things from data directly?\n\nFinally, the discussion of related work on ML + PDE solvers is an afterthought. You should be stating that there is recent related work that learns the solver 'end-to-end' and this is another step in that direction for a different set of solvers. \n\nYou should be citing Bar Sinai et al., 2019. \"Learning data-driven discretizations for partial differential equations,\" as your work is similar methodologically and has essentially the same motivation: learn a solver by from data. Their exposition is substantially more precise in terms of the problem setup and the motivation for their method.\n\nFinally, this paper and related work on deep nets + PDEs seeks to introduce a paradigm shift to the numerical PDE community. Why publish in ICLR, where the usefulness of deep nets is well-established? Shouldn't the paper be appearing in a venue for numerical PDE researchers?\n\n=method=\nNeed to set up the overall ML problem before you introduce a new architecture. What is the train set, what is the test set? Is the test set from the same PDE, or do you seek to generalize, for example to PDEs with slightly different coefficients or grid sizes?\n\nHow is (5) diagonalizationed if you don't enforce that L is orthogonal? Also, you wouldn't need the pseudoinverse approximation if it was orthogonal.\n\nI was also confused by the usage of the pseudoinverse. It appears that you're making the entire A matrix low rank, by giving an inner dimension of N_h. Typically, a pseudoinverse would be used to 'invert' a matrix that is  NxN, but has rank N_h. \n\n=experiments=\nFig 2 confusing. How is it that RoeSolver is so much worse on such a simple problem? I'm assuming there's a well-known value for A for such a simple problem. Did you use it? Same in 4.2. RoeSolver is a family of methods, given by A. What does it mean to beat the RoeSolver?\n\nFig 3: there is no analytical solution, but can't you approximate one using a much much finer space/time grid? In these experiments, why is success defined as matching the RoeSolver, but this wasn't the case in Fig 2?\n\nIn general, I found the discussion of the baseline RoeSolver very confusing. It sounds to me like there are lots of choices in constructing A by hand for a traditional solver. How did you do this? Also, do the values output by your network seem to match that hand-crafted A?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}