{
    "Decision": "",
    "Reviews": [
        {
            "title": "A method leveraging contrastive learning to improve few-shot learning",
            "review": "This work proposed a new method to combine the self-supervised contrastive learning (CL) technique into the existing few-shot learning (FSL) framework, and succeeded in improving the performance of the metric-based meta-learning methods both on standard FSL tasks and fine-grained tasks. Particularly, it brought up a view-generation technique based on spatial transformer network (STN). The idea is straightforward. \n\nI lean towards rejection due to the following concerns:\n\n1. In comparison to a direct combination of CL and FSL, the main specialty is to use STN rather than random augmentation normally used in CL. Table 5 shows that PN+AVCL is better than PN+CL. Can the authors provide the details of how to conduct the so-called\" traditional random-view contrastive learning\"?  What augmentation strategies are used? \nAs written in the paper, \"Aside from local-to-local and global-to-local views which can also be accomplished by random cropping, our auto-view module additionally allows one dimension scaling, Translation transformation, image deformation and proportional shrinkage\", the enumerated augmentation strategies actually can also be used during random augmentation. Therefore more clarification is needed. \n\n2. The description in Section 3.2 is not clear.  \nWhich view is fed to equation 1 to be classified? $F_\\theta(x^{(1)})$ or  $F_w(x^{(2)})$? Or both? Do you drop the original input $x$? In the current version, $F_\\theta(x^{(1)})$ and $F_w(x^{(2)})$ seem to have nothing to do with classification.   \n\n3. Why this idea is particularly useful to fine-grained sub-categories? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Technically not novel, insufficient experiments",
            "review": "This paper proposes to use contrastive learning to improve few-shot learning, where an STN is also learned using meta-learning to generate better views for contrastive learning. Experiments show the proposed method can outperform some variants of PN+self-supervised learning methods.\n\nAlthough the basic idea is interesting, I have several concerns w.r.t the paper:\n- The main contribution of this paper is combining contrastive learning and the meta-learned view generator for few-shot learning. But the idea of using self-supervised methods to improve few-shot learning is not new (Gidaris et al., 2019 and [r1]). I also found a few concurrent works combining contrastive learning and few-shot learning.  The meta-learned data augmentation method is also not new (see [r2]). The proposed method looks like a simple combination of several existing methods, which makes the contribution of this paper incremental.\n- The experiments are insufficient. Many recent methods are not compared, such as [r1] and [r3]. [r1] reports PN+Jigsaw can achieve better performance on CUB and Cars. I suggest the authors conduct experiments using the same backbone as [r1] or [r3], or implement their method based on WRN-28-10 to directly compare with state-of-the-art results. Based on the current results, it's hard to judge the effectiveness of the proposed method compared to the state-of-the-arts.\n- The improvement over PN+CL is marginal.\n\n[r1] When Does Self-supervision Improve Few-shot Learning? ECCV 2020.\n[r2] OnlineAugment: Online Data Augmentation with Less Domain Knowledge. ECCV 2020.\n[r3] DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning. CVPR 2020.\n\nOverall, the proposed method is not very new and the results are not sufficient to show the effectiveness of the method. I lean to recommend rejection for this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work aims to perform the few-shot learning task, where only a few annotated instances from each class are given. To tackle this, a novel contrastive loss is proposed to augment the metric-learning-based loss, and a learning-to-learn module is proposed to overcome the drawbacks of random image transformation in contrastive learning. Extensive experiments are provided to verify its effectiveness.",
            "review": "Pros\n\n1 This paper is well written and the key contribution is easy to understand. Extensive experiments, including ablation study and comparisons to SOTA approaches, well demonstrate the effectiveness of the proposed approach.\n\n2 Applying contrastive learning to few-shot or long-tailed tasks has attracted much attention currently. This work adopts a learning-to-learn module to overcome the drawbacks of random image transformation. This idea is novel and may benefit many other tasks.\n\n3 The entire framework is clean and does not introduce too many hyperparameters. \n\nCons\n\nI have only one concern: The backbone of this work is WRN-28-10. It is better to report the results using ResNet 18/50 to fairly compare other SOTA methods in Table 2, 3. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes auto-view contrastive learning for latent fine-grained structure learning in few-shot image classification. More comparison experiments on fine-grained datasets should be conducted to prove its effectiveness and corresponding analyses should be added.",
            "review": "Pros:\n1.\tThis paper introduces contrastive loss in few-shot classification for fine-grained structure learning and validates on benchmarks.\n2.\tThe content of this paper is clear and readable.\n\nCons:\n1.\tThe comparison experiments are only conducted on one fine-grained dataset. More comparison experiments on other fine-grained datasets should be conducted, which can help show the effectiveness in exploring fine-grained semantic structure more comprehensively. Corresponding comments should also be added.\n2.\tApart from existing MoCo module and Prototypical Network, the core part is the proposal of auto-view contrastive learning. Whereas the experimental results on Places dataset in table 5 show its lower performance than traditional random-view contrastive learning. The authors should give corresponding explanations and analyses about the reasons.\n3.\tThe discussion of results in tables 2 and 3 are not enough. The authors only mention that the proposed approach outperforms than the compared methods, which lacks in-depth analysis.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}