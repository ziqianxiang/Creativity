{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting submission, but falls a bit short on the experimental side",
            "review": "# Summary\nThe paper presents a new multi-output Gaussian process (MO-GP) model, belonging to the class of autoregressors, which structures the GPs according to a directed acyclic graph (DAG). To avoid having to handcraft the relationships between the outputs, the structure of the DAG is learnt by applying structure learning algorithms from the field of graphical models. The contribution of the paper is therefore two-fold: a general view of MO-GP models as DAGs, as well as methods for discovering the most likely structure of the DAG.\nThe proposed model is evaluated against one baseline and four multi-output models on three datasets.\n\n# Evaluation\n## Strong points\n* Proposes an interesting MO-GP model, which is both flexible and shows good experimental performance.\n* Introduces structure learning from graphical models to MO-GPs.\n* Has a nice mix of methodological advancements and practical recommendations.\n\n## Weak points\n* Experimental setup not the most appropriate.\n* Experimental evaluation somewhat weak.\n\n## Recommendation\nBorderline reject.\n\n## Detailed feedback\nOverall, the paper is nicely written and contents well-presented. The background section is rich and related methods are presented and discussed in detail. This was a joy to read, although, given the short experimental section, it is perhaps a little too detailed. A few things could easily be postponed to the supplementary material without impacting the understanding of the model.\n\nGenerally, I think the contributions of the paper are interesting, albeit not as novel. Viewing MO-GPs from the point of view of DAGs is really interesting and is in some sense a unifying view of many MO-GP models, as the authors point out. However, it is in itself a relatively modest contribution. The same is true for the structure learning methods from the graphical models literature - these are both interesting and relevant to the MO-GP community, but there is otherwise no new contributions here.\n\nStill, I think the introduced methods have a real possibility of making an impact in the MO-GP world by offering a different perspective of MO-GPs (as DAGs), and making the community aware of known methods optimise the structure. As such, I think the paper certainly deserves publication at a top-tier conference, but in it's current iteration I do not find the experimental section convincing enough to recommend acceptance.\n\nFirst of all, I think the authors have done a very good job of testing their model in a fair way. They compare against a baseline and four competing methods on three datasets (which is, however, on the low side, but the datasets are quite different, so it's not a big issue), and they consistently test with two covariance functions (RBF and SM2), even though their model is always outperformed when using RBFs. This is not only fair, it also shows important aspects of the model, so I am thankful to the authors for this.\nThat being said, the results in table 1 needs errors. The models are tested by four-fold cross-validation, but only the mean of these seem to be reported in the table. Given that the difference in performance between models is often very little, we really need a standard deviation on these results across the CV folds to judge how significant the numbers are.\n\nMy main criticism of the experiments is that I do not think the experimental set-up tests the relevant aspects of the model. The test sets are constructed such that all outputs at a test location are missing, but I do not think this is the right way to test the models. The main advantage of multi-output models is that they can make use of other outputs at a given location to constrain the predictions. If all outputs are missing, there is very little information to be used when predicting missing a output (unless, perhaps, a strongly linked output is very certain here). I suppose this is also why independent GPs perform just as well, and sometimes even better, than the MO-GPs.\nInstead, I think a better experimental set-up would be to only consider a few outputs as missing at each test location, similarly to how Requeima et al. do for GPAR. In such a set-up, it is easier to see if the MO-GPs have actually learnt to take advantage of output correlations, and I suspect it would also be easier to see cases of negative transfer - especially, considering the time-series experiments, if the test locations are chosen as a longer sequence. At the very least, I think these experiments should be included alongside the current ones.\n\nI am, however, happy to be convinced otherwise by the authors, if I have misunderstood something.\n\nApart from the set-up, I think the plots in figure 2 look odd. All methods seem extremely underconfident, even for the training set data. Most methods have their predictive means very close to the data, but the uncertainties are enormous. What is going on? Additionally, what do the shaded areas actually show - one/two standard deviations or the variance?\n\nLastly, there are some missing experimental details, which would be good to have (say, in the supplementary):\n* How was the structure for GPAR found?\n* For the Andromeda and Exchange experiments, are the test locations chosen in sequence (like in Requeima et al.) or completely at random?\n\nOf smaller issues, I think the authors sometimes overstate the importance of the contributions, in particular the structure learning, which is claimed to only introduce correlations when justified. Surely, these methods can fail too, which the authors also hint to in section 5.\n\n# Questions\nThese are the changes I would like to see to raise my score to accept:\n* Errors across CV folds in table 1.\n* Additional experiments omitting only one (or a few) outputs per test location, similar to Requeima et al. (2019).\n* Why are all methods so underconfident in figure 2, even around the training data? The length scales seem short enough.\n\nMinor questions (not likely to affect my score):\n* In figure 2, why are GPAR and conv not shown? In particular GPAR, as it is the most recent model.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "see review",
            "review": "This paper proposes a model for Multi-Output Gaussian Processes (MOGPs) with the purpose of reducing negative transfer across the outputs. The proposed model relies on inferring a graphical model structure that specifies the correlations between the outputs. The paper models each output as a linear combination of the parent outputs specified by the graphical model, thus effectively reducing the number of correlated outputs due to the conditional independence statements given by the inferred graph. In order to learn the structure of the graphical model from data with observation noise, the paper proposes the evaluates the use of constraint-based method as well as score-base methods. The experimental contribution consists of a comparison against independent GPs, the semiparametric latent factor model (SLFM), Convolutional MOGPs (ConvGP), Gaussian process autoregressors (GPAR) and Collaborative MOGPs (CoGP) on three real-world dataset.\n\nThe proposed model is definitely relevant within the context of MOGPs, not only due to the regularization properties it adds to MOGPs but also by inducing sparsity on the overall covariance matrix which is a most needed property for scaling MOGPs to a large number of outputs. The main weakness of the paper is that it needs to provide more details about the training procedures, it feels particularly confusing about this topic. For instance, it is not clear sometimes whether the different information criteria are being use for structure selection within a score-base method or for testing conditional independence within a constraint-based method. As it is, the paper doesnt guide the reader on which method use nor it takes a decision based on a principled approach.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting extension to MOGPs",
            "review": "##########################################################################\nSummary:\n \nIn this manuscript, the authors propose a novel way of training multi-output Gaussian processes (MOGPs) via a two-step procedure. First, they learn a directed acyclic graph (DAG) from training data to restrict latent-factor dependencies. Subsequently, they train the resulting latent-factor parameters. The prior down-selection of possible MOGPs resolves two problems: negative transfer due to overfitting and (related) reduction of model parameters. Predictive performance is then demonstrated to be comparable or better than in unconstrained MOGPs. The authors have to resolve non-trivial hurdles to train their model, where model assumptions and assumptions on data for DAG learning are contradictory.\n\n##########################################################################\nReasons for score: \n \nOverall, I vote for weak rejection (5). While I am not an expert in MOGPs, I have some expertise in structure learning. My main problems with this submission are:\n1.\tA DAG is (in general) not uniquely identifiable, and a discussion and analysis of the implications of this fact are missing. How do the authors finally select their DAG? A suggestion would be picking the one with the best possible predictive performance.\n2.\tIt is appreciated, that the authors are open about the contradictions between DAG learning and MOGPs (sec 5.1), but their methodology is barely explained and their procedure needs to be analyzed better/supported with simulation results.\n3.\tThe contribution of extending MOGPs to DAG-GPs seems incremental to me.\n4.\tThe simulation study is sparse. As the main claim of the authors is a reduction in negative transfer, I would have liked to see a dedicated (synthetic) experiment highlighting this.\n\n##########################################################################\nPros: \n1.\tThe paper is well written and for the most part understandable to a broad audience.\n2.\tThe combination of MOGP and DAGs is certainly interesting and seems to perform well.\n3.\tTwo real-world datasets\n4.\tRelation to existing methods is established   \n\n##########################################################################\nCons: \n1.\tSee my Reasons for score.\n\n##########################################################################\nQuestions during the rebuttal period: \n \nPlease address and clarify the cons above. \n \n#########################################################################\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Review",
            "review": "Summary:\nThis paper aims to present a method for capturing the relationships between multiple outputs appropriately and to yield better prediction performance over the state-of-the-art MOGP models. The key point is that dependencies between outputs are determined by using graph structure learning techniques developed in the graphical model community. The structure is learned from down-selected data before training the MOGP. This separate learning scheme is advantageous in saving computational complexity. The proposed method is evaluated using multiple real-world datasets.\n\nPros:\n1. The idea of utilizing structure learning techniques to determine dependencies between outputs is interesting and practically useful. \n2. The manuscript is well-written. The proposed method is well-motivated, and the authors indicate the drawbacks of the previously published MOGPs.\n3. The experiments on real-world datasets show that the proposed method with a flexible kernel is superior in terms of prediction performance.\n\nCons:\n1. Although there is a little ingenuity (I.e., dawn-selecting samples), the proposed method itself is a straightforward combination of the MOGP and the existing structure learning technique. I understand this option decreases computational costs drastically, but I am concerned whether the separate learning scheme of GPs and graph structures is an appropriate option. \n2. The technical difficulty happens by combining graph structure learning with MOGPs, as mentioned in Sections 5.1 and 5.2. The corresponding solution (i.e., down-selecting samples) is sufficient to effectively integrate two different techniques? The assumption of small noise variance is natural in practical situations? The technical contribution seems to be incremental. \n3. The experimental results are not convincing to me. Details are described below.\n\nReasons for score: \nThis paper is well-motivated and presents a good idea that is to determine the dependencies between multiple outputs via graph structure learning. However, the proposed method is incremental; in that case, a thorough experiment should be conducted, but it is not currently sufficient. Accordingly, my opinion is that this paper is not ready for publication. \n\nAdditional comments:\n1. The performance of the proposed method seems to be strongly dependent on the pre-trained graph structure. Could the authors provide the results of graph structure learning? Also, the reason why the performance (Table 1) degrades when using a simple RBF kernel is unclear. Probably, the prior graph structure learning might not work well? \n2. In Figure 2(b), the authors mentioned the reason that the crest can be learned by DAG-GP as follows, “because of the troughs on either side”. Does that mean the crest can be captured by using SM kernel? If that is the case, why does the CoGP with SM kernel fail to capture it? \n3. The authors state that one of the advantages of the proposed method design is computational efficiency. It would be better to add a result and/or discussion about it in the experimental section. \n4. Another approach to address negative transfer issues may be to introduce regularization (e.g., L1 or L2 norm) for the weight matrix $\\bf{B}$. Did the authors consider that approach? I think It would be great to compare the performance with the previous MOGPs with regularization.\n\nMinor comments:\nIn References, the term “gaussian” should be “Gaussian”.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}