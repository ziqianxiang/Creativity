{
    "Decision": "",
    "Reviews": [
        {
            "title": "Using graph neural networks for graph coloring",
            "review": "This paper presents an algorithm based on GNN for the graph coloring problem.\nThe presentation is not clear and should be improved. Here are some examples:\n- page 2: what is the index $k$ in $f_k$?\n-page 5: I do not understand the following definition: 'a function is color equivariant if it is equivarian twhen the node attribute is the probability distribution of colors.  A GNN A with L layers is color equivalent if and only if all functions in {AGG(i),COM(i):i∈1,...,L} are color equivalent.' What is the meaning of 'color equivalent'?\n-page 7: I do not understand: 'There are no trainable parameters in the simpler version, which avoids the time-consuming training procedures but may not be the best assignment scheme for these parameters.  An interesting direction for future work is to seek the solution for the vanishing gradient problem and keep the model trainable'  so you are using only the random initialization? Then on figure 2, it looks like training degrade the performances of the algorithm.\n- page 8: how do you get a proper coloring from the probabilities $h_v$?\n\nPlease provide your code with your submission.\n\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good theoretical results",
            "review": "Summary:\n\nThis paper analyzes the usefulness of GNNs for the graph coloring problem, as an illustration of their use in NP-hard graph problems.\n\nStrong points:\n\nThe topic is timely and relevant. The analysis seems technically correct and the insights are useful.\n\nWeak points:\n\nI'm unsure how much of interest these results are outside of the niche of theoretical results on GNNs.\n\nRecommendation:\n\nThe paper is in acceptable condition.\n\nMajor comment:\n\nThe authors make a point of relating this to permutation equivariance. AC-GNN, more commonly known as graph convolutional neural networks, are unavoidably permutation equivariant, as proved in Zou and Lerman, 2020 and Gama et al, 2020. While this may be useful in many practical problems and also helps explain the observed success of GNNs, not all problems of interest are permutation equivariant. As a matter of fact, the authors deal with one such problem. In those cases, it is recommended to use non-convolutional graph neural networks, as is commented in Isufi et al, 2020 (see Fig. 2 and ensuing discussion). This is furthermore evidenced in the numerical section, where GATs (an example of a non-convolutional GNN) performs reasonably well. I wonder what would happen if multi-hop GATs are considered instead (eq. (39)-(41) in Isufi et al, 2020), since they are very likely to improve the performance over single-hop GATs. Likewise, I believe it would be interesting for the authors to further elaborate on the fact that the problem is not permutation equivariant, while the AC-GNN (i.e. graph convolutional neural network) is, and as such, will not be able to appropriately capture the problem structure.\n\nF. Gama, J. Bruna, and A. Ribeiro, \"Stability Properties of Graph Neural Networks,\" IEEE Trans. Signal Process., vol. 68, pp. 5680–5695, 25 Sep. 2020.\n\nD. Zou and G. Lerman, \"Graph convolutional neural networks via scattering,\" Appl. Comput. Harmonic Anal., vol. 49, no. 3, pp. 1046–1074, Nov. 2020.\n\nE. Isufi, F. Gama, and A. Ribeiro, \"EdgeNets: Edge Varying Graph Neural Networks,\" arXiv:2001.07620v2 [cs.LG], 12 March 2020. [Online]. Available: http://arxiv.org/abs/2001.07620",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Understanding the shortcomings of GNNs for graph coloring",
            "review": "The paper deals with investigating the power of graph neural networks for the graph coloring problem (in an unsupervised setting). They show that so-called AC-GNNs, which are quite a general form of GNNs, will always fail to compute a valid coloring, i.e., they will color at least one pair of adjacent nodes with the same color. Moreover, based on known theoretical results, they show that any local algorithm (e.g.. GNNs) cannot provide optimal algorithms.   Moreover, they define the notion of color equivariant, an extension of the equivariance, and derive sufficient and necessary conditions.\n\nBased on their theoretical insights, they propose a simple GNN layer without trainable parameters, which is shown to have a monotonically decreasing cost. The paper is rounded off by a small-scale experimental study comparing the former layer to standard GNNs and heuristic approaches on benchmark and random instances. \n\nRemarks and Questions:\nUnfortunately, a lot of definition are not rigorously defined/or not self-contained, making it hard to understand the paper in full detail. For example:\n- Def. 3: What is the definition of \"r-local\"?\n- The definition of color equivariance is not self-contained. What is meant by \"node attribute is the probability distribution of colors\"? As this definition seems to be a central concept much more space should be devoted to formally define it.\n- Using CORA, PubMed, ... as coloring instances is somewhat arbitrary. Do they have a special structure that makes them hard for coloring?\n- Provide statistics (number of nodes, edges, average degree, ...) about the used datasets! Are these also used in the graph coloring community?\n- Ist Tabucol really a SOTA method? What about ILP-based methods?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A graph neural network method without learning ",
            "review": "The authors formally analyze the ability of GNNs to distinguish nodes and show that AC-GNNs are not able to provide optimal solutions for graph coloring. They formally derive a set of rules, which networks have to fulfill so that they can distinguish nodes and are color equivariant. Based on those rules they propose a GNN without trainable parameters for graph coloring.\n\nStrengths:\n- The summary of rules that the network needs to abide to is interesting.\n- The authors find that deeper networks and training does not lead to better results in coloring, which is an insight. However, the paper is not structured around that. \n- Results seem to be okay. They are on par with related approaches while being faster.\n\nWeaknesses:\n- Results about locality and non-optimality of AC-GNNs don't carry much new insight. That is okay though since it formalizes the limitations and serves as a baseline for the proposed method.\n- Theorem 2, which is a main contribution of this work, is already given in a very similar form by [1].\n- The first impression is that the authors propose a very restricted GNN (only 5 parameters per layer) to ensure color equivariance. Then, they state \"we assign constant values to [parameters], which makes GDN free of training and thus free of the vanishing gradient problem\", which makes this a non learning method. This approach looks like a relaxed variant of Weisfeiler-Lehman to me.\n- The presented approach is more related to graph algorithms than to GNNs, however, the relations to existing algorithms is not discussed at all and no related work is given.\n- Given that no parameters are trained, a large part of this work seems partially redundant.\n- Maybe the speed improvement compared to heuristic algorithms just comes from using the highly optimized GNN frameworks on the GPU. Can the authors guarantee a fair comparison?\n- All in all, there is not much novelty, except a simple, non-trainable method for coloring, which seems to be similar to existing graph algorithms.\n\n\n\nThe paper has issues with technical soundness/clarity in formalization:\n- The integrated combination function COM (equation 7) does not fit the function signature and thus is not defined (although it is clear what it means).\n- Not sure if the definition of color equivariance (which is not explicitly given) is necessary. The distribution is just a vector so Definition 4 suffices.\n- There are several confusions between \"equivariant\" and \"equivalent\" on page 5 after definition 4 and in the proof of Theorem 2, which hurts clarity.\n- Equations 8, 9: 11 is not defined (or is missing a ^T for a correct outer product)\n- Proof of Theorem 2: The first part about the necessary conditions seems to be not complete. It is not argued why other cases than the 3 provided ones lead to the same conclusion. Should be possible using the linearity.\n- Proof of Theorem 2: C_{n,m} = C_{n,m} is always true (probably a typo). The argument here why the matrix has to be in the specific form can maybe also be made using that (1) the identity matrix and the matrix containing equal elements commute with all permutation matrices + (2) linearity of the matrix sum. I do not fully understand the current version, might be due to errors.\n\n\nMinor:\n- Proposition 1 is unnecessarily confusing, I recommend to rephrase it to \"No AC-GNN can discriminate an/any equivalent node pair\"\n- Proofs in appendix should be referenced in main text\n- Equivalent node pairs and r-local equivalence should be defined in the main text (or at least referenced), not only in the appendix. They are crucial for understanding.\n\nAll in all, I recommend to reject this paper. The reasons are lacking novelty in the individual parts, a presentation with low clarity and the fact that the paper misses its mark by proposing a non-learning method while embedding itself in the context of GNNs.\nIf written and analyzed from a different angle, the insights in this work might be publishable. I suggest that the authors rethink the scope and goals of this work.\n\n[1] Zaheer et al., Deep Sets, NIPS 2017",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}