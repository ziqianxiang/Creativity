{
    "Decision": "",
    "Reviews": [
        {
            "title": "ICLR review for \"Illuminating Dark Knowledge via Random Matrix Ensembles\"",
            "review": "This is about applications of RMT to derive generic formulas for the generalization performance of models trained with distillation. I will go straight to the point: it is hard for me to find value in the paper, at least written as such. It \"feels\" like there may be underlying interesting ideas, but it is clear that the paper has been written (too) fast, and as a consequence does not match the standards of presentation and rigor. The paper is supposed to be a mathematical paper, but not a single proof is provided. The appendices are missing. All statements are very loose, if not impossible to understand. There are very few quantities properly defined (just an example: Y, P_T etc which are the first quantities introduced are guessed to be distributions as they appear in KL divergence below, but then they appear to be matrices). Highly non-trivial lemmas are simply stated: Lemma 1: it is not clear whether it is the training or generalization performance that is considered. I guess from what is written later that it is the generalization. But then there should be an averaging in some way of the fresh data/sample distribution, etc. < > is en expectation over what? Then highly non-trivial results from the random energy model are just called without much explanations, etc. Proposition 8: \"Under the hypothesis that SGD dynamics does not introduce long range correlations between the parameters of the model whenever the model’s parameters are initialized randomly\" : What means here long-range correlations? Nothing is precise. The paper requires a full guess work from the reader to decode. \nI think that the authors must make a considerable effort of clarity and rigor before resubmitting to any conference or journal, especially for a paper that seems to be math-oriented",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper is badly written and lacks critical material",
            "review": "This paper wants to better understand “how do overparametrized neural networks generalize so well ?”, which is one of the most important open questions in the learning theory community. They try to study this phenomenon via the prism of Knowledge distillation, and using Random matrix theory.\n\nThe authors claim to develop a theory of distillation. The idea of distillation is to compress trained a overparametrized neural net (a “teacher”),  into a much less overparametrized network (a “student”). The loss function of the student is then a function of both the predictions of the teacher and the ground-truth labels, the idea being that even wrong predictions of the teacher might be interesting for generalization. The authors wish to develop a theory of “when does this idea gives substantial gains ?”, without only empirical evidence.\n\nThe question asked by the authors is : in this context, if the student is less good than the teacher at generalizing, what is its performance when trained using distillation ? This is indeed an interesting aspect of studying generalization vs overparametrization.\n\nUnfortunately, I can only strongly oppose the acceptance of this paper, for the following reasons:\n- All the theorems and lemmas are left unproven. While the authors claim to prove some of them in the appendices, such appendices are not provided. A theorem unproven can only be considered as a claim, and no analytical justification of these claims (even heuristic) is provided. Moreover, the paper is only 6 pages long (while the limit is 8) and these claims take up 4 of these 6 pages (one page being taken by the introduction and one by the numerics). Given that they are not justified, I can only consider these 4 pages of statements as empty.\n\n- The numerics provided in Section 3 are not strong enough to justify the claims of the paper. In particular, Figure 2 is very badly made, as almost 80% of the figure is blank ! \n\nAll in all, I can only recommend to the authors to revise their manuscript, and to resubmit an enhanced version of their work at a subsequent venue.",
            "rating": "1: Trivial or wrong",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper attempts to understand knowledge distillation, where the \"student\" is trained using both a training dataset and a \"teacher\"'s knowledge. By using tools from random matrix theory, it provides a formula for the generalization error under distillation. ",
            "review": "### Major comments\n- The paper is not easy to read.  \n\t- Equation (1) is given without much explanation. It cites [Hinton et al. 2015] but I didn't find the same definition or the \"interpolation parameter\" there. Also, since $\\mathbb{P}$ and $\\mathbb{Y}$ are matrices, what is the cross-entropy/relative entropy? What are their dimensions? \n\t- In Lemma 1, there is no explanation on how $\\gamma^T$ comes out. \n\t- In Lemma 2, what does \"SGD dynamics\" refer to and what does it mean by saying two \"SGD dynamics\" are \"identical\"? \n\t- In Equation (4), $\\mathbb{P}_{dual}$ is a convex combination of $\\mathbb{P}_1$ and $\\mathbb{P}_T$. Are $\\mathbb{P}_1$ and $\\mathbb{P}_T$ the softmax outputs for the distilled students? How is the dual student trained exactly?\n\t- Proposition 5: I assume that Poisson-Dirichlet distribution is over the infinite-dimensional simplex. But here the limit is only driven by $\\gamma\\rightarrow0$. Do $N_{data}$ and $D_{classes}$ also go to infinity? Which theorem or page is cited in the book exactly? Again, no intuition or explanation is provided regarding this theorem. \n\n- Proposition 8 (ii): I assume the generalized Marcenko-Pastur law is the limiting distribution of the eigenvalues of $\\lim_{\\gamma\\rightarrow0}H_\\gamma$, how is it determined by the trace only?\n\n- Length: The main body only exceeds 6 pages. The math formula and cited previous theorems take up a lot of space.  \n\n- There is no proof or appendix provided. I cannot check if the theorems are correct. The numerical results are also not strong enough to justify the theory.\n \n### Minor comments\n- The notations are not conventional. For example, $\\dagger$ usually means pseudo inverse and $\\top$ denotes transpose; $\\mathbb{P}$ usually denotes probability. It's not a big deal but it can be confusing for the readers. For example, it takes me some time to understand Equation (2) because initially I thought $\\mathbb{P}_{\\gamma^T}\\mathbb{Y}^\\dagger$ means the probability of pseudo-inverse $\\mathbb{Y}$. \n\n- Some quantities are not defined before using, e.g. $N_{calsses}$, $N_{data}$, $\\mathbb{P}_{\\gamma^T}^{(dual)}$, etc . They are self-explained but not rigorous.\n\n- Page 3 bulletin 2: there is an extra \"matrix\".\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Presentation issues and unsupported claims ",
            "review": "The current work seeks to predict the performance of a student trained using a combination of ground truth and teacher labels, based on the given ground truth performances of the student and teacher. To derive an approximation for the distilled student performance the authors make two analytical claims  \n\n1. That a certain dual student DNN trained on interpolated labels with interpolated outputs matches the performance of a student trained using distillation. \n2. That the output of the network, viewed as a matrix with (label,data) indices, is described by a generalized Wishart distribution governed by a single parameter. \n\nThey also show numerics on CIFAR-100 where the distilled, non-distilled, and predicted performances are plotted. \n\nDespite reading the entire work several times and attempting to rederive some of their results, I failed to follow much of the details. While I might have blamed my self for this, several clear presentation flaws make me feel that many other readers would encounter the same problem. Let me give a few examples: \n1. The authors often refer to a non-existing appendix. Only its title seems to appears in the manuscript. \n2. The authors show numerics without explaining the experiment, only referring to a non-accessible/non-existing GIT repository.\n3. The vast majority of the paper is written as Lemma and Propositions which come without any form of proof. \n\nThere are various other representation comments. For example, the authors used subjective writing extensively, via quotations marks,  superlatives, and section titles such as \"drilling down\". They define various quantities in a loose manner, in one example, they use an average without specifying the ensemble (Here I comment that their <\\delta Y> in Eq. 7. should be equal to zero by their definition whereas, apparently, it is not).  Their main analytical result in Eq. (12) contains an unknown function $g(..)$. \n\nSome other comments that are not strictly about representation are \n\n1. I tried to prove their Lemma 2. by plugging their definitions of the interpolated labels and interpolated outputs into the non-distilled (T=0) loss they show in Eq. 1. I failed to find any agreement between the loss of this dual student and the student trained using distillation. \n\n2. The numerical predictions in figure 2 seem to fluctuate widely on the scale between the vanilla and distilled student. As the performance at T=0 and T=\\infty is taken as given, I didn't find this to be a strong verification of the theory. \n\n3. It seems too good to be true that a theory which does not take as input various important aspects of deep learning, such as the architectures of the teacher and student and the specifics of the dataset, can make accurate predictions. Similarly unnerving is the authors' statement that \"Empiricallly, we found that the estimator works best for students with fully\nconnected layers trained on whitened inputs\". Where do these apparently necessary assumptions come into the play in their derivation?\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}