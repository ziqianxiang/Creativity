{
    "Decision": "",
    "Reviews": [
        {
            "title": "A very interesting topic, but the technical contribution is limited, some claims are not supported, writing requires significant improvements, and there are several concerns with the experiments ",
            "review": "Summary.\n\nThis paper is on out-of-distribution (OOD) detection using variational autoencoder (VAE). In particular, this work distinguishes between two types of datasets, namely “simple” (e.g., MNIST) and “complex” (e.g., CIFAR). The authors observe that VAEs trained on complex datasets tend to assign higher likelihoods to samples from simple datasets thereby failing to detect OOD samples correctly. To mitigate this issue, the authors propose to train VAE both on a complex (also referred to as a base) dataset and a simple dataset. Both datasets share the encoder and decoder but use different priors p(z) over the latent space, i.e., the prior over z for the base dataset has a higher variance. This configuration is referred to as Bigeminal Priors VAE (BPVAE), and the authors claim that it would assign a higher likelihood to samples from the base-dataset than to those from a simple dataset. Evaluations are carried out on real-world datasets on the tasks of image reconstruction and OOD detection.\n\nMain comments.\n\nThe topic of the paper is very interesting, however, there are several concerns with the current version.\n\n1. The paper does not make any new technical contribution. The proposed method consists in training VAE on two different datasets with different priors over each of them.\n\n2. The authors claim that by adopting different priors with different variances, BPVAE would assign higher likelihood to samples from the base dataset than to samples from simple datasets. This claim is not clear and needs to be supported by some theoretical analysis.\n\n3. The writing requires significant improvements. Several sentences are hard to follow or even incorrect, and there are many typos (some are reported below). Moreover, some acronyms are not defined, especially in the results section.\n\n4. There are also a number of issues with the experiments.\n\n    a. The information regarding the experimental settings is very brief. Important details are missing, such as the detailed settings used for the baselines, the variances of base/simple priors as well as their impact on the performance of BPVAE, the method used to compute the marginal log-likelihood log p(x), just to name a few.\n\n    b. Some comparisons are not fair. For instance, on the reconstruction task it is clear that VAE will perform poorly on MNIST as it was not trained on it as opposed to BPVAE.\n\n    c. A simple and important baseline is missing, which consists in training VAE on both the base and simple datasets using the same prior. This would help to assess the impact of using different priors.\n   \n    d. The experiment of section 4.1 is not appropriate to answer the question regarding whether BPVAE knows what it does not know. \n\nHere are some typos.\n\n1. “… usually give higher likelihood for the simple test samples “but” identify them …” May be “and” instead of “but”. \n2. \\tilde{z} and \\tilde{x} after e.q. 2 are not defined.\n3. After e.q. 5 it should be p(x|z) for the decoder. \n4. In the captions of figures 5 and 6 it should be BPVAE instead of VAE.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but the modelling and the implementation details are lacking",
            "review": "Review:\n\nThis paper addresses the problem of distinguishing between out-of-distribution (OOD) samples and in-distribution (ID) samples using VAEs. The authors train the parameters of their model (BPVAE) for a specific dataset (basic dataset) by feeding additional examples of an external dataset (simple dataset) with more complexity than the basic dataset, such as the model can learn two different likelihood distributions and can assign higher likelihood to the ID samples. They also propose to use two different priors for the latent space to handle different levels of uncertainty depending on whether they model the basic dataset or the simple dataset.\n\nMain questions/comments: \n\n1) I have found several notation and modeling issues in section 3. Some examples include:\n\n - The model starts in section 3 with x and x tilde, z and z tilde, but they do not address what the tilde variables stand for. From Figure 3b I assumed they are related with the simple dataset, but I might be wrong.\n - In section 3.2, y is never introduced. I thought x tilde was the simple dataset, is it maybe y?\n - The tilde variables are never used in equation 5, which contains the loss of the model. Furthermore, only one of the priors defined in equation 4 is used in the loss. It is not clear to me how the two priors influence the training of the model.\n\n2) I miss some implementation details in the paper that I think they are crucial to replicate these experiments (I might have missed some of them)\n\n- Equation 4 introduces two variance hyper-parameters. The authors say only that the variance of the b-prior is set higher than the variance of the p-prior. What are the exact values used in the experiments?\n- The model handles two datasets during training. What is the proportion of samples from one dataset or the other during training? 50/50?\n\nMinor comments:\n\n- You mention in section 3.1 that the decoder uses a Bernoulli distribution. However the images of most datasets are at most gray (not just zeros or ones). Is there any pre-processing of the images to make them black or white? Otherwise, why using a Bernoulli likelihood model?\n- Why is the simple dataset composed only by one additional dataset? Wouldn't make more sense if it was a collection of multiple instances from different datasets, to account for different OOD modes?\n- \"Note that as for PSNR, the value lower is better, and as for PSNR and SSIM, the value higher is better\" -> I assume you meant that for the MSE, the lower the better.\n\nSummary:\n\nI found the main idea of this work interesting, specially how the authors show how that VAEs trained on a specific dataset can provide better likelihood to samples from a different dataset than samples from the trained dataset. This is mainly showcased in Figure 1 and appendix B. However, I have found this work lacking mainly in the model explanation and the model implementation. There are many details that are missing or not properly explained in the text.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but the article has serious issues that need to be adressed",
            "review": "This article provides an approach to answer the likelihood issue observed in VAE used for OOD-detection, in which the models can assign higher likelihood to OOD data than ID, in particular when the OOD data is simpler than the ID one. The proposed method uses a secondary dataset representing simpler data during the training process to alleviate this shortcoming, coupled with a structured prior enforcing that points from the simpler dataset are encoded closer to the origin of the latent space than those of the ID dataset. The approach displays perfect OOD detection on MNIST/FashionMNIST and CIFAR10/SVNH.\n\nI find the general idea of the paper of structuring the latent space according to complexity of the datasets interesting, however I believe this article has some serious shortcomings that need to be addressed:\n\n1. The proposed \"Bigeminal Prior\" is in essence a conditional prior set on the latent space of the VAE. As such, I find it surprising that the link of the proposed model to Conditional VAEs is not discussed at all in the article.\n\n2. The proposed priors differentiated by their variance is justified by the \"uncertainty\" of the prior capturing the features of the datasets. I find this explanation rather unconvincing and quite hand-wavy. I suspect this principle and the variance choice could be much better justified with an analysis of the behavior of Gaussian distributions in high-dimensional spaces, and the concentration of the typical set around a thin shell: the lower variance brings this shell closer to the origin of the latent space, ensuring that the \"simple\" data is encoded in high-likelihood regions of the b-prior.\n\n3. I find it extremely suspicious that the performance of the proposed BPVAE is only reported on test cases where the AUROC and AUPRC are exactly 1.0. Assuming these results are produced in good faith, I think that additional experiments with more complex tasks should be provided to better identify the relative performance of the model compared to the state of the art.\n\n4. The provided model uses two datasets to be trained (an ID one, and a \"simple OOD\" one), yet its performance is only compared with that of models that are trained only on the ID dataset. This is not a fair comparison.\n\nFor these reasons, I believe the article does not meet the threshold to be accepted to ICLR. I encourage the authors to improve their theoretical analysis and empirical evaluation before resubmitting this article.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea; hard to read and limited evaluation",
            "review": "\n**Summary**\nThe manuscript proposes to train a VAE model simultaneously on two datasets, the training/”basic” dataset and another “simple” dataset, with the same encoder/decoder networks but two latent priors. Both priors are mean-zero gaussians with independent dimensions, with the “simple” dataset having a smaller variance than the training dataset. In other words the simple dataset is “inside” the training dataset in the latent space. The authors claim this training leads to substantially improved anomaly detection performance.\n\n**Main Impression**\nThe broader idea to learn a more simple prior for “simple images” and a more “complicated”/specific prior for the actual training distribution seems appealing. Unfortunately, the writing is very hard to read for me, details are difficult to understand, it remains very unclear why the specific setup of this paper should work for OOD detection and the evaluation is very small and not well-described.  \n\n**Concerns**\nThe writing is extremely hard to follow for me. Basic/simple are already very easy-to-confuse terms for the two datasets. Then there are a lot of typos like IMGAENET, CIAFR10, FahionMINST, etc. Or word mixups like here: “Note that as for PSNR [-> should be MSE], the value lower is better, and as for PSNR and SSIM, the value higher is better.” \n\nFirst experiment is  hard to understand for me. In 4.1, VAEs are compared to BPVAEs with regard to reconstruction quality on the simple dataset, when trained with CIFAR10 as the “basic” and MNIST as the “simple” dataset. It is never written explicitly whether VAEs were trained both on CIFAR10 and MNIST or only on MNIST. If only on CIFAR10, it is no surprise that they are worse BPVAEs which were also trained on MNIST. If VAEs were also trained on both, it is still not  that much of a surprise that they are performing worse than BPVAE due to their single prior, it also feels like a bit artificial experiment to me to be honest.\n\nSome sentences are superhard to read, for example: “This illustrates that although adding extra prior can indeed facilitate the VAEs’ robustness and representation capacity, alleviating OOD problem by shifting data distribution of low-complexity dataset, but the distribution scale where it can cover is not infinite, which usually lies in the nearby neighborhood from the data distribution captured by b-prior and s-prior.”\n\nIt is hard to understand for me  why the results turn out the way they are. As an example, take figure 6a). Apparently, here the BPVAE is trained with CIFAR10 as the basic dataset as FashionMNIST+KMNIST  as the simple dataset. FashionMNIST and KMNIST have very low log likelihoods, but why is that so? Likelihood is a combination of MSE and KLDiv for VAEs, and MSE should be quite low on the simple dataset as per Table 1. Then is KLDiv for FashionMNIST so much higher for CIFAR10, even though FashionMNIST lies “inside” CIFAR10 as per the definitions in 3.2? \n\nAlso, it is only mentioned in supplement A that rgb datasets were grayed.  Nalisnick et al. (2019a) already showed that moving the rgb images to grayer images increases their likelihoods. So this graying may affect anomaly detection performance quite strongly, and now the effects of graying and the effect of the two priors are impossible to tell apart. \n\nThe evaluation is very limited only looking at FashionMNIST vs MNIST and CIFAR10 vs SVHN.  I think it is at least also necessary to test reverse, so SVHN in-dist vs CIFAR10 OOD, to see if this method degrades performance on “easy” cases. And further OOD datasets should also be there, like LSUN/CelebA etc… Also log likelihood performance should be reported and compared to other published works for the VAE of this manuscript and the BPVAE.\n\nIt is also unclear from the writing whether the evaluation was correctly performed using only the test datasets of in-distribution, which had sometimes been neglected in prior work.\n\n\nI also find many of the figures not that appealing, e.g. small fonts, or thin bars in Figure 2 a, where no bars are necessary… \n\nOverall, I find it very hard to learn something with confidence from the manuscript in its current form.\n\n**Suggestions for Improvement**\nThe structure, text and figures should be substantially revised to make it easier to follow the arguments. The evaluation should be expanded and effects like graying and the two priors clearly disentangled, and the results should be better described and analyzed (e.g. why does FashionMNIST have low likelihood in Fig 6a, is really simply KLDiv higher due to different variance?). \n\nMaybe also have a look at recent work that uses two distributions (in dist and general dist) in a bit different way: https://arxiv.org/abs/2006.10848\n\n\n\n**More Minor points:**\n\n“Song et al. (2019) demonstrate that OOD detection failure can induce sophisticated statistics based on the likelihoods of individual samples; they proposed a method that is in-batch dependencies for OOD detection.” -> the failure induces statistics?!\n\n“The priors are formulated as followings,”\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}