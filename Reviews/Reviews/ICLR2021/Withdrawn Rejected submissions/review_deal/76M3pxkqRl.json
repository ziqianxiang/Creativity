{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at ICLR."
    },
    "Reviews": [
        {
            "title": "simple and effective idea",
            "review": "##########################################################################\n\nSummary:\nThis paper proposed a loss function that leads to cooperation between two individually optimized agents in a matrix game. The authors demonstrate this result in three matrix games. The authors further proposed a method to generalize this loss to non-matrix games and showed its effectiveness. \n\n##########################################################################\n\nReasons for score: \nOverall, I vote for acceptance. The idea is quite intuitive and simple while effective. The empirical result is strong and clearly demonstrates effectiveness. The authors addressed the generalization issue, which could have been a major concern, by proposing a method mapping non-matrix games to matrix games. \n \n##########################################################################\n\nPros: \n\n1.The writing is very clear and easy to follow. The paper is well-motivated and the contributions compared to previous work are clearly stated. \n \n2. The empirical results are strong compared to the considered baselines.\n\n3. The authors took care of the generalization issue by proposing GameDistill and presented empirical results. \n\n##########################################################################\n\nCons: \n\n1. Iâ€™m not sure what are the major differences between the three matrix games tested in the paper. It would be good to state the hypothesis and discuss what additionally we can learn from the results from different settings. Also, It seems to me that the matrix game is easily parameterized so that potentially the authors can test the loss under various parameters. Thus, though I am convinced by results from the presented three settings, it remains open whether there are bad conditions in which the loss might fail. \n\n#########################################################################\n\nOther questions/suggestions:\nIt is already quite interesting enough to only consider two agents scenario, but it seems to me that the method can be potentially extended to multi-agent (n>2) cases. It would be interesting to see a discussion or a quick experiment. I think that would make the paper even stronger. \n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Effective and simple method for social dilemma matrix games; method extends to non-matrix games; motivation and explanation need improvement.",
            "review": "This paper focuses on the problem of multi-agent cooperation in social dilemmas, in which mutual defection is individually rational but collectively suboptimal. The authors use the bias toward status-quo in human psychology to motivate a new training method, called SQLoss: 1) for repeated matrix games, each agent is trained with additional imagined episodes in which the actions taken by both agents are repeated for a random number of steps; 2) for settings where cooperation and defection are associated with a sequence of actions, the authors provide a procedure called GameDistill based on trajectory encoding, clustering, and action prediction to arive at oracles for \"cooperative action\" and \"defection action\" at each state, which can then be used for the imagination episodes. Experiments show that SQL achieve better social welfare than LOLA and standard independent RL in classic iterated matrix games, as well as in the Coin Game with higher dimensional image observations.\nThis paper deserves a 6 mainly for showing that an agent based on a seemingly simple concept is sufficient for outperforming the well-known LOLA agent on standard benchmark matrix games. However, precisely because the method in this work seems to be so effective, there is a glaring need for improvement in many areas, and there are significant open questions for further research.\n\nThe introduction and motivation for SQLoss needs improvement. The \"status quo bias\" is the central concept in the paper. Hence, in the introduction, the authors must define it precisely (rather than simply relying on references, or even worse, relying on people's 'common sense' understanding of the term). The authors also should explain the main reason from human psychology that it leads to cooperation in social dilemmas. Currently, the text merely states the term and immediately explains their proposed SQLoss, saying that \"SQLoss encourages an agent to stick to the action taken previously\". I doubt \"status-quo bias\" exactly means \"stick to the action taken previously\". More importantly, the authors themselves say in Section 2.3.1 that \"this imagined episode causes the exploited agent in (DC) to perceive a continued risk of exploitation and, therefore, quickly move to (DD)\". Hence it is not correct to say that \"SQLoss encourages an agent to stick to the action taken previously\".\n\nThere is a key question: is there any hidden centralization at all in the method? SQLoss adds to a growing body of work that proposes methods to achieve multi-agent cooperation with decentralized training (Hughes 2018, Foerster 2018, Wang 2019, and two that the authors missed: Yang 2020 and Gemp 2020). However, there is some kind of implicit centralization in many of these works: e.g. in Hughes 2018, the centralized design of the intrinsic reward is imposed on agents prior to training; in Wang, training the reward is centralized; in Gemp 2020, all agents must \"agree\" before training to participate in updating the loss mixing matrix. To me, it seems that the implicit centralization required by SQLoss is in the GameDistill method, which is needed to generate the \"oracles\" from the temporally-extended game for use in imagination episodes. However, perhaps each agent can independently perform this preprocessing step of GameDistill, so that learning is really fully decentralized? The authors should clarify this important point.\n\nAnother key question: how does the method scale to more than two players?\n\nEquation (4) does not seem to match the text. Section 2.3.2 says $\\hat{\\tau_1}$ is a collection of agent's imagined experiences. The authors say that the imagined experiences only involves the repeated actions $u^1_{t-1}$ and $u^2_{t-1}$. But in equation (4), there are additional actions $u^1_{t+1},..., u^1_T$, which is confusing.\n\nThe largest experiments in the paper (Coin Game and non-matrix Stag Hunt) are still somewhat small, even for gridworlds. Do the authors see any potential difficulties in apply the method to larger social dilemma games such as Cleanup (Hughes et al. 2018)?\n\nAdditional comments:\n1) The authors use the word \"evolve\" in many places. This is imprecise and should be corrected, because the agents in question are learning via RL, not via evolutionary methods.\n2) The introduction mentions that LOLA requires complete access to the opponent, but this is not true since LOLA has been shown to work with opponent modeling.\n3) This paper used two clusters for GameDistill. Should the number always be 2?\n\nAdditional references:\nLearning to Incentivize Other Learning Agents. Yang et al. 2020\nD3C: Reducing the Price of Anarchy in Multi-Agent Learning. Gemp et al. 2020\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Blind review",
            "review": "This paper presents a new method for improving coordination in MARL by using the idea of the status quo. The approach uses a status quo loss and a method for converting multi-step games into matrix games (called GameDistill). The methods are described and experiments are given comparing the methods to other related work. \n\nCoordination is a problem in MARL. When multiple agents are learning at the same time, they can get stuck in poor equilibria. Therefore, ideas such as the status quo may be helpful in escaping these poor solutions. \n\nThe status quo loss is straightforward and of questionable use. The idea (in Equation 8) balances the regular RL loss witha  status quo loss that repeats the interaction (i.e., agent actions) for k steps. k is sampled from a distribution. There are weights for each of these losses to balance them out. This idea makes sense as a way to reduce the nonstationarity of decentralized learning, but it doesn't promote coordination. Why is this the right thing to do? What can be said about it theoretically? More motivation is needed for the approach. \n\nThe details of GameDistill are unclear and it also isn't clear how general it is. The method is described in text in the main paper and in pseudocode in the appendix, but each is high-level and not formal enough to understand the details. Furthermore, the approach clusters trajectories based on their rewards using random play. This seems unlikely to work well where exporation is an issue as random play may not be sufficient and in more complex games, you may need many clusters (and not know how many is needed). The paper should make it clear how general the method is. \n\nThe experiments show the method outperforms an independent learning and LOLA, but more extensive comparisons are needed. For example, the focus is on cooperative games. As such, methods that promote cooperation should also be discussed and compared to. This includes optimistic methods such as hysteresis and leniency as discussed in the paper below:\n\nWei, Ermo, and Sean Luke. \"Lenient learning in independent-learner stochastic cooperative games.\" The Journal of Machine Learning Research 17.1 (2016): 2914-2955.\n\nAlso, the results are a bit surprising. The status quo loss shouldn't favor one equilibrium over another so it isn't clear why the proposed method escapes the poor equilibrium for the good one (e.g., DD for CC). The paper should make it more clear what that is the case. Lastly, the results in Figure 4 are somewhat unfair since it appears that additional training was done by the proposed method for GameDistill before learning curve plot begins. \n\nThe paper is generally well written, but some details can be more clear as mentioned above. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting problem, but solution needs work",
            "review": "**Post-rebuttal update:** Thanks to the authors for engaging in the discussion and for the responses. The authors have provided a satisfying argument that with an appropriate choice of hyper parameters, the SQLoss does promote cooperative behaviors whenever all utilities are negative -- this addresses my main concern regarding the validity of the SQLoss objective. I remain skeptical of the value of the visual Coin game, because the results do not disentangle the usefulness of the cooperation objective and the clustering from GameDistill. I (and it seems, R1 and R3) had several concerns about the presentation of the material: mine in particular about lack of clarity, statements provided without motivation, and lack of details in Section 3, both for SQLoss and GameDistill. The authors have provided clarifications to some of these concerns in the response, but the new revision of the manuscript does not seem to reflect any of these changes. I would not be strongly opposed to acceptance, conditional on the visual coin game ablations and clarifications being added in the final version. Nevertheless, it is hard for me to recommend acceptance, given the number of unseen changes that still need to be made to the paper.  \n\n\n\n\n**Summary**: This paper studies the problem of learning cooperative behaviors in social dilemma problems for multi-agent deep RL with independent agents. The authors propose a simple loss that emphasizes the \"status quo\",  which increases the emphasis on the one-step reward, to force agents to avoid short-term exploitation. To extend this idea to a temporally-extended sequential setup, the authors introduce a clustering algorithm that reduces the game into a matrix game with \"cooperate\" and \"defect\" options.\n\n**Assessment:** The paper studies an interesting problem: without explicit communication between two agents, how to impose protocols that ensure convergence to cooperative / optimal behaviors. The paper is also generally well-written and quite easy to understand, which I appreciated. Nonetheless, the algorithmic contributions of the paper appear lacking and incomplete. The paper does not current convince me that the proposed solution SQLoss actually solves the cooperation problem (discussion below). The GameDistill algorithm is an interesting way to reduce general environments to matrix games, but seems highly instrumented for the current environments, and unlikely to work in more general environments (discussion below). Due to my concern and confusion about the effectivity of the algorithms proposed, I currently tend to reject the paper.\n\n**Major Concerns:** \n\n(SQLoss) From what I ascertained from SQLoss, it changes the policy gradient to re-weigh the reward at the current time-step higher than future rewards. The text does not make it clear to me why this should force / encourage cooperative behaviors. I do understand that SQLoss will \"cause an exploited agent (in DC) to ... quickly move to (DD)\", but what remains confusing to me is why SQLoss will force a system in (DD) to move to (CC). If the system originally starts in a place of mutual defection (or more generally, prefers mutual defection to mutual cooperation), how does the SQLoss incentivize either player to switch to a cooperative behavior? If each agent could reason about the other agent's learning procedure, then this behavior might emerge, but the PG update does not do this: it is conditional only on the other agent's current parameters, and not how it learns. With the PG update, even when the reward at the first time-step is emphasized, neither player is incentivized to increase the probability of cooperative behavior, since cooperative behavior has strictly lower future value. One experiment that might help resolve this confuion is to show that SQLoss also induces cooperation even when the initial agents prefer mutual defection.\n\n(GameDistill) The GameDistill algorithm is an interesting clustering method for turning a sequential game into a static game where strategies correspond to \"options\", but seems rather specific to the current tested environments. While the GameDistill algorithm handles discrete behaviors that \"cooperate\" or \"defect\" well, it doesn't seem to capture games where there are potentially multiple (and unknown number of) ways to cooperate or defect, or where there are graded levels of cooperation. A comment about how well GameDistill handles environment stochasticity / etc would also be helpful. Also, since it requires the practitioner to train several intermediary models (a trajectory encoder, a clustering model, an action predictor), it seems to me that the solution may be sensitive to hyperparameters, and thus not scale well to other environments. This final point is hard to judge in the current setup.\n\n(Experiments / GameDistill) It is unclear in the experimental section, whether the performance of the SQLearner agent is due to the GameDistill algorithm reducing the dimensionality of the problem to only choosing between two options, or if it is because the SQLoss actually promotes cooperativeness. Do the other baselines, e.g. Lola-PG and SL also use GameDistill?\n\n**Additional Comments:**\n\n- The derivation on Page 5 is incorrect: Equation (7) does not result from $\\nabla_\\theta \\mathbb{E}[\\hat{R}_0^1(\\tau_1)]$, since $\\hat{R}_0^1$ contains \"imagined\" trajectories only for the first time-step, and not for subsequent timesteps (that is, it does not contain any terms involving $\\hat{R}_t^1(\\tau_1)$ for $t > 0$). It is not immediately clear to me that there is a simple objective function for which (7) is the gradient. \n-  Why does the learning behavior of the SQLearner spike immediately to the optimal policy (after the initial period of being flat)?\n- The related work section appears to be comprehensive, but I am not well-versed with the multi-agent literature.\n- The code for running experiments is provided, which is a positive point for this paper.\n\n\n**Minor Comments:**\n\n- \"Each agent independently attempts to maximize its expected discounted return\": For this sentence in Section 2.1, what does the agent assume about the other agents? That the other agents are fixed? That the other agents can act adversarially?\n- The word loss is used throughout for referring to both \"a loss function\", and a \"bad reward\" -- this makes the discussion in places confusing\n- Use the math operator $\\log$ instead of $log$",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}