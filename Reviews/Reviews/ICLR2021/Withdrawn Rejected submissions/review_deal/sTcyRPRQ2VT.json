{
    "Decision": "",
    "Reviews": [
        {
            "title": "An attempt to build the schema of KB under a very constrained situation, and thus with a very limited contribution to the community",
            "review": "This work focuses on an interesting and important problem of automatic discovery schema for building KBs over a specific problem domain. However, it assumes that the entity types and the relation types are already known, so it only does the work of associate them to build a schema (called meta-graph ).  The assumption is so strong that it greatly limits the value of the work -- linking the entity types with relation types only needs very few works (maybe one-hour or less) even for a non-professional person in that field. \n\nThe technique used in this work is transformer-based sequence-to-sequence learning, and then assemble the predicted triplets into a graph. It does not employ any problem-specific information such as meta-graph-oriented constraints on the learning procedure, so does not provide extra knowledge to the community. \n\nThe interesting part is to use the learned meta-graph to build the knowledge base by detecting entities through the attention weights. However, this part is briefed in the experiments and is not detailed as a formal problem. \n\nThe writing of the paper is no so clear. The background information and methodological discussion are mixed together. Some statements in the Experiment are not easy to understand. \n\nOverall, I believe the work does not focus on the really challenging and essential task for knowledge-base construction, that is, identify the types of entities and relations, or automatically build a knowledge base given a schema. \n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Summary\n\nThe authors propose a KG construction approach using Transformer-based models that construct the knowledge graph and the associated \"meta-graph\". From input text, the meta-graph is first constructed, which then helps in KG generation by reducing manual human processes. \n\n# Positives\n\n- The paper explores an important and active research area: the automatic generation of knowledge graphs from unstructured text sources. \n\n- The paper uses the Transformer architecture, which has advanced the state-of-the-art in many areas since its introduction, on various experimental setups in this paper. \n\n- The paper as a whole is structured clearly and in a straight-forward manner. There are some typos and word choice issues (listed below).\n\n# Concerns\n\n- The paper focuses on the \"domain-specific\" aspect of KG generation, laying out the benefits of a domain-specific approach (reducing irrelevant data, performance reasons). However, I think the definition of a \"domain\" is not clear here. For instance, this part of the paper is worded confusingly: \"The domain definition , here defined as a metagraph having entity types as nodes and relation types as edges, is usually performed by subject matter experts.\". Wouldn't \"domain-specific\" refer to a topic-matter domain such as the bio-medical domain, legal domain, technology domain etc.? It seems like domain-specific is being used to refer to smaller subsets of generic texts/KGs. It would be good to see further explanation on how framing meta-graph/KG generation problem as domain-specific adds value in the context of the WebNLG, NYT, RedDOC datasets and also when the introductory examples are general (London, person, etc.). \n\n- Experimental results are detailed for the meta-graph generation, which forms the first step of the \"two-step approach\". However, there are fewer detailed results for the second step of using NER and RE in this paper. It may be good to re-word the paper's abstract in light of this main focus.\n\n- The concept of a meta-graph is defined in the paper: \"A domain’s meta-graph is a graph that has as vertices all the entity types and as edges all their connections/relations in the context of this domain\" (pg. 2). This meta-graph is also commonly called the ontology of a knowledge graph. As \"ontologies\" are mentioned in the Related Works section and said to use rules-based approaches, it would be good to see further justification of the term that distinguishes it from how \"ontology\" is used. The meta-graph in Figure 1 could very well be labeled an ontology. \n\n- From reading the Appendix, a concern is that transformations reduced the entity types (e.g. city, county, area -> location) and relations (e.g \"place of birth\", \"place of x...\", etc. -> location.related_to.person) when there were too few instances. It would be good to know the distribution of the relation types on this modified version of the dataset (similar to the NYT and DocRED descriptions) as it is the large variety and diversity of relations and entity types is important for KGs. \n\n- Comparing figure 1 and figure 3, it would be good to see further details and descriptions for figure 3. The color-coding is done well, but leaves out details such as the precise relations between nodes, directionality (if any), and a sample of the text snippet.\n\n\n# Typos or word choice issues\n\n- as they shares -> as they share\n- suits better to -> better suits a \n- ineherently -> inherently \n- domain definition , here -> domain definition, here\n- (in Table 1 caption) EntityPairOveral -> Overlap; SingleEntityOveralp -> Overlap\n- (pg. 3, the LaTeX formatting may be a bit off) “i.R.j”\n- (word choice) ... any defined ordering as per definition the edges ...\n- underling -> underlying \n- aims at generate -> aims to generate\n- describing in -> described in \n- does not assessed -> is not assessed\n- avoid to include -> avoid including\n- gives as localized structure information as (?)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Issues with the usefulness of this task",
            "review": "\nSummary:\nThe authors address knowledge graph extraction from a sub-domain of data. To generate this graph, they extract relation types of the form i.R.j from sentences, where i, and j are entity classes and R is the relation type. Given a sub-domain data, they then construct the sub-domain specific knowledge graph from extracted i.R.j tuples by using the entity classes as nodes, and relations as edges. To extract i.R.j from a sentence, they took a seq2seq approach similar to [1], which they referenced for defining the three possible types of relations, i.e. Normal, EntityPairOverlap and SingleEntityOverlap. \n\nStrengths:\n- The paper differentiated themselves from previous work on relation extraction by extracting only i.R.j: this simplifies the problem as there is no need to extract the entity names correctly. They compared against RNN and CNN baselines. As there could be multiple relations in a sentence, the paper improved performance by augmenting the data with re-ordering of the relation tuples in the output sequence. They also improved performance with an ensemble of seq2seq transformers instead of a single one.\n\n\nWeaknesses:\n- Since all the relations have been pre-defined in the training data, it should be simple to extract the subgraph from the full knowledge graph that contains only relations extraction from the sub-domain data. The \"automated domain understanding\" claimed in the title might be a bit misleading, as the full graph is actually manually pre-defined by the relation types in the training data.\n\n- The authors should compare with recent work on relation extraction: while the authors argue that there is no previous work that addresses the problem they defined (i.e. extracting i.R.j), any previous work that jointly extracts entites and relations (e.g., [1] and [2]) can be a baseline by simply extracting the full relation instance and then simplifying it to i.R.j.\n\n\n\n[1] Zeng, Xiangrong, et al. \"Extracting relational facts by an end-to-end neural model with copy mechanism.\" Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018.\n\n[2] Fu, Tsu-Jui, Peng-Hsuan Li, and Wei-Yun Ma. \"GraphRel: Modeling text as relational graphs for joint entity and relation extraction.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}