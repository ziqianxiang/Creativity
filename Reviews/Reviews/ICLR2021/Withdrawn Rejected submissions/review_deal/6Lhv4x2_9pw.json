{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper considers an interesting application of Bayesian neural nets to the geophysics domain;  however, the paper does not make a novel contribution from the machine learning perspective, and the improvements on top of the previously proposed approach by Ahamed & Daub (2019) seem to be quite modest. Overall, the paper does not seem to be ready for publication at ICLR.\n "
    },
    "Reviews": [
        {
            "title": "Review on the usage of BNNs to gain insights on the physical procedure of earthquake ruptures through the study of the parameters",
            "review": "### Summary\nIn the present paper, the author intends to get further insights into the physics behind earthquake ruptures using a BNN to model simulated data from the literature. By using a BNN, the parameters of the model are not deterministic scalar values, but complete probability distributions. Studying the change of the distributions in the parameters before and after training, the author tries to extract information about the relative importance of the input variables, and also comprehend the physical mechanisms behind earthquake ruptures. Results are shown in figure 3, on which the change of behavior of the distributions of the parameters can be observed, as well as in figure 4 where the mean and standard deviations for all the parameters are presented. The pattern in figure 6 seems to indicate that variables previously thought to be important in the task of predicting the presence of the rupture, such as normal stress and friction, are also pointed out as being important in this case.  Finally, the authors also claim an improvement in the F1 metric in comparison to previous NN methods. \n\n##### Pros\n* The idea of the paper seems well directed, i.e., gaining insight on complex physical procedures using an approach that results in the combination of NNs and a Bayesian approach. \n* Using a Bayesian approach is a good way of dealing with small datasets, and also allows to account for the uncertainty of all the latent parameters, while also providing more robust and sensible predictions when new data is presented\n* The approach seems to provide results consistent with the literature findings regarding the important variables in the prediction\n* The final performance of the algorithm seems to improve on the previous state-of-the-art methods by taking advantage of the properties that the Bayesian approach offers\n\n##### Cons\n* The key concern with this paper is that NNs, as well as BNNs, are notoriously black-box algorithms with no easy way of interpreting the inner parameters in most cases. Taking this into consideration, I would suggest the author to motivate in a stronger manner why the usage of BNNs is desirable for the proposed problem, and why not use other already established Bayesian approaches to assess the importance of the input variables. \n* Taking into account the previous point, I consider there is a general lack of rigorous experiments that could, in principle, suggest a clear advantage of using BNNs instead of any other approaches. No systematic comparisons with previous methods are present, such as for example with the Random Forest Feature Importance algorithm, which is mentioned a couple of times. If the main goal is to gain insights on the main variables involved in the presence of an earthquake rupture, I would expect a more detailed analysis comparing how good these insights provided by BNNs are and how do they stand in comparison with the established literature. \n* Other basic techniques for assessing the importance of the variables in the prediction tasks are not mentioned, although it would be nice to use them as a baseline to compare against. Examples such as PCA, LOO cross-validation and others could be used here. \n* The claim of an improvement of 2.34% w.r.t. NNs is not strongly addressed, since the NN experiments are not included here or, at least, there is no mention of the setup of these NNs. As before, there is no systematic comparison between the BNNs trained and the NNs that are used as baselines. \n* There is a lengthy discussion on how to obtain the ELBO for VI. However, in the end, there is no final expression for the loss function which is going to be employed. I would appreciate in section 4 an explicit description of the objective of the system since there's no mention of the final binary classification problem anywhere.\n* The prediction uncertainties lack a systematic evaluation as well since all that is provided is presented in figure 5. How well do the predictions provided stand against other methods for obtaining final predictive distributions? \n* VI is a method whose performance and final predictions are constrained due to its formulation. Is there any reason why using VI instead of any other approach to BNNs? In case that we wanted to study the final predictive distributions, why not use HMC or other, more flexible approaches than VI? \n* At the end of the section 5.1, first paragraph, it is claimed that \"positive and high magnitude weights contribute to the earthquake rupture and vice versa\". This sentence seems a bit confusing since it seems to imply a causal relation between the high magnitude of the weights and the appearing of ruptures. This, I think, is the other way around: very clear rupture conditions imply positive high magnitude weights, which, in turn, return a higher predicted probability of rupture. \n\n##### Minor comments:\n* Even though the paper tackles physical phenomena such as earthquake rupture, it does not provide any description of such process or the variables involved. Concepts such as \"nucleation\" and \"fault barrier\" should be at least briefly introduced, as well as the \"slip weakening law\" or the \"critical slip distance\". A short description of these terms and their relevance to the problem would help to interpret the final results obtained. Also, explicit expressions for the rupture physics would help a lot in section 2 to understand the different roles of the variables and their relations. \n* Throughout the whole text it is used the first person while writing. In case there is only one author this can be okay, I only point it out since it seems to be a uncommon choice.\n* There are a lot of typos all through the paper! Please perform a careful reading and correct them.\n* The description on figure 4 is confusing, does not seem to correspond to the presented images (either that or the text is unclear when selecting the important parts of the figures for the nodes mentioned). \n* 4th paragraph of introduction - not all ML algorithms are black boxes! NN are, but other such as linear regression, decision trees, etc., can be very interpretable!\n* 5th paragraph of introduction - \"exciting\" - avoid usage of these type of subjective adjectives all through the paper.\n* 5th paragraph of introduction - BNNs may work better with fewer data, but we have to pay close attention to the prior formulation to not introduce unreasonable biases. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Minor improvements from ANN to BNNs - application new but additional insight of Bayesian methods not clearly provided",
            "review": "\nThis paper extends on previous works by Ahamed & Daub (2019), from a two-layers MLP, two a Bayesian NN version of it, for predictiong wether a piece of material will rupture, under some conditions.\nAlthough this increase in model complexity improves performance a little bit, it does not represent a major advance.\nThe main point of the paper is to show that Bayesian NN allow to get not only a prediciton, but also naturally provide uncertainties on these predictions.\nAlthough the paper clearly explains the basics of BNNs, it does not provide any new insight into them. The application to rupture physics is interesting, but does not seem groundbreaking.\n\nFor these reasons, I lean on **rejecting the paper.**\n\nAlso, given the github repo referred to, there is a breach of anonymity.\n\nOn the paper itself, I have a couple of remarks:\n- it is unclear as to where the data comes from. A simulation is mentionned, but not how it works. I see in Ahamed & Daub (2019) that it is a finite element simulation.\n- still about the simulation, it is uncelar whether the stress state is heterogeneous or not. It seems it ought to be, however in that case, the description of the stress state would consist in a full field of values, and not just a couple of numerical values.\n- the discussions of the uncertainty in various input variable is tedious and does not really highligh how BNNs help to get transparent interpretations.\n- Fig 5b: there is a clear sqrt(x)sqrt(1-x) shape of this curve. Do you have an epxlanation for that (and can you check the fit?)\n- the conclusion repeats some parts that were stated rearlier. It should instead focus on how BNNs help understand the physics. The physics of rupture itself is not very interesting to a ML audience.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper study an interesting topic and application of machine learning in Geophysics. However, the paper do not provide any novel contribution related to machine learning.",
            "review": "The manuscript do not provide enough details about  the physics about which the BNN provides insight. \nThe manuscript employs the well developed machine learning algorithms in an application in Geoscience and do not provide a novel learning algorithm or contribute to machine learning topics. This paper does not meet the  ICLR standards and therefore I can not recommend for publication.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting subject and paper but may not be enough for ICLR",
            "review": "This paper proposes a Bayesian neural network for predicting if an earthquake will break a fault or not, overcoming 'small data problem' and predicting model uncertainty. The data is composed of 8 features and a binary output, and the samples are all coming from simulations. An analysis on the means and standard deviations of the first and last layer of the neural network's weights has been carried out.\n\nThe problem is interesting, and the method is useful as the study on the weights means and stds is interesting. Yet, I think the paper is not well polished (many incoherence in the text and the figures) and I don't really understand why a synthetic dataset, coming from physical model equations, needs to go into this complex uncertainty quantification: we are making here a 'meta-model' of something that is fully described by equation, so why using machine learning? I also don't understand why the author talks about a 'small data problem', as here we could simply increase the number of simulated samples? Yet, I can see the interest of such a technique if the goal was to later try to apply it to real data, where some physics might be unknown or too complex. I also don't know if the findings of the paper are of interest for the ICLR community, but as I come from an interdisciplinary field too, I know how hard it can be to find an appropriate and yet good place to publish.\n \nOther questions/Remarks:\n - Equ. 8 : I don't see how we can go from Eq. 6 to Eq. 8? this part is not clear.\n - Please cite the reference papers for the Bayesian NN as well as for ELBO. There is clearly a part missing in the state-of-the-art regarding this, while the equations (such as the long and useless (5) one) are just explaining what is already known in this literature.\n - Figure 4: 'Shear stress connected to node-4 of the hidden layer has the highest uncertainty. Similarly,\nthe weights associated with the input parameters and node-5 have high uncertainty. Whereas, the\nweights associated with the input parameters and the nodes 7-11 have relatively low uncertainty. (d)\nUncertainty of the weights associated with the hidden layer nodes and the output node. Weights in\nnode 7 and 8 have high uncertainty while the rest of the weights have relatively low uncertainty.'\n\t--> please update the text or the figures so the numbers match. Same thing for the text. For now, I cannot understand anything.\n- Figure 1: Tau_s decreases linearly: not, tau_s is fixed, maybe you meant tau or 'the shear stress'\n\ntypos:\n- 'perhaps widely studied and applied in many situations' --> 'perhaps'? never seen this word in a paper...\n- ar new distribution\n- is therefore, derived by\n- which is approximate 7% less\n- data is limited data\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}