{
    "Decision": "",
    "Reviews": [
        {
            "title": "Minor contribution",
            "review": "Hi,\n\nFirst I'd like to thank authors for putting the manuscript together. \n\n*Summary* Authors suggested using Bayesian Regression (similar to (Azizzadenesheli et al., 2018)), but with using informative priors derived from previous examples. They used PAC-Bayes theory to prove generalization bounds and tested their method in some experimental setting.\n\nAfter reading the paper, and comparing to (Azizzadenesheli et al., 2018), I believe the contribution with respect to BDQN is minimal. I wonder what authors thinks especially about the comparison with BDQN and if they can elaborate more about the distinction between the two works? To me, the main contribution is using informative priors, which is minimal, and the benefit is not clear.\n\n*Experiments* I believe paper can substantially benefit from more extensive experiments, on larger state space especially. As of know, it's hard to assess the benefit of the proposed method versus BDQN, and the benefit is not clear.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Useful extension of BDQN, but significance is questionable",
            "review": "Originality: Authors propose two improvements to Bayesian DQN. The mean of the prior distribution is maintained and updated across batches, and the noise parameter $\\beta$ is periodically updated with Empirical Bayes (maximum marginal likelihood). These modifications are intuitively sensible, but they also seem somewhat simple extensions of the existing algorithm, and it is unclear to me whether they justify a full conference paper. \n\nQuality: The update of the prior distribution looks largely heuristic, as authors are not really using Bayes rule for updating the prior and override the variance as $\\sigma^2 I$. This heuristic choice is certainly sensible, but a deeper discussion on why the posterior distribution cannot be used would be interesting; maybe there is a more principled way of dealing with slow change of $\\pi$, for example by reflecting that into high $\\beta$ values? Another contribution of the paper is the PAC-Bayesian generalization bound, but its connection to the rest of the work seem weak to me. After all, (12) only provides the intuition that the posterior distribution has to be regularized, and does not play any role in the derivation of the $\\beta$ update rule (16).\n\nSignificance: This paper may bring attention to the problem of specifying the prior distribution for BDQN by leveraging information across episodes. The problem itself seems broadly applicable and practically useful.\n\nClarity: The paper is easy to follow. In particular, explanations of each term in the equation were very insightful.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review #1",
            "review": "## Summary\n\nThis paper proposes a Bayesian-last-layer DQN with an adaptive noise parameter determined by optimizing a PAC-Bayes bound. The proposed algorithm outperforms Bayesian DQN and NoisyNet DQN.\n\nThe reason for my score is that I find the writing rather confusing, especially the motivation is not clearly explained. I also find the evaluations a bit weak. \n\n## Questions and Concerns\n\n* The informative prior reuses the posterior mean from the last iteration, but replaces the covariance with a non-informative diagonal one. This is strange as it deviates from the practice in online Bayes inference. It is argued that keeping the posterior covariance leads to the algorithm quickly losing uncertainty, because \"the training set may change slowly\". But isn't it desirable that the value function be certain in the well-explored region in trianing set, and uncertain only at unseen input?\n\n* Thm.2 seems strange to me because, contrary to standard PAC-Bayes bounds, we can't expect the third term in the bound to be small in training, even as sample size goes to infinity: it involves the difference between the current $q$ and the optimal one. While this term does not depend on $P_q$, it may still make the bound vacuous. In that case, minimization of this bound will not provide much theoretical guarantee that $\\|E_P q - q^*\\|$ may improve. \n\n* The experiments only compare with two methods on the Gym control tasks. How does the proposed method compare with the distributional DQN variants? How does it perform on e.g. Atari games?\n\n* In Section 4 it is claimed that VI for approximating the Gibbs posterior is expensive, and thus you choose to only optimize $\\beta$. However, in Algorithm 2 you are updating the distribution parameters $(m,S)$ using Eq.(7), which has the same cost as optimizing Eq.(15). Moreover, the optimal $(m,S)$ for Eq.(15) have a similar form to Eq.(7), so can you formulate Eq.(7) as part of the optimization process for the PAC-Bayes bound?\n\n### Minor Comments\n\n* Fig 1(a) seems wrong, as it doesn't show the \"huge uncertainty\" as stated in the text. \n\n* The equation after Eq.(18) is incomplete.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Confusing algorithm...",
            "review": "The paper proposes to use Bayesian linear regression with an adaptive prior to estimate the distribution of Q-functions. The paper derives a generalization error bound for the training algorithm, which motivates adapting the noise parameter during training. The paper then shows that their algorithm works well compared to baselines that do not use an informative prior on some classical control tasks, and claims that the performance improvement mainly comes from more accurate approximation of value functions and better generalization. \n\nI find that the training algorithm is presented in a very confusing manner. It starts by citing RLSVI from [1] by Osband et al., but the setting the paper considers is very different from [1]. Perhaps the authors were referring to a different paper?\n\nThe authors assume a linear model for the Q-function and try to infer the weights by doing Bayesian linear regression, but the algorithm is never stated in the main text, and the one presented in the appendix is confusing. In particular, inference (6) requires target values y which are not well-defined. The definition (4) does not make sense to me because w is random. \n\nIf the main intention of this paper is to design a training algorithm that can adapt the distribution of value functions to evolving data, then it seems that [1] already provides several possible ways to do so as well as a few other papers. It is unclear to me what is the main problem that the paper aims to tackle. \n\nAlso, there’s no discussion of how actions are selected and how features are updated, and I have doubts for both of them. According to lines 7 and 8 in Algorithm 1, a random value function is sampled at each time step and a greedy action is taken with respect to the sampled value function. If the agent samples a random value function at each time step, there is no consistency in value estimates across time, and the actions are not going to be consistent across time steps within an episode. In the language of [1], the agent cannot perform deep exploration. \n\nGiven that I find the proposed algorithm extremely confusing and the significance of the paper unclear, I recommend rejection. \n\n[1] Ian Osband, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. Deep exploration via randomized value functions. Journal of Machine Learning Research, 20(124):1–62, 2019. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper but more experiments are warranted.",
            "review": "The paper proposes a Bayesian linear regression with informative prior operator that leverages data-dependent prior in learning randomized value functions. It derives a generalization bound based on PAC-Bayesian theory. In addition, they develop an adaptive noise parameter update algorithm that modifies the noise parameter to balance the prior and posterior distributions. The experimental results demonstrate better efficiency through a jump start and better effectiveness through a better asymptote.\n\nI quite like the paper. I found the idea of using informative priors and BLR to learn randomized value functions quite intriguing. The paper appears to be theoretically sound. As far as I could read, I could not spot any glaring errors. Granted that the theory is heavily based on the prior work pf Alquier et al but that does not diminish the contributions of the current paper. The elegant reduction to the DBLR paper is also quite good. \n\nI thank the authors for Figure 1. This was a good motivation for me. If there is a journal version of the paper, I suggest moving this to the introduction. This will make the motivation clearer and the readers can clearly see the benefit of the approach.\n\n\nWhile I really liked the paper, I have a few minor questions and comments:\n\nI would have certainly liked to see more experimental domains. Nowadays, for RL algorithms, we are used to seeing several domains and more complex domains would have been better illustrative of the work.\n\nI wanted to understand the types of domains in which this work will be best used.From that perspective just using 3 domains was a bit disappointing. I suggest adding at least a couple more domains but more importantly explaining where the impact would be.\n\nI liked Figure 4 as well since it clearly shows the effect of the adaptive parameter update. This type of an ablation study is quite interesting and I would have liked to see the different types of ablation studies could be designed for this task.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}