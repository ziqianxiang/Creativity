{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper improves memSwap with a window-based scheduling and alleviates the fragmentation issue a bit",
            "review": "#### Summary\nThis paper proposes to improve the memSwap (a very well-known technique for memory management in GPU-based DL) with two techniques: \n- using window-based scheduling to swap-in or swap-out in advance so as to best hide the swap overheads  \n- manage the memory allocation plan to reduce fragmentation. The empirical studies verifies the two techniques on a  few vision-based models such as ResNet-50, DeepLabv3+.\n\n#### Technical contents\n\nThis paper misses two important series of related work that optimizes DL gpu memory:\n- first, there already exists a bunch of paper doing memory swap, which I attached below:\n[1] GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server, Eurosys'16\n[2] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design, Micro'16\n[3] Training deeper models by GPU memory optimization on TensorFlow\n[4] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping\nThe techniques presented in this paper is very similar to existing techniques in papers above\n\n- second, there exists a second line of work doing rematerialization for saving training memory, among which I only enumerate a few below:\n[5] Training Deep Nets with Sublinear Memory Cost\n[6] Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization\n[7]  Memory-efficient Backpropagation Through Time\n[8]  Cutting Down Training Memory by Re-fowarding. \nI guess the authors at least should discuss your pros and cons and relations with the rematerialization-based work, and conduct some experiments to show the advantage of this work over them.\n\nI am also very concerned with the novelty in this paper. Reasons are elaborated below:\n- creating window-based swap schedules to hide swap overhead has appeared (in a slightly different form) in many other memswap work, such as [1][2][4]. In [1], a virtual iteration is performed to record the computational dependency and the contents are swap-in andout in advance, based on the dependency, which is exactly equivalent to window-based scheduling.\nin SwapAdvisor [4], a much more sophisticated schedule was generated by co-optimizing tensor partitioning and swap schedule. In this case, a window-based schedule is just a special case of it.\nGiven these related work, I feel this paper just presents existing techniques in a slightly different way, with limited new insight.\n\n- avoid fragmentation\nThe fragmentation problem is also addressed by SwapAdivosr, by letting the memory management system to control the tensor allocation. See section 4.1 and 5.3 of SwapAdvisor paper. The high-level idea is almost the same  -- changing the best-fit strategy to something smarter. Whereas this paper uses a virtual-addressing which might sounds appealing in terms of implementation, SwapAvisor has a much more advanced algorithm based on GA to avoid fragmentation.\n\n#### Experiments\nThis experiment results presented in this paper is not very appealing to me as well, for two reasons:\n- the models experimented in this paper is less interesting. I don't think ResNet-50 is worth attention for memory management work as it is a very small and thin neural network. I suggest the authors pay attention to more challenging models whereas memory is REALLY an issue (please find a solid use case, e.g., larger models, and larger batch size is really needed to train these models). All the models experimented with are vision based -- essentially CNN-based architecture, with a sequential architecture.  To me, MemSwap for sequential structures is very well-explored , please consider looking into more complicated structures such as RNNs and transformers (where memory is in fact more an issue than on CNNs in my opinion).\n\n- the baseline used in this paper is very weak, which makes it hard to judge whether the overall system presented in this paper is good or not.\n(1) For a fair comparison, I think , at least, the author should at least compare to one of SuperNeuron, SwapAdvisor, os ODSwap on CNNs, and justify the advantage of this work over them, or the author needs to show the advantage of this work on other NN architectures where the existing memswap work cannot work well.\n(2) It is also highly recommended, to explicitly compare your technique with other classes of techniques, such as the line of rematerialization work, or even simpler ones such as gradient accumulation, to show your advantage; otherwise, it is very hard to see how many benefits this work can really bring.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The contribution of the paper seems to be incremental and the benefits of the solution need to be better justified",
            "review": "The paper presents a heuristic algorithm to decide when to swap in and out tensors at out-of-core training.  It also applies virtual addressing technique to reduce memory fragmentation.  The results show that higher batch size can be supported, comparing with prior work, as a result of reduced GPU device memory footprint and data transfer between CPU and GPU memory.\n\nMerits of the paper:\n- Given the limited GPU memory comparing with CPU, efficient out-of-core training is an important subject.\n- Good observation on memory fragmentation problem and effective solution.\n- The paper is clearly written and easy to follow.\n\nPlaces to improve:\n- The main idea of the paper seems to be quite incremental comparing with Le et al. (2019).  The proposed scheduling algorithm considers more factors, which is reasonable.  But it does not provide much novel insights.  The paper points out two issues of Le's work, while the second issue - require two hyperparameters to balance a trade-off between memory transfer overhead and GPU memory usage - does not seem to be problem.  These are system configurations which can be rather easily explored and tested out at the first few iterations of the training.  Given that many training jobs require thousands and millions of iterations, such a tuning cost does not seem to be major.\n\n- The evaluation results only show the increased batch size as the benefit.  Would this approach also allow running bigger models with more parameters which otherwise would run out of memory (even with batch size of 1)?  That is another important reason to have out-of-core training.  It is probably even more important than the scenario of improving batch size because such a large model otherwise cannot run at all using a single GPU.\n\n- The evaluation results in table 1 shows that improved memory footprint allows larger batch sizes, but the overall throughput is lower with larger batch size, and the baseline provides the highest training throughput.  In this case, what is the benefit of supporting larger batch size?  Would the ultimate goal be improving throughput?  Otherwise, we can use techniques like gradient accumulation to support larger minibatch size. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official blind review #2",
            "review": "##########################################################################\n\nSummary:\n\nTraining a deep neural network model requires a lot of memory -- both for larger models, as well as enabling larger batch sizes.  GPU memory is limited, and offloading data to the much larger host (CPU) memory is a compelling solution.  Past approaches, however, suffer from inefficient off-/on-load scheduling and, over time, memory fragmentation.  This work presents a novel scheduling algorithm and uses standard memory virtualization to control fragmentation, resulting in much larger usable batch sizes on a given physical memory budget than past work.\n\n##########################################################################\n\nReasons for score: \n\nWhile the results *seem* compelling, it's hard to draw meaningful conclusions from the empirical study as presented.  Without a theoretical backing, an empirical study really needs to be done on the same system; too much has changed between the baseline's environment and the submission's environment to be convinced the results show something important.  Perhaps a slightly worse scheduler could still have less of a penalty, just because bottlenecks have shifted.\n\nEven if we take the results at face value, the importance of these benefits are not clear:\n- If the claims of \"larger models\" (see below) are to stand, results should be shown enabling larger models (not just larger batch sizes).\n- Only one workload shown sees improved performance with the proposed techniques; otherwise the baseline \"no-change\" configuration is able to give the highest training throughput.  (Highlighting the benefit of larger batch sizes for workloads that can only afford very low batch sizes with today's memory would make a much stronger paper.)\n\n##########################################################################\n\nPros:\n\n+ Virtualizing memory to control fragmentation is straightforward.\n\n+ Reducing the complexity of the scheduling algorithm to a single assumption (computation and memory costs are both proportional to data size) is enticing.\n\n+ Similarly, the single hyperparameter for the scheduling algorithm isn't too onerous for users.\n\n##########################################################################\n\nCons:\n\n- The assertion that faster math on V100 compared to P100 creates a more challenging environment isn't obviously true to me.  Surely changes in memory bandwidth will also play a role - since different layers have different bottlenecks, stealing GPU memory bandwidth to swap data in and out could affect performance of layers that aren't math-limited.\n\n- For the above reason, the indirect comparison of the proposed method against a prior baseline on different hardware makes it hard to draw a comparison or argue about the relative goodness of the proposed scheduler.  For example, the host<->GPU bandwidth differs, on-chip compute and bandwidth differ, GPU memory capacity differs, etc.  Showing a relative training speed of 68% vs. 53% at a larger batch size between the two methods may be an artifact of the different environments, rather than the scheduling algorithm's superiority.\n\n- Several times in the paper (at least the abstract, introduction, section 5, and conclusion), it is suggested that there are results showing larger \"model sizes,\" but this is consistently \"just\" the batch size.  As Table 1 indicates, there's no benefit to larger batch sizes in some cases (training throughput in images per second is maximized *without* any of the proposed mechanisms).  Clearly, this isn't the case for every workload (notably when the available batch sizes do not fill the machine, as in Figure 4.b), and there's still a benefit to allowing larger models, but these practical benefits aren't highlighted.\n\n\n##########################################################################\n\nQuestions:\n\n- Training all these networks and tasks is entirely possible with fp16 math, which is roughly an order of magnitude faster than single-precision math on the V100 used.  Intuitively, swapping data in and out will be both harder (since there's less time to do it) and more painful (since workloads are more likely to be bandwidth limited), but do you have any other thoughts or data on this?\n\n- How close to optimal is this scheduling algorithm?  VDNN (referenced in the paper) compared themselves to an oracle scheduler to serve as an upper bound.  Showing a meaningful improvement over naive updates to VDNN to support residual connections (it seems fairly straightforward to treat the generation and eventual use of residuals as a delay between the forward and backward path) may give some idea of the relative goodness of the proposed scheduler.\n\n##########################################################################\n\nMinor suggestions/typos:\n\nI suggest having the paper edited; there are a number of typos/missing words on the first page alone:\n- Abstract: \"... demonstrate higher performance ...\" --> \"demonstrate high performance\"\n- Abstract: \"... we apply virtual addressing technique ...\" -->\n- Abstract: \"... commonly performed in OS, ...\" --> \"commonly performed in an OS\"\n- Intro paragraph 1: \"..., making model larger ...\" --> \"making a model larger\"\n\nFigure 4's caption indicates it shows training time (lower is better), but it is actually the training speed (higher is better).  This also should be fixed in the text in section 5.\nFor at least the initial results figures/tables, please include network/data set information in the captions.\nPlease make the distinction between larger models and larger batch sizes throughout the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some interesting techniques for out-of-core training but the claims are confused and the hyper-parameters introduced require more attention. ",
            "review": "This paper proposes two techniques for improving the performance of out-of-core training of neural networks. Firstly, they propose an algorithm for scheduling when to move data in and out of GPU memory. Secondly, they propose a virtual addressing scheme to reduce memory fragmentation. Experimentally, they show that by using these techniques, given a fixed GPU memory capacity, one can train using a larger batch size than existing solutions.\n\nPros:\n- The scheduling algorithm seems to be effective at enabling larger batch sizes and is based on a relatively simple heuristic (that the computation and data transfer time are proportional to the data size).\n- The paper shows that the virtual addressing also enables scaling to even larger batch sizes.\n\nCons:\n- The authors argue that the experimental results show that the techniques enable scaling to larger model sizes. However, if we agree that model size refers to the number of parameters of the neural network, then the results do not show this.\n- The paper lacks discussion regarding the hyper-parameters of the proposed techniques.\n\nMy primary concern with the paper is the claim that their scheme enables *larger models*. However, the paper does not provide any experimental or theoretical justification for this. In the experimental section, the authors frequently mention that the scheme enables training a larger model, but in fact what they are showing is that the scheme enables training with a larger *batch size*. The batch size is a hyper-parameter of the training algorithm and has nothing to do with the model size! Furthermore, it is not even clear if increasing the batch size is something that brings any benefit. While clearly using a larger batch size enables you to process more examples per second during training (the evaluation metric used in the paper), it often leads to slower convergence. For this reason, it is common practice to use time-to-accuracy as an evaluation metric, to show that moving to a large batch size truly leads to faster training. Recent works have developed new tricks for ensuring fast convergence with large batch sizes (e.g. Layer-wise Adaptive Rate Scaling), and it may well be that when used in conjunction with the schemes proposed in this paper, faster training is possible. However, I do not believe this paper can be accepted until this is proven. \n\nA secondary concern is the lack of discussion of the various hyper-parameters of the proposed scheme. To the best of my understanding, there are three important hyper-parameters. \n1.\tThe schedule window for the adaptive memory swapping scheme.\n2.\tThe maximal memory budget (e.g. from Figure 3).\n3.\tThe size of the physical memory chunk used in virtual addressing (set to 40MB). \n\nThe paper could be improved by including an ablation study or, at a minimum, some discussion of how these hyper-parameter were set and/or tuned experimentally. Furthermore, with respect to the maximal memory budget, it seems that, even when using the virtual addressing, the scheme uses more memory that the maximum memory budget (e.g. Figure 3). Therefore, it appears that the end-user would need to set this hyper-parameter by gradually incrementing it until out-of-memory errors are obtained? In Section 2, this was cited as one of the flaws of the existing work (Le et al. 2019), but the proposed scheme seems to suffer from similar problems. \n\nAdditional comments and suggestions:\n- Figure 2 is very small and I am unable to read the text when printing off the paper, this made it difficult to understand and appreciate the contribution of Section 3.\n- The text is Section 3 is hard to parse, perhaps the algorithm would be easier to explain by using an Algorithm float?\n- In Section 2, the training process is described as executing a sequence of functions in order. How does this account for the backward pass required during training of DNNs? \n- What is the ‘baseline’ in the experiment section? It is not defined. \n- In the mentioned a few times in the paper that 'memory is being transferred', which is a little confusing since I believe it is the *data* that gets transferred from one memory to another. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}