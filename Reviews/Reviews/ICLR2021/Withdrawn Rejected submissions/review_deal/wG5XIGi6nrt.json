{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper focuses on a notion of privacy in learning representations. \n\nOne of the primary concerns of the reviewers was clarity of the writing and results. Numerous concerns are mentioned in the reviews, and also more engagement with the fairness literature was desired. One reviewer felt that some of the claims in the paper were unsubstantiated, for example: understanding the sanitization process in a human-understandable visual way\", \"integration of a notion of interpretability\". It was felt that the changes required were more than could be expected for a camera ready version. The authors are recommended to revise the paper with a particular eye for clarity to a new reader.\n\nThe notion and measurement of privacy was also considered to be somewhat shaky. It is understood that the nature of privacy considered in this paper is different from differential privacy. That said, the latter is a rigorous definition, and the one in this paper seems to be rather empirical in nature. There are no formal guarantees in terms of privacy preservation, and it is not clear whether the representations could leak information when evaluated with a different network. As privacy is a mission-critical property, some justification of why the heuristic measurement of privacy is acceptable.\n\nAs a side note, the authors should consider using the \\citep command for parenthetical citations in the text."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "########################################################################## Summary: The paper studies how to learn private representations that only captures the non-sensitive attributes of the dataset. They propose an adversarial representation learning method that employs VAEs. Specifically, the architecture in the VAE contains 6 players with 2 as adversarial classifiers. They introduce focal entropy as the objective function instead of entropy for adversarial classifiers to achieve deep sanitization. They empirically evaluate the method by reporting the target task accuracy and attribute inference accuracy on two datasets.\n\n########################################################################## Reasons for score: I like the paper largely. It provides a nice practical method for sanitization, although it is hard to give theoretical privacy guarantees of VAEs. The method has shown to be a good defense for individual attribute inference attacks. My main concern is the experimental results are only on two datasets with one task/sensitive attribute setting. As an empirical/methodology paper, I would expect more empirical results. \n\nAlso, I have a question regarding the sanitization section. It's not clear if focal entropy should be used in both $\\tilde{T}$ and $\\tilde{S}$. The paper mentioned \"training of $\\tilde{T}$ leverages a modification of entropy ...\" However, I think it should be $\\tilde{S}$ or both. It seems that Equation (6) should be negative KL divergence since we want to force the distribution to be close to the uniform distribution. \n\n########################################################################## Minor Comments:\n\n1. In Equation (6), the bracket should be after $\\theta$.\n\n2. I don't think $\\pi$ is clearly mentioned before Equation (9). The notation should be more precise.\n\n 3. I'm guessing it can also protect the dataset-level (proprietary) attribute inference. I'm interested in seeing the results, but it is not required for this submission.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Blind Review",
            "review": "## Summary\n- The paper tackles the problem of adversarial representation learning i.e., to learn a low-dimensional representation of the image that encodes target-relevant non-sensitive information, but not (often correlated) sensitive information.\n- The approach follows a VAE-based adversarial framework, with the core novelty being the use of maximizing focal entropy  (i.e., entropy among a set of similar attributes) over sensitive attributes in the representation.\n- Evaluation performed on CIFAR100 and CelebA indicates that the approach outperforms similar baselines under certain conditions.\n\n---\n\n## Strengths\n\n**1. Focal entropy**\n- I appreciate the insight exploited by the authors -- to encourage representations with high entropy over a small set of correlated sensitive attributes\n\n**2. Evaluation**\n- I found the evaluation quite thorough. The paper compares with many recent baselines, demonstrates performances curves and additional analysis to explain the model's behaviour.\n\n---\n\n## Concerns\n\n### Major Concerns\n\n**1. Objective**\n- There are a few things unclear to me in the objective (Eq. 1-6) and would appreciate if the authors clarified them.\n- (a) I don't get the motivation for the two pairs of the classifiers $(T, S)$ and adversarial $(\\tilde{T}, \\tilde{S})$ that tries to simultaneously minimize/maximize both the target $p(a|z)$ and sensitive attributes $p(y|z)$.\n- (b) Especially, I don't understand the reasoning behind having an adversarial loss on the target attribute -- shouldn't this be maximized in all cases?\n- (c) I am also confused with the sanitization term (Eq. 6). In particular of why the sensitive attribute classifier $\\tilde{S}$ is a function of the target attribute classifier $\\tilde{\\theta}_{tar}$?\n- (d) The term $\\phi_{\\tilde{T}}$ appears to be undefined to complement Eq. 6.\n- (e) It would also be nice if the authors extended Eq. 6 (which maximizes standard entropy) with the final objective to maximize focal entropy.\n- (f) Is the VAE term (Eq. 5) necessary (esp. reconstruction)? After all, this appears as a secondary objective opposed to the primary objective of learning a representation $z$ which minimizes information leakage.\n\n**2. Writing - Sec. 3**\n- My issues in understanding can be partly attributed to the writing in Sec. 3, which I found difficult to follow for a few reasons.\n- (a) Some terms seem to be overloaded e.g., two notations for encoder $q(x; \\theta_E)$ and $E(x; \\theta_E)$. I was also thrown off by many ways the components of the model/objective are conveyed: Losses $\\phi_T$, parameters $\\theta_T$, player $T$, adv. player $\\hat{T}$, etc. I recommend finetuning the second paragraph on page 4.\n- (b) While the architecture in Fig. 1 is designed to accompany the text, I find it somewhat incomplete and unclear. For one, there are six players/blocks in the proposed architecture. However, only the encoder and decoder are shown in the figure. Furthermore, while there are five loss terms, only four of them are shown in the figure. I am also not sure if the blue/red backgrounds in the residual and target streams code something in particular. Perhaps one solution is to split the figure into two: one for the architecture and another for information flow?\n\n**3. Results**\n- While the focal entropy makes sense to me (i.e., by increasing entropy over a small set of similar classes), I wonder if the results confidently back up that is it indeed better than related baselines.\n- (a) The improvements seem somewhat marginal e.g., an improvement of 2% accuracy (Table 1) over prior work on both CIFAR100 and CelebA.\n- (b) But what I find more revealing of the performance is the trade-off curve in Fig. 3b (thanks for the presenting this!). It appears that the proposed focal entropy approach (blue curve) outperforms Kernel-SARL (red curve) only for in a small operating range of high target accuracy. Overall it appears that MaxEnt-ARL and Kernel-SRL offers better trade-offs.\n\n\n### Minor Concerns\n\n**4. Decoder/Reconstruction**\n- Since the task is to partly perform reconstruction as well, I find missing evaluation on how good the reconstructed samples are.\n\n**5. Same equilibrium issues as before?**\n- The introduction (second paragraph, p2) remarks that the adversarial min-max formulation of the problem has an issue that there is significant leakage if the optimization does not reach equilibrium.\n- Wouldn't this be an issue in the proposed work as well?\n\n### Nitpicks\n\n**6. Some nitpicks**\n- Please label the axes in Fig. 2.\n- Not sure what this sentence means \"increases the uncertainty in a more organic fashion\" - please rephrase\n- Typo in citation \"Radovanovi263 et al. (2010)\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "I found the paper confusing, obscuring what might be a nice idea & experiments",
            "review": "Summary: This paper gives a method in the class of learning representations which have some information censored. In particular, the authors propose a setup where there are many “private” classes, and some classes are more similar than others – this maps (I think) onto the privacy setting, where each class is like one individual. They give a modification of an entropy loss, focal entropy, which is conducive to this type of learning, and show in experiments that this method can be successful.\n\nI recommend reject due to a lack of clarity in the formulation, motivation, and writing of this paper. I think the idea has promise, the experiments are fine, and if correctly communicated can be a successful paper, but as it stands I found the paper pretty confusing and I’m not really sure what the method is for.\n\nStrong points: \n-\tThe problem as I understand it is interesting and the work is well-situated in the privacy literature. \n-\tFocal entropy seems like a good idea and as far as I know is novel as a loss. \n-\tExperiments on CIFAR and CelebA demonstrate some good behaviours from this method, mostly beating baselines\n-\tExperiments are mostly good, decently thorough\n\nWeak points and Clarifications:\n-\tThe main weak point of this paper is the exposition and clarity – I find that the problem setup is not explained particularly well. Especially as someone who is more familiar with the fair representation learning literature, I get confused when the authors refer to a sensitive attribute, or private part of the data, in this setting – it seems to be a different notion than I am used to and it is never clearly defined. If I squint I can see how it maps onto privacy nicely but I would prefer if the authors make that clear.\n-\tThe introduction could use a rewrite – it doesn’t set up the main points of the paper particularly clearly and leaves the reader a little confused. Consider stating more clearly off the top what the problem statement is, and move much of the content of p1 to related work\n-\tThe motivation of “highly overlapping information” is pretty imprecise. Having read the paper I can kind of see what you’re getting at but it’s not clear – I think you mean instead information hierarchy? Sub and super classes\n-\tAs far as I can tell the exact equation for focal entropy is never actually given? Let me know if I’m wrong\n-\tFigure 3b – you mentioned the better tradeoff in the high accuracy domain, but not what appears to be the worse tradeoff in the low accuracy domain. Is there anything I’m missing about understanding this figure?\n\nOther feedback:\n-\tP2 “the solutions mentioned earlier can only meet its practical promises …” – need a citation for this or explanation\n-\tP2: explain these suboptimalities more, I’m not sure exactly what you’re referring to\n-\tP2: you bring up a “vast number of dissimilar classes” without explaining why that’s relevant – in much work we deal with binary attributes/classes\n-\tEq 4: looks a lot like standard fair representation learning, so need to explain more clearly in motivation why it’s different\n-\t“Although maximization of entropy is sufficient for nonprivate attributes to minimize information leakage across representation partitions, we postulate that proper sanitization must be conducted w.r.t to focus classes in a similarity-aware fashion” – this sentence is very important to the paper, and I don’t quite understand any of it. If you work hard on this sentence (why do you postulate that? What is a focus class? Why would we ever consider similarity-awareness and what is that?) you’ll go a long way to clarifying your intro\n-\tTop of 3.2 – need to clarify what s is\n-\tEq 7 – this makes it look like Y_similar is a set of tuples but I think you mean set of elements of Y?\n-\t“would like the entropy peak shifted such that uniformity wrt similar classes gets dominant” – this is another idea which is not being clearly communicated right now and seems central to this work\n-\tThe “reweighting vector” is not introduced – not clear what we will do with it. You never actually show how this is included in the entropy calculation\n-\tWould like to see some sort of statistic on hubs, along with these pictures\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review ",
            "review": "\n### Summary\nThis paper presented a method for learning private representations. This method is based on adversarial representation learning and the main technique contribution comes from focal entropy. The experimental results show that focal entropy can improve the accuracy of the target predictor without increasing adversarial accuracy.\n\n### Pros\n1. Focal entropy is effective in reducing the information leakage of the learned representation while improves the target accuracy over the state-of-the-art.\n2. This paper covers a wide range of experiments.\n\n### Cons\n1. Some technique details are not presented clearly. I have listed some of them here:\n   a. How to divide the representation $z$ encoded by the encoder into two sub-representations, namely, target and residual representations.\n   b. There is a high-level description of focal entropy instead of equations.\n2. Some notations are very confusing. For example, in the optimization goal (Equation (2)), the adversarial loss $\\phi_{tilde{S}}$ is related to the parameter $\\theta_{res}$. However, in the following description, this loss is related to $\\theta_{tar}$(Equation (6))\n\n### Comments\n1. The optimization objective involves many individual terms. The experiments should further provide results to show how the trade-off parameters $\\beta$ controls privacy leakage and target accuracy.\n2. This work is done in an adversarial training manner, can leakage reduction be achieved in a differentially private training manner, i.e., training the encoder using dp-sgd?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}