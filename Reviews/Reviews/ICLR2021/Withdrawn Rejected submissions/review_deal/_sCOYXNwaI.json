{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper has initially received mixed reviews, with two favorable and two unfavorable reviews. Several serious issues have been raised, in particular on experiments and validation; limited novelty; limited performance improvement; on the preliminary stage of the paper, in particular presentation and writing, and on the justification of key choices.\n\nThe authors provided responses to some of these issues, but in the discussion phase the reviewers (and the AC) judged the the responses did not sufficiently address the weaknesses of the paper, in particular:\n- The experimental setup does not assess the key innovation of the paper. \n- Several contributions claimed by the authors (in the paper and in the response) are judged not to be novel.\n- Concerns regarding the metric chosen to measure smoothness.\nand other issues.\n\nThe reviewers and AC agreed, that the paper has potential and merits, but that at at this point it is not yet ready for publication."
    },
    "Reviews": [
        {
            "title": "Incremental methodological contribution and performance improvement",
            "review": "This submission proposes a diffeomorphic spatial transform network, which considers the specific transformation, i.e., diffeomorphism, in the data. The research topic is quite interesting, but the submission is at the preliminary stage and needs more work before publishing.\n\n1) The writing of the paper could be improved in terms of clarification. Most of the time, the authors present the solutions without explaining the motivations or underlying intuitions. For instance, why do we have to have the time-dependent vector field? Why do we need to use the prior shape? \n\n2) The novelty of the proposed method is limited. The main contribution of this paper is the use of BCH formula to approximate the piece-wise time-dependent sequence of the vector field. Very likely, this approximation is not very efficient, and we also don't know its approximation accuracy. \n\n3) The performance improvement is limited. In the MNIST experimental results, the accuracy improvement is subtle. Compared to CNN+Field-STN, the CNN+Diffeomorhpic-STN has a slightly increased mean accuracy with a larger standard deviation and a slightly decreased number of negative Jacobians with also a larger standard deviation. For the breast tissue segmentation task, if we consider the transformer only (without considering the shape prior), the Dice Score performance of the proposed method is downgraded. \n\n4) The efficiency of the proposed method is questionable. If we use this diffeomorphic transform network as a plugin module in another network, will it become the computational bottleneck of the whole model, since the estimated spatiotemporal vector field is really high-dimensional?\n\n5) Minor question: Is the identity map missing in the Algirhtm 1? phi_0 = Id + v/2^T?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper propose a novel method to incorporate shape prior in neural networks based on Diffeomorphic transformation. This is useful as by design it preserves certain desirable properties of output such as smooth boundaries and connected components which are of interest in medical imaging applications.   The method is validated on Mnist for data invariance and a medical imaging task for segmentation.\n\nNovelty: The idea of  incorporating shape prior information into neural network based image segmentation is inspired by Lee et al. This method shows  how to use a diffeomorphic spatial transformer to warp a shape prior where warping is based on time-dependent parameterisation of multiple vector fields utilizing the Baker-Campbell-Hausdorff formula.\n\nClarity and Presentation: The paper is overall well written and motivated. Minor comment: As an outsider, I found the argument on negative Jacobian determinants hard to follow and it comes up several times. So it seems quite important.\n\nExperimental Validation: It is partially on weaker side in my opinion.  e.g. Lee et al. experiments were shown on segmentation of coronary lumen structures. Is there a good reason to instead choose breast tissue segmentation task only and not show any experiments on the former one? similarly, to prove data invariance, STN is validated on several benchmarks whereas this method only uses one, MNIST.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting novelty but subtle and not backed up with experimental results",
            "review": "Authors present a spatial transformer layer that models\ndiffeomorphisms. Modelling diffeormophisms with neural networks is not very\nnew. Prior work successfully utilized stationary vector field prediction and\nfast solvers through scaling-and-squaring techniques. These prior work have been\nused to tackle registration and segmentation problems. The technical novelty\nhere is to use time-dependent vector fields for modeling\ndiffeomorphisms. Experiments on MNIST classification, using the proposed spatial\ntransformer as a layer, and breast tissue segmentation, using the proposed\ntransformer as a single network to deform a template, are presented and compared\nwith conventional spatial transformer layers using thin-plate-splines as\ntransformation models.\n\nWhile the introduction of time-dependency is interesting, more from a technical\nperspective than a conceptual perspective, I believe there are several aspects\nof the paper that needs improvement. \n\n1. Since the main innovation is integration of time-dependency in modelling\nvector fields with neural networks, I suggest directly comparing time-dependent\nwith stationary vector field approaches.\na. Experimental comparison can focus on the differences between stationary and\npiece-wise stationary vector field modelling. I could not see this comparison in\nthe experiments, only a single mention in the conclusion stating better\nperformance of the proposed model.\nb. Analysis of computational time would be interesting. Predicting and\nintegrating a stationary vector field will be faster I assume. But by how much?\n\n2. MNIST classification experiments raise some concerns:\na. The increase in classification accuracy of the proposed model compared to\nCNN + Field-STN is small. This difference may as well be due to randomness of\nthe optimization. This result is not motivating for the proposed\nmethod. Achieving lower number of negative Jacobian determinants is interesting\nbut its value is questionable.\nb. What is CNN + Field-STN? This is not defined.\n\n3. Segmentation results raise the following concerns:\na. The method proposed here does not seem to be better than the method\nproposed in Lee et al. 2019. The lower HD distance can be motivating but value\nof this may be better justified. If the proposed innovation was substantial,\nlower accuracy would have been completely fine. However, since the innovation is\nsubtle, such a lower accuracy lowers the enthusiasm for the paper.\nb. The proposed method achieves lower number of connected components but the\nvalue of this is not very well motivated from an application\nperspective. Providing such a motivation would be helpful.\n\n4. I suggest editing section 3 to present the proposed method much more\nclearly. The method only becomes clear in Section 4. \n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of Diffeomorphic Spatial Transformer Networks: Weak Accept",
            "review": "Summary:\n\nThe authors present a modification to spatial transformer networks that restricts the transformations to the group of diffeomorphisms. When combined with shape priors, this imposes topological constraints on the mappings produced by the network. These are important considerations in applications such as segmentation tasks where we expect there to be constraints on, for example, the number of connected components. The authors demonstrate the effectiveness of their approach in MNIST experiments and a breast tissue segmentation task. \n\nStrengths:\n1) The paper is clear and well written, and positions its contributions well in relation to existing research.\n\n2) The method is well motivated and the procedure is reasonable. The paper tackles an important consideration when applying neural networks to segmentation tasks, as constraining to the group of diffeomorphisms imposes smoothness on the transformations that ensures the outputs share topological properties with our prior expectations (e.g. number of connected components, smoothness of boundaries).\n\n3) The authors use the scaling-and-squaring to solve the ODEs that describe the diffeomorphic transformations, which I have not seen used in other ML studies.\n\nWeaknesses:\n\n1) Section 3, which describes the components of the method, can be expanded (there is space for this at the end of page 2) and reorganised to make it clearer. Specifically:\n\n\ta) the last two lines of paragraph one of page 4, starting with ‘The scaling-and-squaring...’, say this algorithm is not amenable to time-dependent parametrisations as laid out by the ODE in eq. 1. It is not immediately clear to me as to what the A and B refer to in this situation, are these the separate time steps t?\n\n\tb) I would like to see more discussion about the binary tree assumption on the composition of the diffeomorphisms. I can understand why this is a useful assumption with regards to complexity and numerical stability, but I do not have good intuition about the implications of this assumption on the transformations the model outputs.\n\nMy suspicion is the combination of these two assumptions explain some of the behaviours seen in the results. \n\n2) The results section could also be expanded for a bit more clarity. My main concerns are around the metric that measures the percent of the Jacobian determinants that are less than zero. Is this computed from all observations or just on a hold out test set? Why is it the case that the supposedly non-diffeomorphic models in Table 1 perform better on this metric than the diffeomorphic models?\n\nWhy is there such a large discrepancy in the size of the standard deviations in Table 2?\n\nI do not think the results are particularly convincing in terms of improvements in the quality or the accuracy of the methods compared with, but the point of the paper stands if the behaviour of this metric is properly discussed. \n\n\nReasons for score: \nI vote for accepting the paper. On the whole, it is a solid paper with an interesting and novel contribution, but I think it is hampered by the lack of clarity in the areas listed above. I am willing to revise my assessment up if these concerns are addressed. \n\nQuestions for the rebuttal period:\n\nPlease refer to the questions in the weaknesses section. \n\n\n---------------------------------------------------------------------------------------------------------------\nUpdate after discussion:\nAs stated in my initial review, I wanted to see an in depth discussion of what the implications of the binary tree assumption are for the types of transformations that are produced and the impact on the metric using the Jacobian. If these concerns are addressed, I think the paper will be greatly strengthened in the future. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}