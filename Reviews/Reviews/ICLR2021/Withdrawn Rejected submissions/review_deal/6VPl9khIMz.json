{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The topic covered by the paper is timely, and the way the authors have addressed the problem seems correct. The provided empirical evidence seems to be sufficient to support the main claim of the paper. Presentation is well structured and clear.\nNotwithstanding the above merits, the proposed approach seems to confirm other similar proposals presented in the literature, so the contribution of the paper seems to be limited. Although presentation is good, it is not highlighting enough the differences w.r.t. those proposals and the basic approximation result given by Chebyshev polynomials. Especially a better theoretical characterisation w.r.t. to approximation capabilities by Chebyshev polynomials (with no truncation) would have helped to better gain understanding of the merits of the proposed approach. Finally, some of the experimental results do not seem to have a significant  statistical difference w.r.t to the baselines, so it would have helped to have the result of a statistical test."
    },
    "Reviews": [
        {
            "title": "Nice paper using stacked graph filters for GCNN architectures, but contribution may fall short ",
            "review": "SUMMARY:\nThis paper addresses the problem of vertex classification using a new Graph Convolutional Neural Network (NN) architecture. The linear operator within each of the layers of the GNNN is formed by a polynomial graph filter (i.e., a matrix polynomial of either the adjacency or the Laplacian novelty). Rather than working on the frequency domain, the paper focuses on learning the polynomial coefficients of the filter on the vertex domain. The key novelty is the consideration of a stack architecture for which the polynomial filter is formed by the successive application (i.e., matrix multiplication) of filters of order one. Numerical experiments with real datasets showcase the merits, including superior classification performance, of the proposed architecture. \n \nSTRONG POINTS:\nThe paper is timely and fits nicely the scope of the conference.\n \nThe numerical experiments are convincing, offering insights, and demonstrating some of the advantages of the proposed architecture.\n \nThe writing is clear, making the paper easy to follow. \n \nWEAK POINTS:\nExcept for the numerical experiments, I find that the contribution is quite limited. The postulation of GCNN architectures based on polynomial graph filters where the focus is on learning the polynomial coefficients has been studied thoroughly in the literature. In general, the paper does a good job listing relevant works in that area, although some are missing (e.g., Gama - Ribeiro). Some of the existing works look at ARMA structures and recursive order-one filter implementations. I acknowledge that the architecture considered in those papers may not be exactly the same as the one proposed by the authors in this paper. I also appreciate that the application at hand (vertex classification) was not the goal of many of those papers. However, I still feel that the contribution falls short, especially for a top conference such as ICLR. In any case, I am open to change my mind if the authors are able to strengthen their theoretical claims or address my concerns in their rebuttal.\n\nI believe that the title should be changed. GCNN are not mentioned. The current title places the focus on Stacked Graph Filters. My first concern is that, within the linear paradigm (i.e., as polynomials of the adjacency/Laplacian matrix), this type of architectures have already been investigated. More importantly, the paper focuses on NN architectures, so I think it is reasonable to have that on the title.  \n\n \nOVERALL RECOMMENDATION:\nMarginal reject. The paper is topical, timely, and nicely written. It addresses a problem of interest and does so with contemporary machine learning tools. The results in real-world datasets are convincing. However, the contribution and novelty are limited, falling short of the average contribution at ICLR. \n \nADDITIONAL RECOMMENDATIONS:\nBeing able to obtain additional theoretical results would make the contribution more solid.\n\nFurther elaborating on the robustness of the architecture it is another change that would strengthen the manuscript.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper proposes a relatively simple formulation for a graph convolutional filter similar to GCNII, that has the advantage of providing useful insights on the characteristic of the considered datasets. ",
            "review": "Adaptive stacked graph filter\nThe paper proposes a relatively simple formulation for a graph convolutional filter, that has the advantage of providing useful insights on the characteristic of the considered datasets. Many points of the paper are however not convincing in the present form, mainly regarding the novelty of the proposed formulation.\n\nThe paper proposes a graph convolution operator that is inspired by the well-known approximation of a graph filter using polynomials of the graph Laplacian.\n\nPros:\n- The paper proposes a simple filter formulation that allows to study the dependency on the neighborhood radius on different datasets.\n- The visualisation of the filters is interesting.\n-The reported experimental results are positive, even though in many cases the improvement does not seem significant.\n\nCons:\n-The proposed model is very similar to GCNII: Graph convolution by Kipf and Welling with a single scalar parameters instead of a parameter matrix + skip connections. The main difference with GCNII is the lack of the identity mapping. \nIn fact, eq. of H^l in page 4 is very similar to eq. 5 in https://arxiv.org/pdf/2007.02133.pdf. Authors should deeply discuss the differences between their proposal and other works in literature, clarifying their novel contribution.\n\nComments about specific sections follow.\nExperimental section:\n\t\t-In page 6, authors state that they fix the \\theta hyper-parameter of GCNII to 0.5, even though the recommended values are around 1.5.  Can you justify this choice? Also, since you run the experiments on GCNII, it would be interesting to see its performance on the bipartite dataset with \\theta = 1.5\n\t\t-In Table 3, the results from literature do not report the variance. In general, it seems like the results of the proposed method and baselines are pretty close, and in many cases inside the variance range.\n\nAppendix A:\nthe horizontal stacking variant is not explained in detail. From the figure it looks like several stacked layers with an aggregation that sums the weighted representation computed at each layer. I don't see why this should be \"horizontal\". Probably writing down the equations of this model would help.\n\nB.2. While authors state that for each dataset and for each run they select the hyper-parameters using the validation set, later in the same section they state that the results in the main paper are referred to the hyper-parameters in bold. I don't understand how the hyper-parameter selection procedure is adopted.\n\nMinor: \nTable 3, Chamaleon dataset. Missing bold on SGC. Texas: MLP is in bold while it shouldn't be\nPage 6: \"Note that we also the extact\" -> we use the\n\n-----REBUTTAL\nI acknowledge having checked authors' rebuttal and the revised version of the manuscript",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The Review",
            "review": "This paper proposes to stack the graph filters with learnable polynomial parameters to construct the new graph neural network model. Generally, this paper is well organized and easy to read. Here are my concerns.\n\n1.Essentially, this paper argues that the approximation of Chebyshev polynomials in GCN can only capture the low-frequency features in the spectral domain, and proposes a more general approximation scheme by stacking the graph filter in the spatial domain.  However, the low-frequency property of GCN is highly related to the localized first-order approximation of graph convolutions. Without this first-order approximation, GCN model can capture the high-frequency information in graphs, e.g, ChebyNet [2] with large enough order K. It's better to add more discussions/comparisons with this kind of GCNs.  \n\nMoreover,  my core concern is the superiority of why the proposed polynomial approximation (in Equation 7) is better than the previous Chebyshev approximation from both theoretical and practical justifications.  In graph signal processing, using a polynomial series to approximate the graph filter has been well studied in the literature. As pointed out by [1], Chebyshev polynomial is a good approximator to approximate graph filters. It is better to add more justifications (e.g., numerical analysis) about the proposed approximation scheme.  \n\n2.Another concern is the experiment.\nDataset splitting: It seems like that this paper adopts the new splitting plan (stratified 0.6/0.2/0.2 splits) for all datasets. Meanwhile, the paper also reports the best results reported in the literature. However, I think it’s improper to put them in the same table since we can’t make a fair comparison under different data splitting.  Moreover, I would like to see the results of SGF on the public splitting of these datasets.\n\nHyperprameters: In Appendix B.4, the authors claim that they follow the hyperparameter recommendation in the original paper of baselines. However, it seems that some of the given hyperparameters are not the best hyper-parameters. For example, for Cora, \\alpha of GCNII is set to 0.2, while in   Appendix B.4, \\alpha=0.5 which inconsistent with the original paper [3].\nOn the other hand, In Appendix B.2, the authors adopt the random strategy to search the hyperparameters of SGF. Since the authors re-run all the experiments of baselines in the new splits, it’s better to conduct the same hyper-parameter search process for each baseline to ensure a fair comparison.\n\nThe filter parameters visualization: From the model construction perspective, since the only difference between SGF and GCNII/APPNP is the trainable filter parameters. Therefore, I’m curious about the value of \\alpha and \\beta after the training. Could you visualize the value of two parameters in each layer from SGF?\n\nOverall, I think this paper is marginally below the acceptance threshold.\n\n[1] David K. Hammond, Pierre Vandergheynst, and Re ́mi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.\n[2] Defferrard, Michaël, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" Advances in neural information processing systems. 2016.\n[3] Chen, M., Wei, Z., Huang, Z., Ding, B., & Li, Y. (2020). Simple and deep graph convolutional networks. arXiv preprint arXiv:2007.02133.\n \n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but have weaknesses on both novelty and experiments",
            "review": "Summary:\nThe authors proposed to learn the polynomial graph filter in their model. It can be viewed as adaptively learning the propagation part of APPNP and follows by a linear transformation (in features). They show the proposed model can perform well on both homophilic and heterophilic graphs.\n\nPros:\n1.\tThe idea of adaptively learn the polynomial filter seems correct and reasonable.\n2.\tResults on filter visualization and structural noise are interesting.\n\nCons:\n1.\tThe proposed methodology is not novel. A very similar idea has been proposed previously. (See Detail comments)\n2.\tProblems of over-smoothing.\n3.\tResults on experiment section (Table 2 and 3) are questionable. \n\nDetail comments:\n\nWhile the proposed idea of adaptively learning the polynomial graph filter is interesting, it has been proposed previously in not only the GNN literature [2] but also PageRank based methods [3]. Both of them proposed the idea of adaptively learn the polynomial graph filter, or equivalently the generalized PageRank weights. Hence, I do not think the current paper is completely novel. Nevertheless, the proposed methodology seems to be the correct answer for GNN to adapt to both homophilic and heterophilic graphs. One problem of the current proposed method is that why it can avoid over-smoothing when stacking many layers? The authors use a fixed initialization $\\alpha = 0.5$ which is the same as APPNP so at least at the very beginning it won’t suffer from over-smoothing. However, it is unclear how will the coefficients behave during and after training. Also, it is not clear how to initialize $\\beta$ in the model. Furthermore, if the proposed model can indeed adaptively learn the good polynomial graph filter, why doesn’t the random initialization work? Does that mean the implicit bias of the specific initialization proposed in the paper is necessary? If that is the case, then I do not see why the claim of “adaptive learning” is correct since it is actually sensitive to the initialization.\n\nBeside the methodology and novelty, I also find the experiment section questionable. Firstly, since the main theme of the paper is learning the polynomial filter, the authors should at least compare their method with ChebNet (GCN-Cheby)[5] which also use polynomial filter. Note that in both [4] and [2], they all show that ChebNet can better adapt to heterophilic graphs compare to GCN and GAT.\n\nOn the other hand, according to Appendix B.4, the authors use $K=2$ (propagation step) for APPNP. This is *NOT* the suggested hyperparameter reported in [1] ($K=10$). Note that the authors of [1] even show that if we choose a larger $K\\geq 10$, the performance can be slightly improved on Cora, Citeseer and PubMed. In contrast, SGF use $K=16$ which is not a fair comparison to APPNP. There should be a experiment that compares APPNP with SGF under the same $K$.\n\nFinally, the authors claim the performance of most baseline methods are found in the literature. However, this is also problematic to me. Note that in the original GCN and GAT paper, the date split is much sparse then the $0.6/0.2/0.2$ split proposed by the authors. Also, in the Geom-GCN paper they do test their model on Chameleon in the split $0.6/0.2/0.2$. Why is it stated as not available? Even if we assume all the problem above can be well explained, the improvement of the proposed model seems not statistically significant. For example, on Wisconsin, Cornell and Texas, although SGF has the highest accuracy in average, the standard deviation is very large. MLP is within 1 standard deviation. Please report the confidence interval to show that the gain of SGF is indeed statistically significant. On the other hand, SGF is worse than not only SGC but also GCNII by a large margin on Chameleon. If SGF can indeed learn the near-optimal polynomial filter, then why this is the case? At last, in the original Geom-GCN paper, they also have the Actor dataset. I think it would be great if the authors can put this result at least in the Appendix.\n\nBesides these weaknesses, I still find the paper well written. Also, the experiment on filter visualization and structural noise are quite interesting. I believe the paper can be greatly improved if all the concerns above can be addressed.\n\nMinor comments:\n\nIn page 2, the authors state that the normalized adjacency matrix with added self-loops is $\\tilde{A}= I-D^{-1/2}A D^{-1/2} + c$, where $c$ is some diagonal matrix. This is incorrect. Note that when we add self-loops, the degree matrix $D$ has to changed accordingly. Please see the correct expression in [1] for example.\n\nIn page 2, the Rayleigh quotient $r(\\mathcal{L},x)$ is defined with two input arguments but later the authors ignore $\\mathcal{L}$. While it is clear from the context, the notation is not rigorous.\n\nIn page 1 introduction section, the authors mention that the model does not need to tune the hyper-parameters. However, in the same page contribution section, the authors mention that they use one hyper-parameter setting. According to their experiment section, I think what they mean is the previous. It would be great to clarify the ambiguity here.\n\nReference:\n\n[1] “Predict then Propagate: Graph Neural Networks meet Personalized PageRank,” Klicpera et al., ICLR 2018.\n\n[2] “Adaptive Universal Generalized PageRank Graph Neural Network,” Chien et al., arXiv:2006.07988.\n\n[3] “Adaptive diffusions for scalable learning over graphs,” Berberidis et al., In Mining and Learning with Graphs Workshop @ ACM KDD 2018, pp. 1, 8 2018.\n\n[4] “Generalizing Graph Neural Networks Beyond Homophily,” Zhu et al., NeurIPS 2020. (arXiv:2006.11468) \n\n[5] “Convolutional neural networkson graphs with fast localized spectral filtering,” Defferrard et al., NeurIPS 2016.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}