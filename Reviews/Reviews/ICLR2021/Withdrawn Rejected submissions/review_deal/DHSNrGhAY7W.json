{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper shows that L2 self-attention is Lipschitz and presents a new method for computing the Lipschitz constant. All reviewers are positive about the technical part of the paper. However, the major concern comes from the significance of the computed Lipschitz constant. The paper only presents some numerical results using simple toy examples, which is insufficient to justify the  importance of the proposed method. The paper would be a much stronger paper if better numerical results could be presented."
    },
    "Reviews": [
        {
            "title": "Official Blind Review #1",
            "review": "This paper studies the Lipschitz continuity properties of self-attention. It is proved that the widely-used dot-product self-attention is not Lipschitz continuous. A novel L2 self-attention is proposed and proven to be Lipschitz continuous. Experiments show that the upper bound of Lipschitz constant for L2 self-attention is asymptotically tight. Invertibility of MHA residual map is investigated to prove the Lipschitz continuity of L2 self-attention. Finally, experiments on Transformers with L2 self-attention are studied.\n\nThe problem studied in this paper is a novel problem which is unexplored and interesting. The paper is equipped with solid mathematical analysis and experimental results. It is well written and easy to follow.\n\nMy main concern about this paper is --- why do we need Lipschitz continuity in self-attention. In the introduction, the authors mention some contexts where Lipschitz continuity properties of neural networks are needed. But for self-attention, what are the benefits of Lipschitz continuity, or are there any potential scenarios where non Lipschitz continuous self-attention models do not work but only Lipschitz continuous self-attention models do work. In the experiments of Transformer, the L2 self-attention model behaves just comparable (or sometimes worse) than DP self-attention, which is not a strong evidence of the necessity of Lipschitz continuity of self-attention models.\n\nMinor questions:\n1. The non Lipschitz continuity of DP attention models seem to come from the \"quadratic\" terms of X. L2 self-attention looks like Gaussian kernel in its form. I wonder is there any general assumptions (like using bounded kernel?) under which self-attention models are Lipschitz. This will probably generalize the mathematical content of this paper.\n\n2. In Figure 4, could you provide the exact values of the valid NLL for each panel. It is difficult to distinguish the performance, especially the performance between panel 2 and panel 3, from the figures only.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review Comments of Paper \"The Lipschitz Constant of Self-Attention\"",
            "review": "This paper shows that the widely used dot-product self-attention is not Lipschitz with respect to any p-norm with $p \\ge 1$, and formulates L2 self-attention, which is Lipschitz.\n\nThe theorems and the proofs in the paper make sense to me, and the authors use experiments to show the tightness of Lipschitz constant derived from their theorem, the invertability using L2 self-attention, and the expressiveness of L2 self-attention on a NLP task compared with the transformer using the dot-product self-attention.\n\nI think this paper is interesting, given that there is almost no theoretical paper (compared with fully connected NN and convolutional neural network) studying the attention mechanism of neural network.\n\nHowever, my main concern is the “significance” of Lipschitz property. From a theoretical side, although there are many theoretical guarantees using the Lipschitz constant of the neural network or the general classification function, it may not lead to the best bounds. Actually, even if the original dot-product self-attention is not Lipschitz, I guess there is also some probability that most of the time (or along the trajectory) the self-attention is Lipschitz. This similar phenomenon happens in the Neural Tangent Kernel regime: although wide neural networks with ReLU have local minima and is not smooth everywhere, the function is easy to optimize along the trajectory.\n\nFrom the experimental/application side, I am not so familiar with the invertible NN literature, and I am not so convinced that invertible NN can directly lead to the applications mentioned in the paper. I think the most straight-forward way (for me and general audience) to show the advantage L2 self-attention is to find some application, which may be easy and the experiments can be very preliminary, and show that L2 self-attention works better on these tasks in the preliminary experiments.\n\nIn the light of my previous concern, I would like to ask a few questions:\n(1)\tDo you find any cases through experiments that the dot-product self-attention does not perform well but L2 self-attention can perform better? E.g. with the L2 self-attention mechanism, the training of GAN is more stable than that using the original self-attention, or the adversarial training using L2 self-attention performs better than the original self-attention?\n\n(2)\tIs it very natural to extend the invertible NN to the tasks mentioned in the introduction, i.e. flow based generative models?\n\n(3)\tAre there any references claiming some lower bounds of the performance in terms of the Lipschitz constant? If yes, I suggest to add those references into a related work section, or the introduction, to make the contribution of this paper clear.\n\n(4)\tIn the experiment, you mention that training the original transformer needs lots of tuning and the performance is not stable, while training transformer using L2 self-attention is much more stable. Could you please show more results on this phenomenon, because I think this phenomenon is interesting and can help to clarify question (1) I ask? E.g. the results of original transformer using different parameters with different initializations and the results of L2 transformer by choosing different parameters?\n\n(5)\tA suggestion: I think the proof of Theorem 3.1 can be put in the appendix and the conclusion can be more concise, and the main content can contain more experiment results and related works.\n\n\nI will adjust my score after some of my questions are answered.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A solid theoretical analysis of a state-of-the-art deep learning concept",
            "review": "The presented manuscript studies the Lipschitz constants of self-attention networks. It proofs that the widely used dot-product self-attention is not globally Lipschitz. The authors propose a so called L2 self-attention and derive its Lipschitz constant. An application in a transformer-based architecture shows only a minor performance reduction as price for the Lipschitz property.\n\nThe literature is properly reviewed and the contributions are clearly stated. The authors explain all concepts required to understand the theoretical derivation. However, I suggest that the difference between local and global Lipschitz continuity are introduced in detail as this becomes particularly important. \nFurthermore, the proof of Theorem 3.1. was difficult to follow for me and I think there should be a simpler way to formulate it. A few suggestions: i)  first give a high level overview of the steps of the proof ii) use lemmas to show intermediate results iii) proof it for N=1, D=1 first, which will simplify the notation\nOne point, which is shortly discussed by the authors, is that the dot-product self-attention is local Lipschitz. For most applications, the inputs are naturally bounded e.g. images. i.e. also the DP-MHA is Lipschitz.  So it unclear for me whether the L2-MHA will be need in these cases and maybe an experimental comparison would be convincing here.\nThe experimental section evaluates the proposed approach from three quite distinct directions. While 5.1. relies quite heavily on empirical evidence to support a theoretical claim, the example 5.2. and 5.3. get a clear message across.\nThe supplementary material is excessive, but was not further considered for this review.\n\nIn general, I think the theoretical analysis of deep learning methods is important to advance the field and I am therefore in favor of this paper. \n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Blind Review #2",
            "review": "Summary: \nThis paper theoretically analyzes the Lipschitz constant of the self-attention module. In particular, the authors prove that the vanilla dot-product self-attention is not Lipschitz and propose a Lipschitz L2 self-attention whose Lipschitz constant is upper-bounded. The theoretical results and the asymptotic tightness of the derived bound are empirically verified by analytical experiments.\n\nReason for score: \nI vote for an accept. I really like the theoretical analysis for the Lipschitz continuity of self-attention. And the proposed Lipschitz L2 self-attention has several potential practical applications. My major concern is whether the advantage of the Lipschitz L2 self-attention could outweigh the disadvantages in practice (see cons below). Overall, I think the theoretical contributions of this paper are sufficient for an accept.\n\nPros: \n+ Studying the Lipschitz constant of self-attention is very interesting and motivated, since Lipschitz continuity is a desirable property for neural networks but most previous works only restrict to simple fully connected or convolution networks.\n+ The proposed L2 self-attention is a simple plug-in variant of the standard dot-product self-attention and could potentially be applicable to several Lipschitz-required situations, e.g., the analysis of robustness and the design of invertible models.\n+ Overall, the paper is clearly written and the derivations that I have checked are correct.\n\nCons: \n- From the experiments, the expressivity of the proposed L2 self-attention is a big concern. It is not clear that the advantages of the Lipschitz L2 self-attention would outweigh its disadvantages. Therefore, it would be great to do more extensive experiments to indeed verify the superiority of Lipschitz continuity, e.g., the improved robustness and the training stability or the applicability to invertible models.\n- In the experiments regarding the asymptotic tightness of the upper bound, only the setting where $H=D=1$ is shown. It would be more convincing to include experiments for larger dimensions as well.\n\nAdditional Comments/Questions: \n1. Another potential application of the Lipschitz/Contractive L2 self-attention is DEQ[1] to ensure the uniqueness and the stability of fixed points. It would be great to include some discussions (if it is correct).\n2. In Lemma E.1, is the equation between $\\tilde{J}_{ij}$ and $\\tilde{K}_{ij}$ correct? \n\n[1] https://arxiv.org/abs/1909.01377\n\n##########Post-Rebuttal Feedback########\nI appreciate the author's thorough response and I think the additional experiments on robustness/stability make the paper stronger, so I decide to raise my score to 7. For the future version of the paper, it would be great to see more comprehensive experiment results that show the improved robustness/stability in the main text.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}