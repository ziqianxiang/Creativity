{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes to integrate multiple bit configurations (including pruning) into a single architecture, and then automatically select bit resolution through binary gates. The overall approach can be differentiable and optimized with parameters. However, as pointed out by the reviewers, the novelty of this paper can be the big question. Also, it seems to be hard or unpractical to employ different number of bits for layers, given the standard GPU and CPU hardware. "
    },
    "Reviews": [
        {
            "title": "Good work, but the novelty might be questionable",
            "review": "The paper describes the method to determine optimal quantization bit-width and pruning configuration for the neural network compression. Different from other approaches, the proposed method integrates multiple bit configurations (including pruning) into a single architecture, which is named “Super-bit”. The architecture uses binary gates to automatically select bit resolution. In addition, the super-bit model is differentiable and jointly trainable with parameters.\n\nThe idea of decomposing bit configuration into 2, 4, 8 bits, and using binary gates does not seems to be novel [van Baalen, 2020]. This paper also adopts the trainable gate parameter and unifies the pruning scheme.\n\nCompared to [van Baalen, 2020], the proposed super-bit use different gate function (forward step, backward sigmoid) and different loss function, so there is some novelty. However, there is no direct comparison of the algorithm. To be fair, [van Baalen, 2020] is quite recent work and both ideas may be developed concurrently. So, I think inserting a single paragraph of comparison would be enough.\n\nUsing cost function R is a good idea to directly reduce the computation cost. Experiments, especially ABS-P and ABS-Q are well designed and suitable.\n\nSome minor questions/suggestions:\n1)\tDoes the cost R consider both weight and activation bits? If yes, the input resolution change should affect BOPs and will change the optimal bit-width. It would be meaningful to show the correlation between optimal bit-width and the number of filters (but not necessary).\n2)\tHow is the ‘search cost’ calculated? As mentioned, the key point of the algorithm is to simultaneously train weights and configurations. Is ‘search time’ means the fine-tuning process after training the uncompressed network? Is it equal to other works? (Table 5)\n3)\tIn Figure 3 & 4, there is no layer assigned to 2-bit. Can you provide average bit-width for ABS networks?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official review comments #2",
            "review": "Inspired by gradient-based NAS of single-path formulation, the authors propose a super-bit model, a single-path method, to decide the optimal number of quantization bits and pruning of a group of filters. While it can be a time-consuming process to study the impact of quantization of certain filters (or layers) on model accuracy, the proposed scheme finds a particular compression configuration in a trainable manner. The experimental results show that the proposed method presents higher model accuracy or lower computational cost (measured as the bit-operation count).\n\nWhile the idea of presenting trainable binary quantization gate signals to automatically assign the number of quantization bits to layers is interesting, this reviewer has the following concerns:\n\n- For Table 1 and 2, are all methods assuming the same quantization configurations (e.g., layer-wise vs channel-wise and symmetric vs asymmatric, etc..)? Detailed quantization configuration should be included for fair comparisons.\n\n- For Table 1 and 2, why top-1 accuracy numbers are very high for all methods? How 6-bit precision of MobileNetV2 can be 71.8% (almost no accuracy degradation)? Moreover, the accuracy of the 4-bit-quantized ResNet-18 on ImageNet is even higher than that of full-precision ResNet-18. Even so, why not decrease BOPs while lowering top-1 accuracy for 'ABS (ours)' such that the gain of ABS is maximized?\n\n- BOPs look quite similar for many configurations (including the second row of a fixed number of quantization bits). Even though this reviewer does not believe BOPs can show performance benefits in hardware well, this reviewer cannot see distinguished advantages of ABS in Table 1 and Table 2. The gain on BOPs of ABS seems to be marginal.\n\n- The results do not describe memory footprint savings. Can ABS provide a reduced memory footprint compared to previous ones for a target model accuracy? In Figure 2, if BOPs can be replaced with memory footprints, it would be useful to estimate the compression capability of the proposed method. In Figure 3 and 4, comparisons are missing.\n\n- How can we optimize alpha_p and alpha_q? Is it sensitive to model accuracy? If the goal of this work is a fast exploration of compression configurations, optimizing alpha_p and alpha_q should be easy, but there is no relevant information in the manuscript (hence, Table 5 is not very reliable).\n\nOverall, unfortunately, this reviewer cannot easily find the advantages of the proposed methods in the experiments. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper but maybe lacks novelty",
            "review": "Summary:\n\tThe paper presents a technique called ABS (Automatic Bit Sharing), that combines group-wise filter pruning with residual-aware quantisation in a NAS framework. The presented results on a range of image classification tasks show great performance when evaluating using an Accuracy/BOps metric. In general, I think the paper shipped interesting ideas in terms of automating residual quantisation, but it could be better if the network architecture space is jointly optimised.\n\n\nStrength:\n1. The paper is well-written (although it has some minor flaws), but in general easy to follow.\n2. I like the formulation of using thresholds to automatically decide numerical representations, this looks neat and novel to me.\n3. It is the first time for me to see residual quantisation in a NAS framework and the formulation of the indicator function looks interesting.\n4. The composed search space gives a reduction in search time.\n\nWeakness:\n1. Both channel pruning and residual quantisation are existing ideas, this might reduce the novelty aspect of this paper.\n2. This work focuses on searching for compression on pretrained architectures, such as ResNets and MobileNets. However, these models are not state-of-the-art on the considered datasets, and one might argue that these models are inherently more redundant than models like MobileNetV3. Also, the authors do not consider jointly optimise the network architectures, which could naturally be an extension of this work and might help to further boost the accuracy. \n\nMy suggestions & confusions:\n1. I stumbled with the number representation until finishing reading section 3.1. It is probably worth mentioning that you are applying fixed-point quantisation to residual terms at the very beginning of the paper, especially in figure 1.\n2. The argument that you’ve introduced 0-bit quantisation for pruning is very misleading. Apparently your pruning is channel-wise but quantisation is layer-wise, it is naturally to consider you used 0-bit quantisation on the entire layer, but this is not the case.\n3. In Page 6, you mentioned you train the binary gates for weights and activations alternatively and this provides better performance. Could you quantify the performance gap? Also, if one simply looks at eq(17), this interleaved optimisation is not captured in this over-simplified optimisation target. \n4. Third line of page 6, I guess you mean group-wise sparsity?\n5. Page 6: Specifically, if a group of filters are ... -> if a group of filters is …?\n6. You mentioned you are using group-wise sparsity, do you show the number of groups anywhere? What is your choice of B in Eq(16) for all the experiments? Or you simply mean you are doing channel-wise sparisity? I am a little confused here.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}