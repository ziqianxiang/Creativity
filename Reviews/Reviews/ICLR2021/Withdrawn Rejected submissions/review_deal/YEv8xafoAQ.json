{
    "Decision": "",
    "Reviews": [
        {
            "title": "Experiments are not enough to validate the architecture design",
            "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a new approach for image manipulation by complex text instructions. The basic idea is to treat text as neural operators and use it to locate and modify the image features. The proposed approach locates where to edit by spatial attention over the image regions with the text instructions as queries. It further uses a text-adaptive routing approach to dynamically select different branches for different text instructions, and uses text-specific normalization to modify the image features. The editing network is trained with GAN loss and L1 reconstruction loss. Experiments are done on synthetic datasets Clevr and Abstract scene, and the segmentation masks of cityscapes dataset.\n\n##########################################################################\n\nPros: \n\n1. This paper is clearly written and easy to follow.\n\n2. It proposes the routing approach that dynamically selects different branches for different text instructions. The t-SNE visualization results in Figure.5(b) indicates that the model learns to group similar instructions and shares the network parameters among similar text instructions.\n\n3. The spatial attention allows the model to handle complex scenes and only modify certain regions of the image.\n\n##########################################################################\n\nCons: \n\n1. There lack ablation studies to validate the effectiveness of the routing network. What is there is no routing network, but still uses the text-adaptive normalization (i.e., there is only one routing branch). What's the number of routing branches used for these datasets, and what's the effect of increasing or decreasing the number of branches for routing? What if soft attention is used to aggregate the results from multiple branches instead of hard selection of one branch?\n\n2. In the last row of Table 2, there is an ablation study \"no text-adaptive\". Could the authors explain how to feed the information of the text instructions if you remove the text-adaptive normalization? Have you tried other ways of passing the instruction information, for example, concatenate the text features with the image features at each spatial location, and then pass it through MLPs or convolutional blocks, similar to the way of passing the text information in TAGAN?\n\n3. The method is only tested on synthetic datasets and segmentation masks, but not tested on real-world datasets. I'm wondering if it's possible to apply this approach on real-world datasets? For example, you can generate a dataset of cityscapes images with simple operations like adding, removing, etc, using the same way you used for generating the cityscapes segmentation masks dataset. Can this approach still work on real images?\n\n4. A limitation of this approach is that it relies ground-truth manipulated images for training. Such datasets with real-world images are difficult to obtain, so the experiments are mostly restricted to synthetic datasets. In contrast, other text-guided image manipulation approaches like TAGAN does not require ground-truth manipulated images and can use more real image datasets for training.\n\n##########################################################################\n\nReasons for score: \n\nThe major reason for rejection is that there are not enough experiments to validate the effectiveness of the network design. Specifically, the authors did not prove why routing is a better design and why text-adaptive normalization is better than other ways such as feature concatenation or feature addition. Another concern is that since it requires ground-truth manipulated images for training, it is difficult to collect such datasets with real images and apply this method on real images. All experiments are conducted on synthetic datasets or segmentation masks in this paper.\n\n##########################################################################\n\nQuestions during the rebuttal period: \n\nPlease address my concerns in the Cons part.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A convincing motivation, pipeline, and results, although the details should be improved",
            "review": "This paper proposes a method for manipulating images by text instructions. The authors propose a method for complicated instruction about a complicated image with more than one salient object. The proposed method consists of spatial mask estimation (where to edit) and text-adaptive routing (how to edit). The experimental results on three datasets show that the proposed method outperforms the other baselines.\n\n**Pros**\n- Convincing problem definition.\n- Overall pipeline of the proposed method is easy to understand.\n- Experimental results with automatic evaluation measures and subjective comparisons. They consistently show the superiority of the proposed method. It is also good to know when the proposed method fails in Fig. 10 for further works.\n\n**Cons**\n- The authors should add some related papers. Lanput et al. (2013) propose a method for image manipulation with natural images, although the instruction must include a precise segmentation for the target region. Shinagawa et al. (2020) propose a method for image manipulation through a dialog between the system and a user. This method can be considered that the user routes the appropriate module while the authors propose an automatic routing using Gumbel softmax. The publishment of Cheng et al. (2020) is after the deadline of ICLR 2021; the missing citation is no problem. However, the objective and method are closely related to this paper; therefore, it is informative if the authors refer to Cheng et al. (2020).\n  - Lanput et al., PixelTone: A Multimodal Interface for Image Editing. CHI, 2013.\n  - Shinagawa et al., An Interactive Image Editing System Using an Uncertainty-Based Confirmation Strategy. IEEE Access, Vol.8, pp.98471-98480, 2020.\n  - Cheng et al., Sequential Attention GAN for Interactive Image Editing. ACMMM, 2020.\n- Detailed explanations about the proposed methods are unclear.\n  - The authors should clarify the dimension of each variable, such as $\\phi_x$, $\\phi^{\\mathrm{where}}_t$, and $\\phi^{\\mathrm{how}}_t$.\n  - Related to the first comment, the dimension of $\\phi_{\\hat{y}}$ is unclear. Since $M \\in [0, 1]^{H \\times W \\times 1}$, $\\phi_{\\hat{y}}$ seems to have the same dimension. However, $\\phi_{\\hat{y}}$ also seems to have color information, i.e., multiple channels, for image manipulation.\n  - There is no clear definition of $f_{\\mathrm{how}}$ in Sec. 3.2.\n  - It is unclear how $\\phi^{\\mathrm{where}}_t$ and $\\phi^{\\mathrm{how}}_t$ can be obtained.\n- According to the attention examples in Fig. 5, self-attention about \"how\" seems covering only a part of the corresponding description. For example, the second instruction has \"large gray sphere\" but only \"gray\" is attended. More discussions are required to know if it is okay and how it can be improved.\n\n**Overall rating**\n\nAs summarised in the title, the reviewer leans to accept. The rating can be upgraded if the authors can solve the concerns above.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review",
            "review": "### Summary\n\nAuthors proposed an architecture to edit image content given text operators. I think the paper is well organized, very clear, easy-to-follow, and experimental results are sound. Some details regarding baseline architecture should be clearer (i.e., differences in image and text encoders). One of the contributions, namely the routing mechanism, lacks ablation and/or analysis. Novelty is somewhat limited, though results are promising and compelling.\n\n### Positive aspects\n\n* I really liked Figure 1, which makes it very clear the goal of the paper.\n* I liked their option to decompose the text encoding problem in two separate neural net branches that interact with the input image in distinct ways.\n* Routing mechanism is not novel, though it seems to be a nice trick to provide different interactions with the visual features.\n* There is human evaluation that alleviates my concerns regarding the difficulties of evaluating image generation/modification only with numerical metrics.\n* Nice experiments using segmentation images from CityScape.\n* Good supplementary material.\n\n### Negative aspects\n\n* In the intro authors claim that *\"Cross-modal retrieval is essentially the same as our problem except it aims at retrieving ...\"*. I think the most related task is actually text-to-image synthesis. In fact, authors should better disambiguate the task at hand. For instance, there is image retrieval (image to image retrieval), multimodal retrieval (image to text, and text to image retrieval), which are quite quite different from the task proposed in Nam Vo's paper and used in this work. In a nutshell, it is misleading to say only \"Image Retrieval\".\n\n* Differences in the architectures of the baselines should be clearer. For instance: depth of the image encoder, different text encoders, different pre-trainings. It is hard to tell whether the comparisons are fair without going after the papers for all baselines.\n\n* I don't think the comparison to DM-GAN is fair. First, authors use BERT embeddings, and DM-GAN employs a simple GRU encoder pretrained to the dataset at hand (either CUB or COCO, as seen in their study). This could explain the fact that such method underperformed for retrieval. Authors should have at least pre-trained such encoder in the datasets used in this study. Ideally, the same BERT embeddings that authors used. Not to mention the fact that they simply employed a global pooled image embedding concatenated to the text embedding for introducing visual signal for a task that is heavily based on spatial relations might seem also unfair. That being said, I appreciate the fact that they made an effort to adapt architectures from other tasks as well in the comparison.\n\n* I found a recent multimodal retrieval work that also uses text-adaptive feature fusion for retrieval which probably should be discussed: Adaptive Cross-Modal Embeddings for Image-Text Alignment (AAAI 2020).\n\n* It is lacking ablation results or even analysis of the routing mechanism. For instance, what happens if only a single route is used, or if they are all used at the same time. \n\n* I might be wrong, but it seems that the proposed architecture is the only one which employs BERT as text encoders, which might be giving a head start to the proposed model. Hence the need to better isolate results regarding each component.\n\n* The result discussion comparing to the baselines is overly simplified. I understand there is a very limited space, though only claiming \"decent realism score as well as significantly higher retrieval scores\" does not aggregate much to the paper. It would be better to discuss \"Why the results were so much better for retrieval?\" or \"Such results differences are related to the experimental setup or to the architecture itself?\".\n\n### Stylistic and text details\n\n* Text is well written.\n* In the intro: \"we find that by disentangling how from where in the modification\": maybe use \\textit{how} and \\textit{where} to improve text clarity.\n\n### Questions\n\n* Is it necessary to have specific beta and gamma vectors for each normalization layer. Is it possible to save some parameters by reusing such weights?\n* If a given network simply learns the identity function, replicating the input image *as is*, what will be the metric results? I think they would be quite good given that both base and target images are most of the time very similar (i.e., modifications are not large). That being said, maybe it could be interesting add larger modifications in future releases of the dataset.\n* What happens when using a different route (path) rather than the selected one by the router for a given image modification (in the test time)?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}