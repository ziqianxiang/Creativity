{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The main idea of the paper is to  use image data to guide radar data acquisition by focusing on the blocks where the object has appeared. Four reviewers have relatively consistent rating: 3 of them rated “Ok but not good enough - rejection”, while 1 rated “clear rejection”. The main concerns include ad-hoc choices of algorithm design, lack of algorithm novelty, not adequate experiments in illustrating the performance, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. While the motivation is clear and the work has merits, the ACs agree with the reviewers’ concerns and this paper can not be accepted at its current state.\n"
    },
    "Reviews": [
        {
            "title": "A lot of ad-hoc choices that are not well-reasoned",
            "review": "The paper proposes a sensor fusion approach combining radar and camera to improve the detection of object in an automotive sensing scenario.\n\nThe paper proposes two different algorithms to solve this problem. It is not clear when one should be preferred over the other. It seems that the second algorithm is an improvement over the first. The first one uses only the camera data and also has  a partial blind spot. The second includes the radar data and improves on the first one. It seems to me that the description of the two algorithms lacks focus. Since the second algorithm is the one that the authors are trying to promote, why not focus on that one?\n\nThere are also a lot of ad hoc choices that are not justified. For example, they pick 18 block which determines the range of the radar. Why not 20? Why not 16? Similarly, there is a splitting of the scene into regions, which is also different in algorithm 1 and algorithm 2. Why is that? How was the number and extent of regions chosen? There is no discussion.\n\nOverall the paper seems like a collection of ad-hoc engineering choices. There is no discussion or reasoning in these choices. While the paper might have some nuggets of interesting ideas, these are buried in the detail of all these design choices of the authors. I would recommend a complete restructuring of the paper to clearly expose the goals and contributions of the authors, rather than just list a set of unmotivated design choices. \n\nAlso, given the heuristic nature of such designs, I would recommend the papers provide much more extensive experimental evaluation, especially in comparing their approach with other approaches, rather than just one.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for the paper: Adaptive Automotive Radar data Acquisition",
            "review": "The authors propose an algorithm to select radar return regions that potentially contain objects inside. \n\nStrengths:\n+ It seems to be an interesting topic that uses radar data together with RGB images from the camera. \n\nWeaknesses:\n- The problem to solve in this paper is not reasonably stated. The authors claim that the purpose of detecting the \"important regions\" in the radar domain is to implement compressed sensing during radar data acquisition. However, since object detection on radar data is already proposed and studied by some researchers (e.g., [1][2][3]), the reason why this problem needs to be proposed separately is not clear to me.  \n- The innovation is limited for both Algorithm 1 and 2. It seems that algorithms that trying to provide some bounding boxes on radar from Faster R-CNN and CFAR detections. \n- The experiments are not adequate to illustrate the performance. It's not clear that the dataset used in this paper. The radar points in the nuScenes dataset are significantly different from the Oxford radar dataset, especially on the density of the radar points due to different kinds of radar sensors. Besides, the results in Table 1 seems very simple by selecting some special cases from the testing set. \n\nOverall, I think the paper is not good enough to be accepted by ICLR.\n\n\n[1] Major, Bence, et al. \"Vehicle Detection With Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors.\" Proceedings of the IEEE International Conference on Computer Vision Workshops. 2019.\n[2] Nobis, Felix, et al. \"A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection.\" 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). IEEE, 2019.\n[3] Wang, Yizhou, et al. \"RODNet: Object Detection under Severe Conditions Using Vision-Radio Cross-Modal Supervision.\" arXiv preprint arXiv:2003.01816 (2020).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Reject due to Lack of Algorithmic Novelty",
            "review": "##########################################################################\nSummary:\n\nThe paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. Furthermore, the paper validates that an end2end transformer model trained on both camera and radar data outperforms an end2end transformer model only trained on camera data and hence, supports the argumentation to add a radar sensor to an automated vehicle.\n\n##########################################################################\nReasons:\n\nOverall, I vote for rejecting the paper. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. The result that camera and radar data combined will outperform camera data only is expected. Using detections in images to guide the radar reverses its advantage to work well in adverse weather conditions compared to the camera.\n\n##########################################################################\nPros:\n\n* Interesting and relevant topic\n* Training results support claims\n\n##########################################################################\nCons:\n\n* Lack of algorithmic novelty\n* Use of Faster R-CNN (slow)\n* Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting approach to CS but seems hack-y; need more analysis",
            "review": "Summary:\nIn the paper, the author(s) propose 1) a method to dynamically adjust the sampling rate on radar data using 2D object detections (algo1) and previous image and radar data (algo2); 2) an end-to-end transformer-based 2D object detection model using both radar and image data. The author(s) show experimental results on Oxford Radar RobotCar dataset.\n-----------------\nPros:\n1. The paper is easy to follow and well written.\n2. The author(s) empirically evaluate the proposed DETR model on NuScenes dataset and it beats the faster RCNN baseline.\n-----------------\nCons:\n1. Magic numbers?: in paragraph 2 of section 4, the author(s) list the way to split the regions and sampling rates for different regions, but does not explain the reason to do so. In my view, this is what the author(s) claim as a novelty “to dynamically allocate sampling rates on the different region”. It is somewhat hack-y to me and hurts the novelty of this paper. Thus I would expect the author(s) can show more solid motivations and reasons for using such a split and allocation method. \n2. The evaluation in table 1 looks too subjective to me. If I understand correctly, this evaluates the quality of different reconstruction methods qualitatively. Is this evaluation from one person? And I am not sure if the visual quality of reconstruction has a direct and strong correlation with detection quality. Btw it seems to me it is hard to tell the difference between the reconstruction visualizations in Figure 2.\n3. Need more analysis on the latency. In algorithm 1&2, the proposed method uses detection results about 0.2s  before to aid the CS. Assume a car moves 30 mph, in 0.2s it can move 2.68m, which is about 3/4 length of a car. Would this affect the performance?\n4. In the evaluation of DETR (table2), do you use the image + CS radar or just image + plain radar? I think it would be good if you can also test the detection performance using image + CS radar.\n5. Could you provide the motivation of using the transformer rather than faster RCNN on detection? There are numerous differences between these two, but it is unclear from the paper which one plays the main role in boosting the performance.\n-----------------\nQuestions:\n1. Could you explain more about why the block size is 50x100? On page 3, “The radar data is split into 8 equal regions, in azimuth and 37 equal regions in range.” Should the block size be 8 x 37?\nAlso please address and clarify the cons above\n-----------------\nPost-rebuttal review:\nI carefully read through the rebuttal and other reviews and  I would stick to my current rating.\n1. Cons 1: The rebuttal does not provide sufficient explanation on how the magic numbers are chosen (though it is somewhat explained in the rebuttal, it looks more like a design and lacks experiments to back it up: why these numbers but not other numbers?)\n2. Cons 2: I would suggest the author(s) further improve the evaluation metrics to make it more objective and convincing.\n3. Cons 3: I believe the 2.68m difference in detecting cars should be regarded as a very large error (under IoU 0.5 metric, it would be counted as a misdetection) and should be handled properly.\nI think the approach presented in this paper is interesting, and I encourage the author(s) to do more analysis to make it more solid.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}