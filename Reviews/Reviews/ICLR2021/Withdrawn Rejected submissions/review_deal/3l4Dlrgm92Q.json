{
    "Decision": "",
    "Reviews": [
        {
            "title": "Triggerless Backdoors Aren't Backdoors: Threat Models Matter",
            "review": "This paper adds to the expanding literature on \"backdoor\" attacks on deep neural network models, which work by analogy to other kinds of software backdoors by allowing an adversary to cause the model to behave in a chosen way at a time the adversary chooses. In specific, this paper proposes the concept of a \"triggerless\" backdoor attack, in contrast to attacks which rely on perturbations of the input to achieve a desired output. Specifically, the proposed triggerless attack works by using dropout on specific neural network layers, leading to a scenario where the model predicts one class when so-called \"target\" neurons are included in the model, and another, attacker-chosen class when the target neurons are not in the model. The paper implements this attack and evaluates it under a variety of attack parameter and model hyperparameter choices, finding good success at the chosen metrics, which are slight variants of standard backdoor attack metrics. Although I find the idea of training a model to have behavior which is conditioned on which neurons are dropped out, I have trouble seeing the proposed mechanism as a kind of backdoor attack. Even stipulating to this claim, I am not sure I understand the proposed value of such an attack or the scenarios in which it would be operationalized usefully. Partly, the authors suffer from broad confusion in this area of research as to the value of standard threat modeling assumptions. I could say this is not a problem the authors should solve, but it is at least a problem the authors should recognize. In specific, the paper should be clearer about how a triggerless backdoor comprises an attack in any meaningful real-world scenario. This is further underscored by the fact that the proposed attack requires multiple queries on the same input to be successful. It is not so much an attack as a misapprehension about the behavior of dropout procedures. That's not to say it's not a useful idea that could be built into something, though (see below).\n\nThreat models matter. In doing a security analysis, it is critical to establish what an adversary is trying to achieve and what capabilities and limitations are placed on the adversary in seeking that goal. Here, a \"standard\" assumption is made that adversaries can control training of the model and a less-standard assumption is made implicitly that the adversary also controls the model at serving time. Let us leave aside for a moment the obvious fact that an adversary who controls answers at serving time can give _any answer they like to any query_, and is not bound to the behavior of a specific model (although this situation can be a good model for an adversary-prepared model deployed in an environment the adversary understands, this is not discussed in the paper). Even if we assume the given setting, the attack only works if the model is queried repeatedly until the desired \"target\" neuron is dropped out. Here, I get a bit fuzzy - I don't generally think of dropout as selecting parts of a network to leave out at runtime, but rather as a technique for avoiding overfitting or shrinking a model during the training process. I could be wrong, as I'm not specifically well versed in dropout techniques, but the framing of the supposed attack seems incorrect to me. But even assuming the authors are correct, the result is an attack which works probabilistically. This is fine, particularly as the authors discover that the attack works well with only small perturbations to the model (on the scale of a single neuron in the final pre-output layer), but no application is suggested where this would be considered a valid subversion of the \"correct\" behavior of the system, making it difficult to distinguish between an attack and ordinary variation within an intended output distribution.\n\nA related threat modeling problem is that of the durability of compromise. The paper notes several alternative backdoor attack approaches and also defenses against those approaches, and claims on page 2 (twice!) that existing defenses do not mitigate against the described \"attack\", a claim which seems to rest on the fact that existing defenses inspect the input or the model for perturbations and this attack approach leaves neither sort of signature. Yet pausing to consider what defenses against an attack might render it inert or what escalation of techniques might be possible and what the resulting equilibrium outcome is might have helped the paper overcome the above problem of describing the scenario where the behavior described as \"attack success\" truly represents success in undermining intended model behavior rather than simply variation in behavior caused by choices made during dropout. In particular, the paper makes reference to both obviously perturbed input images and to techniques where the perturbation is ostensibly hidden, but then describes such triggers only ever as obviously visible, perhaps conflating \"visible to humans\" with \"visible to tools\". To make a real attack, these distinctions will matter, and taking more care around what is/isn't possible or is/isn't achieved by current techniques will help dramatically. The difficulty of detecting compromise matters, and the necessity or non-necessity of a sufficiently useful tool also matters, yet none of this is discussed.\n\nI do like the idea of training one model to have two distinct-but-predictable patterns of behavior depending on dropout choices. This could provide the basis of attacks which make weaker assumptions about supply-chain disturbances. For example, rather than giving an adversary full control over training, such behavior could be triggered by perturbations of other parts of the serve-time pipeline. Considering scenarios which would make this feel more obviously like a real attack in a realistic scenario might show ways to make weaker and more realistic adversary modeling assumptions.\n\nFinally, and I don't like to make decisions on this basis, the paper had several serious language problems that could have been fixed with a modicum of editing. For example, in the first sentence of the abstract _and_ the introduction, the subject/verb number agreement is broken.\n\nSome small points:\n* The paper says \"the adversary needs to first decide [sic] on a subset of neurons [...] that will be associated with the backdoor\", but it does not say how this determination should be made other than by selecting a layer containing those targets. Little in the evaluation section speaks to what was intended here, so I remain confused.\n* Attack step 3 says to train the targeted model with incorrect class labels \"for a random subset of batches\" but doesn't specify how many batches or how the batches are chosen (uniformly from the set of all batches? consecutively starting at a random index? Something else?).\n* Although the paper cites many valuable references, some supporting content is cited merely with footnotes to URLs for e.g. datasets. I believe most of these cites have associated papers that could be cited consistently with other supporting work. For others, you could use the footnote to describe the supporting work rather than simply accosting the reader with a URL. Similarly, use a package that allows you to sort your references (e.g., at the top of p3) either by date or by last name of the primary author.\n* I find large portions of the evaluation section inscrutable, likely because I don't fully understand the nature of what makes the \"attack\" an attack. It would help to have a baseline presented of what values you would expect for each measure, e.g. as a function of the number of queries made, and how your measurements either aligned with or differed from this baseline. The paper does describe loosely which direction we expect these measures to move, but little is given in the way of whether measures should move together. (For example, if a perfect backdoor attack has a 100% attack success rate, why doesn't this also destroy utility? Alternately, the way the paper defines attack success rate means it _must_ increase as the number of queries increases but label consistency likely decreases - you see both phenomena, but also both are inherent in their corresponding definitions?) Similarly, the paper could more clearly articulate how much of the available parameter space was mapped, beyond describing the few sweeps explicitly presented. There might be good reasons to prune some of this parameter search, but the paper doesn't say much about this.\n* Why do you only use the CelebA dataset for the Number of Target Neurons experiment? Without a justification, it feels like something is being hidden. There could be a good justification, but the paper is silent on this point.\n* The graphs in Figure 5 have no y-axis labels. I recognize this is because you're comparing scores across metrics, but you could give a generic label like \"metric scores\".",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The novelty is overclaimed",
            "review": "This paper proposed a new triggerless backdoor attack based on the dropout technique. The topic is of great significance and sufficient interest, and the paper is easy to follow and well-written. However, the threat model is unrealistic and its novelty is somehow overclaimed. Accordingly, I vote for rejection. More detailed comments are as follows:\n\n\nMajor Comments:\n1.\tThe novelty is somehow overclaimed:\n- The author claimed that ` we present the ﬁrst triggerless backdoor attack’, which is not true. Based on a most recent survey [1], at least there were two existing triggerless backdoor attacks, including the semantic attack [2, 3] and the TrojanNet [4].\n- In the Introduction, the author claimed that ` the current defenses cannot mitigate our attack’, which needs further justification. I do agree that the proposed attack can bypass trigger synthesis based defenses, which is the straightforward property of triggerless attack. However, since the proposed attack still needs to change some of the training samples during training whether it can bypass sample filtering based defenses is still unknown. Not to mention whether the proposed attack can bypass model diagnosis based defenses and model reconstruction based defenses also has no direct connection to the triggerless attack. The authors need to provide more experiments to justify this claim.\n2.\tUnrealistic threat model:\n- Although the proposed attack is triggerless, however, the attacker needs to sophisticatedly control both the training process and the inference process. Specifically, different from what is claimed in Section 3.1 of the paper (i.e., `We follow the previously proposed threat model for backdoor attacks’), the proposed attack needs to fully control the training process, not just the training data. Note that (Gu et al., 2017) only needs to modify the training set (by adding some poisoned samples) rather than the training process of the model. Besides, the introduction of dropout in the training process makes sense, but it is unrealistic to require the model to also use dropout in the prediction process unless the attacker can also manipulate the model in the inference stage. In my opinion, the introduction of dropout in the inference stage is a more difficult task for the attacker, compared to modifying the image. Please give me a real-world example to prove that the proposed attack may happen.\n- The attack mechanism itself can be simply used for defense. Let we assume that the defender has a (small) local (benign) dataset, similar to the setting used in existing backdoor defenses. The defender only needs to calculate the ASR towards each label. If there is a category with a high ASR, then it is the target label.\n\n\nMinor Comments:\n1.\tFigure 1 is hard to understand. Please provide more descriptions about it in the caption.\n2.\tIt would be better if the author can expand the font size of the caption and axis numbers. The existing size is too small to read clearly when I print the paper out. \n\nReference\n[1] Backdoor Learning: A Survey. arXiv, 2020.\n[2] How to Backdoor Federated Learning. AISTATS, 2020.\n[3] Blind Backdoors in Deep Learning Models. arXiv, 2020.\n[4] TrojanNet: Embedding Hidden Trojan Horse Models in Neural Network. arXiv, 2020.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks ",
            "review": "Summary of contribution:\n\nThe paper mainly proposes a new attack method called ‘triggerless backdoor attack ’ against deep neural networks(DNNs). First, the author introduces the current backdoor attack and defense. Then the author proposes the new thread model, and the author claim that the model’s backdoor activation probability can be controlled by selecting the target neurons and picking the dropout rate. Finally, the author provides some experimental study of the new thread model.\n\n\nStrength:\nThe idea about triggerless is interesting and some experimental results could support the author’s conclusion. \n\nWeakness:\n1. In theoretical part, the author doesn’t provide the analysis in detail about how to pick the target neurons. For example, what different if we pick neuron from the second layer and last layer. Would the number of layers in DNNs affect the attack success probability? \n\n2.  In the experiment part, the author doesn’t analysis the target neuron selecting very clearly. For example, on CIFAR10 dataset, the author only shows the experiment in a pre-trained VGG-19 model. However, the model structure could possibly affect the robustness of attack performance, and for different model structure, the target neuron selecting strategy could be different. For CIFAR10, there are other DNNs are widely use, such as ResNet. \n\n3. in experiment part, the hyperparameters evaluation needs to be explained more in detail. For example, the author only simply discusses the number of target neurons, but doesn’t claim the number of target neurons selected from each layer. For a certain number of target neuron, is it possible to have different attack performance by selecting neuron from different layers? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}