{
    "Decision": "",
    "Reviews": [
        {
            "title": "Comments on multi-view arbitrary style transfer",
            "review": "In this paper, the authors introduced the pioneering algorithms for multi-view arbitrary style transfer. Style transfer is an important image manipulation task aiming at synthesising an image that preserves the semantics of the content image while assimilating the texture of the input image to the style image. The first paper of style transfer should be by Gatys et al in 2015 or 2016. The authors in this paper try to apply the style transfer to multi-view cases. I do not think it is novel enough, which may not achieve the high level of ICLR.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposes a method for stylization of multi view imagery, maintaining consistency between views and achieving a desired style. Previous works have applied stylization to stereoscopic imagery, including Gong 2018 and Chen 2018.\n\nThe paper is missing an explanation for what the purpose of value of the algorithm is. What is this for? Who would use it? I can't think of any application for multi-view stylization. Combined with the fact that the problem statement is a straightforward generalization of stereoscopic stylization to multi-view data, I don't think that this paper makes a sufficient contribution for a top venue in the field.\n\n\nSome relevant citations on stylization of stereo or 3D data are missing:\n\nStereoscopic 3D image stylization\nLesley Northam, Paul Asente, and Craig S. Kaplan\nComputers & Graphics, Volume 37 (August 2013). Pages 389–402\n\nE. Stavrakis, M. Gelautz\nImage-based stereoscopic painterly rendering\nProc. EGSR 2004\n\nStavrakis E, Gelautz M. Stereoscopic painting with varying levels of detail. In: Proceedings of SPIE—stereoscopic displays and virtual reality systems XII, 2005. p. 55–64.\n\nSnavely N, Zitnick CL, Kang SB, Cohen M. Stylizing 2.5-D video. Proc NPAR '06.\n\nFiser et al. Illumination-Guided Example-Based Stylization of 3D Renderings. SIGGRAPH 2016.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited applicability",
            "review": "The paper proposes a method for consistent style transfer of multiple views of the same object. The main idea behind the method is to enforce pixel and feature consistency for the patches visible from different views by reprojecting them on each other and filling the gaps caused by occlusions. The papers introduce two versions of the method: iterative optimization-based and feed-forward network-based.\n\n### Pros:\n+ Visually appealing qualitative results.\n+ Interesting problem.\n\n### Cons:\n- Limited applicability. No limitations discussion.\n- Experimental section is limited. More comparisons with video stylization methods would be good.\n- The writing is too brief in some places (method) and too verbose in others (ablations related to depth and occlusion estimation).\n- The novelty is limited.\n\n\n#### Detailed cons:\n- Limited applicability. The papers only consider the experiments on the multiple views with a small baseline and no drastic changes in the camera pose. I have concerns that the method would work equally well for a larger number (100 - 1000 views) of views with drastic changes in camera pose, e.g. 360-degree flight around the object.\n- Eq. (8) scales quadratically with the number of views and would become infeasible for a large number of views. The same applies to the feed-forward version of the methods since the loss computation requires all views to fit in a single batch and is limited by GPU memory. I guess the method would not be possible to run for > 200-300 views.\n- Video style transfer works (e.g. [1, 2, 3]) seem very related to the setup considered in this paper and could be discussed as well because they produce stable video stylizations. Multiview stylization is quite similar to video stylization, especially if the camera pose changes only a bit (as in the experiments conducted in this paper). Hence, it would be very insightful to see the comparisons with video stylization methods as well.\n- Sec. 3.2 and A.1: The feed-forward stylization explanation is very brief and the paper would benefit from a more detailed explanation.\n- The captions for figures are very scarce. It improves the readability of the paper if one does not have to look-up in the main text to understand the meaning of the figure.\n- The method is very similar in spirit to video stylization methods [1,3] which used reprojection consistency losses but based on optical flow estimation instead of MVS.\n\nOverall, I think the paper is slightly below the acceptance threshold.\n\nMinor:\nEq. (3): It should be $\\sum (1 - M_{v,v'}^\\epsilon(p)) ...$ ?\n\n\n[1] Artistic style transfer for videos and spherical images, 2017  \n[2] A style-aware content loss for real-time hd style transfer, 2018  \n[3] Learning Blind Video Temporal Consistency, 2018",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Multi-view Arbitrary Style Transfer ",
            "review": "The paper introduced an algorithm that improves the visual consistency of style transfer across multi-view images. The essential contribution is identifying the source of visual inconsistency across multi-view images for conventional style transfer methods as so-called “stroke inconsistency.” According to the authors, stroke inconsistency originates from the projective distortions between scenes and the characteristic of the conventional neural networks and can be significantly reduced via performance local patch alignment before decoding.\n\nThe work described in the paper is well motivated. The idea is easy to understand, and the results are convincing. The author did comprehensive ablation studies and demonstrated the value of their novel stroke inconsistency loss and the usage of auxiliary modules, including depth estimation, occlusion estimation, and coarse-to-fine optimization. The concept of stroke consistency was also implemented in two ways: via optimization and via a feed-forward network. Last but not least, the author released their code for reproducing the results in the paper. \n\nWhile the results look promising in the submission, I think it is helpful to discuss the limitation of the method, especially insights for where the algorithm breaks (wide baseline etc.). Also, MVAST-O_CamFrame*.gif from the supplementary materials have spots in the results -- it will be useful to discuss this phenomenon if they are consistent in the experiments.\n\nAlthough the novelty is not ground breaking, I think this is a solid submission that contains a careful study of a short come in today's style transfer literature. I am happy to see it published in the conference.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}