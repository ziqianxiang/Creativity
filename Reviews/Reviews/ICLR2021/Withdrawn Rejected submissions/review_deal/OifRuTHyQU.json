{
    "Decision": "",
    "Reviews": [
        {
            "title": "Impressive results; exposition needs a lot of work",
            "review": "The empirical results, especially the visualizations, are quite impressive. The Swiss roll is fully unrolled to a rectangle, the MNIST clusters are well-separated, the smaller spheres in SpheresB are correctly clustered, and the classes in the COIL datasets are embedded with correct loop topology.\n\nOn the other hand, I found the exposition unclear and unmotivated. The method is called \"elastic locally isometric smoothness,\" but after reading the paper, I am not sure I see why it is elastic, locally isometric, or smooth. The paper is riddled with typos and awkward wording, but Section 2—comprising the theory and algorithm setup—deserves a deeper edit.\n\nMost importantly, the motivation for the choices made in the method is unclear. For example, why is the KL-like two-way divergence a better way to compare distance matrices/metric graphs? Why is this superior to the LIS loss, which compares distances directly? The paper repeatedly stresses that ELIS is more \"flexible\" than LIS due to nonlinearity: \"It imposes a more flexible nonlinear similarity-preserving constraint as opposed to the distance-preserving (isometry) constraint of Vanila LIS. More specifically, ELIS transforms a distance into a similarity metric using a nonlinear function and defines a KL loss based on similarities between nearby pairs and far-away pairs.\" The use of \"constraint\" to refer to loss terms is misleading here. And while more \"flexibility\" sounds good, it is unclear what exactly it means or why it should be true. Adding more nonlinearity generally makes problems harder, right?\n\nPerhaps the justification for these choices is purely empirical: the method achieves good results on a variety of data, so it must be better. If this is the intended argument, the paper should make this explicit. Moreover, the relative complexity of the method compared to LIS should impose a high burden of evidence. Otherwise, if there are theoretical motivations for the choices made in the paper, they should be made explicit.\n\n**Other assorted nitpicks:**\n\n- The \"dependence\" of $g(\\eta)$ and $h(\\eta)$ on $\\eta^2$ is confusing. The square function is monotonic over nonnegative real numbers, so this talk of dependence is vacuous as long as $\\eta$ is nonnegative. If you mean to make the dependence explicit, why not just write $g(\\eta^2)$?\n\n- The article asserts that \"An ELIS (and LIS) encoder can learn an NLDR transformation without the need for a decoder (as required by autoencoders), and this encoder can generalize to unseen data (that ISOMAP, LLE, t-SNE and UMAP cannot).\" This requires evidence/justification.\n\n- The table of \"merits\" on page 8 seems awfully subjective. It is OK to make claims in the text, but they should be backed up by hard data. Putting subjective claims in a table makes it look like they are measurable.\n\n- The hyperparameters require clearer explanation. The appendix states on page 21 that \"ν in input space controls the range of sensations in data space.\" What is a range of sensations? Similarly, it states that \"ν in latent space controls degree of display in latent space (show detailed information or show global information).\" What is a degree of display? Are these parameters related to scale or feature size? How do they scale relative to the data? What \"units\" are they in? Finally, the appendix uses the same parameter name, ν, for parameters in both the input space and the latent space, referring to, e.g., \"latent space's ν.\" Why not just use the layer indices as in the main text?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Implicit versus explicit mappings for nonlinear dimensionality reduction",
            "review": "The paper investigates objectives for training a multilayer neural network to map data to a lower dimensional space. It investigates an explicit non-linear dimensionality reduction approach compared to implicit approaches (multidimensional scaling, ISOMPA, stochastic neighborhood embedding, local linear embedding, etc.).  To do this, the paper proposes penalties calculated between the pairwise similarities of data points representations at different layers. The penalties themselves use the fact that the similarities are treated as a probabilities [0,1] and penalize the KL divergence. The proposed approach is qualitatively and quantitatively contrasted to implicit mappings (t-SNE and UMAP) and a recent work that also uses an auto-encoder neural network with a topologically based (persistent topology) objective. \n\nStrengths: The approach provides a way of training an explicit mapping that achieves meaningful non-linear dimensionality reduction. In this it takes cues largely from t-SNE and UMAP to define its objectives but uses them to adjust parameters of the mapping rather than the implicit points. The qualitative results seem as good or better than existing state of the art for standard data sets. \n\nWeaknesses:\nNo variation across runs is given. The performance values differ marginally and the hyper-parameter selection objective is not given. It is not clear if the qualitative differences are representative. \n\nPerformance metrics are not defined in the main body nor is their description in appendix referenced.  After looking at the appendix, it appears the linear SVM metric is pointless. Why would the representation be separable?  Overall, the set of chosen performance metrics are not that informative, coherent, or comprehensive.  Some are meaningful, others are hand-chosen to illustrate 'deficits', and the results are for one run across the board (no details of variability are given).  I would suggest a more interpretable approach such as Peltonen and Kaski's \"Generative Modeling for Maximizing Precision and Recall in Information Visualization\" in JMLR Workshop and Conference Proceedings 15:579-587, 2011.\n\nThe table describing the major merits of ELIS seems largely subjective; there is no criterion on the determination of the entries. \n\nNo description of the run time necessary to perform the entire training and hyper-parameter selection is given. Nor is there a description of how the hyper-parameter selection was done. Perhaps the parameters were chosen to optimize the supervised performance metrics, which would not appropriate for an unsupervised approach. If unsupervised performance metrics are used, then this is fine for visualization purposes, but it is not clear how long this entire scheme would take. One of the strengths of a method like t-SNE is a minimal amount of design choices need to be made besides hyper-parameters such as $Q$. Designing a neural network architecture is not yet non-trivial. \n\nThe paper does not provide any new perspectives or theory to support the approach. \n\nThe paper needs careful proofreading.\n\nConclusion:\nThe paper is a borderline contribution. It could be useful in practice; however, the experimental design, description, and results need improvement. Primarily, is not clear what is entailed to choose tune the network to get this explicit form versus using implicit mappings (UMAP or t-SNE). No new insight or theory about manifold learning are discussed. \n\nSuggestions: \n\nThe paper claims \"ELIS is aimed to surpass LIS\".  LIS takes on a similar approach of neural network mapping but uses a nearest-neighbor distance preservation simultaneous with a repulsion term. One page is devoted to this method, but it is not necessary to understand LIS to understand ELIS. Thus, I don't find the inclusion of LIS useful. \n\nIn introduction, it is not clear that t-SNE and UMAP should be considered \"traditional machine learning\". They are techniques that optimize implicit mappings without learning anything.\n\nIn abstract, \"UMAP and t-SNE for and\" has extra for.\n\nExtra \")\" after Bronstein in second paragraph.\n\nThe phrasing at top of page 2, \"avoid the transformation from collapse, twisting, or crossing\" is not grammatically correct or mathematically clear. Later, not clear what \"straight distance-preserving\" is. \n\nPerhaps it would be better to say \"nonlinear, but monotonic in distance\" rather than \"nonlinear in distance\". \n\nIt is a bit of overreach to say \"for which none of the methods can achieve\". The performance metrics and objectives differ, and the hyper-parameter selection is not stated clearly as in the TopoAE paper. \n\nThere is an extra parenthesis in equation 1. \n\nShorthand $d_{ij}$ is not defined. \n\n\"the influence of a point on the others is propagated to afar. \"\n\nIn section 2.2, it is not clear to the reader what specific parts are \"Following UMAP\". \n\nEquations (5) and (6) are said to be 'similar' to t-distribution when in fact it is is nothing more than a squared and scaled t-distribution function. The approach borrows heavily from SNE and t-SNE in that the scaling of each point \" is estimated from the data by best fitting the equation\" \"for the perplexity-like hyperparameter Q given\" , which after rewriting (to fix the grammar) would be clearer. \n\nAt the bottom of page 4 it is not clear to the user what \"continuation tool\" is. \n\nAfter equation 8 \"On the other hand\" is dangling without a preceding \"On the one hand\". \n\nWhen discussing the choice of neighborhood scale parameter $\\sigma_i$ \"the necessity becomes not so demanding\" should be rephrased. \n\n\"Formulating the ELIS losse\"\n\n\"only needs to minimize second part\" -> \"only needs to minimize the second part\" \n\nTable A1 \"Coli100\"\n\nA.2  References on performance metrics are missing. Details on performance metric are not uniform. Different parameters are used for 'continuity' and 'trust' than on other ones.\n\n\"as a autoencoder- based mehtod\"\n\n\"resules\"\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Pre-Mature Work",
            "review": "This manuscript proposes a model named Elastic Locally Isometric Smoothness (ELIS) that is claimed to help and perform all the tasks in the sol called \"Manifold Computing\" tasks, ranging from visualization, nonlinear dimensionality reduction, manifold data generation, and learning representations. The claim here is that the proposed model imposes, some how, constraints on the geometric isometry across the network layers to preserve the local geometric structure of the data manifold. The ELIS model is based on another, unknown model/framework (and yet unpublished), or the so called: \"Markov-Lipschitz\" Deep Learning [Li et al. 2020]. The main technical contribution here is that ELIS, can handle cases/scenarios that cannot be handled by its predecessor, Locally Isometric Smoothness (LIS) again due to [Li et al. 2020], such as highly nonlinear manifolds, non-Euclidean challenges, bridging gaps between non-Euclidean manifolds in the input space and the resulting Euclidean hyperplanes!\n\nComments:\n\n1.  Eq. (1) which seems to describe LIS raises some concerns; it's the difference between two distances in the two different spaces: one for the input before the transformation \\Phi function, and the other one is the output of the transformation function. Unfortunately, as presented in the manuscript, these two distances are not comparable and hence they have different scale; this is similar to performing a comparison operation on two different scale systems: metric vs. imperial, say. This is especially more concerning since \\Phi is usually a nonlinear transformation.\n\n2.  There is a lot of mix with back and forth discussions between LIS and ELIS; LIS is a recent 2020 not published work yet and the Authors are treating it as a legitimate ground truth. In fact, various parts of the manuscript seem to be repackaging and re-selling LIS to the reader and it confuses the reading and the understanding of the manuscript.\n\n3.  What is also disturbing in the current version of the manuscript is the over-selling and high-pitch on the power and capabilities of LIS/ELIS; it seems this bad habit of various Authors in the deep learning regime have been spreading constantly to so many papers these days. The manuscript has a significant amount of subjective statements than cannot be justified even by simple logical arguments. Here are few examples:\n\nEx. 1: \"Such local homeomorphisms avoid the transformation from collapse, twisting, or crossing, so as to improve generalization, stability, and robustness.\"\nHow local homeomorphisms will improve generalization, stability and robustness, assuming that the proposed model/constraint really preserves local homeomorphisms.\n\nEx. 2: \"LIS has demonstrated significant advantages in manifold learning and NLDR\" ?!\n\nEx. 3: \"Extensive experiments, comparisons, and ablation study demonstrate that ELIS-based neural networks produce results not only superior to the SOTA t-SNE and UMAP for NLDR and visualization but also better than other algorithms of manifold and autoencoder learning, including LIS, for NLDR and manifold data generation.\"\nThe results in Table 2 do not support this claim. In fact, ELIS results are insignificant from UMAP results and hence one can easily question the merits of LIS/ELIS. A\n\nEx. 4: \"It inherits the metric-preserving property of LIS so that the resulting layer-wise transformation is geometrically smooth, hence topologically homeomorphic, yet possesses more flexibility than LIS in handling highly nonlinear manifolds in high dimensional spaces.\"\nFirst, LIS is not known nor proved to have such properties. Second, any mathematical argument or result to support the claim that a geometrically smooth layer-wise transformation will lead to a topological homeomorphic transformation?? What is the definition of a geometrically smooth function?\n\nEx. 5: \"Both ELIS and LIS aim to accomplish the following 4 tasks, of which few neural networks can do all:\"\nAny particular examples to support this claim?\n\nThe paper and the work are still not mature yet.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "What is the problem you are solving?",
            "review": "This is yet another NLDR paper. But I don't see any shining point here. It is a marginal modification of UMAP, parameterized by neural networks.\n\nWhat is the problem you are solving? You must be aware of tens of existing NLDR methods. Have you identified a severe problem among them and does your method solve the problem? Throughout the paper I don't see any exsiting problem but just an non-target proposal.\n\nIf your method preserves locality better (just guess from your title), then your experiment should quantified the improvenment in this sense. However, now Section 3 uses a lot classification benchmarks.\n\nIf your methods aims at showing clusters better, clearly ELIS loses to UMAP. Once the colors are removed, users cannot see the cluster boundaries in ELIS visualizations.\n\nI doubt the Coil20 NNACC for ELIS-Enc. For 1440 instances, 0.9965 NNACC means there are only five misclassified points. But as shown in Fig.2(a), several classes, e.g.,(3), (4), (6) are heavily mixed. \n\nIt says ELIS is scalable to large datasets, but all experimented datasets are quite small.\n\nThe name ELIS is not good. LIS is just a rename of multidimensional scaling (MDS), and reference to MDS is missing.\n\nCaptions of figures and tables are too short. Descriptions should be moved from the text to the captions.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}