{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting assumption with comprehensive experiments.",
            "review": "Summary:\nThe paper verified an assumption that during natural training, BN shifts the model to rely more on non-robust features (NRFs) instead of robust features (RFs). \nOverall, the paper is well writing and easy to follow. The experiments are comprehensive, they addressed the problem in many different aspects and explored the possibilities almost exhaustively. \n\npros:\n1.  The problem may help in understanding the reason for adversarial robustness.\n2.  Totally disregard BN may not possible, however, the finds in the paper may give us some insights into improving adversarial robustness, like controlling the distribution of input features.\n\ncons:\n1. I noticed that both $\\ell_{\\infty }$ and $\\ell_2$ used in the evaluation is small. What if use like 8/255 $\\ell_{\\infty }$ ball as Madry's adversarial training suggested? I guess in large $\\epsilon$ the performance gap may decrease, it will make the assumption a little bit trivial. \n2. All robust accuracy is evaluated by PGD 20. I would suggest some more powerful attacking method like MI-FGSM in \"Boosting\nadversarial attacks with momentum\".\n3. \"Adversarial Examples Improve Image Recognition\" also claimed that clean and adversarial examples are in different distributions so that one BN is not appropriate to train them together, I think it needs to be discussed in related work.\n\nI hope the author can address my concern especially the first one during the rebuttal.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "[Summary]\nThis paper studies an interesting phenomenon that batch normalization (BN), in *natural training*, may decrease model robustness to adversarial perturbation. This paper proposes a hypothesis that BN tends to shift the training to leverage non-robust features (NRF). It then performed various experiments to validate the hypothesis.\n\nThere are two broad categories of experiments: (1) Examining LIGS, which shows that training with BN leads to smaller LIGS (as represented by Figure 2), which thus shows that BN is learning NRF. (2) Examining the training on disentangled features. This is to leverage the work by Ilays et al. to extract RF and NRF from a model, and then training on these datasets. Here we also train on random features (which is entirely non-robust). As represented by Figure 4, it finds that when training on random features with BN, the model *can converge* and get a very low LIGS. This offers another evidence that BN is leaning non-robust features.\n\n[Assessment]\n1. I am not entirely sure whether the authors want to claim that decomposing robust-usefulness into (natural) usefulness and (pure) robustness is a (key) contribution. From pp 4., this seems to be the case. But I'd say that this idea has already been proposed many times before. For example, TRADES (Zhang et al., 2019a) has proposed to decompose the risk into natural accuracy (usefulness) plus a term that is only about the stableness of predictions in a neighborhood. The measurement using gradient vectors is also not new. So I view the definition of LIGS more along the line of a new application where we use it to study BN.\n\n2. The result with random feature training with the training w/ BN can converge is interesting. And the fact that LIGS gets very low is a clear sign that BN is shifting the training to learn NRF.\n\n3. The paper seems to make a larger point that F-robustness (i.e., pure robustness say in view of LIGS) should be evaluated even in the natural training (e.g., the abstract says \"while we believe evaluating F robustness is equally important, for which our work fills the gap\"). While this point is interesting, it needs more support because this paper only studies the utility of such evaluation for BN. In that sense, this paper seems to have two stories (one is about BN learning non-robust features, one is about how evaluating robustness is useful for DNN study in general).\n\n4. The writing and organization of the paper is good.\n\n5. All in all, I think this paper is interesting and there's good evidence to support that BN learns NRF in natural training. However, I am not really sure about the utility of the results because in adversarial training, which explicitly targets at learning RF, this effect of BN will go away after all, and it needs more evidence to support that evaluating pure robustness will be useful for DNN research in general (i.e., even for the natural training case). For these reasons, I vote for a weak reject.\n\n[A related but independent remark]\nMadry et al.'s recent work studies why BN works (https://arxiv.org/abs/1805.11604), and it argues that BN will smoothen the optimization landscape. They provided both theoretical and empirical evidence. Can this paper provide some theoretical evidence about BN learning NRF? (for example, a toy theoretic model where after we smoothen the optimization landscape, we learn NRF, just cooking up some ideas).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A paper with extensive empirical studies but limited theoretical insights. ",
            "review": "Summary: The paper investigates vulnerability of batch normalization by empirical studies on robust and non-robust features and proposes a metric for the robustness of DNNs. To validate the conjecture that BN models rely more on NRFs than RFs, evidence on adversarial robustness, corruption robustness and feature transferability is given. Based on the proposed robustness metric, LIGS, robustness and usefulness of neural networks are considered separately and the correlation between LIGS and standard training, adversarial training or dataset features is investigated. Other factors like network width, depth, optimization, shortcut connections etc are also investigated in the experiment. \n\n\nPros: \n1)\tThe paper gives substantial evidence on the conjecture that batch norm learns NRFs. Besides the adversarial accuracy, sensitivity to corruption and feature transferability of several normalization methods are also included as the evidence. It is interesting to see the impact of different normalization methods on the robustness to different corruptions and relate this to adversarial robustness.\n\n2)\tThe correlation between the proposed LIGS and many factors in DNNs is thoroughly investigated, like adv/std training, features in the dataset, width, depth, activation functions etc. \n\n3)    The writing of this paper is clear and easy to follow.\n\n\nCons: \n1)\tSome experiment designs in this paper are similar to [1] like the feature extraction in the transferability demonstration, D_R, D_NR and D_rand in disentangling RF and NRF. Although LIGS is somehow a new measure for robustness, the reason of using LIGS instead of other metrics, e.g. [2] and [3], is not well demonstrated. \n\n2)\tIn contrast to extensive experiment results in this paper, there is no discussion on the mechanism behind these observations. Like why does a batch norm DNN eventually learn non-robust features? What are possible reasons for the empirical results in this paper? (The argument that DNNs with batch norm learn non-robust features is not an explanation but a description of this phenomenon. )\n\nComments and Questions: \n1)\tDoes resnet without BN in Table 1 and Table 2 also use fixup init as in Figure 9?\n\n2)\tWhat is the relationship between NRFs and high-frequency features? If NRFs align with high-frequency features, does that mean NRFs always have more high-frequency components than RFs? Is there any evidence on this conjecture?\n\n3)\tCould the author explain more on D_conflict? In Figure 7, does each line denote a network that only learns one class, i.e. the output of the network is a scalar? \n\n4)\tFigure 9 shows the influence of shortcut connections on robustness. Could the authors explain what the influence of shortcut connections is on robust accuracy? \n\n5)\tDisentangling robustness and usefulness is probably not reasonable. An absolutely robust function is a constant function, i.e., one will get the same output no matter what the input is. But that network is apparently not what we want. Could the author explain the reason why we should disentangle the two properties and its impact on future research?\n\n6)\tThe LIGS measure may not reflect robustness in some cases. Assume that a DNN has the linear activation function, then the LIGS is always 1 no matter what the weights are. But deep linear networks with difference weights have difference levels of robustness. For example, Wx and 10^10*Wx have different robustness but the two networks have the same LIGS. \n\n\n\n\nMinor: \n1)\tFigures in this paper are not in high resolution and may prevent readers checking more details. When there are several subplots in a Figure, it is better to add index to each subplot. \n2)\tIn Section 4.2, second paragraph, the sentence should be “…Here, to demonstrate the effect of BN on either NRFs or RFs, we report the results trained on…”\n\n\n\n[1] Ilyas, Andrew, et al. \"Adversarial examples are not bugs, they are features.\" Advances in Neural Information Processing Systems. 2019.\n[2] Qin, Chongli, et al. \"Adversarial robustness through local linearization.\" Advances in Neural Information Processing Systems. 2019.\n[3] Moosavi-Dezfooli, Seyed-Mohsen, et al. \"Robustness via curvature regularization, and vice versa.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Initial review",
            "review": "This paper shows that batch normalization (BN) and some other normalization techniques like Instance Normalization (IN), Layer Normalization (LN), and Group Normalization (GN) will increase adversarial vulnerability and conjecture that it is because BN shifts the model to rely more on Non-robust features (NRFs) instead of Robust Features (RFs). They support this conjecture with some empirical evidence. They also disentangle the robust usefulness of model features into usefulness and robustness by proposing a metric called Local Input Gradient Similarity (LIGS) to measure “pure” robustness independent of usefulness. With this metric, they find BN and IN/LN/GN decrease the robustness and BN is crucial for learning NRFs. They also show other network structures and optimization factors have no significant influence on the model robustness. Then they conclude that normalization is the only factor found to have a significant and consistent influence on the model robustness. \n\nI think the main contributions of this paper are: 1. Use robust features vs. non-robust features perspective to explain the phenomenon that batch normalization will make the standard trained model less robust to adversarial attacks; 2. Propose Local Input Gradient Similarity (LIGS) metric to measure “pure” robustness which is independent of usefulness, and use it to explain the phenomenon that BN decreases model robustness; 3. Some other interesting findings like the model first learn RFs and then NRFs. \n\nBut I have the following concerns: 1. Is the phenomenon that BN decreases adversarial robustness due to the effect of “gradient masking”? I notice that the attack strength used in the paper is weak (e.g. $\\epsilon=1/255$ and attack steps=20), what about stronger attack strength, like using $\\epsilon=8/255$ and attack steps=100 on SVHN and CIFAR-10? Also, could the authors try auto-attack proposed in [1] and report the results to see if BN still increases adversarial vulnerability? 2. As shown in the paper, if we use adversarial training, we don’t have such a phenomenon. Then I am wondering whether the finding of this phenomenon is useful. If we don’t consider adversarial robustness, then we can use standard training with BN which could lead to better clean accuracy. But if we care about the adversarial robustness, then we need to use the adversarial training and BN doesn’t harm the robustness in this case. So in both cases, we can use batch normalization, but it seems that this paper proposes that we should not use BN as it could hurt the robustness. I hope the authors could clarify the practical value of their findings; 3. Most claims and explanations of experimental results are just conjectures. It is hard to know if they are true or not. It would be good if the authors could show some rigorous proofs, even in a simple setting like Gaussian data distribution and shallow neural networks. 4. I don’t quite understand why we need to disentangle robust usefulness into usefulness and “pure” robustness. With F usefulness and F robust usefulness, we already could distinguish RFs and NRFs since NRFs are not robust useful while RFs are robust useful. Based on the definition of RFs and NRFs, they are both useful features. Thus, we cannot determine whether a feature belongs to RFs or NRFs just based on “pure” robustness. It is possible that a feature belongs to “pure” robustness, but is not useful (e.g. a feature that maps all inputs to a constant); 5. It is not very convincing that local linearity could be used to measure “pure” robustness. I think it is not a direct measure of “pure” robustness and the authors should give a formal definition for “pure” robustness. \n\nBesides, I think the writing needs to be improved. It is hard for me to understand some experiments and claims: 1. Could the authors give detailed descriptions about the setup of Feature Transferability experiments? What does the new dataset constructed by extracting features look like? How to do cross-evaluation? What’s the metric used to measure transferability? 2. Could the authors explain the dataset of conflicting RFs and NRFs a bit more? What’s the target class t? And why could we deduce insights from the clean accuracy by using such a dataset? \n\nMinor things: 1. Figure 2 lacks legends. 2. Figures are too small. \n\n\n[1] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" arXiv preprint arXiv:2003.01690 (2020).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}