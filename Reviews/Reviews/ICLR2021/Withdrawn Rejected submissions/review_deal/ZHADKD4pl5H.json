{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The rationality of the proposed method, especially its implementation detail, is challenged by the reviewers. Additionally, the experimental part and the writing of the paper should be improved. According to the feedback of the reviewers, I don't think this work is qualified enough at its current status. "
    },
    "Reviews": [
        {
            "title": "Limited novelty and insufficient experiments",
            "review": "This work studies the missing data issue in the graph-based learning tasks, including node classification and matrix completion. The idea is to perform dimension reduction of the observed features. Then, allow these features to diffuse over graphs by computing barycenter update to fill in the missing features. The prediction is performed by adding MLP over the obtained features. The paper is written well and easy to follow. However, the idea reads very heuristic with some potentially issues. The experiments read weak. \n\nFirst, regarding originality, this work seems to combine some previously well-studied techniques: Use optimal transport to complete missing data; Graph-based semi-supervised learning. For example, [1] discussed using optimal transport to fill in missing data, which is not referred in this work. [2] studied label propagation over graph via optimal transport, which is also not referred in this work. The idea in this work seems to be a combination of techniques from different previous works. It is okay to combine techniques but one needs to sufficiently demonstrate the effectiveness, which have not been well done in this work. \n\nSecond, it reads weird to use exp() followed by a softmax to obtain a discrete distribution. \"exp\" function is not properly transformed function to keep the feature information. For example, if some components in the features obtained by SVD have a few negative ones and a few large positive components. Of course, both the negative and positive features are important. However, if we adopt the method proposed in the work, the model only tends to capture those large positive components and ignores those negative components (due to the exp function). Later, the authors use l2-distance (a very component-wise well-balanced metric) to define W-distance. This yields a very imbalanced treatment on the features.\n\nThird, the experimental section is not persuasive. It is widely know that GCN does not work very well for node classification tasks. There are many other models, e.g., APPNP [3], performing  good node classification over homorphilic networks. These better baselines should be used. Moverover, for only topological information, GCN without node features is not a strong baseline. Better methods with only graph structures should be traditional methods, such as label propagation. Moveover, to demonstrate the effectiveness of this approach, experiments should also be done over heterophilic networks.                \n\n\n\n[1] Missing Data Imputation using Optimal Transport, Muzellec et al., ICML 2020\n[2] Wasserstein Propagation for Semi-Supervised Learning, Solomon et al., ICML 2014.\n[3] Prediction and propagation, 2020",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Wasserstein diffusion on graphs with missing attributes\"",
            "review": "Summary and contributions: Briefly summarize the paper and its contributions:\nThe paper considers node representation learning in the setting of incomplete node attributes. The proposed method transforms node features to a latent space endowed with the Wasserstein metric, uses the Wasserstein barycenter of nodes’ neighbors to recover missing features, and finally transforms them back to their original space using a proposed inverse mapping. The algorithm has been tested on benchmark classification tasks as well as matrix completion tasks.\n\nStrengths: \n+ The paper appears to be a novel application of ideas from both matrix completion and optimal transport. \n+ In the node classification experiments, multiple benchmark datasets were used and multiple methods to complete missing values were considered as baselines. \n\nWeaknesses: \n- While the experiments show promising results, the authors do not explain the choices  behind their method. \n- Why do the authors use SVD and define the distance between transformed features in the way they did? Is there a reason for using the exponential function in the transformation other than for inducing positiveness of probability? How does the inverse mapping come up and why does it work? \n- In the classification task experimental section, it is not clear why methods, like IGMC (used in Section 4,2) paired with an MLP, weren’t tested for the classification tasks. As for the choice of architectures, is there a reason for using 7 WGD layers versus 2 GCN layers for the baselines?\n- Because many choices are not justified or described, a number of questions are left unanswered. What is fundamentally different between a WGD layer and a GCN layer? What is the influence of the number of WGD layers? And what is changing in the representation after each WGD layer? What does the induced latent space look like? How does it relate to the complete feature information setting? Is the dimension of the latent space equal to that of the original space? Why does feature recovery using Wasserstein barycenter update trumps simple average or k-means of neighbors, what makes the metric be more adequate than Euclidean distance? \n- The authors claim that their work “is the ﬁrst work to compute embeddings of a graph with incomplete attributes directly.” but their proposed algorithm is two-stage, first matrix completion and then neural network training. The paper seems to be more relevant to data pre-processing than representation learning itself. \n- The authors conducted experiments in two settings, partially missing and entirely missing. It seems that the performance of the proposed method is exactly the same in both settings. (Figure 1) This is curious. Can the authors comment on what is happening in this case?\n- In multiple instances, the authors introduce experiments but don’t comment on them or explain the similarity or differences in performance between all of these experimental settings.\n\nClarity: \nMany terms are used without properly defining them; the equivalence of p-Wasserstein distance to eq. (4) is unclear unless readers are already familiar with optimal transport; the iterative Bregman projection and Gram-Schmidt process have not been elaborated clearly. The statement “However, most of the methods, which embed nodes into a lower-dimensional Euclidean space, suﬀer from common limitations” is in reference to GCNs which are not limited to low-dimensional spaces. So it’s not clear what the authors meant by this.\n\nRelationship to prior work: \nThe state-of-the-art alternatives have not been well-addressed and compared with. A lot of work has been done with graph convolutional networks and in this literature, the issue of missing attributes is very common. There is no mention of matrix completion methods in related works, these methods were used in Section 4.2, without explaining how they are different from the proposed method, or why this set of methods was selected for comparison.\n\nAdditional feedback:\nMultiple typos are present throughout the paper, these can be addressed to improve the readability, for example:  In section 3.3 “expect” should be “except” in the sentence: Note that it is similar to the message aggregation in graph neural networks, expect we do it in a Wasserstein space and introduce no parameters.\nIn section 4.1 “some entities of the feature matrix is missing” should be “some entities of the feature matrix are missing”. “90% attributes” should be “90% of attributes”. “Pumbed” should be “Pubmed” in “on Cora and Pumbed datasets.”",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning graph representation with missing node features using Wasserstein diffusion",
            "review": "In the paper, the authors proposed a framework to learn representation for graphs with node attributes that are allowed to be missed.   The learning process includes two steps: (1) node features transformation using SVD which allows representing a node as a discrete distribution with support points of orthogonal basis vectors. (2) message passing and updating the node distribution using Wasserstein barycenter, i.e. one node representation is the barycenter of its neighbors and itself.  Two downstream tasks were used to demonstrate the performance of the proposed methods:  node classification and multi-graph completion.\n\nOverall, the idea is interesting however the writing of technical ingredients is not convinced enough. There are points not clear and can be improved to get a better version of the paper.\n - For the ground metric in Eq. (3), what is the rationale for choosing as the norm (not sure which norm) of square eigenvalues?\n - The paragraph including Eq. (6) (\"Recall that the set ... notoriously difficult to solve.\") does not contribute any further information as it is not used in the rest of the paper.\n - In Eq. (9), do the inverse map of  X need to be consistent with the observed feature of that node?\n - How to handle SVD with missing nodes? It seems that if the graph is 100% missing features, i.e. the graph with no features, the method does not work as we can not perform the SVD step.\n - Is there any convergence guarantee for the prosed Wasserstein diffusion process in Eq. (7)? If there is no rigorous proof, an intuition justification is the least.\n - Some SOTA baseline methods such as GCNmf [1] or methods therein (e.g. GAIN) (if you think the paper is not peer-reviewed yet) should be compared with.\n\nThere are several typos and mistakes:\n - Page 3, \"matrixV\"=> \"matrix V\"\n - Page 4, \"sqaure root\" => \"square root\"\n - Page 4, \"In practice, We use\" => \"In practice, we use\"\n - Page 6, \"training process with patience 100,\" ?!\n - Table 1, to tight table caption\n\n[1] Taguchi, H., Liu, X., & Murata, T. (2020). Graph Convolutional Networks for Graphs Containing Missing Features. arXiv preprint arXiv:2007.04583.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This is an interesting paper that contributes to a developing recent body of literature applying Wasserstien methods to graph learning problems. The reported results are competitive with state of the art methods for label identification and matrix completion and the methodology is natural and straightforward to implement. ",
            "review": "This paper presents a Wasserstein diffusion based method for estimating missing node labels on attributed graphs. The algorithms use linear algebraic decompositions to represent the node labels in a low dimensional space and then uses a Wasserstien Barycenter approach to implement the diffusion before lifting back to the original feature space. None of these components is novel in this setting but the combined algorithm is interesting and the provided code is helpful and demonstrates the naturalness and ease of implementation of the proposed model. \n\nThe empirical experiments are beyond sufficient given the space constraints and highlight the flexiblity of this formulation of the problem. The application to multigraph completion is particularly interesting - this is related to several other recent problems of interest around node embeddings and inference around families of graphs on common node sets that would be an interesting extension of this work. \n\nThe typesetting of text in equations should be placed in something like \\operatorname so that it isn't squashed and italicized. Also the final table needs to be reformatted, as it is difficult to parse currently. \n\nTypo second to last paragraph of page 5 low-rand -> low-rank",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}