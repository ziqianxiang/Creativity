{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review report",
            "review": "This paper extends TRADES and aims to improve both standard accuracy and robustness by leveraging explainable feature disentanglement. I have several concerns about this submission.\n\n1)  The presentation of this paper should further be improved.\n \n1-a) Informative figures and illustrations are required. For example, it is very difficult for readers to acquire ideas behind Figure 3 and Figure 4.\n\n1-b) Precise statement for the important concepts is needed. For example, the authors claimed that \" Then the counter fact is: we only have the relative non-robust features, model robustness should be lower than the fact. \" What does ' model robustness should be lower than the fact ' mean? Another example is \"The counter-factual features should have lower robust loss than the factual features, which can be formalized below\". The definition of a counter-factual feature is vague. How to determine the binary mask?\n\n1-c) Informal mathematical presentation. Since Eq. (15) is the proposed eventual objective function, it will be better to clearly state the optimization variables and prior information used for the model training.\n\n2) In Table 2, it achieves improved robustness and accuracy over TRADES. However, it is not clear why the proposed approach outperforms. More explanation and ablation studies are required. \n\n3) I wonder what useful insights are provided by Sec. 4. It would be better to clarify the connection between empirical results and the proposed theoretical results. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The intuition is a bit difficult to catch, worried about experiments",
            "review": "This paper proposed new methods for defending white-box adversarial examples. The method is a three head model with corresponding loss. If I understand correctly, the feature representation of the last layer (or the second from last layer depending on understanding) will go through three heads: trade-off, benign and adversarial. The three heads with corresponding losses are defined to disentangle robust features and non-robust but more accurate features. \n\nPros\n+ The experimental results are good. Better then TRADES on CIFAR-10.\n+ Disentangling robust/non-robust features seems to make sense. \n\nCons\n- It is hard to believe the disentanglement, which is done by three-heads with corresponding loss at the last layer, could have a significant effect on the robustness of the entire model. Could the authors provide more insights?\n- Somewhat related, how many more parameters are introduced in the three-head model that can potentially increase the performance?\n- Just for sanity check, how are \"adaptive\" adversarial examples generated for the three-head model? Are the three heads considered?\n- What about training speed?\n  \n\nMinor comments:\nThe resolution of figures could be improved.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "**Summary**\nThe authors present a new architecture for adversarial defense based on the observation that neural networks tend to weigh both non-robust and robust features requally. They use a counterfactual manipulation to motivate their approach and further train a model with multiple outputs corresponding to robust and non-robust features. Each output branch is trained with either benign, adversarial or a combination of benign and adversarial data, while simultaneously minimizing a KL based counterfactual loss to ensure disentanglement of robust and non-robust learned representations. They show empirical resutls on CIFAR-10 showing higher robust accuracy for multiple attacks. They also present theoretical analysis regarding the number of heads required for robustness as well as an additional application showcasing that robust features can be used for image generation.\n\n**Strengths**\n1. The architecture is a novel approach to defenses, leveraging observations from Ilyas et al.[1], and extending TRADES [2]. \n2. I like the case study analysing the common features between two classes learned by the TRADES model which motivates their approach. \n2. The experiments are few but convincing, and show improvement in standard and robust accuracy for PGD and CW attacks. \n3. The image generation experiments support similar observations by Ilyas et al. and shows that the robust network supports interpretability.\n\n**Weaknesses and Clarifications**\n\n1. The theoretical analysis seems wrong. Even for a small $|S|=10000$  and high $p (>0.9)$, the lower bound for $n$ is higher than 3, which contradicts their empirical observations. It is also not well presented and has conflicting notations and missing assumptions. \nCould the authors to clarify if each $x_i$ is considered independent and identical in the given setup? Further, if so, the tighter bounds can be obtained by simply treating the sum of $x_i$ as a binomial random variable. If not, how is $p$ related to $p_i$s.\n2. The authors claim that experiments have also been done for SVHN. They are missing from the paper. \n3. According to Section 2, the counter-factual loss is enforced on $f(x)$ (before the trade-off head. Why then is the trade-off head used as the input to the robust and non-robust heads? Should the respective heads not just look at the disentangled features themselves? Is the counterfactual loss contributing to the final performance, or is it just the selective training of the two heads?\n4. I also suggest that the authors repeat experiments with various attacks, and a wider variety of parameters. This would allow for a fair comparison with TRADES.\n\nThe paper appears to be incomplete and needs work. I do not think it is ready for publication in it's current state.\n\nMinor comments:\nThe caption for Fig. 5 is missing. \n\nRefs:\n1. Andrew Ilyas et al. Adversarial examples are not bugs, they are features.  NeurIPS 2019.\n\n2. Hongyang Zhang et al. Theoretically principled trade-off between robustness and accuracy. ICML, 2019.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}