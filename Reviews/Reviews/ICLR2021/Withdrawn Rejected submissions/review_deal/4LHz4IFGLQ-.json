{
    "Decision": "",
    "Reviews": [
        {
            "title": "In-depth work from a theoretical point of view, but the experimental side is lacking",
            "review": "Having shown the equivalence of classical planning problems and right-regular grammars, the authors take the interesting step of formulating text generation as a planning problem using the STRIPS formalism.  They propose a model based on the classic Continuous Bag Of Words approach, and learn a set of \"propositional\" word embeddings. These are effectively sets of propositions, compatible with planning solvers, which can then be used to perform tasks such as paraphrasing.\n\nAt 35 pages, this paper is extremely information dense, and often crucial details have to be relegated to the appendix. I expect a large number of researchers in NLP will be unfamiliar with the formalism of classical planning theory. The authors do a reasonable job of introducing this in sections 2 and 3, but I found the parts of section 4 introducing the proposed model to be harder to follow. The significance of reformulating CBOW is not immediately clear, and the final paragraphs of that section required multiple readings.\n\nMy main comments are regarding the evaluation section:\n* First of all, word analogy tasks, which play an important role throughout the paper, have been shown to be a poor measure of embedding quality (Linzen, 2016; Schulter, 2018, inter alia).\n*  Word similarity tasks are fine, but are rather simple and provide limited insight into what the models are learning. Using something like the \"probing tasks\" of Conneau et al. (2018) could have provided more insight into the level of syntactic and semantic knowledge that is present into the proposed approach.\n* Why reimplement CBOW, instead of using one of the many implementations? It's easy to miss details when reimplementing other algorithms. Using a battle-tested implementation can avoid a lot of pain (and shuts reviewers up). What makes me a bit suspicious of the authors' implementation (or the training settings) is that the results on some of the individual similarity datasets (Table 8 in the appendix) are below other published results. Taking, for instance, the dataset of Hill et al. (2015), the dataset's webpage reports a Spearman correlation of 0.37 for a 300-dimensional word2vec model trained on 1B words – this is higher than any of the figures reported in the paper. Similarly, one can find results for WS353 on the ACL wiki, and even LSA outperforms all of the figures in the paper.\n* For the classification tasks, there are several popular classification datasets which could have probably been selected, such as those in Conneau et al. (2018), or even the more recent GLUE.\n* Finally, I would have liked to see some external baselines reported, to give readers more context on what the current state of the art is and what comparable models can achieve.\n\nReferences:\n* Linzen (2016) Issues in evaluating semantic spaces using word analogies. ACL workshop on Evaluating Vector Space Representations for NLP.\n* Schluter (2018) The Word Analogy Testing Caveat. NAACL. \n* Conneau et al. (2018) What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. ACL.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "ambitious, maybe promising, but currently inscrutable",
            "review": "This paper describes a way to obtain word embeddings that have a discrete flavor that corresponds to aspects of classical planning. Methods from VAEs and discrete representation learning are used to learn the word embeddings from text and the embeddings are evaluated by using them to embed words and sentences and considering word similarity tasks, analogies, and text classification. \n\nThere are many fields colliding in this paper, including word embedding learning, latent variable modeling with variational methods, classical planning, neural modeling for planning (cube-space autoencoders, etc.), and grammars. As a result, the paper will be very difficult to follow in its current form for the vast majority of ICLR attendees. I think the paper would benefit by spending significant effort to simplify and clarify the presentation. I have many questions and concerns below that hopefully will be helpful for the authors to revise their paper, but in many places I was unsure of what exactly was going on, so I'm not sure how helpful my comments will be. I think there are some interesting things happening in the paper, so I would be interested to read a version that is easier to understand. \n\nI've numbered my points below for ease of communication. \n\n1. The description of the CBOW model in Sec 2 describes a sum of context word embeddings to represent the context, but all other descriptions that I've read of CBOW (including the original paper) describe an average rather than a sum. Since the authors used their own implementation for the experiments, I'm concerned that their implementation may have used sum rather than average, which may mean that the CBOW baseline results are weaker than they should be. \n\n2. The details about the discrete latent variable word embedding model are relegated to the appendices and even there are quite short. I am concerned that readers may conflate the use of the binary concrete distribution with a binary concrete VAE. The binary concrete is more general and can be used in other settings than a VAE, while this paper describes it as if it is only a type of VAE. I think the authors should explain their discrete latent variable model first and only after that discuss VAEs, the ELBO, and the binary concrete distribution. Appendix B.3 contains the loss for the proposed model, which includes several KL terms that resemble those in the ELBO, but there is no description of what the latent variables are and what independence assumptions are being assumed in the variational posteriors. There appears to be a factorized posterior being used corresponding to an independence assumption in the posterior (though I can't tell for sure because I don't know what the model actually is), but this factorization should be described explicitly. \n\n3. I don't understand why the paper uses the term \"zero-shot\" when describing its language generation component. What is the definition of \"zero-shot sequence generation\"? How does this differ from language generation settings in which we are just generating text from, say, a language model, with or without context? Is that also zero-shot?\n\n4. I fail to see the benefit of drawing a connection between classical planning and generation under a right-regular grammar. The connection seems straightforward, but is it really necessary? Can't we just frame generation using planning without involving grammars? What benefit does connecting to grammars provide us? Perhaps the authors view the grammar perspective as being more familiar to natural language generation researchers? \n\n5. I am unsure what exactly is meant by \"discrete embeddings\", e.g., in the sentence \"Unlike existing discrete embeddings, our binary embedding supports vector arithmetic operations similar to continuous embeddings.\" It would be good for the paper to define what it means by \"discrete embeddings\", because if they are merely embeddings with discrete values only, then they can easily be used with arithmetic operations like addition and subtraction (just add and subtract the discrete values, which will preserve discreteness of values). \n\n6. On p.2, what is an \"action label\"?  I don't see this term elsewhere in the paper. \n\n7. It would be better to use a standard formulation of the Stanford sentiment treebank (SST) as binary classification. It's unclear whether the methodology used represents a standard data split. Rather than creating the data split yourselves, you can find some standard text classification datasets here, including a standard binary classification formulation of the SST dataset: https://github.com/harvardnlp/sent-conv-torch .  The provided results for SST dataset are quite low, with the highest being 77.3% and the lowest being 62.9%. One would expect at least 80% accuracy for a classifier based on word embedding averaging for SST, though maybe that's due to nonstandard splits being used. \n\n8. In App. B.3, a grid search over hyperparameter values is described -- what criterion was optimized by that grid search?\n\n9. Overall, I think the results in wordsim/analogies/textcat may be promising, but without a solid understanding of the overall approach I find it a little difficult to assess whether the comparison to CBOW is a fair and convincing one. The paraphrasing experiments are very difficult for me to evaluate as there is no baseline to compare to. For an example baseline, one could use CBOW and search for the set whose average embedding is the nearest neighbor to a given input word. The set would be unordered of course, but I am not that convinced that the proposed embeddings are modeling word order in an effective way anyway based on the generated paraphrases, so maybe a set of words that form a paraphrase would still be a reasonable baseline. \n\nMinor things are below:\n\nSec. 5.1: \"movie sentiment treebank\" --> \"Stanford sentiment treebank\"\n\nSome citations in Sec 5.2 and the appendices should use citep instead of citet.\n\nIn Table 3, why use \"hamburgur\" instead of \"hamburger\"? There were also some nonstandard spellings used in Table 12, such as \"concent\", \"fantacy\", and in Table 13 (\"surgion\"). I do not think these forms would be among the most frequent 8000 words. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting paper with a novel method?",
            "review": "This paper proposes to modify the CBOW word embeddings so that they can directly be used in the STRIPS action modelling framework. The authors evaluated their methods on several embedding evaluation tasks and on a zero-shot unsupervised phrase generation task.\n\n* Strengths:\n- The proposed method is quite unique and therefore novel.\n- Experimental results on intrinsic tasks look promising.\n- Qualitative analysis on the zero-shot phrase generation task looks interesting. \n\n* Weaknesses:\n- If the main focus of the work is to answer the question is whether a zero-shot sentence generation is a planning-like symbolic processing, the evaluation is rather weak and therefore not convincing. I don't find any quantitative analyses. \n- It is not clear for me how large is the benefit of the proposed method in the chosen zero-shot unsupervised phrase generation task. There is no comparison with other methods.\n- Why not modify state-of-the-art contextual embeddings, such as BERT?\n\n* Reasons to accept:\n- I have never seen the proposed method before. It is quite interesting but unfortunately, I don't know the related papers very well to judge the novelty of the paper. \n\n* Reasons to reject: \n- The authors builds their ideas based on CBOW word embeddings which are not state-of-the-art embeddings at the moment. \n- The evaluation framework is rather weak. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting idea",
            "review": "**Main Claim:**\n\nThis paper formulates the learning binary word embeddings as a planning problem, and solves the problem in the continuous space with STRIPS. The proposed embeddings achieve comparable performance on the word similarity and word analogy tasks. \n\n**Strong points:**\n\nBroadly speaking, the problem of learning discrete embeddings is an interesting topic, because discrete embeddings may have advantages in efficiency and interpretability. \n\nFormulating the problem as a planning problem is also a novel idea. \n\n**Weak points:**\n\nI hope the authors can show some observations of the learned embeddings, for example, try to identify some meaningful dimensions. \n\nIt’s also worthwhile to compare the DSAW with other discrete embedding methods, or naive baselines. \n\nThe reported performance of CBOW baseline in this paper is lower than other works. CBOW’s Top 1 accuracy on word analogy for is 65% in the GloVe paper (Pennington et. al), while 43.8% in this paper. Authors should explain the reason.\n\n\n**Recommendation:**\n\nborderline.\n\nThis work provides a new perspective to learning word embeddings, so it could be inspiring for future research. As mentioned in the paper, it could be applied to generate more structured language. \n\nThe major concern is the performance of the model. \n\n \n\n**Questions:**\n\nA clarifying question. Using the artificial example in Table 1, can you explain what are the results of King+Queen and Queen+King?\nIf I understand correctly, \n\tKing+Queen = s - {female} + {status} - {} + {female, status} = {female, status}\n\tQueen + King = s - {} + {female, status} - {female} + {status} = {status}\nSo the embedding is order-sensitive.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}