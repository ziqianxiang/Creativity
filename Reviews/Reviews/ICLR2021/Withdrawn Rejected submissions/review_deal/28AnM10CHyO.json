{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of A Spectral Perspective of Neural Networks Robustness to Label Noise",
            "review": "This work has two main contributions. First, the authors analyze an univariate neural network trained to fit a set of points generated by a ground-truth function that can be fit perfectly, under the assumption that the network can be optimized to reach the minimum of a least squares cost function. The analysis shows that bounding the spectral norm of the neural network weights attenuates high frequencies. Second, the authors show that spectral normalization provides a modest increase in performance in the presence of label noise.\n\nQuality: \n\nThe theory in the paper relates spectral normalization to the Fourier coefficient of a 1D function learned via regression by a neural network under the assumptions that it attains a global minimum. This setting is very far from the problem of performing classification on high-dimensional data with a neural network when the labels are noisy. In fact, the theory does not even consider  noisy data. The authors provide some heuristic discussion connecting smoothness to robustness to label noise, but that is all. In addition, the significance of the numerical results is not clear, since the authors do not combine their approach with state-of-the-art methods. \n\nClarity: \n\nThe paper is not written clearly. The authors keep referring to noisy labels when explaining their theory, but there are no noisy labels in the actual theoretical results.\n\nOriginality: \n\nI am unsure to what extents the connection between spectral normalization and smoothness is novel. \n\nSignificance:\n\nAs explained above, the theory does not seem very relevant to the main topic of the paper. For the numerical results the authors do not consider state-of-the-art methods. The potential impact is therefore quite limited. \n\nPros:\n\nThe numerical results show that in some cases spectral normalization can help with label noise.\n\nCons:\n\nDisconnect between theory and topic of the paper. Limited computational experiments.\n\nI would like to thank the authors for their response, but I don't agree that they have addressed my concerns. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studied the connection between smoothness regulation and the robustness of neural networks to random noise. The connection is that a smooth function has fast decay fourier coefficient, and random noise in the label mostly contribute to the high order fourier coefficient, thus forcing the neural net to be smooth can make it robust to the random noise in the label. Various techniques can be used to encourage smoothness of the neural net, including weight regulation with spectral norm and early stopping. Real and synthetic data experiments are conducted to verify the theoretical findings.\n\nWeakness/questions:\n1. Novelty of the theoretical findings. I am not sure about the novelty of the theoretical findings. Random noise has high order fourier coefficients and the fact that Lipchitz coefficient of a neural net can be bounded by the product of the spectral norms of the parameter matrices seem well known.\n2. About early stopping. The authors argue that early stopping makes the network robust since low frequency components are learnt first. However, I can’t find an explanation of that. At the end of page 4, it reads “In addition, we conclude that a network trained with WD first learns the low frequencies.” It is not clear to me how this conclusion arises.\n3. Experimental findings are concerning to me. Table 2 and Table 3 seem to suggest that even with spectral constraints/regularization, the accuracy of the network still decreases quite a lot, and in fact regularization only improves the accuracy by ~2% compared to no regularization at all. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs more work and motivation. ",
            "review": "The authors propose a spectral normalization approach to improve robustness of neural networks under mistaken labels. For the theoretical portion, first they examine a regularization approach based on the first derivative of the neural network, and show that there is a decay of the higher frequency components, at least when training size goes to infinity. They then propose to bound the derivative by the network’s weights. They then empirically draw relationships between the noise in the training data and the smoothness of the learned network. They then went back to the regularization and considered the constrained optimization formulation, and came up with (shrinkage) expressions of the global optimal, and then showed again that in the constrained weights setting this corresponds to decay in the higher frequency components. \nThis paper considers neural networks from a Fourier/spectral perspective, which is an interesting trend in the recent theoretical literature. However, there are several comments and issues that the authors should address. \nThe authors should comment/write more clearly on what their motivation is? In the abstract they claimed “This work uses recent developments in the analysis of neural networks function space to provide an explanation for their robustness”,  but in the actual paper they are mainly analyzing a regularization approach of neural networks, with little analysis of the function space of the neural network itself. It seems like they are really analyzing how a regularization approach improves robustness, rather than explaining why neural networks themselves are robust to noise. Perhaps they could either 1. Tone down their claims on explaining robustness of neural networks via functional analysis or alternatively 2. Actually more explicitly link their regularization approach to function spaces that are relevant to Fourier etc, say Sobolev spaces etc. \nThe authors need to address where the substantial novelty comes from. Spectral normalization/looking Fourier components has already been looked at quite extensively in the literature (which the authors have cited). Their proposed way of bounding derivatives by weights is based on a lemma by Sokolic et al.  In the formulation in proposition 3 they are basically doing a slightly modified L2 regularization on the frequencies. The constrained approach is a simple consequence of adding the regularization term (the dual), and the resulting thresholded/shrinkage expression is well known within the optimization literature. The authors should state and argue clearly where the substantial novelty/big idea is. \nIt appears to me that the results in some of the propositions are not sufficient to justify their clams and is quite repetitive (they are essentially saying the same thing). In proposition 1, they show that the Fourier coefficients decay in the first derivative regularization scheme. Then In proposition 2, they use weights as a proxy upper bound for the derivative. How good is this approximation? Is the upper bound tight? Are there better approximations? Then in proposition 3, they looked at the constrained version (dual) of proposition 1 (essentially l2 regularization), and in proposition 4 they showed that in the constrained version with weight normalization, the Fourier coefficients decay. It appears that propositions 3 and 4 is an extension of proposition 1, which says that if you penalize the derivative (or normalize the weights) fourier coefficients decay. To summarize the propositions, this is basically a modified low pass filter, written in different optimization formulations. It is well known in signal processing that low-pass filters attenuate high-frequency noises, so again I am not sure where the novelty is, and how an analysis of a regularization method (already proposed previously in the literature) could substantiate the authors claim of explaining robustness to noises of neural networks (without the regularization)\nThe authors should address certain assumptions in their theoretical analysis, and link the experiments to the theory in a better way. First, they consider a univariate output setup  in their analysis with smoothness assumptions, but in their experiments, they are doing classification (presumably not univariate output). Second, in the experiments where they looked at CIFAR-10 accuracy of different regularization methods, it is unclear how this links to their theoretical claims. They did not look at SN alone (they looked at SN + l2), and also SN has been proposed in the literature before. They should link this to the theory better. \n\nOverall, the idea is interesting, but the authors need to justify where the novelty and substantial contribution comes from, needs to motivate the paper more clearly, should consolidate some of their propositions, and should better link the experiments to theory. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction, limited analysis",
            "review": "Summary: the paper discusses the effect of a form of spectral regularization on the Jacobians of a model as a way of handling overfitting to noisy examples. The authors show that the Fourier coefficients grow as O(1/k^2) where k is the frequency index. Also, they show that the regularization term can be upper-bounded by L2 norm or Frobenius norm of the weights. The analysis is performed for a univariate function approximation.\n\nReview: the authors adopt an interesting take on analyzing spectral normalization and WD. However, the analysis is quite limited and in some cases, lacks rigorousness. Also, it is not clear how the analysis generalizes to a wider class of multivariate models. \n\n1) The analysis is performed for a univariate function approximation and therefore, upperbounding the integral in Eq (4) becomes trivial. It is not clear how this argument generalizes to the multivariate case.\n2) The argument that the noise usually has high frequency components is not convincing and requires some mathematical proof. Currently, it looks like the authors are trying to match the noise model to the analysis, not the other way around.\n3) How good is the O(1/k^2) bound in practice? This requires some careful analysis and experimentation. \n4) The proof of Proposition 1 (Eq (15)) heavily depends on the assumption that the inputs are uniformly spaces (which is not a reasonable assumption). The authors mention that generalization to the random case can be done by non-orthogonal subsampled Fourier frames, but this is not immediately obvious!\n5) Proof of Proposition 4 is unclear and might have some errors: \"We get equation 10 by simply observing that ...\". Eq (9) has an equality constraint while Eq (38) is upperbounding the requirement of Proposition 2 by this constraint. This doesn't seem correct.\n\nMinor: A later work (Amid et al. 2019a) generalizes the GCE loss (Zhang and Sabuncu 2018) by introducing two temperatures t1 and t2 which recovers GCE when t1 = q and t2 = 1. A more recent work, called the bi-tempered loss (Amid et al. 2019b) extends these methods by introducing a proper (unbiased) generalization of the CE loss and is shown to be extremely effective in reducing the effect of noisy examples. Please consider referencing these papers as part of third approach for handling noise in the Related Work section.\n\nAdditional references:\n\n(Amid et al. 2019a) Amid et al. \"Two-temperature logistic regression based on the Tsallis divergence.\" In The 22nd International Conference on Artificial Intelligence and Statistics, 2019.\n\n(Amid et al. 2019b) Amid et al. \"Robust bi-tempered logistic loss based on Bregman divergences.\" In Advances in Neural Information Processing Systems, 2019.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}