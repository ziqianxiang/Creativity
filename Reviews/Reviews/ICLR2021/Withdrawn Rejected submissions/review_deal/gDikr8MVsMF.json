{
    "Decision": "",
    "Reviews": [
        {
            "title": "Graph convolutional neural networks for value-function factorisation in MARL",
            "review": "This paper proposes a novel framework (called GraphMIX) for value function factorization in multi-agent\ndeep reinforcement learning (MARL) using graph neural networks (GNN). The main contribution of the paper is the method that allows to factorise the joint state-action value function and explicitly assign credit to each agent by considering the team of agents as the set of nodes in a complete directed GNN. The edge weights of the graph are governed by an attention mechanism. Empirical results are performed to show the performance gain of the proposed method over a state-of-the-art approach QMIX using some scenarios from a popular SMAC benchmark.\n\nPositives:\n\n- The idea of using a GNN for state-value function factorisation is interesting.\n- The paper is written with clarity and is easy to follow. The proposed method, GraphMIX, is thoroughly described. The related work section also puts the proposed method into context. Albeit the proposed method having limited novelty (GNN instead of MLP in QMIX, primarily), it is well-motivated by pointing out the limitations of existing methods.\n- The experimental results are well structured. It's nice to see the visualisation of average attention received from others in Fig 4.\n\nConcerns:\n\n- My primary concern about the paper is the lack of thorough experiments to investigate the usefulness of the proposed method. Other than the gains in performance being only marginal, here are some additional comments regarding the results\n    - GraphMIX is trained by minimizing the aggregate loss: L_global + L_local. However, no ablation results are presented to quantify the effectiveness of the introduction of local optimisation term.\n    - The architecture of the mixing network in GraphMIX is substantially bigger than that of QMIX. Therefore, it would be fair to compare GraphMIX with a larger QMIX model to understand whether the improved performance is due to architecture choice rather than model size.\n    - To evaluate the importance of attention-based edge weighting, it would be nice to include results with unit edge weights in GraphMIX.\n    - It is unclear to me why exactly these four SMAC scenarios have been chosen. I'd be interested to know how GraphMIX scaled in the number of agents, so having results from SMAC scenarios with a larger number of agents that require fine-grained coordination would be welcomed (e.g.  27m_vs_30m, MMM2 and 10m_vs_11m). Though bane_vs_bane includes 24 agents, it doesn't require coordination between all agent as much as the other mentioned scenarios.\n- Regarding GraphMIX outperforming QMIX in terms of reduced variance in bane_vs_bane, I don't think the statement is accurate. Yes, QMIX showed more variances on that particular map, but its performance was better than that of GraphMIX. The statement would have been more precise if the two methods had the same mean.\n- The choice of visualisation method for attention weights in Figure 4 is suboptimal. It's hard to identify the correct x and y coordinates of given 3d points (most noticeably in 6h_vs_8z). Maybe it would be worth presenting in 2d somehow.\n\nOther comments:\n\n- POMDPs are generally used for describing single-agent partially observable RL problems. A more suitable setting for the given problem is the Dec-POMDP framework (see [https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf](https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf)).\n- I'd appreciate it if the authors compare the results of their method with Weighted QMIX ([https://arxiv.org/abs/2006.10800](https://arxiv.org/abs/2006.10800)) in the upcoming versions of the paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting research topic, but the paper needs to be improved",
            "review": "This paper proposes a new value function factorization method with a graph neural network based on the intuition that agents in the multi-agent systems can be treated as nodes in a graph structure. Applying a graph neural network to a multi-agent reinforcement learning problem is a fascinating problem. However, this paper is still incomplete, and several points can be improved.\n\n1. Method's novelty is lacking. As the author argues, applying graph neural networks to value function factorization seems to be a new research topic. However, constructing a graph neural network through attention-based edge weights in multi-agent reinforcement learning is existing research. The only differences are that (i) nodes use the scalar feature, a local observation-action value function, and (ii) the layers satisfy the monotonic properties using non-negative weights to apply to the value-based method.\n\n2. It seems that additional discussion is needed on another contribution, local losses. Current value factorization methods do not require the local losses and are learned only as end-to-end training only through the global loss. The additional loss function can enable efficient representation learning through other signals such as self-supervised learning. However, it may hinder learning so that optimal policy cannot be trained. To show the effectiveness of this method, the authors should first show the experimental results when using only global or local losses as an ablation study. Alternatively, it is necessary to prove that even if the agents learn by adding local losses, they can learn the same optimal policy only through global loss.\n\n3. The structure of the experimental section needs to be updated. First of all, it is difficult to read because it does not distinguish between experimental settings and experimental results. Also, the use of hypernetwork architecture in GraphMIX should be mentioned in the method section instead of the experiment. Also, there is a lack of experiments overall. Authors need to add baselines, environments, and ablations all together.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review 2",
            "review": "This paper proposes GraphMIX, a method uses GNN for value decomposition in multi-agent reinforcement learning. The paper introduces GNN with several special treatments for monotonic value function factorization, where the agents are seen as a fully connected graph. Experiments are performed in SMAC and show GraphMIX outperform QMIX in several settings.\n\nPros: \n- Modeling multi-agent environments as graph is promising. \n- The reward fraction is interesting. \n\nCons: \n- The main concern about this paper is the lack of novelty. It can be simply summarized as using GNN for monotonic value function factorization, instead of hypernet in QMIX. Therefore, the contribution of the paper seems limited. \n- The reward fraction makes Q-function at each agent somehow meaningful. However, it is rather an *ad hoc* choice. What supports that intuitively or mathematically? It seems a credit assignment at each step, but why the features from nodes could determine that? \n- The literature review is not thorough. Many recent papers about value decomposition in MARL are missing, like Weighted QMIX [Rashid et al., 2020], Qatten [Yang et al., 2020], etc. Therefore, the comparison with these methods (not just QMIX) will make the paper more convincing. \n- Moreover, ablation studies should be considered in experiments to verify the proposed components.\n- In addition, as the proposed method still uses monotonic value function factorization, the paper must answer the question why it outperforms QMIX.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Proposed method is a combination of QMIX and DICG, seems to have potential for multi-agent learning, but experiments are not sufficiently convincing.",
            "review": "This paper contributes a value function method for multi-agent reinforcement learning in the paradigm of centralized training with decentralized execution. The method, called GraphMIX, uses the attention mechanism in Deep Implicit Coordination Graphs (Li et al. 2020) to define edge weights of a fully-connected graph among agents, then uses a Graph Convolutional Network (GCN) to compute a global action-value from local action values with monotonicity constraints to enforce the \"individual-global-max property\" (Son et al. 2019). Similar to QMIX (Rashid et al. 2018), the GCN parameters are produced by a hypernetwork that has access to centralized global state information. Another component learns a decomposition of the global team reward into individual rewards, which allows the use of an auxiliary local Q-learning loss function in addition to the standard global Q-learning loss. GraphMIX was evaluated on four maps in the StarCraft 2 micromanagement environment, and is shown to perform better than QMIX on some metric in each map.\n\nI rate this paper as 5. The proposed method is a clever combination of previous works---specifically, hypernetwork and monotonicity from QMIX (Rashid et al. 2018), soft edge weights from DICG (Li et al. 2020). I believe there is potential for this combination to perform well in multi-agent settings where the dynamic graph structure induced by agents should be captured by the inductive bias of the model. However, the most optimistic interpretation of the experimental results is that the SC2 environment does not benefit significantly from a graph-based learning method and hence does not provide sufficient evidence to conclude that GraphMIX is a significant research contribution beyond existing methods.\n\nWhile there is nothing factually incorrect about the author's claim that GraphMIX outperforms the QMIX baseline on some metric (learning speed or variance or reward or win rate) on each SC2 map, there is no map where GraphMIX significantly outperforms QMIX on three or more metrics. In fact, one may also claim that QMIX outperforms GraphMIX on some metric on each map. GraphMIX learns slower than QMIX on 3 out of 4 maps. Looking at the test win rate alone, which may be the most important metric in real-world applications, one cannot claim that GraphMIX performs better than QMIX.\n\nThe method that learns a credit assignment scheme to get individual rewards for computing local losses is interesting and new. However, the authors must conduct an ablation experiment to determine the contribution of the local loss (equation 10) to overall performance. Currently it is impossible to judge the value of this method.\n\nMore explanation of Figure 4 is necessary. The only textual explanation provided in the paper essentially says that the plots for the four SC2 maps look different. But what should a reader understand from each individual plot? Even the explanation of the bane_vs_bane subplot is insufficient: the authors say that most attention values are small, but there are still some data points with high attention values, and \"distance to enemies\" seems to follow some bimodal distribution. What should a reader learn from the subplots for the other maps? It's unclear what the color of the data points mean. The text says that each point was taken from a sample from the replay buffer during training, but does not specify exactly what time point during training? Values early in training should be very different from values later. Or is that what the red/blue colors mean?\n\nMinor comments that did not affect the decision:\n1. Section 4 first paragraph: the local observation-action value function $Q^{\\pi_m}$ is introduced for the first time here. This could be confusing to readers who are unfamiliar with QMIX, since this function is defined implicitly and is not a regular Q function. It would be better to explain this in the paragraph.\n2. Section 4.2 is highly specific to the SC2 environment and should be moved to the experimental setup or to the appendix.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}