{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper conducts a theoretical and empirical analysis of the Generative Adversarial Training method (GAT). Although many comments have been addressed in the rebuttal, the reviewers still have few (but important) concerns, including the memorization effects and the lack of comparisons. "
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "In this paper the authors provide a theoretical and empirical analysis of the Generative Adversarial Training method (GAT) which is used to train models for OOD and adversarial example detection.\n\nThe GAT method is analyzed from a game theoretical prespective, focusing on the differences with GAN training, which are sometime conflated with it in the literature. The authors show that GAT and GAN have different training problems (maximin vs minimax) which have different optimal solutions.\n\nThe authors also propose a variant of GAT called Unconstraned GAT, which replaces the PGD attack in the inner optimization loop with an unconstrained steepest ascent update. They propose this training algorithm for both OOD detection, adversarial OOD detection and generative sampling.\nThey discuss the sensitivity of the proposed algorithm on the step size, which seems to critically depend on the model architecture and dataset. Experiments are performed on standard image datasets, using ImageNet as a source of known OOD examples.\n\nOverall the work is interesting, however I find some issues:\n- The key point of the theoretical anaysis comparing GAT with GAN is only made at the end. It would be better to mention in the introduction at a high level why GAT is preferable to GAN.\n- Maximin and minimax are terms specific to game theory which a typical ML researcher might not be familiar with. They should be defined, possibly first in an intuitive way and then formally.\n- The proposed algorithm seem quite sensitive on the step size. This might limit its practical applicability (minor issue: the step size is denoted as lambda in the text and gamma in the algorithm box).\n- The proposed generative sampling procedure will likely return a mode of the distribution rather than an unbiased sample. You would need to use an SGLD-like algorithm in order get unbiased samples.\n\nEDIT:\n\nThe revision addressed my concerns, I'm raising my evaluation to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting Ideas, but not ready yet. ",
            "review": "**1. Summary and contributions: Briefly summarize the paper and its contributions**\nThis work analyzed the optimal solutions for the Generative adversarial training (GAT) and the convergence property of the training algorithm. This work also compared the minimax and maximin games, both theoretically, and empirically, with the help of a nice 2D toy example. This work also developed an unconstrained version of GAT, and evaluated it on image generation and out of distribution detection tasks. \n\n##########################################################################\n\n**2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.**\n\nI found the theoretical analysis to be interesting, and I especially like the 2D toy experiment in Figure 2, it strongly and clearly justified the importance of using a uniformly distributed p_(-k) distribution. \n\nI also liked the thorough discussion of the distinction between the minimax and maximin games. \n\nThe paper has interesting ideas and a lot of content, it also introduced adversarial OOD samples, which are all very interesting to me. \n\n##########################################################################\n\n**3. Weaknesses: Explain the limitations of this work along the same axes as above.**\n\nMemorization: \nI think optimizing the D approach is problematic in terms of memorization, there’s nothing stopping the model to memorize the data, especially in the high dimensional space which makes the mass distribution to be very sparse. \n\nIn the Celeb A results in the middle of Figure 3, notice how the images (4,1), (4,3) look almost exactly the same, and they are also very similar to (1,3), (3,1) and (3,4).  \n\nIn figure 2b and 2c, notice how the distribution all collapses to 1 point, I wonder whether this is one perspective or intuition on this problem. \n\nUnverified claim: \nAt the bottom of page 7, “These results suggest that with a high capacity model and proper training, a robust OOD detection system is within reach.” where these results referred to reducing the data complexity. I think the results in this paper are not enough to make this claim. The authors’ logic here is that, if their model performs better on a simple dataset, then it implies the problem is insufficient model capacity. The assumption here is that the model can scale, which is completely unverified. Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet. An unverified claim like this is always a warning sign as it may mislead the readers and community. \n\nLack of ablation study: \nI really liked the toy experiments in Figure 2, which justified the use of uniform distribution in the data space. However this only provides intuition for the higher dimensional cases, it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases, for scientific rigor.  \n\nUnfair comparison: \nFor image generation results, the authors’ method was compared with GANs, and the motivation is that both of the methods are trained adversarially. However, the generators of GANs never got to see the real data during train time. Here authors are optimizing D, which is trained on real data, thus I don’t think it is fair to compare with GANs.  \n\nWeak baseline for OOD (outlier exposure) and insufficient comparison:\nOutlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10, for example, see Detecting Out-of-Distribution Examples with Gram Matrices: https://arxiv.org/abs/1912.12510 \n \nLack of Related Work section: \nThis paper does not have a related work section. The related works are very briefly discussed in the introduction, but I think that is far from enough. I understand there is a page limit, but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands. \n\nUnclear writings: See section 4.\n\n ##########################################################################\n\n**4. Clarity: Is the paper well written?**\nTypos: \nAt the top of page 3: as a results -> as a result\nBottom of page 8: a OOD -> an OOD\n\nAmbiguity: \n“elementary mass” was not defined before being used, it’d be helpful to the readers to briefly define what “elementary mass” means in this specific context. \n\n“In order to cause more local maxima to be eliminated” could be worded better. \n\n##########################################################################\n\n**5. Reasons for score**\nIn conclusion, the ideas are very interesting but I think there is a lot more work to be done before this paper could be accepted. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theoretical analysis of GAT; but need to compare with state-of-the-art and add quantitative comparison on generation.",
            "review": "The authors investigated generative adversarial training (GAT) and made two main contributions: 1) theoretically analyzed its maxmin objective and compared  with the minmax formulation used by GANs, and 2) applied GAT to OOD detection and generative models. Extensive experiments were performed for evaluating the proposed algorithm.\n\n############################\n\nStrong points:\n\n. The authors analyzed the maxmin formulation of GAT theoretically by deriving the optimal solutions under different scenarios, and also showed the conditions under which the algorithm converges to the optimal solution.\n\n. The authors pointed out the difference between the maxmin formulation and the minmax formulation used by GANs, and extended GAT for two applications: OOD detection and generative models.\n\n. Extensive experiments were performed for evaluating the proposed algorithm.\n\n############################\n\nWeak points:\n\n. In Section 3.1, for the convenience of analysis, the authors transformed equation (4) to (5). Are these two optimization problems equivalent? Seems not. Please add brief explanation.\n\n. Algorithm 1 is claimed to solve equation (5). However, in Step 2 the maximization is still over B(x,\\epsilon). It seems that Algorithm 1 is to solve equation (4). Please explain.\n\n. Section 3.2: a practical consideration is that Step 2 cannot be perfectly solved and the authors thus proposed to use a p_{-k} that is uniformly distributed in the data space. The authors claimed that  such condition always results in no maxima and global maxima at Supp(p_k). Can this conclusion be theoretically proved? One potential limitation is that using a p_{-k} that is uniformly distributed could lead to a long training time for achieving a satisfactory performance especially in high dimensional.\n\n. OOD detection is an active area and there are a lot of papers regarding this. In experiment, the authors mainly focused on evaluating the proposed algorithm, and did not compare it with any other OOD detection methods, e.g., log-likelihood based and likelihood ratio based. Although K = 0 can be thought of as exposing to outliers, the resulting algorithm is still under GAT framework. As OOD detection is suggested as a main application of GAT, it is important to compare it with state-of-the-art.\n\n. It is hard to compare the quality of generation by only observing the generated images. It's better to show some quantitative comparison by using, e.g, FID score, which is commonly adopted for evaluating GANs. \n\n####################\nSome typos:\n\n. In equation (3), should be \\lambda.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}