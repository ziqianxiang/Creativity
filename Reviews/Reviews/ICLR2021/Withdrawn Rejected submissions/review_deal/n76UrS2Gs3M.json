{
    "Decision": "",
    "Reviews": [
        {
            "title": "A new setup for bias mitigation in transfer learning with potential but possible experimental problems ",
            "review": "# Summary\n\nThe paper proposes a new setup for bias mitigation in the context of fine-tuning pre-trained models: first fine-tune in a source domain with bias mitigation and then fine-tune in a target domain without bias mitigation. \nA motivation is that attribute annotations (which are needed for bias mitigation) are not available in the target domain, but are available in the source domain, where bias mitigation is run. Two bias mitigation algorithms are used: explanation regularization and adversarial debiasing. Experiments are conducted on hate speech detection with two bias factors: group identifier bias and African-American English dialect bias. The main experiments are done in a cross-domain setting, where a model is fine-tuned with bias mitigation in one domain and then fine-tuned without bias mitigation and evaluated in another domain. Some experiments are also conducted in a same-domain setting.  The main result is that the proposed setting leads to better bias mitigation than fine-tuning without bias mitigation in the target domain or than fine-tuning without mitigation first in the source domain and then in the target domain.  There are also a couple of other interesting variations, where the encoder is either frozen or regularized to not diviate much from its initial stage. Those yield counter-intuitive results, in that such restrictions do not reduce bias in the target domain. \n\n# Evaluation\nThe paper studies an interesting new setup for bias mitigation in transfer learning / fine-tuning. This is a novel idea as far as I know. Beyond that, the paper uses existing bias mitigation methods and bias evaluation measures, but I think that is fine and not every paper should come up with a brand new method. \n\nThe experiments are fairly comprehensive and yield new results. The proposed approach seems to work better than the straightforward alternative. Naturally it doesn't outperform doing bias mitigation in the target domain, but that is acknowledged and rightfully discussed in the paper. \n\nI think there are some potential confounders in the experimental setup. The paper attempts to control for some of them, but maybe not all. See more on this below. \n\nThe paper is limited in scope in that it only studies hate speech detection, although with several datasets and biases. But the arguments in the first part of the paper are made to be much broader, and I think it would strengthen the paper significantly if more tasks were included. \n\nI am missing some analysis of the encoder representations in different setups, to show whether bias is indeed removed, beyond the false-positive rate metric. See suggestions below. \n\n\n\n# Comments\n1. Clarify early on that the setup is done in the fine-tuning phase, that is, a pre-trained model is assumed and is never fine-tuned on the LM task, only on the hatespeech task in various ways and phases. It took me some time to understand it. \n2. In the AAE experiments, the AAE dialect predictor may introduce errors. What is the effect of such potential errors on the conclusions? \n3. Why is adversarial learning not used in the hate speech detection case? \n4. All models start from a pre-trained RoBERTa-base. Then, they are fine-tuned twice, first with bias mitigation and then without.   It is not clear what part of bias comes from RoBERTa-base pre-training, what part comes from the initial fine-tuning with bias mitigation, and what part comes from the second fine-tuning. I think the comparison experiments tease apart a subset of these, for example by comparing vanilla with *-Transfer experiments. But we still don't know how much bias is from RoBERTa pre-training vs any of the fine-tuning steps. \n5. When comparing Vanilla and Vanilla-Transfer there is a potential confound of Vanilla-Transfer having access to more data during the two fine-tuning steps. So, we can't be sure that transfer itself is helping with bias mitigation, as opposed to just more data. The same may be true for comparing Expl.Reg/Adv.Learning with Expl.Reg/Adv.Learning-Transfer. Is there a way to control for this issue by matching dataset sizes? \n6. There is quite a lot of variation in the results, measured by the std (I assume) appearing in parentheses. I appreciate the inclusion of this variation. I wonder, which differences between settings are statistically significant? \n7. Some things are left open, specifically some unexplained patterns in the multi-bias experiments in section 5.2. That's not necessarily a shortcoming, as not everything has to be resolved. \n8. The same-domain experiments in 5.3 are useful. I would like to see a more detailed discussion of them. Perhaps, the paper can start with them and then move to cross-domain experiments, which are more complicated. \n9. The experiments in 5.4 are illuminating. Indeed a counter-intuitive result and the explanation of linear models learning more bias seems plausible. There might be difficulty in training a linear classifier with frozen/little-changing encoder representations on these tasks, which on the whole seem rather hard. Could an experiment in an easier setting (maybe a simulation) be reveal more insights here? \n10. Is the bias really gone? A common method for analyzing whether bias mitigation helps is to later freeze the weights and attempt to decode the bias with a classifier (Elazar and Goldberg). There are also other methods for evaluating the success of bias mitigation besides measuring false positive rates (Lipstick on a Pig paper). It would be very interesting to study this question in the context of the present paper. \n11. The related work section mentions work on bias definitions and taxonomies in NLP/ML (Mehrabi et al., 2019; Blodgett et al., 2020; Shah et al., 2020). It would be good to situate the current work within the definitions/taxonomies in these studies. \n12. I appreciate that inclusion of two bias factors and multiple datasets. On the other hand, there is only one task, hate speech detection, albeit in several datasets. It would strengthen the argument of the paper for a general approach if more tasks were investigated. The same may be true of using only one pre-trained model, although that is less of an issue in my opinion. \n\n\n# Minor comments\n1. There are many grammatical errors in the paper. I suggest having the paper proof-read for English usage. Some examples: \n- Abstract: model behaviors that discriminates -> discriminate \n- same number female .. -> same number of female.. \n- frameworks fits -> framework fits \n- These work keeps -> This work keeps or These works keep \n- We illustrates -> illustrate \n2. Inline citation of multiple authors is plural: Kurita et al. (2009) demonstrates -> demonstrate \n3. Notation: a sentence x is in R^m. How does that work? A sentence is either a discrete object or, if already mapped to indices, an object in N^m for example. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Efficient Learning of Less Biased Models with Transfer Learning ",
            "review": "Summary: This paper investigates the possibility of bias mitigation in downstream tasks that use language models. Their investigation first applies 2 bias mitigation techniques (those based on adversarial learning and explanation regularization) to the language model, and investigating whether or not fine-tuning on out of domain datasets results in mitigated bias. They investigate bias that has been uncovered by prior work in hate speech detection (w.r.t. various groups and African American English Dialect) and conduct their experiments on 4 datasets. They use the equal opportunity metric which measures differences in false positive rates across groups, and show a reduced difference in false positive rates among the groups defined in the datasets in a number of settings. \n\n\n+As larger and larger language models are being used for many downstream tasks, it is becoming increasingly important to study the characteristics of language models and how they affect the downstream tasks they are used for. This is extremely context specific but extensive research should be done in this area as many harms can be perpetuated, especially towards people in marginalized groups, by uncritically applying these models in many tasks. \n\nMajor Concerns:\n\n-As noted by Blodgett et al. [1] (also cited in the paper) and many others, it is important to be very clear and specific while discussing \"bias\". In this case, while the paper engages with a specific type of issue concerning language models for English and investigates a specific task using a specific measure (out of the more than 30+ fairness proposed metrics) as a metric, the title, abstract and motivation is much more general and it takes a while to get to these specifics. I suggest that the authors from the very start write very clearly about the specific context they are addressing rather than any generalized claims. This is especially important as there is no such thing as a universally unbiased model or unbiased dataset and the downstream task and context always determines what one means by \"bias.\"\n\n-While the paper presents the beginnings of an investigation, in my view this is not sufficient for publication at this time. In addition to the specificity with which these topics should be investigated and discussed,  I would like to see a little bit more analysis of the results: how much of an improvement in this specific metric is significant while considering these specific tasks and why? As noted in works like [2] too much abstraction without taking a sociotechnical approach has the danger of rendering \"technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems.\"\n\n-The paper notes:  \"Similarly, fine-tuning from vanilla models trained on Stormfront and FDCL improves bias metrics compared to vanilla training on GHC or FDCL. However, we see mitigating bias in source models does not bring further improvements. The results imply different tasks may interfere with each other when applied for mitigating bias in upstream models. We leave further study and solution to such interference as a future work.\" It would be good to further investigate these types of issues in this paper since as I mentioned, it has the beginnings of an investigation but in my view not enough for publication. \n\n-There is a statement: \"we consider false positives to be more harmful; however, the framework also applies to other bias, e.g., where false negative rates are equally important\" in the paper. Have the authors done the same investigation with a different metric measuring other characteristics instead? If so, it would be good to know the results. If not, then I'm curious how this statement can be made. \n\n\nMinor Comments:\n\n-The paper has a number of typos which would be good to fix\n\n-\"For example, in adversarial learning, the loss terms coerce attribute a to be predictable from the data representation z = g(x).\" The authors might have meant to write the opposite since the end goal is to learn a representation from which the attribute \"a\" cannot be predicted. \n\n[1]Blodgett, Su Lin, et al. \"Language (Technology) is Power: A Critical Survey of\" Bias\" in NLP.\" arXiv preprint arXiv:2005.14050 (2020).\n\n[2]Selbst, Andrew D., et al. \"Fairness and abstraction in sociotechnical systems.\" Proceedings of the Conference on Fairness, Accountability, and Transparency. 2019.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a framework for reducing the bias—with regard to protected attributes such as group identifiers or dialect—of downstream models by transferring them from less biased source models. To this end, the authors propose to fine-tune a pre-trained language model with a bias reduction algorithm in a source domain and then transfer it by fine-tuning it on the target domain. They employ two existing bias reduction algorithms, explanation regularization (Kennedy et al., 2020) and adversarial de-biasing (Elazar & Goldberg, 2018) and evaluate on different directions of transfer on existing datasets, including the Gab Hate Corpus, the Stormfront corpus, FCL, and DWMW, using both single-source and multi-source transfer. Overall, bias reduction on a source domain outperforms naively fine-tuning a pre-trained model on the target domain or naively transferring a model trained on the source domain without bias reduction but is generally outperformed by performing bias reduction directly on the target domain.\n\nPros:\n1. The authors tackle an important and impactful setting, how to efficiently mitigate the bias when applying pre-trained language models to downstream tasks.\n2. The proposed framework is simple and easy to use in practice.\n3. The authors evaluate on a number of different datasets and the proposed method outperforms the naive baseline.\n\nCons:\n1. The main goal of this paper is to propose a framework that enables more efficient bias reduction compared to annotating user data directly in the target domain and performing bias mitigation there. The experiments do not demonstrate the generality of such a bias-reduced source model as each source model is only transferred to a single target domain. Instead, I would find it more convincing and more useful for downstream applications if the authors designed an experiment where a single model trained on a general source domain was transferred to multiple target domains. (The GHC + FDCL model is transferred to both DWMW and Stf. datasets but it's not clear to me whether this is the same pre-trained model.)\n2. The experimental results are somewhat weak. The transfer approach is outperformed by target domain bias mitigation on GHC and Stormfront on the bias metric and accuracy.\n3. Score discrepancies across domains are not explained. The proposed approach outperforms target domain bias mitigation on DWMW but the authors provide no explanation for this behaviour. In order for the approach to be of practical usefulness, it is important to provide some intuition what aspects of the data might affect the model's behaviour.\n4. You mention in Section 3.3 that you consider \"the similarity between the source and the target domain\" but this is never formalized as far as I can see. In this setting where bias reduction is performed on the source domain to achieve low bias on the target domain, the choice of the source domain is key. In this context, I am also not convinced of the purpose of the \"same domain\" transfer experiment as conclusions from transferring within the same distribution do not seem generalizable to transferring between different distributions.\n5. Given that no new method is proposed, I would have liked to see a deeper analysis on how the source domain and transfer impacts the bias of the downstream model and in what ways bias is amplified or reduced through the transfer, possibly using some methods that the authors described in the related work section.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The novelty and experiments are my main concerns.",
            "review": "This paper proposed a framework for learning less biased models based on transfer learning strategies. Specifically, there are two major components, 1) the bias mitigation algorithms and 2) the model fine-tuning algorithms. The experimental results demonstrate the effectiveness of the proposed model compared with the basic/vanilla methods.\n\nThis paper has a clear introduction about the motivation and the algorithms of the proposed model. This is the good point. However, my main concerns of this paper is the novelty and the experiments. The details are listed below:\n\n1) The novelty of this model is very limited. The setting including the issues of data bias which is a general challenge in machine learning tasks. The proposed algorithms include the methods in transfer learning and domain adaptation tasks which are also the general operations. \nSpecifically, Section 4.1 proposed two approaches. The score/attention-based method and adversarial-training based methods. Both of them are very normal operations. In addition, Section 4.2 is a general model fine-tuning strategy with an existing $l^2$-sp regularizer as loss function.\nTo this end, I can’t find much novelty of this work. The application for Hate speech detection and AAE dialect bias analysis maybe practical/valuable for real-world. But for novelty consideration, the novelty is limited.\n\n2) The experimental results are not convincing. There are not any other state-of-the-art baselines exist in the experiments. There are a lot of supervised/unsupervised domain adaptation methods in recently years which could be used for comparison, the author should do the experiments with some of the methods to demonstrate the effectiveness of the model.\n\nIn summary, this paper proposed a framework for transfer learning. The model is not novelty and the experimental results are not convincing. To improve the paper, more explanation about the novelty and SOTA baselines should be involved.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}