{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper focuses on the point cloud generation task inspired by non-equilibrium thermodynamics. To cope with various shapes, the latent code is applied to construct a conditioned Markov chain based framework to model rich point distributions. This is a very early work that deduces the point cloud generative task by the diffusion probabilistic model, with quantitative results showing that it can achieve state-of-the-art performance. ",
            "review": "This paper focuses on the point cloud generation task inspired by non-equilibrium thermodynamics. Transforming noises to a desired shape is considered as the reverse diffusion process, where points are viewed as particles. To cope with various shapes, the latent code is applied to construct a conditioned Markov chain based framework to model rich point distributions. The authors conclude an analytic loss function and formulate an end-to-end learning pipeline. This is a very early work that deduces the point cloud generative task by the diffusion probabilistic model, with quantitative results showing that it can achieve state-of-the-art performance. \n\n\nAs a novel diffusion probabilistic method, the authors first model the forward diffusion process by Markov Chain, which transforms the well-generated point cloud to Gaussian noises. During the inference, the reverse diffusion process is adopted in order to recover shape from random noise. The latent prior distribution is also learned via a PointNet encoder. \n\nHowever, such method based on diffusion probabilistic has been well studied in the image domain, as shown in Sohl-Dickstein (2015). By considering the image pixels as points in a point cloud, the framework derivation is very similar. Although the authors claimed that the previous methods only consider unconditional generation, the conditional generation problem has also been widely studied for models that are not based on diffusion probabilistic, which degrades the novelty of proposed method.\n\nAdditionally, is the shape latent independent from the subsequent processing steps? If so, we may assume that more powerful and discriminative shape latent leads to better generation results. Please provide more discussions about this issue.\n\n\nPros:\nThis paper proposes a novel framework based on diffusion probabilistic for point cloud generative task, which achieves state-of-the-art performance.\n\nCons:\nSimilar model structure, concept of nonequilibrium thermodynamics, and the Markov chain kernel seem to be already used in related articles in the 2D image field, therefore weakening the contribution of this article. Besides, conditional generation via latent code has been widely adopted in generative models. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A sloppy adaptation of the existing model to a novel application context.",
            "review": "This work considers the problem of generative modeling of 3D shapes, represented as point clouds. It embraces the probabilistic approach, and proposes to use reverse diffusion process modeled by Markov chains to model point cloud densities on the surfaces of 3D shapes and a parametric shape latent prior implemented by normalizing flows to capture global shape information. This model was mainly developed in the related work by [1, 2] (for applications with images), while a flow-based shape latent prior was proposed in [3] with continuous flows and [4] with affine coupling flows. The model is optimized with the maximization of the lower bound approximation of the log-likelihood.\n\nExperimental results suggest that the proposed method achieves state-of-the-art results on point cloud generation and autoencoding tasks.\n\nPros:\n1) Strong experimental results, according to quantitative evaluations.\n\nCons:\n1) There are minor sign-related mistakes in math and overall quality of the written text could be improved. Additionally, Equations 9-10 are not properly derived neither in the submission, nor in the related works ([2] uses these results without any references, so, these results should be justified by a proper derivation or a correct reference).\n2) Somewhat limited novelty. Proposed derivations of the training objective do not differ in principle from the derivations provided in [1,2] (except for the addition of the conditioning on the global variable $z$ in some distributions and an additional KL-divergence term between the posterior and the prior for this latent variable, associated with the variational inference performed for this global variable). However, given the strong performance, this might not be the most critical issue, since the application context of the model differs from the related work.\n3) Non-informative qualitative evaluations. Since strong results is the main potential reason to accept this paper, and the authors claim to achieve state-of-the-art, it is required to provide both quantitative and informative qualitative evaluations. Provided visualizations are not really informative in terms of the qualitative evaluation of the approach, because of their low quality. The authors should consider using better visualization techniques, e.g. the authors of [3] released their code for much prettier visualizations. In case of autoencoding, there should be a figure which put the results from different approaches next to each other, comment on the differences between the them, and clearly indicates the improvement.\n\nGiven the stated issues, I can not recommend to accept the paper in its current state. However, if the text is polished, training/testing protocols and results for the generative task are clarified (see additional comments) and a proper qualitative analysis is performed, I will consider improving my rating.\n\n###################################################\n\nAdditional comments:\n\n[4] should be cited in the related work.\n\nEquation 7:\n\nA «$-$» is missing after the first equal sign. Both $D_{KL}$ terms should have the same plus sign after the second equal sign.\n\nEquation 8:\n\nBoth $D_{KL}$ terms should have the same plus sign.\n\nEquations 9-10:\n\n[1] do not provide these results, [2] states these results without references and do not provide any additional justifications why it is true. Thus, since [2] is not published, either the derivation of this should be provided or an appropriate reference should be found.\n\nEquation 13:\n\nBoth $D_{KL}$ terms should have the same plus sign. Resulting $D_{KL}$ between posterior and prior distributions for shape latent does not have a closed form. To obtain the closed form for it it should be decomposed into log-likelihood and entropy similarly to [3, 4].\n\nTable 1:\n\nMMD-CD, MMD-EMD, and JSD should have scaling coefficients for metric values. Why did you decide not to include oracle performance evaluations? They are also informative about the suggested approach. Reported results for PointFlow [3] for those metrics, differs significantly from those reported in [3, 4]. How can you explain this difference?\n\nSection 5.2:\n\nSince CD and EMD, as well as the generative metrics, are sensitive to the scale of the input data and point clouds cardinality, it does not make sense to compare approaches trained and tested «using the setups reported in their papers», (unless they are the same). On the contrary, it is desired to fix the protocol for all the approaches by using the same input data and preprocessing (rescaling, normalization) for training and the same evaluation procedures obtaining metric values. It is not clear from what you wrote, if you compare the models properly.\n\nSupplementary:\nDerivation, after application of the Bayes rule:\n\n$q()$ is missing.\n\nEquation 15:\n\nBoth $D_{KL}$ terms should have the same plus sign.\n\n[1] Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In ICML’15.\n\n[2] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Arxiv preprint, 2020.\n\n[3] Yang, G., Huang, X., Hao, Z., Liu, M.Y., Belongie, S., Hariharan, B.: PointFlow: 3D point cloud generation with continuous normalizing flows. In ICCV’19.\n\n[4] Klokov, R., Boyer, E., Verbeek, J.: Discrete Point Flow Networks for Efficient Point Cloud Generation. In ECCV’20.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #4",
            "review": "============================\nSummary\n\nThis paper proposes a new paradigm for learning 3D point cloud generative model using the diffusion process from Nonequilibrium Thermodynamics. The method models the forward diffusion process as adding noises to an input 3D point cloud shape and finally make it to be a noise distribution, while learning the parameters for a reverse diffusion process parametrized by a neural network that converts a noise point cloud back into a 3D shape point cloud. The forward/backward process is modeled as a Markov chain process and each point is considered independently. Experiments show that the proposed methods show superior or comparable performance compared to CNF-based methods and score-based PointGF.\n\n\n============================\nPros\n\n1) There are many different ways for learning 3D shape point cloud generative models, other than the traditional GAN, VAE. For example, auto-regressive models, CNF-based models and score-based models are showing promising results. This paper presents yet another idea of modeling points in the shape point cloud as independent particles, and formulate a forward/backward Markov chain diffusion processes for point cloud shape auto-encoding, generation and interpolation. \n2) The method/losses are well curated, reasonable, sound and easy to understand. The losses and network implementation details are well clarified and are quite novel in my opinion.\n3) Experiments show promising quantitative and qualitative performances on tasks, such as point cloud shape generation, reconstruction, unsupervised feature learning.\n\n\n============================\nCons/Questions\n\n1) the numbers are not creating significant margins over previous works, and the qualitative figures look similar as well. Can you show shape generation examples with more complicated part structures?\n2) Why do you need the \"Prior Flow\" module? It's a CNF-based module right? You are claiming that in the training you don't need ODE-integration? Why is this necessary? Can you directly align the z shape latent code space to the w Gaussian random variable space, like a standard VAE?\n\n============================\nOverall Rating\n\nI like this paper since it brings to the readers a new paradigm of learning 3D point cloud shape generative model using forward/backward Markov chain diffusion processes, borrowing ideas from the field of Nonequilibrium Thermodynamics. This brand-new idea can potentially inspire future works on point cloud generative modeling. Experiments also demonstrated that this method can achieve state-of-the-art performance on shape generation and auto-encoding tasks.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}