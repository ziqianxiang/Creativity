{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Reviewers liked the concept of the zero-day attack and yet raised different concerns about the other parts of the paper. In general, Reviewers wanted to see more thorough experimental evaluations (e.g., against blackbox attack and adaptive attack) and improved clarity of the theoretical analyses. AC encourages authors to incorporate Reviewers' comments when preparing the paper for elsewhere."
    },
    "Reviews": [
        {
            "title": "Confusion in the proof of lemma 1",
            "review": "Summary:\n\nThe authors propose a novel approach to detect adversarial examples by measuring the perturbation norm after a counterattack. They theoretically provide a certified detecting performance and empirically show that AttackDist can characterize zero-day adversarial samples.\nStrength:\n1. The paper is well-organized and easy to understand.\n2. Zero-day attacks are challenging but more practical than other adversarial samples detection problems. AttackDist can perform significantly better than the baselines to distinguish zero-day adv examples.\n3. The authors show experiment results across different attacks and adversarial distances; AttackDist is consistently better than baselines.\n \n\nQuestions:\nThe idea of this paper is interesting and i would like to raise my rating if the authors can clarify my questions.\n\n1. The proof of lemma 1 is the key to developing AttackDist. However, I am confused about 'Due to the continuous of f, â€¦'. My question is how you make sure there always exists the point P between x and x*, such that g(P)=0; what if the deep neural network output is bounded.\n2. Why not evaluate the BlackBox attacking approaches like ZOO and NATTACK[1,2]? Especially, NATTACK learns adversarial examples' distributions, and AttackDist is based on adversarial samples' distribution.\n3. I am curious about whether AttackDist still works for large margin-based classifiers; what if the deep neural network is optimized with large margin loss function.\n\n\n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017.\n[2] Li, Yandong, et al. \"Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks.\" arXiv preprint arXiv:1905.00441 (2019).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "NA",
            "review": "Summary: this paper is about adversarial detection based on counter-attack. The main intuition is that the adversarial sample lies closer to the decision boundary and hence if we do counter-attack to the data, the perturbation is expected to be much smaller than the clean data.\n\nAlthough the idea sounds interesting, I have a few questions to be answered:\n1. in eq(1) you already defined $\\Delta$ as the minimum distortion, how come it is larger than $\\delta^*$ in eq(3)?\n2. you assumed the $\\| y-x \\|_p=D(x)\\sim N(\\mu, \\sigma^2)$, have you verified this empirically?\n3. since $D(x)$ is obtained with optimal perturbation, i.e. $\\|\\delta^*\\|$, how do you obtain the $\\mu$ and $\\sigma$?\n4. from theorem (3) it seems that this detection method works for advanced attack, which requires $r_2 < r_1$, what if the attacker is a bad algorithm causing adversarial samples far from the boundary?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Paper2915",
            "review": "This paper proposes a method to detect adversarial examples. The detection\nscheme is based on the observations that typical adversarial attacks generate\nadversarial examples on the decision boundary, so if we use a \"counter attack\"\non the adversarial example, it will be easy to change its label.\n\nA main weakness of this paper is that the proposed approach does not include an\nadaptive attack for evaluation.  If the proposed detection scheme is known to\nthe attacker, the attacker can still generate visually indistinguishable\nadversarial examples that the detector fails to detect. This can usually be\ndone by adding the detection objective to the loss function for attack.  Many\nheuristic adversarial example detections and defense methods have been broken\nby stronger and adaptive attacks [1,2], and the use of adaptive attacks is\ncrucial [3].\n\nAdditionally, although the paper claims to detect zero-day, or unknown attacks,\nin evaluation the selection of attacks are quite limited. For example, it only\nincludes gradient based attacks, but not decision based attacks or evolutional\nadversarial attacks.\n\nThe paper attempts to make several theoretical justifications, but these\ntheorems are too simple (e.g., based on direct application of triangle\ninequality) and do not significantly improve the contribution of this paper.\n\nAs a conclusion, I cannot support the acceptance of this paper because the\nnovelty of the proposed method is limited and evaluation is insufficient.\n\n\n[1] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420 (2018).\n\n[2] Carlini, Nicholas, and David Wagner. \"Adversarial examples are not easily detected: Bypassing ten detection methods.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017.\n\n[3] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705 (2019).\n\n[4] Brendel, Wieland, Jonas Rauber, and Matthias Bethge. \"Decision-based adversarial attacks: Reliable attacks against black-box machine learning models.\" arXiv preprint arXiv:1712.04248 (2017).\n\n[5] Cheng, Minhao, et al. \"Query-efficient hard-label black-box attack: An optimization-based approach.\" arXiv preprint arXiv:1807.04457 (2018).\n\n[6] Alzantot, Moustafa, et al. \"Genattack: Practical black-box attacks with gradient-free optimization.\" Proceedings of the Genetic and Evolutionary Computation Conference. 2019.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Insufficient experiments and unreasonable assumption ",
            "review": "This paper proposes to use a counterattack strategy to attack an input x, and calculate the distance between x and the crafted example x' as the detection metric. There are two main concerns about this paper:\n\n1. The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism. There is no guarantee on the effectiveness of AttackDist when the attacks do not follow assumed patterns.\n\n2. In experiments, there are only oblivious attacks, where the adversaries do not know the mechanism of AttackDist. For a defense method, it is necessary to carefully design a convinced adaptive attack and demonstrate the effectiveness of the defense under the adaptive attack.\n\nMinors:\nIn the introduction section, the authors claim that \"Existing adversarial defense techniques could be classified into two main categories: adversarial training and detection\". Actually, there are many other types of defenses including input processing, robust architecture, random smoothing, certified defenses, etc. Besides, existing adversarial training methods like FastAT can easily scale to ImageNet, running for several hours on a single GPU. The authors should be more updated on these related progresses.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}