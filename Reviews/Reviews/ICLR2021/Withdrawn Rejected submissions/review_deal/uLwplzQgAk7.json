{
    "Decision": "",
    "Reviews": [
        {
            "title": "'max-affine spline operators'-framework applied to Deep Generative Networks",
            "review": " In the presented paper, the authors apply the well-established powerful framework of Max-affine spline operators (MASO) to analyze Deep Generative Networks (DGN). The authors relate the local intrinsic dimension of the generator to the rank of the network layers and nonlinearities. The effect of dropout on DGN  is analyzed from a MASO perspective, and the authors hint at the possibility to utilize that for intrinsic dimensionality estimates based on the approximation error of DGN (without actually demonstrating it). Finally, among other results,  mathematical expressions for the output density and the differential Shannon entropy are provided. \n\nMain strengths:\n* The presentation is very clear\n* The perspective of MASO on DGN is a refreshingly innovative research agenda.\n\nMajor weaknesses:\n* The theoretical analysis seems still to be in a rather premature state. The paper reads partially more like a research agenda than a fully completed project. Besides some (rather obvious) bounds on dimensionality, a careful comparison of analytical and numerical results is missing (e.g. of the analytical expressions for entropies and densities).\n* The analytical results are mostly bounds in terms of rank or implicit expressions, so there seems no direct relevance or applicability for real-world problems. At the same time, I agree with other reviewers that the theoretical insights are rather limited and not particularly surprising.\n\nThe research agenda seems very promising, so a resubmission at a later stage might be advisable.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A well written theory paper on the properties of piecewise affine generative models, but I could not identify a non-trivial results.",
            "review": "The authors propose to investigate the mathematical properties of Deep Generative Networks relying on max-affine non-linearities. This encompasses many classical networks, essentially using (leaky) ReLU activation functions. The authors provide results pertaining to various aspects of the mapping from latent to generated samples. \n\nStrengths: \n- This is as far as I know the first paper providing results for the case of generative models.\n\nWeaknesses:\n- Almost all results are rather straightforward application of basic linear algebra considerations,\n- The statement of Theorem 2 and the proof unclear,\n- It is unclear where the Max-affine assumption plays a role,\n- The insights and impact of the work is moderate,\n- Compare to published papers in the field of characterization of DNN topology (mostly focused on classifiers though), the mathematical novelty is very low.\n\nMain comments: \nGoing through the large number of formally stated results (theorems and propositions), they mostly rely on the basic assumption that the mapping is piecewise linear, combined with some basic linear algebra (reasoning on the interperplay between rank and dimension, orthogonality,...). Theorem 2 is of slightly different kind, although also intuitive at a high level (fitting a low dimensional manifold to higher dimensional data), however it is unclear to me that as stated it is correct, and the proof was too elusive to help me understand. Essentially, I assume that to make such a strong statement (with a strict inequality on the error for an arbitrary choice of points), one needs to dig into the \"adaptivity\" properties of the boundary between affine domains, which does not seem specified in this setting.\n\n\nQuestions:\n- It is unclear in several places what assumptions on the network are necessary, and why. I think of the max-affine assumptions (implying convexity) that I failed to see used explicityly, also some results explicitly enforce leaky reLu. Could you elaborate on that?\n- The main challenge to me is recovering the boundaries of affine domains. What are the possible approaches and their complexity?\n\n\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A powerful analysis framework that can sheed light into the training of GANs",
            "review": "Spline operators are develop to analyze DGNs. Among many interesting results, authors are able to analytically compute key quantities such as marginal likelihood and Shannon entropy of the induced distributions. The importance of these results is very large, and may bring potential robust methods to train DGNs. \n\nExperiments are exhaustively analyzed and the paper its clear in all points, though certainly the theory behind is not straightforward and the reviewer might be losing important aspects in the theoretical derivations. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "subpar article on deep generative models using neural networks with piecewise linear activations",
            "review": "This paper explores deep generative models built from neural networks with piecewise linear activations. The main directions of study are the dimension of the images of the affine pieces, the training error when reconstructing a dataset, the invertibility of the generator, the disentanglement of the representation, and some observations about the push forward of a latent space density under the mapping from latent inputs z to the generated outputs G(z). \n\nStrong Points: Understanding, bounding (both above and below), and exploiting the complexity of generative models is of significant interest. Using the combinatorics and geometry of functions computed by neural networks with piecewise linear activations is a sensible vector of approach for such problems. \n\nWeak Points: I feel the theoretical results in this paper are (a) not helpful in understanding deep generative models and (b) simple restatements of elementary results in linear algebra and analysis. For each of the results, I indicate the elementary facts from which they follow and my reasons for thinking they are not very useful for understanding deep generative models. In each case, I may well be missing something and am happy if the authors point out some aspect I didn't understand.\n\n1. Theorem 1: The image of a polytope under an affine map is a polytope and the image of a connected set under a continuous map is connected. Moreover, the union over w of Aff(w;A_w,b_w) need not be a disjoint union, so saying that Im(G) is made of the connected polytopes Aff(w;A_w,b_w) is misleading unless the map G is injective. It is claimed in Proposition 3 that G is often injective,  but I am not convinced that statement is correct (see below).\n\n2. Proposition 1: matrix rank is sub-multiplicative\n\n3. Proposition 2: matrix rank is sub-multiplicative\n\n4. Proposition 3: this statement, at least precisely as written in the paper, appears to be true only when restricted to each region w, in which case it is the statement that an affine map from a lower to a higher dimension space is injective if and only if it has full rank. A counterexample to the general statement is the following single hidden layer leaky ReLU network with leaky parameter 0 < a < 1 : \nG(z) = c_1 * max{az, z} + c_2 * max{-az,-z} = |z|,\nwhere c_1 = c_2 = 1/(1-a). Here S = D = 1 and the rank of A_w is 1 on both affine pieces but the network is not injective. \n\n5. Proposition 4: Even if the conclusion of Proposition 3 holds, I am not sure I see the purpose of solving a minimization problem for the inverse of x if one already assumes that G_w^{-1} is given. If it is not given, then I don't see how to solve the minimization problem. \n\n6. Proposition 5: Right multiplication of a matrix by a standard basis vector returns a column of that matrix. The notion of disentanglement here seems to be that the columns of the matrices A_w are nearly orthogonal. I donâ€™t find this to be particularly interesting since learning a disentangled representation should be a global property of the full generator G instead of a property locally on each of its many many affine pieces.\n\n7. Lemma 1: Change of variables formula for an affine transformation. \n\n8. Theorem 3: Change of variables formula for an affine transformation. Moreover, this formula does not strike me as particularly helpful. The number of affine pieces A_w is typically very large. So any formula that is the sum over all such pieces of some quantity does not seem very useful. \n\nRecommendation: Unless I a missing some key aspects of this paper, I strongly recommend that it be rejected because I found that the results are neither technically nor conceptually insightful. ",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}