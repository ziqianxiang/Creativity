{
    "Decision": "",
    "Reviews": [
        {
            "title": "The idea  of using mirror samples is interesting, but the effectiveness needs further justification. ",
            "review": "In this paper, an unsupervised domain adaptation approach is proposed based on mirror samples. They construct a minor sample in the target domain for each source domain sample, and vice versa. The minor sample is constructed by a (weighted) average of the top K nearest neighbors in the other domain. Then, they align the two domains using the local similarity of each sample and that of its mirror sample, where local similarity is defined as the similarity to the class centroid of two domains. They achieve SOTA results on benchmark datasets.\n\nPros:\n+ Methodological development is clear. \n+ The idea of using mirror samples to align distributions is interesting.\n+ The proposed method achieves SOTA results on benchmark datasets.\n\nCons:\n+ The proposed method is actually quite similar to the SRDC approach, where a discriminative clustering is employed. Moreover, a source sample selection approach was used in SRDC to reweight the source loss. Basically, the L_{mirror} loss on the source domain samples follows the similar idea. The authors should further clarify the relation with SRDC through a rigorous derivation. \n+ On the other hand, constructing mirror samples for domain adaptation is not new. barycenters are kind of virtual mirror samples in the optimal transport, which has also been exploited for domain adaptation. What is the advantage of using the proposed mirror sample construction method rather than the barycenter? Could you also present a comparison with using the barycenters to replace the virtual mirror sample in this work? \n+ Although SOTA results are achieved, the results are quite similar to SRDC. From Table 2, this largely attributes to the usage of DC. The FC mirror and Backbone mirror actually bring little improvement. It is better to show more ablation studies on other datasets.\n+ From Table 3, the results of using K=1 is already quite competitive, this also validates the usage of mirror sample is not so effective. \n+ Visualization like t-SNE is necessary to illustrate how the proposed method works. It is better to provide a sequence of visualization to monitor the procedure of alignment. \n\n\nminor issues:\nThe paper writing needs a significant improvement:\n+ The usage of $\\mathcal{D}$ and $D$ is confusing. A clear definition of symbols is necessary.\n+ Many typos, for example, Table ?? in Section 5.3. \n+ Several references are incomplete. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "method not well motivated",
            "review": "In this paper, the authors propose a mirror-sample based distribution alignment method for unsupervised domain adaptation. Specifically, virtual pairs in the two domains were found such that additional losses can be enforced to push the mirror points closer to each other. The method shows slight improvement over existing methods.\n\nThe motivation of the proposed method is not convincing. The authors claim that “traditional class-aware method aligns the statistics (moment estimations) of the distribution but cannot align the shape of distribution”. How to define the “shape” of a distribution? Existing methods that based on MMD or adversarial loss are guaranteed to match distributions rather than just moment estimations.\n\nGiven two distributions without any correspondence, it is theoretically impossible to detect virtual correspondences without additional assumptions. The proposed method relies on some heuristics to choose those mirror points, which is not principled.\n\nThe domain adaptation setting is not correctly discussed in the introduction. Not all the existing methods assume covariate shift. Many methods such as the class-wise or conditional matching assumes changes in p(y|x). That is why these methods could benefit from conditional matching. Also, the change in p(y|x) is not called label shift. Label shift means p(y) changes while p(x|y) stays the same [1][2].\n[1] Zhang, Kun, et al. \"Domain adaptation under target and conditional shift.\" International Conference on Machine Learning. 2013.\n[2] Lipton, Zachary C., Yu-Xiang Wang, and Alexander J. Smola. \"Detecting and Correcting for Label Shift with Black Box Predictors.\" ICML. 2018.\n\nMinor issues:\nIn the abstract, the underline distribution -> the underlying distribution. Also, the definition of the “internal distribution of the underline distribution” is not clear.  A distribution is simply a probability distribution over random variables.\n\nIn Sec 5.3, table reference is missing.\n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review comments",
            "review": "This paper considers the problem of unsupervised domain adaptation, where the training set is composed of a labeled source domain and an unlabeled target domain. The goal of this task is to learn models from the source domain and transfer the learned knowledge to the target domain. The core issue of the unsupervised domain adaptation is that there exists a domain shift between the two domains. The domain shift between the two domains can be the lighting issue, the style variations, the class distributions, to name a few. To address this task, the paper develops a method called (virtual) mirror, which establishes the connection across domains and pushes the virtual mirror pairs together in the aligned representation space. Contrary to most of the existing methods, the proposed method does not align the samples across domains, resulting the advantage that there is no internal distribution of the underline distributions. Experiments are conducted on the Office-31, Office-Home, ImageCLEF, and VisDA2017 datasets. Results of the proposed method show some improvements over competing approaches.\n\nQuality: Although the paper is mostly easy to follow, there are some low-level errors that should be detected before submission. For example, in page 7, the is a \"??\" symbol in the sentence that I do not know what it refers to. The paper should be proofread thoroughly before submitting it for review.\n\nClarify: The propositions and lemmas are given with proofs. This is good. Figure 2 and Figure 3 are clear to me. This paper is in general easy to follow.\n\nOriginality: Given that the core contribution of this paper is the idea of virtual mirror, which represents the counterpart sample in the other domains, I believe this kind of idea has been explored in some form in existing literature. For example, unsupervised domain adaptation methods that leverage image-to-image translation models to generate counterpart sample in the other domains, e.g., CyCADA [a] and CrDoCo [b]. The idea of using image-to-image translation models in these two papers to me is analogous to the idea of virtual mirror. However, the paper did not discuss the advantage/disadvantage with these two papers. \n\n[a] Hoffman et al. CyCADA: Cycle-Consistent Adversarial Domain Adaptation. In ICML, 2018. https://arxiv.org/pdf/1711.03213.pdf\n[b] Chen et al. CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency. In CVPR, 2019. https://arxiv.org/pdf/2001.03182.pdf\n\nSignificance: Given that the virtual mirror idea is similar to those in [a] and [b], without a clear discussion with these two papers, the significance is downplayed. Based on this ground, the contribution of this paper is limited.\n\nRequest for author response: I would like to see how the authors discuss the pros and cons of their paper with [a] and [b].\n\nRating: Given that the virtual idea is similar to [a] and [b] and they are not acknowledged in the paper, I can only rate 4 for this submission at this point. I will reevaluate this paper after seeing the reviews from the other reviews as well as the author response.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "a method for unsupervised domain adaptation by exploring cross-domain sample-to-sample consistency and clustering-guided pseudo-labeling",
            "review": "This paper proposes a new method for unsupervised domain adaptation (UDA) that leverages the cross-domain sample-to-sample consistency (called mirror samples) and employs a pseudo-labeling loss via a previous deep clustering method (Jabi et al. 2019).\nGenerally speaking, the proposed method is easy to understand and achieves better results than prior state-of-the-art methods on several closed-set UDA benchmarks.\n\nStrength:\n1. a new cross-domain alignment technique through sample-to-sample consistency\n2. better results on several benchmarks for closed-set UDA\n\n\nWeakness:\n1. missing related works e.g. [a]\n2. the contribution or effectiveness of the proposed terms. First, as shown in Table 9, the weighting term in Eq.(6) seems to be redundant, using 1/k works quite well. Second, as shown in Table 2, the improvements brought by backbone mirror or FC mirror are somewhat small, it is hard to believe the main contribution in this paper (i.e. mirror loss) is significant. Third, it seems 72.6 for \"office-home\" in Table 1 is a typo, which is not consistent with 72.00 in Table 2 and 72.01 in Table 3. BTW, the accuracies shown in this paper look messy, e.g. different significant digits.\n3. novelty is somewhat limited. Besides the mirror loss, another proposed loss $L^t_{dis}$  is the same as that in Jabi et al. 2019. As shown in the experiments, this term is the most important one for closed-set UDA tasks. Furthermore, even the proposed mirror loss looks quite similar to that of Pan et al 2019.\n4. Another concern lies in whether the proposed method can work for non-closed-set UDA, e.g. partial-set UDA since clustering is involved in the learning method that needs to know the number of clusters.\n5. Final reminder. It is a good habit to report the results of your baseline methods since it would help readers to really know the improvements of the proposed method. For instance, directly using the results from older papers in the second line for comparison is not fair. As far as the reviewer knows, the average accuracy of the \"src-only\" baseline is expected to be around 55\\% on Office-Home.\n\n[a]. Das, Debasmit, and CS George Lee. \"Sample-to-sample correspondence for unsupervised domain adaptation.\" Engineering Applications of Artificial Intelligence 73 (2018): 80-91.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}