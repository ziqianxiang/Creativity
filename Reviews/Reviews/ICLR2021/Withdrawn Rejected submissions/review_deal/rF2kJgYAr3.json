{
    "Decision": "",
    "Reviews": [
        {
            "title": "Defines a language that may be interesting, but there is no syntax, semantics and no examples of the language and results are not reproducible.",
            "review": "This paper defines a language that might indeed be interesting and significant. There are no examples of sentences written in the language. The semantics of the language - what statement in the language might mean (even informally) - is never defined. The results look impressive, but they are not reproducible. My guess is that they have nothing to do with the language, but have a better neural architecture or better optimization. You need to argue that the results are the result of the language. Perhaps using the language allows the user to think and debug at a higher level and explore more of the design space? What are the statements in your language that produced the results? I saw the example in table 4, but that makes no sense to me; it has the syntax of logic, but I don't know what the semantics has. How would someone come up with such statements? Are there in principles?)\n\nYou spend a lot of time and an example on the rule graph construction. However, this seems like an implementation detail. Sections 4.2 and 4.3 are unreadable; diving into details without giving the big picture.  (\"how to compute an individual rule\" makes no sense to me.)  I suggest that you present simple examples of statements in your language and then show the network that is created.\n\nThe results (Section 5.2) are not reproducible. Give the exact inputs that produce the outputs. (Perhaps use these as motivating examples early in the paper). Why do you think the results are the result of using your language?\n\nMinor comments:\n\nwhat does \"makes *all* proves all...\" mean? (p 2)\n\nCan't relations have real values? eg, How would you represent the length of flight? Do you have to reify all such relations? How does that affect the code produced?\n\nwhy do you think \"first-order logic expressiveness cannot be exactly represented by means of differentiable function operations\"  (p 6) mean? Do you have references or a reason? (Your Horn clauses without function symbols cannot represent first-order logic).  What does \"exactly represented\" mean?\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice paper but needs another round of improvement.",
            "review": "The key idea in this paper is to combine the power of logic programming and neural networks. Specifically this allows for neural networks to be defined as logic programs but handle numeric attributes and functions. The paper proposes NeuralLog, a FOL language that is compiled to a neural network. The key improvement over the other neural logic systems is that it can handle numeric values. The experiments compare against two SRL/neural logic systems with the same underlying theories and the results broadly demonstrate the superiority of the proposed approach.\n\nThe paper is written well. The problem is well motivated and the solution appears to be reasonable. The background and related work is covered well with the authors citing both recent work and some long time work in the area. The solution (both the graph construction and the knowledge representation parts) are presented well enough to clearly understand the compilation procedure. The use of walks to describe the features is quite reasonable. I specifically like the introduction of any predicate. The intuition of any/2 predicate seems to introduce the latent relationships between the features. If correct, this is really an elegant way to \nintroduce more richer relationships.\n\nWhile there are quite a few things I like about the paper, there are some room for improvement:\n\nThis may be my misunderstanding, but is the rule below Figure 1 correct? Reading tree appears to be a disjunction of conjunctions and these are not necessarily the paths from source to target. (similar to a relational random walk). Shouldn't it be just p0 \\wedge p5 \\wedge \\p2?\n\nThe biggest issue seems to be with the experiments. First, just two baselines are considered. Why not use these as features for a different neural model (all of which have been cited in the background section)? More baselines are mandatory to publish this work.\n\nSecond is the set of data sets -- most of these are old SRL data sets where many methods have previously shown to exhibit good performances. In such cases, the baseline performances are quite weak for the considered paper.\n\nThird is the discussion of the results. Most of the space in the experimental section, can be moved to the table. Using long sentences for presenting numbers seem a waste of space. I would have instead preferred that the authors use the space to speculate their superior performance. Over all, this is the weakest link of the paper.\n\nI feel that the experiments part was rushed to make the submission. This needs at least one more iteration before it can be accepted.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper with some unclear points",
            "review": "The paper introduces a new approach for learning the parameters of a first-order logic program using deep learning techniques. It describes a system, called NeuralLog, which compiles a first logic program into a (Deep) neural network and uses a deep learning framework such as Tensorflow to learn the parameters. Experiments on well-known datasets comparing NeuralLog with similar approaches such as TensorLog and RelNN were performed.\n\nOverall, the contribution is very good. The idea of compiling a logic program into a neural network (NN) is very challenging and interesting. It bridges the gap between symbolic (first-order logic) and sub-symbolic (Deep learning) AI favouring AI systems that are explainable and human-understandable i.e eXplainable AI (XAI). The paper is well written and well presented. However, there are some unclear points that, if solved, could considerably improve and validate the proposed work.\n\nSection 3: Background Knowledge\n- The paper does not allow the negation of atoms. This means that queries such as ?- not(p(a, Y)) are not possible. This reduces the expressiveness of the approach. The paper should also clearly motivate this limitation.\n\nSection 4.1: Inference\n- The paper presents how to perform inference by answering the query ?- p(X,Y) for X=a or Y=a but it does not clearly describe how to perform ?- p(X,Y) with X and Y unknown. This query generates couples of entities (constants) (X=a, Y=b) such that P(X,Y) is true. \n-  Minor issues: second paragraph\n “... for a given a query ...” -> “... for a given query ...”\n\nSection 4.2: Rule graph construction\n- The reflexive edge in Fig 1.a should probably be p5(Z) not r5(Z)\n- This statement should be improved: “In order to improve the compiled network, by accounting by the influence of disconnected terms, ...”\n- The paper states that unconnected nodes may influence the output node. But it is not clearly explained in which situation.\n- When collapsing many any/2 nodes into one any/n node, the paper states that the last argument of any/n should be the destination term but nothing is said about the order of the other terms. Is the order random? If yes, what is the default one?\n- Typos in paragraph 9:\n“we recursively transverse” -> we recursively traverse\n“... whose value in given from the input. ” -> whose value is given from the input. \n\nSection 4.3 Network construction\n- By construction, the depth of the compiled network depends on the logic program. However, the paper states that this depth can be increased by means of predicate invention. It is not clearly stated how predicate invention is done and how it integrates the basic network. Is this left for future work? Or does the current proposal support predicate invention?\n- Major issue:\nCompiling a program into a neural network should maintain the first order logic semantics. The paper clearly states that the authors “try to make the semantics of the neural network as close as possible as the one of the logic program.” This should be explained in detail. What are the situations in which both neural networks and first-order logic semantics are equivalent? Are they never equivalent? This core issue should be clearly explained.\n- The paper limits the arity of predicates in the program and background to 2 and motivates this limitation by the fact that Tensorflow 2.0 cannot handle sparse matrices with more than 2 dimensions.  Even if it is possible to convert a predicate of n arity to arity 2, this is a strong limitation of the proposed approach since many datasets allow predicates of arity more than 2. Converting them to predicates with arity 2 not only could reduce the expressiveness of the initial program, but also could increase the computational burden necessary to compile the program.\n- In order to understand the structure of the compiled network it is necessary to provide a full (but simple) example starting from a program and background knowledge to the compiled neural network. Example 1 in the Appendix could be used. \n- Unfortunately, the code has not been made available. I believe that publishing it would have helped the revision work, favoring the reproducibility of the tests and the control of correctness of the system.  \n\nSection 5 Experiments\n- Since the depth of the compiled network depends on the program as stated in Section 4.3 it would be necessary to provide the depth of each program used to perform experiments.\n- Besides the number of epochs, the paper does not explain how the hyper-parameters (eg. learning rate) have been set.\n-Besides the area under the ROC curve used to compare the performance of NeuralLog with respect to the other systems, the area under the Precision-Recall must also be computed. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "a bit incremental",
            "review": "The paper proposes NeuralLog, an integration of neural networks with logic that is similar to TensorLog and differs from it\nin the way the logic rules are compiled to a neural network and by allowing numeric attributes and numeric \nfunctions.\nThe proposal is evaluated on five dataests, three for link prediction and two for classification and compared\nwith TensorLog and RelNN respectively. The results show an advantage of NeuralLog in terms of AUCROC.\n\nNeuralLog seems an incremental improvement over TensorLog, changing slightly only the way in which the network is built.\nThe authors claim that they allow numeric attributes which are not allowed by TensorLog but in the TensorLog paper\nthe authors say \"for instance\nfor f = costar(ginger rogers,fred astaire) one might plausibly set \u0012\\theta_f to the number of\nmovies those actors appeared in together.\" so numeric attributes are allowed also in TensorLog.\nMoreover, the differences in neural network construction are not discussed in details and seem minor to me.\n\nThere is a point that is not clear: the author say that they apply tanh to the output of the rules. \nHowever, tanh has output in [-1,1] so the resulting weights could be negative. In this case, as also\npointed out by the authors of TensorLog, multiplication to represent AND does not make sense, how do\nthe authors deal with negative weights?\n\nThe experimental results do not seem to show a clear superiority of NeuralLog. On WordNet, the differences\nare small and a significance test would help. On Cora, I do not understand the difference between learning\nthe predicates separately or together. \n\nOn UWCSE, the dataset is highly unbalanced and the area under the precision recall curve would be more adequate.\nThe difference between NeuralLog and TensorLog when supplied with the same theory is small and again a \nsignificance test would help.\n\nAmong the reason for the better performance of NeuralLog, the authors say that it may be due to \nbetter initial parameters but the paper does not discuss the setting of the initial parameters.\n\nThe presentation is also problematic, see the comments below\n\nThe sentence\n\"It is important to notice that the use of first-order logic is not the only way to allow neural networks\nto be applied to relational tasks. However, the other approaches, although promising, are beyond the\nscope of this paper.\"\nshould be either removed or the other approaches to combine neural network with relational data should be described\nas they are related to NeuralLog.\n\nThe authors say they use Horn clauses (which have no negation) but then discuss how to handle negation.\n\nOn page 3 the authors say that the nodes of the graph are terms: it seems to me they are only variables.\n\"remaining terms in the head\": there is only one\nPage 5 \"we recursively transverse the DAG until we\nreach a source node, whose value in given from the input\" it seems that algorithm 1 starts from source nodes.\n\nThe distinction between numeric attributes and function is sometimes blurred. I would add a description of functions\nin the paragraph where numeric attributes are presented.\n\nThe term \"target predicate\" is not defined, similarly for \"function predicates\".\n\nThe term \"rules with free variables\" is defined only in the appendix. Also, free variables have a specific meaning\nin logic which is different from the one used here, I would change the name to singleton.\n\nThe English is sometimes problematic, with various grammar errors.\n\n---After reading the other review and the authors' comment, I still think the paper is too incremental with respect to Tensorlog, so I will keep my score.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}