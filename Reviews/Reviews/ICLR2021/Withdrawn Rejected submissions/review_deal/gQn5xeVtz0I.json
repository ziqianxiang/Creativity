{
    "Decision": "",
    "Reviews": [
        {
            "title": "Data reconstruction  from gradients",
            "review": "The authors present an improved analysis of data leakage from gradients over Zhu et. al.  2019 in a distributed deep learning setup. \nThey show analytically the relationship between the structure of Fully Connected and Convolutional networks and the re-constructability of the input, and propose an improved algorithm for reconstruction.\n\nThe paper is well-written and the analysis looks sound. The contributions are impressive and valuable.\nI have a few concerns with regard to the empirical results. \n1. In Figures 1 and 2, the authors should show the original images as well for comparison.  \n2. Though the results look reasonable visually, the authors should also include a quantitative measure to verify the accuracy of the reconstructed images. \n3. It would also be useful to see how the reconstruction works when the batch size is larger, say 32 or 128, which are usually the batch sizes employed in practice.  \n4. Are there limits to the efficacy of the proposed reconstruction algorithm? These have not been discussed. \n5. There are a couple of typos that need to be corrected. For instance, on Page 3 line 32 it should be decrease instead of decreases, on Page 5 line 8 it should be derivation instead of derive.\n\nTo summarize, this is a well-written paper making a good incremental contribution. I recommend acceptance. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposed a theoretical study to a recent work by (Zhu et al., 2019) on the adversarial attack of Federated Learnings. The paper introduces a simple yet effective image prior to the whole learning paradigm, then provided theoretical studies on the effect of the proposed approach as well as several related approaches. Experimental results demonstrate that the method is effective and is a good addition to Zhu et al., 2019.\n\nThe contributions of this paper are important in that it provides theoretical analysis that supports the practical results of a few recent works. The writings are clear too and the conclusions are convincing. The paper serves as a good addition to Zhu et al., 2019 as well. \n\nI have a few additional comments, but mostly on the experimental results section:\n- The proposed approach sample initial data from a uniform distribution and apply L2 regularization. It seems tempting to test and provide theoretical analysis for other prior distributions or regularizations. Of course, I fully understand this is too much to ask and beyond the scope of this work, but it'll be helpful if authors can provide some quick explanations in a paragraph of the paper.\n- In Figure 1, the left two figures all have a huge spike in the middle. Could the authors explain what has happened there?\n- I enjoyed reading the first half of the paper before the experiment section. However, in terms of writing, the experiment section looks to be completed in a rush. I saw multiple typos/inconsistencies as well as unclear figure layouts/references. I strongly suggest authors polish this section in future revisions, given that the idea of the paper looks interesting enough\n- Moreover, I had a question about the potential impact of the work for more complicated/modern network structures that utilize, for example, ReLU as well as other more complicated network modules beyond the description of the paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not very interesting",
            "review": "Summary:\nThis paper improves the \"image reconstruction from leaked gradient\" method (Zhu et. al. 2019). It adds two extra loss terms to make the reconstruction converge faster and yield better reconstructed images. It gives some theoretical analysis about when the reconstruction could be perfectly successful. \n\nReasons for score: \nOverall, I vote for rejection. Although the introduced two loss terms improve the reconstruction, as the major contribution they are kind of incremental. Moreover, the theoretical analysis about when we can do perfect reconstruction is not so interesting (reasons given below).\n\nPros: \nThe new loss terms help reconstruction, as shown by experimental results.\n\nCons:\n1. The L2 regularizer in Eq.(2) is quite straightforward. The orthogonal regularizer is kind of interesting, but why it brings benefits as shown in Section 6.2? Each instance in a batch is optimized separately, why this regularizer helps? The authors should provide deeper understanding of it, instead of just throwing this term into the objective.\n2. The theoretical bounds take two pages, but I think they are not very relevant to practice. Because there are strong continuity priors within images, and empirically we probably need much less information to well reconstruct an image. A simple term like the total variation regularizer (\"Understanding Deep Image Representations by Inverting Them\") may have greater help. Stronger image priors could also be considered, such as \"Deep Image Prior\".\n3. Both (Zhu et al. 2019) and this paper are only evaluated on very small images. Such evaluations are sufficient for (Zhu et al. 2019) to demonstrate the idea works, but to make this follow-up work stronger, the authors are suggested to consider reconstruction of larger images of the size of a few hundreds pixels. If such reconstruction can be done efficiently, this could be an important contribution of the paper.\n4. A few typos and formatting problems. E.g. \n    In Page 1, \"... and a new initialization mechanism the primary aim is to ...\", the => whose. Some inline references miss ();\n    In Page 3, \"by two modifications relative to Zhu et al. (2019)\" => \"with two modifications to Zhu et al. (2019)\"; \n    Gan, Cnn => GAN, CNN.\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting topic, wide array of experiments, several network architectures. Impact of the contribution seems marginal, inconsistent structure of the paper.",
            "review": "#### Summary\nThe paper investigates two aspects of input reconstruction based on the gradients of a differentiable model: First, an improvement over the DLG method proposed by Zhu et al.. Second, an analysis on the theoretical bounds that certain neural networks (e.g., fully-connected networks with logistic activations and certain types of convolutional networks) have to meet for the input to be reconstructable from the gradient. Experiments on fully-connected and convolutional networks were carried out to support the effectiveness (faster convergence) of the proposed gradient optimization task.\n\nThe proposed improvements to DLG consist on a different initialization for the dummy data, and the addition of a regularization term to the cost function that minimizes the L2 norm of the dummy and the clean gradients. Two scenarios were considered: when a single sample is passed and when dealing with mini-batches.\n\nThe analysis of the lower bounds for network architectures (whose gradient can be used to fully reconstruct an input sample) shows that said boundaries are as low as one neuron (for fully connected layers), and some ratios that depend on the batch size.\n\nExperiments have been carried out on a wide variety of datasets, with results indicating that the proposed normalization and initialization of dummy data lead to faster convergence (less iterations) and higher quality of reconstructions (qualitatively) when compared against the previous work (DLG) this paper is largely based on.\n\n\n#### Strong and weak points of the paper:\n##### Strengths:\n - Proposed improvements are simple to understand and easy to implement.\n - Theoretical analysis looks into fully connected layers with logistic activations as well as convolutional layers.\n - Propositions indicate that there is indeed a lower bound that is easily met by modern networks\n - Experiments have been conducted in a variety of datasets, supporting the consistency of their findings.\n\n##### Weaknesses:\n - The paper is poorly written, making it hard to follow and assess the validity (and the extent) of the contributions. There are issues with the format, the wording, syntax errors, etc., giving the impression that the paper has not been thoroughly revised before submitting.\n - On the initialization of \"prior image knowledge\": the name is somewhat misleading as there is nothing in the proposed method that is unique to images â€“an initialization for $\\hat{x}_0$ and $\\hat{y}_0$ in the unit cube $[0, 1]^{x_d}, [0, 1]^{y_d}$ instead of the unit ball. More importantly, the proposed initialization seems like a rather small improvement for two reasons: (1) faster convergence is justified but only because input samples are being (arbitrarily) normalized to that same range, and (2) there are no claims on any theoretical benefits for choosing the hypercube over the hypersphere.\n - On the regularization of \"prior image knowledge\": the proposed regularization term is again not uniquely related to images but rather a generic (albeit useful) optimization strategy to prevent arbitrarily large changes in the reconstructed input. These are useful and valuable additions to the general setup of the problem. However they are more likely to fall in a list of \"tips and tricks\" to speed up convergence rather than a novel idea, central to a scientific contribution.\n - Regarding the lower bound for batched inputs: in the absence of any batch-dependent operation inside the network (e.g., batch normalization or a non-iid loss), approximating the gradient of a batch can be reduced to a problem of reconstructing a single input. The lower bound for batched samples could therefore be reduced to the one for individual samples.\n - On the lower bound for convolutional layers: the lower bound found is somewhat trivial (the output has to be bigger than the input). This bound implies that the input can be preserved through identities in the convolutional filters.\n - Experimental results: evaluation on reconstruction has been carried out under various assumptions regarding the architecture of the model (small MLPs and 2-layer CNN models). These disregard their actual performance for the original task (e.g., classification of hand-written digits). We are left wondering if these networks can both leak data through gradients *and* have high performance in their task.\n - There are reoccurring issues with the plots in section 4:\n   - Figure 1 shows the exact same plot on the left and in the center.\n   - Figure 2: no mention of the number of iterations in total ((d), named \"final\")\n   - Figure 3: what loss function is being plotted (same goes for figures 1, 8 and 9). Is it the L2 norm of the gradient reconstruction?\n      - __Taking the log of the actual function (assuming it is the L2 norm of the gradient reconstruction) is misleading as it exaggerates small differences in the actual loss space__\n   - Figure 4: plots (c-f) are really hard to see.\n   - __Figure 5: x-axis deceptively present results (on two occasions) using uneven sampling on an even grid.__\n      - __This is a gross misrepresentation of results and must be corrected immediately!__\n\n\n#### Recommendation:\n - Due to the marginal impact of the contributions and the lack of structure on the experiments, I recommend that this paper cannot be accepted in its current form. I would encourage the authors to consider a major revisions before submitting again.\n\n#### Supporting arguments for recommendation:\n - There are major issues across the entire paper. See the list of weaknesses listed above.\n\n#### Questions to clarify:\n - In which scenario would bounds for batched samples be interesting, if the per-sample bound is already much lower (and a batch can be expressed as a sequence of individual samples)?\n - How big a role does lambda play in the whole optimization process? In other words, how sensitive is the result to small changes in the value of $\\lambda$?\n - How reproducible are the results (several runs, different initialization)?\n - Are the results reproducible with networks that have been trained (or even converged) on their original task? It is known that training a network may cause neurons to saturate and kill both activations (and/or their gradients). How does this affect the bounds claimed in this work?\n - On the paragraph after Proposition 3 (just before Equation 8): what is meant by \"vectorization operation\"?\n - How representative is the convergence of the proposed method? Figure 3 shows what it seems to be the log of the loss of the gradient approximation. First, the plot suggests that the L2 norm is already below 1 (as the log is already < 0); after 5 iterations DGL is already by 4.5e-5 (the proposed work is about one order of magnitude lower but those seem like really low numbers for an L2 norm; the log just exaggerates that difference).\n - Is the loss reported in Figure 3 accounting for the full loss, as defined in Equations 2 and 3? Usually, regularized losses have a higher value because of the extra norm of the regularization term. However, the loss reported in Figure 3 is lower than the non-regularized one nonetheless.\n\n#### Additional feedback (not necessarily part of the decision assessment):\n - Section 2, first paragraph: $m_i$ is the amount of data on device *$i$*.\n - Section 3: All *GAN-based* attacks\n - Section 3: maximum a *posteriori*\n - Section 5.1: *ReLU*\n - Algorithm 2, line 1: missing inputs, namely $\\lambda, \\eta$ and $m$.\n - Algorithm 2, line 11: does $n$ correspond to the batch? This was denoted as $B$ before.\n - Section 6.1: with the optimizer *L-BFGS*",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}