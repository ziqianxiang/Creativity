{
    "Decision": "",
    "Reviews": [
        {
            "title": "It is an interesting idea to ''poison'' GAN's training data.",
            "review": "This paper studies the transferability of the embedded artificial fingerprints in GAN training. The authors first ''poison'' the training data using image steganography technology and then train GAN with them. Experimental results show that such it is possible to transfer fingerprints from training data to GAN model. Authors argue that this discovery can be applied to the fields related to deepfake detection. This paper proposed a simple, well-motivated approach. It is clearly written, and easy to understand. I think the paper is a positive rating.\n\nAlthough I like the idea, I think some questions worth investigating did not appear a lot in the paper:\n\n0. What is the essential difference between the proposed method and directly adding a digital watermark to the generated image?\n\n1. I am curious that at what stage were the fingerprints learned when training GAN? Does the GAN model first learn how to generate images, and then learn how to generate fingerprints? Or the GAN learn to generate fingerprints and images at the same time? It is interesting because although fingerprints are almost invisible to human, they are easy to distinguish for CNN. What if a plot is provided to show when the fingerprints are embedded?\n\n2. Related to the above problem, will the change of the discriminator network affect GAN's fingerprint learning? Is learning fingerprints an easy thing or a hard thing? Can I use arbitrary GAN to train fingerprinted data to achieve good results?\n\n3. In the proposed approach, an image steganography network is trained to embed fingerprints. What if we train a network to remove fingerprints? Will it cause the methods in this paper to be attacked or misused?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Artificial GAN fingerprints: rooting Deepfake attribution in training data",
            "review": "This paper tackles the problem of distinguishing GAN generated images from real ones by embedding a signature, called artificial fingerprint, inside the image.\nAn encoder/decoder deep learning architecture is proposed to embed such fingerprints in the training data.\nThis allows to generate GAN images that possess the same fingerprint, a property which can be eventually used for the detection/attribution task.\nExperiments are carried out on a dataset of real images (CelebA human face dataset and LSUN bedroom scene dataset) and synthetic images generated using ProGAN, Style-GAN and StyleGAN2.\nIn my opinion this work is not ready to be published.\nI have serious concerns on the presentation, the novelty and the experimental validation as detailed below.\n\nPros\n\nThis active approach is different from most of the current passive methods for the detection of synthetic media.\nIn the right scenario, this could be an interesting direction to follow in order to solve some of the major challenges of passive methods.\n\nCons\n\nI cannot understand the scenario considered in this work: which is the threat model?\nIn the introduction, the authors put great emphasis on the importance of deepfake detection to prevent misinformation in political campaigns and fake journalism.\nHow can the proposed method be applied in this scenario? This is absolutely not clear.\nIf a malicious attacker uses a synthetic GAN image to build a fake news, how can the proposed method be of help?\nMaybe this is not the right context, however there is no description of the reference scenario, and this creates a lot of confusion in the reader.\n\nI find that this work is not well contextualized.\nIn Section 2 there is a large paragraph on passive methods, while image steganography is only superficially described in a short passage.\nHowever, since the proposed approach is inspired to concepts in steganography, this part should be presented in much more detail.\nMore in general, existing active methods with similar aims as the proposed one should be analyzed in depth\nand their differences, and especially shortcomings, with respect to the proposed method should be highlighted and discussed.\n\nThe proposed approach exhibits some similarities with other active methods recently proposed in the literature.\nIn particular, there are several architectures that include an encoder and decoder for steganography/watermarking (Baluja 2017; Zhu et al. 2018; Tancik et al. 2020).\nIt is not clear which is the novelty with respect to these approaches.\nIt seems to me that the contribution is limited to the application to GAN images.\n\nThe term 'artificial fingerprint' can be misleading, since it has been recently used in the literature (Marra et al. 2019, Yu et al. 2019)\nto indicate the traces that characterize a synthetic image because of its generation through a GAN architecture.\nThe same term should not be used with a completely different meaning or, at least, the different meaning with respect to previous works should be clearly emphasized.\nOverall I find that this ambiguity increases the confusion between active and passive methods.\n\nI think that the comparison with state-of-the-art approaches is seriously flawed.\nSince the proposal is an active method, it should be compared with other active methods (including some baselines from the steganography field).\nIt is not fair comparing an active approach with a passive one (Yu et al. 2019), and especially such a comparison does not provide any useful information.\nMoreover, a comparison with a single method does not provide a solid experimental validation,\nand there are recent works that explicitly face the generalization issue and provide very good results in the open world scenario for GAN image detection (e.g. Wang et al. CVPR 2020).\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "initial review comment",
            "review": "In this work, the authors studied the transferability of artificial fingerprints from the training data to GAN models (i.e., the generated data). Inspired by recent image steganography works, the authors first apply steganography to GAN training data, and hope that the introduced artificial fingerprints can represent the source identity of data. Then, through training different GAN models with these ‘encrypted’ data, the empirical study validated that the trained GANs not only achieve high fidelity generation results, but also inherit artificial fingerprints encode in training data. Furthermore, the authors claim that this discovery can further contribute to deepfake detection and attribution, and performed some experiments for justification. \n\nOverall, I think the authors are working on an interesting topic. The paper and related experiments may help our community to further understanding GAN models. Meanwhile, it demonstrates a possible new path to solve deepfake detection. However, I still have some concerns:\n\nFirst of all, in the submission, the main contribution claimed by the authors is about solving deepfake detection and attribution. However, I find that related experiments and theoretical discussion may be hard to support this point. In my opinion, the authors may distract the core contribution of this work. \n\n1) Even though this technique/strategy may potentially apply to deepfake attribution, there exist many challenges and uncertainties need to be solved. For example, if different sources (person/company/apps) employ different fingerprints (i.e., fingerprint encoder), can the decoder successfully perform the detection/attribution? This concern seems to be discussed in the open-world experimental part, yet related experimental settings are unclear to me. In section 4.3 (deepfake detection), the open-world setting is described as \"depending on whether or not the set of GAN models used for classifier training covers that used for testing\". What about the fingerprints? Are those unseen GANs using the same fingerprints (or training data)? In section 4.5 (attribution), the open-world setting seems changed, i.e., \"introduce another four GANs trained on unknown fingerprints\". Here, although the detector could identify they do not belong to unknown GANs, it may be hard to distinguish those unknown sources from other real-world collected data (some of them may also be encrypted by unknown fingerprints). \n\n2) \"The solution/end of the current adversarial arms race\" could be overclaimed. Considering the complex real-world situations, the trained detector can still be challenged. \n\nOn the other hand, in my opinion, studying and exploring how and how well the fingerprints can be learned by GANs is a more valuable contribution. If there exist some fingerprints that cannot be learned or be transfer to GAN generated images. And what the GAN will perform if we use mixed training data with different fingerprints to train it. Answering/exploring these questions may help us to solve deepfake attribution problems. \n\nIn conclusion, I think this paper finds an interesting topic. However, related questions and potential problems are not been well explored and answered. Meanwhile, writing and organization should be improved. For now, I give my rating as \"ok but not good enough\". My final suggestion will consider the authors' feedback.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}