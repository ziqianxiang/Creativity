{
    "Decision": "",
    "Reviews": [
        {
            "title": "An adaptive bootstrapping method that progressively modifies the target predictions, intuitive somewhat heuristic",
            "review": "The paper proposes Progressive Self Label Correction, which modifies the target predictions adaptively during the training. The method is based on label correction/bootstrapping (Reed et al., 2015), where the training targets are the combination of noisy labels and the predictions themselves. Instead of fixing the coefficient $\\epsilon$ of the predictions during the training, the paper proposes to increase $\\epsilon$ adaptively as the training continues and the confidence in the predictions becomes larger. \n\nThe paper is easy to follow and the proposed method is clear and easy to understand. However,  the choice of \"self-trust\", i.e. the main technique in eq.(7) is not justified scientifically, rather than a practical trick. I expect some theoretical analysis of the choice of this $\\epsilon$. I suspect whether the novelty and the contribution are above the bar of ICLR.\n\nRegarding the experiments, since the simply baselines such as CCE, LS, CP implemented by the authors are already better than the competing methods (see Table 4), e.g. GCE, D2L, SL, I am wondering whether the improvement is significant if the competing methods are also under the same setting. Some critical hyper-parameters are tuned using a clean validation dataset. What if the clean validation dataset is not available? On the real-world noisy dataset, the improvement is also marginal given a much stronger reimplemented backbone network (Table 6, reimplemented CCE 71.8 beats almost all the previous methods except Joint-soft). \n\nOverall, I think the paper is interesting and well-motivated, but the technical contribution and novelty are somewhat limited.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting idea of improving self-label correction by automatically decide the trust degree of a learner, but the clarity and evaluation are not very strong.",
            "review": "#######################################################################\n\nSummary:\n \nThis paper proposes ProSelfLC, a new self label correction method which is designed to automatically decide the trust degree of a learner. In particular, this paper points out two issues of target modification approaches when training robust deep neural networks: 1) how much should we trust a learner to leverage knowledge, 2) should penalize or reward a low-entropy status? To address these issues, this paper proposes to set the trust weight automatically according to learning time and the prediction entropy, and to reward a meaningful low-entropy status redefined by the proposed ProSelfLC. Empirical studies are performed to show the superiority of ProSelfLC over several noise robust approaches. Results are shown on the task of image classification under standard and noisy settings.\n\n#######################################################################\n\nReasons for score: \n \nOverall, I vote for weak rejection. I like the idea of automatically decide the trust degree of a learner for self-label correction. My major concern is about the clarity of the paper and some additional issues (see cons below). Hopefully the authors can address my concern in the rebuttal period. \n \n#######################################################################\n\nPros: \n \n1. The paper study several target modification techniques, and point out two important issue of the self label correction methods. To address the issues, ProSelfLC proposes to set the trust weight automatically and adaptively according to the training time and entropy. \n \n2. This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. Empirically, this paper shows that the proposed ProSelfLC has the least wrong fitting and most semantic class correction in the synthetic label noise setting. \n \n#######################################################################\n\nCons: \n \n1. The main contribution of this paper is the Eq. (7), which is designed for setting the trust weight of a learner automatically and adaptively. However, though this paper has provided the design reason for Eq. (7), it is unclear to me how it is derived. For better understanding the contribution, I suggest the authors to clarify the motivation and add more details about it.  \n\n2. For me, the paper is not very easy to follow. I suggest the authors to further polish the writing and correct some potential confusing definitions. For example, q is one-hot representation in Eq. (1), while q is referred as the probability mass function in Eq. (2). \n\n3. Comparisons with many related and state-of-the-art robust methods are missing. It would be more convincing if the authors can provide comparisons with such methods, e.g., Tf-KD [1], P-correction [2], M-correction [3], DivideMix [4], and etc.\n\n[1] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label smoothing regularization. In CVPR, 2020.\n\n[2] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In CVPR, 2019.\n\n[3] Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In ICML, 2019.\n\n[4] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semisupervised learning. In ICLR, 2020. \n \n#######################################################################\n\nQuestions during rebuttal period: \n \nPlease address and clarify the cons above. \n \n#######################################################################\n\nMinor issues: \n\n(1) The sizes of numbers and symbols in Figure 1 are different. It is better to reprepared the figure for better visual experience.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Need more clarification ",
            "review": "This paper focuses on training robust deep networks under noisy supervision. The authors first provide detailed analyses for several target modification methods, and then point out their limitations. The authors further propose an end-to-end method named ProSelfLC to handle noisy labels. The core idea of the proposed method is to increase the trust in the predictions of models gradually during training. The author's analyses of existing methods are relatively complete. However, the paper itself lacks clarification. The detailed review comments and questions are as follows:\n\nThe authors argue that we should increase the trust of predictions as the number of iterations increases. This view is reasonable. The authors give some analyses on this. However, I have some concerns about this. Firstly, we all know that real-world datasets are complex and challenging. Even though with the memorization effects of deep networks, the networks still overfit the noisy labels in the early stage of training. In this case, increasing the trust in the predictions brings side effects unexpectedly, because the predictions have high confidence to noisy labels. How do we address this issue? \n\nSecond, the proposed method is complex and has a clear restriction. Although $\\epsilon_ProSelfLC$ can be set automatically, it introduces two other hyperparameters, i.e., $B$ and $\\Gamma$. It is hard to set these task-dependent hyperparameters. The authors need to search for them by exploiting a clean dataset. The size of the used clean dataset is relatively large, i.e., 20% training data, which is unpractical, posting a clear restriction, and severely decreasing the technical contribution of the paper. \n\nIn Section 2, the authors criticize that the methods exploiting an auxiliary trusted training set requires extra annotation cost. Label engineering methods relate to the proposed method, but this work also needs a clean dataset. This slight contradiction is confusing. If a clean dataset is provided, the proposed method is very similar to the existing ones, e.g., \"Learning to Reweight Examples for Robust Deep Learning\". The authors should compare and justify the similarities and dissimilarities.\n\nI personally don’t think joint optimization is substantially complex than the proposed method.\nThe authors said they will not implement joint optimization but reported the performance on the clothing dataset, showing that the authors may have directly copied some results from others without considering the settings, which is unfair and unreasonable.\n\n\nThe experimental comparisons also seem unfair. The proposed method exploits clean data, but the baselines do not. A clean dataset is valuable and helpful. It can help Forward to improve the estimation of the transition matrix, and help Joint or SL to tune hyperparameters. These operations will improve the classification performance of baselines. The authors state that they retrain a model on the entire training data after searching hyperparameters to achieve a fair comparison. This is not fair. The experimental results are not convincing at all. \n\nIn summary, this paper gives meaningful analyses for some target modiﬁcation techniques. However, the novelty, clarification, and contributions are not enough. Besides, the experimental results are not convincing. I hope the authors can address the mentioned issues to improve this paper.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper introduces a label-adaptive loss for learning with noisy labels. Although the authors elaborately design the self-adaptive weight between the label and the annotation, it is not clear that how this method performs better than previous work e.g., D2L, which has the similar idea on label re-weighting.",
            "review": "This paper introduces a label-adaptive loss for learning with noisy labels. Note that, this is not a new idea since previous works [1-2] adopt the similar mechanism to re-weight the label with the predictions. Thus, it is not proper to claim that it is the first work to trust self knowledge progressively and adaptively in the contributions. Besides, the authors have not considered the similar D2L as  the baseline and more recent SOTAs in the experiments which makes it is not convincing how ProSelfLC works and whether it is better than D2L and recent SOTAs. \n\n\n[1] Y. Li et, al. Learning from Noisy Labels with Distillation. ICCV 2017\n[2] X. Ma et, al. Dimensionality-Driven Learning with Noisy Labels. ICML 2018",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}