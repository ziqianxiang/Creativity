{
    "Decision": "",
    "Reviews": [
        {
            "title": "Marginal novelty and unfair comparison",
            "review": "This paper proposes a two-level temporal modeling module for action recognition based on temporal difference. It consists of a short-term temporal difference module (TDM), which captures motion patterns within a segment centered on each sampled frame, and long-term TDM, which leverages cross-segment feature difference for attention. \n\nPros\n1. Relevant topic for video representation learning\n- As well documented in the related work, temporal modeling is crucial for action recognition.\n2. Improved module for exploiting the temporal difference \n- An effective two-level structure is introduced to mix within-segment and cross-segment temporal differences. \n3. Good ablative studies of different design choices\n- The proposed module is empirically justified based on an extensive ablative experiment.\n\nCons\n1. The marginal novelty of TDM compared to the related work\n- The use of the temporal difference for motion modeling has been used in previous work, both for images and features (Wang et al. 2016, Zhou et al. 2018, etc), and the proposed one is a two-level extension with different operations based on them, which does not have a significant technical innovation. \n\n2. Insufficient justification\n- The effect of the temporal difference in short-term TDM is unclear. Is the temporal difference operation relevant? What if the raw frames are used instead? What is the effect of varying the number of RGB differences in D(X_i)?\n- The long-term TDM appears very limited in terms of temporal interaction because it involves a subsequent segment only. \n- In the long-term TDM, the authors use the channel-wise convolutions to resolve the miss-alignment issue in computing the temporal difference, but it is not clear how it becomes to perform the alignment. Does the convolution really align the two feature maps, thus helping to compute the temporal difference? This needs to be analyzed. \n\n3. Unfair comparison\n- As can be in Section 3.2 and Figure 2a, the short-term TDM requires 4 nearby frames for each segment. Considering this, the frame count in comparisons of Tables 1 and 2 is misleading; in fact, the proposed method, TDN, uses 4 times more frames than the others with the same frame count shown in the tables. This should be corrected and the experimental section needs to be revised accordingly to give a fair comparison with other methods. Given the correct number of used frames, other recent methods, e.g., TEINet and TEA, perform better. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Overall, the proposed temporal difference networks seem to be effective. Extra experiments and plots can improve the thoroughness of the paper.  ",
            "review": "The paper proposed temporal difference networks for efficient action recognition. Two modules are proposed to capture short-term and long-term temporal information. Short-term temporal difference module (S-TDN) operates at early layers of networks, and long-term temporal difference module (L-TDM) operates at later layers of networks.  The authors perform empirical studies on Something-Something V1 & V2 and Kinetics-400. The empirical results demonstrate the proposed method outperforms other state of the arts on ResNet50. \n  \n\nArguments:\n- Although the authors demonstrate the experiments using ResNet50 backbone for fair comparison. Extra experiment results using different backbones (e.g. ResNet101, EfficientNet) could offer a better understanding of the effectiveness of the proposed method. \n- The ablation study should cover that S-TDM and L-TDM operates at all layers.\n- The performance improvement seems to be marginal compared to TEA (e.g. 70G 51.9% vs 72G 52.1% on Sth-Sth v1) \n- In addition to Table 2 and 3, plots of the accuracy against the GFLOPs per test video are desired for comparing the accuracy given the same computational budget. \n\nOverall, the proposed temporal difference networks seem to be effective. Extra experiments and plots can improve the thoroughness of the paper.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "What does the “Long-term” represent?",
            "review": "This paper presents a new video architecture, termed as Temporal Difference Network (TDN), with a focus on capturing multi-scale temporal information for efficient action recognition. To fully capture temporal information over the entire video, the TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation.\nSome concerns are listed as following. \n1.\tThe illustration of Short-term TDM in Fig. 2(a) is not consistent with Eq. (1) and Eq. (3). It will be better to give complete equations of Short-term TDM. Besides, since Short-term TDM is applied in the stage 1 and 2 of ResNet50, it means that the input of Short-term TDM may not be RGB frames, thus the X_i in Eq.(1) is confusing to some degree. Please make sure Fig. 2 is easy to understand and consistent with Eqs.(1)-(6).\n2.\tThe video is divided into T segments, and a frame is sampled randomly from each segment. The authors claim that the long-term TDM aims at leveraging cross-segment temporal structure to enhance frame-level feature representation. Does it mean that the cross-segment temporal structure is actually cross-frame if the temporal receptive field caused by layer-stacking is not taken into consideration? So, what does “Long-term” represent? Furthermore, according to Fig. 2(b) and Eq.(2), only the features of adjacent segments are fused.\n3.\tIt seems that S-TDM contributes more for the final recognition, according to Table 1. How do you prove that the improvement brought by L-TDM is actually caused by the temporal difference architecture but not the increase of layers and parameters?\n4.\tThe usage of symbols are confusing. Such as I(t) and X_i for frames, F_i and F_t for features of segments, F_{out} and \\hat{F_i} for the fused features. \n5.\tThe caption of Table 1 is missing. This results in a poor readability of Table 1.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}