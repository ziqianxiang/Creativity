{
    "Decision": "",
    "Reviews": [
        {
            "title": "The main contribution is a collection of construction tasks a robot could do, and an experimental analaysis of different algortithms.",
            "review": "The paper addresses the problem of robots doing mobile constructions in 1/2/3D grids.  Doing mobile constructions is difficult because the robots need to move blocks for the construction while localizing in space and planning how to do the construction.  The problem is addressed only in simulation with relatively simple tasks.   The paper proposes a suite of construction tasks that can be learned  using reinforcement learning, a benchmark of baseline models, and an ablation study that highlights difficulties with current approaches.\n\nThe paper has some originality compared to existing work on robot construction, but some of the assumptions make the solution not specially useful for real robot construction.   The lack of consideration for problems that real robots will have to face makes the work academic and not applicable in practice.  \n\nThe paper is easy to read, but misses a lot of details.   \n\nThe significance is limited.  The stated objective is to propose  construction tasks and analyze the performance of a set of algorithms on those tasks. The tasks have been chosen to be learnable using reinforcement learning, but the experimental results reported show they are too difficult for model free reinforcement learning.  So, the tasks are not complex enough to be useful for real applications, but are too complex for the solution method chosen.\n\nPros:\n1. The problem is potentially interesting if robots were to be used to build constructions.\n2. The collection of construction tasks and algorithm tested is sufficiently large. \n\nCons:\n1. The setting is simplified in a way that does not make it realistic for real robots.  Using a grid is ok, but the assumptions made are not.  For the 1D and 2D cases, the robot can drop a block in its own position, while in 3D it cannot.  It would have been useful to show a potential model of a real robot that could drop a block in its own position and move away from it.  The paper says the robot in 2D \"prints binary plans\", which seems  different from constructing objects using blocks. For the 3D cases the robot can trap itself in the middle of blocks. No indication is given on how a robot can avoid that. \n\n2. The problem is modeled as a POMDP, even though in the Related work section the paper says that the state space for the POMDP models needed us too large, hence they focus on model-free reinforcement learning. It is not clear why the robot motion is modeled as  having an unknown probabilistic transition model, while dropping a brick is assumed to have no uncertainty.\nDropping a block is not a simple task for a real robot, so the assumption made simplifies the problem but does not make its solution useful for applications with real robots.\n\n3. The settings for the experiments are not entirely clear.  First, is the robot given just the template for the construction and has to decide how to fill up the template with blocks?  what happens if the robot leaves some empty grids that should instead have been filled? It seems that in 2D (and perhaps in 1D)  the robot can go over the construction done so far.  Suppose a grid element was left empty. If the robot was moving real blocks, being able to get to the empty grid element and drop a block there would not be easy in practice, because the block might get stuck and not get exactly in the desired place.\n\n4. The term IoU is used but not defined. This makes it difficult for readers not familiar with the term to understand the results, since IoU is the only  evaluation criterion used to compare results.\n\n5. The experimental results are not explained very well, the explanations are short and generic. Fig 8 shows the best testing result obtained by each algorithm for each task.  What about the worst results? No explanation is given on the meaning of the colors in the  figure.  They are sort of intuitive, but the readers should be told what they mean.\n\n6, The conclusions are disappointing.  The task is too difficult for model free reinforcement learning.  \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting benchmarks - but the paper could use some more work",
            "review": "The paper presents a new benchmark for deep RL based on mobile construction. The authors create several environments that measure the ability to add blocks in grid worlds (1d/2d/3d) to match the desired construction plan. The environments are interesting and represent an interesting challenge for current state of the art deep RL approaches. Specifically, they present the joint challenge of localization and environment manipulation. Overall, I find that the benchmark could be very interesting for the deep RL community.\n\nMethodology:\n\nI appreciate the comparison with multiple baselines and the ablation study. However, I’m not entirely sure how the authors performed their random searches over the hyperparameter space. Moreover, its disappointing to only see results with one training seed. It makes drawing any conclusions very difficult. Specific comments follow:\n\n4.dqn:  “…random search over hyperparameters, especially the LR.” \nWhat does especially mean. How many samples? What ranges for the hyperparameters? \n\n4.SAC: “We search the main hyperparameters based on 1D static case and we used different network architectures through a random search approach for relatively complex 2D and 3D cases” \nwhat are the exact random search configurations?\n\n4: \"layers network architecture containing [64,128,256,128,64] nodes.\" \nLooks like 5 layers. \n\n5: Ablation is interesting, obstacles hurt, gps helps, but you need multiple seeds here as well. I disagree with the conclusion of the rainbow ablation study, if anything it confirms that if you add all the parts you can be more sample efficient. The fact that the agents’ plateau to the same performance is most likely a limitation of the model or the environment itself. \n\nWriting clarity:\n\nThe writing could use some work. For example, second sentence of the abstract is long and confusing: “Without GPS or similar techniques, carefully engineered and learning-based methods face challenges to exactly achieve the plan due to the difficulty of accurately localizing the robot while strategically evolving the environment, because common tasks (manipulation/navigation) address at most one of the two coupled aspects. “\n\nMissing citations:  \nThe authors should definitely add a citation and a discussion between the differences between their environments and sokoban (botea et al 2003, racaniere et al 2017). In sokoban, the agents push boxes onto goal locations which make it similar (yet still different) than the mobile construction benchmark presented here. \n\nMissing citations for SLAM in dynamic environments (Zou et al. 2012, Yu et al. 2018).\n\nOther notes :\n\nPOMDP par: “100^100 for 100 grids with max height 100.” \nYou haven’t introduced yet how grids are built or modified – so this is incredibly confusing. \n\nBaseline selection par: “Since our tasks live in grid worlds similar to many Atari games….” \nAtari games are typically treated as pixel based and not grid worlds.\n\n3.1: Dynamic plan tasks – agents don’t need the full plan but just the delta that’s left to construct. \n\n3.2: Reward function can be scaled by 1/5. Could potentially stabilize training, with the same optimal policy. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper as is requires extensions",
            "review": "# Summary\nThe submitted paper proposes a mobile constructions environment. The environment is articulated into three versions: 1D, 2D and 3D. All tasks is only partially observable since the agent observes only part of the state. The 1D task is enriched with a two variants: a static (one shape across episodes) and a dynamic plan (procedurally generated shapes). The 2D task is enriched with four variants: static, dynamic, dense (solid shape plan) and sparse (unfilled shape plan). The 3D task comes in only variants: sparse and dense; differently from 1D and 2D the agent cab be physically obstructed by the construction built so far and bricks are placed on tiles not occupied by the agent. \n\nAuthors propose the tasks as a benchmark and provide several baselines: DQN, DRQN, DRQN+Hindsight, PPO, Rainbow and SAC. Also a human baseline is provided. Performance on the proposed task are relatively unsatisfactory. \n\nAuthors conduct ablation studies to understand which components of the task/environment effect performances the most. Considered ablations are obstacles, partial observability (GPS) and stochasticity (environment uncertainty). Obstacles are reported to have to most apparent effect.  However, in some conditions (e.g. sparse shapes) partial observability and stochasticity have a important impact on performances too.\n\n#### Quality\nThe paper is overall well executed. \n\n#### Clarity\nThe paper is well written and sound in the approach.\n\n#### Significance\nThe considered problem is interesting but possibly too abstract to be relevant to a wider community.\n\n#### Major comments\n* **Task complexity VS interest.** Authors make a good argument to prove that agents struggle with solving the proposed task. A popular benchmark should be adopted by a wide community. I believe the current proposed task lacks of a few ingredients in order to become attractive to the wider community. First the task is too artificial and lacks of connection with a real-world task (e.g.  constructions). Second it lacks of diversity: there are only three proposed tasks with variants but these tasks can't be considered a proper and diverse task suite. Third, the proposed is focused on the environment characteristic (e.g. stochasticity, partial observability, physical obstructions) and nothing is mentioned about the skills that an agent needs to acquire to solve the task (e.g. memory, beliefs estimation, localization and mapping, etc). \n\n#### Minor comments\n* **Figure 7.** The human baseline is missing a lot of data points. Also, agents do not seem to perform much worse than the best trained agents. This seems to indicate that eventually the task is not so difficult for agents. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting and creative benchmark, but lacking broad significance",
            "review": "### Summary\nThis work proposes a set of new benchmarks for model-free reinforcement learning inspired by mobile construction. Each benchmark involves an agent navigating a 1D, 2D, or 3D grid. The agent may create a “brick” at its current grid cell with the overall objective of creating a structure that matches a design plan, which may be either static or dynamic, and may be either dense or sparse. Observations are very partial, transitions are stochastic, rewards are relatively dense, and task horizons are relatively long. In addition to proposing the benchmark, the authors evaluate several popular deep RL techniques and demonstrate that all struggle, especially on the more difficult benchmarks.\n\n### Pros\n- The mobile construction benchmarks are creative and well-motivated. It is easy to see why both existing deep RL techniques and SLAM techniques struggle in this domain.\n- The different variations of the benchmarks -- static vs. dynamic, sparse vs. dense, and 1D/2D/3D, offer very good breadth.\n- Mobile construction is an especially clever starting point for benchmarks because it is easy to create a large diverse family of tasks using parameterized curves, as the authors have done.\n- The 3D version of the benchmark, where the bricks that have been built act as obstacles, is especially compelling.\n- The benchmarks are superficially very toy tasks, and are limited to grid worlds, but ultimately I think it is a strength that the authors try to isolate what is challenging about this domain, without dressing it up with distractions.\n- The paper provides a good level of detail throughout, especially pertaining to the baseline methods run.\n- The human experiments are very helpful to get a sense of the task difficulty and what results we should reasonably hope to see from automated methods.\n- The ablation study is very helpful and answers some questions that I had while reading up until that point.\n\n### Cons\n- Perhaps the biggest con for me is the absence of code in the supplementary materials. Code is always good to include for reproducibility, but it is especially crucial for a submission where the main contribution is a benchmark that is meant to bolster more research. Without code, I cannot fully evaluate the extent to which this benchmark might be useful to the community, which is important for judging this paper’s significance.\n- These benchmarks are very challenging, but not necessarily in the “right” ways. For example, localization in an initially wide-open grid with no landmarks and no GPS seems more difficult than any real-world localization challenge would be. (Even driving around in the desert, you’re bound to see some cacti!)\n- An important omission from the related work section is Minecraft. There has been a considerable amount of work on Minecraft and there are many variations (including simplified toy versions) of the domain as benchmarks for RL. It would be important to include a discussion of these existing benchmarks and contextualize this new benchmark accordingly.\n- These benchmarks involve fairly extreme partial observability, so it is not at all surprising that non-recurrent deep RL methods (DQN, PPO, Rainbow, SAC) are unable to perform well. I would have liked to see more emphasis given to methods that are meant to handle partial observability. DRQN is one good method to test, but there are many others, many of which would be more appropriate for this benchmark than the generic non-recurrent model-free deep RL methods.\n- The prospect of learning to build your own landmarks is mentioned in the introduction and it is speculated that humans are learning to do this in experiments. It is not clear, though, how landmarks would actually manifest in such a restricted environment, given that the only thing that the agent can do is move and build bricks.\n- Overall, I am not sure whether this paper includes enough of a standalone contribution to merit publication. It is a good project and an interesting benchmark, but it may be lacking in terms of significance.\n\n### Detailed Comments\n- In the first sentence, “reborn” is an awkward word choice\n- The use of the word “plan” as a “design plan” throughout the paper is confusing, given the main use of the word “plan” in RL and decision making as a sequence of actions. Maybe “specification” would be a better term?\n- Most of the images in the paper suggest that the “plan” and its superposition on the grid is clear to the agent, e.g., as part of the observation, as it is clear to us as viewers. It would be good to at least emphasize in the captions that this is not the case.\n- In 5.1, “terminate grid world state” should be “terminal grid world state”\n- Figure 7 should be e.g. bar charts instead of line plots; it doesn’t make sense that we would interpolate between the points on the x axis.\n- In 5.2, it says “To test this, we added ground truth location state information l(x, y) into the feature vector as described in Section 4 for DRQN on 2D static tasks.” I am not sure which part of Section 4 is being referenced here.\n\n### Questions\n- What would an optimal or nearly optimal policy look like in each of these benchmarks? Ideally, please implement these policies and include their performance in the results. If that is not possible within the time constraints, please describe these policies qualitatively in the rebuttal.\n- Is any variation in the initial location of the agent? If there is no variation in the initial location, then in the static versions of the benchmarks, there would be no variation in the initial state whatsoever. Is that correct?\n- Can the agent tell when it is adjacent to a boundary? In other words, are there special “off screen” values that are distinguished from empty grid cells and bricks? If so, then I would conclude that the boundaries are very important for localization. If not, then I don’t see how the agent has any mechanism whatsoever for localizing with respect to the “plan” or any absolute reference frame. And in this latter case, the only explanation for the nontrivial results that I can surmise is that the initial location of the agent is fixed. Please clarify these points.\n- Can you clarify what an agent- or human-built landmark would look like in these environments?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}