{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents an approach to use spatio-temporal self-similarity (STSS) as a feature for a convolutional neural network for video understanding. The proposed approach extracts STSS as a descriptor capturing similarities between local spatio-temporal regions, and adds conventional layers such as soft-argmax, fully connected layers, and conv. layers on top of it.\n\nOn one hand, all of the reviewers agree that the novelty of the paper is limited. On the other hand, most of the reviewers (except R1) appreciated thoroughness of the experiments and ablations. In the end, the reviewers gave 3 marginally above the acceptance threshold ratings and 1 marginally below the threshold rating.\n\nThe AC views this paper as a borderline paper. None of the reviewers are excited about the paper, and it is a typical \"Nice experiments with limited novelty\" (by R1) paper. The concept of the STSS itself was already proposed in prior studies as mentioned in the paper and by the reviewers, and this paper 'engineers' a new way to take advantage of STSS without further theoratical or conceptual justifications on why it should work. The newly added Kinetics and HMDB results in the rebuttal are nice, but the impact of STSS seems to be minimal in these results.\n\nOverall, the AC find the paper slighly lacking to be considered for ICLR."
    },
    "Reviews": [
        {
            "title": "Good paper with good results",
            "review": "#### General\nThis paper proposes spatio-temporal self-similarity (STSS), which captures structural patterns in space and time, for action recognition from videos.  \nOverall, I would like to recommend ICLR to accept the paper.  \nPros and Cons I found in the paper are summarized as follows.\n\n\n#### Pros.\n1. SELFY, the proposed neural block that implements STSS, is a good extention and adjustment of prior self-similarity works to modern CNNs, resulted in achieving state-of-the-art performance on different datasets.\n1. The ablation study reveals that SELFY with a long temporal range successfully incorporate the temporal information without explicitly exploiting optical flows. \n1. The paper is generally well and clearly written.\n1. The literature review is thorough, and the work is well contextualized in the literatures.\n\n#### Cons\n1. The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper.\n1. The reasoning or intuition behind some of the design choices are not always clear. For example,  \n    - The output of the feature extraction block is $F \\in \\mathbb{R}^{T \\times X \\times Y \\times L \\times C_F}$, and the authors say \n        > The dimension of L is preserved to extract motion information across different temporal offsets in a consistent manner.\n\n        But the motion information across different temporal offsets can be extracted in the feature extraction module if MLP or convolution is employed in the feature extraction block. In other words, even if the output is $F \\in \\mathbb{R}^{T \\times X \\times Y \\times C_F}$, the motion information can be encoded in the output tensor. Possibly the authors empirically found the present design choice is better. If so, how does the performance change if $L$ is not preserved?\n    - In the feature integration block, firstly $3 \\times 3$ spatial convolution kernel is applied, then the temporal offset ($L$) and the channels ($C^*_F$) dimension is flattened, and finally $1 \\times 1 \\times 1$ spatio-temporal convolution is applied. Is this design chosen experimentally or is there any intuition behind?   \n\n#### Minor comments\n1. It may be interesting to discuss the relationship with self-attention based networks such as [1-3]\n1. About Table 7 in Appendix, how is the result if SELFY block is used in $res_1$ and $res_5$? I guess these option do not work well, but I think it is beneficial to list all the results.\n\n[1] Hu+, Local Relation Networks for Image Recognition, ICCV 2019  \n[2] Ramachandran+, Stand-Alone Self-Attention in Vision Models, NeurIPS 2019  \n[3] Zhao+, Exploring Self-attention for Image Recognition, CVPR 2020",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting model to integrate bi-directional self-similarity into spatial-temporal representations with empirical results showing gains in performance over SOTA on motion-centric data but some concerns remain over applicability to data that requires more appearance-centric representation learning.",
            "review": "The paper introduces SELFY, a neural module that learns spatio-temporal self-similarity across longer timescales in both directions to obtain visual features that provide consistent empirical gains on three action recognition datasets. Ablation studies show that modeling longer, bi-directional motion similarity can help handle motion blurs and (artificially induced) occlusion.\n\nStrengths:\n+ The paper is well written and shows strong empirical gains over prior SOTA.\n+ The ablation experiments on the effect of temporal length and the different feature extraction methods are nicely done and show the effect of different design choices.\n+ The experiments for testing the robustness of the learned representations is nice to see and helps highlight the strength of the proposed approach.\n\nConcerns:\n- From what I can see, the contribution is in the modeling of (motion) similarity in both directions, across a longer time scale. My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51. In fact, the results on SS-V1 and SS-V2 show some gains (~2-3%) compared to the very closely related MotionSqueeze Network (Kwon et al, ECCV 2020) which computes correlation (rather than similarity) between adjacent frames (t and t+1). MSNet did not see any significant advances in datasets (HMDB-51 and Kinetics) where appearance plays a greater role and I am not sure how this proposed approach would help in those conditions.\n- I assume the function sim(.) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere. Is there any specific reason the correlation function was not used? What is the effect of using correlation in place of cosine similarity? It would interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net (the most closely related network) and help highlight the contribution of the proposed model beyond the obvious empirical gains.\n- While the robustness experiments test out the occlusion setting by cutting out a rectangular patch of a single center-frame, that does not, IMHO, shows robustness to occlusion since the features from surrounding frames (on both sides) will help mitigate this artificial occlusion. In practice, the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge. Experiments on HMDB51, which has strong camera motion and hence introduces natural occlusions, would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This is an interesting work utilized STSS to help learn representation end-to-end and achieved good empirical results on several action recognition benchmarks.",
            "review": "### Summary: \n\nThis submission proposed a motion representation method based on spatio-temporal self-similarity (STSS), which represents each local region as similarities to its neighbors in both spatial and temporal dimension. There are previous works (e.g., Ref[1] , [2], [5] listed here) which utilize STSS for feature extractions, authors claim that this work is the first one to learn STSS representation based on modern CNN architecture. The proposed method is implemented as a neural block, i.e., SELFY, which can be applied into neural architectures and learned end-to-end without additional supervision. On 3 standard human action recognition data sets, Something-Something-V1 & V2, Diving-48, and FineGym, the proposed method achieves quite good empirical results. \n\n### Strengths & Originality: \n\nThe idea of utilizing patio-temporal self-similarity (STSS) for feature representation in the modern CNN framework for human action recognition is interesting. I also like the concept that \"fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame, the problem reduces to optical flow ...... In contrast, we leverage the whole volume of STSS ......\", as to view the proposed method as a generalized & rich optical flow. \n\nFurthermore, STSS should be helpful for view invariant action recognition, i.e., one of the fundamental challenges from video data recognition. \n\nEmpirical results on Something-Something V1 & V2, Diving-48, FineGym show that the proposed achieves the state-of-the-art results, though seems marginally.  For example, the proposed Ensemble model (SELFYNet-TSM-R50EN ) achieved 56.6% and 67.7% attop-1 accuracy for V1 & V2, vs. the MSNet-TSM-R50EN got quite similar performance as 55.1% and 67.1% (Kwon et al., 2020, Ref. [3]). \n\nOn FineGym, SELFYNet-TSM-R50 achieves 49.5% and 87.7%, these do look better & more clearly, compared with 46.5%, 81.2% reported from (Shao et al, 2020, Ref. [4]).\n\n### Weakness: \n\nFrom Figure. 2 or Section 3.2.2, the final STSS representation Z is the combination of original input V and the STSS feature F(S(V)), so kind of unclear how much contribution is made from the original V in terms of the final recognition performance. Maybe this part is addressed in the experimental section somewhere. \n\nThe proposed neural block, SELFY, includes 3 parts as self-similarity transformation, feature extraction, and feature integration, it seems none of them is original but the combination of these put into the end-of-end NN structure is new as claimed by authors. There are some additional experiment results presented in section 4.3 (for different types of similarity, and for different feature extraction), still, it seems unclear how important or sensitive for each step within the whole framework based on TSN ResNets and TSM ResNets, given the complexity here. \n\nSelf-similarity can be applied for both image and videos (i.e., Section 3 or Ref [5], etc), I know the focus of this submission is video action recognition, still, seems interesting to know the proposed framework \"3 steps combined + CNN end-of-end\" apply to image object recognition. If that also achieved good results compared with state-of-the-art for object recognition, it will be a strong support for the proposed methodology, and if not, which is also good to know & could be multiple reasons behind. \n\nBesides these video based action benchmarks, it should be interesting to see results on a depth enriched data set (i.e., RGB-D), as missing depth information is one of the limitations from video data. Ideally, we should see similar good performance if the proposed methodology is effective for representation learning.\n\n### Reference:\n\n1. Videos as Space-Time Region Graphs, Xiaolong Wang, Abhinav Gupta, Proc. European Conference on Computer Vision (ECCV) 2018\n\n2. TSM: Temporal Shift Module for Efficient Video Understanding, Ji Lin, Chuang Gan, and Song Han,  Proc. IEEE International Conference on Computer Vision (ICCV), 2019\n\n3. MotionSqueeze: Neural Motion Feature Learning for Video Understanding, Heeseung Kwon, Manjin Kim, Suha Kwak, Minsu Cho, Proc. European Conference on Computer Vision (ECCV) , 2020\n\n4. Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for finegrained action understanding. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 \n\n5. Imran N. Junejo, Emilie Dexter, Ivan Laptev and Patrick Perez, Cross-View Action Recognition from TemporalSelf-Similarities, Proc. European Conference on Computer Vision (ECCV) 2008",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice experiments with limited novelty",
            "review": "The paper proposes a novel nerual block based on the classifical spatio-temporal self-similarity (STSS), named SELFY, which can be easily inserted into nerual architectures and learned end-to-end without additional supervision. SELFY could capture long-term interaction and fast motion in the video with a sufficient volume  of the neighborhood in time and space. \nThe nice point of the method is that it is heavily investigtated through experiments. Evaluated on three standard action recognition benchmark datasets,  the proposed SELFY is demonstrated the superiority over previous methods for motion modeling as well as the complementarity to spatio-temporal features from direct convolution. Moreover, the paper is clear and seems correct ,technically.\n\ncomments:\n\n-The first concern is about  the limited novelty of the method. Despite revisting the self-similarity, from section 3.1, the learning of generalized and long-term information is a property of the STSS rather than a contribution of this work. If necessary, authors should give more details on the classifical STSS to show the improvement. \n-The authors briefly mention the differences between the new proposed and non-local approaches (Wang et al., 2018; Liu et al., 2019) and correlation-based methods (Wang et al., 2020; Kwon et al., 2020), which appears inadequate and is better to integrate them into a comparison in the form of Figure 1.\n-How are parameters (5, 9, 9) and (9, 9 ,9) chosen? The best result occurs as the  temporal offsets being chosen from {-3, ..., 3}. Will there be better accuracy when the range is larger? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}