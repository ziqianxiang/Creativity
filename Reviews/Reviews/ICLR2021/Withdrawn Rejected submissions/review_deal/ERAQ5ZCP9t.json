{
    "Decision": "",
    "Reviews": [
        {
            "title": "Robust Multi-view Representation Learning",
            "review": "This paper proposes a new autoencoders-based method for multi-view representation learning, called Robust Multi-view AutoEncoder (RMAE). Empirical studies are performed on both synthetic and real data to show the performance and applicability of RMAE over three baselines. \n\nI think this work does not pass the acceptance bar at ICLR conference. The reasons are as follows:\n\n- Firstly, the paper failed to give motivations for the significance of this work. In Conclusion the authors say, \"We proposed two methods, one based on view-dropout and its flow-based generative modeling extension.\" The authors do not very clearly explain these two methods in the text, neither are the results demonstrated in that way. Based on the experimental results, the proposed method RMAE didn't perform well in real datasets. From my understanding, it seems that all compared methods are self-designed as no citations were given. The presented experiments are also not thorough, there are several stronger baselines for multi-view representation learning such as [1-3] which are not evaluated and compared. \n\n[1] Zhang, Changqing, et al. \"CPM-Nets: Cross Partial Multi-View Networks.\" Advances in Neural Information Processing Systems. 2019.\n\n[2] Lu, Run-kun, et al. \"Auto-encoder Based Co-training Multi-view Representation Learning.\" Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, Cham, 2019.\n\n[3] W. Wang, R. Arora, K. Livescu, and J. A. Bilmes, “On deep multi-view representation learning,” in ICML, 2015, pp. 1083–1092.\n\n- Secondly, the paper is a somewhat poor presentation in certain parts of the paper. For instance,\n\n1) The best summary of the main approach is given in Figure 1, but there are not much details on how it was done. E.g., $L_S$ mentioned in the Caption is not shown in this Figure; it is also unknown how it relates to different views in the learning process. \n\n2) The authors should also give more details about how to extend Equation (1) from the single-view flow-based model to multi-view learning.\n\n3) The main problem definition of \"missing views\" is too vague to be a proper definition. You want to address \"missing views\", but at this point, it remains unclear what these \"missing views\" in fact are. Perhaps it is more accurate to say \"incomplete views\" or partial views\", see [1].\n\n4) The paper has severe grammar issues, suffers from a sloppy presentation, and apparently not even a spell checker has been used. E.g., in Section 4.1, \"Equation 1 is gives the likelihood the training procedure optimizes\", and the full name of \"pdf\" is never given.\n\n5) Figures 5 and 6 are not well structured and take too many spaces.\n\n6) In Figure 3, the baseline methods should consistently use the same short name.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The addressed problem is unclear, the experimental evaluation is lacking.",
            "review": "This paper proposes an autoencoder scheme for multiview data that is claimed to be robust to missing views due to the training over view-based dropout.\nFirst, it is not clear what type of problem is addressed and which type of data. It is important to erember that each type of data has specific properties and statistics. Aa far as my understanding of the few details available in the paper goes, the authors consider images of natural scenes taken from different viewpoints (eg, multiple cameras acquiring the same scene). If the authors were considering different scenarios, eg time-variant signals acquired by multiple sensors, that was not clear. \nSecond, the experimental evaluation is lacking in several aspects.\nDataset: the authors claim they experimented over a synthetic dataset of their own about which they provide no information at all. algorithm: the authors do not spend a single work describing which algorithm they considered for their experiments.\nThe authors also experiment with the MNSIT dataset and two other datasets, however N-MNIST is a surprising choice as this cannot be considered really a multiview dataset.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "lack of novelty and inconclusive results",
            "review": "The paper proposes a heuristic way of stacking AEs, to be less prone to missing views. During training, the method leaves some views out and tries to reconstruct all of views including the missing ones. It first computes some view-specific representations and then combine them to get a global shared representation space.  \n\nI do not see any strong point in this paper.\n\nA clear reject. \n1)The proposed method is only compared with some baselines and the baselines are too simple. (see below) \n2)According to the reported results, the baseline methods beat the proposed method in most cases. \n\npp.4 last paragraph says, \"the CAT baseline is not robust to missing views, since it does there is no inter-relationship modeling.\" First the sentence has grammatical issues. Second, it is not clear how this baseline is trained to be robust against missing views. I suspect if the CAT baseline properly be trained with missing views (e.g. impute the missing view with mean or median), its performance will improve.\n\npp.5 In the IMAE baseline, averaging codes from all views tends to fail because even if there is one less informative view, its code would act like an outlier and results in a poor estimation of the final code via averaging. As far as I know, using k-nn classifier (not averaging codes) is common practice in the shared space. \n\nFigure 5 and 6: In most cases, baseline methods beat the proposed RMAE method. In Figure 5, a simple concatenation approach almost always beats the proposed RMAE method. In figure 6 on the right, the IMAE baseline always beats the proposed RMAE method. \n\npp.5 in the synthesized experiment, each view has only 6 features. Usually the reason that a toy dataset has a very small number of features (e.g. 2 or 3) is to allow 2D or 3D visualization. The choice of having 6 features seems too low specially because it is not help with visualization. You could try 100 features.\n\npp.2 line 2: shouldn't this be \"a shared decoder\"?\n\npp.2 \"NN are not always able to gracefully handle missing data from modalities.\" Reference needed. On what basis neural networks are any different than other learning methods in handling missing data?\n\npp.2 last paragraph: \"learning a single bottleneck representation shared across all view ... captures intersection of information across views... but MVAE captures the union of information across the views instead\". MVAE also has a shared layer. why it captures union of info?\n\npp.5 all these network does is to reconstruct while a view is missing. The reconstruction is not the goal, but how good the representation in the bottleneck layer L_S is the important thing. Unfortunately, there is not many experiments on this. \n\npp.3 Figure 1 shows L_{all} but caption says L_S. Is it a typo?\n\nAssumptions are not adequately discussed. For example, what if a crucial view go missing. The effect of the number of features per view not discussed. Could a view with larger feature vector size dominate others?\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Lack of related work",
            "review": "The paper proposed a multi-view learning framework by reconstructing all views from randomly subsampled views. Along the same direction, the paper proposed two approaches, including a deterministic and a probabilistic one, to generate intermediate representations. It is very easy to follow the paper. My comments are as follows:\n\n1 - Generally, there are two categories of methods for learning from multiple views, including learning from reconstruction and learning from contrastive classification. The paper only discussed former one, and ignored the latter. \n\nIn terms of the training speed, contrastive classification is much faster than reconstruction, especially in text domain. Empirically, [1] has shown the effectiveness of simply combining multiple views through individual contrastive classifiers. Since there is no decoding process, more elaborate and sophisticated encoders can be applied in the scenario.\n\n2 - The paper defines \"robust\" as being able to handle missing views. I would argue that there is no need to specifically deal with it. The most difficult situation is when two provided views are not paired at all, however, I don't think the proposed framework can handle it either. Other situations with at least some paired views are easy to deal with. In addition, the proposed method in [1] can handle most situations effectively and efficiently. \n\n\n\n\n[1] Tian, Y., Krishnan, D., & Isola, P. (2019). Contrastive multiview coding. arXiv preprint arXiv:1906.05849.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}