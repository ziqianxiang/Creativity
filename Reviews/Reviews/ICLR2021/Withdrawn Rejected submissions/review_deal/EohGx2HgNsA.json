{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind review #4",
            "review": "Summary:\nThis paper proposes and introduces a Neural architecture search library built on pytorch which provides high level abstractions for designing search spaces, common interfaces to datasets and benchmarks such as Nasbench-201 and Nasbench-301, and extensible implementations of some Neural Architecture Search(NAS) methods. The library is designed to help facilitate scientific research and reproducibility for NAS. The library aims to streamline into one framework implementations for both discrete and weight-sharing based algorithms in a way that the algorithms themselves are agnostic to the search spaces. They demonstrate their framework by reproducing results for several algorithms such as DARTS and GDAS. They show how DARTS can easily be applied even to hierarchical search spacings using their modular formulation.\n\nPros:\n\t1. Provides a modular framework for Neural Architecture Search which could encourage easy reproducibility and accelerate Neural Architecture Search research.\n\t2. Flexible and modular framework for defining search spaces.\n\t3. Provides a great interface to evaluate on Nas-bench and benchmark datasets.\n\t4. \n\nCons:\n\t1. Unclear support or documentation on multi-gpu and distributed training support limits the usability for larger scale experiments. Better documentation and support and results for larger scale experiments would greatly improve the submission.\n\t2. It's somewhat unclear how close the implementations reproduce original results. For example, the GDAS results on CIFAR 10 seem to achieve much worse performance than was reported at twice the parameter cost while using 10x the search cost. Uncertainty could limit it's usage for strong baselines. Additional work on the search algorithms would be of great benefit to demonstrate the flexibility and extensibility of the framework.\n\t3. Would benefit from more documentation since the usage and api for things such as the trainer is somewhat unclear. \n\n\nReasons for score: \nOverall, I weakly recommend rejecting this paper. Without a doubt I believe that a unified architecture search library is truly beneficial to the field and would help accelerate architecture search research. It would be of great benefit to be able to provide strong baseline comparisons without every author having to search and integrate algorithm implementations into their training pipeline. The framework most notably provides very helpful tools to access the nas-bench datasets and a flexible framework for defining search spaces. However, it's not entirely clear that this framework is entirely mature enough, especially related to it's documentation, algorithms, and training support, but could be invaluable with improvements. \n\nQuestions:\nHow extensible is naslib for multi-gpu and distributed NAS?\nOne-shot NAS is discussed several times in relation to weight-sharing approaches, but it's somewhat unclear if results are shown after one training epoch or supernet training?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The results are good but novelty is not enough",
            "review": "\nThis paper builds a library NASLib towards NAS which integrates the core parts, including search spaces, optimizers and trainers/evaluators, into a modularized framework. NASLib may help researchers to easily develop individual NAS components.\n\nStrengths\n1. The built library can facilitate implementation or developing NAS methods. This stream of works will be benefit for the community, which eliminate the burden of human engineers.\n2. NAS methods implemented in NASLib achieve comparable results with that released in the original related papers. The code is attached for better reproduction.\n\nWeaknesses\n1. My main concern lies in how broad impact this paper can bring. The built library only involves the DAG (directed acyclic graph) cell-based search spaces. However, this type of search spaces normally bears high hardware latency, due to the complicated topology structures. Recently, block-based search spaces, e.g. MobileNet, ShuffleNet, have achieved great success in the NAS field. Though the blocks of theses search spaces may also be represented as a particular case of a DAG, how well the library can be generalized to more types of search spaces should be clarified. It is suggested to integrate the block-based search space and compare with some important works, e.g. ProxylessNAS[1], FBNet[2]. Moreover, multi-objective optimization is commonly performed in nowadays NAS methods but not mentioned in this paper. It seems many advanced technologies of NAS are missing in the library, which makes me concern about the impact of this work.\n2. This is a promising engineering project. Does this paper carry any academic novelty? This should be explained.\n3. In many cases, the small scale dataset is not enough to evaluate the method due to the large variance of the final results. The large scale dataset, e.g. ImageNet, is required to be included. Related experimental results need to be reported.\n4. The codes of DARTS and GDAS are all released and widely used. Though this works integrates more modularized parts, is there any other advantage over the original projects?\n\n[1] Cai H, Zhu L, Han S. Proxylessnas: Direct neural architecture search on target task and hardware[J]. arXiv preprint arXiv:1812.00332, 2018.\n[2] Wu B, Dai X, Zhang P, et al. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 10734-10742.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Need to provide better proof to support claims",
            "review": "##########################################################################\n\nSummary:\nThe paper presents a framework for development and creation of Neural Architecture Search (NAS) algorithms.\nNASLib structure is broken down into 3 blocks: search space, optimizer and trainer/evaluator.\nThe authors demonstrated some use-case: DARTs, NAS-bench201 and one-shot model on hierarchical search space.\n\n##########################################################################\n\nReasons for score: \nThe authors claim easy-to-use modular library and minimal prototyping time. However there is no measure of how easy or how well accepted this tool is in the dev community.\n\nNASLib is built upon other libraries: pytorch and NetworkX. Some features claimed are native pytorch or other library features: checkpointing, data loading, logging. It is hard to discern what is a differential between this library from all others NAS libraries.\nThe paper does not provide proof or examples that NASLib is supperior than others more mature and well accepted frameworks: NNI, AutoGluon, Auto-Keras. \n\n##########################################################################: \nPros: \n- flexibility to combine some NAS methods\n\n##########################################################################\nCons:\n- no clear evidence that is better than others\n- no metric of ease of use\n- well acceptance in dev community\n- hard to distiguish the features from other built in libraries \n\n##########################################################################\n\nQuestions during rebuttal period: \nPlease address and clarify the cons above \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "\nThis paper presents NASLib, a code repository for performing NAS experiments. The motivation for doing this is sound, as reproducibility is a key problem in NAS (https://arxiv.org/abs/1912.12522v3). However, actual implementation seems limited.\n\nI don’t think a code repo should be published as a research paper at a top machine learning conference unless it is a game-changer. A good example of this is BackPACK (https://openreview.net/pdf?id=BJlrF24twB) from ICLR 2020. Its primary contribution is a codebase that facilitated computations that were previously too expensive in PyTorch e.g. getting individual sample weight gradients on the backward pass.  It opened the door to a whole suite of feasible new optimisers in Python. \n\nBy contrast, it doesn’t appear to me that this codebase allows NAS researchers to do anything new. It provides a wrapper for running NAS experiments that there is already good open-source code for (e.g. https://github.com/D-X-Y/AutoDL-Projects). The authors use the original DARTS code as an example of what needs remedying but this is 2 years old and has been improved in other repos.\n \nIt is quite sparse in terms of the actual spaces and optimisers defined. There is an emphasis on one-shot methods and no code for REINFORCE. As this is so prevalent in the NAS literature (https://arxiv.org/abs/1611.01578, https://arxiv.org/abs/1707.07012,https://arxiv.org/abs/1802.03268) then this is a fairly glaring omission. The list of improvements planned in the paper emphasises that this is not finished work.\n\nThe authors point out that there are other libraries for this purpose (https://github.com/microsoft/archai, https://github.com/walkerning/aw_nas) that “aren’t yet published” and these are not mentioned again. Firstly, I don’t think it is particularly important that there exists a published conference paper to supplement a code repo. Secondly, a comparison is needed. Why should I use this library over those libraries? What are the functional differences, limitations etc.\n\nThe evaluation consists of results comparisons to existing code. The DARTS performance in Table 1 is very different. Is there an obvious reason for this?\n\nThere are no research contributions, the technical contributions seem limited. I don’t see what this library provides that doesn’t already exist in the ML ecosystem, or how it can be a  game-changer in NAS research. It is limited in its range of NAS algorithms and search spaces. I recommend rejection.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}