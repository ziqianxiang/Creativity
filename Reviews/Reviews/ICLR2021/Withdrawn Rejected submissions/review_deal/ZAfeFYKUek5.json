{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Originality: The paper can be developed into a very nice contribution, if the value of the newly introduced optimization variance is evaluated more thoroughly (e.g., through simple theory, or through more rigorous experiments).\n\nMain pros:\n- One of the early works studying epoch-wise double descent\n- Optimization variance is an interesting concept and might very well be useful for finding good early stopping points.\n\nMain cons:\n- Findings about epoch-wise double descent remain inconclusive\n- optimization variance is not sufficiently evaluated to judge its usefulness: theoretical justification for why is this an important quantity and when does it arise naturally is something missing at this point.\n\nOverall: there was a consensus that the paper focuses and provides an interesting story, with new ideas; however, paper's conclusions are not strongly supported by experiments; more experiments are needed to make arguments conclusive. "
    },
    "Reviews": [
        {
            "title": "The paper is in the right direction, but more work needed to be more covnvincing. ",
            "review": "Summary:\n\nThe paper studies the trajectory of the test error as a function of training time focusing on Epoch-Wise Double-Descent.  Similar to \"Rethinking Bias-Variance Trade-off for Generalization of Neural Networks\" by Yang et. al., the paper shows that if one decomposes the test error to bias and variance terms, Double Descent occurs as a function of train time as a result of unimodality of the variance term (while the bias term decreases monotonically).  The paper also introduces a quantity they name optimization variance (OV) and that correlates with the test error (while being only a function of the train set) and can be useful for early stopping.\n\n=====================================================\n\nMain Concerns and improvement points:\nRegarding the variance as the main culprit behind Double Descent:\nWhile it is interesting to see that the analysis of Yang et. al. carries over to epoch double descent, by itself, this experiment is not too surprising. Citing the discussion section \"Contradicting to the traditional view that the variance keeps increasing because of overfitting, our experimental results show a more complex behavior: the variance starts high and then decreases\nrapid\". This is exactly the claim made in Yang et. al, for model double descent making the novelty of the empirical result more limited. As the authors mention (and promise to study in the future), understanding why the variance diminished with time after the interpolation threshold is an important question that could substantially strengthen the paper. \n\nRegarding Optimization Variance:\nWhile it is cool that the optimization variance not too hard to compute and correlates well with the test set (for a fixed architecture) I'm not sure what is the fundamental contribution here. Just defining a quantity and showing some properties it has without motivation for why and when should we expect this quantity to arise is not a persuasive result. A couple of specific concerns for me are: \n1) Correlation does not mean much by itself. For example, the train accuracy is very correlated with the test accuracy. There are other empirical quantities that correlate with the test (and provably so at times, see for example https://arxiv.org/pdf/1912.00528.pdf). \n2) Usually, training for long enough is sufficient to have high correlation with the test.  For example in fig 5 we could ask where would lie a model trained for a very long time (say 100 epochs) my bet it would right there on the line of best test error.  \n3) If the argument here is early stopping I can just use the test set.\n4) The quantity does not correlate through different model architectures.\n\nTo alleviate my concerns I would love to see some theoretical justification for *why* is this an important quantity and when does it arise naturally. Alternatively, if we can predict test set across different model architectures, that would give more motivation to as why we might want to look at this quantity more deeply.\n\nTo summarize, the first part of the paper is lacking in novelty and in the second part, the OV is not deeply enough motivated. \nI do believe that the paper is on the right track and with some work should be publishable in a future conference.   \n\nMinor Comments:\nIt might be a personal preference, but I find the notation confusing and in disconnect with the standard notation in the field. For example, using t as labels and y as predictions is highly non-standard and would confuse many readers.  (why not y for label, \\hat y or p for prediction, t for training time?)\n\"The expected loss should be small to ensure good generalization performance\" - this is a very informal sentence that I found confusing rather than helpful.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Hard to see the novelty of the metric and how is OV different from simpler metric",
            "review": "This paper proposes the metric of estimating the variance of gradients during optimization of deep networks and empirically found that this metric correlates with test errors, which may indicate a good point for performing early stopping during training.\n\nMy first thinking is that the metric is indeed interesting and informative if, it could be applied to indicates the generalization on the test set, of course. Then my concern rises from the methodology used in this paper, as using 'a correlation' between optimization variance and generalization error to indicate a 'causal relation' between these two numbers. Granted, it might be difficult to actually prove it. From a big picture, it would still be nice to see a comparison analysis with other simpler metrics, such as the gradient magnitude itself?\n\nThe definition of \"optimization variance\" seems to be weird, as in equation 6, the nominator scales with gradient magnitudes, and the denominator tends to have very smaller changes after some iterations, comparably. So why is this OV so different from the gradient norms as to constitute a novel metric? At least from experiments, the changes in denominator do not matter that much.\n\nThe dataset is kind of simple in the empirical analysis. It would be better if Imagenet size level ones or regression tasks are also performed.\n\nSmall questions:\n\nFigure 3, what is 'different number of training batches'? is that batch size?\n\nFigure 4 shows some strong upheaval in terms of accuracy, is that from a large gradient that somehow explodes during training?\n\nFigure 5 has really unreadable captions and legend notations.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Validation datasets are no longer needed?",
            "review": "Having a stopping rule without the validation set is intriguing, especially for datasets with a low number of samples. The authors propose a rule that doesn't require the validation dataset, i.e. it is solely based on training data. It introduces the notion of optimization variance which is different from the variance of gradients. \nI give them credit for the idea and also the analytical comparison with the variance of gradients.\n\nOn the one hand, the experiments in Section 3.3 are geared towards showing the role of the validation dataset, but on the other hand they lack rigor. It would be interesting to learn the impact at different levels of the percentage of the samples in validation. I also think the work can be impactful on small datasets. While they experiment with large-scale datasets, experiments with even smaller datasets and different validation set proportions would be of great interest. ",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting idea, but needs more work",
            "review": "* quality\nThe idea is interesting, but the paper lacks theoretical insights. And it was difficult to find in the paper how OV is related to generalization.\n* clarity\nThe paper needs to make it more clear how OV is related to generalization - is it always monotonically decreasing/increasing?\n* originality\nThe idea seems new, but more theoretical insight is needed.\n* significance\nIf the authors include more theoretical justification, the paper's results would be more significant. The current version is too empirical and not very convincing. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "The paper under review studies the epoch wise double descent phenomena empirically. The epoch wise double descent phenomena is the observation that the risk of a large neural network trained with SGD first decreases, then increases, and finally decreases again as a function of the epochs or SGD steps. In addition, it proposes a quantity called ``optimization variance (OV)'', and it demonstrate that OV correlates with the test error. Based on this observation, it proposes to early stop when the OV reaches a minimum.\n\nStrong points are that the paper\ni) is an early work studying epoch-wise double descent, and that \nii) the newly introduced optimization variance is an interesting concept and might very well be useful for finding good early stopping points.\n\nWeak points are that \ni) the findings about epoch-wise double descent are not conclusive, and that \nii) optimization variance is not sufficiently evaluated to judge its usefulness. Both is discussed in more detail in Comments 1-6 below.\n\nBased on comments 1-6 below I think that the paper's conclusions are not yet supported by the papers experiment. I do think that the paper can be developed into a very nice contribution, if more substantive findings about epoch-wise double descent can be obtained, and/or if the value of the newly introduced optimization variance is evaluated more thoroughly (e.g., through simple theory, or through more rigorous experiments). \n\nComments:\n1/ The paper's main finding on epoch-wise double descent is that ``the variance dominates the epoch-wise double descent of the test error'' meaning that it ``is mainly the variance that contributes to epoch-wise double descent''. This statement is overly general, because it is drawn from observations for one particular setup (20\\% label noise, CIFAR10 and SVHN). If the label noise is less, it becomes apparent that both the variance and bias work together (as a sum) to generate a double-descent like curve. Adding curves for other distributions (e.g., less label noise, other datasets) can illustrate that.\n\n2/ The paper's main contribution to the study of epoch-wise double descent is the empirical bias-variance decomposition. However, this approach does not enable to draw conclusions on why double descent occurs, as the paper notes itself ``the reason why the variance begins to diminish at the late phase of training is still unclear''. \n\n3/ The paper finds a strong empirical correlation between the test error and the newly introduced optimization variance (OV) (Fig. 2), and proposes to early stop based on the OV. However, Fig. 2 also shows when this fails: c) ResNet 34 on CIFAR-10 shows that OV is minimal at 250 epochs, while the accuracy is highest after about 15-20 epochs. Of course, such an early stopping rule is allowed to fail sometimes, but the evidence that this is a good early stopping rule is not convincing.\n\n4/ The value of OV would be much more convincing if the paper could provide a theoretical result even for a simple toy case like a linear model.\nIt would also be more convincing if the benefits of OV hold for a broad set of models; the paper points out that it doesn't generalize to a large class of models: ``we need to point out that we did not observe a strong correlation between the OV and the test accuracy when using significantly different architectures''.\n\n5/ The paper omits a recent reference on epoch-wise double descent: https://arxiv.org/abs/2007.10099, which is very related because it explains epoch-wise double descent as overlapping bias variance tradeoffs for a variety of setups, both empirically and practically. \n\n6/ As https://arxiv.org/abs/2007.10099 finds, epoch-wise double descent can bit mitigated, and this can typically improves performance. How does the paper's approach to find the early stopping point work in this setup, where double descent is mitigated?\n\nMinor comments:\n``20\\% labels of the training data were randomly shuffled to strengthen epoch-wise double descent'': This is misleading as it suggested that with less or zero label noise, epoch-wise double descent does occur, but it does not for the models and setup considered in the paper.\n\n-----\nUPDATE: Thanks for the response, I have responded below and kept the score constant.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}