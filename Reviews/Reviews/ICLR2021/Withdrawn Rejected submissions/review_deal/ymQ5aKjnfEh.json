{
    "Decision": "",
    "Reviews": [
        {
            "title": "Augmentation of one-dimensional time series ",
            "review": "The authors propose a way to augment one-dimensional time series with additional information that capture the symmetry in the original time series. The framework was validated using three baselines, each baseline converts the time series into a network of different topology. The framework was then used to augment the time series using the duality between the original time series and the generated network. These embeddings were fed to CNN as part of the RL algorithm on the experiment section. The results (indicated by the cumulative return of the RL model) shows improvement over using the original time series. \n\nThe idea of augmenting time series so that it preserves the symmetry in the data does look very nice to me. And the results support that idea. Although I could not comment on the method as it is out of my expertise. \n\nFew question to authors, how does the method cope with missing data? What is the computational time? Is the method applicable only on graphs generated from the time series? If not what else could be used? \n\nEquation 1 needs more explaination. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "A study of three tools that transfer 1-D time series into graphs to capture correlations/symmetries in data. Two studies are presented to show the effectiveness, but the evaluation is weak and preliminary to get conclusive results.",
            "review": "The paper studies the effect of symmetry augmentation of 1-D time-series data for two case studies, time-series classification and reinforcement learning for financial portfolio management. The paper exploits three existing tools to transform 1-D time series into graphs that aim to capture symmetries/correlations/co-occurrent events that otherwise cannot be detected. In addition, the work attempts to quantify the effect of augmentations using the notion of persistent homology. The experimental results demonstrate improvement in learning efficiency.\n\nPros:\n\n- The paper is well-written and motivated; it studies a timely problem for the time-series literature\n- Three augmentation approaches are considered to transfer 1-D time series into graphs/networks\n- Experiments results show the potential of such augmentations for two distinct applications\n\nCons:\n\n- The experimental evaluation is weak. Only a synthetic dataset was considered for the classification task (despite the existance of hundreds of other datasets). Similarly, only a particular portfolio was used for the RL task.\n- Comparison against other augmentation strategies is missing.\n\nDetails:\n\n- In both case studies the achieved results in test sets are marginally better. For the RL case study, it's clear that even the original 1-D representation does quite well, which makes the reader wonder how much true benefit someone can get by employing such augmentation strategies. The same holds for the classification task. There are dozens of methods, even simple one nearest neighbor classifiers with simple distance measures that achieve perfect accuracy for the CBF dataset. Employing an expensive DNN architecture + augemtation and not having the ability to reach such accuracy is problematic and make it unrealistic for true impact.\n\n- For classification, the paper needs to perform a robust evaluation against 100+ datasets available in the UCR times-series archive. Showing results on just a sythnetic dataset is not sufficient, especially when even for that a state-of-the-art performance is not achieved. \n \nThis paper provides an overview of that area and existing approaches: \n\nBagnall, Anthony, et al. \"The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances.\" Data Mining and Knowledge Discovery 31.3 (2017): 606-660.\n\n- For RL,  the paper needs to compare using different portfolios, not a single case. Current results show marginal improvement during testing, which as before, makes such approaches impractical considering the extreme additional effort needed. If across multiple portfolios, the test accuracy shows statistically significant improvement, then we can assess the importance of considering such augmentations\n\n- Comparison against different augmentation methods is needed. There is a vast literature in this area, so focusing only on network-based augmentations is limiting.\n\n- The paper cites works from Aboussalah et al which is not publicly available. This is problematic for a double-blind submission.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "using symmetry augmentations to learn time series representations. ",
            "review": "This work mainly studies how to use symmetry augmentation to learn a good representation for time series. \nActually, in section 3, the authors present their main conclusion that by using a direct sum based representation we can maintain at least the same amount of symmetry information regardless of the quality of the augmentations being used. \nThen three different image-from representations transformed from time series are used in their experiments (including classification). \n\nComments:\n- since the main contribution in this work should be the theorem in section to show using a direct sum of more symmetry augmentations will still help representation learning for time series. For those three image-from representations (GAF, MTF, RP), I wonder whether it is really suitable for *long* time series. Because it will be a time-consuming process if we train a CNN from a much bigger image.       \n- it would be better to provide an intuitive explanation of the question of what is symmetry. What is the relationship between the symmetry and the concept of features in deep learning?  It will help readers better understand your work.  \n- some terminologies in section 3 are unclear. For example, what’s the “world state” and the agent?   \n- in your experimental results in table 14 (appendix), the proposed model on that CBF dataset seems not good because the CBF should be an easy dataset from UCR (accuracy should be easily above 98%).  It would be better to evaluate your model on more challenging time-series datasets. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Unconvinced by the results",
            "review": "### 1.\tSummary\nThis paper presents a concept called “symmetry augmentation”, where they theoretically show that a direct sum of vector spaces cannot decrease the total number of symmetries of the overall representation. The term “augmentation of symmetry” is essentially an additional embedding extracted from the given observation. To demonstrate the usefulness of this concept, they consider one-dimensional time series data, where they transform the 1D input data into three 2D features, RP, GAF, MTF, motivated by the duality between time series and graph. They conducted experiments in reinforcement learning and supervised learning tasks.\n\n### 2.\tDecision\nI recommend a rejection for this paper. I am not convinced by the author’s hypothesis that more symmetry in the input representation leads to better “ML performance”. The theoretical results do not show that their approach can benefit learning. Additionally, the empirical results are weak due to the atypical tasks/datasets.\n### 3.\tReasons\na)\tThe usage of symmetry augmentation is very confusing to me, especially when the related works included “Augmentations for RL” and cited Laskin et al. (2020),  a “data augmentation” approach. This is completely orthogonal to the proposed “symmetric augmentation”, which is increasing the complexity of the representation; I fail to see the connection.\n\nb)\tThe section title “Learning Symmetry from CNN” in related works is not precise. The reviewed group equivariant networks do not “learn” symmetries. For example, one would not say CNN learns “translation symmetry” as the translation symmetry is a priori built into the model.\n\nc)\tThe following statement is unsupported and imprecise. “... even if the augmentation does not expose more symmetries, ML performance would not be worse...” First, what exactly is being referred to as the “ML performance”, is it referring to the training error? generalization error? Second, I am unable to follow the reasoning on the relationship between the number of symmetries and the ML performance, please elaborate. \n\nd)\tThe experiment of choice is atypical. I think it would be nice to compare to more commonly used datasets for sequential modeling, see [1]. I mean, the experiments in Sec. 4.2 has “30 training examples”. Use a larger dataset and sweep across different training set sizes to study the effect on data efficiency.\n\n[1] Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. \"An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.\" arXiv preprint arXiv:1803.01271 (2018).\n\ne)\tSome experimental details are unclear, and no code was provided. \n\n&ensp;&ensp; i.\tIt is unclear to me how the 1D baseline is being used with CNN architecture which takes in a 2D input.\n\n&ensp;&ensp; ii.\tPlease report the number of trainable parameters for models in Table 3. There are more parameters for RP+MTF+GAF than RP due to the change in input representation?\n\n&ensp;&ensp; iii.\tHow are the hyperparameters being chosen? There are 30 training examples, and 900 testing samples, how about the validation set?\n\n&ensp;&ensp; iv.\t“We conducted ten trials. Table 3 shows the average test accuracy.” Please report the standard deviation; Similarly error bars for the RL plots.\n\n### 4.\tAdditional Comments\na)\t“state G to GL(V)”. To be precise, maybe define what is GL.\n\nb)\tThe text on the figures (2,3 5) are barely legible; They are too small and blurs when zoomed in.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}