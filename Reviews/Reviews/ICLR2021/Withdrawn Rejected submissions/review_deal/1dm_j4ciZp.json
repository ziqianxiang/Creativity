{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committe members, who provided a binding decision. The decision is \"Significant concerns (Do not publish)\". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee's decision .\n\nThe technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:\n\nThis paper studies the problem of evaluating optimiser's performance, which is important to show whether real progress in research has been made. It proposes several evaluation protocols, and used Hyperband (Li et al. 2017) to automate the tuning of each optimiser in the bench-marking study. Evaluations have been conducted on a wide range of deep learning tasks, and the paper reaches to a conclusion that none of the recently proposed optimisers in evaluation can uniformly out-perform Adam in all the tasks in consideration.\n\nReviewers agreed that the evaluations are extensive, however there are some shared concerns among reviewers. The paper argues that manual hyper-parameter tuning by humans is the right behavior to target for, which is the motivation to use Hyperband as an automating tool, and there is a human study to demonstrate that Hyperband tuning resembles human tuning behaviour. Some reviewers questioned about this desiderata choice that favours human tuning behaviour, also concerns on how the human study is conducted (and to what extend the human study itself is reflective enough for the human tuning behaviour in general).\n\nPersonally I welcome any empirical study that aims at understanding the real progress of a research topic, and I agree it is important to make rigorous automation tools in order to enable such a large scale study. Therefore, while the presented results are extensive, I would encourage the authors to incorporate the feedback from the reviewers to better examine their assumptions. "
    },
    "Reviews": [
        {
            "title": "Official Blind Review",
            "review": "This paper mainly proposed an evaluation framework for optimizers in machine learning jobs. It points out that existing benchmarking often applies best hyperparameter or random search hyperparameter. Their proposed framework re-evaluates the role of hyperparameter tuning in machine learning. \nIt mainly deals with two cases, end-to-end training efficiency and data-addition training efficiency. The major findings are as follows.\n1. Random search might lead to unnecessary training when the loss does not converge. Therefore, given limited budgets, it is better to have a benchmark strategy for finding the best hyperparamater.\n2. Training on the same model repeatedly is necessary when there are new data. However, the existing hyperparameters might not be optimal when training data updates.\nFor end-to-end training efficiency, they assume users apply Hyperband and adopt \\lambda-tunability to meansure the performance of optimizers. For every optimizer, the framework computes the CPE value based on the complete trajectory and evaluates the optimizers. For data-addition training efficiency, the framework extracts a subset to tune the hyperparameter and then apply them to the entire dataset and evaluate the performance of various optimizers. \n\nStrengths:\n1. The overall structure is clear with a detailed explanation of the limitation of the existing optimizer benchmark scheme. The two cases the authors emphasized are practical and common in real-life scenarios.\n2. I think the idea of applying Hyperband instead of random searching is quite reasonable based on the result in figure 2.\n3. The experiments are conducted based on a variety of datasets and optimizer. The findings are well presented with different angles, task type, optimizer mechanism.\nQuestions:\n1. It seems the algorithms 1 and 2 are similar. If it is possible to combine the algorithm together, like run the model on the subset to find several good optimizers and conduct it on the overall dataset.\n2. I think overall the idea is good and easy to understand. However, I wonder if there is any way to support the effectiveness of the solution besides running different setups on the model.\n\nWeaknesses:\n1. I wonder the time cost of adopting this evaluation framework, like the time it needs to have a consolidated result on the optimizers' performance.\n2. Some explanations regarding the details of the algorithms should be added beforehand. It is better to have an explanation of all the variables mentioned in the algorithm for clear referencing. For example, in the equation 1, the meaning of P, I assume it should be accuracy. Variable M, how it is decided and how it will affect the ultimate performance.\n3. Some minor suggestions. There are some spelling mistakes in the paper. For example, in table 1, it should be non-adptive. And the format and location of figure and table could be improved. E.g. Figure 5 is mentioned in page 5 and located at page 8. It could be better to give a general picture of how the result looks like in the 3rd section first and then detailed experiment figures later.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting optimizer evaluation criterion",
            "review": "This work proposes two protocols for evaluating and comparing the quality of different optimizers. It points out that some existing, commonly used ways of comparing optimizers may over- or under-represent the amount of time that hyperparameter search can take. To fix this, it proposes using the Hyperband algorithm to guide the hyperparameter search. \n\nI think the authors make a convincing argument that existing approaches for comparing optimizers can downplay the role of hyperparameter search, which can be significant and can vary greatly across optimizers. I think the paper presents a fairly convincing approach for comparing optimizers, and thus for evaluating new ones against existing ones. \n\nI find the argument that Hyperband is a good choice because it more closely resembles human behaviour somewhat weak. Instead I would be more convinced by something showing that Hyperband (or whatever alternative) does a good job of terminating bad runs early, since this is the point of using a bandit algorithm over random search. \n\nI would also like to see some discussion of how others could use the proposed procedures  when deciding which optimizer to choose for their task.\n\nFinally, I think the authors could do a better job of explaining why CPE is the right metric to use (i.e., why is considering peak performance not a good choice?) \n\nIn general I like this work and recommend accepting it. However, I think it could be strengthened by more discussion of how this could be of use to the community in the future. This is especially important given that the no one optimizer seems to be universally best. \n\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Evaluation setup is interesting, but there are concerns about the protocol",
            "review": "**The score does not represent the initial review. It was updated following the discussions below.**\n\nThe paper proposes a new benchmarking protocol for optimizers in deep learning. The main argument is that previous papers have either neglected hyperparameter tuning or have employed hyperparameter search method that is far from how humans tune hyperparameters in practice. To mitigate the latter, the paper proposes to use HyperBand for automatic hyperparameter search. They show through a human study that HyperBand resembles human tuning performance more closely than random search. They then evaluate several optimizers on a multitude of tasks, including many recently proposed methods, under two scenarios: 1. Given an unfamiliar task, the effectiveness of each optimizer is measured through a metric that is cognizant of the hyperparameter tuning time. 2. After initially tuning on a subset of the dataset, how well does the obtained best hyperparameter configuration transfer to the full dataset? The main result is that the recently proposed optimizers are not substantially better than Adam.\n\nBy independently comparing recently proposed optimizers in two realistic scenarios, the paper would constitute a valuable contribution to the machine learning community. However, listed below, I take several issues that negatively impact my confidence in the protocol. If these issues could be resolved by answering my questions, I am inclined to update my rating upwards.\n\nMajor questions:\n* The main argument for replacing random search with HyperBand is that HyperBand resembles human tuning behavior much more closely. You provide the results of a human study as evidence for this central claim. However, the paper provides little detail on the nature of the study. How many participants did you have? How were they sampled? What was the expertise of the participants? If they happen to be familiar with computer vision, there is a good chance that they have trained on CIFAR10 before, thus already knowing good hyperparameter values. This would contradict the scenario that you assume in your benchmark, which is unfamiliarity with the task. Non-experts in computer vision would likely take a lot longer to tune CIFAR10 to good performance, perhaps more closely resembling the curve of random search. In its current state, I have little confidence in this human study.\n\n* You claim that in the evaluation protocol of Sivaprasad et al. (2020) each bad hyperparameter has to run fully. This is not true, because the protocol incorporates early stopping after two successive epochs in which the validation performance doesn't improve, thereby preventing at least _very_ bad configurations. It is obvious that HyperBand is a more sophisticated solution, but there is no direct evidence that it is so much better that it justifies replacing Sivaprasad et al. (2020)'s protocol. Perhaps you could include random search with a simple early stopping criterion in Figure 2?\n\n* In Algorithm 1, the performance trajectory is computed M times, and you average the peak and CPE values over all repetitions, which is good to account for stochasticity. But you never mention how large M is. Is the cost of HyperBand low enough to allow for a sufficiently large M? Would it instead be possible to compute expected validation performance as is done in Sivaprasad et al. (2020) to decrease this cost significantly? If I understand HyperBand correctly, the hyperparameter configurations are still drawn independently via random search so that the expected validation performance could be computed, but I am not entirely sure.\n\n* One of Sivaprasad et al. (2020)'s motivations for using random search is that it requires no hyperparameters (except for the search space, which is however assumed to be given by optimizer designers), which could otherwise inject some human bias into the evaluation process. In contrast, HyperBand does have additional hyperparameters that could introduce human bias. E.g., you state \"We set $\\eta = 3$ as this default value performs consistently well, [...]\". Can we be sure that this choice is not biased towards some optimizers?\n\n* Choi et al. (2019) make the case for choosing hyperparameter search spaces independently for each optimizer, noting that a unified search space may contain biases towards certain optimizers. You consider a unified search space, making the opposite argument by citing Metz et al. (2020). I could not find that argument in Metz et al. (2020). Could you please elaborate or point directly to their argument in their paper?\n\n* Among your summarized findings you state that Sivaprasad et al. (2020) find Adam to usually outperform SGD. This may be misleading, since they only suggest Adam to be more likely to yield good performance than SGD if nothing is known about the task. This is a very similar result to what the performance profile shows in your Figure 4. On CIFAR10 and CIFAR100, Sivaprasad et al. (2020) also find an SGD variant to perform as well or better than Adam.\n\nSuggestions for improving the paper:\n* It would be good to provide not only average CPE and peak performance values, but also their variance.\n\n* In the description of Scenario I you state that you compute the expected performance under different time budgets. It would be good to clarify what you mean (I suspect peak performance vs. CPE?).\n\nMinor issues:\n\n* In the related work on hyperparameter tuning methods, Sivaprasad et al. (2020) is falsely cited as a Bayesian optimization method.\n\n* The paper claims that Sivaprasad et al. (2020) consider optimizer B in Figure 1 as better than optimizer A. This is not true; towards the end of Section 2 in Sivaprasad et al. (2020) acknowledge that the value of each optimizer depends on the available budget.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs to be more rigorous ",
            "review": "This paper studies the topic of evaluating the performance of optimizers for neural networks. The paper makes the argument that existing evaluation procedures either over emphasize the finding of optimal hyperparameters or under-evaluate the performance of an algorithm by randomly sampling hyperparameters. This paper's primary objective is to propose an evaluation procedure that better aligns with a practitioner's goal than existing evaluation procedures. The proposed procedure evaluates optimization algorithms by using the hyperband hyperparameter optimization algorithm to tune hyperparameters and then score the algorithm using a weighted combination of validation performance scores over regularly sampled training intervals. The aggregate performances of algorithms are then ranked using performance profiles. \n\nThe paper's main contributions are an evaluation procedure and a new problem setup where an algorithm is evaluated over its long term use as new data is added. The presented evaluation procedure better captures practitioners' interest in that they tend to care about how much \"effort\" is required to find a near-optimal solution when allowed to tune the algorithm's parameters. The second evaluation procedure best captures the practitioners' interest by considering performance over retraining the model as new data is added. This evaluation setup is a good direction for evaluating optimizers. I think the paper accomplishes its goals and could be a useful evaluation procedure for the community. \n\nDespite what this paper does well, I cannot recommend it for acceptance because there are issues with the paper's arguments and some gaps in the evaluation procedure.\n\nI believe the paper mischaracterizes the performances being reported. The performance of the algorithms being reported is not the performance of an optimization algorithm but a meta-algorithm that combines the optimization algorithm and hyperband. Furthermore, the performance depends directly on the hyperband algorithm's hyperparameters, but these are not accounted for in the evaluation. I think it is ok to evaluate these meta algorithms, but their performance should not be presented as representing the underlying optimization algorithm. Another way to view these meta algorithms is that they are performing a global search using successive applications of local search algorithms (the optimizers). In this view, it is evident that the random hyperparameter search method is inferior to hyperband. However, it also becomes clear that one should use whatever global search method is best and not rely solely on hyperband. \n\nCan the authors specify an exact research question this procedure is designed to answer? It should be evident directly from this question what the right way to evaluate the performance is. \n\nWhy is being similar in performance to a human's ability to tune hyperparameters desirable? Shouldn't it be better? How was the study using humans conducted? Did an institutional review board approve it? The primary support for using Hyperband is that it is similar to human performance. The details of this human experiment are needed to establish how and why they are similar. \n\nThe performance of all of these experiments are random. How is randomness accounted for in the results? How many trials are needed to ensure a statistically significant result? Quantifying uncertainty is needed at both the per task level and the aggregate measure, similar to that shown by Jordan et al. (2020). The authorâ€™s may also be interested in probabilistic performance profiles (Barreto et al., 2010). Quantification of uncertainty is a necessary component for a scientifically rigorous evaluation procedure. \n\n\nMinor notes: \nThe second paragraph in the intro says a \"biased benchmark.\" What does it mean for a benchmark to be biased? Every benchmark is biased to favor one method or another. \n\nPage 4: \"Still, we argue that the random search procedure will overemphasize the importance of hyperparameters\" - this depends on what question is being answered. For example, one could ask a question about an algorithm's performance without hyperparameter tuning. Random hyperparameter search is then a good route. \n\nIn the RL experiments, it is said that the reward is the metric used. This is incorrect. The metric for that environment is the return or cumulative reward. \n\n\n\nBarreto, A. M., Bernardino, H. S., & Barbosa, H. J. (2010, July). Probabilistic performance profiles for the experimental evaluation of stochastic algorithms. In Proceedings of the 12th annual conference on Genetic and evolutionary computation (pp. 751-758).\n\nJordan, S. M., Chandak, Y., Cohen, D., Zhang, M., & Thomas, P. S. (2020). Evaluating the Performance of Reinforcement Learning Algorithms. In Proceedings of the 37th International Conference on Machine Learning.\n\n\n-----------\nupdate\n-----------\nAfter the discussions below I have changed my score from a 5 to a 6. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}