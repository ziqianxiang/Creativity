{
    "Decision": "",
    "Reviews": [
        {
            "title": "The key idea of the paper is to use self-supervised learning rather than labels for a set of base classes to learn a representation that is then useful for few shot learning",
            "review": "The main idea of the paper is very interesting: can self-supervised learning help to train a representation for few-shot learning - and how well does this compare to the classic setting, where a representation is learned on a set of base classes with the respective labels. The paper performs a wide range of experiments to compare various settings. \n\nThe paper does not contain a novel algorithm or method but rather aims to experimentally compare self-supervised learning representations in isolation or combined with base-label supervised training that is classic for few-shot learning. In that sense the paper is purely experimental - which is perfectly fine for me also for ICML. Others might have a different opinion on that front. \n\n´When starting to read the paper I was quite intrigued about the idea of the paper and what was claimed to be the main results. However, the further I progressed in the paper, the more I became disappointed as the paper quite strongly over-claims the achievements and additionally the presentation is so misleading in my view that it would require a significant rewrite to become acceptable. Therefore I cannot suggest acceptance of the paper in its current form. \n\nLet me be more specific: \n- Fig 2 is quite misleading as it does not make a clear separation between transductive and non-transductive settings. It is well known that transductive settings can outperform significantly non-trunsducitve baselines (as also confirmed by Tab 1 e.g. in the paper). Also a sensible trunsductive baseline is missing in Fig2 to be compared to. However, I would strongly argue that transductive and non-transductive needs to be fare more clearly separated ideally in separate figures to not paint a misleading picture. In fact the points (3) and (4) in section 4.1 - which are the most exciting in terms of claims of the authors - are relying on the non-comparable transductive results (UBC-TFSL). In my view this is too much overclaiming and pushing and requires significant rewriting to become scientifically acceptable.\n\n- Tab 1 is better and that respect and clearly separates these two settings. Also the picture is more honest: self-training can achieve good results essentially on par with competitors and can slightly improve in the combined setting\n\n- Table A1 in the appendix is again lacking sensible baselines given in Table 1 from the so called \"competitors\" in the paper (so called by the authors on page 5 top)\n\n- Similarly Figs 3, 4, 5, 6 are all misleading for the same reasons that fig 2 is misleading - a sensible transductive baseline is missing and thus painting a misleading picture. The only fair comparison is UBC-FSL vs. FSL-baseline (even though that is obviously not the best baseline as shown in the results) \n\n\nA detail that would be could to make clearer is the definition of the term \"combined\": While the authors clearly state what they mean by \"combined\" in sec 3.2, when looking at the fig 2-5, one might expect that \"combined\" refers to a combination of UBC-RFSL with FSL-baseline. I would suggest to choose a more descriptive name that would is less prone to confusion. Again, the authors clearly stated what \"combined\" means and this is just a suggestion as I was confused about this. \n\nSo while I think the paper contains some interesting results the main issues I have with the paper in its current form is that it is over-claiming its achievements and the presentation needs significant change to become not mis-leading to the reader. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review for \"Shot in the Dark: Few-shot Learning with No Base-class Labels\"",
            "review": "### Summary\nThis paper proposes a method for FSL that does not rely on any supervised label during training time. Instead of training an episodic  meta-learning model to generalize to few instances of new categories, the authors propose a method that only perform self-supervision (therefore, no labels) during training time.\nIn the standard FSL (ie, non-transductive) the SSL training achieve worst (yet comparable) performance to current methods. When combining SSL with supervised features, the method achieves SOTA. The method achieve state-of-the-art performance on transductive FSL when using very large network.\n\n\n### Pros\n+ Leveraging information from unlabeled data to perform classification with small amount of labeled data is definitely a problem worth exploring for many practical applications.  The idea of leveraging unsupervised/self-supervised learning for this task is interesting and have great potential.\n+ The method achieves good results when using a very large network (Resnet-101).\n\n### Cons\n- My biggest issue with this paper is with respect to novelty. The method is simply an application of MoCo on a new downstream task, namely, few-shot learning. The main contribution of the paper is then saying that state-of-the-art contrastive SSL is good for few-shot learning.\n- Because of this, it would be nice to see more exploration/experiments on this direction. For example, it would be nice to see how different SSL (either based on instance discrimination or not) perform compared to MoCo and why.\n- The proposed approach only is comparable with other FSL methods when using a much larger architecture. The comparison is, therefore, not necessarily very fair. The authors argue that their approach (which is basically MoCo) work with larger architecture while others do. More experimentation on this direction would also be necessary top really validate this claim.\n\n### Comments\nDue to the lack of novelty and unfair comparison w.r.t. different capacity of different methods (see above), my current rating for this paper is 4.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "----------------------------------\n**Summary**\n\nThis paper studies the setting of unsupervised few-shot learning, when the base class labels are not available. The paper leverages self-supervised representation learning using the MoCo-v2 algorithm. The paper found that the unsupervised representation works much better than supervised algorithms in the transductive setting, where the images of the test set is available during training, but worse than supervised in the regular setting. The paper also found that unsupervised representation is worse on cross domain transfer. The paper found that the proposed algorithm works better with deeper architecture. In general, this paper is able to push the limit of unsupervised few-shot learning by a lot without additional tricks.\n\n----------------------------------\n**Strengths**\n\n1. Unsupervised few-shot learning seems like a good setup in general, as previous supervised settings do not seem to yield interesting results with supervised pretraining.\n2. I am glad that the authors have run experiments on deeper architectures and found a good amount of improvement.\n3. I am also glad that the authors tried the transductive setting, which makes the proposed algorithm better than supervised.\n4. Overall, this seems to be a good paper to study if people want to perform unsupervised few-shot learning.\n\n----------------------------------\n**Weaknesses**\n\n1. **Few-shot learning.** My main concern with the paper is that it seems like it is another readout study of a previous self-supervised representation learning algorithm. Although the conclusion is interesting, we are basically seeing another readout study on MoCo-V2. It seems a little disappointing that there is nothing in the problem structure of few-shot learning that the authors could leverage to further improve the performance. \n2. **Self-supervised learning.** If studying the self-supervised representation is the focus, instead of improving few-shot learning, then it would be better if the authors could compare a few state-of-the-art self-supervised representation learning algorithms, e.g. SIMCLR, BYOL, etc. Also it would be good to directly compare with (Gidaris et al., 2019) since both tried self-supervised representation on few-shot learning. In the related work, the authors mentioned that (Su et al., 2020) was not able to show improvement of using self-supervised representation, but did not show why this is the case. Have the authors tried the setting of Su et al.? Is it because of any specific experimental settings? Conclusions on this topic will make this paper much more impactful.\n3. **Deeper architecture.** I appreciate the interesting results on deeper architectures, as it seems like UBC works much better on R50/101. But it doesn’t necessarily suggest that the previous methods were “methods that are limited to shallow networks,” as the paper doesn’t show R50/101 results on previously proposed tricks and architectural modification. Would wider networks also benefit UBC (e.g. SIMCLR is able to work much better on wider networks)?\n4. **Understanding of the results.** Some conclusions are not analyzed with deeper understanding. First, why is UBC much better on deeper networks? Second, why is UBC worse than supervised when dealing with non-transductive & cross domain experiments? In other words, why are supervised features generalizing better? Why is it different from the conclusion of (He et al., 2020)?\n\n----------------------------------\n**Minor comments**\n\nAside from the cited work of (Su et al., 2020) and (Gidaris et al., 2019), I would like to see some of the previously proposed self-supervised/unsupervised few-shot learning methods to be directly compared (and cited) as well (although their numbers are not as great). It would be good to run their methods in the transductive settings too. Disclaimer: I am not an author of any of the papers below.\n- CACTUS (Hsu et al., 2019)\n- AAL (Antoniou & Storkey, 2019)\n- UMTRA (Khodadadeh et al., 2019)\n\n----------------------------------\n**Conclusion**\n\nThis paper is a very interesting experimental study. Although I would like to see it published, I feel like the conclusions are not analyzed with deeper understanding, as detailed above. Specifically, I would like to understand more about comparison of different self-supervised methods and why is this paper able to make it work while prior literature was not able to, and I would also like to understand the impact of deeper architecture and the gap in cross-domain settings. Therefore, my initial rating is leaning towards rejection but I am willing to consult other opinions and see the author's feedback to make up my final score.\n\n----------------------------------\n**References**\n\n- Hsu, Kyle, Levine, Sergey, Finn, Chelsea. Unsupervised learning via meta-learning. In ICLR 2019.\n- Antoniou, Antreas & Storkey, Amos. Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation. arXiv:1902.09884, 2019\n- Gidaris, Spyros, Bursuc, Andrei, Komodakis, Nikos, Pérez, Patrick & Cord, Matthieu. Boosting few-shot visual learning with self-supervision. In CVPR 2019.\n- Khodadadeh, Siavash, Bölöni, Ladislau & Shah, Mubarak. Unsupervised Meta-Learning For Few-Shot Image Classification. In NeurIPS 2019.\n- He, Kaiming, Fan, Haoqi, Wu, Yuxin, Xie, Saining & Girshick, Ross. Momentum contrast for unsupervised visual representation learning. In CVPR 2020.\n- Su, Jong-Chyi, Maji, Subhransu & Hariharan, Bharath. When does self-supervision improve few-shot learning? In ECCV 2020.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An extensive experimental investigation with interesting findings",
            "review": "Summary\n========\nThis paper proposes to use a self-supervised objective for the purpose of learning a representation for few-shot learning tasks. They consider the standard (inductive) setting where no examples of the novel classes are available at training time, as well as a transductive setting where unlabeled examples from the novel classes can be used for training. They compare the quality of purely self-supervised-learned representations in these two settings to the commonly-used supervised baseline that learns a classifier on the base classes and uses the resulting backbone as the learned representation. They also experiment with a “combined” representation that is obtained by concatenating the supervised features with the (inductive) self-supervised features. In all cases, each downstream few-shot test task is solved by learning a logistic regression classifier on top of the chosen representation. They conduct an extensive experimental investigation using these models in both single-domain and cross-domain settings, and analyze the effect of the backbone size, the test-time shot, the dataset size among other aspects. They obtain state-of-the-art in the standard few-shot setting using their “combined” model, and state-of-the-art in the transductive setting using the purely unsupervised representation, which is quite interesting.\n\nPros\n====\n[+] Using self-supervision for representation learning for few-shot is an underexplored area, so the idea of this paper is important and timely, given the recent success of self-supervised models. Their experimental conclusions indeed support that these objectives might be useful for downstream few-shot tasks. It is particularly interesting that, for the transductive setup, state-of-the-art can be obtained without actually using any labeled examples at all. \n\n[+] Another advantage of this work is the thoroughness of the experimental investigation, on several datasets and with respect to different aspects of performance.\n\n[+] Finally, a strength of the paper is the clarity of the writing and model / experimental setup descriptions.\n\nCons\n====\n[-] A disadvantage of the work is that there is no discussion about (or experimental comparison to) the unsupervised meta-learning literature, which is very relevant to this work as it also does not use any labeled data when obtaining a representation that will be used for downstream few-shot tasks. A couple representative works of this family are [1,2]. These methods are different in that they perform episodic training, but they propose ways of designing episodes for training without knowledge of class labels.\n\n[-] Additionally, there were certain sentences in the paper that I would suggest re-writing as they otherwise might be misleading to someone who has not read the paper in its entirety. For example, the claim that self-supervision alone is enough doesn’t seem to be true in general (depending on what is considered “enough”). Although it’s enough to reach state-of-the-art in the transductive setting, that is not the case in the standard (inductive) setting, in agreement with (Su et al). So for example I found this sentence confusing: “(Su et al) [...]. However, we come to the opposite conclusion: self-supervised learning alone is enough to develop a strong inductive bias”. As far as I understand, the results here don’t contradict (Su et al), so I would phrase this as an additional finding (in a different setup), rather than a contradiction of a previous finding.\n\n[-] Further, there was a result that I found hard to interpret: in the section “A Deeper Network is Better”, the authors mention that all of their models benefit from a deeper architecture, including the FSL baseline. I’m trying to consolidate this finding with the claim (in the same paragraph) that “Closer is worse with deeper networks”. The “Closer” model is actually very similar in spirit to the FSL baseline reported here (modulo certain “bells and whistles”, using a cosine classifier instead of a standard one, and using the penultimate layer as the extracted features, instead of the logits as used in the FSL baseline here). It would be useful to comment on what is responsible for this difference in behavior between these two similar models. Under what conditions is a model able to reap the benefits of deeper architectures? Is the implication that some of the previous \"bells and whistles\" actually hurt those models in this regard? I wasn't sure what was the take-away message there.\n\nOverall\n======\nOverall, I vote for acceptance of this work due to the interesting experimental results, and the merit of the idea to experiment with self-supervision for the aim of few-shot learning. To improve this work further in terms of the writing, I would recommend more appropriately placing it within previous literature (see above) and fixing the minor writing issues brought up in the section above. I also think it would be useful to add either a paragraph or perhaps a table summarizing the pros / cons of different learned representations: e.g. self-supervised can leverage larger datasets (better than supervised which saturate earlier), but are not a good choice for transferring to new datasets, and so on. This would help to consolidate the findings of experiments across the paper. Finally, I’ve suggested a couple experiments in the next section that I think would strengthen the paper too.\n\nSuggestion for additional experiments\n=================================\nAn interesting additional investigation in my opinion would be to answer the following question: For the UBC-TFSL model, is the unlabeled data from the base classes actually useful? It would be informative to run an experiment where the self-supervised learning uses (unlabeled) data *only* from the novel classes. Assuming that that unlabeled dataset is large enough in size, perhaps these results would be better compared to using unlabeled data from both base and novel classes.\n\nA second suggestion is to run an experiment to estimate the “ceiling” performance on the novel classes. This can be done by running an “oracle” that trains a (supervised only) FSL baseline on the novel classes. This is clearly “cheating” as the evaluation tasks won’t really be “few-shot” anymore and is meant to provide an upper bound estimate (though an example split can be created so that the final evaluation is at least ran on held-out examples of those classes). Then, given that estimate we can assess to what extent we can bridge the gap between the reported FSL baseline and this upper bound by adding in various amounts of unlabeled data from the novel classes. Can we entirely close that gap using only unlabeled data? If so, how much unlabeled data from the novel classes is required for that?\n\nReferences\n=========\n[1] Unsupervised Learning via Meta-Learning, Hsu et al. ICLR 2019.\n[2] Unsupervised Meta-Learning for Few-shot Image Classification, Khodadadeh et al. NeurIPS 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}