{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes adding noise regularization, iteratively during training to word embeddings.\nThe method is evaluated on CNN-based text classification.\nOverall, there is novelty in the proposed method, however there are concerns about the experiments and analysis of the proposed approach."
    },
    "Reviews": [
        {
            "title": "Ok paper, but not good enough",
            "review": "This paper proposes to improve word embedding by iteratively 1) adding noises to the trained word embeddings, and 2) retraining the model with the noised embeddings.\n\nWhile the paper has some interesting points, I feel the paper lacks novelty because adding noises as regularization is a well-known, and the iterative incurs significant costs. Besides, I found the experiment comparison unconvincing. Thus, I give this paper the overall rating \"rejection\".\n\nComment:\n1) In the introduction, the author claims the embeddings will often stuck in local minimums during training as one motivation for this work. It would be better to have some experimental demonstration of this claim.\n2) The iterative approach requires an task-specific validation set, and consults the validation accuracy multiple times during the whole training process. This provide an unfair advantage for the proposed method, compared to the baselines which does not involves any task-specific signals.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A novel regularization for embeddings in CNN-based text classification.",
            "review": "It's already known that embeddings like word2vec and glove are biased [1] and needs postprocessing for better performance. This paper designed a novel approach to do embedding normalizations. After each round of training, noise is intentionally introduced to perturbate the finetuned parameters. Afterwards, another round of training starting from perturbated local optima could in potential converge to a better one. This method is validated via CNN-based text classification.\n\nPros:\n+ The idea to iterate the training process is interesting and the proposed approach shows reasonable consistent improvement in different domains, comparing to existing regularization methods.\n\nCons:\n- Text classification is a standard NLP task and CNN is not the only solution. The performance boost on top of RNN or transformer-based approach could make the regularization more convincing.\n- The power of word2vec and glove comes from the fact that they're trained using a large unsupervised corpus. The regularization method proposed in this paper, however, requires supervised labels on the training and validation set. The proposed approach is more stronger if the idea can be further generalized to pretraining a better set of word embeddings.\n- Retraining cost. Even though the model size of CNN is smaller than BERT so the cost on retraining seems to be reasonable, there is still a performance gap between multi-round and one-shot BERT. \n\nMinor comment: the notation is not clear in the paper. In the algorithm, one perturbation is added to W' but the other one is added to W, is this a typo?\n\n[1] Mu, Jiaqi, Suma Bhat, and Pramod Viswanath. \"All-but-the-top: Simple and effective postprocessing for word representations.\" arXiv preprint arXiv:1702.01417 (2017).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear exposition",
            "review": "\n##########################################################################\n\nSummary:\n\nThis paper presents a simple meta-learning method for iteratively fine-tuning\nword embeddings. The method works by adding noise and bias terms to the learned\nembeddings after each training session, and initializing the same model on the\nsame task with the resulting representations for further fine-tuning. Noise is\nadded to a subset of the vocabulary, while bias is added to every element of it.\nThe process is repeated until noise has been added to all the elements of the\nvocabulary.\n\nResults show that the proposed method consistently improves results in the set\nof classification tasks in which it was tested.\n\n\n##########################################################################\n\nReasons for score:\n\nHowever, the paper is not very thorough in its exposition and leaves too many\nquestions without answer. In all, it is difficult to assess the real\ncontribution of the suggested method.\n\n\n##########################################################################\n\nPros:\n\n- Method could be useful to the community working on non-contextualized word\n  representations.\n\n##########################################################################\n\nCons:\n\n- The proposed method is unclear.\n- Analyses are lacking\n\n#########################################################################\n\nComments, suggestions and questions for the authors:\n\n- p. 1, sec. 1, par. 1: I do not understand the difference between\n  \"starting with better representations\" and \"building more sophisticated\n  architectures\". For the former you cite ELMo as an example, but ELMo was both\n  a new architecture and training methodology, hence my confusion with\n  qualifying it as \"starting with better representations\". For the latter you\n  cite the Transformers, which makes more sense as a new architecture. But then,\n  how would you classify BERT, being both a new architecture sharing transformer\n  components, and a new training methodology? (This is more of a rhetorical\n  question; I suggest clarifying the points above directly in the paper.)\n\n- p. 1, sec. 1, par. 4: Did you take into consideration phenomena such as\n  catastrophic forgetting (https://arxiv.org/abs/1312.6211), when re-training?\n  Even though you are not changing the training task, I think it would be\n  valuable to at least mention this.\n\n- p. 2, sec. 2.1, par. 1: Mikolov et al. 2013b make no mention of the\n  distributional hypothesis, and even if they did, there is no implication\n  between this hypothesis and the fact that pretrained embedding vectors are\n  composed of the pairs you mention.\n\n- p. 2, sec. 2.1, par. 2: A reader not deeply familiarized with the differences\n  between contextualized word representations, and pre-trained word-vectors will\n  probably have a hard time understanding this paragraph. I suggest making it\n  clear that contextualized representations in the context of ELMo and BERT are\n  actually pre-trained models, and not only pre-trained word representations\n  such as GloVe, word2vec, and fasttext.\n\n- p. 2, sec. 2.2, par. 2: References to previous works doing \"Word Dropping\"\n  would be a welcome addition to your paper. Is this paragraph an allusion to\n  stopword removal, or something else?\n\n- p. 3, sec. 3.2, par. 2: `MaskingRate` is a hyperparameter of the noise\n  maskers, but in sec. 3.1, par. 2 it says that the bias is weighted by\n  1/[`MaskingRate`]. I suggest clarifying the differences between noises and\n  biases, and their relationship to maskers.\n\n- p. 3, sec. 3.2, par. 3: You mention you add a bias to the fine-tuned word\n  embeddings, in order to take them to the same embedding space. What do you\n  mean exactly with this? Addition does not change the dimensionality of the\n  summands, therefore adding something to W' will only shift its values and not\n  modify their embedding space.\n\n- p. 5, sec. 4.2, par. 3: How do you initialize word embeddings randomly? Is it\n  also by sampling from the uniform distribution between -1 and 1?\n\n- p. 5, sec. 5.1, par. 2: It is unclear what you mean with \"since we train a\n  model from scratch except for word embeddings.\"\n\n- p. 6, sec. 5.1, par. 2: You mention that GraVeR makes substantial improvements\n  when using relatively poor embeddings, but I can see that GraVeR has a\n  significant impact on fasttext embeddings (Table 1), which I would not call\n  poor.\n\n- p. 6, Table 2: I suggest specifying what the values between parentheses and\n  after the $\\pm$ are; standard deviation? confidence interval? if so, with\n  which confidence level?. Same suggestion for Table 3.\n\n- p. 7, sec 5.2: Why did you decide to show cherry-picked results for GloVe,\n  despite your method not working well with these pre-trained embeddings, as\n  shown in Table 1? Why not show results for fasttext embeddings instead?\n\n- p. 7, Table 4: If NoiseScale 1 means that the values from the masker were\n  sampled from Uniform(-1; 1), as explained in section 3.2, I assume that\n  NoiseScale 5 implies Uniform(-5; 5). If this is true then what does NoiseScale\n  0 imply?\n\n- p. 7, sec. 6.1, par. 1: How can you tell from Table 4 that the model\n  overfits?\n\n- p. 8, Table 5: As mentioned previously, I am confused by the choice of\n  parenthesis versus plus-minus symbols, and the meaning of the numbers they\n  refer to. Why do you use $\\pm$ for every Gen variant, but parentheses for Sp?\n  why do you use $\\pm$ for BERT (Sp), despite the previous? Also, why did you\n  decide to invert the Gen-Sp row order in the BERT entry?\n\n- I am a bit confused about the role Maskers play in your model. You mention\n  them for the first time in the caption of Figure 1, without much context. Then\n  you mention that you \"add maskers filled with random values to a portion of\n  W'\", which makes me think that maskers are nothing other than random vectors,\n  but I am not 100% sure. The paper goes on assuming that the reader has a clear\n  understanding of what maskers are, but I personally found this not to be the\n  case. Further, maskers are not mentioned in Algorithm 1, only `MaskedWords`\n  and `NextFrequentlyUsedWords`, which I guess are somehow related to maskers,\n  but again, I can't be sure exactly how just by reading the paper.\n\n- Another pain point related to the above is that you superficially mention (in\n  Algorithm 1, and the two paragraphs before it), that word frequency somehow\n  conditions which words are masked, but no details are given as to how this\n  happens.\n\n- I found the tables to be a bit overcrowded and difficult to parse in general.\n  I suggest trying to simplify them to better get your point across.\n\n- I think the Table 1 mentioned in the Appendix should be Table 6.\n\n\n#########################################################################\n\nSome typos:\n\n- Abstract: \"some of noises\" -> \"some noises\"\n- Abstract: What do you mean with the last sentence?\n- p. 1, sec. 1, par. 2: \"the general and meaning of words\" -> \"the general\n  meaning\"\n- p. 1, sec. 1, par. 4: \"to the next re-training process\" -> \"in the next\n  re-training process\"\n- p. 1, sec. 1, par. 4: What do you mean with \"the model must be overfitted\"?\n- p. 2, sec. 2.2, par. 3: \"It can regularize the models,[...]re-usability.\" is\n  not a grammatically correct sentence.\n- p. 2, sec. 2.3, par. 2: In \"Also, dropout discourses the weights\" I think you\n  meant to write something other than \"discourses\".\n- p. 3, sec. 3.1, par. 2: \"best-performed\" -> \"best-performing\"\n- p. 4, sec. 3.2, par. 4: I think the sentence \"Although this approach cannot\n  warrant moderate noise, we take a safe approach.\" is not clear enough.\n- p. 4, sec. 4.1, par. 1: \"in Appendix\" -> \"in the Appendix\"\n- p. 5, sec. 4.2, par. 1: \"with the 32 channels\" -> \"with 32 channels\"\n- p. 5, sec. 4.2, par. 1: \"max-pooled\" -> \"max-pooling\"\n- p. 5, sec. 4.2, par. 2: \"small parameters\" -> \"few parameters\"\n- p. 5, Table 1 caption: \"vector\" -> \"vectors\"",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}