{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper provides a novel method for calibrating probabilistic regression models without requiring a held-out calibration set. The technical advances are interesting, and the experimental results look promising. The authors made a number of improvements based on the reviews, and the authors have done a good job with reproducibility of the experiments. Nevertheless, it ultimately does not meet the bar for acceptance. In revising the article for a future submission, I would encourage the authors to emphasize the observation in Section 4.3 that overall we want models that are both \"well-calibrated and sharp\". This helps motivate the method and will help the reader interpret the results. "
    },
    "Reviews": [
        {
            "title": "Good basic idea, well structured paper, some weaknesses and unclear points",
            "review": "This paper presents a novel method to produce probabilistic models whose predictive uncertainty is quantile-calibrated, without requiring the use of a calibration dataset separate from the training dataset, and with sometimes improved calibration error. \n\nThere are three major technical ideas in the paper: \n- sec4.0 interpreting the definition of quantile calibration as an equality between two CDF, of which one is of the uniform distribution\n- sec4.1 estimating differential entropy using sample spacing\n- sec4.2 overcoming the need for ordering in the recalibration set with NeuralSort\n\nThese ideas support the entire algorithm and have merit. The approach is novel, improves over the baseline defined by Kuleshov 2018. The advantage of not requiring a calibration set is definitively important, especially in the case of small datasets of course. This lends the paper applicability. I can figure this method making its way into mainstream libraries such as scikit-learn.\n\nThe experiments are sound. However, the table reporting experimental results is unconvincing due to inaccuracies: bold (which is not defined in the caption, hopefully not to escape contradiction) seems to be used when the average result is better, *irrespective of standard deviation/error bars*, contrary to standard practice, where a method is not said to be \"better\" when it does not beat the baseline with non-overlapping mean +/- one standard deviation intervals. The results on max likelihood are insufficient to support the claim made before claim 2 (on a purported weakness of isotonic regression), because eg in the case of table 1, only half of the UCI datasets exhibit a base+iso max likelihood which is definitely larger than the QR+iso case.\n\nThe exposition has a good structure, especially sec3 is useful in pointing out weaknesses of isotonic calibration. Sec4 relatively clearly exposes the main ideas behind the approach. Sec5, the experiment description, should be improved. At a finer level of detail, the text is not well edited, and is probably hard to access for somebody not familiar with the matter; despite knowing the background literature well, I found myself having to guess and resolve ambiguities too often to call this a \"clear exposition\".\n\n# Major errors, suggestions\n- sec2+2.2: define X in the introduction of sec2: why can we order x's in sec2.2 all of a sudden? I didn't expect there to necessarily be a total order relation on X. Does the entire paper only apply when X can be ordered?\n- claim 3: what is F when indexed with $\\mu, \\sigma$ ? is it the Gaussian CDF, errf ? is it the same as $\\Phi$ in Algo 1 (undefined there as well)?\n- eq between eq5 and eq6 : this is probably wrong, should be m/(n+1) (also cf Learned-Miller 2003 eq 4)\n- eq6 should reference Learned-Miller 2003 eq 6 (the paper is long, you should reference the exact place to allow the reader to check)\n- sec4.2 I disagree with your argument for using the sample spacing entropy estimator: \"one has to use beta kernel\": you seem to imply that there are only these two alternatives, and that the latter is unfeasible; you cannot make such a strong claim without proving that there are no other methods.\n- Algo 1 line 4 what is $\\Phi$ ? ; line 8 where does this come from (mentioned nowhere else)? refer to Learned-Miller 2003 ?\n- text below Algo 1: so is CL the same as $\\hat{H}$ in eq6? why use a new notation?\n- table 1: the second column is repeated between top and bottom rows, with one error: the second ocurrence of (kin8nm, QR) is wrong (maybe incorrectly copied while editing the table, is equal to (kin8nm, QR+iso)) -- are the rest of the results reliable?\n- table 1: how is max likelihood obtained?\n- table 1 and 2: The text does not explain what the case without $\\lambda$ stands for -- this seems important to analyse the results however.\n- sec5.3 The meaning of \"monotonicity constraint\" is unclear -- what is the argument?\n- sec5.3 last sentence: Why is this a reason for a reduction in RMSE, NLL ? This is unclear.\n\n# Minor errors, typos and language issues\n\nThe text numerous syntax errors and typos. In a few instances, they obscure the meaning of a few sentences, which is not acceptable. \n\n- \"confidence interval\" is used inaccurately throughout the text\n- sec1 \"three main notions of calibration\", but I see only 1) post hoc and 2) implicit methods\n- sec3: basic interpolation: you mean linear interpolation?\n- claim 2: what is f ?\n- fig 1 is useful, but definitely needs labels on all axes.\n- table 1 and 2: why duplicate two columns between top and bottom rows ?\n- table 1, time: seconds?\n- usage used\n- after it has shown that\n- intervals improves\n- assigning zero likelihood: to what ?\n- wide range of architectures: duplicated\n- call...classification *as* canonical\n- 2.0 last sentence: rewrite\n- O(m),.\n- set *as*\n- per say -> per se ??\n- in many of cases\n- after claim 3: remedy is to use: do you mean \"is to add\" ?\n- the model this r.v. \n- fig 1: (A) instead of (a)\n- which is model is quantile-calibrated\n- is general that it does not\n- sec5.3: \"but again it is negligible\": why \"again\" ?\n- reference Gal 2016 is incomplete",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The authors consider the problem of learning quantile calibrated regressions model. A probabilistic regression model is a model that, given an input, outputs a distribution over possible scalar values. A quantile calibrated model is one is such that, for all quantiles $p$, the probability over $X, Y$ that the model predicts $Y$ is in the $p$th quantile is equal to $p$. Machine learning models are not usually calibrated after standard training, so the authors consider a regularization approach to improve the calibration of the model during training.\n\nThe authors first discuss an existing approach from (Kuleshov et al., 2018) where we first learn a calibration function $R(p)$ that describes the probability over $X, Y$ that the model on $X$ assigns a quantile $p$ to $Y$. If we had this function, then we can compose it with the model to get accurate quantiles. Kuleshov et al. uses isotonic regression to fit the function $R$.\n\nThe authors improve using isotonic regression for calibration when the probabilistic regression model outputs Gaussian distributions. Specifically they show that the mean calculation of the calibrated model can be done in constant time, rather than in O(calibration set) size.\n\nThey then argue that isotonic regression has some disadvantages, specifically (1) it requires an additional step of training on a hold out dataset and (2) it creates PDF distributions that can be spiky and noisy.\n\nThey then note that if the calibration function $R$ is uniform then the probabilistic regression function is already calibrated. They then directly regularize the probabilistic regression function with a KL-divergence term between $R$ and a uniform distribution. They estimate KL-divergence by using a differentiable entropy estimator. This avoids the two-stage calibration function and allows us to directly use the learned probabilistic regression function, which may have a more convenient form than the compose regression function.\n\nThey experimentally show using UCI datasets and small neural networks that their approach leads to a more calibrated model, potentially at the expense of accuracy. \n\nOverall I found the paper interesting and the main idea principled with decent experimental results. I would have liked to see experiments on larger scale networks and more challenging datasets, as they may behave differently. Additionally I would have liked to see how accuracy and calibration trade off based on the penalty parameter.\n\nCan you explain why adding isotropic regression in the experiments doesn't always improve the calibration over base?\n\nAlso, could you use a nearest neighbor entropy estimator rather than a sorting-based one (see, e.g., Gai et al., 2016, https://arxiv.org/pdf/1604.03006.pdf)? This is directly differentiable and would avoid needing a relaxation to sorting.\n\nThe paper could be improved for clarity. There are many seperate parts to the paper (results improving Isotonic calibration, issues with Isotonic calibration, KL-regularization, degenerate calibrated models). I think the paper can be improved by making these sections more self-contained and by removing results that are not the main focus of the paper (such as degenerate calibrated models).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good paper suggesting a novel implicit regression calibration mechanism but can be improved with better presentation of the material and adding more detailed experimental results",
            "review": "Summary: The paper discusses a new calibration mechanism for regression models which produce better model prediction and uncertainty estimates. In Section 3, the paper first discusses some properties and drawbacks of the approach based on isotonic regression in Kuleshov et al., 2018 which uses a post-hoc calibration dataset after model fitting for calibration. Section 4 discusses a new approach based on regularization to achieve implicit regression calibration during model training instead of using a post-hoc processing approach. Section 5 is devoted to experimental results supporting the theory outlined in Sections 3 and 4.\n\nReview: I think the approach to achieve implicit regression calibration by adding a suitable regularization to the loss function objective is novel and a good contribution. Also the experimental results can be reproduced as the code has been included as part of the submission.\n\nBut I do have certain concerns on the results in Section 3 and the overall presentation. I am a bit confused by the discussion in Section 3. The proof of claims 1 and 2 seems to define the c.d.f. $F$ based on the assumption that $P(y|\\boldsymbol{x})$ is homoscedastic, while the discussion at the beginning of Section 3 assumes a heteroscedastic model with different $\\sigma_k$’s for different datapoints in the calibration dataset. I think the proofs can still hold with a slight modification by assuming the c.d.f. $F$ is for a standard Gaussian distribution by suitably accounting for different $\\mu_k$’s and $\\sigma_k$’s but, in general, I think the proofs and results can benefit from precise definitions and assumptions of the various quantities like the c.d.f. $F$, p.d.f. $f$ and how they depend on data $\\boldsymbol{x},y$.\n\nAs for the overall presentation, I think the paper can benefit from adding concise introductions to differential entropy estimation and the NeuralSort method from Grover et al., 2019 which are important concepts in the implicit calibration method discussed in Section 4. The material in section 4 seems to one of the major contributions of the paper and as such I think more space should be devoted to it. The optimization of loss function equation 7 is not discussed as also the selection and sensitivity of the hyperparameter $\\lambda$ in the loss function. Also it will be easier if the various mathematical symbols are cleanly defined before use in the main paper. For example, the symbol $F$ seems to be have different definitions in Section 3 and Section 4?\n\nIn the experimental section, I am missing the description of the QR + iso model. Also I will like to see more discussions on the increase in RMSE in Tables 1 and 2 for the QR model which is attributed to the loss of sharpness in the degenerate case outlined in Section 4.3 in the paper. For example, the RMSE seems to be impacted more for the Dropout VI model compared to the deep ensemble model. What factors impact the decrease in sharpness and how sensitive is the decrease of sharpness to the choice of hyperparameter $\\lambda$? \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Missing link between QR and side effects of ISO-regression",
            "review": "The manuscript discusses the side-effects (or drawbacks) of Isotonic regression and proposes an alternative approach for calibration in regression problems. The authors demonstrate the limitiation of Isotonoc regression such as nonsmooth PDFs and truncation of support under some constructions of the calibration dataset (line 2 in page 4) on a simple linear regression problem (last paragraph before section 4). On the light of these observations, they propose quantile regression (QR) which does not require an additional dataset.\n\nThe discussions on the shortcomings of isotonoc regression are valuable. However, it is not clear why the proposed approach QR should address these limitations. The main motivation of QR is based on pushing KL between the predicted cumulative density of the target value M(X)(Y) and the uniform distribution to 0. First, KL is not a symmetric metric so the authors should clarify why they use KL(M(X)(Y) || U) instead of KL(U || M(X)(Y)). Second, uniform distribution has a bounded support and Gaussian distribution, which is the main focus of the manuscript, has unbounded support. As a result, KL(M(X)(Y) || U) can suffer from division by 0. Third, I would like to see if the limitations of Isotonic Calibration discussed in Figure 1 are resolved by this approach.\n\nMy other concerns/questions about the manuscript are below:\n\n- I do not follow how truncation points in Fig 1b-c were computed. More discussion in text on these figures would be helpful.\n- In the first paragraph of section 2, the authors mention that they only consider Gaussian likelihood but in conclusion they claim that they propose a black-box calibration loss function. This confusion should be clarified. Indeed, Gaussian model was used in most of the claims so my question is how the claims in section 3 would be like if the true data distribution is not Gaussian.\n- I do not follow the relevance of the statement \"This way the time required is reduced from O(m) to O(1)\" following Claim 3 on page 4 to the main approach.\n- Is expectation in equation in section 4.1 over i ?\n- Why was m in Algorithm 1 set to square root of n?\n- I am surprised that the datasets picked in Table 1 have no overlap with the ones in Kuleshov et al., 2018.\n- In discussion section, the authors mention that Isotonic Regression suffers in case of smaller calibration datasets. Kuleshov et al., 2018 state that K-fold cross-validation as an alternative approach to using a separate calibration set. I wonder if such approach would be helpful for small datasets and such addition to discussion would be helpful.\n- Lemma 1 in supplementary material was not discussed in main text.\n\n===========================================================================\nUpdate after Authors' response:\n\n- I thank authors on their detailed response and clarifications. I increased my score based on their response.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}