{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper tackles the problem of long term time series forecasting. One challenge in long term forecasting is that often no sufficient date may be available. This paper proposes to use GANs to generate data that can be used to improve long-range forecasts.\n\nWhile reviewers agree that this paper presents an interesting idea towards tackling the problem of long term time series forecasting and appreciate the effort authors put forward in addressing their concerns and comments during the response period, the reviewers believe that in the current form paper is not ready for the publication. In particular:\n\n1. Reviewers find that overall technical contribution of this work is limited compared to other submissions.\n2. Comparison of LSTM and cWGAN-GEP in the response is only on synthetic data, which does not address reviewers concerns.\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "Novel generative TS prediction, extensive experiments, moderate win",
            "review": "Summary:  This paper proposes a technique for long range time-series forecasting that leverages a generative model to extend the existing time series to train a time-series model to make the final prediction at horizon N.  The authors design a GAN, exploring an enhanced loss function, and provide a methodology by which to train it.  They evaluate the combined approach on a number of different time series, while comparing the technique to both traditional forecasting approaches as well as more recent RNN/CNN-based methods.  On those datasets, the technique meets or exceeds performance measured by MSE, in some cases improving MSE by 10% or more.  \n\n=== pros ===\n\n+ Long range forecasts are indeed important, and the work appears to be a novel amalgam of recursive and direct forecasting with synthetic data generated from a GAN.  The paper is well organized and citations are extensive.\n\n+ The evaluation compares against a range of traditional and current forecasting techniques, and for those experiments illustrates performance that almost always exceeds that of other approaches. \n\n+ The ablation study indicates that the modifications to the traditional GAN architecture (extended loss function) and the use of mutual information to best train the GAN and predictive model provide benefits.   \n\n=== cons ===\n\n- The paper chooses to use MSE to compare models.  This metric has some drawbacks; it is not at the scale of the data and it amplifies changes in error.   The paper claims 10% improvements in some time series, but this is 10% WRT MSE.  MSE inflates the perceived benefit.   Using MAE or RMSE would be more appropriate, though time series challenges even eschew those metrics in favor of symmetric MAPE (sMAPE) and MASE (see M4).  This raises the possibility that the concluding statement of \"significantly outperform(ing)\" other techniques may be a bit exaggerated. \n\n- The evaluation investigates hourly and yearly time series.  It would be good to at least include a daily time series, and try to use a legitimately long-range forecasting horizon, like 30, 60, or 90 days.  \n\n- It's great to have the ablation study, but it is essentially anecdotal, as it only explores the performance for a single time series. \n\n- Model comparisons are typically performed by time series cross validation (or in the case of hyperparameter searches, nested TS CV).   This would help to remove the possiblity of leakage when training on time series that might have correlations across time across sites (such as energy consumption or air quality).  \n\n=== suggestions / questions === \n\nI enjoyed this paper and its topic.  Multi-step ahead forecasts always force the practitioner to choose between direct, recursive, or seq2seq modeling approaches.   Evaluating a new technique that combines the two is a useful exploration.   \n\nIt would be good to quantify what is meant by \"long-range\" forecasting.   Is it in steps or time?   As noted above, sometimes you have to forecast perhaps hundreds of steps into the future.  I'd probably call the TS forecasts here \"medium-range\", though this does not invalidate your results. \n\nThe motivation for ITC could be fleshed out -- it's a core contribution.   It would be good to motivate why a uniform random sampling of TS at the unit level does not lead to the best training data sets.   ITC ranks each unit time series by the sum of its MI score with all other units, and then creates Z same sized groups with similar scores.  Might explicitly say the sampling was proportional to J.   S3.4 claims that units with the same J scores tend to be dependent on each other.  What about the group with the lowest scores? \n\nIt is not clear whether you tune the hyperparameters of each direct forecasting model for each horizon.   \n\nBecause you balance direct and recursive forecasting, a discussion comparing to RECTIFY (another approach that combines the two that you cite) might be useful in the related work section.  In addition, it would be good to mention that the Yoon 2019 NeurIPS paper trained predictive models on the generated time series; it might not have been \"long range\" but it's an important aspect of that work. \n\nThere are so many variations on your approach -- it would be great to say why you didn't evaluate them.  They include using the generative model for predictions (directly and recursively), or using an interative predictor instead of a direct predictor.  Similarly, why just generate 3 steps?  \n\nS3.4 You mention that ARIMA models multi-step prediction performance is equivalent between direct and recursive.  A direct N-step ahead model would have to use lags at least N days in the past.  Other work has found recursive outperforming direct (https://arxiv.org/pdf/1108.3259.pdf).   In our own work we have found that, for AR models, it depends on the time series itself.  \n\n==== nits ==== \n\nPlease learn the difference between that and which.   Remove \"in order to.\"   \n\nAbstract \"for THE next few time steps\"\n\nS3.2P1 \"improve classification results\"\nS3.2P2 \"to generate synthetic\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper presents a framework for long-range forecasting of time series data using synthetic data. The whole pipeline contains three main components: cWGAN-GEP to predict/generate the following data based on observed ones; an LSTM predictor to forecast results based on real/generated data; and ITC to cluster data points by mutual information which forms training data for the first two components according to clustering results. The proposed framework is evaluated on three public datasets and results seem promising.\n\nPros:\n\n1. This paper is well-structured and easy-to-follow.\n2. The idea of utilizing synthetic data for improving the long-range forecasting of time series data is interesting.\n3. The proposed method is widely evaluated on three datasets and the results are promising.\n\nCons:\n\nMy main concern towards this paper is that it lacks novelty on the technical side.\n\n1. In cWGAN-GEP, adding MSE loss (mean squared error) for data generation (or augmentation) is a quite standard principle in the literature. It is hard to say this is novel.\n2. Using LSTM for long-range data prediction is also widely used given LSTNet and TLSTM.\n3. I think the main technical contribution of work is the Information Theoretic Clustering (ITC) algorithm.\n\nTherefore, the contributions of this work are over-claimed by the authors in the introduction part and its technical contribution is minor. Overall, I think the current version of this paper cannot meet the bar of ICLR.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\nThis paper proposes cWGAN-GEP for long-range forecasting of time-series data. cWGAN-GEP is a combination of data generation of data prediction, where given some observations, GAN iteratively generates a short synthetic time-series data, and an LSTM subsequently makes a long-range prediction based on the generated synthetic data. In order to train both components, the authors use information theoretic clustering. The proposed model outperforms various baselines on the prediction task using three time-series datasets.\n\nPros:\n- The paper reads very well. The entire paper seems to be carefully written with as much details included as possible. Good presentation.\n- The proposed method outperforms various baselines for time-series prediction using three different datasets.\n\nCons:\n- It is not convincing to use a complicated GAN architecture for iterative data synthesis, when a simple LSTM could already do a decent job (as shown by figure 4). The authors should expand their ablation study by replacing the GAN module with an autoregressive LSTM/Transformer (with reasonably tuned hyperparameters) to evaluate the necessity of the complicated cWGAN-GEP. \n- It is not well motivated why a two-stage approach (data synthesis first, and prediction later) is specifically helpful for long-range forecasting. The authors could have easily used an ensemble approach by using the two components to make predictions for all future time-steps and mixing (averaging or gating, etc) the predicted outcomes, which would be way easier since this will obviate the necessity of ITC. The authors should test this approach to justify the necessity of the two-stage approach.\n- The claim that the proposed hybrid approach is helpful for long-range forecasting is not strongly justified. It seems that the performance gap between LSTM-based baseline models and the proposed methods stay rather unchanged as the prediction horizon widens. Maybe the proposed architecture is just generally a more powerful time-series predictor with many working parts.\n\nAfter the author response:\nAlthough the authors conducted additional experiments, they have only partially addressed the first comment regarding the use of LSTM instead of cWGAN-GEP by showing LSTM's comparison against cWGAN-GEP in synthetic data generation, when the question was to measure the forecasting performance, not synthetic data generation performance. Therefore increasing the score to 5.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}