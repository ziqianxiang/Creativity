{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting way of sharing parameters, but the method seems a bit too complex and with many hyper-parameters for real use",
            "review": "This paper introduces a new factorization of a convolutinal neural network (CNN) learning parameters. A convolutional layer is decomposed in a 2D dictionary of spatial-only filters plus a channel recombination layer. Dictionaries among different layers are shared among the entire network or among blocks. This acts like a regularization that reduces the number of CNN parameters and helps reduce overfitting.\nExperiments on CIFAR10, CIFAR100, TinyImageNet with different network configurations show that the propose method is quite competitive in terms of parameter reduction and performance.\n\nQuality & Clarity:\nThe presentation of the approach can be improved. For instance, in the introduction it seems that canonical correlation analysis is used in the approach, while after reading the rest, I understood that it was used just for motivation. Also illustrations could be improved. For instance Fig. 4 is too small.\n\nOriginality & Significance:\nSharing weights in CNN is not new. Authors should consider methods based on weights clustering such as (\"Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions\", 2018) and (\"Clustering Convolutional Kernels to CompressDeep Neural Networks\", ECCV18). \n\nPros:\n- The way of sharing weights seems new and leads to good results.\n- Fig. 5 shows that for few-shot learning reducing the number of parameters is important to obtain better performance on larger networks.\n\nCons:\n-  The way of sharing weights introduces additional hyper-parameters that should be defined and makes the method more difficult to be used in practice.\n- Instead of presenting results with all the different configurations of the method, it would have been easier to select the best setting form table 1 and then compare with the state of the art on only that setting.\n- With many different configurations it is difficult to fully understand which trade-off between accuracy and compression would be achieved\n- Table 4 shows the computational cost of different configurations, but without the corresponding accuracy it is not very useful for understanding the real capability of the method to reduce the computational cost of a model. Also, it is not clear if the number of flops in the table would translate in a real speed-up.\n\nAdditional comments:\n- The decomposition in 2D convolutions seems to me similar to depth-wise convolution (Chollet et al. 2017). Authors should at least acknowledge those similarities in related work. \n- The use of symbols is a bit confusing: in Fig.1 \"x\" is used for DA matrix multiplication, while in fig. 2 the matrix multiplication is represented with another symbol and in the equations as normally it is represented without any symbol.\n- The reduction of overfitting can produce also a model that is more robust to noise. Could the authors evaluate that?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reviews",
            "review": "Summary:\n  This paper proposed to use shared-weights crossing multiple layers to learn compact CNN models. This idea is similar to the structural learning for a CNN. The results show that the proposed achieves very high parameter efficiency.\n\n\nStrong points:\n\n  1. This paper proposed to learn a compact and shared weights to increase the parameter efficiency while achieving good results.\n  2. This paper cooperates with dropout idea to regularize the network and achieves better results.\n\n\nWeak points:\n\n  1. The novelty from ACDC-group is limited due to the group convolution has shown the parameter efficiency in the prior art, e.g. ResNeXt.\n  2. The comparison is somehow unclear and few heursitc baselines should be considered.\n  3. Some important references are missed. \n    - \"Learning Filter Basis for Convolutional Neural Network Compression\" ICCV 2019\n    - \"Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression\", CVPR 2020\n    - \"Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks\", NeurIPS 2019\n    - \"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\", NeurIPS 2019\n\n\nQuestions:\n\n  1. How does the author handle a network with different kernel sizes? like in resnet has a 7x7 in the beginning and then 1x1 and 3x3 layers, under -net settings, do all layers share the same dictionary? \n\n  2. The author introduces drop-atom as a regularizer to further improve performance. From Fig. 6, it provides about 0.5% gain on cifar10. Deop-atom is like droppath/stochastic depth for a normal CNN. I think it will be fairer to apply similar techniques to the baseline for a more fair comparison.\n\n  3. In Table 1, comparing to regular convolution is straightforward but since there are many known techniques to increase parameter utilization, like  (1) point-wise convolution + depthwise convolution (2) grouped convolution + channel shuffle. Those should be the baseline to compare.\n\n  4. In Table 2, why the authors' method is all based on WRN? to more fair compare to others, using the same/similar backbone will be more clear to see the advantage of the proposed approach as authors claim the ACDC is plug and play.\n\n  5. How does this approach saving runtime? Is it proportional to the FLOPs reduction? I expected the speed will be slower because there are additional steps to construct kernels from atoms. but is this overhead significant?\n\n\nOthers:\n\n  1. The author mentioned the results of cifar10/cifar100 are the mean of five runs, in such cases, it will be better to show the variance of those runs as well. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "ConvNet compression",
            "review": "The paper explores a form of model compression that reduces the number of parameters and FLOPs for VGG/ResNet type models.\nIn isolation, the results look promising but there is a lack of comparison to related work.\nSome relevant points of comparison would be:\n1) Models using 'hand-designed' group or depthwise convolutions such as \n a) ResNext, 2016\n b) MobileNet and EfficientNet, ICML 2019\n2) Model compression methods like \n a) ConDenseNet, CVPR 2018\n b) Stable Low-rank Tensor Decomposition for Compression of Convolutional Neural Network ECCV 2020\n c) And the Bit Goes Down: Revisiting the Quantization of Neural Networks, ICLR 2020\n d) TCNN: Towards Convolutional Neural Networks Compression via Global&Progressive Product Quantization, BMVC 2020\n\nComparing in terms of the number of parameters:\n**Here** 5.77M parameters 72.22% top1, versus EfficientNet B0 5.3M parameters 77.1% top-1 accuracy\n**Here** 16.37M parameters 75.96% top1, versus EfficientNet B3 12M parameters 81.6% top-1 accuracy\nTable 3 does not give #FLOPs, but based on Table 4 I would guess the method uses five times more FLOPs than comparable EfficientNets.\n\nCompared to other methods using compression to minimize model size: \n**Here** 72.22% top1 with 5.77M 32bit parameters vs TCNN: 77.55% top1 with 5.09MB of storage\n\nLastly, the name is a bit confusing. Is there suppposed to be any relationship to:\n'ACDC: A Structured Efficient Linear Layer, https://arxiv.org/abs/1511.05946'\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Missing critical references and lacking necessary FLOPs comparisons",
            "review": "The authors propose to decompose the standard convolutional layer into a 2D dictionary and the corresponding linear coefficients. By sharing the decomposed coefficients, the authors argue that cross-layer correlations can reside in such low rank formulation. The coefficient sharing also leads to model size reductions. The paper investigates the flexibility and compatibility of the proposed ACDC weight sharing mechanism by image classification and few-shot classification tasks.\n\nThe main weakness of the submission might be that there have been many studies on low-rank representation of the convolutional operation. However, the authors did not reference and discuss the fundamental related work [R1] that is one of the first few early works on exploring low-rank representation of convolution layers. Besides, the authors cited recent work (Kornblith et al., Morcos et al., and Raghu et al.) in the submission, but did not discuss the differences between the proposed atom-coefficient decomposition and other mentioned works, either in fundamental formulation aspect or practical performance aspect. The paper can be largely improved if the fundamental difference and performance gap (accuracy and efficiency) were shown.\n\nWhile the paper shows clear model size reduction, in practice FLOPs might be equally if not more important when deploying edge friendly models. Models with less number of parameters do not necessarily translate to higher efficiency. It largely depends on how those parameters are wired as a whole. In the main experimental results (Table 2 and Table 3), the authors did not present FLOPs reduction. In table 4, the authors only show FLOPs comparisons with VGG16 and ResNet18, which are few years old and are known to be highly redundant networks. I suggest the authors clearly demonstrate the FLOPs reduction in Table 2 and Table 3 to strengthen the submission.\n\nAgain in the main experimental results (Table 2 and Table 3), the competing methods are a few years old such as SqueezeNet, ShuffleNet, MobileNet-V2, etc. It is important to compare the proposed weight sharing technique with more recent baselines such as MobileNet-V3 [R2] or EfficientNet [R3]. Although models found by architecture search algorithms can be orthogonal to manually designed decomposition approaches, it is important to understand the ballpark comparison with recent best performing efficient network architectures. \n\n[R1] Jaderberg et al. Speeding up Convolutional Neural Networks with Low Rank Expansions. In ECCV 2014.\n[R2] Howard et al. Searching for MobileNetV3. In ICCV 2019.\n[R3] Tan et al. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In ICML 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}