{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The covered topic is timely and of potential impact for many application domains, such as drug design. The paper is well written and presentation is clear. The proposed approach seems to have some degree of originality. Experimental results seem to be generally good, and in the rebuttal the authors have provided further experimental support to their main claim.  There are however some issues that have not been solved by the author’s rebuttal. I think two of them are the most important and related: \n\ni) significance of contribution: although the authors have tried in the rebuttal to explain how the proposed approach differs from related papers, it seems that there are still doubts about the amount of innovation introduced by the paper. This issue could have been mitigated by SOTA experimental results in presence of a proper model selection, that, however does not seem to be the case here (see next point);\n\nii) model selection: the authors did not clearly explain the model selection procedure in the rebuttal. This is an important issue since it is often easy to get good results by picking the best run a posteriori. Unfortunately in the literature there are highly cited papers where model selection is not performed in a proper way and reviewers very often reject papers just looking at numbers without looking at how the numbers were obtained. So, I believe it is important to accept only papers where model selection is properly done and properly explained, so to allow for reproducibility of experiments. "
    },
    "Reviews": [
        {
            "title": "Incremental work",
            "review": "This work considers the graph deconvolution networks. It proposed a graph deconvolutional networks to reconstruct the graph signals. The concerns for this work are as below:\n\n1.\tThe contributions in this work are incremental. It seems most parts are based on the previous works. The deconvolution part follows the scheme defined in (Bianchi et al. 2020). The overall graph autoencoder also follows a very common settings by involving pooling and unpooling operations. In pooling part, two pooling methods are employed that are hard pooling and soft pooling. However, it is not clear to me how to combine these two methods since they are very different. The equation (3) shows the combination of these two methods but fails to make sense to me. I would like the authors to clarify this and provide detailed explanations for this part.\n2.\tIn both Section 4.1 inverse of GCN and Section 4.2 wavelet de-noising, the authors talked about two methods but didn’t explain why they are related to the proposed graph deconvolution networks. Also, most of the content in these two parts are following previous works like (Li et al. 2019b) and (Xu et al. 2019). I would recommend the authors to clearly point out what is the contribution and how these are related to the proposed methods.\n3.\tThe visualization section in 4.3 is confusion. Firstly, why the authors believe this is a contribution to the graph deconvolutional networks. To my understanding, there is no technical contribution for this paper. It would be better to show this in the experimental parts. Secondly, there is no description on the settings for the illustrations. I didn’t understand how the image are converted into graph and how the inverse GCN and the proposed models are used to them. Thus, the authors need to clarify this and point out the technical contributions for this part.\n4.\tThe experimental results are very limited. The four datasets used in table 1 are very small in terms of the number of graphs, which cannot provide comprehensive evaluations to the proposed results. The baseline methods are not well established. There are far better results on these datasets such as GIN. The authors claim that the unsupervised settings are used. However, it is weird to me why the unsupervised settings do not help the  overall model performances.\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks.",
            "review": "Graph Autoencoders with Deconvolutional Networks\n\nThe paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks.\n\nThe proposed deconvolution incorporates a denoising component based on graph wavelet transforms.\n\nPros:\n-The idea of defining graph deconvolution operators is appealing and and may potentially lead to improvement in the performance of graph reconstruction/generation tasks.\n-The Visualization section of the paper shows an advantage of the proposed approach compared to other methods for reconstruction. It maintains information about high-frequency signal.\n\nCons:\n-The potential of a graph deconvolution operator are not fully exploited in the paper, mainly because of the considered tasks that do not require deconvolution because they are not intrinsically reconstruction tasks.\nThe paper applies the proposed approach to tasks of graph classification in Table 1 and social recommendation (matrix completion) in Table 2. While the comparison with other unsupervised learnimg methods looks favourable to the proposed approach, supervised learning methods are naturally more suited for the tasks in Table 1 and tend to perform slightly better (a comparison with supervised approaches would be appreciated).\nGraph Autoencoders are usually applied to tasks of graph generation such as molecule design, where they are one of the most suited approaches. Many works in literature face this problem. A comparison on the generation task would be interesting.\n-Considering the Ablation results, the improvement with respect to the ablation approaches seems marginal. Again, my opinion is that the considered tasks are not well suited for the proposed model.\n-Hyper-parameter selection: In Appendix A the hyper-parameter selection procedure is not sufficiently detailed. How do you choose the hyper-parameter values? You report the considered ranges but not the procedure you adopt to select them. \"parameters of downstream classifiers\", in my understanding it refers to the C parameter, or to other hyper-parameters as well?\n\nMinor:\nALATION-GCN -> ABLATION\n\n---Rebuttal--\nI acknowledge having checked the authors' response and the revised version of the manuscript, that has been improved since the first revision. Authors did not answer to my request for more details about the hyper parameter selection procedure that has been adopted. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The presentation of the paper is clear for the method part. Need clarification for the experiments.",
            "review": "The main contribution of this paper is that the authors design a graph deconvolutional network that combines inverse filters in the spectral domain and de-noising layers in the wavelet domain. Further graph autoencoders are proposed based on the graph convolutional networks and the graph deconvolutional networks. Many experiments are done and many previous methods are compared to show the effectiveness of the proposed networks.\n\nMy reason for the rating:\nBefore the experiments part, I think authors clearly present the methods and related works. The results also look quite good compared to other methods. However, I have some questions for the experimental setups that they present at the end of the paper, which somehow confuses me.\n\nPros:\n1. In general, this paper is well-written and the presentation is clear and easy to understand for the method parts.\n2. The results well demonstrate the proposed network as reported. \n\nCons:\nI have some comments and concerns for this paper before I made my final decision:\n1. It would be great to give a reference to the Maclaurin series approximation for the function of a matrix near (8), or give a simple derivation. Please mention that $n$ starts from 0.\n2. For the running time part, it would be great to quantitatively show the time, when choosing a baseline. $10$ and $15$  seem approximation numbers to the reviewer.\n3. I recommend the authors explicitly state the previous methods that you have the same setups in the experiments.\n4. I find that the authors use $\\lambda_{A} = 0$ in the experiments, which means that in (6), the loss function contains only the second term. Could you please clarify this setting?\n5. The authors mention that they use best average classification accuracy for all the methods. Could you clarify this part?\n\n---------------------------\nAfter rebuttal\n\nThank the authors' response to my comments. They also provided new results (i.e., graph generation tasks) to address the Q.4 in my comments. I was about to lower my score when the first reply came as graph generation tasks are just future works in the original paper, but I changed my mind with these results. Although other reviewers may still question the novelty and contributions, I think I would stay with my score according to the comparison and good results reported in the paper. Looking forward to seeing the code someday.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed GDN to learn high quality unsupervised graph embedding.",
            "review": "Summary:\n\nThe authors proposed graph deconvolution layers (GDNs) and employ GDNs to learn graph embedding in a encoder-decoder framework. The authors performs inverse signal recovery in spectral domain and then conducts a de-noising step in wavelet domain to remove the amplified noise. The proposed method can outperform baseline methods on graph classification and social recommendation. However, there are some issuses should be solved.\n\nPro:\nThe proposed method is very novel. Encoder-decoder network is firsted employed on graph data for unsupervised learning. The writing and organization of this paper is good. The authors provide a clear and detailed describtion for the proposed method. For the decoding part, the authors proposed a de-noising step in wavelet domain to remove the amplified noise.  \n\nCons:\nIn this paper, the authors employ encoder-decoder framework to learn graph embedding. Why encoder-decoder framework is needed for graph data? One option is that we can use Variational Graph Auto-Encoders[1] to learn node embedding and use SUM or AVG function to obtain graph embedding. This should be added as a baseline to show that encoder-decoder framework is better. In addition, the authors should also mention how many pooling layers are used in encoder-decoder framework. And the experiments with different number of pooling layers can help understand the proposed method. \n\nOverall: I vote to accept this paper considering the importance of the task and the novely method proposed.\n\n[1].Kipf, Thomas N., and Max Welling. \"Variational graph auto-encoders.\" arXiv preprint arXiv:1611.07308 (2016).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}