{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "In this paper, the authors propose a method to find disentangling embeddings of the structure and the attribute of the graph. \bOverall, this is an interesting paper and the paper is well-written and easy to follow, and the paper has some merits. However, the reviewers were still not convinced by the response, and the paper is still below the acceptance threshold.  I encourage authors to revise the paper based on the reviewer's comments and resubmit it to a future venue. \n\n"
    },
    "Reviews": [
        {
            "title": "The intuition, model and experimental results are good, but lacking solid theoretical support and analysis. ",
            "review": "This paper focuses on disentangling embeddings of the structure and the attribute of graph. \bThe authors' key idea is that the structure and attribute information should be split in GNN. Based on this, the authors try to disentangle the structure embedding and the attribute embedding. With two different components, two different kinds of embeddings can be captured at the input stage.   In addition, these two different kinds of embeddings can be obtained by reconstructing the edge and minimizing the mutual information. At last, the authors propose a metric to evaluate the disentanglement. The models in this paper outperform baselines in node classification and graph classification task. \n\nThe pros of this paper is as following:\n1, The motivation/intuition of this paper is good. Current GNNs tend to aggregate the attribute information by graph structures, but ignore the relation between the structure and attribute information.\n2, This paper brings out two models to disentangle the structure and attribute information, at the input stage and the embedding stage-INPUT-SAD and EMBED-SAD, which seem reasonable to some extent. \n3, The experimental results are good on both node classification task and graph classification task.\n\nThe cons of this paper:\n1, Key weakness: the authors raise the problem that aggregation way of structure and attribute in current GNNs but not analyze the problem theoretically. For example, the true relation of structure and attributes and how the model in this paper can be derived naturally. The makes the novelty of the paper incremental.\n2, The authors use degree serves as attribute in INPUT-SAD. As I know, degree is a usually used attribute in graph. This design is contradictory to the authors' claim.   \n\nQuestion for authors: If some theoretical analysis about why the disentanglement is essential can be added into this paper. I may consider changing my reviews.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting problem setup and method with clear intention, but with small questions for the experiments.",
            "review": "Summary: This paper presents a novel method called Embed-SAD (as well as Input-SAD) to learn graph/node representations to disentangle structure and attribute information. Input-SAD is a simple baseline that tries to get structure-attribute disentanglements by individually processing graph structures and node attributes. For structure, the original node attibutes are replaced by out-degrees only, and passed to GNNs, while for attibutes, the node attibutes are passed to fully-connected networks. Embed-SAD is a more elaborate method to disentangle the GNN embeddings by posing two types of additional losses, i.e., the edge-reconstruction loss for structures, and the Noise-Contrastive Estimation (NCE) loss to maximize the mutual information against the structure-encoding vectors, in addition to the original loss for supervision. The paper also develops an interesting evaluation metric called SAD-Metric where node attibutes or graph structures are exclusively perturbed for each graph, and prediction for whether that perturbation is for structure or for attibutes made by the element-wise absolute differences between embedded vectors before and after the perturbation. This SAD-Metric can quantify the extent to which the obtained representation can detect which perturbation, that for structures or that for attibutes, is made for each sample graph. The experimental results also demonstrated that the structure-attibute disentanglement by Embed-SAD learning strategy actually improved the prediction performance of many off-the-shelf GNNs over many different graph- or node-level tasks.\n\nComments:\nThe paper is well-written and easy to follow since the intention and corresponding ideas are quite clear. The combined loss fo Embed-SAD also has ablation study results that was also informative. Basically I liked the idea and have not too much to say, but here are some  comments:\n\n- The SAD-metric would be more carefully evaluated since it includes \"training\" and we might be able to get zero-training error if embedding vectors are sufficiently high dimensional or we can use a strong predictor here for memorization. Even for a simple linear classifier (a \"low capacity\" classifier), I think it's better to replace this part by cross validation, or at least, a simple training-test split. \n\n- This study would depend on what node attibutes (i.e. \"featurization\") we use, and it should be more clearly described in the main body what node attibutes are used in each benchmarking experiment. In particular, we often include \"structure-derived features\" such as degrees (or the number of attached hydrogens in the case of molecular data) to the node attributes of GNNs, and how this practice affects the disentanglement would be informative in the paper's context. In particular, for Input-SAD.\n\n- Also, we need more information on the baselines of Table 3 for evaluating the SAD-metric. What is \"Random-GCN (2017)\"? (GCN with random node attibutes?) What is the intention to include this one here? Even Random-GCN had around 90% accuracy, and we need an additional baseline that clearly fails to disentangle structure and attibute information.\n\n- First, I personally felt that Input-SAD is actually not for \"disentanglement\" and a bit confusing, rather it's a simple baseline to see what if we input structure- and attibute- information separetely in the first place. However, to my surprise, the performance of Input-SAD was not that bad for molecular graph classification benchmark, in a sharp contrast to those for citation and coauthor network cases. Actually, Input-SAD was better than Embed-SAD for SIDER and BACE. Are there any possible explanations on this?\n\n- For eq (2), t_uv seemed undefined (though we can guess it). Do GIN, GAT, GraphSage use the edge attibutes?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper describes a graph representation learning method which disentangles structure information and feature information, and combines them for downstream tasks. Generally speaking, I think this is a good paper. But some design choices and explanations are not very convincing.",
            "review": "This paper describes a graph representation learning method which disentangles structure information and feature information, and combines them for downstream tasks. Generally speaking, I think this is a good paper. But some design choices and explanations are not very convincing. Details are as follows.\n\n\n[Pros]\n(1)\tThis paper is well written and easy to follow.\n(2)\tThe proposed problem is novel and the solution is intuitive.\n(3)\tExperiments are thorough.\n\n[Cons]\n(1)\tRegarding the SAD-Metric, why do you choose the undirect scheme based on a classifier? The goal is to minimize the change of structure (attribute) part of the embedding when altering the attributes (edges) of the graph. Using a classifier is a undirect measure about this, and would lead to misunderstanding when the classifier treats some elements for the structure (attribute) part as those for the attribute (structure) part. Imagine that such elements are coincidentally correlated with the opposite part. In that case, the accuracy could also be high. I think here it could be better to design a measure directly to reflect the above goal. Another question is, why do you define the measure on the graph embedding level, rather than the node embedding level, where the node is the one affected by the modification. I think this scheme could lead to more sensitive measures.\n(2)\tThe authors argue that the previous metrics for measuring disentangled representations are mainly designed for images, while the proposed SAD-Metric is tailored for graph-structured data. However, this is not convincing since the proposed measure is based on the learned embeddings (i.e., there is no graph structures). I think the general problem is still the same with the previous work. Hence, the novelty of SAD-Metric seems to be limited.\n\nMinor:\n-For model optimization, what does the $F$ stand for in Eq. (12)?\n-The examples in the second paragraph of section 2.1 are redundant.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Feel confused on the motivation",
            "review": "1. I am confused on the motivation part. Although the authors provide two examples (the prediction of a userâ€™s social class mainly relies on his/her social network structure) to illustrate the benefits of disentangled representation for certain downstream tasks and I agree on the statement, it raises another question that what kind of representation, structure component or attribute component should be used for a specific task. \n2. If we take a further through on the relationship between structure component and attribute component, it should be separated into three parts, a common part, structure-specific part and attribute-specific part. In my eyes, the authors put the common part and structure-specific part together. Why? What is the benefit? So, the name \"structure/attribute component \" is not accurate. \n3. I do not see too much novelty in the technical part (edge reconstruction and mutual information minimizing). I am fine with this. But I do not see a deep insight of the problem-solving philosophy, either.\n4. It is nice to see the performance gain in the experimental part. But  I want to know the authors use one type or two types of representation.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}