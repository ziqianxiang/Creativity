{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers and authors have had a significant and healthy discussion around this manuscript. The reviewers remain concerned about the some of the central claims in this manuscript. While they have appreciated the clear communication and willingness of the authors to clarify most of their concerns, this central issue unites the reviewers in maintaining their desire to see a more significant revision of this work before publication. I recommend that the authors take the reviewers' recommendations in improving the presentation and comparison of their ideas."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper proposes cascading decision trees (CDTs), which applies representation learning on the decision path of trees, and applies it to explain reinforcement learning models. The main empirical results are that CDTs are claimed to achieve better performance with more compact models than SDTs (soft decision trees), and CDTs as a post-hoc explainer of RL models using imitation learning is unstable.\n\nThe results don't seem fully convincing yet. The paper only compares to SDT, although the references mention several other tree-based methods to explain RL models. VIPER would be a nice baseline to add for the imitation learning task, and ANT would also be a nice baseline in general. \n\nSome important results are not yet reported. For example, what is the performance of the black-box model on the imitation learning task? This would contextualize the accuracy numbers of SDT and CDT. Also, discretized SDT has around 50% accuracy while undiscretized SDT has around 94% accuracy, which is a surprising gap. What is the accuracy of a vanilla, hard decision tree on this problem? In Figure 4, CDT doesn't seem much better than SDT for the Lunar Lander problem. Do the authors have any explanation for why this is the case? Perhaps it was mentioned in the paper but I missed it.\n\nBelow are some suggested improvements to the presentation of the results: \n- For Table 1, perhaps separate out the bracketed results into a different row, e.g. CDT with discretization for the feature learning tree vs. CDTs with only discretization for the decision making tree and discretization for both sub-trees. In fact, maybe 2 columns for SDT — SDT not discretized, SDT discretized — followed by 3 columns for CDT — CDT not discretized, CDT discretization for decision making tree, CDT discretization for both sub-trees. And then make 3 rows: accuracy, depth, # parameters. This is a very detailed suggestion but the gist is that the presentation of this table could be much improved. And similarly for Table 2.\n- Why are the Figure 5 cart-pole results distributed over 2 plots? 6 lines on one plot is possible if done with care. Splitting them over 2 plots makes the SDT (Figure 5a) and CDT (Figure 5b) results less comparable, especially when tables are not provided in that section, only figures. The y-axes are also inconsistent, e.g. there is a -300 for the y-axis of Figure 5d but not for 5c, which makes them less comparable.\n\nWhile the trees being proposed may be more compact, are they more interpretable? I found it hard to parse the linear coefficients and retain all of them in my brain. The trees presented in Figures 6-7 had 4 features, but what if you have more features, or one-hot-encoded features? The literature has user studies to test if human subjects can use trees, but this addition of the linear coefficients to the nodes perhaps warrants a user study to see if human subjects can retain that additional information and make use of it. If a user study is not feasible, could the authors comment on how the interpretability (or lack of) of the feature representation part can be quantified? \n\nI found the findings about instability of trees as a post-hoc explainer interesting and appreciate that the authors including several runs of the methods with the same setting (Figures 15-22). However, I found the figures hard to read, and the heatmap in the tree nodes also needed more explanation, which made me further concerned about the interpretability of the feature representation part of the trees.\n\nMinor points:\n- Typo in capitalization in \"In this paper, We propose Cascading Decision Trees\"\n- Citation needed for \"some methods have axis-aligned partitions (univariate decision nodes) with much lower model expressivity\"\n- Some sentences are not precise and perhaps too casual -- “it basically gives a similar solution”, “kind of like an estimated future position” ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "CDT review",
            "review": "This paper introduces a new cascading decision trees (CDT) for interpretable RL tasks. As an extension to soft decision trees (SDT), CDT adds a feature learning tree before the decision making tree. By utilizing a low-rank matrix model in the feature learning tree, CDT reduces the parameters of SDT. \n\n1. Novelty is limited. The key novelty of this paper is to add a low-rank representation learning, i.e., feature learning tree, before the decision making tree. Therefore, it is expected that the number of parameters of CDT is less than that of SDT. The improved interpretability is in fact mainly due to such representation learning procedure. \n\n2. It is unclear how to select the tree depths of both the feature learning tree and the decision making tree in a data-driven way. Although the authors provide some preliminary experiments on the tree depth in Figure 5, it is unclear why only $1+2$, $2+2$, $3+2$ are considered in CartPole-v1 and $2+2$, $2+3$, $3+2$, $3+3$ are considered in LunarLander-v2. As shown in Table 5 in Appendix G, the feature learning tree depth, the decision making tree depth, and the number of intermediate variables of CDT are different in three examples.\n\n3. The hierarchical CDT is included in Section 3.2. The authors claimed the hierarchical CDT might be able to improve the prediction accuracy while increasing the model capacity. No experimental study was provided on hierarchical CDT. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An appropriate extension of prior differentiable decision tree approaches. Intriguing concepts in nature but claims of improved explainability are not justified and may not be accurate.",
            "review": "## After Rebuttal and Discussion Period\nI want to first say that I really appreciated the opportunity to review this paper. It was awesome to see the authors willing to respond to the various suggestions and questions that the reviewers provided. The paper has improved over time but some significant areas of improvement remain. There continues to be some discontinuity between the stated scope of this paper as XRL and the proposed CDT approach. During the rebuttal period, I felt that the authors didn't do enough to soften their claims to match the support provided by the contributions present in the paper through both the methods and experimentation. This is not to say that this is not valuable work. As discussed, the concepts and ideas are strong, I however feel that the execution and scoping of this work is a little off from clearly communicating the contributions it makes. One aspect will be through a full evaluation of the utility of their approach as explainable through user studies or other qualitative means. The claims made in this paper regarding explainability are currently unsupported.\n\nAdditionally, I believe that there is significant room for improvement in the experimental portion of this work. If there were a way to provide some ablations of the CDT approach as additional baselines as well as the SDT/DDT benchmark, it would significantly improve this portion of the paper.\n\nAs it stands, I have not chosen to adjust my score. I do however urge the authors to continue in this line of work. I believe that there is strong merit with the direction they've begun. I look forward to seeing a future completed version of this work.\n\n\n#### **Summary**\nThis paper builds from recent developments in differentiable decision tree approaches to address explainable/interpretable Reinforcement Learning. A primary focus of this paper is on the development of rich feature representations to improve the expressivity of the probabilistic splits in the downstream decision nodes of the tree function approximators. Extensive experiments are run to compare the proposed CDT against a general representation of prior differentiable decision tree approaches in two tasks: imitation learning and online policy development.\n\n#### **Assessment**\nThis paper does a great job introducing the proposed CDT approach in the context of the relevant literature, addressing valid criticisms about prior work. The experiments are extensive, it is clear that a lot of care and thought was put into demonstrating the apparent benefits of CDT. The ideas of cascading improved feature representations to the differentiable decision trees is a reasonable improvement over prior approaches. However, I found the claims of improved explainability to be tenuous at best. Multivariate decision boundaries, and fewer parameters do not generally equate to improved explainability, especially in high dimension input spaces. This challenge of preserving explainability is complicated further by allowing for multiple layers of multivariate decision rules. Even linear functions, when using several variables, lose their ability to be clearly understood after two or three parameters. Traditionally the notion of explainability, especially within RL, is built around the interaction between observed features and the criteria a model uses to arrive at the suggested action. By transforming the decision criteria of the function approximator to be based on a, now uninterpretable, representation of the input, this explainability is no longer preserved. I'd be open to changing my mind if user studies showed the proposed CDT model to be more explainable but the major claims of improved explainability rest on an unsupported conjecture that fewer parameters in linear equations are better. This may have been true in analyzing the results presented in this paper but the experiments are performed over low-dimensional state spaces that have relatively similar oscillatory dynamics admitted by optimal policies. Other weaknesses and suggestions for improvement can be found below.\n\n#### **Strengths**\n- The paper does a great job outlining the prior literature in framing the proposed CDT approach. It is clear where the proposed contributions lie within the space of what has been done before. The idea of improving the feature representation for use in a decision tree model is similar to Konstschieder, et al yet the insistence on maintaining model simplicity is refreshing. I'm not convinced that this automatically equates to model explainability but the effort to maintain simplicity is appreciated.\n- The paper highlights two central tasks that CDT can be used within RL and extensively evaluates its performance against a generalized form of the prior literature (SDT).\n- Within the experimental evaluation the paper evaluates several architectural settings of both CDT and SDT providing a reliable analysis over possible options for the reader to build from.\n\n#### **Weaknesses**\nIn general, I found the technical development in Section 3 to be unclear. The path $\\mathcal{P}$ is described to be a set of nodes yet the only formal definition of $\\mathcal{P}$ is in selecting a single node. Within the arg max over possible nodes, it's not clear how a choice of $u$ affects the product of path probabilities. Overall, the discussion about path probabilities is confusing. Continuing, The development for CDT in Section 3.2 only describes a single layer of decision nodes and doesn't account for multiple layers. The transformation matrix $T_{K\\times R}$ is only defined for a single decision node. Other points that are unclear throughout section 3 are:\n- In the paragraph following Equation 5, it is written: \"During the inference process, we simply take the leaf on $\\mathcal{F}$ or $\\mathcal{D}$ with the largest probability...\" Aren't these path probabilities input dependent? Don't they vary stochastically due to the probabilistic definition?\n- After equations 6-7, there is some analysis about the reduction of parameters in  CDT. Yet this is hard to place any significance on because a similar analysis is not provided about standard DTs or SDTs. This comparison is also a little tenuous given the different types of SDTs.\n\nTouching on this last point. As defined, SDT is a class of models with very different configurations. It is probably not appropriate to lump them all together in a generalized setting. Even then, the explicit setting of the SDT used to compare with CDT is never described. Without a clear description of the baselines, the experimental results are difficult to fully interpret and place much confidence in. It's not clear why the authors didn't choose several SDT models to evaluate CDT against. In particular, it appears that Silva, et al [AISTATS; 2020] accomplishes a lot of the same goals as CDT albeit without learning a feature representation. It would've been nice to see a more dedicated comparison between CDT and the specific approaches in the literature. More discussion or justification for the use of a single SDT approach is necessary if the current experimental analysis is going to be the final one.\n\nOn this point, it is unclear why discretizing the decision trees lead to reduced parameter counts as well as a reduction in overall performance. In most of the prior literature using differentiable or soft decision trees, the discretization is done after training to speed up inference as well as secure the splits at the decision nodes to allow for model explainability. It appears that perhaps the \"discretized\" form of each of these evaluated models was indicative of how the models were trained? If so, this is a deviation from how the baseline methods were actually developed, leading to an inaccurate comparison. Further clarity about what is mean by discretization here would improve this initial set of experiments greatly. \n- Accuracy of an imitation learned model isn't very indicative of how effective that model is in solving the actual task. While informative, the results presented in Tables 1 and 2 don't really communicate how effective CDT or SDT are in solving the tasks. Average return from executing the imitation learned policies would be a better metric here. This would help separate out the differences between a 94% accurate model and a 91% accurate model. Ultimately, we care about how a policy performs on the task. It would be nice to have that analyzed.\n\n- The definition of stability is never formalized. Stability of what? What is the ideal score or metric for stability? Are we looking for the weight vectors to be equivalent between different random initializations of the same tree architecture? If so, I'm not certain that Equation 8 is appropriate as it's measuring the distance a between weight vectors between arbitrary nodes. There is no relation between where in the tree compared weight vectors are coming from. Also, the distance metric will be heavily influenced by the number of parameters and depth of the tree. This makes it an insufficient comparison between tree settings as is done in Table 3.\n\n- In Section 4.2 The results figures over experimental domain are inconsistent. Figures 4 and 5 show results from Cartpole and LunarLander, omitting Mountain Car while Figures 6 and 7 omit LunarLander. Looking at the appendix, it seems that the learning curves for Mountain Car are not as clearly separable (with a lot of variance) and the decision tree for Lunar Lander is not as interpretable. This feels like these results being less aligned with the desired narrative got swept under the rug, hoping that they wouldn't be looked at.\n\n- While the magnitude of weights within the decision node do certainly help determine feature importance, the weights alone doesn't tell me anything about the value of the input and why it leads to one decision being made over another.\n \n\n#### **Additional Comments**\nOne paper that I felt was overlooked within the great literature review was:\n*Wu, Mike, et al. \"Beyond Sparsity: Tree Regularization of Deep Models for Interpretability.\" AAAI. 2018.*\n\nThis omission doesn't negatively affect my estimation of how the authors framed their work but I do feel that it stands to be mentioned alongside other distillation approaches. Not only does this paper distill a Neural Network into a Decision Tree for interpretability, it also regularizes the neural network based on the complexity of that distilled decision tree, ensuring that the end interpretation of the network is simple yet informative. \n\nAt present, I do not feel as though this paper is ready for publication. I am not confident in the claims being made by the authors with regards to explainability and would suggest that they either fully justify these claims or place more emphasis on the performance improvements made through CDT at the expense of some explainability. Being more honest with the limitations and assumptions being made in the development of a model is always the best way to go. I do however find the idea of cascading feature representations to encourage more expressive decision nodes within the decision tree to be intriguing. Particularly within the RL space. Representation learning within RL is an open problem and I wonder if this simple modeling strategy may highlight unique aspects of learning representations that deep RL is unable to.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "CDTs are not demonstrated to be interpretable",
            "review": "Edit:\n\nI have read the authors' response and the other reviews. I still believe that this paper is not ready for acceptance. \n\nSummary: \n\nThe authors propose to use Cascading Decision Trees (CDTs) to express the policy for an RL agent. The authors describe CDTs and evaluate their use in imitating a trained expert as well as representing a policy during training. \n\n\nReasons for score: \n\nThe interpretability of CDTs is not convincingly demonstrated by the examples provided. CDTs do perform better than the tested SDTs, but the experiments are insufficient to conclude that CDTs perform well compared to other models which are less interpretable than SDTs. \n\n\nPros:\n\n-CDTs are explained well.\n\n-CDTs are shown to solve basic RL environments which are commonly used for evaluation in XRL work. \n\n\nCons:\n\n-The authors argue that linear partitions are superior to axis-aligned partitions based on the number of parameters. However, the authors do not consider the number nor complexity of operations required for a \"forward pass\" through the model. A fairer evaluation of explainability would include the average or worst-case number of operations required to select an action (e.g., 4 multiplications, 4 additions, and 2 comparisons in the worst case for Figure 1a). By using a single D tree for all F trees, the number of parameters is reduced, but the length of any given path is still long (with many operations). \n\n-The authors claim that CDTs are more \"explainable\" than SDTs, but this is not sufficiently demonstrated. One advantage of DTs over MLPs is that all splitting operations occur on the original features. If the original features are interpretable, then the partitioning process operates on meaningful features. When learned features are used (as in CDTs), this property is lost. \n\n-Between the leaf of a feature learning tree and an internal node in the decision tree, the input is multiplied by a set of weights and put through a non-linearity. When this is performed several times in sequence, this begins to resemble a MLP. The authors do acknowledge this potential problem (as motivation for not evaluating hierarchical CDTs), but this concern also applies (to a lesser degree) to the \"single F, single D\" case. \n\n-The authors use \"heuristic agents\" as experts in their experiments. This does not follow the procedure established by prior work, and this is at odds with the motivation of CDTs as useful for explaining RL agents.\n\n-The experimental evaluation is lacking in a number of ways: \n\n--The authors should report policy performance (in imitation learning experiments). Accuracy is a useful metric, but not sufficient on its own. A model can have a high accuracy without learning a well-performing policy. Also, results are reported for a different set of configurations in Table 2 as compared to Table 1 (without any justification). This suggests that discretization of CDTs must be performed in a more nuanced way than otherwise stated in this work.\n\n--The authors do not compare to VIPER in the imitation learning experiments they perform though VIPER was shown to yield higher-performing policies than standard classification-based learning.\n\n--In the RL experiments, the authors compare to a MLP. They find that CDT has a similar (but sometimes worse) final performance despite having fewer parameters. However, the authors should also compare to a smaller MLP, ideally with the same number of parameters as CDT. Without this comparison, conclusions cannot be drawn about the \"parameter vs performance\" benefits of CDT.\n\n-State normalization (based on a \"well-trained policy\") is not standard and generally not feasible when applying RL. It is unclear why this was done. \n\n-The second paragraph on page 8 attempts to explain a learned MountainCar-v0 model. The lack of certainty and vagueness of the insights (\"kind of like an estimated future position or previous position, and makes action decisions based on that\") suggests that additional work is required to make CDTs interpretable. Why are \"future position\" and \"previous position\" both options for this explanation? The environment has two features (position and velocity), so discovering that the agent selects actions based on intermediate features derived from position is a given. Ideally, the authors select a way to measure interpretability and perform a quantitative evaluation. \n\n\nQuestions During Rebuttal Period:\n\nPlease address and clarify the \"Cons\" above.\n\n\nMinor Comments:\n\n-The method's motivation is best placed in the main paper, not in the Appendix (page 3, footnote 2).\n\n-The figures would be more helpful if they appeared on the same page as the corresponding text.\n\n-The caption for Table 1 is not clear with respect to which CDT accuracies are for which discretization schemes.\n\n-The authors note that discretization decreases performance and \"claim that this is a general drawback for tree-based methods in XRL...\" However, this is only applicable to soft DTs. This should be made clear (e.g., VIPER does not have this drawback).\n\n-The y-limits of Figures 5a and 5b should match so that performances can be more readily compared across plots (given that CDT and SDT are not plotted within one figure). The same applies for Figures 5c and 5d.\n\n-The paper would benefit from another editing pass for grammar. \nSome Typos:\n\n-Abstract: \"trees (DDTs) have [been] demonstrated to achieve\"\n\n-Introduction: \"are generally lack[ing] interpretability\"; \"In this paper, [w]e propose\"\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}