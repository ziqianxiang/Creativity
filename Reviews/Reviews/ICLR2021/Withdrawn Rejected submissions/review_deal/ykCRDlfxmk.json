{
    "Decision": "",
    "Reviews": [
        {
            "title": "Another tricky paper",
            "review": "\nThis paper proposes a search framework that is very similar to \"Neural Architecture Search with Reinforcement Learning\", except that the authors claim their method can search discrete training hyper-parameters. The authors evaluate their method on several datasets and claims to achieve SoTA results.\n\n1. REINFORCE V.S. Bayesian Optimization and other derivative free optimizations\n\nI strongly encourage authors to take a comprehensive review of literatures in policy gradients (REINFORCE), and derivative free optimizations. There is a weird trend in NAS community that re-makes the wheels in the search. I believe several claims made by the authors are questionable:\n\na.  Sample-efficiency is a well known issue in RL, and RL usually requires millions of trajectories before working well. The author now claims a well known policy gradient method is actually the most efficient one. In fact,  [1] also uses REINFORCE to update the controller, and it required a lot of samples to work.\n\nb. the comparison to HPO methods is unfair and several claims are wrong. First, I'd like clarify one point, in derivative free optimization, we maximize f(x) s.t. some constraints, and x can be anything, including the configurations of training pipeline, architecture hyper-parameters, etc.. So, adding training hyper-parameters into x and optimize f(x) is not a well justified research problem. Besides, as you argue in many places in the paper, existing derivative free solvers support the search over a mixture of [continuous, discrete] variables. Check this package for example. https://github.com/facebookresearch/nevergrad.\n\nYou can see some paper claiming they can do NAS really fast, e.g. [2], simply because they are using a supernet or using bi-level optimizations in DARTS. They terminate the training of supernet earlier, then use some tricks to fine-tune the final architecture to a reasonable result (simply because the search space is well defined). That does not necessarily mean HPO methods cannot be applied with these tricks; in fact, using HPO together with a supernet has achieved far better results.\n\nFYI, you can get gradients in the discrete space using finite difference. Therefore, calculating gradients over discrete variables can not be counted as a contribution.\n \nc. questionable experiment results: I highly doubt the Bayesian Optimization in Fig.2 is not setup correctly. Please also plot the figure by samples. If the authors use different tricks to reduce the search time for your agent, please also apply to Bayesian Optimization to ensure a fair apple-to-apple evaluation.\n\n2. ResNet-50 baseline used in sec.3.2 is questionable, and please follow the setup below:\nhttps://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv, where they achieve 79.039 top-1 accuracy with resnet 50.\n\n3. ImageNet results are far from SoTA: [3] shows 300 MFLOPS model achieve 79.6 top-1 accuracy. I understand you may use different tricks. but given the current situation, it is really hard for a reviewer to judge if two paper use the same tricks. \n\n4. I'm not sure if it is still meaningful to claim NAS from 5 hours -> 1 hour. Training a CIFAR-10 model from scratch to SoTA result takes 3 days. Now NAS becomes a task even easier than training a model. Do you really believe that? or perhaps NAS has over exploited our prior knowledge in the development of CNN. If I draw the first sample from a well defined search space and apply lots of hacks to boost the network performance to a reasonable level, does it make sense claim NAS in 1 second?\n\nIn summary, this paper is more like an engineering study, rather than a rigorous scientific research. My main concern is that this paper does not provide any good insights.\n\n[1] Neural Architecture Search with Reinforcement Learning\n\n[2] Searching for A Robust Neural Architecture in Four GPU Hours\n\n[3] Neural Architecture Transfer\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": " Overall, AutoHAS technique seems to be practical for hyper-parameter tuning with architecture searchs. ",
            "review": "This paper proposes a framework for both hyperparameter and architecture search.  For continuous hyper-parameters, this paper discretizes the continuous space into a linear combination of multiple categorical basis and extend the idea of weight sharing to hyper-parameters.  This addressed a critical issue in AutoML area: non-optimized hyper-parameters could mislead NN architecture performance. \n\nStrengths: \n+ Address important issues (both hyper-parameter and architecture) in AutoML\n+ Demonstrate capability to optimize both hyper-parameter and architecture search.\n+ Computation cost of this method is not very expensive, compared to Differentiable approach (Table 3).\n\nWeaknesses:\n- Experiments are not comprehensive enough.  This paper focuses on image applications; it does not include any speech, text etc. \n- Not enough results in large datasets.  There are just 2 examples in ImageNet (Table 2).  The results in large datasets are marginal (incremental, 0.63% and 0.77% compared to state of art results).  Since the paper emphasizes in hyper-parameter search, it is important to provide enough experimental evidences in hyper-parameter search.  It will be better if authors can pick up one case that require several hyper-parameters and state of art solutions did not optimize them well.  Then we can test out AutoHAS 's advantages in a significant amount of improvement (>1%). There is not enough evidence in this paper to support one of the main claims: AutoHAS optimized hyper-parameters in large datasets such as ImageNet.\n- Interaction between architecture search and hyper-parameter:  Because of better hyper-parameters, some architectures originally were ignored in NAS; now they could become much better (due to optimized hyper-parameters) and they can be selected by architecture search.  I did not see an in-depth discussion about this.\n\nQuestion (similar to weakness):\n1. Please demonstrate this method in other kinds of applications such as speech and text.\n2. Please demonstrate enough advantages for hyper-parameter search in large datasets.\n3. How does architecture search interact with hyper-parameter search? Please provide some examples that originally ignored in architecture search in ENAS and DARTS (due to non-otpimized hyper-parameters) and now these architectures with better hyper-parameters are selected.  This is an important issue, but I cannot see in-depth discussions in this paper.\n\n\nSummary:\nThe work can potentially find right hyper-parameters and that also helps architecture search. This work had demonstrated better hyper-parameters to improve ~0.63% in ResNet50 (ImageNet), compared to state of art baseline (77.15%). The paper is well motivated and organized.  However it will be much better that this paper can provide comprehensive experiments in large datasets.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "AutoHAS: Efficient Hyperparameter and Architecture Search",
            "review": "1. Summary and contributions: \nThis paper generalizes single-trial architecture optimization to both hyperparameters and architecture optimization. To achieve this, it discretizes the continuous hyperparameters into a linear combination of categorical choices to unify architecture and hyperparameter search and uses temporary weights by optimizing the sampled candidate architecture using the sampled candidate hyperparameter to allow a relatively stable inner optimization. Optionally, to allow sufficiently RL reward for optimization in the large joint search space, it also proposes to use a greedy schedule to optimize some components while fixing others in a certain phase. \n\n2. Strengths: \n1). This paper proposes an automated method to search for both hyperparameters and architectures in an efficient way.\n2). It generalizes weight sharing techniques from architectures to hyperparameters.\n3). It proposes some tricks (e.g. temporal weights, phase-wise optimization) to eliminate some practical issues in the controller optimization. \n4). The improvement in strong baseline models looks good.\n\n3. Weaknesses:\n1). Since it is a straightforward extension of ENAS with the combination of categorized hyperparameter choices, I would view the main novelty of this paper as the use of temporal weights in updating the RL controller. However, it is not fully justified why it works such effectively.\n2). The phase-wise optimization is a little bit inconsistent with the main claim of this paper. The main claim of this paper is to propose a gradient-based search method in a large joint search space. However, It looks like optimization in such a large search space is not very effective so that a progressive search procedure is then proposed to eliminate such a problem. This is technically sound but there may exist a better solution only using single-phase optimization. \n\n4. Correctness: Are the claims and methods correct? Is the empirical methodology correct?\nThe claims and methods appear to be correct.\n\n5. Clarity: Is the paper well written?\nYes, although a few aspects could be improved (see feedback).\n\n6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions?\nYes.\n\n7. Reproducibility: Are there enough details to reproduce the major results of this work?\nYes. \n\n8. Additional feedback, comments, suggestions for improvement, and questions for the authors:\nI would suggest the author clearly illustrates why the temporal weights is needed and how it works by providing more discussions and some meaningful ablation studies because I view it as one of the main contributions of this work. For example, how it helps to decouple shared weights and the RL controller. \n\n\n\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns regarding efficiency of the method to solve the joint optimization problem and lack of technical details.",
            "review": "**Brief Summary**\nIn this work, the authors propose with AutoHAS a method that jointly optimizes the neural architecture and its hyperparameters. This is done efficiently by using the idea of weight sharing. The proposed method is closely related to ENAS. A reinforcement learning controller is learned to sample the neural architecture and its hyperparameters. Since the proposed method only supports categorical decisions, the authors propose to discretize continuous hyperparameters into a linear combination of multiple categorical basis. Different to a straight-forward extension of ENAS that would additionally sample hyperparameters as well, this method introduces additionally temporary weights that will only be used to compute the reward but it will not influence the learned parameters of the super network. This method is evaluated on seven different datasets for various different search spaces including those that optimize only the hyperparameters.\n\n**Detailed Feedback**\nThe topic by the paper is interesting and well-motivated. I found some aspects not sufficiently explained and some results to contradict the claimed contributions. I will detail them in the following.\n\n*Does the method efficiently solve the joint optimization problem?*\nIt is unclear to me whether this method benefit from a joint optimization of architectures and hyperparameters. At least Table 4 indicates that it might be sufficient to only optimize the hyperparameters. The results indicate that a if the method jointly optimizes the different decisions, it performs worse than if it does it phase-wise. With phase-wise the authors refer to the technique of optimizing for one hyperparameter while fixing all others. The phase-wise optimization refers in some experiments to first optimizing the architecture and then optimizing the hyperparameters or vice versa (authors could be a bit more clear what the different phases optimize in the different experiments). The method still optimizes both types of decisions but that seems to be very similar to simply running ENAS at first and then use RL to optimize the hyperparameters. This seems more like a baseline than an actually proposed method. The main difference seems to be the weight-sharing idea for hyperparameter optimization rather than actually the joint optimization. Which brings me to my next point.\n\n*How to update the temporary weights?*\nThe use of the temporary weights seems to be one of the major differences to the state-of-the-art. However, it is not well-motivated why this is necessary (a comparison to a method that only uses W is missing) and it is not fully clear how W and W* are obtained. Unfortunately, the pseudo code is not very helpful in understanding how to update W and W*. Section 2.3 also only mentions how W* is used but not how it is obtained. Figure 1 seems to be the only information with regards to details about how W and W* are updated. It seems like you obtain W* by starting from the sampled architecture with weights W and the sampled hyperparameters and take one gradient step. But Figure 1 shows that W is also computed by taking both the sample architecture and hyperparameters (and probably also the old W) into account. Then again, the paper explicitly mentions that W* is discarded. So how is the estimation of W* different to the update step of W? Could the authors please provide a clear pseudo code? Based on the current description, I am not able to reimplement the method.\n\n*Details about Table 4*\nIt is not fully clear what \"Baseline\" refers to in Table 4. I assume it is a method with default weight decay and no MixUp? If this is the case, it would be interesting to know how the proposed method compares to following baselines: i) Baseline with MixUp (default settings) and ii) a differentiable hyperparameter optimization method which optimizes the weight decay with MixUp (default settings).\nIs there any particular reason why Weight Decay + MixUp + Arch rows are missing in Table 4?\n\n*Comparison to NAS methods*\nIt seems like there is no direct comparison to any NAS method on traditional search spaces. It would be interesting to see if the proposed method can discover better architectures than the state-of-the-art by taking the hyperparameter choice during the search into account. In all current experiments the method makes only minor modification to existing architectures.\n\n*Why use only discrete actions?*\nReinforcement learning is also able to make continuous decisions. Why is it necessary to discretize the continuous decisions into a linear combination of multiple categorical decisions?\n\n**Conclusions**\nConcluding, the paper addresses the interesting and relevant topic of joint architecture and hyperparameter optimization and rightfully claims to have proposed a time-efficient efficient method for it. The method is compared on various datasets and search spaces and the authors show improvements over the considered baselines. I would like to address the authors following aspects:\n1. clear description of how W and W* are updated\n2. addition of Weight Decay + MixUp + Arch and simple differentiable hyperparameter optimization baseline to Table 4\n3. direct comparison to NAS methods in the scenario of discovering architectures from scratch\n\n\n**Minor Remarks**\nTypos: block-box, hyperparmaters, hyperparamter",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}