{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper presents an adversarial training method via an additional saliency map perturbation input.  However, due to unfair comparison, I tend to reject  this paper. .  ",
            "review": "This paper provide a new adversarial training method without adversarial examples.  The key idea is adding saliency map as an additional input while finetune the neural network.  With the perturbed saliency map, the BN can adaptively update the weights and it results a more robust model for adversarial input.  Compared to existing methods, the proposed method is able to do the robust training without any adversarial examples. \n\nThe strong points in this paper:\nThe intuition comes from the gradient map difference between adversarial  examples and origin images. \nThe idea is simple and effective if we see the numbers from the experimental results.  \n\nThe weaknesses:\nHowever,   there is key point missing when this paper claims it is better than baselines for defending attacks. In white-box setting,  this paper needs an additional saliency map as input, but the adversarial examples are from clean images.  Since the adversarial examples are not calculated from saliency map, this may be the reason why the proposed method is better than existing ones.  \nTherefore, when attach such model, the saliency map should also be perturbed.  Another setting is  we should give the saliency map for other baselines, if all these setting have done, I will be more convinced by the proposed method. \n\nThe proposed method claimed it only needs to finetune with saliency map to achieve a more robust model. But it didn't consider the attacker are known about this additional input.   ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea, but lacks evaluation against adaptive attacks",
            "review": "This paper proposes a new adversarial defense method without requiring adversarial training. The defense works by updating BN parameters on adversarial inputs perturbed with the saliency map. Robust accuracies on CIFAR-10 and CIFAR-100 are reported to demonstrate the effectiveness of the proposed method.\n\n1. This paper proposes an empirical defense method, I think for a complete evaluation, you need to consider all possible potential attack methods. The authors should also try the BPDA attack proposed in [1] and the gradient-free SPSA attack proposed in [2].\n2. The results of SAD without adversarial training are suspicious. Without further evidence or discussion by the authors, I tend to believe that the model is evaluated on weak attacks. The authors can refute me by showing the distance to the decision boundary is indeed larger, as claimed in the paper. For distance to the decision boundary, the authors can refer to the metric here https://github.com/uclaml/RayS#average-decision-boundary-distance-adbd. Also, the authors can include discussion in explaining why SAD without adversarial training has such good results, rather than just showing numbers, especially when the reported number is easy to raise questions [3].\n3. A technical question: why only update the BN layers. The authors mention that BN helps to push the decision boundary. What would happen if we update all the parameters?\n4. What method did the authors use to generate the saliency maps?\n5. Last question, but maybe the most important one. Did the authors consider the case where an adaptive attacker attack the saliency map generation method? Specifically, in terms of the terminology of model inference in algorithm 1, what if the attacker creates adversarial examples such that the standard saliency map information is completely destroyed? The authors did not evaluate against this threat model.\n\n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. Anish et al, ICML 2018.  \n[2] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. Jonathan et al, ICML 2018.  \n[3] On Adaptive Attacks to Adversarial Example Defenses. Tramer, Neurips 2020.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper proposes an interpretable defense method. Although some experiments show the effectiveness, there still exist some problems that induce the rejection. ",
            "review": "This paper proposes an interpretable defense method (SAD).  Some experiments show its effectiveness. I have the following comments:\n\n1. The author claim “SAD is much more efficient than adversarial training”. But in this paper, I only see the author said “Adversarial training is one of the most effective methods for defending adversarial attacks, but it is computationally costly”, and I did not see the relevant calculation cost analysis. In my opinion, Faster-adversarial training (Faster-AT) uses the FGSM to achieve adversarial training which reduces the computational cost. I did not think the proposed method is faster than it. Moreover, it only uses 15min for ResNet18 on Cifar10.\n2. The author claim “SAD achieves better defense performance than adversarial training under various attacks, including white-box and black-box.” The author should compare the proposed method with SOTA attack or defense methods. Such as AutoAttack[1], AdvMixup[2].\n3. This method is called an interpretable defense method. The authors should discuss more reasons.\n4. The author claim “By adjusting the strength of saliency maps overlaid to the input data, we can obtain different defense effects and strike a good balance between robustness and accuracy.” The Authors should discuss how the SAD can strike a good balance between robustness and accuracy.\n5. The English grammar, spelling, and sentence structure of this paper should be improved, so that the goals and results of the study are clearer to the reader.\n[1] \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\" Francesco Croce, Matthias Hein\n[2] Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}