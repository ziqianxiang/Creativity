{
    "Decision": "",
    "Reviews": [
        {
            "title": "Problem is interesting. However, the supporting evidences are not strong. ",
            "review": "This paper presents a heterogeneous model transfer(HTM), which transfers the knowledge from a pre-trained neural network model to a target model. This problem of heterogeneous model transfer is an interesting setting in the neural network community. In this manuscript, the author first gives two challenges in heterogeneous model transfer learning: architecture-level and layer-level. Then presents three properties of weight space in neural networks and proposed their HTM employing those properties. While those properties are the partially experimental conclusion and not solid. Moreover, the experiments are not enough. \n\nP4. Property 1 is in the setting of a multi-layer perceptron(MPL). But the other properties are in the setting of convolution neural networks(CNN). Are different network architecture sharing the same properties?\n\nP5: Property 2. \" K' \\bigotimes I' \\approx K' \\bigotimes I' \" is an experimental result. But the equation ( K' \\bigotimes I \\approx K' \\bigotimes I' ) misses description and explanation. And the approximated operation would be enlarged with the depth aggregation in deep models. \n\nP5: I can't entirely agree with the authors' claim: \" This implies that removing/repeating a portion of rows or columns of weights in neural layers does not signiÔ¨Åcantly harm the performance of a neural network.\" \nThe activation function state is fired by the aggregation of different neons inputs. But, the repeating weight operation would easily impact the input value of the activation function that changes its fire state. \n\nThe authors claimed that three properties might be sufficient conditions in the weight space of neural networks. But they are not convincible to be the necessary conditions to support the authors' methodology. \n\nFigure 4 (b) illustrates that only parts of the filters are selected after cyclic stack operation. The experimental proof is needed to support the authors' claim.\n\nThe experiments are not enough that miss strong comparisons. Moreover, the deep model all has other components, i.e., batch normalization. \n\nThe results of rank-1 on Market-1501 are around 95 with the filter interpolation(Figure 5), as well as around 95 with the cyclic stack(Figure 6). Why the performance got decline to 93.7 (Figure 7) with both filter interpolation and cyclic stack?\n\nP2: Figure 1. sub-figure titles are missing.\nP4: \" ...., whose weights are denoted as ( W^l, W^2, ..., W^l, ...,W^L), ....\". The first \"W^l\" typo.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review",
            "review": "This paper proposes an interesting model transfer setting: transfer knowledge from one pretrained neural network to another neural network with different network architecture. The challenges of this transfer problem are: (1) how to find the layer-to-layer relationships and (2) how to transfer weights between two layers with different channels and filter sizes.\n\nTo handle these problems, this paper proposes heterogeneous model transfer (HMT) method. To address the first challenge, the authors choose the longest chain to do transfer, which is the sub-network with the longest path from the provided neural network. As for the second one, a filter interpolation method and a channel stack-and-slice idea are adopted to make the weights have the same tensor shape.\n\n+Strengths:\n1. This paper proposes an interesting model transfer setting and adopts a simple approach to address it.\n2. The presentation of this paper is good.\n\nConcerns:\n1. The authors provide filter interpolation ablation in Section 3.1. In the ablation, the target kernel sizes are 5x5, 7x7, and 9x9, which are larger than the source 3x3 kernel. In this setting, simply zero padding can make the target network achieve the same performance as the pretained network. The authors should compare the interpolation strategy and simply zero padding. Also, the authors should do experiments with smaller kernel sizes. For example, transfer 3x3 to 1x1 or transfer 5x5 to 3x3. And a simple center crop strategy should be a baseline.\n2. For the cycle stack setting, the author should compare the stack-and-slice method to the Net2WiderNet operator proposed in net2net when the target channel size is larger than the source. \n3. What about transferring a shallow pretained network to a deep target network? \n4. Where are the CIFAR-100 experiments?\n\nOverall, I vote for weak rej. This paper proposes an interesting problem and a simple solution. However, the experiments are weak. The authors only compare their method with the baseline trained from scratch. Actually, there are several approaches to reduce the performance degradation when doing weight transfer. The authors should compare the proposed method with them, as I comment in Concerns.\n\n[1] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. \"Net2net: Accelerating learning via knowledge transfer.\" ICLR 2016.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The novelty is not enough",
            "review": "The paper proposed an algorithm for transferring weights from one network to another.\n\nStrength:\n1. The topic of transferring weights in the case of heterogenous networks is good.\n2. With the proposed method, the performance is better than no-transfer baseline on person re-id datasets.\n\nWeakness:\n1. The properties proposed in the paper are hard to understand. In fact, the properties are not supported by theoretical analysis or experimental verifications. Thus, they cannot actually be called properties. They are assumptions made in the paper. Even so, the assumptions seems not true. For example, the filter interpolation assumption is not correct. Let us consider an extreme situation: Can a 9x9 kernel be interpolated into a 1x1 kernel while preserving the performance?\n2. The experiments are evaluated on person re-id dataset. This is not convincing as person re-id datasets are not big and the images are of small scale. I suggest the authors evaluate their method on standard model evaluation benchmarks like ImageNet or COCO.\n3. No comparison methods are chosen. At least some baseline methods can be used, like knowledge distillation\n4. CIFAR 100 results are not reported, although the authors claim to use it.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Major concern is about related work and lack of experimental results on large-scale datasets like ImageNet",
            "review": "This paper proposes a heterogeneous model transfer method (HMT) for knowledge distillation by selecting the longest chain. The proposed method transfers the knowledge from one model to another layer-by-layer in a chained way. By achieving the longest chains from the source model, weight transfer is then used to transform weights from the source model to the target model by using bilinear interpolation and cyclic stack. The solved problem seems to be a very general knowledge distillation problem.\n \nPros:\nDetailed ablation study showed the effectiveness of their approach to different model transfer settings using or without using their methods.\nIt enables the KD for different deep models.\n \nCons:\nThe methodology section is difficult to read.\nLack of comparison with many existing knowledge distillation approaches [a]. KD is also used among different architectures in BigNAS [b].\nThe method is difficult to understand. The introduction of the longest chain principle and the proposed method should be combined with the concept and the notation of the chain and weights in the previous Property section. Directly introducing $L$ leads to some confusion with the understanding of the concept. In addition, the collision permutation problem is a little bit hard to follow w.r.t the context of the longest chain principle. Why after the 2nd and 5th layers permute, the 3rd layer can be randomly permuted, and why mixup will cause collision afterward? Maybe introducing this together with the concept of the longest chain and figures would be better to understand.\nThe performance of the source model should be provided for comparison purposes. It seems the results are not very convincing after transfer from the source model to the target model.\nExperiments for larger datasets, e.g. ImageNet, should also be provided to show the effectiveness and robustness of the proposed method.\n \n[a] Heo, B., Kim, J., Yun, S., Park, H., Kwak, N. and Choi, J.Y., 2019. A comprehensive overhaul of feature distillation. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1921-1930).\n\n[b] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc Le. BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models, ECCV, 2020.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}