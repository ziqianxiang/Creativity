{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Reviewers raised several concerns about the paper guided by unfounded heuristics as well as the artificiality of the tasks involved.  Rebuttal only answered a few of them and did not convince the reviewers which has been clearly stated in the response. We hope that the authors will improve the paper for future submission based on the reviews. "
    },
    "Reviews": [
        {
            "title": "Ok but not good enough. Conceptually and empirically not entirely convincing. ",
            "review": "\n### Summary\n \nOk, but not good enough. The authors present Hash-Routed Convolutional Neural Networks (HRNs), intended to enable learning of stable representations for continual learning, i.e. representations that change little for previously-learned tasks as more tasks are learned. The authors benchmark HRNs against several baselines in a number of tasks. Overall, the proposed method does not appear as conceptually and empirically convincing as those of other papers at ICLR and similar conferences. \n\n\n### Reasons for score\n\n- The presented algorithm does not appear as conceptually compelling as existing work on the subject (see below for papers that the authors could have cited and evaluated against, but did not). \n- Overall, the architecture, and in particular the use of hash routing as it is used here, does not seem natural to me. (See explanation below)\n- The results from the evaluation section are mixed, reinforcing my intuition that the proposed architecture is not as suitable as existing solutions. \n- The paper omits information about hyper-parameters, hyper-parameter tuning and choices, making the paper’s results impossible to reproduce. Overall, there is so little information that it is impossible to tell if the experimental evaluation was fair to existing algorithms. \n\n\n### Pros\n\n- The paper addresses an important question: how can we train models in a continual-learning set-up, build representations that are meaningfully shared across successive tasks, and avoid performance degradation on previously learned tasks as new tasks are learned? The authors attempt to solve this problem using feature hashing, which had not previously been used in this particular way before. The proposed architecture is novel. \n\n### Cons\n\n- Conceptually, I find the HRN algorithm presented here somewhat unnatural. The authors write “Similar feature maps get to be processed by the same units, as a consequence of using feature hashing for routing. ” That’s technically true, but since the similarity is measured right in the raw feature space, the similarity will often not be meaningful. For example, a black dog in front of a green background would have little measured pixel-level similarity with a beige dog in front of a blue background, even though conceptually both images contain dogs. I would expect that first computing higher-level representations and then using those to route to an appropriate CNN would be more useful than measuring similarity on raw inputs. I do understand that the HRN inference proceeds through multiple iterations, and that the output of the first iteration becomes an input to the second iteration. Still, I imagine that hashing raw inputs right at the start to decide where to route is  harmful. Note: while Weinberger et al (https://alex.smola.org/papers/2009/Weinbergeretal09.pdf) did hash their input features, but they were using bag of words features, not pixel features. \n- Figure 2 (top) is misleading and cherry-picks data that make the author’s proposed algorithm look better. The graph suggests (green line) that the authors’ HRN algorithm’s performance on Task 0 stays nearly constant at a high level as more tasks are added. However, a closer look at Table 1 (top) indicates that HRN performs significantly worse than baselines on several tasks (see e.g. task T1, T2 and T3), both in max and min accuracy.  \n\n- The paper does not provide (enough/any) information on the precise architectures and hyperparameters that were used for the “Experiments” section. This makes the paper very challenging to reproduce. Even for the HRN algorithm presented in this paper, I could not find what value was used for the maximum basis size parameter “m” was used. I could not find much information on whether the authors selected or tuned the hyper parameters for the algorithms that they benchmarked against in the “Experiments” section. Based on the information provided, I cannot tell if the experiments made were fair to the baselines. \n\n- Some relevant existing work has not been cited and compared to: the authors did not cite the ANML algorithm (https://arxiv.org/pdf/2002.09571.pdf), which has achieved state-of-the-art performance on tasks similar to the one that the papers of the paper reviewed here evaluate on. I would suggest that the authors cite this paper, and benchmark againnnst ANML unless there is a good reason not to do so. \n\n- The authors should also have cited “Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning” (ICLR 2018, https://openreview.net/forum?id=ry8dvM-R-) . \n\n- The authors write ' *The output of a typical CNN is a feature map with a dimension that depends on the number of output channels used in each convolutional layer. In a HRN, this would lead to a variable dimension output as the final feature map depends on the routing.* ' Why would there necessarily be a variable-length output dimension? One could fix the output width of multiple CNNs, and sum the outputs element-wise to get a fixed-length representation. \n\n- the authors write “As the network trains, hashed features will also change and routing might need adjustment. If nothing is done to update full basis, the network might get ”stuck” in a bad configuration. ” Would these basis updates not lead to changing output representations, thus breaking one of the key properties that the authors were looking for in this architecture? In fact, in Task T1 Table 1 (top), it appears that the model’s performance degraded quite significantly as tasks were learned. \n\n### Questions during rebuttal period:\n\n- I would be grateful if the authors could address as many of the cons listed above as possible. \n- In Figure 4, on the right, there are arrows going into U_4, but no arrows coming out of U_4. Is this an error? \n \n### Some suggestions:\n\n- There are possessive apostrophes missing here and there, e.g. “units” vs “unit’s”. I suggest that the authors review their use of possessive apostrophes one more time. \n\n- “0 This is a 11 tasks” should be “0 This is an 11 tasks”\n\n- “The following scenarios were considered.” might better be followed by a colon than a period.  ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "Summary:\nFor a learning model to learn continuously, it needs to handle new datasets without catastrophic forgetting or requiring the model to grow larger. This paper proposes a hash-routed convolution neural network where a different set of convolution filters are used depending on the data. New convolution filters can be added to the network without increasing the computational cost of the network.\n\nQuestions:\n1) What is the running time performance for hash-routed networks (HRN) during training and inference?\n\n2) How are new classes added to the Softmax layer during training?\n\n3) Couldn't you train a single model on the old and new datasets such that it performs well on both? If so, does the continual learning model train faster than building a data-centric model?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Hashing is an interesting idea for routing networks, but I would like to see a more complete evaluation and a clearer description of the method",
            "review": "############## Summary ##############\n\nThis submission addresses the question of how to avoid catastrophic forgetting while being scalable and adaptable to multiple tasks. The paper introduces the hash-routed network (HRN), a new routing mechanism for neural networks whereby feature hashing (Weinberger et al.) is used to determine the similarity between data points; similar data points are then routed to the same units. HRNs  work by keeping various convolutional units, each coupled with an orthonormal projection basis. The parameters of the convolutional layers are trained via vanilla back-propagation and the orthonormal basis is created following the Gram Schmidt procedure. An aging mechanism enables updating basis vectors that have become obsolete over time. The proposed method is evaluated in three standard continual learning settings, and is demonstrated to avoid catastrophic forgetting.\n\n############## Strengths ##############\n\n1. The idea to use hashing to determine routing paths is novel and could be interesting to the general deep learning community.\n2. This is to my knowledge the first mechanism that enables learning routing networks for lifelong learning, which is certainly relevant.\n3. Code is included for reproducibility.\n\n############## Weaknesses ##############\n\n1. The obtained results show that, while catastrophic forgetting is avoided, this comes at the cost of a model that is too inflexible to handle new tasks.\n2. The proposed algorithm hinges too heavily on heuristics, including counters, thresholds, and manually deciding when new units should be added. While this is not necessarily wrong, it would require substantial experimentation and validation that each component is necessary to achieving the desired behavior.\n3. The paper is hard to follow and so it makes it tough to assess exactly how the different parts of the proposed method are implemented.\n\n############## Recommendation ##############\n\nUnfortunately, at this time I have to recommend this paper for rejection. I think a future iteration of this work could be interesting to the community if either much of the heuristics were replaced by more well-founded techniques, or each of the heuristics was appropriately analyzed in terms of its empirical effect. Moreover, the paper should provide other views of the obtained results, including final overall accuracy, which is omitted.\n\n############## Arguments ##############\n\nThe key idea studied in this work is how to leverage hashing to decide different routes in a routing net in a continual learning setting. I believe this to be a powerful and interesting idea, but I am of the opinion that the proposed approach uses too many heuristic tricks, and therefore makes it hard to assess the benefits of the key contribution. In particular, the method uses a threshold (1) on the projection residual magnitude to decide when to stop chaining, a threshold (2) on the projection magnitude to decide when to pick a random, un-initialized unit, another threshold (3) on the projection magnitude to decide when to add a new basis vector to the chosen unit, and a final threshold (4) on the age of a basis to choose when to update it. While it is possible that such a heuristic method is the right way to go, it warrants a much more comprehensive evaluation that enables us to understand what the contribution of each of the aspects is to overall performance.\n\nOn the other hand, it seems that the obtained results are not very strong, and so I didn't find enough evidence in the paper to convince me that the proposed method is appropriate for continual learning. In particular, it seems that the cost of avoiding forgetting is to almost entirely prevent the model from adapting to new tasks (e.g., Table 1-top). It would be much easier to evaluate these results if the authors provided other views into their results. For example, the authors could show accuracy evaluation of all tasks throughout the training process, average final accuracy of all tasks, average forgetting across all tasks. \n\nThe other major point I have is the lack of clarity in the paper. Section 2 is supposed to provide background for feature hashing. Ideally, the reader should finish reading this section and come out with an understanding of what the point of hashing is in general, how it's computed, and how it will be useful for the provided approach. Instead, this section gives a few mathematical properties of feature hashing, with no intuition or high-level description of it or how it will be used. It seems like the rest of the paper similarly doesn't really give an intuition for why hashing might be useful, other than the fact that it gives a constant-size dimensionality to the hashed features.  The manuscript places emphasis on the fact that inner product is maintained under hashing, so I suppose the point is that distance metrics can be computed in the hashed space, which is used to compute similarity. I believe this fact is never explicitly stated, even though it is a key point of the proposed approach.\n\nI'm also surprised that the paper did not provide a comparison to stronger lifelong baselines, like those based on experience replay, which have been shown to vastly outperform regularization-based methods in recent work (e.g., Lopez-Paz & Ranzato). \n\n############## Additional feedback ##############\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nAbstract\n- The abstract mentions that the method is potentially useful for unsupervised or reinforcement learning unlike prior approaches, but this is never tested. Why should the reader believe this to be the case?\n\nIntro\n- Some paragraphs are spaced and some aren't. The rest of the paper has un-spaced paragraphs only. Please be sure to use consistent formatting.\n- What are \"stable features\", and how are they useful for unsupervised learning? The manuscript states that the proposed approach would find such stable features, but I didn't see any evidence of this in the evaluation.\n- The paper initially says that the method will be trained only with gradient descent, but then it mentions regularization techniques. Would it be fair to say that EWC is also trained only with gradient descent, since it only adds a regularization term and then uses the penalized objective for gradient descent?\n- If the three benchmarks are MNIST, Fashion/MNIST, and SVHN/incremental-Cifar100, then the text shouldn't say that benchmarks \"include\" those three, but rather that those three \"are\" the benchmarks used.\n\nSec 2\n- I recommend using `` '' for quotation marks\n- This section focuses on providing some theoretical properties of feature hashing, but no good intuition about what it's doing, how it's computed, or how it will be useful for the approach.\n    - What's N in the summation? In Weinberger et al., there's no N. It seems to be j=1^N where h(j)=i.\n    - Need to define \\sigma.\n\nSec 3.1\n- Elements of the proposed architecture are introduced, but we're still given no intuition of what they'll be useful for. This section should guide the reader by explaining from the beginning what the purpose of each piece of the architecture will be.\n\nSec 3.2.1\n- Why are feature maps of vanilla CNNs growing in size?\n- What is the i index? It seems to be the selected unit. Oddly, in Sec 3.1 k was the selected unit, but here it seems that k is the operation number in the chain of operations.\n- Does the set subtraction in Eq. 4 mean that the units used so far cannot be used for the subsequent operations? This should be stated in text.\n- Why is there no mention of how the bases are initialized yet?\n\nSec 3.2.2\n- Where is there an inner product between hashes computed for similarity? Is this the inner product with the basis? We can't know that at this point, since we have no information about how the bases are created.\n- Overall, this section could clarify a lot of things like why is orthogonality important in this context, why the residuals are chosen. It attempts to do so, but leaves me somewhat confused still.\n    - This statement: \"the orthogonal subspace’s contribution to total variance is much more important than that of B_k\" could be explained in more detail. Does this mean that, since hashes are similar, there is not much variance across projections and so the residual contains more information? \n- Why are residuals encouraged to be sparse?\n\nSec 3.3.1\n- basis --> bases (plurals)\n- Using two separate thresholds on the projection magnitude seems to introduce too many hyperparameters. It seems odd to use projection magnitude simultaneously for 1) choosing the current unit, 2) choosing whether to initialize a new unit, and 3) choosing when to expand a unit's basis projection. Why do this?\n\nSec 3.4\n- Scalability depends on manually adding units. Would it be possible to come up with an automatic way to do this? This section states that it should be possible, but I think a significant contribution of this work could come from designing a technique for it.\n- This manual addition seems to go against the stated goal of keeping data scientist involvement to a minimum.\n\nSec 5\n- Is EWC also allowed a task-specific classifier? A fair evaluation would give EWC a task-specific classifier like other methods get.\n- The description of the experimental setup is fairly complete, including details about when units were added, and which data sets were \"semantically different\"\n- Table 1 uses \",\" instead of \".\" for decimals. The \"%\" sign is missing.\n- The fact that catastrophic forgetting is avoided at the cost of lower overall performance suggests that the stability-plasticity trade-off was not well chosen: there is too much stability at the cost of very little flexibility.\n- We don't get to see average performance to accurately assess performance in Table 1.\n    - Manually computing it for SVHN/CIFAR-100 gives 55.46 (HRN) - 58.93 (ELL).\n- How were parameters of baselines chosen? It's odd that EWC neither retains performance on earlier tasks nor adapts to new tasks. This suggests a very poor choice of \\lambda.\n- It's nice to see unit usage plots, but these don't really show substantial differences in usage across tasks: all bars except the first look about the same.\n- The hyperparameters and ablation evaluation does not seem to add much value. Here, I would like to see a much more comprehensive ablation study that goes in depth into the effect of the different parts of the proposed algorithm.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper has values in the exploration of data hashing for neural networks. At the same time it needs more evidence make sense the motivation.",
            "review": "##########################################################################\n\nSummary:\n\nThis paper studies the problem of continual learning and proposes a new learning framework named hash-routed convolutional neural networks (HRN). HRN has a set of convolutional units and hashes similar data to the same unit. With this design, the paper claims three key contributions. (1) HRN provides excellent plasticity and more stable features. (2) HRN achieves excellent performance on a variety of benchmarks. (3) HRN can be used for unsupervised or reinforcement learning. \n\n\n##########################################################################\n\nReasons for score: \n\nOverall, I like the scope of this paper that studies continual learning. The experiments in Figure 2 verify that the proposed HRN outperforms a number of baselines on incremental-Cifar100 dataset. However, I am not sure whether the dataset makes sense for benchmarking continual learning. It seems the authors split the 100 classes into 10 groups, with each group having 10 classes. The 10 groups and corresponding labels serve as 10 distinct tasks for evaluating continual learning. Such formulation is odd that leads to decreasing accuracy scores as shown in Figure 2. I cannot find a real-world application that can benefit from such formulation due to decreased accuracy scores. It will be good if the paper can clarity the above points to strengthen its motivation. \n\n##########################################################################\n\nPros:\n\n1. The paper proposes a novel neural network HRN for continual learning. HRN leverages multiple units of CNN and hashes similar data to the same unit for training.  \n\n2. The proposed HRN achieves impressive performance when compared with a selected set of baselines. HRN consistently shows more robust accuracy scores on three datasets, as shown in Figure 2 and Table 1. \n\n\n##########################################################################\n\nCons: \n\nThe most important point relates to the motivation of problem formulation. The paper splits a 100-classes dataset into 10 10-classes datasets. Any two datasets have no overlapping in the classes. Under such formulation, a model suffers from decreased accuracy score as shown in Figure 2. It is necessary to explain why decreased scores are appealing in practice given they are the base of the concerned continual learning. \n \nAccording to Figure 2 and Table 1, all models suffer from decreased accuracy scores. Besides, the accuracy scores decrease monotonously regarding the task ids. Task id should not be a factor of performance. It will be good if the paper can provide some explanations. All existing machine learning researchers seek better accuracy scores so the decreased accuracy scores seem unusual. \n \nIt may not be fair to compare baselines with HRN that exhibits a larger model size. HRN leverages multiple (e.g. 6 in figure 2) units of CNN. To be fair, the paper should use the same number of units (or equivalently ensembles) for a baseline method. It will be good if the paper can run some experiments or show a comparison on model sizes. \n\n\n##########################################################################\n\nQuestions during rebuttal: \n\nIt will be nice if the authors can add experiments or discussions related to decreased accuracy scores. It will be perfect if the authors can show real applications of the proposed continual learning with decreased accuracy scores. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}