{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies the training of multi-branch networks, i.e., networks formed by linearly combining multiple disjoint branches of the same architecture.  The four reviewers seem to reach a consensus that the paper is not ready for publication for ICLR. "
    },
    "Reviews": [
        {
            "title": "Good intuition, not convincing experiments",
            "review": "##### Summary\nThis paper proposes to scale down the features of multiple branch networks by $1/\\sqrt{C}$ during aggregation where $C$ is the number of branches. The authors provide some theoretical insights to back up the method. However, the experiments are not very convincing. I had a good impression of this paper when I read the method sections but until the experiment sections. There are many problems in the experimental setup that makes me hesitate to accept this paper. The authors have a very unclear definition of the number of branches which gives me the impression that the authors just introduce a hyperparameter $\\alpha = 1/\\sqrt{C}$ and they can pick whatever $\\alpha$ or $C$ they want in the experiments tailored for the story.\n\n##### Strength\n- The arguments and theory in sections 3 & 4 are sound and interesting.\n- The proposed method is quite simple.\n- The proposed method leads to improvements on CIFAR 10 & 100.\n- By tuning the model architecture with the proposed method, the authors also show improvements on IWSLT 14 & nestest2014.\n\n\n##### Weaknesses\n\n- This paper is lack of comparisons with a series of related works that sets the outputs of residual branches to zeros such as FixUp (Zhang et al., 2019) and Re-Zero (Bachlechner et al., 2020).\n- It is very suspicious that the authors do not report the BLEU (+STAM) of the first row in Tables 2 & 3. It gives me the impression that the authors try to hide the fact that the proposed method does not do well in this setting and they have to tune the model structure and training settings to find some cases that the proposed method works.\n- In Table 3, the baseline (Ott et al., 2018) is trained for about 43 epochs. In my experience, it starts to overfit after that. However, the authors train all their models for 150 epochs. Otherwise, it is possible that the Transformers without STAM BLEU converge faster and overfit at epoch 150, which makes them look worse than the ones with STAM. Based on Appendix D, the authors also use a new technique head-level dropout which was not used by Ott et al., 2018. I would like to see the authors use the same setting as the baseline or report curve of BLEU scores at different epochs to convince me that this is not the case.\n- The author set $\\alpha = 1/\\sqrt{C/8}$ for Transformers and $\\alpha = 1/\\sqrt{C/4}$ for ResNeXt and just provide some very hand waiving explanation, which gives an impression that the theory doesn't work and all we need is to tune a hyperparameter $\\alpha$.\n- On page 6, \"We give an upper bound on the forward process when the softmax behaves more like a “max” operator, which is usually the case after initial training.\" This is a false claim. It is more often that the attentions assign large weights to multiple positions. Making the attention sparse usually restricts the models and leads to worse results.\n- The claim in Appendix C.3 makes the authors' definition of $C$ very arbitrary. The author claim that the 2-layer FFN with $d_{fc} = 4d$ in a Transformer-big has 4 branches. I don't see why there is a reason that this should view as having four branches. This should be considered as having only one branch only. Otherwise, since it has only 2 layers, we can view each hidden neuron in the middle layer as one branch. The authors just take their advantage to group ever $d$ neurons as one branch, but some other people can also group them in any other ways and say there are $C$ branches take doing them arbitrarily. For example, I can group ever 4 neurons as one branch and say this FFN has $d$ branches, or even group the first $2d$ dimensions as one branch and the rest of the neurons as $2d$ branches and say this FFN has $2d + 1$ branch. Overall, this makes me feel that the authors are playing a game of definitions and choosing whatever $C$ or $\\alpha$ as they want.\n\n\n##### Questions & Suggestions\n- How does STAM work when applied to depthwise separable convolution in MobileNets and EfficientNets which can be viewed as an extreme case of multi-branch convolution.\n- It would be more exciting to see if this method can be applied to the field of neural architecture search (NAS) where people explicitly train a gigantic multi-branch network and has problems with having some branches undertrained.\n\n##### References\n- Zhang, Hongyi, Yann N. Dauphin, and Tengyu Ma. \"Fixup initialization: Residual learning without normalization.\" arXiv preprint arXiv:1901.09321 (2019).\n- Bachlechner, Thomas, et al. \"Rezero is all you need: Fast convergence at large depth.\" arXiv preprint arXiv:2003.04887 (2020).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "[Summary]\n\nThis paper studies the training of multi-branch networks, i.e. networks formed by linearly combining multiple disjoint branches of the same architecture. The core contribution in this paper is the “STAM” aggregation rule which is to set the combination coefficient to $1/\\sqrt{C}$ for a network with $C$ branches. This aggregation rule is justified by (1) theoretical analysis on the function values and gradient norms at initialization, and (2) experiments on residual networks and transformers showing that this rule performs better than the baseline rules (such as sum or average).\n\n[Pros]\n\n--- The problem this paper focuses on (multi-branch architectures) is important and I think worth more studies since they are used more widely these days. From a theoretical perspective, the number of branches seems like a similar thing as the width of a network, yet the understanding is not yet as comprehensive as our understanding on the width (for which we know things like what are their effects, how should we initialize a wide network, etc).\n\n--- The experimental results are interesting and could be a good reference point for designing these multi-branch architectures. Specifically, it is interesting to see it is better to explicitly do a fixed $1/\\sqrt{C}$ scaling, rather than making it trainable or embedding it into the individual linear layers in each block. \n\n--- The paper is generally clearly written and easy to follow. The notation between the theory and empirical parts matched quite well.\n\n[Cons]\n\n--- I feel like the core contribution of the $1/\\sqrt{C}$ scaling rule is rather straightforward and not necessarily justified as a new method or theory, especially with the analogy to width in mind: It is standard to initialize weight matrices to have scale $1/\\sqrt{d}$ where $d$ is the either the input or output dimension as this gives the right normalization. Even with the other “baselines” (making the $1/\\sqrt{C}$ trainable or embedding it into the individual blocks) performing worse according to this paper, I still feel like a researcher or practitioner who encounters the issue of choosing the aggregation rule could still end up with the same solution proposed in this paper. \n\n--- The theoretical results also build on the exact same normalization idea ($1/\\sqrt{C}$ scaling gives the right normalization) as for width, and do not seem to provide us with much new theoretical understandings for what is unique about the multi-branch architecture. From my (maybe biased) perspective, I would have been more interested in some more in-depth understandings (either theoretical or empirical), such as what is the effect of adding more branches (and where does diminishing return kick in). Reading this paper leaves me more questions than answers about these multi-branch networks.\n\n------\nAfter rebuttal: Thank the authors for the response. I think my original concern largely stands and would like to keep my original evaluation of the paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Borderline paper",
            "review": "This study focuses on the stability of multi-branch networks. It analyzes the forward and backward stability of multi-branch network, and builds the relations with some widely-adopted initialization and normalization schemes. A simple new aggregation method is proposed that enjoys better stability than the sum and average aggregations. The method is extended to the multi-head attention layer in Transformer. Experiments on image classification and machine translation tasks using ResNeXt and Transformer are conduced to show the efficacy. \n\nPros:\nThe paper is well-written. Its motivation is clear and the analyses are technically correct. The proposed method is well interpretable and easy to follow. The connections with some initialization and normalization methods are well explained. \n\nCons & questions:\n1.\tThe paper should offer the definition of “stability” first. Does it refer to the numerical stability of features or the robustness to noise? What does $\\epsilon$ in Eq (3) mean? Is it the perturbation to the input feature $h_{in}$? \n2.\tHow to quantize stability? The experiments on image classification and machine translation show that the proposed method STAM is able to improve classification accuracy and BLEU score. I think these metrics are more related to representation ability. How do we know the stability is improved? \n3.\tThe extendibility is limited. The analyses are based on the assumption that all branches have the same structure without normalization layers. Does it work for network with variant branches? How to generalize this study to network with BN layers? What about the results of experiment 5.1 when BN layers are enabled? Since BN layers have been successful in solving the training difficulties of deep networks (not only multi-branch), could the authors explain the superiority of this study over BN in practical implementation? Besides, are the analyses and proposed methods also applicable to concatenation aggregation? \n4.\tThe improvements on CIFAR-10 and CIFAR-100 over baseline are insignificant. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper analyzing multi-branch neural networks.",
            "review": "This paper studies the multi-branch neural networks. It analyzes the feedforward/backward dynamics of multi-branch networks, proposes an aggregation design named STAM, and shows the feedforward/backward dynamics of multi-head attention layers, and discusses some of its properties.\n\nHere are some issues that might need the authors’ attention.\n\n1. In the introduction, the paper states that simply adding branches multiple times does not often work as expected. I am aware that there are experimental results in the later sections that support this argument. But putting it in the introduction without any concrete examples might be confusing.\n\n2. I think it would also be helpful if there are some brief descriptions of what the proposed STAM does and how it improves over other designs in the introduction.\n\n3. The paper seems to lack a strong main theme and all the contributions and results are loosely connected.\n\n4. The theoretical results are mostly based on the assumption that the weights are randomly initialized. However, the distributions and the properties of the weights might be different after the training begins. Does this imply that the significance of some of the results is limited?\n\n5. The stability analysis is mostly done on norm analysis. The proposed STAM is also inspired by the norm analysis where the goal is to keep the norm of activations and gradients within a reasonable range. However, it seems to me that the theoretical results are a little bit weak: it’s not hard to imagine that the relationship between norms of activations/gradients and the norm of $\\alpha$ is somewhat linear, and sum/mean becomes improper when the number of branches grows.\n\n6. Missing words in paragraph 5 of section 3.2: … as the number of branches (see Section 5.2).\n\n7. The aggregation form that the paper has studied is the weighted sum of multiple branches. The paper claims that concatenation is equivalent to sum aggregation for ResNeXt. I’m not fully convinced that this is true considering it’s impractical for a sum aggregation to perform matrix multiplication, batch normalization, and optional ReLU. And I don’t see an easy way to absorb the last 1x1 conv+bn into the branches while satisfying the assumptions of the theoretical results, plus bn will also cancel the effect of $\\alpha$ if my understanding is correct.\n\n8. ResNeXt uses the batch normalization layers, which automatically adjust $\\alpha$. How is this related to the proposed STAM? Do they serve similar purposes?\n\nSummary:\nThe paper presents some interesting findings. However, it seems lacking a strong main theme and the novelty/significance of the theoretical results is not strong, either. The proposed STAM only provides incremental design changes to the networks.\n\nUpdates:\nThanks for the authors' response and the revisions. However, the paper still lacks strong and significant results. Therefore, I keep my previous rating. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}