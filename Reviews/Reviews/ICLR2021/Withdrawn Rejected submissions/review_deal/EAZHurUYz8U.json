{
    "Decision": "",
    "Reviews": [
        {
            "title": "A novel training pipeline, but experiments can be improved",
            "review": "This paper proposes an orthogonal over-parameterized training (OPT) framework for neural networks. The motivation of this framework is to minimize the hyperspherical energy to diversify the neurons on a hypersphere, so as to enable better generalization of the network. The idea is simple that OPT fixes the randomly initialized weights of the neuron and learns an orthogonal matrix R that mutiplies to those new neurons. To maintain the orthogonality of the matrix R during the training, several approaches can be adopted, such as unrolling orthogonalization algorithms, orthogonal parameterization and orthogonality-preserving gradient descent. To make it scalable, the author proposes a stochastic version of the training pipeline (S-OPT), in which the some dimensions of the neurons in the same layer are selected and construct a small orthogonal matrix to transform these dimensions together. Such a stochastic algorithm is also shown to preserve the hyperspherical energy of the neurons. Empirical studies show that OPT can improve several neural network baselines (multilayer perceptron, convolution networks) on different datasets (CIFAR-100, Imaginet).  \n\nOverall, the contribution of this paper is to propose a novel training pipeline that can preserve the orthogonality fo the network, which seems to improve the generalization.\n\nHere are some comments:\n\n1. Why do you list methods from section 3.3 to 3.5. They seem to be the classical algorithms in the literature but not the even used in the experiment. If my understanding is correct, what is used is the formulation in sec 3.6, which is an unconstraint optimization problem and are much easier to solve.\n\n2. In the experiments, the baselines are not very strong. \n\ta) only a 6-layer or 9-layer CNN is used for CIFAR-100, and the results are very far away from state-of-the-art. \n\tb) Also, only a 3-layer MLP is adopted for MNIST.\n\tc) the imagenet error rate is also quite high.\n\nAlthough I donâ€™t think it is necessary to pursue state-of-the-art to show the value of the proposed method, I believe using relatively up-to-date neural networks to show improvement is more convincing.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review: Interesting idea, but some concerns. ",
            "review": "### Summary \n**Objective:** improve generalization of neural networks by attaining small hyperspherical energy. \n\n**Approach:** initialize neurons so hyperspherical energy is provably low in a probabilistic sense. Then, optimize an orthogonal transformation of the neurons weights, instead of optimizing the weights of the neurons. The orthogonal transformation does not change hyperspherical energy, so the trained network retains the same low hyperspherical energy it had at initialization. \n\n### Strengths\n**[+]** Optimizing an orthogonal transformation instead of neuron weights is interesting.\n\n**[+]** Experiments seem to indicate consistent improvements in generalization, relative to baseline networks and networks with orthogonal weights (see Table 4). \n\n**[+]** Using orthogonal matrices to preserve lower initial hyperspherical energy is quite clever. \n\n### Weaknesses\n**[-]** Writing. It was hard for me to understand the key points of the paper. \n\n**[-]** The article spends 1.5 pages covering previous work in section 3 which presents their work.  \n\n**[-]** It seems to me there are a few statements that are misleading bordering to wrong (not anything relating to main idea), see Concern 1 and Concern 3. \n\n**[-]** Over-parameterized is defined differently in the referenced work, e.g., [1, 42]. It think the paper would be more clear if such differences were clarified, especially since 'over-parameterized' is in the title of the article. Please note that [1] clarify what they mean by over-parameterization in their abstract and [42] clarify what they mean by over-parameterization in the second line of their introduction.\n\n**[-]** For fully connected networks the time of evaluating a layer increase from $O(n^2 \\cdot \\text{batch size})$ to $O(n^3)$. Typically $n>>\\text{batch size}$. This seems to be a big drawback. While section 4 attempts to address this issue, I do not believe the issue is properly explained and clarified for the reader. \n\n### Recommendation: Leaning towards rejection 5\n\n**[+]** Optimizing an orthogonal transformation instead of neuron weights is interesting (main reason I did not give 4).  \n\n**[-]** Writing. I think there is a lot of opportunity to help future readers of this paper. \n\n**[-]** For fully connected networks the time of evaluating a layer increase from $O(n^2 \\cdot \\text{batch size})$ to $O(n^3)$. Typically $n>>\\text{batch size}$. This seems to be a big drawback. While section 4 attempts to address this issue, I do not believe the issue is properly explained and clarified for the reader. \n\nI am open to changing my recommendation if the authors address the questions/concerns below. That said, I'll first re-evaluate my opinion based on the comments from the other reviews. It is likely that I missed things the other reviewers caught. \n\n### Questions and Concerns\n\nQuestion 1. How is over-parameterization defined in [1, 12, 16, 31, 42], how do the definitions differ, and how would the authors define over-parameterization? \n\nThe article cite [1, 12, 16, 31, 42] which contain different definitions of 'over-parameterization'. For example: \n\n[1] defines over-parameterization when network width is polynomially large in the number of layers L and the number of samples n. \n\n[42] defines over-parameterization as \"the technique of using more parameters than statistically necessary\". \n\nBy the definition in [1] OPT is not over-parameterized, but by the definition in [42] OPT would typically be over-parameterized. You later write \"In this light, OPT also introduces over-parameterization to each neuron ... \". This is true/false depending on which definition of over-parameterized you are using. \n\nI apologize if I am misunderstanding previous work. If that is the case, perhaps it would be a good idea to clarify this in the related work. Alternatively, one might attempt to make such statements more precise. \n\nConcern 1. You write that \"Givens rotations can also construct the orthogonal matrix, ..., which takes $O(n^2)$ complexity and is too costly\". The matrix-vector multiplication used in your OPT approach also take $O(n^2)$ time for each neuron, so I do not understand why $O(n^2)$ is too costly. I think the main issue with Givens rotations is lack of parallelization and not time complexity. \n\nIf I am wrong, please explain why, if not, please explain how you will clarify the issue. \n\nConcern 2. In section 4 you write \"If the dimension of neurons become is extremely large, then the orthogonal matrix to transform the neurons will also be large. Therefore, it may take large GPU memory and time to train the neural network with the original OPT. \"\nFirst of, I think you need to remove \"is\" or \"become\" from this sentence. Second, in fully connected networks, the memory requirements increase from $n^2$ to $n^2+n(n-1)/2<2n^2$ while time consumption increase from $O(n^2)$ to $O(n^3)$ if you have $n$ input/output neurons. I don't think this is clear from context and it could be clarified. \n\nConcern 3. Consider, for example, the sentence \"... , all OPT variants maintain minimal energy in training. \". To me, this sounds like OPT attains the global minimal hyperspherical energy, which, in general, is false. To me, this seems to be overselling the method a bit. This happens a few times through-out the article, and I recommend you make such statements more precise, e.g., \" ..., all OPT variants maintain much lower hypherspherical energy during training. \".  \n\nConcern 4.  In the introduction I think the wording \"strong interpretability\" is misleading and over claiming, I suggest to only write \"flexibility\". \n\nConcern 5. Sections 3.3 - 3.6 seems to cover related work, but is presented in the section describing your method. I think this could be removed, or moved to the related work section in a very condensed form. This would make it more clear what is your work and what is previous work. Finally, it seems to me that your key idea is minimization of hyperspherical energy by using orthogonal matrices, therefore, it doesn't seem important exactly how you use orthogonal matrices, just that it can be done without increasing training time too much. The details might even be removed to appendix. \n\n### Additional Feedback\n\nComment 1. It seems to me that uniformly sampling neurons as done in section 4 resembles dropout. If this is the case, I think you may be able to use column-row-sampling instead, which has provable guarantees wrt. Frobenious norm which dropout does not. \n\nComment 2. I think it is possible to shorten the abstract, and thus help future readers get a clear understanding of your idea. For example, the first sentence with inductive biases could be completely removed, so future readers get to the main point faster. I think the second sentence could be rephrased. \n\nComment 3. In the introduction you cite [15] two times ```[15,15,16,31,42]```. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review",
            "review": "***Summary***\nThe paper presents a model to keep the hyper-spherical energy of the linear layer of a network low. It argues that this is beneficial for the generalisation of the models. The authors show experiments to back up this assertion.\n\nThis paper explores the idea of fixing a matrix $A$ and optimising $Q\\in\\operatorname{SO}(n)$ in the expression $QA$, and use $QA$ as the linear kernel of a linear layer.\n\n***Comments***\n\nOn overparametrisation.\n\nI do not see how this method accounts for an overparametrisation of the layer. This paper substitutes a linear layer $n^2$ parameters with an orthogonal layer $n(n-1)/2$ parameters. At the same time, in section 6.3 the authors day that an OPT network has $\\times 5$ the parameters as the original network. Could the authors expand on this?\n\nOn the optimisation with orthogonal constraints.\n\nThe paper spends a page and a half out of the eight describing different optimisation methods for orthogonal constraints. This is completely tangential to the point of the paper, given that the paper does not introduce any new optimisation method. At the same time, as explained in appendix M, CP is the optimisation method that is most efficient and performs best. Why did the authors use GS in many of the experiments? Using just CP would be much more efficient and it would save them 1 page of explanations.\n\nOn the importance of the hyperspherical energy.\n\nThe paper is claims in several places that the hyperspherical energy \"improves generalisation\" in neural networks. The authors put their method at the level of other well known theoretical papers in section 5.2. On the other hand, I do not think that this comparison is valid as it is, as these other papers study very specific forms of neural networks. I think that, if the authors are going to make these claims, they should show how their method can be proven to have the same advantages as the cited ones via some corollary of their theories.\n\nOn the CNNs.\n\nThe exposition of the definition of hyperspherical energy is always done for linear layers, but almost all the experiments are performed in CNN networks. It is not very clear to me how does one define the HE for a tensor. I think that this should be explained in the main paper. Could the authors expand on this?\n\nOn the experiments.\n- In the first experiment the authors claim that doing OPT with GS works better than doing unconstrained optimisation. Could the authors explain this?\n- We see the application of OPT to Few-shot learning, improving over a previous experiment. On the other hand, we see that the improvement is marginal, and the variance says that it is not significant. It is not clear whether the little improvement gained over the initial method comes from the framework or simply from having a model with multiple times the parameters of the original model.\n- In the experiment \"Fixed weights vs. learnable weights\" the authors claim that this proves that OPT is beneficial to generalisation, without providing the any sort of validation loss!\n\n***Conclusion:***\nThis paper proposes a framework where linear layers are reparametrised via a fixed matrix and an orthogonal trainable matrix. It is not clear to me how to apply this to CNNs or how this is an overparametrised network. In general, I do not think that this is going to be a practical method that can be used in practice. In particular, I do not like that the experiments just get a marginal improvement over their baselines while using 4-5 times the parameters. S-OPT comes to solve this, but it is just evaluated in one experiment, when it the authors could have added it as an extra experiment in every of their previous experiments.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}