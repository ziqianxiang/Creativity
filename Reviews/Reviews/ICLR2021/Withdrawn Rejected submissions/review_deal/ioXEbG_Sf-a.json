{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed a new experience replay approach, applicable to deep RL methods. Two reviewers suggested acceptance and two did rejection. The first negative reviewer R1 raised a concern on continuous vs. discrete issue, but AC thinks that the authors' response is not fully convincing enough.  The second negative reviewer R2 pointed out that the reported performance of SAC is poor compared to the existing implementation (although authors claim a different set of hyperparameters is used), which AC thinks is a critical weakness to judge the value of the experiments. Two other positive reviewers (even R4) shows mixed opinions. Overall, AC thinks this is a borderline paper, a bit toward rejection."
    },
    "Reviews": [
        {
            "title": "Good paper with a few points not clearly described",
            "review": "This paper is on an experience replay approach, as applied to deep RL methods, that uses a density ratio between on-policy and off-policy experiences as the prioritization weights. The objective is to find appropriate bias-variance trade-offs for importance sampling from the replay buffer. In particular, there's the bias issue from replay experiences of other policies, and the variance issue from the recent on-policy experiences. \n\nIt's not entirely clear to the reviewer about the necessity of maintaining two replay buffers (slow and fast). Instead of maintaining two replay buffers, it seems one can simply retain the standard single-buffer strategy, and evaluate how likely the experience is with respect to the current policy. Then the likelihood can be used as the weight for prioritized experience replay. This simple strategy also takes the bias-variance trade-offs coming from on-policy and off-policy experiences. The reviewer is curious about the advantage of the developed approach that uses two buffers over what's described above. \n\nIn the paper, the slow buffer is considered for maintaining off-policy experience, and the fast buffer is for on-policy experience. Accordingly, the sizes of those two buffers are supposed to be very important parameters. For instance, if the two buffers have similar sizes, then the developed approach are expected to function like standard deep RL (on policy or off policy depending on the buffer size). However, Figure 2 (b) shows that the performance is not sensitive to the size of the fast (on-policy) buffer. The result is counter-intuitive, though it's clear that the results are supposed to show the insensitivity to such parameters. \n\nExperiments were conducted by combining the experience replay approach (called LFIW, likelihood-free importance weighting) with three existing deep actor-critic methods, and then comparing the combinations with their originals. The results look good, and demonstrate the effectiveness of the developed approach. It's unclear why the results were presented in tables instead of curves (minor point), which can be potentially more better for readability. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learnable re-weighting of samples for actor-critic algorithms",
            "review": "The paper proposes a generally applicable modification to experience sampling in the context of actor-critic algorithms using a Q function as a critic. The modification is called \"Likelihood-free Importance Weights\" (LFIW). The authors describe the approach in Appendix A in the form of pseudocode.  Comparing to a generic actor-critic algorithm, the changes include the keeping of two replay buffers (\"fast\" and \"slow\") and inclusion of an additional re-weighting function w which in turn is used in the update of the Q function. The paper includes a thorough performance comparison on MuJoCo and DM Control Suite. \n\nThe results are good, but the authors seem to use a weak implementation of SAC. For comparison, I am referring to SAC as implemented in https://github.com/tensorflow/agents/blob/v0.6.0/tf_agents/agents/sac/sac_agent.py#L62-L634  Using this implementation I am getting, e.g. for the Humanoid-v2 environment,  at 500K steps results above 4000, compared to 3189 (SAC+LFIW) and 2033 (SAC) reported in the paper. Hence it is hard for me to assess whether LFIW offers a real improvement of the SOTA or perhaps fixes some problems of the underlying implementation of SAC.\n\nThe mathematical analysis contained in Theorem 1 is interesting, but in my opinion, it is written confusingly. It would be better to decompose it into two separate statements:\n- the first statement stating the inequality with a simple proof based on the convexity of the square function,\n- the second statement proposing a counter-example in the form of Q+epsilon for appropriately small epsilon. \n\nAlso, the statement of the theorem is slightly weaker than I would like: can we just prove, that the counterexample exists regardless of gamma? The current statement says that the mapping is not a gamma-contraction, but one can imagine, that relaxing gamma would still lead to a contractive mapping.\n\nThe example presented in Figure 1 is interesting, though in my opinion, somewhat detached from the main focus of the paper which in my opinion is the analysis of Algorithm 1. Also, the three-state MDP may seem too simple to conclude about the performance of the sampling method.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Mixed Impressions, early problems with related work but overall appealing approach",
            "review": "The authors propose likelihood-free importance weights applied to\nexperience replay buffers in reinforcement learning. When the sampling\ndistribution is not the on-policy distribution the RL agent may learn\na biased estimate. This could happen in a number of ways, but two\nobvious ones are data being generated off-policy or prioritized\nexperience replay (PER). In both cases the sampling distribution is\nnot the one we wish to minimize over. In PER this is partly fixed by\nthe importance sampling correction. This work replaces this\ncorrection, in the off-policy setting, with a learned ratio.\n\nAlthough exposition and some of the early details about related work\nand background seem problematic, the experimental results are strong\nand encouraging. The fundamental idea is broadly applicable, although\nsome of the related work touches on related ideas this particular\napplication of the idea appears novel and potentially impactful.\n\n-- Pros:\n\nBroadly useful technique for LFIW, and specific application using the\ntwo replay buffers is quite nice.\nRelatively strong empirical results\n\n-- Cons:\n\nIssues with the background / related work (see comments below)\nExperiments could be presented and discussed more clearly\n\n-- Comments:\n\"However, such a heuristic could be highly sub-optimal in actor-critic\nmethods...\"\nThis sentence and the rest of the paragraph seem to be missing\nsomething essential. Prioritized experience replay is an importance\nsampling technique. That is, while sampling under a biased\ndistribution, they use importance weights to correct for this sampling\nbias (only partly in the case of PER in practice). As well, it is not\njust actor-critic methods that require the expected loss to be under a\nparticular distribution over states.\n\nThis misunderstanding reappears just before section 3, where the\nauthors discuss PER as a heuristic for improved performance by\nchanging the expectation in Equation 2. This is, indeed, incorrect. In\nterms of Equation 2, we could accurately represent PER by saying that\nD is already reweighted by the priorities (which is how the authors\ncurrently interpret it), and further adding a reweighting where inside\nthe expectation we divide by something like p_D(s, a), to convert the\nexpectation into one over the uniform distribution. The most accurate\nway obviously would be the seperate these two out explicity: a\nsampling distribution (data generated by a behavior policy and put\ninto replay) and an importance sampling distribution (the prioritized\ndistribution), and multiplying inside the expectation by their ratio.\n\nIt is strange that these errors would crop up considering the primary\ncontribution of the work is about essentially doing this without\nexplicitly computing this ratio of likelihoods. The lack of correction\nre-appears in equation 3 as well\n\nFinally, this comes full circle just before section 3.1 with the\nstatement that the 'prioritized' distribution needs to be d^\\pi. Of\ncourse it has to be d^\\pi when you do not correctly correct for the\nsampling distribution.\nOverall, I don't want to attribute this to an actual\n'misunderstanding' by the authors, I think section 4 clearly shows the\ndepth of their understanding. So instead, I think of this as an issue\nwith the writing and how things are explained. However, how PER is\ncommunicated here does make me fairly wary about the empirical results\nfor PER.\n\nRegarding Theorem 1, one direction of this has been known for quite a\nlong while. That is, the contraction holds when the distribution is\nd^\\pi. So, it might be better to spend more time emphasizing the other\ndirection and discussing its proof.\n\n\"In actor-critic methods, the critic objective typically estimates the\nQ-value function\"\nArguably the most typical estimate for actor-critic would be the value\nfunction (V) not the action-value function (Q). Not that AC methods\ndon't ever use Q-values, but this doesn't seem like the typical case.\n\n\n-- Minor suggestions / Typos:\n\nPage 2, just above equation 1: \"based on Bellman equation Q\", missing a \"the\"\n\nFootnote 1 on page 3 should probably come earlier, during\npreliminaries as it is not specific to PER.\n\nPage 5, last paragraph, \"We observe that using the weights to correct\nthe policy updates does not demonstrate provide much...\" (remove\neither 'demonstrate' or 'provide')\n\nI struggled to find where the authors specify the f-divergence they use.\n\n-- Questions:\n\nDid you consider using your technique combined with prioritized\nreplay? This would mean changing the 'slow' replay buffer\nprobabilities to be prioritized probabilities, but otherwise would be\nidentical to what is currently shown in the paper. More specifically,\nalthough the authors compare against PER as though this is an\nalternative to PER, it seems more fitting to think of this as an\nalternative to the IS correction being used by PER (among many other\nmethods).\n\nThe authors report the accuracy of w_\\psi used as a discriminator,\nwhich is great because I was going to ask about this exactly before I\ngot to that part of the results. However, I wonder about a few\nadditional details here. Have you looked at how this\nprecision/accuracy varies over training? In particularly, you might\nexpect that it becomes nearly impossible to discriminate as the policy\nconvergences and thus the two distributions slowly become identical.\n\nAlso, how robust is performance to degradations in this accuracy? This\nseems like something you might have looked at. If so, do you have any\ninsights? The reported accuracy is low enough, with empirical\nperformance strong enough, that I suspect it *is* quite robust.\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "FA or tabular, continuous or discrete?",
            "review": "The paper proposed a method to address the distribution mismatch problem in off-policy learning. Its method is to estimate the density ratio between the stationary distribution under the target policy and the data distribution in the replay buffer, and then correct the state distribution with the ratio so that it can use experience from the replay as if they were generated from the target policy.\n\nAlthough I have many questions about the paper, the following two are most important currently and are hoped to be addressed first.\n\nIt seems not clear to me whether this paper considers the tabular case or the function approximation case. It wrote in section 5 that \"our goal is to improve performance of TD learning with function approximation\", but its preliminaries (Section 2), its example (Figure 1), and its main theoretical result (Theorem 1) are all for the tabular case. In fact, the state distribution mismatch only matters when multiple states share the same parameter. For the tabular case, as each state has its own parameter, there is no interference between them, and thus there is no need to make the state distribution on-policy.\n\nEmpirically, all the experiments are designed to solve continuous control problems, where both state and action are continuous. However, the density ratio d^\\pi(s, a) / d^D(s, a) is only defined under the discrete setting. In fact, because both state and action spaces are continuous, all elements in the replay buffers are different from others and this ratio makes no sense. Therefore this algorithm doesn't seem appropriate to me for continuous control tasks. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}