{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #1",
            "review": "This paper proposes a layout generation framework, named as LayoutTransformer Network (LT-Net). This work models implicit object/relation from textual inputs for producing semantics consistent yet diverse layout outputs. The proposed model utilizes Gaussian Mixture Model and co-attention mechanism to predict the output layout with relation guarantee. \n\nHowever, there are several weakness of this work. \n 1) The novelty is a little weak. The main contribution of the proposed LayoutTransformer Network is the Gaussian mixture model and co-attention mechanism. The novelty of these two parts is limited. The visual-textual co-attention is usually adopted on the text-to-image generation, e.g. AttnGAN, SD-GAN and text-to-image retrieval. The speciality and priority of this work, especially the key components, are supposed to be clarified clearly in the paper. \n\n2) Another weakness is the experiment. More analysis is suggested in the experiments. What's more, the generalization of the proposed module, i.e. GMM and co-attention is suggested to be evaluated by combing these modules with previous SOTAs.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not ready for publishing",
            "review": "##########################################################################\nSummary:\n \nThe paper introduces a new scene-graph to layout generation method. In order to model the input scene graph, previous approaches typically rely on graph convolutions. The proposed LT-Net leverages a transformer-based encoder instead, which sounds very interesting. The empirical evaluations also look good.\n\n##########################################################################\n\nReasons for score: \n\nCurrently, I vote for rejecting the paper. In general, I like the idea of using a transformer-based architecture for scene-graph modeling. The empirical results also look promising. But in its current state, I’m not sure if the paper is ready for ICLR: (1) the paper is not written very clearly (2) I’m not sure if the paper is using a conventional evaluation protocol. \n\n##########################################################################\n\nPros: \n\nThe idea of encoding an input scene-graph by a transformer is interesting;\n\nThe proposed method is technically sound;\n\nThe empirical results look good, especially on the mIoU metric which explicitly evaluates the layout structure.\n\n##########################################################################\n\nCons:\n\nThe authors are urged to revise the paper, especially the technical section, as it is hard to follow in its current state. For example, it is not clear to me what are F and H_p (sec. 3.1, Fig 2) until I read the appendix. The definitions of s_t and \\hat(s)_t in equation(1) are unclear. I don’t think c_t is mentioned in the main text until equation (2). It does show up in Fig. 2. Also what are the visual features used in VT-CAtt (sec. 3.4)? From the main text and the appendix, it seems they are just box coordinates? Overall, I think the presentation of the paper should be improved a lot before being accepted to ICLR.\n\n\nFrom sec. 4.3, it seems the method is not evaluated on the whole test sets of COCO and Visual Genome, but on some randomly selected subsets. On the one hand, I don’t think it is conventional practice, as, for example, in SG2IM and LayoutVAE, when evaluated on COCO, the whole official validation set was used as test set. On the other hand, as the image ids of these random subsets are not included in the submission. I’m not sure if it is possible to reproduce the results presented in the paper.\n\n##########################################################################\n\nMinor comments: \n\nI would suggest the authors revise the contribution section, as now it just repeats the individual technical components developed in the paper.\n\nThe authors are also encouraged to compare the proposed method with a more recent approach to this problem:\n\nPasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph (NeurIPS 2019). \n\nI believe the source code of this related work is public.\n\n\nFig 3: ouf → our\n\n#########################################################################\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for LayoutTransformer",
            "review": "**PAPER SUMMARY**\n\nThe paper presents a method that inputs a set of (subject, relation, object) tuples describing a scene, and outputs a layout for the scene that specifies a bounding-box location for each object. \nThe model consists of several major components:\n- A relation/object predictor that computes contextualized embeddings for all input tokens; this is a Transformer pretrained for masked \n language modeling\n- A layout generator that predicts initial box locations for each object, which models the position of each object as a Gaussian Mixture Model\n- A layout refiner that uses visual-textual co-attention (VT-CAtt) to refine the position of each bounding box using a Transformer\nExperiments are performed on the COCO-Stuff and Visual Genome datasets, where the model is compared with three prior approaches (SG2IM, LayoutVAE, and NDN).\n\n**STRENGTHS**\n- The paper is the first to my knowledge to apply Transformers to the task of scene graph to layout generation\n- Using GMMs to model the multimodality of box positions is a clear improvement over the unimodal distributions used by prior work\n\n**WEAKNESSES**\n- The paper repeatedly claims to operate on textual data; however this is misleading since in actuality it operates on scene graphs\n- The utility of inferring implicit relationships is not clear\n- Insufficient discussion of baselines\n- Though the model does outperform prior work at layout generation, the paper does not show that these improved layouts can be useful for the downstream task of image synthesis\n- The motivation behind some modeling decisions is unclear\n- Missing citations\n\n**TEXTUAL DATA VS SCENE GRAPHS**\n\nThe paper repeatedly claims to operate on “textual inputs” (e.g. page 1 paragraph 4, Figure 1, Section 3.1, etc). This is misleading: the model does not operate on unstructured text; instead it operates on highly structured (subject, relationship, object) tuples (Section 3.1), together with object IDs to link the presence of object instances across multiple tuples. Taken together, this data is a scene graph; and indeed Figure 7-12 in the Supplementary material visualize their “textual inputs” as scene graphs. The distinction between text and scene graphs is hugely important, since scene graphs require much more annotation effort to produce than text. The paper should not obfuscate the fact that it requires scene graphs during both training and inference.\n\n**IMPLICIT RELATIONSHIPS**\n\nOne claimed point of novelty is that the proposed method can model model “implicit relationships between objects”. This is not well-explained in the introduction, but in Section 4.2 it is made clear that this means that one or more elements of a (subject, relation, object) tuple can be masked and predicted by the model.\n\nThough the paper does demonstrate that the model can do a reasonable job of predicting these masked entries, it never motivates or explains why this would be a useful thing to do.\n\nFurthermore, though this particular form of predicting layouts from incomplete scene graphs may be novel, the general idea is not. Indeed, several of the baselines also predict layouts from incomplete information: LayoutVAE predicts layouts from label sets that do not contain any relationship information, and the main idea of NDN is to generate layouts from a sparse set of relationships. The paper does not discuss how their notion of modeling implicit relationships relates to any of this prior work.\n\n**BASELINES**\n\nThe paper compares to three baselines: SG2IM, LayoutVAE, and NDN. Each of these prior works tackle a strictly harder problem than this paper, which predicts a scene layout from a scene graph. In contrast:\n- SG2IM inputs a scene graph and predicts both a layout and an image (so it produces richer outputs than this paper)\n- LayoutVAE inputs a set of object labels and predicts both the number of instances of each category, as well as a scene layout (so it receives more impoverished inputs compared to this paper)\n- NDN inputs a partial scene graph and both completes the scene graph and predicts a layout (which is the most comparable to this paper)\nThe fact that these prior methods all tackle strictly more challenging problems should be more clearly discussed; furthermore the comparisons in Table 1 are not particularly fair, since all of the methods are trained to solve different tasks.\n\n**IMAGE GENERATION**\n\nThe paper motivates the task of layout generation since it is an intermediate task for some image generation methods. However, although Table 1 demonstrates that the proposed method outperforms prior methods on metrics used to assess layout generation (mIOU, R-Pre, Rel Acc), these do not translate to improvements in image generation -- for the end task of image generation, the proposed method is outperformed by SG2IM on COCO, and matches SG2IM on Visual Genome.\n\nAs an additional point, the paper never mentions how images are generated for this experiment -- Section 3 only discusses layout generation, and has no discussion of image generation.\n\n**MODELING DECISIONS**\n\nThe motivation behind some modeling decisions is unclear:\n\nWhy does the relation/object predictor treat the (subject, relation, object) tuples as an ordered sequence? Conceptually these tuples form a graph, and there is no reason why they should be serialized into a linear order (per the Sentence ID embeddings). How do you decide what order to present tuples to the model? Does changing the order result in different predictions?\n\nHow important is the $\\mathcal{L}_{KL}$ regularization term? This should be ablated.\n\nWhy is the relation/object predictor pretrained using a masked language modeling objective before being jointly finetuned with the rest of the model? Does this provide any benefit over training the entire model jointly from scratch? This should be ablated.\n\nWhy do you predict box disparities from relationship tokens? Are these disparities used at all during inference, or is their only purpose to enable the relationship consistency loss? What is the purpose of the relationship consistency loss? Table 2 shows that it improves performance, but why should this loss be beneficial? Is it basically serving as a form of regularization?\n\n**MISSING CITATIONS**\n\nThough they do not address the exact same problem as this paper, I think the paper should have cited and discussed other works on image synthesis from scene graphs and layouts, such as:\n- Zhao et al, “Image Generation from Layout”, CVPR 2019\n- Ashual and Wolf, “Specifying Object Attributes and Relations in Interactive Scene Generation”, ICCV 2019\n\n**MINOR POINTS**\n\nSection 2.1: “Nevertheless, most existing text-to-image methods only focus on nouns in the textual descriptions for synthesizing image outputs.” -- I’m not sure what the basis for this statement is, since there are many text-to-image methods that operate on COCO captions (such as StackGAN and StackGAN++) which mention more than nouns; also scene-graph-to-image methods like SG2IM also explicitly model relationships.\n\nSection 4.2 states: “Comparing with other state-of-the-art models such as Sg2Im, NDN, and LayoutVAE, our LT-Net is a generative model, and thus is capable of generating diverse yet plausible layouts given the same textual input.” -- This seems incorrect; all of these prior works are also generative models and could in theory produce different layouts for a given input; however in practice they tend to suffer from mode collapse, and don’t have much variety in their predicted layouts.\n\nTypos:\n- Second paragraph of Page 4: “calcualte” -> “calculate”\n- Section 3.3.1: What is $c_t$? Should it be $f_t$ instead?\n- Section 3.4: What is $C_{1:T}$?\n- Figure 3 caption: “ouf” -> “our”\n\n**OVERALL**\nOn the whole this paper is not ready for publication due to the shortcomings discussed above.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #4",
            "review": "**Summary Of Contributions**:\nThe authors propose a generative model of LT-Net for text conditioned layout generation. The model derives semantics-aware and contextual features given textual descriptions. This paper also introduces GMM to describe the layouts for each object with a consistent relation enforced and a co-attention module across textual and visual features to produce the final output. Finally, the paper presents experiments on both MS-COCO and Visual Genome (VG) datasets to demonstrate the effectiveness of the proposed model.\n\n **Strengths**:\nA) The paper is well-organized with motivation and approaches clearly stated and illustrated.\nB) Overall this paper is well written and the technical details are easy to follow.\nC) The text-to-image task is an important area of machine learning and computer vision.\nD) Generating layouts with Transformers should be an important mechanism for achieving better semantics-aware features.\n\n**Weaknesses**:\nI have some critical concerns about this work, which I expect the authors to address them in the rebuttal:\n\nA) This paper is missing essential related work, which is not mentioned at all. Here is some highly related work:\n\n[1] Justin Johnson, et al. Image generation from scene graphs. CVPR 2018. (firstly introduced in the Experiments section (sec. 4)).\n[2] Roei Herzig, et al. Learning canonical representations for scene graph to image generation, ECCV 2020.\n[3] Oron Ashual, et al. Specifying object attributes and relations in interactive scene generation, ICCV 2019.\n[4] Sun, W., et al. Image synthesis from reconfigurable layout and style. ICCV 2019.\n[5] Subarna Tripathi, et al. Using scene graph context to improve image generation.\n[6] Li, Y., et al. Pastegan: A semi-parametric method to generate image from scene graph. NeurIPS 2019. \n[7] Deng, Z., et al. Probabilistic neural programmed networks for scene generation. NeurIPS 2018.\n[8] Subarna Tripathi, et al. Heuristics for image generation from scene graphs. ICLRW 2019. \n[9] Schroeder, B., et al. Triplet-aware scene graph embeddings. ICCVW 2019.\n\nThe task of image generation from scene graphs [1,2,3] is highly relevant here since the paper converts the text into a triplet of <subject, relation, object> form. By definition, a set of triplets is a Scene Graph, and thus the authors should discuss thoroughly in the related work how the SG-to-Image task relates to their paper. Moreover, [2] improved image and layout generation in both the standard and packed scenes by enriching the SGs with inferred relations. This is very similar to the proposed inferring implicit relation from the textual inputs the authors introduced. Last, there are more updated works that are used to generate layouts (e.g., LostGAN[4], Grid2Im[3]), which are not mentioned.\n\nB) I am concerned with the limited novelty of the paper. As I mentioned before, [1] already proposed the approach of converting text into an SG (a set of <subject, relation, object> triplets) as an intermediate step for generating layouts. [2] also showed that enriching these triplets and transforming to a canonical scene graph (infrared relations) can give a much more robust layout generation. The authors should discuss and explain how their approach is different from the already existing related work.   \n\nC) The authors present an interesting idea of modeling the bounding boxes as GMM for layout generation, but they are not showing any ablations that motivate the usage of GMM. They need to show an ablation that shows the GMM is indeed useful and better than a simple baseline. From the ablation section and Table 2, it is unknown whether the GMM contributes to the general performance or not.\n\nD) The authors present the FID score in the experiments section, although their model does not generate any image. Thus, I wonder why and what it means to show an image generation metric if the proposed model does not generate an image.\n\nE) I could not find what the image resolution the authors used to generate the images is. Moreover, why do the authors compare themselves to image generation methods such as SG2IM and LayoutVAE?. In Table 1, it seems the authors compare apples to oranges since these methods contain different supervision and different input/output.\n\nF) Why do the authors compare themselves to image generation methods such as SG2IM and LayoutVAE?. In Table 1, it seems the authors compare apples to oranges since these methods contain different supervision and different input/output. Moreover, some of the related works, which I mentioned earlier, are missing to compare with (e.g., [2], [3], and more).\n\n**Suggestion To Authors**:\nA) This paper is well written and easy to follow.\nB) I could not find what the image resolution the authors used to generate the images is.\nC) The teaser figure (Figure 1) shows a generated image, although this paper does not propose any generation model.\n\n**Preliminary Rating Justification**:\nThis paper addresses the problem of generating layout from textual descriptions. The authors propose to predict from the input text an SG (a set of triplets of the form <subject, relation, object>), and by inferring implicit relation/object, they produced layouts from learned contextual features. Experiments show that the proposed approach improves the layout generation on the COCO and Visual Genome datasets.\n\nOverall, this idea is well-motivated and the paper is well written. However, this paper is missing substantial related work (e.g., [1,2] from above), and an informative discussion is missing. The discussion should explain why the proposed model is not compared with [1,2] and the differences between [1,2,3] and the proposed model. As long as the paper does not explain these issues in more depth, it limits the novelty. In addition, some additional justifications would further strengthen the paper. I am open to the authors' feedback and other reviewers’ opinions.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}