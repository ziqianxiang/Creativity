{
    "Decision": "",
    "Reviews": [
        {
            "title": "Better classification head for one shot object detection (OSOD)",
            "review": "The paper argues that OSOD suffers from a large number of false positive detections, and proposes a method to alleviate that problem. The methods aims at improving the classification head in a Faster R-CNN type detector. It improves the OSOD performance on MSCOCO by 1-2%.\n\n\nThe paper is not very well written and it is hard to follow the arguments in many places. Eg.\n\n\"In this paper, through the preliminary experiments, we find that when the classification branch does not introduce the reference feature information, it can cause more obvious performance degradation due to more false positives detected than the regression branch without the reference feature information.\" (and the rest of the paragraph)\n\nIt is not clear what is a reference feature here; it is a forward reference.\n\nThe exact problem setting is never given, a formal definition of the problem being addressed should be given in the beginning so that the problem being addressed is clear.\n\nThe end of Sec 3.1 says: \"(3) Our method can improve accuracy by reducing the number of false positives.\" -- while the method has not been introduced formally. Again a kind of a forward reference.\n\nThe architectures in and discussions around Fig1 are hard to understand (Sec 3.1) and more details should be given.\n\nThe method architecture is introduces in Sec 3.2, in Fig. 3, and if we look at figure 3, the caption is uninformative and there are a lot of forward references again.\n\n\"In the heads, we recalculate the similarity map for each proposed region, to be more specific, we utilize RoI-Align(10) on the reference feature and the query selected feature to obtain the pooling features with the same shape, and then propose the classification region deformation on the two pooling features to get a more effective classification comparison region.\" -- Sentences like these are quite dense and very hard to follow. These should be broken down into smaller sentences and each part should also be explained a bit more briefly.\n\nThe results obtained on the larger and more challenging COCO dataset are relatively less, ~1% compared to the self implemented baseline.\n\n\nOverall, I think the paper has some interesting elements, but the way it is written, it is very hard to appreciate.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "The paper proposes several improvements over the standard \"siamese\" few-shot object detection pipeline, focusing on trying to improve the post-RPN classification head performance reducing its false-positive rate. Mainly two improvements are advocated: (1) adding a cascaded classifier having into the RCNN head; (2) adding deformable ROI pooling that would allow to predict and apply a deformation (a set of relative offsets, one for each cell used in the ROIAlign grid) to apply before pooling features with ROIAlign - applied both to the reference image features grid as well as to both stages of the cascaded classifier in the RCNN. Improvements over some baselines are reported in a series of FSOD experiments on Pascal and COCO using the setup of (17).\n\nPros:\n- few-shot object detection is an important task in the few-shot learning line of research, and works in these directions are welcome\n- the reviewer agrees with the claim that focusing on improving RCNN classifier false positives is interesting and would benefit in the long run\n\nCons:\n-  the writing could be improved, some parts are difficult to follow and not clear how they relate to the overall story, some clear mistakes are present: CoAE (12) and CoAE (17) are both present, while clearly, it is not (17) as in (17) itself CoAE is used as a baseline (and interestingly its number there is 12). This last thing is just an example, yet the relation to (17) is even less clear as explained below.\n- the experimental comparisons are all done in the \"setting of (17)\" (quoting the paper) yet the comparison to (17) is not given. I checked, (17) is not this paper although such a situation could occur in arXiv age :-). And yet, clearly, on PASCAL the authors of the current paper copied the corresponding table from (17), then replaced the \"ours\" of (17) with their result (numbers are indeed different) and did not add back the original result reported by (17), which, interestingly is exactly the same on average: 69.1 mAP on unseen (quite a coincidence!) while being obtained from a different set of numbers per category. At any rate, it does not beat (17) only matches it. Similarly, the relation to (17) results on COCO based few-shot evaluation is not clear. Eligibly it seems (17) results are higher on AP0.5:0.95 yet it is organized a bit differently so not sure, but the authors should definitely add (17) results to their tables and show they are better than it to claim the win.\n- novelty: this is my major concern, deformable ROI pooling (as well as deformable convolution, same paper if I recall) have been proposed long before in the context of general object detection, so it seems nothing is new here (also they were applied to FSOD before). Cascaded RCNN head was also definitely proposed before (in the context of General detection). So it seems this paper just adds these existing ideas on top of a Siamese approach to FSOD, overall making it seem more like a system paper and hence expected to significantly improve results (which as I explained before, I am not sure it really does). \n- final thing, perhaps more minor - in all explanations and experiments, it seems that the test is - given a query image and a reference image for some object guaranteed to be present in the image - find a bounding box for that object. This sounds more like a localization task to me, and not really detection. I remember that in (at least some of) the FSOD there are episodes with multiple reference objects from different classes and multiple queries where not all reference objects are on all queries so it is more resembling generic object detection (eg COCO benchmark) and is not strictly localization as in the task presented. But I might have missed something in this point so I urge the authors to explain better and prove me wrong if I misunderstood their intention.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "1. Summary\n\nThe paper identifies false positives as the major problem in one-shot object detection and proposes a model (FOC OSOD) to overcome this problem. The method is tested on two one-hot object detection benchmarks on Pascal VOC and COCO improving upon previous work.\n\n2. Strengths\n+ Having worked on one-shot object detection as well I think the identification of classification and especially false positives as the main issue in one-shot object detection is absolutely correct.\n+ The method improves upon the baseline and CoAE and ablation studies are performed.\n\n3. Weaknesses\n- The write up is hard to understand and far from publishable. The line of argumentation and the descriptions are often unclear and there is an abundance of grammatical errors. I am aware how much of a burden it is to write publication in English, a foreign language, but the number of errors is simply too large for a publication at a major conference.\n- The method performs better than the baseline but only by a small margin (~1.3% on unseen classes). For such a small gain it is however extremely complex combining double headed rcnn, cascade rcnn, and region deformation.\n\n\n4. Recommendation \n\nIn it's current form I recommend to reject the paper. It does however address the key problem in one-shot object detection and I encourage the authors to improve their method and write up. With some additional work I can see myself recommending accept as the addressed problem is very relevant.\n\n\n5. Questions/Recommendations\n\nTo improve the paper I suggest the following:\n- Simplify the analysis in Section 3.1 by keeping the RPN as is and removing the reference information only from the classification/bounding box regression head(s).\n- Add some quantitative analysis to demonstrate that indeed false positives are the problem (qualitative analyses can be found in the literature but I am not aware of any quantitative analysis)\n- Make the method significantly simpler. Right now it is simply a bunch of methods thrown together to improve the performance of the classification head. While they undoubtedly work I am not a fan of this type of \"architecture hacking\". Rather think if there are any straight forward ways to solve the issue e.g. a different loss function that makes classification easier. Is it enough to split the regression and classification? Is the learned metric reasonable or is there a better way along ides like matching or prototypical networks? Is there a way to use the fact that in each image there are often multiple objects of the reference category?\n- Replace most of the formulas in Section 3 with Figures.\n- Rework the writeup. From grammatical correctness to the omission of the most important references [2, 17, 21] from the related work section (they are in the introduction so it's not wrong I'd just expect them in related work also).\n\n\n6. Additional feedback ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}