{
    "Decision": "",
    "Reviews": [
        {
            "title": "Let me talk now!",
            "review": "Summary: The paper deals with the problem of talking-head video generation from an audio input. In this work, the authors propose a learning mechanism that is agnostic of the person. Instead of going directly from audio to video, this work factorizes the problem in the following parts: (1) learning expression, geometry, and pose from audio; (2) generating a 3D mesh model from (1); and (3) finally a neural rendering network that generates final video. There are other crucial aspects: (1) a pre-processing step that selects frames from the existing video; (2) temporal flickering removal algorithm. The proposed formulation is evaluated on GRID dataset and speech dataset with seven videos collected by the authors. \n\nPros: \n\n+ Compelling video results!\n\n+ Reduced the data requirement for Obama example from 14 hours in prior work to 14 minutes.\n\n+ extensive pipeline considering insights from graphics in learning machinery.\n\n\nConcerns: \n\nHere are my concerns with this submission: \n\n1. While there are many comparisons with the prior art,  but I think the comparisons are not rightly placed. \n\n- Comparison to Fried et al., 2019 (Text-based editing) and NVP: The comparison is shown on the example of Yoshua Bengio which is used for training in the model. However, authors mention in the supplementary video that Fried et al., 2019 train a person-specific model whereas they do not. It is true, but how does that statement hold when you have also used a \"particular identity\" for training? \n\n- Comparison with DVP: DVP is trained for mostly portrait videos. The \"Yoshua Bengio\" video example is never seen by this approach. However, the proposed approach has been trained on it. The remarkable comparison would be when this particular example is not used at all for training.\n\n2. Comparison with LipGAN (https://github.com/Rudrabha/LipGAN): This recent work (published Jan 2020) is a precise *apples-to-apples* comparison with the proposed approach. The code is publicly available and should be included in the comparison.\n\n3. Test on the unseen person: The current results on the unseen people are drastically simple than seen people. My suggestion is to show results on the videos used by LipGAN or random videos from YouTube or videos from Chung et al., 2018. Without this, the claim on an unseen person does not hold much value.\n\n4. Another person voice for Obama, Bengio example -- How does the approach perform a voice from someone else for Obama and Bengio? Is it possible to get another video (say Geoff Hinton) and then use Hinton's audio to drive Obama and Bengio outputs?\n\n5. Table-5 (User Study): Instead of Mean Opinion Score, A/B testing might be a better way to measure the quality of results.\n\n6. Table-6: two points here\n\n (a) PSNR and SSIM are not an appropriate way to evaluate this problem. The standard deviation on these values would make all the approaches look the same. For e.g., in both Jamaludin et al., 2019 and Chen et al., 2018 -- only the lip moves and yet it is able to get high scores competing enough to the proposed formulation. \n(b) I am not able to understand how Vondrick et al., 2016 fits this comparison? On a related note, Section 4.1 (Large Pose Results) -- how is Vondrick et al., 2016 a valid reference in the fourth line of this paragraph?\n\n7. Figures in the paper are not properly thought. For e.g., it is not clear to me how does one infer Figure 4, Figure 6, and Figure 7. Similarly, in Figure -1 -- it is not clear what is the input and what should be the output of the system? \n\n8. Page-2 (second paragraph): I am really not sure if the claims in this paragraph are correct. \n\n- 15 minutes of the target person video is already quite a duration when there is recent work that uses a few seconds or no data at all.\n\n- there are contemporary approaches that model without scene geometry, lighting, and material. \n\n\n9. Related Work: \n\n- Audio-based Facial Animation: May want to include LipGAN. \n- Video-based Facial Reenactment): May want to include Recycle-GAN.\n\n\n10. Page 7: Correction in the first line -- it should be \"around 14 hours\" of data as mentioned in the first paragraph of second page.\n\n \n\nRelevance to ICLR (only a suggestion): I am not sure if a broader ICLR community would be interested in this work. The work and the flow of the work are more relevant to graphics conferences like SIGGRAPH/TOG (having said this I know that there has been graphics paper at ICLR but they have been more focused on learning representation). ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple but nice method, however paper organization is questionable",
            "review": "The paper present a new method for editing face video using audio. There are 2 main contributions of this paper:  Audio ID-Removing Network and Neural Video Rendering Network. Audio ID-Removing Network removes an identity from source audio in order to transfer only the content and not the ID. Neural Rendering Network generate video from masked face images and mouth keypoint heatmaps (obtained from 3DMM model).\n\nWhile the method seems simple and intuitive, there are some questions:\n\n1. For Audio ID-Removing Network, the authors suggest to modify the audio in such a way that pretrained VGGVox network assign uniform probability to all speakers. Lets consider that the speaker is not in the original training dataset for VGGVox. Then the probability may be uniform however, it is not guaranteed that the identity of this person is removed, just VGGVox network don't know where to assign this speaker. This argument is partially supported by the Fig. 9, in GRID dateset there are no celebrities so nobody is in vox dataset, while in the collected dataset there is only famous celebrities which are likely in the dataset. So there is clear separation for the collected dataset, and almost no difference for the GRID dataset. Another doubt is that optimizing over pretrained network prune to adversarial examples. \n\n2. For neural rendering network, the authors suggest to first build 3DMM model and later project from mesh to 2D keypoints when producing heatmaps. So it is not clear how 3d is beneficial here, and why authors emphasize 3d if later it is removed? Maybe performance could be improved if the 3d keypoints are used? For example variance of the heatmap could depend on the depth of the keypoint.\n\n3. Comparison in Appendix. B3 does not seem to have much sence. First of all the task is different. And second proposed method uses 6 frames, while baselines methods use only one.\n\nThe paper is not self contained, it has poor structure and Appendixes seems overly abused. Mainly all experiment explanation, quantitative evaluation, baseline comparison and ablation studies is moved to the Appendixes.\n\n1. For example without looking to the Appendix (e.g without this phrase \"Each speaker contributes 15 minutes video for training and 2 minutes video for testing...\"), it very hard to understand that the method need to be trained on the identity that will be used at the test time.\n\n2. Another example is this phrase \"Interestingly, once scaling up the number of subjects in the training set further by an order of magnitude, our model can generalize to audio and subject that are totally unseen during training.\", without going trough entire supplementary it is impossible to understand if this is a hypothesis or there is an experimental validation to this. \n\n3. On the other hand \"Large Pose Results\" and \"Audio Editing & Singing Results.\" seem like not relevant for the main explanation and could be easily moved to supplementary.\n\nOverall structure of the experimental part of the paper should be reconsidered. Also some parts in the task formulation should be explained better. For example it is also not that clear if any video could be edited with frame rate preserved? Or the model select some specific frames using Selection Algorithm, and it will be impossible to embed generated sequence back to the original video. For example this could be useful for animating several persons.\n ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Compelling results of audio-to-video synthesis in many-to-many setup, but the experiments can be improved",
            "review": "Summary:\nThis paper presents a system to synthesize photorealistic speech sequence from an audio input as well as the speech video of a target subject. The proposed approach differs from prior works in two aspects: 1) the audio processing module can take an arbitrary speaker by converting audio inputs into person-agnostic expression coefficients, and 2) the rendering module supports multiple video inputs by focusing on synthesizing only the mouth region. The proposed approach is evaluated on several datasets, demonstrating its superiority over the existing approaches. \n\nWhile the paper demonstrates compelling results, the exposition and experiments can be improved. At this stage, I’m not convinced that the paper is ready for publication at ICLR.\n\nPros:\n- The proposed system supports multiple subjects for both audio input and target video.\n- The use of Audio ID-Removing network is interesting and shown effective to obtain identity-agnostic speech features.\n- The pipeline is end-to-end trainable (except the rendering module).\n- Extensive study shows that the proposed approach outperforms existing works in terms of reconstruction accuracy and realism.\n\nCons:\n- The proposed audio-to-expression module can be replaced with that of NVP [Thies et al.], but no comparison on this module is provided. Thus, the technical contribution of the audio-to-expression module including Audio ID-removing network is difficult to assess. \n- It’s not clear to me why the proposed method does not use 3d face for the later rendering process even though the pipeline heavily relies on 3d face model for the audio-to-expression module. Previous works such as [Nagano et al.] show utilizing 3d mesh leads to more photorealistic synthesis compared to facial key points alone.\n- The claim about supporting many subjects is a little confusing. As the method requires subjects to be part of training data, advantage over NVP is not significant if this only scales to about 10 videos. Also it is not clear whether the quality degrades by incorporating more videos or not. Such evaluation is highly recommended to better understand the tradeoff between quality and generalization. \n- The qualitative results on a few unseen subjects are not sufficient to support the generalization claim. More thorough experiments (both qualitative and quantitative) are recommended.\n- I’m also confused about the result of DVP. Since the background is static in the target video, DVP would not produce moving background as shown in the results. As the quality is substantially worse than the original DVP paper, please clarify why DVP degrades significantly. \n- Exposition of data processing is not very clear. Although the overall pipeline is highly dependent on the accuracy of training data (2d landmarks and expression coefficients), the paper does not provide detailed algorithms and hyperparameters for reproduction.\n- One of the major limitation of using expression coefficients is that no information about inner mouse motion is provided, which may lead to incorrect inner mouth synthesis. \n\nQuestions/Remarks:\n- The sentence \"The underlying reason could be that the expression parameters are defined and related to the3DMM model that is hard to model by the net-work.” in Sec. 3.2.2 is not backed by any evidence in the paper. Unless the authors show experimental evidence or analysis, this claim should be removed. \n- The paper claims the proposed approach can handle larger head angles than previous works. However, as far as I can tell, this is because the training data includes videos with large head pose. If nothing fundamentally differs from the previous works other than data to support large head angles, please tone down the claim.\n- The main paper barely mentions several key algorithms to refine the final outputs (frame selection and teeth proxy). I would suggest mentioning these processes in the main text and refer to Appendix with ablation study.\n- According to the exposition and Fig. 2, the neural video rendering network only observes images masked inside the mouth region. While the network can remember mouth appearance for trained subjects within the network weights, it’d be quite strange if the network can synthesize the mouth region of unseen subjects without seeing the mouth appearance as shown in Fig. 10. Please clarify if the current exposition misses any implementation details. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper discusses a method to re-enact videos of a particular person by using audios of an arbitrary person",
            "review": "**1. Presentation and clarity**\n\nI think the paper is overall well written, contains sufficient details for an experienced reader to re-implement the approach. \n\n**2. Method and originality**\n\nThe method itself is similar to that of DeepVideoPortraits. Both approaches require retraining for each person. Both approaches get expression (re-enactment) parameters from a different source, in DVP it's a video of a driving person, in this paper it's an audio. The rest of the framework is similar from a high level. \n\nTo get the animation parameters from the audio sequence, the paper proposes to remove identity information from the audio stream. They learn weights of a linear combination to modify the audio stream and use to VggVox audio identifier to guarantee that it's not able to identify the identity. Such approaches have been employed in vision many times, here it's somewhat original, since it's applied on the audio channel. \n\nWhen the expressions are obtained, the rendering network is used to draw only the pixels in the mouth region. It's a standard image-to-image network. \n\nOverall, the paper does not present sufficient contribution to recommend acceptance. The method is quite similar to previous techniques apart from the audio part. It tackles the mouth region only, which is insufficient if the paper aims to address re-enactment. Many of the subtle expressions are in the upper part of the face, which the present paper doesn't address. \n\n**3. Results**\n\nIn the main paper, no numerical results are presented, only qualitative results are available. No comparisons are made with the state-of-the-art. The paper suggests the reader to eyeball some images in Fig. 7 to derive a conclusion about the method and its standing. I believe this is not sufficient. It is the responsibility of the authors to show the benefits of the method. At least a user study can be made, or a reconstruction loss can be computed and compared with the state-of-the-art. Otherwise, it won't be clear whether the presented method should be considered for any practical use-cases. \n\nFinally, I watched the supplement. To me the quality of the videos is not exciting. If we compare with the real videos (since state-of-the-art comparisons are not reported ) in each case it's clearly visible the video was manipulated. When we are presented with the side-by-side comparisons with state-of-the-art the decision is not always in favor of the presented method. It's hard to derive any other conclusion as comparisons are insufficient.\n\n**4. Rating**\n\nThe presented paper is a good system paper or a workshop paper. I believe the paper itself can be substantially improved if proper comparisons are added. However, even in this case the paper will most likely land below the acceptance threshold as methodological novelty is rather limited. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}