{
    "Decision": "",
    "Reviews": [
        {
            "title": "Very weak paper ",
            "review": "This paper proposes Subformer, a variant of efficient Transformers that aims to reduce parameters by parameter sharing or sandwich based parameter sharing.  I think this is overall a very weak submission probably at best, only good for a student workshop at maybe a NLP conference. \n\nThe authors proposed SAFE which stands for self-attentive factorized embeddings. This section is poorly described that that it is confusing. In fact, being very well acquainted with Transformers and their variants even in this line of work, I had troubles understanding what the authors are doing. The naming factorized is strange because there is actually no factorization happening. At best, this is just adding another smaller projection layer to that layer. Am I missing something here? Nevertheless, the writing can be significantly improved.\n\nThe next \"contribution\" is the sandwich layers and shared parameters of sandwich layers. This is an ad-hoc scheme that might not be a strong contribution for a conference paper.\n\nExperiments section are not fantastic, but also not very convincing. In fact, at approximately the *same* parameter costs, this method does almost exactly the same as the baselines (!!) on MT. On the other datasets, there is also insufficient experiments or ablations to show a parameter-by-parameter apples to apples comparison. \n\nIn terms of constructive feedback, I would personally hope to see more controlled experiments and even more comparisons with other weight sharing schemes presented in Albert or other models. It is also worth considering pretraining experiments like in Albert or Electra. There is nothing much that one can do to the idea, so im just hoping the authors can improve the experiments and perhaps resubmit it to a workshop in the future. With a stronger set of empirical results I think this paper will stand a better chance in the future. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "weak baselines and lacking empirical evidence",
            "review": "The paper studies parameter reduction approaches for encoder-decoder transformer models that are used in sequence-to-sequence tasks.  The parameter reduction approaches proposed are similar to those presented in the ALBERT model proposed by Lan et al., but there are differences such as adding a self-attention transformer layer with embedding factorization, and empirical determination of which layers can share parameters and which layers need to remain untied to achieve best performance on sequence-to-sequence tasks.\n\nTwo main contributions of the paper in my view are:\na) The Self-attentive factorized embeddings which are shown to perform better than the embedding factorization proposed by Lan et al.\nb) The results of table 2 that show that sharing all layers of encoder (as done in the ALBERT model) as well as sharing all layers of the decoder model leads to very poor performance on seq-to-seq tasks.\n\nHowever, when demonstrating the value of the proposed sandwich style parameter sharing approach I feel the empirical results are very unclear and fall short of being convincing.  For instance, it is not clear how the various baselines are selected and certain others are left out.  For instance\na) On the WMT’14 EN-DE task the Evolved Transformer w/ 48M parameters is BLEU of 27.7 but the 64.1M parameter model has BLEU of 28.4.  Only the 48M parameter result is mentioned.  Among the bigger models, the Evolved Transformer deep model has 218M parameters and achieves BLEU score of 29.5.  What is the reason for leaving this out?\nb) On the Wikitext-103 LM task, the kNN-LM approach of Khandelwal et al. achieves, with 247M parameters, a perplexity of 15.79 which is 25% relative better than the subformer ppl of 20.39 reported in the paper.  Similarly there are Transformer XL models with 257M parameters which obtain PPL of 16.4 (Krause et al.).  Why are these, and other beselines which are between PPL of 15-20 (and this is a huge range!) and many of them are transformer based, not mentioned?\nc) Similarly, on the CNN-daily mail summarization task, there are models w/ Rouge-1 of over 44, and quite a few models in the 41-44 range which perform better than the proposed model performance of Rouge-1 of 40.9.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-presented but with some flaws",
            "review": "Summary:\n\nThe paper proposes two methods to reduce the number of parameters in the Transformer. The first one is on compressing the embedding layer, which stores the embedding matrix in low dimension and maps it to high dimension with a self-attention layer. The second one is to share all the Transformer layers except the first and the last layer. The model is tested on machine translation, language modeling, and summarization.\n\nReasons for score: \n\nThe paper is well-written and the idea is clear. My concern is mainly on whether the contribution of this work is incremental. ~I’ll raise the score if the authors address the concerns below.~ Thanks for the additional experiments! I have raised my score to 6.\n\n\n\nPros: \n\n1. I like the finding that unsharing the first and the last layer can greatly improve the final performance of the model. This finding is valuable for people trying to compress the model with parameter sharing.\n2. The authors test the model in a wide set of tasks and show consistent performance improvement.\n3. The paper is clear and well-written and the idea is easy to understand.\n\nCons: \n\n1. For ablation studies in Table 2, the experiments are not fair for the baseline models because they have much fewer parameters. An ablation study with an equal number of parameters will be much stronger for the final result.\n2. The first technique proposed by the paper is to increase the dimension of the compressed embedding vectors with self-attention. My main concern is this seems redundant to the first Transformer layer, where there is exactly a self-attention layer at the very beginning, and the only difference is the residual connection. This self-attention in the SAFE module becomes even more redundant given that you unshare the first Transformer layer. Some suggestions on experiments to resolve this issue:\n\n    a. In experiments in Table 1, include a model with one extra layer for linear projection.\n\n    b. In experiments in Table 3, include a model with only SAFE and a model with only Sandwich Module.\n\n3. To compress the embedding matrix, how about directly reduce the vocabulary size? With BPE and subword vocabularies, this should be easy to achieve and can also significantly reduce the number of parameters in the embedding matrix.\n\nMinor comments: \n\n- What exactly is “Every 2 layers shared” in Table 2? It would be better to define it clearly in the paper.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}