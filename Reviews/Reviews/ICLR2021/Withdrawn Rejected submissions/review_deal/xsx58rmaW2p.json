{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The average score of the reviewers is 6. There are various pros and cons pointed out by the reviewers. Unfortunately, the AC and SAC found that the merit could be outweighed by the limitations of the work, and would like to recommend rejection. For example, a central concern raised by the reviewers is the lack of theoretical justification---the measurement that the paper proposes does seem to show an interesting empirical phenomenon, but as a few reviewers pointed out, it's unclear how the interesting phenomenon directly links to the generalization mechanism, and it's unclear whether the new interesting phenomenon is caused by the change of measurement of the gradient coherence or it is something fundamental. The arguments related to these are found to be generally vague and hand-wavey by the reviewers.  The AC would like to encourage the authors to address the reviewers' concerns thoroughly, especially those regarding the difference and similarity with prior works, the interpretation, and limitations of the results, etc., and consider adding more rigorous analysis to justify the proposed measurement of gradient alignment.  "
    },
    "Reviews": [
        {
            "title": "The paper studies a new notion of coherence to explain generalizability of neural networks. Coherence is defined as the degree to which the gradients w.r.t the data points are aligned with each other. The paper presents experiments to study the evolution of the proposed metric during training and testing of neural networks.",
            "review": "The paper studies a new notion of coherence to explain generalizability of neural networks. Coherence is defined as the degree to which the gradients w.r.t the data points are aligned with each other. The proposed metric to measure coherence improves upon existing definitions in the literature by ensuring a natural normalization such that the metric is always between 1 and the number of data points, thus having a direct interpretation of the number of data points whose gradients are aligned. \n\n## Pros: \n1. The experiments presented are pretty interesting and the paper does a good job of interpreting the results of the experiments. Studying the evolution of the coherence can be seen to be insightful in understanding generalizability.  \n2. The proposed definition of the metric enjoys a better computational complexity, making it possible to experiment on larger datasets. \n3. The paper also provides some general theory on coherence w.r.t to general sets, which is appreciated and manages to provide a better intuition to the readers. \n\n## Cons\n1. The general discussion in section 6. seems hand-wavy. Examples: 1)  \"One may imagine an uneasy equilibrium between these opposing tendencies leading to expansion\nand contraction in coherence. As soon as significant coherence builds up, it leads to an increase in the\neffective learning rate (higher relative gradient norm) leading to faster consumption\" - It's hard to say which one is the cause and which is the effect.  2) \"Since this creation happens even with random labels where there is nothing to learn (i.e.,\nno generalization), there is reason to believe that this creation is purely an optimization phenomenon.\" \n\n2. In general, although section 6 adds value to the paper, it does not seem to be the result of a principled analysis and seems more like a commentary on the plots. \n\n\n##  Overall score:\nI am giving this paper an overall score of 6. The paper is interesting and the ideas presented are sound. My reasons for not providing a stronger recommendation for acceptance are \n1. I am not convinced that there is one particular message that the paper finally arrives at. There is a new metric defined, and some general theory provided w.r.t this metric and experiments that study the evolution of the proposed metric. It would have been good to have a discussion how this particular set of experiments can better inform practice or theory of deep learning. \n2. The analysis is vague and hand-wavy. \n\nSome potential directions for improvement:\n1. Studying how the coherence changes w.r.t the complexity of the model. This can maybe performed using toy datasets and small architectures. What happens when layers are added, removed, made wider, and so on. \n2. Studying the evolution of coherence across different cross-validation folds to see if coherence can be used a way to tune hyper-parameters.\n3. Study of what happens when there are adverserial examples.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A nice, simple method for understanding gradient coherence",
            "review": "######################################################################\n\n1.  Paper Summary \n\n\nThis work introduces m-coherence, a new method for understanding gradient coherence when training deep neural networks.  The authors then present theoretical properties of m-coherence (importantly scale invariance) and demonstrate that m-coherence can be computed in a computationally efficient manner.  The authors then compute m-coherence across steps/epochs when training ResNet-18 and EfficientNet on ImageNet without label noise, with 50% label noise, and 100% label noise.  The experiments demonstrate that (1) m-coherence matches intuition after an initial phase  (i.e. m-coherence is large and decreases over time for real data) (2) m-coherences increases for all datasets to a peak in the first 100 epochs before decreasing.  \n\n######################################################################\n\n2. Strengths\n\n2.1. m-coherence is a very natural metric for understanding gradient coherence in that it is scale invariant, easy to compute, and is easy to analyze mathematically.  \n\n2.2. The description of m-coherence and its properties is presented well in comparison to recent work.   Additionally, the proofs follow almost immediately from the definitions and highlight how m-coherence is a simple, intuitive framework for understanding gradient coherence.  \n\n2.3. As m-coherence can be computed in a computationally efficient manner (both in algorithm complexity and memory usage), the authors are able to analyze gradient coherence on modern convolutional networks trained on 50,000 examples from ImageNet.  Importantly, the authors present both step-wise m-coherence and epoch-wise m-coherence, which highlights the phenomenon of m-coherences increasing for the first few steps of SGD.  This empirical finding could be an important stepping stone for theoreticians in understanding optimization and generalization for over-parameterized models.  \n\n2.4. The authors also highlight that the initial increase in m-coherence is robust across a variety of architectures, label noise settings, and initializations/minibatch constructions.  This finding is rather surprising in that it occurs even on datasets with 100% label noise.  \n\n######################################################################\n\n\n3. Limitations/Questions\n\n3.1. I would have liked to see an explicit comparison of m-coherence with the other recent methods on a subset of CIFAR10.  In particular, does a similar trend arise of gradient coherence first increasing before decreasing under these other methods?  I read through the discussion in the appendix about this result being consistent across these papers, but there appear to be caveats with what samples were considered in other works.  It would be interesting to compare gradient coherence under these methods using a fixed random subset of CIFAR10 across methods (please let me know if I missed anything though in case this is already done).  \n\n3.2. (Minor) Just for some clarity, it would be interesting to understand the makeup of the 50,000 samples considered in the experiment.  How far away is the sample set from having 50 examples per class?  ImageNet also has a large number of classes of similar objects (e.g. ~100+ dog classes).  Is there indeed a higher m-coherence for such subsets of ImageNet during training or does m-coherence actually increase on these subsets as well?\n\n\n######################################################################\n\n4. Score and Rationale\n\nOverall, I vote for accepting this paper.  I find the contribution of m-coherence to be novel, practical step towards understanding gradient coherence for modern machine learning settings.  In particular, I feel that these findings will be of interest to theoreticians interested in the intersection of optimization and generalization.  My only criticism of the work would be that the authors should try to provide a more concrete comparison with other works on a random subset of CIFAR10 to determine whether gradient coherence phenomena are robust across all these methods.     \n\n\n######################################################################\n\n\n5. Minor Comments\n\n5.1. I think the \"overfit\" label in Figure 1 is a bit confusing in the second row of Figure 1- is this just the gap between and test error? If so, I feel that just re-labelling this as the \"Gen. Gap\" or even just removing it would be fine.  \n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Empirical Results, Incremental and Misleading Explanations",
            "review": "In this work, the authors propose m-coherence as an replacement of sign stiffness / cosine stiffness / gradient confusion to measure the coherence of gradients in SGD training. Compared to existing measurements, m-coherence is more scalable and well-normalized. Using the m-coherence, the authors perform empirical studies on SGD optimizing a ResNet on ImageNet with natural or random label. The experiments show that for both real label and random label datasets,  the initial coherence is low, then increases as the training goes on, finally decreases as more and more data gets fitted.\n\n# Interesting empirical results.\n- I think the empirically similarity of gradient coherence for SGD learning real label and random label datasets is interesting. It suggests (1) the optimization of real label dataset and random label dataset is not totally different ---- they only differ in terms of fitting time; (2) we should look closer at the initial optimization of real label dataset though, since the \"coherence increasing\" phase in this case is super fast.\n\n- On the other hand, the observation is not too surprising since even real label dataset contains noise, and random label dataset is only an extreme case where the noise in the dataset is super large.\n\n- In sum, I personally do not think this pure finding, without an insightful analysis, is significant enough for an acceptance. \n\n# Misleading interpretation.\n\n- I think the writing of this paper is quite misleading. For example, in page 6 Real Labels paragraph, it claims for real labels, the initial coherence is very high? But then in page 7 Early Training paragraph, it instead claims the initial coherence is not very high if you look closer. Personally I find it misleading to make a claim first but then correct the claim with another one which is totally contradictory to the formal. I think it is better to explicitly point out the claims are for different scenarios, i.e., in large timescale/small timescale.\n\n- Moreover, in page 8, the last part, Separation of Generalization and Optimization. The statements are super vague and not supported by experiments/theory. I do not understand why these arguments can make sense. I strongly encourage the authors to backup their statements with evidences.\n\n\n# Experiments\n- The authors claim several benefits of the proposed m-coherence compared with existing measurements like sign stiffness / cosine stiffness / gradient confusion. Indeed in terms of computation, m-coherence can be cheaper. But it is not clear whether or not sign stiffness / cosine stiffness / gradient confusion agrees with m-coherence empirically. In my opinion there should be some ablation studies to check the trend of gradient coherence with other measurements.\n\n- Looking at Fig. 1, I find the plots are not consistent in format. For example, in the third row, the first two plots have test m-coherence, while the others do not have this; in the second row, the last plots misses a training accuracy curve; and so on. Please explain why.\n\n# Minors\n\n- Page 3, below Eq. (2). I guess it should be a \"lower bound\" instead of \"upper bound\"? Since in Eq. (2) there is a negative sign.\n\n- Page 2, second paragraph last sentence. I am not sure about this claim. If the loss functions are not linear and gradient directions change every iterates (even for the same data point), the claim can be wrong. Because even at every iteration the gradients are orthogonal, abandoning one data point does not imply GD/SGD cannot optimize the direction along the gradient on this data, since other data points can contribute this direction afterwords.\n\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "In this paper, the authors provide a new metric named m-coherence to measure the gradient coherences, i.e., the alignment of per-sample gradients. The proposed metric is to normalize the expected pairwise dot  product by an expected gradient norm. The authors show that the proposed metric is computationally efficient, mathematically simple, and may help researchers to investigate the gradient coherence behaviors during the entire training process. The authors provide some simple mathematical explanations and characterizations of the properties of m-coherence, e.g., boundedness, relativeness to sample minibatch, the impacts of zero gradients. The empirical results based on m-coherence are mostly intuitive. For example, as more training samples are fitted, the gradient coherence will decrease because more per-sample gradients are close to zero. There is also some surprising results. For example, the experiments on random labels show that the gradient coherence does not go to the expected 1.  \n\nStrength:\n\n(1) The proposed metric is computationally efficient, and can be of interest to researchers who aim to explore the generalization behaviors of SGD or other algorithms in training neural networks.\n\n(2) The paper is well written and easy to follow. The empirical results seem reasonable.\n\nWeakness:\n\n(1) Some of these empirical results are not surprising (and such results can be also easily made based on some other metrics, of course with additional cost). For me, it is more interesting to provide a completely theoretical demonstration via recent advances on theory of over-parameterized neural network rather than explain these results via merely looking at an empirical metric.  \n\n(2) Although the paper provides some surprising findings, e.g., experiments on random labels, this paper fails to provide an explanation on such findings, which, in my opinion, is more interesting. However, it seems to me that the current metric or tool cannot achieve this goal.\n\n(3) One motivation of the new metric is the computational cost. Therefore, it would be better to demonstrate such an advantage over other metrices in the experiments. In addition, I am wondering whether the proposed metric is looser than other metrics (of course with higher computational cost), e.g., those in Fort et al., 2020 and Sankararamen et al., 2019. If so, it would be better to check whether such tightness will affect the empirical observations?    \n\nOverall, I am not excited about this paper, especially about its empirical results. For such reasons, I tend to weakly reject this paper. Since I am not an expert in this area, I am open to change my mind after reading other reviewers’ comments.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}