{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents PIVEN, a deep neural network that produces a prediction interval in addition to a specific point prediction.  PIVEN is distribution free and does not assume symmetric intervals.  \n\nAll the reviewers agree that the paper investigates an important problem and the paper is well-written. The reviewers also identified a couple of weak points, namely:\n- Novelty: The key idea seems to a combination of prediction loss (common) and prediction interval loss which has been proposed by Pearce et al. 2018.\n- Claims that PIVEN outperforms existing methods (QD and DE) empirically as some of the improvements seem marginal. \n\nGiven these concerns, I think the current version falls a bit short of the acceptance threshold unfortunately. I encourage the authors to revise the draft and resubmit to a different venue.\n"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The authors propose a method for predicting a prediction interval. The authors compare this method to deep ensembles and show improved performance on a set of benchmarks.\n\n\nThe are several things I like about this paper:\n- The paper is written quite clear \n- Uncertainty calibration is an important topic.\n- A lot of benchmarks are considered\n- The method is easy to implement.\n\nThese are the things  that could be improved:\n\n1. The authors say \"\"First,  our approach is the first to propose an integrated architecture capable of producing both PIs and exact value predictions.\"\n\nThis is not correct.  A simple approach such as Lakshminarayanan et al., 2017 will produces the same, even without using ensembles.  In that work,  the predictions parameterize a Gaussian with state dependent mean and variance. From this quantiles and thus prediction intervals can be derived on top of an \"exact value prediction\" which would be the mean.\n\n2.  One of the papers main weakness is the lack of distinction between epistemic and aleatoric uncertainty:\n\n\nSection 2:\nThe way  Section 3 is formulated it speaks about aleatoric uncertainty:    P(L_1 < y_i < U_i) is the probability that y_i falls into th ebound defined by L_i and U_i and thus is \"data uncertainty\" (aleatoric) and not due to the lack of knowledge given by the model (epistemic).  it is thus similar to a Softmax in classification or  a neural network that predicts both mean and variance (as in  (Lakshminarayanan et al., 2017)). Indeed as Section 4 show,  the model does not contain any parametric uncertainty. \n\n\n3. Results and method comparisons:\n\nSection 4: \nI am not sure why RMSE is a useful metric when proposing a method for uncertainty calibration.  Secondly, as mentioned above the comparison to deep ensembles is not fair, as deep ensembles will also model epistemic uncertainty.  I would just use an ensemble of size 1 for a more accurate comparison (thus excluding epistemic uncertainty). Thirdly, the two other metrics need to be explained (at least write them once without acronyms.)\n\nOverall,  the improvements appear  to be only marginal improvements.\n\n4. Related work:\nHow is this work different from  [1] and why wasn't it compared to this work?\n\n[1] Salem, Tárik S., Helge Langseth, and Heri Ramampiaro. \"Prediction Intervals: Split Normal Mixture from Quality-Driven Deep Ensembles.\" Conference on Uncertainty in Artificial Intelligence. PMLR, 2020.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A new approach for combining prediction and uncertainty quantification in DNNs - good empirical results, explanation can be improved",
            "review": "**Quality and Clarity**\n\nWhile the overall message of the paper is clear, the explanation of the method in Section 4 is a bit hard to follow. Specifically it is a bit hard to keep a track of the meaning of the multiple terms in the loss function and the associated hyper parameters (see Queries and Suggestions below).\n\n**Originality and Significance**\n\nThe authors seek to jointly address the problem of making accurate predictions and generating tight prediction intervals by designing a new loss function that combines these two goals. I believe this work can be quite impactful as there are multiple areas where good quality prediction intervals and accurate value prediction are equally necessary.\n\n**Strengths**\n\nThe proposed approach appears principled and seems to give good empirical results in the experiments considered.\n\n**Weaknesses**\n\nThere seem to be multiple hyperparameters ($\\lambda$,$\\beta$) whose choice might affect the performance of the approach. Moreover as this seems to be the first time such a loss function is used in this context there is no evidence on good choices of hyper parameters (other than the values used in the experiments herein). I would highly appreciate an ablation study or some comments on good values of hyper parameters which can guide readers intending to apply the proposed approach to a new dataset.\n\n**Queries and Suggestions**\n\n1. I'm curious about the performance of the ensemble of PIVEN architectures approach. It does not seem to have been used in any of the experiments which is fine since a single model seems to be doing well. But have you tried it on any dataset? What are the settings where the improvement with an ensemble would be significant enough to compensate for the added cost?\n\n2. Why are the MPIW results for DE on Boston and Concrete not in bold since DE seems to be the best in terms of MPIW on the two datasets?\n\n3. Why is only a single dense layer (and not an ensemble) used when applying DE to large scale image datasets? If the backbone is pre-trained then training an ensemble of dense layers should not be too expensive. The current comparison does not seem fair since the NN baseline only has a MSE loss while the PIVEN approach uses multiple regularizers (for good MPIW, PICP etc.)\n\n4. What is the difference between the MOI variant of PIVEN and the QD baseline?\n\nIn addition to responding to the above queries, I would recommend improving the explanation (one suggestion is to write the entire loss i.e. $\\mathcal{L}_{\\text{PIVEN}}$ at the beginning of Section 4 right after Figure 1 and then explain the meaning of each term in the loss, as opposed to the current approach where the terms are introduced first and then the loss is given) and commenting on good choices of the hyper parameters or including an ablation study for the same.\n\n**Comments after Author Response**\n\nI thank the authors for their response. Queries 1,2, and 4 have been adequately addressed. Regarding Query 3, I appreciate the addition of the Deep Ensemble results though I find that the text of Section 5.5 has not been changed to reflect the same. Specifically the paper still says \"For the IMDB age prediction dataset, results show that PIVEN outperforms both baselines across all metrics\". This is now incorrect since there is a third baseline, DE, which appears to outperform/match PIVEN for this dataset. However the explanation that this is because the population age is approximately Gaussian makes sense to me and so keeping in mind the good performance of PIVEN on the other datasets, and the improved explanation and added ablation study for hyper parameters, I recommend accepting the paper as long as the relevant corrections are made in Section 5.5.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "limited novelty and evaluation",
            "review": "Summary:\nThe submission considers the continuous real-valued regression problems and how to obtain accurate point predictions (specific value prediction in the text) and prediction intervals (the uncertainty of the predictions, given by [lower bound LB, upper bound UB]). The paper proposes a loss function which is the weighted combination of (i) the coverage constraint [proportion of outputs that fall between LB and UB] so that the coverage meets a required level, (ii) the prediction interval width [for those that satisfy (i), their prediction intervals should be tight], and (iii) the prediction loss [which makes sure the predictions match the outputs]. The LB, UB and predictions are parameterised using three separate heads: one for LB, one for UB and one for the prediction weight (which makes sure the prediction stays in between LB and UB. The proposed loss function and parameterisation architecture are evaluated on UCI regression datasets and two age prediction problems using image inputs. \n\nAssessment:\nWhilst I think the submission tackles an important problem that could be of interest to the ICLR community, the novelty and experimental evaluation are limited and thus I do not recommend acceptance. Below are some of my concerns/questions and I appreciate the response from the authors.\n\n1. The key contribution is the paper is the combination of the prediction interval loss and the prediction loss. Each of these losses are not new, for example: the prediction interval loss has been considered by Pearce et al (2018). The UCI regression also do not show strong evidence that the proposed method is better than QD, in contrast to the claim of “state-of-the-art results in uncertainty modeling by the use of PIs”.\n\n2. The use of a separate head for v is also not strongly supported by the UCI results, compared to directly parameterising the prediction (POO) or MOI. In fact, PIVEN is probably better because of the hyperparameter search over \\beta.\n\n3. The contribution of the paper includes the output head architecture and the loss function, so I’d not call this a deep neural network. \n\n4. In several places, the paper alludes to skewed output distributions and how PIVEN can handle this better than alternatives. However, it is not clear from the experiments that this is the case. PIVEN is very poor at predicting the Sine example in the appendix -- could you clarify this? Could the \"skewed distribution\" argument be formalised theoretically? like the PIVEN objective is inspired from another likelihood which can support skewed outputs compared to the usual Gaussian likelihood/L2 loss?\n\n5. it would be good to compare to the post-hoc calibration procedure for regression by Kuleshov et al (2018)\n\n6. Does the ensemble of PIs work in practice?\n\nminor: abstract: read-world -> real-world",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "PIVEN",
            "review": "This paper proposes a new objective function for training regression networks with prediction intervals. The goal is to provide tight confidence bounds to accompany predictions, which is of course important for practical deployments of ML systems where uncertainty quantification is critical. Previous work has largely assumed that uncertainty is symmetrical, or even Gaussian distributed, which is often not the case in practice. The innovation of this work is to simultaneously predict the bounds for a given confidence level and a point prediction within those bounds, which is not necessarily the mean. It is accomplished by predicting 3 values: an upper bound, a lower bound, and a mixing parameter that allows making the point estimate as a weighted sum of the bounds. Experimental results are provided on 3 datasets, with 2 baseline methods compared against.\n\nThe strong points of this paper are:\n+ The problem being addressed is important.\n+ The approach is relatively simple.\n+ The paper is clear and well-written.\n+ The metrics introduced for evaluation make sense.\n+ The results are strong and improve over the baselines.\n+ The choice of how to produce the point prediction is well motivated and backed up by ablation experiments.\n\nWeak points:\n- Since the method produces bounds rather than a distribution, it's necessary to know at training time what confidence levels you are interested in. It also therefore can't be evaluated on standard negative log likelihood metrics, or produce something akin to accuracy vs rejection-fraction plots. This is potentially a big problem for practical deployment, since the tradeoffs of alpha vs RMSE can't be evaluated without retraining, and the same model can't be used for multiple downstream tasks with different confidence requirements.\n- The claims about using PIVEN in an ensemble context don't seem to be experimentally validated anywhere.\n- The NN baseline for large-scale image datasets seems unnecessarily weak. Why not predict both mean and variance (basically DE, but a single model)? That way you could at least compute PICP and MPIW.\n- In table 1 it's not at all clear what the bold numbers are. They don't seem to correspond to the best result in each row.\n- In table 2, why is PICP not provided? It seems like the value should not be identical between the different experiment arms.\n- It would have been nice to see an evaluation under dataset shift, similar to the \"Flight Delays\" experiment in https://arxiv.org/pdf/2007.05864.pdf\n\nOverall, this seems like a simple and valuable technique that addresses an important problem, but its applicability is somewhat limited by the fact that it produces bounds rather than a distribution.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}