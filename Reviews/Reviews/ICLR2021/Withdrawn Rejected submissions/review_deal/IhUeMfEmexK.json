{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated “Ok but not good enough - rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state.\n"
    },
    "Reviews": [
        {
            "title": "The paper proposes a novel knowledge distillation (KD) framework of directly using teacher model's classifier to distill the student feature learning. There are several empirical results showing the method's effectiveness.",
            "review": "This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself.\n\nThe idea of using teacher model’s classifier to directly reshape the student model’s feature representation is somewhat novel. It considers the situation of single teacher model and multiple teacher models. The teacher ensemble is achieved by concatenating features from each of the teacher model and then conducting dimension reduction using PCA. The methodology illustration is simple yet clear. There are multiple experiments on major face recognition datasets and demonstrate superior performance against baselines such as L2KD-s.\n\nRegarding the concerns, I am listing them into bullets.\n\n1. why the experiments make the setting of templates using teacher model to extract feature, while the query using student model to extract feature?\n\nWould the comparison of using student model for extracting both template and query feature be possible? It can provide a direct comparison to other methods, i.e. ArcFace trained using ResNet18 compared to the student model with ResNet18. \n\nCurrent experiments lack the comparison to the state-of-the-art methods, i.e. ArcFace and CosFace.\n\n2. the ablation is emphasizing on the ProxylessKD combining with different losses. It does not consider the knowledge distillation itself. For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD? Meanwhile, in many KD papers, there are also intermediate layer feature distillation, would it harm the overall performance under this paper's setting? It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation.\n\n3. In ablation, how would the number of teachers influence the student performance? Meanwhile, how would the network architecture influence the student performance? i.e., fixing the teachers to be the same, while varying student architecture with multiple hypothesis, i.e., ResNet, AttentionNet, DenseNet? It is good to know what specific architecture is favored under the authors' proposed framework.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This work has simple idea and is easy to implement. However, novelty is somewhat limited and more comparison shoud be provided.",
            "review": "The paper proposes a knowledge distillation method for face recognition, which inherits the teacher’s classifier as the student’s classifier and then optimizes the student model with advanced loss functions.  The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation.\n\nStrength:\n- The proposed method is simple and easy to implement. \n- The experimental results demonstrate the effectiveness of the technique.\n- The paper is well organized and well written.\n\nWeakness:\n- The novelty is limited. Directly inheriting the teacher’s classifier is a common strategy in the face recognition community, which can be found in Ref.1.\n- The experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition.\n\n【1】Deng, Jiankang, and Guo, Jia et al. Lightweight Face Recognition Challenge. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops\n【2】Hinton, G., Vinyals, O., Dean., J.: Distilling the knowledge in a neural network. In: arXiv preprint arXiv:1503.02531 (2015)\n【3】Xiaobo Wang and Tianyu Fu et al. Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition. ECCV2020\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good idea, but need more experiments",
            "review": "This paper proposes ProxylessKD method from a novel perspective of knowledge distillation. Instead of minimizing the outputs of teacher and student models, ProxylessKD adopts a shared classifier for two models. The shared classifier yields better aligned embedding space, so the embeddings from teacher and student models are comparable. Since the optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart.\n\nHowever, I still have some concerns.\nFirst, ProxylessKD makes an assumption that the subjects of the dataset for training both models are somehow overlapping, while L2KD does not have such limitation. So more analysis and detailed discussions on the pros and cons of ProxylessKD and L2KD are needed.\n\nSecond, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. It would be interesting to see how performance changes with more layers of student model inherited from teacher model. For example, the last two layers, the last three layers.\n\nThird, experimental comparisons with more advanced KD methods are necessary, e.g.[1], [2], [3] etcs. Currently the only comparison with other method is a self-implemented L2KD, which couldn't comprehensively show the effectiveness of the proposed method.\n[1] Park, Wonpyo, et al. \"Relational knowledge distillation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Peng, Baoyun, et al. \"Correlation congruence for knowledge distillation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[3] Karlekar, Jayashree, et al. \"Deep face recognition model compression via knowledge transfer and distillation.\" arXiv preprint arXiv:1906.00619 (2019).\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}