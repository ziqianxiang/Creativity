{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "While reviewers appreciated the simple approach of this work, the biggest concern reviewers had was with the security guarantee of the method. R4 argued that in a certain case recovering an original image x_1 amounted to guessing 2 coefficients. In the discussion phase the authors argued that security amounts to the adversary guessing 4 floating point numbers, not 2, which requires 100s of millions of years to decode an image correctly. However, R4 is correct that only 2 floating point numbers are necessary. This is because, as described by R4 when one sees outputs x_1 * a_{2,2} and x_2 * a_{2,1}, they can reconstruct x_1 as:\n\nx_1 = (x_1 * a_{2,2} - x_2 * a_{2,1}) / (a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\n\nNow define:\n\nb_1 := a_{2,2} / (a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\nb_2 := a_{2,1} / ((a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\n\nThus the above equation can be written as:\n\nx_1 = x_1 * b_1 - x_2*b_2\n\nSo an adversary needs to guess 2 floating point numbers. Further, R4 points out that an adversary can obtain x_1 up to a scale factor by simply guessing the relative ratio of the the 2 unknown floating point numbers, i.e., if our guess is c:\n\nx_1/c = x_1 * (b_1/c) - x_2 * (b_2/c)\n\nThis is a single floating point number, and not all floating point numbers need to be checked. For many images, information can be leaked even if the true scale of the image is not known.\n\nFor this reason I would urge the authors to strengthen the security guarantee of their approach. One way to do this would be to adapt the method so to make the resulting guarantee be a more standard one (e.g., differential privacy, standard cryptographic hardness guarantees). This would eliminate the main reviewer concerns and greatly strengthen the paper."
    },
    "Reviews": [
        {
            "title": "Review of Reviewer 1",
            "review": "\n\nResults - The two main proposed benefits of the approach are increased speedups in inference as well as training over  Slalom and just SGX. Looking at Figure 2 and Figure 4, the spped-ups do not appear to be consistently significant. I do appreciate that the authors show the results for MobileNetV2, for which the proposed algorithm is supposed to have less improvement in training time. Overall, I think, given that the main selling point of the time is faster training and inference, it fails to provide a reliable boost in either of them.\n\nExperimental Comparisons - I would have appreciated a comparison with other methods especially those that allow training while maintaining privacy.  While the authors make a note of them in Table 1, their differences with the proposed approach and where one is supposed to be better than the other is discussed neither conceptually nor experimentally (except Slalom)\n\n\nNovelty - The main algorithmic novelty of the paper is exporting the compute-heavy linear computes to a GPU outside the secure enclave by using blinding-unblinding techniques to protect the privacy.  In my opinion,  while it is a very nice and concise thing to do (along with the results on its privacy guarantee), it is not significantly impactful or interesting to provide enough novelty to this paper. If this would have led to a huge increase in performance margins, I would have said a simple solution that leads to dramatic increase in performance is amazing! However, that is not the case either. \n\nSo, given these reasons I am inclined to reject at this point. However, I would encourage the authors to discuss the other techniques in more detail, compare with them and point out situations where this technique can have a larger impact.\n\n=====\n\nI have read the authors' response and my comments remain the same as above especially the paragraph regarding novelty. I am keen to see a revised version of this paper.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Some concerns about the correctness of the claims",
            "review": "Paper Summary:\n\nThe paper builds on previous work like Slalom to propose a new secure training and inference protocol in the TEE+GPU paradigm. The main technical contribution of this work is a new blinding algorithm that dramatically reduces the memory required to store the blinding parameters (decoupling it from the input/model size). The authors then build on this to extend their blinding scheme to the training use-case.\n\nScore Rationale:\n\n- The proposed blinding scheme does indeed provide a ~1.5x performance improvement over the Slalom baseline\n- There two concerns about the correctness of the security argument provided by the authors\n  - The security proof as argued in the paper does not extend from the single pixel to multi-pixel case and as such does not apply to real-world images\n  - The reviewer believes that this not a mere gap in the security proof. Infact for certain allowed settings of parameters there are practical attacks.\n\nDetailed Comments:\n\n- The core security argument is rooted in Theorem 1. This theorem in prose rightly claims that the blinded pixels at any specific index, leak almost no information about the pixels at that specifc index in the source images.\n- This statement critically makes no claims whether all the pixels in all the blinded images in their totality will leak information about a given pixel index in the source images.\n- The authors then argue that the total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels at that particular index).\n- The statement is not true. The total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels across all indices)\n- The next concern to evaluate is whether this is just a technical/theoretical gap in the proof or whether it leads to a practical attack.\n- Consider the case where the parameters are set as in section 5.2 with K=1. Note that reducing K improves the leakage bound in principle.\n- In this particular case we can rewrite the blinding equations s.t. $x^{(1)} = \\frac{\\overline{\\mathbf{x}}^{(1)}\\alpha_{2, 2} - \\overline{\\mathbf{x}}^{(2)}\\alpha_{2, 1}}{\\alpha_{1, 1}\\alpha_{2, 2} - \\alpha_{1, 2}\\alpha_{2, 1}}$\n- Thus the source image can be represented as a simple linear combination of the outputs and attacker job reduces to the task of guessing the two weighting coefficients\n- Given the strong priors on natural images this seems to be a very tractable problem.\n\nAdditional Comments:\n\n- There are three potential ways to work around to address the above comments\n  - Devise a new proof for information theoretic security\n  - Argue the computational hardness of the search problem\n  - Devise a new blinding scheme that does not have this issue\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Secure training and inference of DNNs, guarantees are a bit shaky",
            "review": "Summary:\nThis paper aims at addressing both  secure training and inference of DNNs. The proposed method relies on securely off-loading the compute-intensive part of the operations from a trusted CPU environment which has low-performance, to an untrusted high-performance gpu. Their suggested method builds on SOTA, Slalom, but is shown to have less memory over-head. Also, they point-out that their proposed scheme supports training phase as well, as opposed to Slalom which targets inference. For off-loading the computation to the untrusted GPU they propose some matrix blinding techniques.\n\n\npros:\n\n+ The problem is interesting, and moving towards a solution for it is really helpful, since right now trusted environments only exist for CPUs in practice, which hinders fast execution of matmuls, as done in GPUs. \n\n+ The approach seems to offer some speedup over the SOTA, Slalom. However the provided guarantees seem questionable. \n\n\ncons:\n\n- About the training phase, I think the nuances of security and privacy are a bit tangled here. Private training refers to protecting the training data, so that sensitive information within the data is not encoded/embedded in the model once the training is finished. This would be similar to what differential privacy and differentially private SGD provide. However, the suggested method in the paper seems to aim at providing secure training, in which data is not exposed to outsiders during training, however private information might still leak to the model, based on my understanding. I would like to know if that is the case. \n\n- I don't quite understand that during training, what is sent to the GPU and what is calculated locally in the trusted environment? Since the GPU needs the weights in the clear for the forward pass, and calculates the gradients for backprop, it seems to have access to the gradients as well, which can be subject to attacks, shown in [1]. \n\n- The privacy guarantees are not completely clear to me.  It seems that the work is built upon the notion of Mutual Information, a heuristic approach. However, since this notion is average case, what would that mean for the guarantees? How does it effect each data point? It is stated that \" Using these parameters DarKnight guarantees that no more than one bit of information is\nleaked from a one megapixel input image. \", is this on average? or for each image, the guarantee is that no more than one bit leaks?\n\n\nReferences\n\n[1] Zhu L, Liu Z, Han S. Deep leakage from gradients. InAdvances in Neural Information Processing Systems 2019 (pp. 14774-14784).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper uses a combination of a Trusted Execution Environment and masking techniques to perform secure inference and training with the help of GPUs.\n\nOverall the paper is well written but could be improved in the presentation of its security guarantees and how it presents related work.\n\nTable 1 is confusing; please consider removing it or clarifying that security/privacy guarantees in each approach are completely different and solve orthogonal problems. Additionally please consider referencing the work more precisely on what training and inference algorithms you refer to.\nFor example:\n- how do DP mechanisms such as Rappor perform training\n- which inference algorithms do you refer to in MlCapsule, which training algorithms in ObliviousTEE.\n- is Chiron used only for training.\n\nPrivacy/integrity guarantees obtained should be clearly stated; how they are different from those achieved by SGX, FHE, MPC, DiffPrivacy. Please see table 1 in Slalom (ICLR 2019) for an example.\nIt also seems that weights are revealed after every batch update. The paper says that it is fine referring to the work by Bonawitz et al.; but they do so in federated learning setting which is different from the one considered here.\n\nClarity:\nThe paper is presented in an accessible manner. \nSome details however need to be explained further regarding generation of the masking variables for inference and training. For example, how are \\alpha, \\beta and \\gamma generated. It is also important to include the time it takes to compute them in the evaluation. It seems they have to be re-generated for every batch.\nIn the experimental section please clarify if blinded batches can be generated in a streaming manner.\n\n\nOriginality:\nThe paper expands the ideas from Slalom (ICLR 2019) of outsourcing some computations from SGX to GPU to speed them up. While Slalom considered only inference, this work considers training as well. The masking technique is different from Slalom and gives a performance overhead in the number of input/batch parameters as opposed to the size of the network. But it seems, that integrity and privacy that the authors obtain in return is different.\n\nSignificance:\nThe paper tries to overcome some of the issues of using TEEs for ML — CPU and small memory sizes — by offloading linear computations to GPUs. The evaluation shows that it does obtain speedups compared to pure SGX baseline, that is, blinding/unblinding is still faster than linear operations. Hence, the work is important. However, privacy and integrity guarantees are weaker than what one will obtain with Slalom (for inference) and SGX.\n\n\npros:\n- an interesting idea to mask only the inputs and not the weights during inference and training\n- speedup from outsourcing linear operations to GPUs from SGX seems worth paying the cost of masking\n\ncons:\n- overall security guarantees are not formally/informally concisely stated\n- integrity and privacy guarantees are weaker than related work\n- it is not clear these guarantees will be desirable in practice\n\nSpelling:\n- Basline\n- DiiP\n- page 4 colours seem to change from blue to red when referring to the same variables.\n- Intel Soft Guard Extension\n- we simply replaces",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}