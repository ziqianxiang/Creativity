{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper approximates the Whittle index in restless bandits using a neural network. Finding the Whittle index is a difficult problem and all reviewers agreed on this. Nevertheless, the scores of this paper are split between 2x 4 and 2x 7, essentially along the line of whether this paper is too preliminary to be accepted. Therefore, I read the paper and propose a rejection.\n\nThe reason is that the paper lacks rigor, which was brought up by the two reviewers who suggested rejections. For instance, in the last line of Algorithm 1, it is not clear what kind of a gradient is computed. The reason is that \\bar{G}_b is not a proper baseline, as it depends on the future actions of the bandit policy in any given round. I suggest that the authors look at recent papers on meta-learning of bandit policies by policy gradients,\n\nhttps://papers.nips.cc/paper/2020/hash/171ae1bbb81475eb96287dd78565b38b-Abstract.html\n\nhttps://arxiv.org/abs/2006.16507\n\nThis is the level of rigor that I would expect from this paper, to make sure that the gradients are correct."
    },
    "Reviews": [
        {
            "title": "A more thourough comparison is needed",
            "review": "This paper considers the problem of learning how to control restless bandits.  When all parameters of the system are known, Whittle index policy usually offers a good performance. The main contribution of the paper is to propose an algorithm, NeurWIN, that uses a neural network architecture to learn the Whittle indices. Most of the paper is devoted to the description and the derivation of the algorithm. At the end of the paper, the authors present four illustrations of how the algorithm works. The algorithm is compared to an index-based policy and an (old?) RL algorithm REINFORCE for the small systems. The learning behavior of NeurWIN is very good in all tested cases.\n\nThe paper does not contain theoretical result. It is mostly about presenting an algorithm and performing numerical experiments to demonstrate that it works. Yet, there is no comparison with (reasonable) related work. In the numerical part, the only algorithm to which the authors compare their algorithm is REINFORCE algororithm of Williams 92, that is clearly non-adapted to learning whittle index (because of the large state-space). As acknowledged by the authors, there are a number of recent work on learning Whittle index (last paragraph of Section 2): I do not understand why these solutions are not implemented. Also, I am not convinced that the algorithm of (Avrachenkov,Borkar,2019) is not applicable to the current context.\n\nThe paper is a bit sloppy and not very precise. For instance:\n- The authors write several times \"Finding Whittle index is typically intractable\" without proof or precise reference.  I doubt that this statement is true.\n- page 4, top: the definition of strong indexability needs some clarifications: how strong is the assumption? Do the presented example satisfy this assumption? And if not, is it important?\n- page 4: is it really the problem statement? Then why are the simulation results not showing this?\n- page 7 (end): \"The size of the state-space is 10^{12} : given the appendix, this seems largely exaggerated (the order seems closer to 10^6 if one discretize byte per byte). Moreover, discretizing at the kB level would lead to about 10^3 states in which case Whittle index would be applicable.\n- page 8 (beginning): \"REINFORCE [...] improves a litle\" -> this seems wrong.\n- page 8, Figure 4(a): how can NeurWIN perform better than Whittle index? I thought that NeurWIN was learning those indices.\n\nThe paper would be much stronger, if it would contain either some theoretical guarantee, or a more thourough comparison of performance with related work.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A principled approach for Whittle Index based neural network training that seems to work well in practice for restless bandit problems",
            "review": "The paper proves a simple but important result which essentially states the following: It is possible to construct an RL environment for a one arm restless bandit system, such that any neural network which achieves a certain discounted net reward has to approximate the whittle index of that arm well.\n\nThe paper then uses this observation to form exactly such an environment and train a neural-network using REINFORCE to maximize discounted reward for the environment, thus in principle achieving a good approximation of the Whittle index of the arm. The network can then be used to approximate a Whittle index based policy for restless bandit problems. \n\nI think the idea is interesting and it seems to work well in practice on three simulated but well-studied environments. In particular in the deadline scheduling problem where the optimal strategy is known, it can be observed that this policy can match the performance of the optimal policy after several hundred training epochs.\n\nI recommend the paper for acceptance. I would encourage the authors to provide confidence bars for their algorithm as the results are presented over 10 independent runs only. It is vital to see the variance in rewards obtained by the strategy.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "very interesting, more baselines would be welcome",
            "review": "The paper \"NeurWIN: Neural Whittle Index Network for Restless Bandits via Deep RL\" proposes a new RL approach for estimating the Whittle index in restless bandit problems, which allows to define effective strategies for selecting arms to activate at each round of the bandit process. \n\nI found the paper very interesting. Maybe this was because I was not familiar with the Whittle index  and such kind of policies for restless bandits. But the paper is very didactic, I liked to discover these concepts. \n\nMore importantly, I found the proposed approach very well presented, with relevant  theoretical justification provided. The approach is elegant and looks useful from the results. \n\nI still have some concerns however. First, one limitation of the approach is that it requires to have access to a simulator in the training phase, that strictly follows the dynamics of the real world it is designed for. It would have been nice to consider the impact of discrepancies between simulator and real-world dynamics for the setting considered. What happens if dynamics are not stationary? \n\nSecond, it would have be very useful to consider more baselines in the experiments. Authors argue that they only could applied a classical Reinforce strategy on the (4,1) setting, since the combinatorial aspect of the action and state space is problematic beyond that point.  They also claim that if this approach is not effective even in that simple setting, it is mostly due to the combinatorial dimension of the state space. Ok, but other instances of this could have been considered. At least, it would have been important to consider a policy trained with Reinforce on individual states, and then select the M best outputs at each round,  as it is done for the proposed approach ! Also, multi-agent RL approaches, such as MADDPG or Q-Mix, could have been considered to cope with the combinatorial aspect of the problem, and assess if cooperation would help improve the results. \n\nAt last, authors give conditions for the applicability of their method. It would have been useful to help the reader with some intuitions about what imply these indexability and strong indexability conditions. Are they very constraining? What kind of restless bandit settings are excluded due to these two necessary conditions ?\n\n    ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The experiment on recovering bandits is not convincing",
            "review": "This paper proposes the use of Deep Learning, namely a multi-layer perceptron, for approximating the Whittle index in restless bandits.\n\nThe introduction and the related works are well written. The problem and the background on restless bandits are clearly exposed.\n\nHowever, there are a lot of issues in the presentation of the proposed algorithm, NeurWin, in the analysis of the proposed algorithm, and in the experimentations.\n\nThe statement of Corollary 1 is not correct. In the general case, the Whittle index is not optimal, it is a heuristic. So if the neural controller produces the Whittle index, it does not produce the optimal discounted reward. As the authors suggest the statement of Corollary 1 should be rewritten as a necessary condition for a neural network to be Whittle-accurate.\n\nThe proof of Theorem 2 is wrong. \nThe statement of Theorem 2 is for any states s_0,s_1, for any \\lambda \\in [f_\\theta(s_0)-\\delta, f_\\theta(s_0) + \\delta ] â€¦ then the neural network is \\gamma-Whittle-accurate.\nIn the proof the authors only show that the neural network is \\gamma-Whittle-accurate, when s_0=s_1 and when \\lambda =,f_\\theta(s_0) + \\delta. So they cannot conclude that Theorem 2 holds for any states s_0,s_1, and for any \\lambda \\in [f_\\theta(s_0)-\\delta, f_\\theta(s_0) + \\delta ].\n\nIn the section 4.2 where the training procedure is described the authors write that the choice of s_1 depends on the MAB problems, and hence it has be chosen knowing the MAB problem. The hearth of MAB problem is precisely that you do not know if some states or some arms have to be less-frequently-visited that others. This is the exploration / exploitation dilemma. \nSo if for tuning the proposed algorithm you need to know the MAB problem, the proposed algorithm does not work at all.\n\nIn the experimental section the experiment on recovering bandits is not convincing. \nThe authors write that the algorithmic complexity of the algorithms proposed in recovering bandits is (\\binom(N,M))^d. It is not correct. In recovering bandits the arms are the recovering functions. In your experiment it should be N, and d is the number of times the arms are sampled in a round, so in your experiment it should be M. The algorithmic complexity is N^M, that it is still very large. However in the cited paper the authors propose the use of optimistic planning with a given budget B. So the algorithmic complexity of their algorithms is B, which is a parameter. \n\n\nSo the choice of d=1 in the experiment is unfair.\nMoreover, in the experiment the choice of only four different recovering functions for 100 arms is a little bit strange. In recovering bandits the arms are the recovering functions.\n\nFinally, the choice of training offline the neural network is not realistic in a bandit setting, where the environment is not known at the beginning. To make a fair comparison with other bandit algorithms the authors should train the neural network online and compare the accumulated rewards taking into account the convergence time to a good solution.\n\nApproximating the Whittle index by a multi-layer perceptron is a good idea, but the submitted paper is not ready for publication. \n\n__________________________________________________________\n\nAfter rebuttal, the authors reformulate Theorem 2, and then I think it is right. I raised my score.\nI am still not convinced by their experiments on recovering bandits.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}