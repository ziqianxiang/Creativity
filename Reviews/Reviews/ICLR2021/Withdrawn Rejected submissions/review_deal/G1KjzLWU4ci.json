{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting idea, problematic presentation",
            "review": "The paper presents an  idea for regularizing the output of a neural network for both improved accuracy and improved confidence calibration. Inspired by the mixup method of Zhang et al 2018, the basic idea is captured in the loss function in equation (3): convex mixing of logit layer  outputs  (for unmixed inputs) should be similar to the logit layer output for a mixed input (under the same mixing coefficient). The authors argue that enforcing this similarity loss at the logit layer helps to preserve class relationships, since logits serve as an auxiliary weak-supervision signal.\n\nPros:\n+ The idea is interesting, and seems to be grounded in existing work that shows convex mixing strategies can be beneficial and lead to smoother classification boundaries.\n+ Image classification results show improvement over existing methods for multiple datasets and architectures.\n+ Calibration error (Guo et al 2017) and overconfidence penalty (Thulasidasan et al 2019) are also improved with this method.\n\nCons:\n- My biggest concern is the quality of writing. I found many sentences hard to parse, and grammatically unsound to the point that it interferes with the flow and understanding of the paper. This is unfortunate, as I think the technical idea has merit, but the paper needs a rewrite.\n- Results are somewhat underwhelming for CIFAR-100 and do not appear to justify the extra computation. \n- It appears that the benefit is somewhat diminished for  residual networks.  Is this possibly due to the effect of skip connections? It would make the paper stronger if this can be addressed.\n\nAdditional question:\n- It seems to me that what $L_{sim}$ in  equation 3 is really doing is forcing the network ($f$) to behave linearly as follows:\n if $f$ were linear, and given that $x_{mix} = \\lambda x_1 + (1-\\lambda)x_2$, then,  $f(x_{mix}) = f(\\lambda x_1 + (1-\\lambda)x_2) = \\lambda f(x1) +  (1-\\lambda)f(x_2)$. In which case $L_{sim}$ would be 0. But we know $f$ is not linear, and the presence of the classification loss would anyway suppress this trend towards linearity.  But as the authors show, the decision boundaries are smoother (as one  would expect from a classifier whose output was linear in the inputs).  Can the authors discuss this?\n\nIn summary, even though I think the idea has merit, in my opinion the quality of writing (i.e language;  not the technical content) does not meet the bar for ICLR acceptance in the current state. I strongly encourage the authors to polish the paper, and also address the other concerns raised above. \n\nReferences:\n+ Zhang et al 2018:  mixup: Beyond empirical risk minimization. ICLR, 2018.\n\n+ Guo et al 2017:  On calibration of modern neural networks. ICML 2017 \n\n+ Thulasidasan et al 2019: On mixup training: Improved calibration and predictive uncertainty for deep neural networks. NeurIPS 2019.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting paper that tackles the issue of data-efficient training strategies in the case of multi-class problems",
            "review": "The paper proposes a new training strategy modeling the inter-class relations for data augmentation in order to improve both prediction performance and calibration errors. The method is evaluated for image classification problems with a huge number of classes.\n\nThe paper addresses a challenging issue that is the use of neural networks in the case of multiclass problems, by proposing a new data-augmentation technic. The idea behind the proposed approach is to reproduce human ability to infer inter-class relations. \nThe paper is well written and the positioning towards the state of the art is very clear. The experimental part proposed toy examples, as well as experiments on image datasets. The method improves results.\n\nIt would be interesting to discuss in the experimental part what kind of inter-class relationships have been learned on the studied datasets and to investigate how relevant they are from a human perspective. Did the method manage to reproduce the human association process?",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good results, could use better narrative",
            "review": "The paper introduces a loss/data augmentation method that is quite similar to mixup. The difference is that, rather than computing the cross-entropy between predictions (prediction from mixed data point and convex combo of pure predictions with same mixing weight), squared distance is computed between logits. Also, this method keeps the original cross entropy terms for the original two data points.\n\nStrengths:\n* The empirical outcome seems quite promising, and it could be of interest for applications where confidence measures and classifier calibration is important.\n* It is as relevant to the field as other data augmentation techniques\n\nMajor comments:\n* The biological motivation behind the technique looks somewhat unfounded. The message the authors are trying to drive home is \"deciding between choices is easier when there is a notion of closeness/distance between them\". The examples given of how humans do things in their lives seem more incoherent, and point more to the ability to navigate by using surrounding context, which is quite generic (and underlie many of the general ideas behind computer vision and NLP already). I would either be very specific about how the inspiration is drawn, or not mention this bit.\n* The reason given in the introduction section for advocating for logits (vs softmaxed logits) is carrying better information because of having negative values and higher dynamic range. This is a faulty argument: 1) logits are recoverable from the softmax output up to a constant, 2) logits themselves are arbitrary and solely cater to the needs of the final optimization function, and you can still achieve the same classifier accuracy if you fixed the logits to be always non-negative. The effect of any particular supervision signal (logit or softmax) cannot happen in isolation from the optimization function. In fact, one might argue for the squared euclidean distance as the main agent for sharpening the prediction distribution with logitmix.\n* The text has a tone toward differentiating the contribution, and this is completely understandable. In doing so, the authors should take care of mainly detailing methods and showing results, trusting the audience to make their interpretations and to validate the merits of the presented work.\n\nMinor comments:\n* In page 2 paragraph 2, it reads \"the proposed method improves generalization performance by preventing both over-fitting and under-fitting\". Generalization in ML deals with the gap between seen and randomly unseen data. Under-fitting is typically an issue of high bias, and it deals more with training loss than generalization.\n* In the paragraph under eqn 2, the definition of the softmax function is not mathematically valid (what is z? is it a vector? if not,  what is $z_i$?)\n* In eqn 3,  $\\mathcal{L}_{cls}$ apears without a definition. I can guess that this is the individual softmax terms, but it should be defined.\n*In eqn 3, it's worthwhile mentioning that $\\text{x}_{\\text{mix}}$ is as defined in eqn 1.\n* Section 4.2 should really be an experiment section, not a proof of concept section.\n* In Table 2, the metrics for vanilla logitmix  should also be shown.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The premise is not convincing, and there is a lack of uncertainty estimation in the results",
            "review": "I don't find the premise of the paper to be very clear or convincing. From what I understand, the problem statement is that deep neural networks can exert overconfidence (which indeed happens, and can be a bad thing) and that we can mitigate this by encouraging more exploitation of 'inter-class relationships', and that the softmax objective doesn't do this or does it badly (quoting: \"...because the one-hot labels induce zero probability for non-target class labels, it implies no inter-class relationship explicitly\"). The proposed solution to this issue is mixup, though rather than mix in probability space, do it in the logit space before the softmax.\n\n- Firstly, there is no reason why a probability distribution can't reflect inter-class relationships -- it's a probability distribution. Assuming 'mouse' and 'keyboard' were both classes in your example dataset (first paragraph introduction), a well regularised classifier could plausibly put probability mass on both those objects. If 'keyboard' wasn't one of the classes, with enough similar examples the network could plausibly infer that relationship. I somewhat grasp the 'gist' of your point -- minimising the training objective down to zero would give you one-hot probabilities for all examples in the training set -- but we are mostly interested in the corresponding objective on the validation set and its accuracy, and we tune hyperparameters (including those that can mitigate overconfidence) to perform well on those sets. \n- Secondly, the softmax is simply a normalisation of those logits so that each of those now-transformed 'logits' sum to one, so any negative logits would be closer to 0 than positive ones. I get that the softmax function is non-invertible and does reduce the range of dynamic values that can be expressed -- so some information is lost compared to what is in the logits -- but this doesn't mean that complex reasoning steps are not happening in the layers beforehand. However, in that case LogitMix could probably be more appropriately motivated by simply saying that performing mixes in logit space makes sense because it's more 'information rich' than the probability space. Or, since you have also explored confidence calibration, you could also motivate LogitMix from that point of view.\n- Table 1: what is the accuracy measured on, valid or test set? Make it clear how you performed hyperparameter tuning (hopefully not on the test set). What mixup loss is being used in Table 1, vanilla mixup or CutMix?\n- Equation 1 is slightly confusing: x_mix is defined as both the scalar convex combination (with lambda), and a mask-based combination that looks like it may be CutMix. If x_mix is really the binary mask based approach, what is lambda? Is this the number of 1's in the mask? (since y_mix also seems to be a convex combination involving lambda as well.)\n- I am skeptical of the section \"Why LogitMix?\" All it seems to do is rewrite the L_sim equation to show that the pre-logit layer is performing global average pooling (GAP) over each feature map. To me, this seems to border on decorative math.- Table 2 does not have any measures of variance, you should add these since you mentiond doing 5 runs per experiment. Otherwise I cannot be sure about the statistical significance of these results.\n- Also, I would avoid using vague buzzwords like in the sentence \"LogitMix can create more synergy when...\".\n\nIn summary, I don't find the motivation of the paper to be clear, Table 2 does not appear to have any measures of uncertainty, and Table 1 needs more clarification (even if it does seem to indicate mixing in logit space is better than in probability space). I am open to raising my score if my points are addressed, but for now I am unfortunately giving this a clear reject.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}