{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes inner ensemble method where output of inner layers are replaced by an ensemble average of them during inference time to reduce inference time and reduce variance. The authors include experiment results showing performance improvement of their method and use the theoretical analysis of the dropout and maxout to justify the goodness of the proposed method. \n\npros.\n-The authors consider a useful and interesting problem.\n\n-The proposed method is simple and easy to plugin. \n\n-The results show performance improvements in a number of cases.\n\n\ncons. \n-However there are various concerns raised by the reviewers that I find are not well-justified.\n\n-The use of inner ensembles are not well justified.\n\n-The baseline in the experiments miss the usual ensemble methods instead the authors use single model which does not have the same number of parameters.\n\n-novelty and significance- The authors do not clearly distinguish their method from earlier results and how it differs and improve over existing work. Given the current state of the paper, the novelty is not significant.\n\n-clarity of theoretical analysis is lacking.\n\nOverall I suggest the authors improve the manuscript considering concerns and suggestions listed above and through reviewers and submit to an upcoming venue.\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "proposed model equivalent to baseline model with different hyper-parameters?",
            "review": "The paper proposes replacing each linear layer with a linear ensemble (average) of m (m>1) linear layers.  Authors argue that this helps to reduce network internal variance and achieve better accuracy.\n\nI think it is possible to see that the proposed model is equivalent to the baseline model with a different hyper parameter setting (specifically the random initialization and learning rate), and therefore the observed variance reduction and improved performance are a result of better hyper-parameter choice in the proposed model.  The equivalence between the baseline model (with single linear transformation in every layer) and proposed model (m linear transforms in every layer whose output is averaged) can be seen by observing that:\n\nBaseline:  y = w.x\nProposed: y = (1/m) sum_j w_j x = (sum_j (1/m)w_j) x\n\nWe see the following relation between initial weights of baseline and proposed model: w = \\sum_j (1/m)w_j.   Furthermore, the gradients of objective w.r.t. w and w_j also follow a linear relationship: grad_{w_j} = (1/m)*grad_w, and due to this gradient scaling, the effective learning rate of the proposed model is smaller by a factor (1/m) as compared to the learning rate of the baseline model. Gradient of objective w.r.t. layer input x is same in the two models as long as the gradients w.r.t. output y are same and weights follow the relationship w = \\sum_j (1/m)w_j.\n\nThus I would think that if the baseline model follows hyper-parameter settings that are equivalent to the proposed model — i.e. weight initialization such that w = \\sum_j w_j (leading to reduced variance of initial weights), and a learning rate that is smaller by factor (1/m) — then it should see variances and performance that is akin to that of the proposed model.\n\nI'd like to hear author's response and discuss if my reasoning is flawed.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A practical way to reduce the variance within the neural network but lacks theoretical rigor",
            "review": "The authors propose a drop-in replacement for CNN and FC layers that use multiple instances of the same layer and apply the average operation to those layers’ output. For inference, the weights are averaged themselves, so the params count in inference is the same as without the inner ensembles. The authors show that IEN ensembles of size m decrease the overall variance of a deep network by a factor of $\\frac{1}{m^{L-1}}$. They also perform experiments with various image models and finally show theoretically and empirically that their method leads to a greater decrease in variance and error rate than similar methods (maxout, dropout).\n\nPros:\n+ A simple and practically viable variance reduction scheme\n+ Good experimental design for the main set of experiments that shows clear advantages of the proposed scheme\n\nCons:\n- The theoretical analysis lacks rigor. There are hidden  assumptions in the methods used to achieve the conclusions. Please see the comment section below for more details.\n- The proposed method being simple means a lot of related works used methods similar to the proposed one. The related work section lacks a clear explanation for what distinguishes the paper from the others and makes it novel (for example, comparing to Abbasian et al., 2013)\n\nGeneral comments/questions:\n* (Section 3) It seems that $x_l$ should be $n_l$ dimensional and $y_l$ should be $d_l$ dimensional for variance in (4) to be correct\n* (Section 4.1) The derivation from (6) to (7) is too important to be hidden inside the appendix as it hides the assumptions made in A.2. Better move it to the main part of the paper.\n* (A.2) There are two parts here aiming at one goal: one is (24)-(29) and one from (sgK). Better keep only one of them for clarity.\n* (A.2) The equations in (24)-(29) are not generalized. In (24) and in (sgK) x is assumed to have zero mean. The authors have to prove (24)-(29) without this assumption. This can be done for specific activation functions, for example, RELU, but for RELU (24) doesn’t stand true without any additional assumptions.   \n* (Section 4.1) Equation (11) should probably use n_l\n* (Table 1) What is $Base_{\\widetilde{w}}$?\n* (Table 1) This table has the $IEN+FC_{\\tilde{w}}$ results but no $IEN+FC$ results. Can those be added for the sake of completeness? \n* (Table 1, Figure 2) Nit: the extensive use of colors makes the paper harder to process for colorblind people \n* (Section 2) (Abblasian et al., 2013) seems to be the first paper introducing Inner Ensembles. The authors should have a more extensive explanation of what is new in their paper and what distinguishes it from that existing one.\n* (Section 2) (Opitz et al., 2017) does not force the use of a special loss function instead of a main loss but instead suggests using additional loss function to achieve better performance through making weights more diverse. Potentially Inner Ensemble Networks may benefit from using a similar loss (but maybe not the $IEN_{\\widetilde{w}}$ versions). Adding those additional losses may be an interesting new set of experiments to run. \n* (Table 2) It is unclear why the outer ensemble outperforms single model IEN (not the $IEN_{\\widetilde{w}}$). The paper will benefit from a discussion on why this may be happening. Is it because IEN makes model weights too close to each other?  \n* (G) The dropout results are very poor. It suggests the models were not operating in a normal state. This may be the same issue as discussed in https://arxiv.org/abs/1603.05027. The https://arxiv.org/abs/1605.07146 suggests that proper placement of dropout can fix this problem.. In any case, comparing Dropout+IEN and Dropout is misleading as all Dropout experiments look catastrophically broken\n* (Section 5) A more detailed description of CNN setup with IEN would be for the paper’s benefit.   \n* (Table 1) Was maxout also used as a drop-in replacement without hyperparams tuning? It may be beneficial to also tune both maxout and IEN to distinguish IEN advantage of being a drop in replacement and possible IEN advantage when both methods are tuned to their maximum potential\n\n\nOn rating:\nThe Inner Ensemble Networks method looks like a simple, clean and easily reproducible way to reduce variance within a network. The authors extensively test their method empirically and show that this method also decreases error rates for various image-based models. However, the presented theoretical analysis lacks rigor and has to be updated to be formally correct.\n\nUpdate: taking authors comments and improvements into account I've updated the rating from 6 to 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An efficient to reduce model variance but some modificiations are needed to be more convincing. ",
            "review": "Summary:\n\nIn this paper, the authors proposed Inner Ensembe Networks (IEN), which trains an ensemble of layers and therefore reduces model predictive variance. Experiments on image classification demonstrate that the proposed method is effective in reducing model variance and it also leads to an improved accuracy, compared to other variance reduction baselines such as maxout and dropout.\n\nPros:\n\nThe theoretical analysis (under some assumptions) in Section 4.3 and 4.4 illustrates the connection between IEN and dropout & maxout. Therefore, the improvement of the proposed method (IEN) on variance reduction over dropout and maxout is theoretically justified. The authors also show the boundary case where IEN achieves the same variance reduction as dropout and maxout. \n\nThe empirical evaluation matches the theoretical analysis. Figure 2 demonstrates one of the core contributions of this work: the proposed method IEN reduces the model variance even further than dropout and maxout.\n\nFigure 3 demonstrates how ensemble size affects the ranking of all methods and all architectures considered in this work. It shows that the improvement of IEN is consistent among all network selections and ensemble sizes. The authors also showcase the benefit of IEN can be carried to deep ensembles (outer-ensemble defined in this paper) and NAS cells.\n\nCons:\n\nMy concern is the motivation of why variance reduction is needed in deep neural networks is not highlighted. This paper implicitly assumes that lower model variance leads to better performance (in terms of accuracy metrics). However, the connection between model predictive variance and averaged model accuracy remains unknown (especially in deep ensembles). Recently many papers aim to improve ensemble diversity. It improves the ensemble performance but it also leads to larger model variance. On the other hand, regularization can be connected to better generalization performance, but this paper spends more paragraphs on the variance reduction effect. \n\nThe authors mentioned that reducing the variance in the prediction helps the ensemble of networks generalize better to unseen data. This implies that the proposed method IEN is supposed to outperform other baselines on out-of-distribution dataset (CIFAR-10C and CIFAR-100C). It would be more convincing if the authors can add empirical evaluations on out-of-distribution datasets.\n\nThe proposed method shares some similarities to Stochastic Weight Averaging (SWA) [1]. SWA also leverages the mean of multiple copies of the network weights, which also reduces the model variance. One main difference is SWA is a global mean of network weights while IEN takes the local (layer) averaged weights. The paper would be more inspiring if the authors can discuss the connection to SWA and make a comparison in the empirical evaluations.\n\nOverall, the paper proposed an interesting method to efficiently reduce model variance, which leads to an improved performance. But the cons outweight the pros in its current version.\n\n[1]: Izmailov, Pavel et al. “Averaging Weights Leads to Wider Optima and Better Generalization.” UAI (2018).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns on the limited novelty of this paper",
            "review": "**Summary**\nThis paper proposes \"Inner Ensemble Network\" that ensembles intermediate layers in a deep neural network, which would be trained simultaneously during the training. On the inference time, the ensembled portion of the network would be substituted by the averaged layer, enabling lighter-weighted inference while preserving the benefit from the ensemble. Similar to the classic ensemble methods, this method can reduce the variance of the training and achieve better than the models without the ensemble.\n\n**Originality and significance aspect**\nThis is probably the most weak aspect of this paper. The proposed methodology, Inner Ensemble Networks (or IEN) is very similar to the idea of average pooling layers that are widely used in computer vision architectures.  It is also fairly similar to some variants of online ensemble methods such as [1, 2, 3]. [2] is actually already cited in this paper; however, my point is that there is not enough novelty on this paper compared to these prior works. Authors claim this paper is simpler to apply than [2], but they do not have any empirical evidence that their proposed method is better than [2]. What if [2] works much better because their *sophisticated* loss function actually works out nicely?\n\nFrom my understanding, IEN works just as the average pooling layers of duplicate layers and probably only novelty here is that they substitute the ensembled layers with the averaged layer similar to drop-out technique. I feel this is not enough for a full conference paper even though this paper may be still useful in practice.\n\n[1] Liu, Yong, and Xin Yao. \"Simultaneous training of negatively correlated neural networks in an ensemble.\" IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 29.6 (1999): 716-725.\n[2] Opitz, Michael, Horst Possegger, and Horst Bischof. \"Efficient model averaging for deep neural networks.\" Asian Conference on Computer Vision. Springer, Cham, 2016.\n[3] Lee, Stefan, et al. \"Why M heads are better than one: Training a diverse ensemble of deep networks.\" arXiv preprint arXiv:1511.06314 (2015\n\n**Quality aspect**\nThe paper could be polished further in their figures and equations. For example the bottom line of the right side table of Figure 1 is missing and the left picture is not that informative. Many of the equations are not typesetted correctly. I think authors may consider using LaTex for tye type setting. The parenthesis in the equations (e.g. eq 4-11) should match the inner element’s size.\n\nFor the experiments, I think authors should actually use the classic version of ensemble (call outer-ensemble in Section 5.2 or in Table 2) as their baseline instead of a single model. It is because IEN costs almost 2x during the training and that is similar to the classic ensemble. If authors are concerned about the inference computation time, they can consider a distilled model from the classic ensemble method, which is a fairly standard technique.\n\n**Clarity aspect**\nThe paper is mostly clear. \n\n**Recommendation**\nOverall, the paper is interesting; however, I cannot recommend this paper mainly due to weak novelty and weak empirical support.\n\n----\n**Post rebuttal comment**\nI thank the authors for detailed rebuttal and new empirical results. I also have read other reviewer's comment, and decided to keep my original score. The main concern of this paper, the weak novelty, still remains (also pointed out by Reviewer#1/4). \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}