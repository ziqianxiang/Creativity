{
    "Decision": "",
    "Reviews": [
        {
            "title": "The authors prove the lack of local minima in a specific CVNN, but the generality and of this result is not clear.",
            "review": "In the present work, the authors focus on a fairly specific type of complex valued neural network and prove that with analytic activation function the optimization landscape has no spurious local minima. On the other hand, when the activation is not analytic, e.g. with Crelu activation, local minima can appear in the landscape.\nPersonally, I found the polemic tone of the paper a bit unmotivated and the clarity of presentation a bit lacking. Describing all previous theoretical work on the subject as full of unrealistic assumptions (the detailed table is hard to read and referencing to external work seems to be necessary in order to understand it) and then taking a very specific example of a CVNN as the unbiased solution is slightly confusing. While many readers and practitioners may be unfamiliar with Wirtinger calculus, I think the basic crash course shouldn't appear in the main text of a conference paper.\nI am not confident in believing that the presented study case can be really held as proof of superiority of CVNN to usual DNN, since the generality of the arguments used by the authors is not clear. Moreover, in practice optimization is usually not at all a problem for DNN and the focus is on generalization (not discussed here), so I don't think this work provides a strong reason for moving to CVNN as claimed by the authors.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting result, proof clarifications will help",
            "review": "### Summary:\n\nThis paper is theoretical and aims to prove a theorem stating that no spurious local minima exist for one hidden layer complex-valued networks with a quadratic activation. This is done without unrealistic assumptions. The bulk of the literature focuses on real-valued networks and given the already demonstrated potential of complex-valued networks it makes sense to perform such analysis for complex-valued networks. Wirtinger calculus and related technical tools are used to obtain derivatives for complex-valued functions. The proof is inspired by Soltanolkotabi et al. 2019 who prove the same result for real-valued networks of the same structure (with assumptions). \n\nThe paper also introduces another theorem for similar structured networks with a complex-ReLU instead. The way the paper is written, it is not the main focus of the paper. The proof for this follows Yun et al. 2019. \n\n\n########################################\n\n### Strong points:\n\nThe paper is well motivated and the authors convey the importance of the problem. Furthermore, required knowledge such as Wirtinger calculus is explained with care.\n\nThe result that there are no spurious local minima is interesting.\n\nUnlike previous similar results (mainly for real-valued networks), the proven theorem with quadratic activations does not have any assumptions that are unrealistic. \n\n\n########################################\n\n### Weak points:\n\nIn Lemma 2 the defined matrix, $\\tilde{\\nabla}^2 \\mathcal{L}(\\tilde{\\mathbf{W}})$, is not the Hessian (compare to the Hessian at the bottom of page 5). However, the proof of Lemma 2 uses this matrix as a Hessian in order to do a second order expansion. Why was $\\tilde{\\nabla}^2 \\mathcal{L}(\\tilde{\\mathbf{W}})$ defined in this way? If the actual Hessian is used, it should still be possible to show that the quantity is real. \n[Note that actually the order of the derivatives in $\\tilde{\\nabla}^2 \\mathcal{G}_i (\\tilde{\\mathbf{W}})$ in the proof is not the same as the order of the derivatives in $\\tilde{\\nabla}^2 \\mathcal{L}(\\tilde{\\mathbf{W}})$ of Lemma 2. This should be corrected]\n\nLemma 2 states that it hold for *all* complex-valued $\\mathbf{h}$. However, the last step of the proof of Lemma 2 states that $\\|\\|\\mathbf{h}\\|\\|$ should be ‘small enough’. How does this let the lemma hold for all $\\mathbf{h}$? Furthermore, how come the final inequality in the proof of Lemma 2 is guaranteed to always hold by truncating the series expansion and ignoring higher order terms?\n\nPlease can the authors explain the final steps starting from the proof by contradiction step in the proof of Theorem 1 in more detail? It is mentioned that $(b_1, b_2)$ can be chosen depending on $(a_1, a_2)$. However, after $\\mathcal{M}$ is defined, $\\mathbf{b}$ is being designed without any consideration for $(a_1, a_2)$. It seems that we need to guarantee that $(b_1, b_2)$ obeys certain values depending on $(a_1, a_2)$---for example when $a_1 >0$, $a_2 <0$ we do not want $b_1 < 0$, $b_2 < 0$.\n\nThis work is a theoretical contribution and should be mainly viewed from that perspective. However, simple simulations verifying whether Theorem 1 holds would strengthen the paper. For example, using Wirtinger gradient descent, can the same loss function value be obtained using different random initialization points? As Soltanolkotabi et al. 2019 considers a similar network structure, simulations showing that their assumptions are indeed not required in complex-valued networks would also help.\n\n\n########################################\n\n### Recommendation:\n\nOverall I lean towards rejection. I think the result that there are no spurious local minima with no assumptions is interesting, however, there are some questions regarding some of the proofs which should be clarified.\n\n\n########################################\n\n### Additional questions and clarifications that will help assessment:\n\nPlease address and clarify my above questions and queries.\n\nIt would be useful to describe the proof strategy and high level steps for the proof of Theorem 1 before proceeding with the proof. Furthermore as some of the proof is similar to Soltanolkotabi et al. 2019, it would help if the differences and novelties in the proof presented are highlighted.\n\nThe proof for Theorem 2 is tricky to follow because it appears to prove a more general result and multiple new quantities are introduced. It would be helpful to connect back to the statement of the theorem more. Furthermore, while it is following Yun et al 2019, it would be useful to explain the high level methodology in more detail. As a reader who is not familiar with their original proof this would be invaluable.\n\n\n########################################\n\n### Additional feedback that does not necessarily impact recommendation:\n\nThere is a typo with the vectors in the Taylor expansion in the proof of Lemma 2. For example the first order term has a dot product with $\\begin{pmatrix} \\mathbf{h} \\\\\\\\ \\mathbf{h} \\end{pmatrix} $\n\nShould the final sentence of Lemma 2 be $\\mathbf{h} \\in \\mathbb{C}^{kd}$ because $\\mathbf{h}$ is a vector?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary\n\nThis paper studies the loss surface of complex-valued neural networks with quadratic activation and (complex-)ReLU activation functions. This paper shows that in case of 1-hidden-layer complex neural networks with quadratic activation functions, there is no bad local minimum in the training loss. In contrast, complex ReLU networks are shown to have bad local minima.\n\nStrength\n\nThis paper studies the optimization landscape of complex-valued neural networks, which has not received much attention in the literature. The paper extends results by (Soltanolkotabi et al., 2019) and (Yun et al., 2019) to shallow complex-valued networks and obtains similar results as the real-valued counterparts. The paper gives a good literature overview of complex-valued neural networks, and also a good review on calculus of complex variables.\n\nWeakness\n- The paper considers the setting where only the first layer of the network is optimized, while the parameters in the second layer are fixed. I am skeptical whether the positive result on quadratic activation can be extended to deeper networks.\n- I believe the proofs given in the paper are not self-contained. In the beginning of the paper the authors emphasize that the difference of real vs complex variables can result in nontrivial differences in the loss landscape. However, when it comes to proofs, the paper carelessly states “the rest of the proof can be done in the same way as the real-valued networks”, without fully addressing the potential issues in extending real variables to complex variables. To be more specific, at the end of page 7, the paper says “the proof of the first case is the same as in (Soltanolkotabi et al., 2019)” without even pointing out which part of the cited paper should be looked at. Also, the proof of Theorem 2 (on complex-ReLU activation) ends abruptly by saying “the rest of the proof follows as in (Yun et al., 2019)”.\n- In Section 1.1.2, the authors claim that Assumption C in (Soltanolkotabi et al., 2019) is restrictive and unrealistic. However, given that we have the freedom to choose the second layer parameters $v$, I am not fully convinced that this assumption is necessarily strong or unrealistic. Also, although I didn’t find it stated anywhere, it seems to me from Section 3.3 that Theorem 1 of this paper also implicitly assumes $k \\geq d$, which is not much different from the requirement $k \\geq 2d$ in Assumption C (Soltanolkotabi et al., 2019) because the network is complex-valued. The reason why I believe Theorem 1 assumes $k \\geq d$ is that the proof considers a case \"${\\rm rank}(D_v W) = d$,” which can hold only if $k \\geq d$. This makes me think that both theorems are rather direct extensions of the real-valued counterparts, without providing any interesting difference between complex vs real neural networks. Please do correct me if I’m mistaken.\n\nOverall Assessment\n\nAlthough this paper brings up an interesting problem to look at, due to the weaknesses pointed out above, I believe the paper does not meet the standard for acceptance.\n\nMinor comments\n- Some uses of \\citep should be replaced with \\citet, e.g. in Section 1.1.1.\n- At the end of Section 1.1.2, it is stated that “non-linear neural networks can approximate any function by universal approximation theorem.” However, if we are considering 1-hidden-layer networks, just non-linearity is not enough; the activation must be non-polynomial. For example, if the activation function is quadratic, the network can only represent quadratic functions of the input, so one can never approximate higher order polynomials.\n- Section 3.1.1, 2nd derivative w.r.t. $w_p$: missing $\\partial$ in the denominator.\n- Lemma 2: shouldn’t $h$ be a $kd$-dimensional vector instead of a matrix?\n- In Section 3.3, the matrix $U$ comes out of nowhere, without any explanation.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}