{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "**Problem significance** This paper proposes an attack mechanism in the latent space of a neural network f(x), which produces out-of-distribution examples. The AC agrees reviewers on the significance of the OOD detection problem, particularly addressing the vulnerability aspect is relevant and of great interest to the community. \n\n**Technical contribution** The AC shares the concern with several reviewers on the limited technical novelty as well as the problem formulation. While the authors have clarified the difference between adversarial attack vs. OOD attack, the underlying attack mechanism is not new to the community (except for allowing for a larger degree of search space without constrained by the visual imperceptibility). In some sense, the search is made easier than the standard adversarial attack by removing the similarity constraint. Given the unrealisticness of the created OOD examples (largely noisy patches), the AC thinks perhaps a more interesting problem is to look at naturally occurring OOD examples that would lead to the similar latent encoding w.r.t in-distribution data, or adversarial robustness w.r.t the OOD detector.  This to me, would steer the community in the right direction. \n\nFrom a problem formulation perspective, the AC thinks it's useful to differentiate three highly related attacks (that are distinct but can cause confusions):\n\n- adversarial attack w.r.t the classifier\n- OOD attack w.r.t the classifier \n- adversarial attack w.r.t the OOD detector (see recent works [1][2][3] which considered the robustness aspect of OOD detector)\n\n\n**Rebuttal feedback** The AC recognizes the effort made by the authors to address the concerns and comments raised by reviewers. The AC agrees with R1/R2/R3 that the additional experiments are valuable, however, the changes to the manuscript are substantial enough to deem another round of review in the future venue. The paper can improve with better organization and presentation, moving the results in the appendix to the main paper. \n\n**Recommendation** The AC recommends rejection. \n\nReferences\n\n[1] Sehwag et al. Analyzing the robustness of open-world machine learning. 2019\n\n[2] Hein et al. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. 2019\n\n[3] Chen et al. Informative Outlier Matters: Robustifying Out-of-distribution Detection Using Outlier Mining. arXiv:2006.15207\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "Review AnonReviewer1",
            "review": "\n**UPDATE**\n\nI acknowledge that I have read the author responses as well as the other reviews. Overall, I appreciate the clarifications and added experiments given by the authors. \n\nMy concerns about the low novelty of the presented algorithm and findings remain, however, as I find the OOD attack to be only a slight modification of existing adversarial attacks.\n\nI also appreciate that the defense solution claim has been weakened and moved to the appendix, yet these promises are still left to be validated.\n\nLastly, I find all the many added experiments positive, but these have significantly changed the content of the initial submission at this point, which is somewhat out of scope of the ICLR rebuttal phase (see Q4 in the FAQ of the Reviewer Guide: https://iclr.cc/Conferences/2021/ReviewerGuide).\n\nFor these reasons, I would keep my recommendation to reject this work for ICLR 2021, but I encourage the authors to further improve and re-submit the now extended work to some future venue.\n\n#####\n\n\n**Summary**\n\nThis paper presents an algorithm for out-of-distribution (OOD) attacks on neural networks. Given a target network and an initial OOD input, the proposed algorithm performs projected gradient descent (PGD) w.r.t. the input to obtain an output that is close to the embedding of some desired in-distribution sample. The paper makes the case that dimensionality reduction is one key reason that makes standard deep networks vulnerable to OOD attacks, due to the non-bijective nature of such mappings. An experimental evaluation on two ImageNet- pretrained classifiers (ResNet-18 and DenseNet-121) is presented, where a subset of ImageNet serves as in-distribution and chest x-ray, lung-CT, and noise images serve as OOD samples, that demonstrates empirically that these networks can be attacked with the proposed algorithm. Another experiment on the likelihood-based normalizing flow model Glow is carried out which demonstrates that such bijective, dimensionality-preserving deep generative models are also breakable by the proposed attack. Finally, a theoretical sketch for a solution of the problem is described.\n\n\n**Pros**\n+ The paper presents a simple OOD attack algorithm and demonstrates empirically that OOD samples can be perturbed such that they map to the embedding of some arbitrary in-distribution example.\n+ The paper makes a plausible argument that dimensionality reduction enables OOD attacks. Inversely, it is argued that reconstruction methods, which map back to the original space, are favorable for OOD detection for which the proposed attack is also ineffective.\n+ Attacking the dimensionality-preserving, likelihood-based Glow model is an interesting experiment to consider in the context of the presented dimensionality reduction argument.\n+ The paper is structured well and overall well-placed into existing literature.\n\n**Cons**\n- I find the novelty of the presented algorithm and experimental findings to be rather low.\n- The experimental evaluation of the proposed attack is limited to standard classifiers and does not include deep networks that have been trained to increase OOD robustness [1, 2, 4, 3].\n- The defense solution is only a sketch and makes promises that are left to be validated.\n- There are many errors in language and grammar, e.g. wrong use of tense, missing articles, etc. (see minor comments below)\n\n\n**Recommendation**\n\nI think the current paper is ok but not good enough (score: 4) due to (i) low novelty, (ii) a limited experimental evaluation, and (iii) solution claims that are left to be validated.\n\n(i) Though I see and agree that the OOD attack setting is slightly different to adversarial attacks, I find the brittleness of standard classifiers in this regard not surprising. In particular, the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation (pure noise is also OOD, as remarked in the paper), i.e. there is no similarity constraint on the input as there is for adversarial attacks. Moreover, I find the algorithmic novelty to be low as well, since the proposed algorithm essentially is a slight adaptation of previously introduced projected gradient descent (PGD) attacks.\n\n(ii) Following (i), I think the current experimental evaluation is limited and the findings are not surprising for the two standard, pre-trained classifiers (ResNet-18 and DenseNet-121). There exist many approaches that have shown to improve OOD robustness [1, 2, 4, 3], which should be included in the analysis. It would be interesting to see how these approaches perform and compare, which could be insightful for improving OOD robustness.\n\n(iii) Proposing an attack begs the question what possible defenses could be. Currently, the main paper is only phenomenological, i.e. demonstrates that OOD attacks are an open issue, but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated. I don’t say or think such a solution would be necessary for an interesting and valid contribution, as OOD detection poses a hard problem which likely lacks a simple solution, but the current solution is a mere sketch making promises with questions left open. For example, how can a sufficient space saturation be achieved with a finite sample in practice? Which measure to use?\n\n\n**Additional feedback and ideas for improvement**\n\nI think the OOD detection problem, both from an attack and defense perspective is relevant and of great interest to the community, which is why I encourage the authors to build upon and extend the current manuscript. Some ideas:\n\n- Including methods that have shown to improve OOD robustness [1, 2, 4, 3] would greatly improve the value of the analysis. This could be insightful to further improve OOD robustness and see pros/cons of existing approaches.\n- Does adversarial training, as mentioned in Section 2.2, also help OOD robustness?\n- Implementing and validating the presented OOD detection solution would be interesting.\n- Further investigate the reason why the dimensionality-preserving Glow model can be attacked as well, as this suggests dimensionality reduction is only a part of the issue.\n- ‘Could we design an evaluation method (experimental or analytical) that does not rely on OOD samples?’ I think this is a worthwhile question to ask and research.\n- Add missing related work [5].\n\n\n**Minor Comments**\n\n1. ‘The algorithm needs a weak assumption that $f(x)$ is differentiable’ I think sub-differentiable would be sufficient, correct?\n2. ‘If the neural network only uses ReLU activation, then the input-output relationship can be exactly expressed as a linear mapping’ Only piecewise linear mapping, right?\n3. ‘[...], the above mentioned classification-based OOD detection is theoretically almost ineffective [...]’; ‘[...], then it is highly possible that the entire latent space is crawling with the shadows of OOD samples.’; ‘[...], there are a huge number of “holes” in the space, [...]’ Rather avoid such vague formulations.\n4. Published works should be included as such in the references, not by their preprint reference (e.g., [2] appeared at ICLR).\n5. A list of citations in a sentence should be concatenated within single parentheses/brackets, separated by commas (not individual parentheses/brackets per citation).\n6. When referring to a specific section, section should be capitalized, e.g. Section 3.\n7. Abstract: ‘Deep Neural *Networks (DNNs)*, especially convolutional neural *networks (CNNs)*, *have* become ...’ It is a class of models, hence plural.\n8. Algorithm 1: ‘[...] not similar to *any sample* in the dataset.’; ‘$\\alpha$ the learning rate of *the* optimizer’; $h$ used in the Algorithm is not defined; I would use $\\nabla_x J$ instead of $J'$ for the gradient to stress the derivative is w.r.t. the input.\n9. Many figure labels are tiny and hard to read.\n10. The captions of Figure 3 + 4 are (almost) identical. Add a label indicating the network.\n11. Section 1: ‘It was shown by *Nguyen et al. (2015)* [...], and *an* evolutionary algorithm was used [...]’\n12. Section 1: ‘Since then, many methods *have been* proposed for OOD detection [...]’\n13. Section 1: ‘For instance, *Hendrycks et al. (2016) show* that a classifier’s prediction *probabilities* of OOD examples tend to be *more uniform* [...]’ ‘lower’ is ambiguous as the softmax probabilites always add up to 1, right?\n14. Section 1: ‘For *the evaluation of OOD detection methods*, an OOD detector is *usually* trained [...]’\n15. Section 1: ‘[...] pre-trained on *the* ImageNet dataset.’\n16. Section 1: ‘[...] which could be any kind of *image* (even random *noise*) [...]’.\n17. Section 1: ‘[...], which is the input to the last fully-connected linear layer before *the* softmax operation.’\n18. Section 1: ‘[...], and *one* fix to the problem could be using likelihood ratio [...]’\n19. Section 1: ‘[...], we will show that *the* OOD sample’s likelihood score from the Glow model [...]’\n20. Section 2: ‘[...] projected gradient descent (PGD) which is used for adversarial *attacks*.’\n21. Section 2: ‘In practice, *Algorithm 1 can be repeated* many times [...]’\n22. Section 2: ‘[...], the algorithm will have a better chance to avoid *a local minimum* [...]’\n23. Section 2: ‘This *is* simply because the vectors in a lower-dimensional space [...]’\n24. Section 2: ‘[...], which is the Pigeonhole Principle.’ Citation?\n25. Section 2: ‘Usually, *the* training set is only a subset’\n26. ‘3 EXPERIMENT’ Do not capitalize and ‘3 Experimental Evaluation’?\n27. Section 3: ‘*A* Nvidia Titan V GPU was used [...]’\n28. Section 4: ‘We would like to point out that it is *difficult* to evaluate [...]’\n\n\n#####\n\n**References**\n\n[1] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In NIPS, pages 6402–6413, 2017.\n\n[2] K. Lee, H. Lee, K. Lee, and J. Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In ICLR, 2018.\n\n[3] K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS, pages 7167–7177, 2018.\n\n[4] S. Liang, Y. Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In ICLR, 2018.\n\n[5] A. Meinke and M. Hein. Towards neural networks that provably know when they don’t know. In ICLR, 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nThe paper defines an out-of-distribution attack, a process which drives an out-of-distribution (OOD) input to have the same latent representation to an inlier. The paper also analyzes that an encoder is inevitably vulnerable to out-of-distribution attack when its latent dimensionality is smaller than the dimensionality of the input.\n\nDecision:\n\nReject\n\nStrength:\n\nThe paper addresses an important vulnerability of classifier-based OOD detection. As classifier-based method is one of the currently dominating approaches for OOD detection, investigating its weakness is a significant contribution to the research community.\n\nWeakness:\n\nThe proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial.\n\nThe main quantitative result, Table 1, is not very convincing. I suggest the authors  provide AUC scores computed before and after OOD attack, so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance.\n\nThe paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods. Currently, only a relatively simple method of Hendrycks and Gimpel, 2016 is used. The method should include at least [1,2,3] to show the effectiveness of the proposed attack. At least some of these OOD detection methods may be able to resist the proposed attack. For example, multiple hidden layer representations from a classifier are used in [1], and therefore it can still detect OOD even if a specific latent representation is under attack.\n\nThe organization of the paper needs to be improved. In the last sentence of the abstract, \"a simple theoretical solution\" is mentioned but is only addressed in Appendix. If it is a contribution that is important enough to be mentioned in the abstract, it should be covered in depth in the main manuscript instead of Appendix.\n\nMinor comments:\n- The visibility of figures are poor. The axis titles and MAPE values in Figure 2, 3, 4, 5 should be larger.\n- In Section 2.2, 8^{224*224*3} should 256^{224*224*3}. An 8-bit integer can represent 256 values. \n- The captions of Figure 3 and Figure 4 are the same.\n- The captions of Figure 5 and Figure 6 are too close.\n\nTypos:\nlaten → latent (Section 1 paragraph 2 line 10)\nDicussion → Discussion (Section 4 title)\nDifficulty → difficult (Section 4 paragraph 4 first line)\n\n[1] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.  \n[2] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017).  \n[3] Grathwohl, Will, et al. \"Your classifier is secretly an energy based model and you should treat it like one.\" arXiv preprint arXiv:1912.03263 (2019).",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review",
            "review": "Summary of paper: this work shows that adversarial perturbations can make any OOD image, map to the same latent code as an in-distribution - creating an attack on confidence-based or flow-based ODD detection methods. Results are shown on a few datasets with some attempts at evaluation.\n\nNovelty: Unfortunately, I don't think there is much new here. Adversarial attacks are of course well known - I am not sure that attacks on intermediate latent codes present novelty either. Adversarial attacks against anomaly detection methods have also been investigated before (e.g. [1] [2], although their setting is a little different) and there is nothing in the proposed method that is particularly tailored to OOD.\n\nEvaluation: the evaluation is not extensive - only one adversarial attack is investigated and no reasonable baselines have been selected. I am not sure that MAPE is an appropriate metric - it really depends on the allowed perturbation. Ss the allowed perturbation small enough? do the perturbed images look realistic - if I understood Fig.12, they don't - but the caption there is not clear.\n\nClarity - the paper is not particularly clearly written - although the idea is simple enough. E.g. I don't see the scatter plots clearly explained, the analysis in Sec. 2.2 is very dense for a fairly simple idea. \n\nOverall: ultimately, this is conceptually repeating the same thing as any other adversarial examples work, perturbations can make a network confident that any image has the label of another image - and this obviously would overcome confidence based OOD detection methods. I therefore do not see a strong contribution by this work. As there is also very limited methodological novelty, I do not think it should be accepted.\n\n###############################################################################\n\nI understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD. I still don't think that technically or conceptually, there is much difference. The authors presented many fresh results during the rebuttal (which might have been better presented just as a table in the manuscript, rather than on this thread). The experiments can form a part of a resubmission of this work, that will incorporate the extensive comments presented by the current reviews.\n\n[1] Rigaki, Maria. \"Adversarial deep learning against intrusion detection classifiers.\" (2017).\n[2] Bergman and Hoshen, Classification-based anomaly detection for general data, ICLR'20",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments are not comprehensive - Claims are not validated - Organization needs improvement",
            "review": "Summary:\n--------------\nThe paper presents a method that attacks existing out-of-distribution (OOD) detection methods. Most of the existing OOD detection methods perform detection using a latent representation. Main motivation of the paper is that the size of the latent representation is much smaller than the input images which results mapping both OOD and in-distribution images to the same place in the latent space and diminishing OOD detection performance. With this motivation, the proposed method perturbs input images to obtain an image whose latent representation is similar to the latent representation of an in-distribution image. Since such perturbations can be obtained for any OOD image, existing OOD detection algorithms fails distinguishing such OOD samples. The paper contains experiments on multiple dataset to demonstrate that the proposed method obtains a latent representation similar to the representation of an in-distribution image.\n\nComments:\n----------------\n1 - From the abstract, I infer that the paper has roughly 3 contributions: 1) the paper shows existing OOD detection methods are practically breakable, 2) Glow likelihood-based OOD detection is ineffective, and 3) present a simple theoretical solution with guaranteed performance for OOD detection. However, the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea. I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea. Also, since this contribution is mentioned in the abstract, I would expect seeing its description and the results in the main paper rather than the Appendix.\n\n2 - Until the end of the Introduction, it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods. I liked the motivating example in Figure 1, but the contribution can be given in a more clear way.\n\n3 - In the last paragraph of page 2, it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out. I found this statement quite vague. How this operation \"ensures\" that x_out to be OOD after the modification?\n\n4 - In the last paragraph of Section 2.1, it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization. Is this something that you observed empirically? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima. Also, it would be interesting to show how this loss evolves during the optimization.\n\n5 - In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values. So, for an image with size 224x224x3, there are 256^(224x224x3) possible images that can be generated; not 8^(224x224x3).\n\n6 - I didn't quite understand the message in the last paragraph of Sec. 2.2. Why \\Omega_in is split into \\Omega_{in_clean} and \\Omega_{in_noisy}? How this is used in the proposed method?\n\n7 - I think that experimental evaluations are not comprehensive enough, \n7.1. It is mentioned that implementing OOD detection methods are not necessary since Alg.1 already produces z_out ~= z_in. I don't agree with this since most of the SoTA methods do not rely on only a single representation but multiple representations at different layers [ref1, ref2, ref3]. Therefore, it is crucial to show the performance loss of these methods due to the proposed method.\n\n[ref1] Erdil et al., Unsupervised out-of-distribution detection using kernel density estimation\n[ref2] Lee et al., A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks\n[ref3] Sastry et al., Detecting Out-of-Distribution Examples with Gram Matrices\n\n7.2. Benchmark datasets that have been used in OOD detection paper are usually different than the ones used in the paper. Please see the references above. I wonder why the datasets choice are different than the standard benchmarks?\n\n7.3. As mention in paragraph 3 of Sec. 4., there are other generative models that show promising results. However, I didn't quite understand comparison with these methods are \"out of the scope\" while Glow is relevant.\n\n8. The parameters used in different experiments are different than each other. How are these parameters determined? How sensitive is the proposed method to parameter choice?\n\nMinor comments:\n-------------------------\n1 - In Introduction paragraph 2, there is a typo: laten -> latent\n2 - In Sec 2.1, paragraph 3: the algorithm is referred as \"above algorithm\" but it appears \"below\" in the paper.\n3 - In Sec 2.2, paragraph 3, there is a typo: state-of-art -> state-of-the-art\n4 - In Sec 2.2, paragraph 3, there is a typo: laten -> latent\n\nOverall:\n-----------\nI found the idea of attacking existing OOD detection methods interesting. However, I believe, this paper needs improvement in terms of clarity of the presentation, experimental evaluations and the presentation of the contributions. Therefore, my initial rating is reject for this paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}