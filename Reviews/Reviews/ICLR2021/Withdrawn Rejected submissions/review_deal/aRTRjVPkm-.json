{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "We want to acknowledge that there has been a tremendous amount of work done during the discussion period on this paper for clarifying multiple points, adding multiple new comparison methods and new analysis. This is a very different draft than what was submitted and the reviewers acknowledged that. The draft is much closer from acceptance at ICLR than it was at submission time. However, despite all those additions, we do not support a publication at ICLR. \n\nThe main issues with the current draft are its positioning and motivations.  Right now the draft is in-between a paper about Knowledge Base Construction (KBC) and a paper analyzing the knowledge contained within a large language model. This in-between came up in multiple places during the discussion and is what causes the biggest confusion around this work. And, since there is no clear choice, the draft has limitation on either side. \n* If the main point is around KBC to build general-purpose KBs, then one would expect experiments on downstream tasks powered by a KB, language understanding tasks for instance. Indeed, KBs are just a means to an end and the latest advances in very large language models have shown that KBs were not essential to be state of the art in language understanding tasks (GLUE, QA datasets, etc). So we would like to see whether these enhanced KBs could be beneficial. Or the KBs are studied as a way to encode commonsense like in (Bosselut et al., 2019) or (Davison et al., 2019), but this is not the point of the current draft.\n* If the main impact of the draft is around what the language models learn, bridging the deep language model and knowledge graph communities through enhanced model transparency, as it has been said in the discussion, then the discussion with (Petroni et al. 19) should be more prominent and the introduction, motivation and experiments of the draft should reflect that.\n\nThat's why, even if this work is of solid quality, the current draft can not be accepted."
    },
    "Reviews": [
        {
            "title": "Interesting unsupervised IE approach but not 100% convinced",
            "review": "Paper summary: The paper introduces an unsupervised method that utilizes an off-the-shelf BERT (without any fine-tuning) to create an information extraction system without any training data. It first creates ungrounded triples (a.k.a. OpenIE) from raw text by looking into the attention weights between words and finding a sequence of words through a beam search that has high attention weights between every consecutive words. When such sequence is created, the first and the last word become the head and the tail entities and the words between them becomes the relation. The next step is grounding each triplet to a known Knowledge Graph by utilizing off-the-shelf entity and relation grounding mechanisms. The proposed method shows 1-2% F1 score advantage over Stanford OpenIE on TAC KBP and Wikidata.\n\nStrengths:\n- The proposed method is unsupervised (doesn't require any training process or data).\n- The proposed method only requires a single application of BERT per paragraph (instead of, for instance, applying BET for every possible triplet), which is relatively efficient.\n- The idea of looking into the pooled attention weights for linking entities and relations is novel.\n\n\nWeaknesses:\n- The head and the tail entities are always single words, whereas many entities such as names have two or more words.\n- The comparison is made only against Stanford OpenIE, which was proposed 5 years ago.\n- The paper highly depends on the grounding techniques from many years go, namely Spitkovsky & Chang (2012) and Stanford OpenIE (2015). Given that the proposed model and Stanford OpenIE have similar performance, I think it is necessary to perform ablation study and show how much is the dependency. In the worst case, I am worried that most of the work is being done in this grounding stage (i.e. ungrounded triplets are not so good)\n- If all possible head-tail pairs are enumerated in a sentence, my expectation is that there will be a lot of garbage triplets. It is amazing yet hard to believe that such simple thresholding techniques make it work and get rid of all of the bad triplets. More analysis will be helpful.\n\nOverall: The approach is interesting in that it is completely unsupervised, and is able to achieve a meaningful performance (compared to Stanford OpenIE). However, I am not 100% convinced how this could work so more analysis would be helpful, and given that Stanford OpenIE is pre-BERT from 5 years ago, the gain seems to be not so exciting.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting question but the methodology needs improvement.",
            "review": "This paper targets an ambitious question -- constructing a knowledge base from scratch using pre-trained language models. The proposed approach deals two problems: (a) constructing facts from raw corpora and (b) disambiguating the entities in the fact by linking them to WikiData entities. \n\nFirst, the insight that knowledge base is embedded in the world knowledge, which is captured by the pretrained embeddings, is not new. [1] studies the joint embedding of knowledge base and text and thus performs knowledge base construction in a semi-supervised fashion, which seems to be more promising than reconstructing the knowledge base from scratch in an unsupervised fashion. I'm curious to see the performance of the power of existing knowledge bases, the signals there can be used to initiate the training.\n\nSecond, baseline comparisons seem to be missing in the experiment section. Even though the task sounds difficult, the numerical performance on recall and F1 are not satisfactory. It's hard to understand the current performance without a reasonable baseline. Apart from that, it would be nice to see the error analysis -- what's the common pattern where MaMa fails? \n\nIn addition to the above major comments, I have the following questions:\n(a) In the map phase where MaMa converts a sentence into facts, do we have an underlying assumption that the input sentence is a fact passage. What will happen if not? \n(b) What if the input passage contains more than one fact? E.g., \"Bob Dylan (born Robert Allen Zimmerman; May 24, 1941) is an American singer-songwriter, author, and visual artist.\" (the first sentence from Wikipedia) contains facts: (bob dylan, is, song writer), (bob dylan, is author), (bob dylan, is visual artist), (bob dylan, also named, Robert Allen Zimmerman), (bob dylan, date of birth, May 24, 1941).\n\n[1] Wang, Zhen, et al. \"Knowledge graph and text jointly embedding.\" Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach but unconvincing evaluation and a lack of details",
            "review": "This paper presents an unsupervised approach for extracting OpenIE style triples from a corpus. The approach leverages the internal attention maps of pretrained transformers to identify paths which correspond to relations between a head entity and a tail entity. The extracted open triples are then mapped, wherever possible, to an existing KG to create what is referred to as an Open KG.\n\nStrengths:\n- The unsupervised matching approach for extracting triples is quite novel and, to my knowledge, the first evidence that the attention maps of pretrained transformers contain paths which capture relational knowledge.\n\nWeaknesses:\n- The main evaluation in the paper is done on the grounded facts after the mapping stage. While the details are hazy, based on Section 2.2.1, it seems the mapping stage relies on a pre-existing KB aligned with a corpus to do the entity linking and relation linking steps. In this case another strong baseline would be to learn distantly supervised entity and relation extraction methods. Of course, this approach would not be able to produce the ungrounded facts, but when evaluating only the grounded facts it would be nice to see how the proposed approach compares to this more traditional setting.\n\n- Related to the above, important details are missing about the interaction between the KG used for the entity and relation linkers and the KG used for evaluation. Specifically, what is the size of the KG available for entity / relation linkers, and were there any overlapping facts between this and the facts used for evaluation?\n\n- Several recent papers have looked at probing LMs for knowledge facts (e.g. \"Language Models as Knowledge Bases\" Petroni et al, EMNLP 2019, and follow up papers). These are very relevant, as another approach for constructing KGs from pretrained LMs is to use natural language templates. But there is no discussion of these works in the paper.\n\n- Selecting the threshold for the matching degree seems to be an important step influencing the quality of the final KG, but it seems to have been done in an ad-hoc manner here.\n\nOther comments:\n- Section 2.1 states that \"self-attention weight matrix ... is the main container of the knowledge information in pre-trained LMs\". This is clearly not true as a large amount of knowledge is also stored in the word embedding table.\n\n- Section 3.2 has some confusing terminology where the facts supported by the KG schema are also referred to as \"ungrounded\" which contradicts the definitions in section 2.2.2.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Conceptually interesting paper but with some limitations and missing related work that may need to be addressed",
            "review": "Update:  Thanks for the detailed response.  I appreciate the additional figures and other results that you have provided.  However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.  It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.  Additionally, there's some limitations to the way the language model is being leveraged and the types of knowledge it can extract.   \n\nI also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews.\n\n----------------------------------------------\nOriginal:\nThis paper is aimed at using pre-trained language models to create open-ended knowledge graphs.  They introduce an unsupervised approach (MAMA) to construct a knowledge graph in two phases in which they take a target corpus and output a knowledge graph.  In the first phase (match), they extract knowledge tuples from each sentence in a corpus using a beam search over the self-attention within a pre-trained language model. In the second phase (match), they ground facts to a knowledge graph schema by using combinations of entity linking and relation matching techniques from previous work. \n\nWhile this is a promising direction for future research, I tend to lean towards rejection for a few reasons.  The main limitations seem to be: (1) the proposed method is a bit limited in that it can only be used with a corpus in which the target head, relation, and tail spans need to be directly mentioned in a single sentence (2) it’s not clear whether the quantitative improvements are due to factual knowledge in the pretrained model or the syntactic/semantic relationships encoded in the self-attention.  There's also some missing related work in extracting knowledge from pretrained models that should probably be discussed.\n\nContributions:\n- A new algorithm for unsupervised knowledge graph creation from a target corpus\n- Demonstrating the utility of large pre-trained language models towards knowledge graph creation (though, there are other works in this area that should probably be discussed more.  I will list a few in the limitations section.)\n- Improvement over previous state-of-the-art models.\n\nStrengths:\n- The algorithms presented here are relatively straightforward but surprisingly effective.  They improve computationally over previous work.\n- Most of the paper is clearly written.  The experiments seem easily reproduceable.\n- The algorithm presented here is able to be used in an unsupervised way and can work with both open-ended and more structured knowledge graph schema.  \n\nLimitations:\n- There are some limitations to the type of knowledge that can be extracted with the proposed approach: the only tuples that can be selected must be described within the target corpus in which the head and tail must appear within the same sentence.\n- There is some missing prior work in creating knowledge graphs from pre-trained language models. To name a few:\n  -“Language Models as Knowledge Bases?” EMNLP 2019\n  -“Commonsense Knowledge Mining from Pretrained Models.” EMNLP 2019\n  -“Comet: Commonsense Transformers for Automatic Knowledge Graph Construction.” ACL 2019\n- Although the proposed model achieves quantitative improvements over Angeli et al. (2015), it seems unclear whether these improvements are due to the factual knowledge encoded in the pre-trained LM, as claimed.  It seems possible that these improvements could instead be due to the high quality semantic/syntactic relations encoded in the attention mechanism.\n- The title of the paper is a bit strongly worded and may be over-claiming what is shown quantitatively in this paper.\n\nQuestions for the authors:\n- How the parameter study was conducted? Was this using the development set, and if so for which dataset?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}