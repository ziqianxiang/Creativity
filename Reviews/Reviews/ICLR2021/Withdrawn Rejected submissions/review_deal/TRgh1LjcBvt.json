{
    "Decision": "",
    "Reviews": [
        {
            "title": "Happy to see real data, but concerned about validation",
            "review": "### Summary \nThe paper tackles denoising transmission electron microscopy data using CNNs. It characterizes the noise model associated with TEM data associated with platinum nano-particles. It then uses this model to simulate noisy data of the same class (highly structured nano particles). It trains a handful of CNNs architectures on this simulated data. U-nets are shown to work much better than conventional algorithms as well as other CNNs architectures at denoising this highly structured data. Following (Mohan et al. 2020), gradients were used to visualize the uncertainty associated with the reconstructions.\n\n### Strengths\nDemonstrates that training on simulated data (under a variety of different microscope parameters) is an effective approach for denoising TEM data with a CNN.\n\nDemonstrates large improvement over existing approaches with simulated data.\n\nValidated on real data*.\n\nThe paper investigates a problem where the SNRs are lower (~3dB) than are typical in existing works.\n\nThe authors promised to release a public dataset\n\n### Weaknesses \n\nNovelty: The benefits of a large FoV, and U-nets in particular, have been known in the context of denoising for several years now. See for instance [A].\n\nGeneralization of results: The method was trained and tested on very specific and very structured class of data; nanoparticles surrounded by a vacuum. The proposed pipeline (training on simulated data) may not generalize to less structured denoising problems.\n\n*The validation on real data is quite limited. See comments.\n\n[A] Liu, P., Zhang, H., Zhang, K., Lin, L., & Zuo, W. (2018). Multi-level wavelet-CNN for image restoration. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops (pp. 773-782).\n\n### Comments \nThis was an applications paper about applying CNN based denoisers to a particular class of real-world data. While the methods were relatively straightforward, the application seemed new and the simulation results were relatively comprehensive. The public dataset could be of independent value.\n\nMy main criticism with the paper is that the experimental results make limited attempts to capture and compare against a ground truth image. In figure 5 the authors show an image formed by capturing, registering, and averaging 40 images. What happens if one applies the various denoisers to one of those 40 frames? Does the result look like the average? Why can't one capture, register, and average a thousand frames to form a ground truth with which to validate the methods?\n\nIf the authors can satisfactorily address this concern, I would be happy to adjust my review.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "the proposed method seems to be overfitted to TEM imaging, some of the contributions are not really new",
            "review": "This manuscript proposed a simulation-based denoising framework specifically for TEM. It also explores non-local signal structures with large FOV using a modified U-Net and finally proposes to use likelihood maps as a by-product for the proposed denoising method. However, in my opinion, the proposed method seems to be overfitted to this specific application of TEM imaging, some of the contributions are not really new, so that it is not easy to see this manuscript in its current form can be a meaningful contribution to ICLR community (however, it could be for researchers in TEM imaging, though).\n\n1) This manuscript argued that \"However, their potential has barely been explored in the context of scientific imaging.\" in the abstract. I am not sure if I can agree with this. As cited in this manuscript, [Zhang2019] investigated fluorescence microscopy using deep learning. The following two works that were not cited in this manuscript also investigated cryoEM imaing using deep learning. Thus, in many scientific imaging domains, there are a number of works going on in my view.\n[Bepler2020] Topaz-Denoise: general deep denoising models for cryoEM and cryoET, Nature Communications 2020.\n\n2) The proposed method is strongly based on the case that one can simulate scientific phenomenon with underlying structures, noise patterns and so on. This does not have to be a weakness for this specific application, but I am not sure if this work will be beneficial to ICLR community since it is too application-specific. The problem in this manuscript can be seen as the problem of domain adaptation for real image denoising that has been explored in some works such as\n[Agresti2019] Agresti et al., Unsupervised Domain Adaptation for ToF Data Denoising with Adversarial Learning, CVPR 2019.\n[Kim2020] Kim et al., Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive Instance Normalization, CVPR 2020.\nThe work in this manuscript has precise simulations to generate synthetic dataset for training and it worked so well unlike more general cases in [Agresti2019], [Kim2020] that has the gap between synthetic and real datasets. Thus, I am not sure if the proposed method in this manuscript can be extended to other scientific fields other than TEM imaging problem in this manuscript since there should be a lot more cases that can not be simulated accurately and therefore there are large gaps between real and synthetic domains. All experiments in this manuscript seem to use regular patterns with ball shapes, so I suspect that the generalization gap for this specific TEM imaging problem might be quite narrow. Thus, if the authors would like to demonstrate the true potential of the proposed method, more complicated structures should be used for experiments. Unfortunately, it does not look easy to do so since it is related to obtain new real data. Thus, the topic of this manuscript could have very narrow scope for machine learning conferences such as ICLR, but could be more appropriate for TEM related conferences.\n\n3) There have been many recent works on unsupervised denoising methods such as Noise2Void, Noise2Self that this manuscript cited, but there was no experimental comparisons with them. It could make this manuscript stronger if the proposed method was compared with recent state-of-the-art methods. Most compared methods in Table 1 are developed in between 1990 and 2012. Note that Noise2Void includes some scientific imaging experiments such as simulated microscopy imaging, cryo-TEM, and so on. Another recent work to consider comparing is\n[Quan2020] Quan et al., Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image, CVPR 2020\nThis method does use only one noisy image to train and to yield a denoised image. This manuscript should include some of these state-of-the-art methods to compare.\n\n4) This manuscript used DnCNN (Zhang2017) as a state-of-the-art denoising method. However, since then there are so many works proposed and some of them already explored to use U-Net as their denoisers with large FOV. For example, Noise2Void already used U-Net for their studies and other recent denoising network was also proposed to yield state-of-the-art performance such as\n[Brooks2019] Brooks et al., Unprocessing Images for Learned Raw Denoising, CVPR 2019.\nThus, it is hard to see \"EXPLOITING NON-LOCAL SIGNAL STRUCTURE\" section in this manuscript as a new contribution.\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A  better method for denoising TEM images of oxide crystals supporting metal nanoparticles",
            "review": "###################################################################\n\n######### Summary\n\n###################################################################\n\nThe authors present a method for denoising TEM images of platinum nanoparticles supported on larger cerium oxide nanoparticles. It is clear that the authors are experts in the capture of TEM images. Their method consists of using simulation software to generate images of these particle configurations to serve as ground truth for learning a model of added poisson noise using a modified UNet architecture. This model not only works well on this simulated data, but also with real data consisting of the same nanoparticles. They provide a method of detecting spurious 'phantom' particles by using poisson likelihood and leveraging features of the denoised image. Furthermore, they provide a dataset of the type of simulated images used in this work to be used as a testing benchmark. \n\n\n###################################################################\n\n######### Contributions\n\n###################################################################\nThe authors list their main contributions as follows:\n\n1)  a) general framework to perform deep-learning based denoising on scientific-imaging data.\n\n     b) The framework is applicable beyond electron microscopy.\n\n     c) Proposed method outperforms existing methods on benchmark TEM data.\n\n2) Proposes a method to visualize the output of an image in the form of 'likelihood maps'\n\n3)   a) Provide thorough analysis of the generalization capability of the method, showing that trained methods are robust.\n\n       b) Show domain where networks optimized for natural photographic images seem to have shortcomings.\n\n       c)  gradient based analysis shows that substantially increasing the field-of-view of the networks allows them to leverage non-local and periodic patterns found in the domain-specific data.\n\n4) Presents a dataset of ~18,000 simulated TEM images for the first publicly available denoising benchmark of TEM data.\n\n\n\n##################################################################\n\n######### Reason for Score\n\n#################################################################\n\nMy recommendation is to reject this paper in its current form. Substantial revisions to the claims of the paper, as well as better placement of the work in the context of related work are needed. Regarding specific problems, see the Cons section below.\n\n\n\n##################################################################\n\n######### Pros\n\n##################################################################\n\n\n1) Authors do a very thorough job describing their specific control experiment as well as the structure of the data that is available in the associated simulated benchmark dataset. \n\n2) The paper is well written and includes many figures that help to understand the type of data that is being processed.\n\n3) Using the simulated data in the context of denoising is an interesting approach to solve the problem of not having clear ground truth data.\n\n\n\n####################################################################\n\n######### Cons\n\n####################################################################\n\nclaim 1 a,b) \nThere are no studies done outside of TEM imaging of platinum nanoparticles on cerium oxide nanoparticles. Any claims that this method will be useful on other types of scientific imaging datasets are dubious at best. There are no experiments to conclude this method will function well outside of this very specific dataset. \n\nclaim 2)\nIt is not clear to me that this likelihood method will be useful outside of this specific dataset. How well will it work when the data doesn't have such overt structure?\n\nclaim 3 b,c)\nWhile I don't disagree that this use-case shows a data domain where networks optimized for images appear to be at a disadvantage, I don't find the implication that substantially increasing the field-of-view of networks in other less heavily-structured domains to be supported.\n\nOther issues in rough order of appearance:\n\n1) Using simulation in scientific imaging data to train a denoising network has been done previously in Gong et al 2018[10.1109/TRPMS.2018.2877644] for PET imaging, though the authors only mention this work in the context of denoising in the medical domain.\n\n2) There was no reference to the use of related work in using synthetic images to train a deep neural network. ( e.g. arXiv:1804.06516, arXiv:1809.10790, arXiv:1807.10931, arXiv:1612.07429, and many others...)\n\n3) In figure 4c, it is not clear why the color of the 'spurious' atom defines it as spurious when there are other atoms in the likelihood map with similar, if not the same color, and are not spurious. How is the cutoff made for the value?\n\n4) In the conclusion it is claimed that this \"is a proof of concept that CNNs trained on simulated data can be remarkably effective in improving results on real data.\" This is too broad of a claim for this paper as well as incorrect as this has already been shown in many other works, see con 2 above.\n\n5) The description of the CNN model used is quite lacking in details. I would have expected a description more akin to that of Appendix C.3. As it is, I would have difficulty building the UNet structure described in Appendix C.1. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Deep Denoising for Scientific Discovery: A Case Study in Electron Microscopy",
            "review": "This paper focuses on image denoise task, and propose a simulation-based denoising\n(SBD) framework for images obtained from transmission electron microscopy (TEM). Also, this paper released the first publicly available benchmark dataset of TEM images with 18,000 examples. Though,  from the experiment result both on a simulated benchmark dataset and real dataset, SBD shows the higher performance compared with the existing techniques. However, this paper lacks methodological innovation.\n\n\nStrengths:\n1. For denoise tasks on scientific applications, noiseless ground-truth images are usually not available. So, this paper presents a likelihood map to visualize the agreement between the structure of the denoised image and the observed data\n2. This paper has a contribution on the public dataset. The authors released the first publicly available benchmark dataset of TEM images with 18,000 examples.\n\n\nWeaknesses:\n\n1. The methodological innovation is not sufficient. In this article, the authors mentioned that “the crucial difference between SBD and previous methodology for deep denoising is that the procedure for generating the training set needs to be explicitly designed”. However, I think the idea of generating simulated images with imaging parameters rather than adding Gaussian noise is a trivial idea for the denoise task of transmission electron microscopy images. \n2. In the experiment with simulation data, this article lacks the performance analysis for images that have different noise levels.\n3. Missing the link of the released dataset. \n4. The text size of the left top corner of the first sub-image in Figure 2 is small.\n",
            "rating": "2: Strong rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}