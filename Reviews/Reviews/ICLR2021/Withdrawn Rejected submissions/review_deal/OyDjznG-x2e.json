{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers positively valued the proposed idea of performing permutation selection in permutation decoding via combining node embedding and self-attention, which seems to be of high originality. I found that this paper is mostly clearly written, except Section 3.2 as AnonReviewer5 commented. The main concern among the reviewers is regarding applicability of the proposal beyond the BCH codes. \n\nPros:\n- The proposal of utilizing self-attention for permutation selection in permutation decoding is novel and interesting.\n- Computational complexity in the decoding phase is only slightly increased compared with random permutation selection, and is far smaller than performing decoding for all permutations. The GPS classifier can be parallelizable to further reduce latency.\n- The proposal should be applicable beyond the BCH codes to those with decoding based on the Tanner graph, including polar codes.\n\nCons:\n- Only the BCH codes were considered, whereas in the authors responses they will add a short analysis on polar codes.\n- It seems that systematic enumeration of the PG is required, which would limit applicability of the proposal.\n- There is a room for improvement in presentation:\n-- In Section 3.2, the description of \"positional encodings\" was unclear to me, in that the ordering of the codeword entries is arbitrary, unlike typical sequence transduction problems to which attention mechanism is being applied.\n-- Dependence of the input vector sequences of the attention head on the permutation $\\pi$ is not clearly explained.\n-- Performance of the proposed method might depends on choices of the parity-check matrix, which is however not discussed in this paper at all.\n\nBased on the above concerns, the paper is not yet ready for publication in its current form.\n\nMinor points:\nIn page 3, line 14, \"that\" should be deleted.\nIn references list, \"hdpc\" should be in capital. \"reed-muller\" should be \"Reed-Muller\"."
    },
    "Reviews": [
        {
            "title": "Novel application of self-attention mechanism to improve permutation decoding",
            "review": "The paper focuses on improving the computational complexity of permutation decoding. In permutation decoding, one aims to decode a permutation of the received codeword in the hope that it will lead to successful decoding as compared to applying the decoding algorithm on the received codeword. In practice, one performs decoding with multiple permutations to identify the ones which lead to successful decoding. This paper proposes a classifier that given a received codeword and permutation pair predicts whether the permutation will lead to successful decoding. This alleviates the need to perform the entire decoding with permutations to identify the successful permutations. The classifier itself relies on an embedding model for the permutations which utilizes a self-attention mechanism to embed permutations.\n\nOverall, I think that the paper makes good contributions to the growing field of ML for communications. The utilization of self-attention to this area is novel (to the best of my knowledge). My major concern is about the comprehensiveness of the paper (see below) and the quality of the presentation. The latter should be addressable with minor revision. \n \n\nPros: \n\n1.  The paper utilizes the self-attention mechanism to improves the computational complexity of permutation decoding. To the best of my knowledge, this is the first application of self-attention in ML for communications.\n\n2. The paper empirically demonstrates the utility of their proposed method on BCH codes.\n\n3. The paper performs an ablation study to showcase the importance of different components of their solutions.  \n \nCons/Comments: \n\n1. The paper does not present any broader ML techniques that might be useful beyond the immediate scope of the paper. If the authors feel otherwise, please include a discussion on this.\n\n2. Even though the proposed solution can be potentially employed to decode various codes, the treatment in the paper is restricted to the BCH code. The authors should make this point clear in the abstract/introduction.  Otherwise, please include experimental results with other codes or add a discussion on this.\n\n3. There is some room for improvement in the presentation of the paper. E.g., \n\na. Make the scope of the contribution precise (see the comment above)\n\nb. The discussion in Section 3.2 can be made more clear. How is the same node embedding v being used for all $i$ in $w_i = u_i + v$?\n  \nc. $d$ is being used to represent the embedding dimension as well as an indicator variable in the definition of the loss (Section 3.4)?\n\nd. The computational complexity discussion at the end of Section 3.4 can be improved.\n\nQuestion:\n\nDo authors restrict themselves to a single layer of self-attention mechanism? Have the authors explored the utility of multi-layer self-attention mechanisms, as usually practiced in the ML literature? \n\n######## Post rebuttal ##############\n\nThank you for your response. After going through other reviewers' comments and the authors' responses to those, I am comfortable with my original score. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea, but concerns about the justification of usefulness ",
            "review": "Summary: \nThis paper suggests a new decoding algorithm of linear error corrections codes based on self-attention. \n\nReasons for score: \nThe main idea of using self-attention for decoding linear error correction codes is interesting.\nMy main concern is about the justification of the usefulness of the proposed scheme.\n\nConcerns: \n- My main concern is that there is an insufficient amount of reasoning to explain why we use the proposed method over the existing decoding schemes. It would be better to compare the error performance of the proposed method over the other state-of-the-art decoding methods. For example, Fig. 3 (a) contains the BP lower bound and ML bound. I like this point, but I think the authors need to show the performance of the other existing schemes (other than the \"baseline decoders\" of the current manuscript) altogether to easily show the usefulness of the proposed method. Also, it would be better to clearly explain the complexity/cost of using the proposed scheme.\n\n- In the abstract, the authors state that \"the lack of theoretical insights currently impedes the exploitation of the full potential of these algorithms\". For me, this sentence seems important as it might be directly followed by the statements that explain the usefulness of the proposed algorithm. However, it is hard to find for me whether the theoretical foundation of this decoding algorithm is more robust than the other decoding schemes. \n\n- For me, it is not clear why the top k (k>1) performance matters to measure the quality of the decoding algorithm of error correction codes for communication. We usually consider the top 1 performance only for this kind of application which generally requires a very low level of error rates. Instead of plotting the top k (>1) performance, it would be better to include the comparison of performance over the best existing decoding schemes (in terms of top 1 performance given a reasonable complexity).\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Graph Permutation Selection for ECC Decoding - Review",
            "review": "The focus of the paper is error correction codes. Some structured codes have a permutation group (PG) that maps codewords via permutations. The PG structure is useful for some soft decoding algorithms (e.g., belief propagation, message passing) because of the observation that while the decoding of a codeword may fail, it often happens that the decoding of a permutation of the same codeword succeeds. The paper is concerned with obtaining the best permutation to facilitate such soft decoding, through the means of deep learning. \n\nIn order to select the most promising permutation without running the soft decoding algorithms, two main ingredients are used: (1) node embeddings of the Tanner graph of the code via the node2vec method, (2) a self attention mechanism that preserves the similarity between two permutations through the node2vec transformation, resulting in close geometric representations. The node2vec embedding is pretrained on the Tanner graph of the code. A permutation classifier is trained jointly at the same time, outputting the probability that a given codeword and permutation will decode correctly.\n\nI find the construction proposed in the paper quite interesting. However, except for some intuitive reasons it is not clear that the approach would have practical merit. The experimental evaluation should provide a convincing argument. \n\nI am leaning towards rejecting the paper, because I find the experimental evaluation quite limited. Unfortunately, it seems that the experiments were run on a very limited set of codes. Only BCH codes were used for evaluation. The size of the codes is really small (n = 31, 63, 127). The rate of the codes is very low, for three of them ½, for one ¾. The size of the permutation group given in the paper is n*log2(n+1) (pg. 2). Considering Table 1 we see that 100,000 mini-batches of size 5000 each have been used for training; is the network learning only because the space is relatively small and the number of training examples very large? \n\nIn the conclusion we read “our method is independent of the code length and therefore is considered scalable”. It would be great to see evidence that the method scales to large code lengths (thousands of bits). Would the learning algorithm still produce significant results? Moreover, how does the method perform on codes with high rate? \n\nMinor comments:\n- It seems that WBP is not defined (is it weighted BP?)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper discusses new applications of self-attention mechanisms to permutation decoding problems and shows the approach's goodness with four different BCH codes on AWGN channels.",
            "review": "[Main Strengths] \n\nThis paper's main strength is that the background section is well-drafted, the problem statement is clear, and each algorithm block's motive is adequately referenced.\n\n================================================================= \n\n[Main Weaknesses] \n\nThe paper's main weakness is that it seems the motivations for choosing four different BCH codes (in Section 5) are not justified clearly. It will make the authors claim stronger if they explain why these represent linear codes cases (unless the permutation decoding only applicable to the BCH codes). Additionally, the SNR gains from baselines (rand+BP) are shown large for the BCH codes (e.g., Figure 2). However, the principal question is that one will observe similarly large gains on the other capacity-achieving codes (e.g., LDPC codes), in particular, when the baseline performs better.\n\n================================================================= \n\n[Technical Comments] \n\n1) Can this approach be extendable to other types of channels (e.g., channels with memory)? \n2) Does the algorithm require LLR to represented as simple as the AWGN channel (in Section 3.3.)?\n\n================================================================= \n\n[Typographical comments]\n1) For better readability, Section 4 can be moved to later (e.g., after Section 5).\n2) page 2: \"a self-attention model (introduced in Section 2)\" should be \"a self-attention model (described in Section 2)\".",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Methodology for optimal selection of permutations for permutation decoding",
            "review": "##### Summary\nThe use of permutation decoding improves the performance of some types of structured channel codes. Still, the question remains on which permutations (out of the vast set of possible ones that most of these codes allow) select to decode, in a way that we increase the chances of a successful recovery of the transmitted codeword. Obviously, we could just pass all the possible permutations through the decoder, but this is clearly extremely inefficient and hinders the usage of these methods on real-world scenarios. In the current manuscript, the authors propose a method to select the best possible permutation for every received corrupted codeword $\\textbf{y}$. This allows increasing the performance in terms of error rate, and preserve the efficiency of the decoding process, up to the overhead introduced by the permutation selection procedure. The methodology is based on a permutation classification procedure (trained beforehand), where each permutation is encoded into an embedding vector obtained using the concept of self-attention, to account for the geometric similarity between transformations. The pipeline presented allows improving previous permutation decoding methods by several dBs at almost no extra cost. Hence, it renders an interesting alternative to improve further the decoding of powerful and widely used coding schemes such as Polar codes, that also accept permutation decoding.\n\n##### Strong and weak points\nHonestly, the use of self-attention for this specific task sounds quite convoluted. Still, there is no doubt it actually works, and the performance is significantly outperformed. It is unclear how this attention looks between the permutation vectors, but in the end what matters is the embedding obtained, and how it captures the similarity between them, and also, I believe, some relations with the syndrome. Still, the most important advantage is that it actually should not decrease significantly the efficiency, as the forward pass required during test could be run in parallel for all permutations, as the authors suggest. This is extremely important, and it is one of the most positive aspects of the present methodology. Besides, the paper is nicely and clearly written, with all the important details explained and all the required background information, which allows perfectly understanding the methodology.\n\nStill, there are some important aspects, concerning the results, that may require, from my point of view, some further comparison/study (more details in the section ''Questions and additional evidence''):\n* First, I do consider the comparison of GPS + (W)BP vs rand quite unfair. Is there not any other, more advanced, methods for selecting at least a subset of permutations? IN BPL, although it is applied to Polar codes, the authors finally make use of only 5 different permutations, which is not that ineffective, and enable them to report a great performance. \n* The previous concern also takes me to Fig. 3a, where we can observe how there is still a lot of room for improvement, as taking the 5 top permutations still yields some extra considerable improvement. \n* When performing the ablation studies, it is not clear to which of the codes those degradations correspond. I mention this because the performance degradation, for all ablations, looks quite large and pernicious, but it is difficult to assess their impact without a proper comparison. Hence, it would be interesting to see a plot showing the actual SNR values at a BER of $10^{-3}$ for some of the most harming ones, such as the exclusion of $\\mathbf{s}’$. I consider depicting such comparison quite important because, in some cases, it seems that the degradation may take the model to worse performance than rand-WBP.\n\nFrom my point of view, the aforementioned problems hinder the comparison of results, therefore impeding a full assessment of the scheme used, the decisions taken, and its performance compared to other similar approaches. And this is for me the weakest point of the present manuscript. \n\n##### Decision, and key reasons\nAccept, after discussing and further elaborating some of the previous concerns. Despite the previously described issues, I believe the paper presents an interesting method to advance the current state of permutation decoding. Thanks to the possibility of freely generating training samples on these schemes, it is possible to achieve very satisfactory training for permutation embedding and classification. Once that is achieved, this model can be applied at almost no extra overhead, which indeed renders the current pipeline much more useable on real-world scenarios. Additionally, the paper is nicely written and justified, easing the understanding of the methodology and the need for such an approach.\n\n\n##### Questions and additional evidence\nNevertheless, I believe the authors should elaborate more on the following concerns:\n* Would it be possible to propose a more fair comparison? If not, perhaps please comment on the selection of permutations done in approaches like BPL, and how that selection criterion won’t be applicable here. This may help to alleviate the feeling that comparing against rand seems a bit unfair.\n* The results of Fig. 3a are quite revealing, and the room for improvement is still pretty large. Could you discuss a little bit more about the source of this error in the selection of the permutation? \n* Also, and as part of some supplementary material, I would appreciate a more visual comparison of the different SNR values when performing the ablation studies. And also, discard subsets of them at the same time, etc. This will help to understand better the most key components of the pipeline, as currently it just seems everything is extremely essential, and changing any single aspect will degrade significantly the performance.\n* I would like to understand why you take the absolute value of the LLR. I understand that this is the only way to proceed when you train with all $\\mathbf{0}$ codewords, but is it not harming your training?\n* Finally, I am curious about the Block error rate, as I can imagine that sometimes, when a permutation is quite wrongly chosen, it might push $\\mathbf{y}$ to a different $\\mathbf{c}$, hence leading to $\\mathbf{s}=\\mathbf{0}$, but still resulting in a wrong decoding. Could you provide some more insights about this?\n\n##### Extra feedback\nI just want to conclude with a suggestion for a correction and a typo:\n* In Section 2, second paragraph, the sentence ''Codes with good decoding performance are represented by graphs with cycles’’ is a bit confusing, as it could be understood that indeed graphs are helping with the performance. Although you clearly explain this in the following sentence, I would rephrase that sentence.\n* In page 4, when indicating the dimensions for the matrices to learn, I believe the dimensions are wrong, as they should be $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\in\\text{R}^{d_p\\times{d_w}}$.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}