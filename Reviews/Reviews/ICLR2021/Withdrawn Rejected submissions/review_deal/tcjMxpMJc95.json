{
    "Decision": "",
    "Reviews": [
        {
            "title": "Great Questions, Interesting Experiments, Confusing Execution",
            "review": "# Overall\n\nThis paper asks an important question: why does knowledge distillation work? It also conducts a number of interesting-looking experiments and many of the results seem intriguing or surprising at a high level:\n* Relying solely on the KD loss (rather than the cross-entropy loss) and increasing the temperature lead to better performance on CIFAR-100.\n* The KD loss can be replaced by an MSE loss while retaining or potentially improving performance.\n* There are occasions where distilling with a smaller teacher leads to a better student.\n* There are occasions where distilling using less data leads to a better student.\n\nHowever, this paper suffers from a number of weaknesses at the structural, presentation, and technical levels that make me concerned:\n\n## Structural Concerns\n\nThis is really two papers. Sections 2 and 3 focus on the role of the logits and the possibility of replacing the MSE loss with the KD loss. Section 4 focuses on the role of dataset size and teacher size. These two stories essentially have nothing to do with each other except that they relate to distillation. Section 4 doesn't even use the logit MSE loss that is touted in Sections 2 and 3. While both sets of experiments are interesting, I don't see how this paper tells a coherent story or what the takeaways should be. When looked at separately, I don't think either of these stories is fleshed out rigorously enough that they can stand on their own (see the Technical Concerns section below). I don't think putting two unrelated, imperfect contributions together makes for a strong paper. I think this could have been written as two separate papers and, with the extra space available to each story, could have been two much stronger contributions.\n\n## Presentation Concerns\n\nIn general, the paper is really difficult to read. I have summarized my concerns here, but please see my full notes below for places where I was particularly confused.\n* Research questions are too broad and vague to be sure that the experiments have answered them. In many places, no specific research question is articulated at all.\n* Experiments are loosely sketched in prose, but there is not enough detail for me to be sure what experiment was conducted (let alone to reproduce it). \n* The paper does not take the time to explain the possible results of experiments or how one would interpret them. It also asserts the interpretations of results without explaining the underlying logic. This makes it very hard to follow what is happening, believe the authors on those assertions, or to interpret results in the context of the counter-factual of what other outcomes might have meant. Including this information is *really* important: it makes it much easier for a reader to keep up with the story of the paper and believe the assertions.\n* Everything (claims, experiments, results, interpretations) are stated too briefly without explanation. I really struggled to keep up and had to do a lot of work to justify claims to myself. The prose should do that work so that the reader doesn't have to.\n* Related work is mentioned but is not explained in enough detail that I could understand what was going on without actually reading the related papers. This was really frustrating. It left me confused while I read the paper and forced me to read several other papers to make sense of this one (which is quite disrespectful of a reader or reviewer's time).\n* Several terms are used inconsistently or undefined. For example, MSE vs. L2 regression loss, KD vs. FKD, \"dark knowledge\" in the related work (what is dark knowledge?).\n\nI don't know whether these problems are a symptom of bad writing or of the fact that the authors were cramming so much content into eight pages, but - regardless - everything needs to be explained such that a reader can make sense of it on the first pass. The paper was written in a manner that the authors (who are intimately familiar with their own work) can probably understand clearly, but it was difficult for me (a reader new to this work) to parse. In many cases, there simply wasn't enough information or help for me to make sense of what was happening.\n\n## Technical Concerns\n\nBeyond my general confusion about the details and interpretations of many claims/experiments in the paper, I also have several concrete technical concerns.\n\n**Is MSE SOTA?** In the introduction, the paper mentions that \"Tian et al. (2019) argue that even original KD can outperform various other KD methods that distill the hidden feature vector.\" Do the authors of this paper agree? If the authors agree and the original KD can outperform other distillation methods, then MSE (which outperforms the original KD according to Table 1) should be a new SOTA method, right?\n\nThe paper appears to study this question half-heartedly: it only provides a comparison to other baselines on CIFAR-100 in Table 1 (rather than ImageNet and translation as in Tables 6 and 7). None of these other baseline methods are cited or described. Is this list comprehensive? What KD methods are currently SOTA on the tasks studied? Are those SOTA methods included? Does MSE match or beat them? If so, what is the takeaway - is MSE SOTA? Why doesn't this comparison include ImageNet or translation - is there something to hide?\n\nAlternatively, if the authors do not argue that MSE is SOTA and do not agree with Tian et al. (2019), then why is the original KD/MSE worth studying at all if other methods work better? Shouldn't the authors focus on studying the kinds of methods that work best?\n\nIt's important that the authors take a clear position on this question. If the answer is yes (or even that MSE is competitive with SOTA methods), that claim needs to be demonstrated in much more detail. If the answer is no, it undermines the relevance of the paper. I don't think it's acceptable for the paper to leave this question unaddressed.\n\n**What is happening in Section 3 in detail?** I was able to follow this section only at a very high level. Figures are missing axis labels and captions are insufficient. In general, the prose moves so quickly that I couldn't follow the details. I am not convinced of any of the claims because I couldn't understand what was happening (see the notes section below). The authors need to take advantage of the additional space to explain this section in more detail so that I can actually evaluate the technical claims.\n\n**What is happening in Section 4?** Section 4 introduces a whole different set of research questions that has nothing to do with the previous sections. The answers to those questions are jumbled together, and I had to do the work of trying to make sense of a giant table covering many different axes. Like with Section 3, I wasn't clear on what the research questions were, which specific experiments addressed them, how to interpret different results to those experiments, and which parts of Tables 6/7 were relevant. This section (and Tables 6 and 7) need to be broken down into smaller sections/figures so that I can understand each of these points one by one. Like the issue of whether MSE is SOTA, the claims made by the authors are exciting and provocative, but the prose is not clear enough that I can evaluate whether these claims are supported by the data. In general, this means that I have to analyze the data for myself in order to figure out whether the claims were supported, and that makes for a very difficult paper to read.\n\n# Score\n\nI have given this paper a 4/10. I argue in favor of rejection. The paper asks exciting questions, and the results/experiments seem interesting. However, the presentation is so problematic that I couldn't perform a detailed technical evaluation on Sections 3 and 4, and the paper tries to make so many disparate points that it doesn't make any of them effectively (both in terms of prose and technical rigor). As such, I do not think this paper is in a state where it can be accepted to ICLR.\n\n# Improving My Score\n\nI am very open to improving my score and potentially championing acceptance of this paper, so the authors are encouraged to address my feedback and stay engaged with me throughout the reviewing process. The following concerns need to be addressed for me to improve my score.\n\n1. The authors need to clarify which specific research questions this paper aims to address. That means which questions each section aims to address, and the full list may include many questions (Section 4, for example, addresses many different questions - I think). These questions need to be specific and answerable by a single, targeted experiment.\n2. The authors need to improve the presentation throughout the paper. This is especially so in Sections 3 and 4 (for technical content as described above and in the notes) and in Sections 1 and 5 (for clarity). I cannot in good faith argue for acceptance if I cannot rigorously evaluate the technical claims in Sections 3 and 4. Right now, the prose is not such that I can do so.\n3. The authors need to address my questions about the goals of Section 2 and Section 4 and convince me that the results presented are rigorous enough to support the claims that are made. Please see the paragraph above about my concerns in Section 2 (about MSE and SOTA).\n4. The authors need a much more detailed related work section that clearly describes what other papers have studied and concluded and makes the case that the work in this paper is indeed novel. The related work is far too cursory for me to be certain of that.\n5. The authors need to explain how Section 4 is related to Sections 2 and 3 beyond that they all relate to scientifically studying distillation. (In general, I really do think this paper should be split into two papers, but I'm open to being convinced otherwise).\n\nIn sum, the authors need to improve the presentation such that I can (a) understand the broader story, (b) understand the claims, (c) verify that the claims are adequately supported (after all, that's my job as a reviewer), and (d) understand where these claims fit into the context of the literature (to evaluate relevance and novelty). If the authors can help me understand that and improve the paper to ensure that's clear to future readers, I am very open to substantially increasing my score.\n\n# Notes on First Read\n\n## Abstract\n\nNot sure I agree with the characterization that teachers are necessarily \"cumbersome\" or students are necessarily \"lightweight.\" In performance-focused contexts, it's certainly a goal to have a student that is less costly than a teacher, but (1) it is the difference in size that matters rather than whether one model is \"cumbersome\" or the other \"lightweight\" in absolute terms and (2) KD is also interesting for theoretical reasons (in which case the student might not necessarily be more performant than the teacher). The paper itself acknowledges the heterogeneity of distillation applications in the second paragraph.\n\n## Introduction\n\nThird paragraph: a pet peeve of mine is when a paper generically says there \"is still a lack of understanding\" in a broad topic rather than explicitly stating the qualities we don't understand. \"Why and when KD should work\" is much too general. The sentence that follows obliquely references the question of whether performing KD on outputs alone outperforms distilling hidden features as well, but it's not explicitly made clear that this is the central question of the paper. Likewise for \"to uncover several mysteries of KD.\" The paper should make a specific case that a specific research question is (a) interesting and (b) unanswered.\n\n\"Shed light upon the behavior of neural networks trained with KD\" - need to be more concrete and specific here. Many readers will only read the abstract and intro, so it's important that it communicate the whole story.\n\n\"The amount of distilled knowledge changes\" - again, what does this mean?\n\nI've now read the entire abstract and the entire intro except for the contributions, and I'm still unclear on the paper's specific motivation, specific research question, main experiment(s), and findings. The goal of an introduction is to communicate these four pieces of information, so it's important to revise with that in mind.\n\nFirst contribution: \"vast experiments\" tells nothing about which experiments are being performed. \"two hyperparameters\" are not made explicit. The second sentence is much more valuable. Either be explicit in the first sentence or cut it completely.\n\nIn Table 1, it appears that the FKD method underwent extensive hyperparameter tuning. I assume that the table contains the best performing hyperparameters that the authors were able to find. Is this the case? How extensive was this tuning? Did the other methods receive similar amounts of tuning? I want to make sure this is a fair comparison and that it is fully reproducible.\n\nTable 1 does not include errors. Were the experiments run multiple times? Are these the means across multiple runs? Are these the maxes across multiple runs? How many teacher models were created? How many student models were trained for each teacher? What are the means and standard deviations of performance?\n\nSecond contribution: How do you demonstrate this? Empirically? A proof? What is the \"original KD\"? Is it formally defined somewhere? What is a temperature? It's been mentioned several times but hasn't been specified explicitly. I have an intuition for what it probably is, but it would be valuable to know for sure. What does it mean to \"show almost the best results?\" Is it competitive? Does it perform only a little bit worse? I'm concerned that, by using this sort of hedged language, this paper may be over-selling.\n\nThird contribution: Might want to take out the \"perhaps surprisingly.\" State the results and let the reader draw the conclusions about surprise. Otherwise a really nice contribution.\n\nFourth contribution: What is the intuition that it provides?\n\nFifth contribution: This really seems like two contributions: dataset size and model size. \n\nOverall, the paper appears to be a collection of experiments about KD rather than a single coherent narrative about any single experiment. This is completely okay  - it seems like each of the experiments is interesting in its own right. But it does explain why the research question is so vague. The authors should try to frame a specific research question for each major experiment and make those explicit early in the paper. Overall, it's great that the authors are studying this problem: KD is widely used in practice but hasn't seen the same level of scientific study as other compression methods. I encourage the authors to simply state the paper for what it is - a set of (what appear to be loosely-connected) experiments about KD - rather than to sell it as something much bigger (\"understanding knowledge distillation\").\n\n### Section 1.1\n\nThe notation is really hard to read. Are f and k sub and superscripts of z, or are they sub and superscripts of e? Does tau correspond to z, f, or k? I would suggest you define softmax separately (with one argument that is the temperature) and then simply call the softmax function on z^f. I think this would be much clearer.\n\nWhy q(x) and not y?\n\nWhy is the KD loss defined in this way? It looks like the KL between the softmax output of the teacher and the student, but I'm not sure where tau^2 fits into it. For those of us who are newer to this literature, it would be helpful to have a little explanation here. I imagine that, if this paper succeeds at its goals, it will serve as the entry point into this literature for many new researchers, so the paper should strive to explain some of these foundations as it goes.\n\nWhy are these the standard choices for alpha and tau? Where do they come from? What makes them standard? Please provide citations, footnotes, or an appendix explaining this.\n\n### Section 1.2\n\nPlease refer to appendix D here so the reader knows where to find the details on the training setup. I appreciate that you included so much detail here.\n\nThe related work section should appear here rather than at the end of the paper. At this point, the paper has mentioned several other papers (Tian et al. 2019 in the introduction and Heo et al., 2019a, Cho & Hariharan, 2019, and Zhou et al., 2019 here). As I try to assess the novelty of the ideas, experiments, and claims in this paper, it's important that I know what questions/experiments these papers do and do not perform and how they relate to your experiments. The paper has only cited the papers without explaining how they relate. Given the breadth of the research questions articulated at the end of Page 1 and the reference to Tian et al., in this context, I'm concerned at this point in my first read that the results in this paper may not be so novel and that this lack of detail may be an attempt to cover that up. To be clear, this isn't an accusation. I'm just trying to give you the first impressions I have as a reader so that you can amend the narrative to ensure future readers have no doubt in their minds.\n\nAt this point in my reading, I skipped ahead to Section 5 and then circled back; I recommend you move that section up and integrate some of that content into Section 1 where appropriate so future readers don't have to keep track of these questions/concerns and read the paper out of order.\n\n## Section 5\n\n\"Dark knowledge\" - not sure this is the right phrase. Why is KD mysterious? Why isn't our understanding obvious? Does it work surprisingly well? What makes that surprising? The paper would benefit from specific claims about the mysteries of KD rather than broad statements like these that are strewn throughout the abstract/intro.\n\nThe second sentence of this section doesn't read clearly. Not sure what the \"wrong answers of a teacher\" are, how that would strengthen KD, and what the \"concept of similarity information between classes\" means. Not sure exactly what the experiment described in the next sentence means either.\n\nWhat is the \"row vector of the fully-connected layer corresponding to the class\"? What is a \"super-class\" in this context? Are we talking about some sort of hierarchical task (like a hierarchical view of ImageNet)? There isn't enough information here to parse what I should take away from this paper.\n\n\"On the other side\" of what?\n\nWhat is \"dark knowledge\"? It comes up repeatedly in this section as if it is a formal term, but this is the only place it appears in the entire paper.\n\nNot sure what it means to \"release the challenge of training a network.\"\n\n**In general, this related work section is inadequate. It mentions many interesting-sounding references, but none of the snippets about the papers are enough to actually determine what those papers found or what that has to do with the experiments in this paper. It doesn't answer my question about how the claims and purported novelty of this paper fit into the context of related work.**\n\nMost importantly, this section doesn't mention Tian et al. (2019), which comes up the introduction as a key source of inspiration. The paper doesn't at any point mention what this paper actually does or shows. I'm now going to have to read several of these papers in detail to assess the novelty of the submitted work, and that makes for a difficult time for future readers (and makes for a grumpy reviewer).\n\n## Section 2\n\n\"vast experiments\" - let the reader decide that. Just say the experiments that you conducted. That you feel the need to use hyperbolic language like this doesn't make me confident that the experiments actually speak for themselves.\n\nNot sure what \"as less as that of the teacher\" - I think you mean \"lower than that of the teacher?\"\n\nFor what it's worth, I despise heatmaps like you use in Figure 1. The colors cover completely different numerical ranges on the left and right, and it's impossible to discern specific numbers from the colors. I strongly recommend that you use a line graph where alpha is on the x-axis and each line is a different temperature.\n\nFigure 1:\n* Is it really that surprising that low values of alpha help training performance and hurt test performance? With alpha=0, the student is training only on the task (without using the teacher). Most models can get to 0% training error, but a smaller model will generalize worse than a larger model (see the mysteries of double descent).\n* However, it's certainly surprising that high values of alpha lead to worse training performance and better test performance. I don't have a good explanation for that, and you've piqued my curiosity to see your explanation (although I wish that explanation had been described a bit more clearly in the intro).\n* I don't have a good intuition for temperature and the role it plays, so I'm eager to see what you find.\n\nWhat is the gradient analysis of logit in Hinton et al. that you're extending? That context is helpful to understand where their work stops and your work continues. \n\nStatement (3) would be more intuitive if you rearrange such that $z_k^s - z_k^t$ is grouped and $z_j^t - z_j^s$ is grouped. The first term means that, if logit k is different in the student model than the teacher model, then the gradient will push the logit to be closer to the teacher model. That supports your claim of logit vector matching and makes it more apparent to the reader. The bias term is then explained by your statement that \"the softmax output is the same even if all elements in the logit increase equally.\"\n\n## Section 3\n\nThe first sentence doesn't actually articulate a question. It's worth stating outright that it is surprising that this phenomenon occurs and that you seek to understand why.\n\nFigure 2 does not have any axis labels.  The captions are not descriptive or self-contained enough for a reader skimming a paper to make sense of the graphs. Please correct these issues.\n\nI'm a little confused by this section: I'm not sure I'm convinced that TLD should tell me much about the functional similarity between the student and teacher or about the student's \"capacity to learn the teacher.\" Nor do I fully understand why matching logits is better than matching labels.\n\nI can't figure out how to read Figure 4 because there are no axis labels and the graph/caption are not detailed enough for me to understand what experiment is being conducted. I'm not sure what I'm supposed to look for here or what \"success\" looks like. I generally understand the experiment in the third paragraph of page 5, but I don't understand how to interpret the results. The paper should explain different possible results and what they would mean; it should then identify which result occurred. I'm pretty confused by Figure 4 and how to make sense of it.\n\nDoes Table 4 use MSE or KD? The caption says one thing and the table says another. Again, I'm not able to follow the prose: I'm not sure exactly what experiment is being conducted or what possible outcomes would mean. The paper needs to walk through that slowly and explicitly so a reader can make sense of what is happening.\n\nAre the same examples in the same quantiles over different runs? Or does it vary from run to run? Are certain examples always more difficult than others? In general, none of the figures seem to have error bars or error numbers, making me concerned that none of these experiments were conducted with multiple replicates.\n\nI'm leaving this section very confused. The experiments seem interesting at a high level and the authors seem convinced that the results provide a compelling explanation for the value of $L_{KD}$ and $MSE$, but I don't actually understand why this is the case. The text is too dense, details are left out, and not enough is explained for me to make sense of what experiments were conducted and what different outcomes of the experiments would imply. With the extra page that the authors have during the rebuttal period, it's important that this section is fleshed out; it seems like the heart of the paper, and it's a big problem that I don't fully understand it.\n\n## Section 4\n\nSo far, the paper has built a narrative about the role of logits and the possibility of using MSE as a replacement for $\\mathcal{L}_KD$. The change to focusing on data size seems abrupt and unrelated to the main story of the paper. Upon encountering this section, my immediate question is: is this really part of the same story that the paper is telling?\n\nHow does class-imbalanced sampling work? If an entire class is excluded in its entirety, then presumably the model won't be able to positively identify members of that class? You haven't provided enough detail about Hinton et al's experiment for me to determine what is taking place here. Now I need to go read Hinton's paper to understand the experiment you're conducting.\n\nThe contributions claim that MSE outperforms KD, but that experiment is never conducted explicitly on all datasets. Since that's the big takeaway of Sections 2 and 3, shouldn't that result be illustrated and emphasized somewhere? It's just shown in Table 1 for CIFAR-100. It doesn't actually appear anywhere in Table 6 or 7, since MSE isn't included...\n\nThe results in this section go by way too fast for me to understand what I'm supposed to take away. What are the explicit research questions here? I see mention of smaller teachers helping generalization, and also the effects of different sized datasets, and also out of distribution questions. It's too much to keep track of all at once, and it's all jumbled together in the prose. This section should be re-structured to include multiple explicit research questions separated into subsections, each making a specific comparison using the data in the massive Table 6 and Table 7.\n\nThe second paragraph on Page 7 seems to make some big claims: smaller teachers may be better and less data may help (although only on CIFAR-100)? In general, these are huge claims that deserve a lot more detail than the paper gives them. If the authors intend to stand by these claims, they need to spend a lot more time describing whether the evidence supports this. There are just too many axes in Table 6 (type, teacher size, student size, dataset subsampling, class subsampling), and it's a lot of work for me as a reader to try to figure out which parts of this table correspond to which claims and whether the evidence is sufficient. The authors need to break this table into smaller comparisons and address smaller research questions one by one.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Understanding Knowledge Distillation",
            "review": "This paper investigates the role of the temperature scaling hyperparameter in KD. It is theoretically shown that the KD loss focuses on the logit vector matching rather than the label matching between the teacher and the student as the temperature grows up. \n\nThe overall idea is interesting, and also shows that the KD with MSE consistently shows the best accuracy for various environments. \nThe paper shows that the temperature hyper-parameter can lead to two extreme special cases, and explained the connection using theorem 1, which leads to MSE loss for KD.\n\nA few questions/suggestions are summarized as below:\n1. The idea of using MSE loss is quite similar to the below paper:\nhttps://papers.nips.cc/paper/2829-two-view-learning-svm-2k-theory-and-practice.pdf \nI suggest the authors to add some discussions as well.\n\n2.The assumption for MSE loss is that the teacher network is better than student network, it maybe beneficial to directly use the MSE. If reversely, we have a weak teacher, which may not have better performance than teacher network, the MSE loss may give worse results. The authors can study this scenario as well. \n\n3. How did the authors tune the hyper-parameter (weight, alpha) for MSE loss? Is it based on the test data set?\n\n4. For Theorem 1, it could be named as proposition or lemma as I don't see very strong theory behind.\n\n5. It may also be good if the authors can further investigate why KD works, and why MSE loss is better than standard KD loss from theory perspective. The current version of the paper still is mostly based on the experimental results only. It seems that we still cannot fully understand why KD works. \n\n6. This paper needs to be cited and discussed:\nUnifying distillation and privileged information\nhttps://arxiv.org/abs/1511.03643  \n\n7. It will be good if the authors can release the source code used for the paper so that others can easily get the results for KD research area. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper mainly conducts some analysis on the hyper parameter setting of KD, especially temperature, and then present analysis on why high temperature generally leads to better result. Overall the contribution is quite sparse, and lack discussion with previous literatures, also the writing is not very clear to understand.  ",
            "review": "This paper present analysis about KD, first it conduct experiment on the choice of loss weight an temperature for KD, and find that high temperature leads to better KD result. then through derivation of the KD gradients over logits, it find that when temperature is high, the gradients is close to MSE learning between student and teacher's logit, and verifies the performance of KD is similar to MSE. Then to understand why high temperature gives lower training acc but higher test acc (i.e. better generalization), the paper propose a metric called TLD which measures the difference of ground truth label's confidence and predicted label's confidence. Last, the paper studied the effect of data size on KD.\n\nPros.\n\n1. This paper theoretically shows the formulation of gradient changes with the temperature being 0 and near infinity, and find that when the temperature is near infinity, KD is close to MSE with teacher's logit prediction.\n2. This paper conducts a thorough experiment on the choice of hyperparameters of KD like temperature, and to verify its other findings.\n\nCons.\n1. In the original KD paper[1] from Hinton, it is already showing the equivalence of Matching logits [2] to KD when the temperature is high,  thus the major contribution of this paper seems repetitive, and it does give any discussion about the difference of this aspect. \n2. The paper did not provide clear relations with existing work. e.g., in the previous paper Revisiting knowledge distillation via label smoothing regularization, they have shown the when temperature grows to sufficiently large, the KD acts like simple label smoothing, while the paper did not discuss those existing works and compare with its analysis.\n3. The paper lacks central contribution and takeaways, seems like concatenating multiple different technical analysis and experiments. Also, I believe the data size part is quite off the topic, the paper did not provide any links before present this section, if the purpose is to verify the temperature finding in previous sections in different data setting (long-tail, imbalanced data, e.t.c) then should give clear introduction and motivation.\n4. This paper certainly needs improvement on high-level writing and explanation, the writing of some part is hard to read, (1) the last part of section 2 (the MSE part) (2) section 3 (the TLD analysis part)\n5. The paper is not clear in low-level sentence construction, e.g., (1) in the last part of the introduction there is 'These findings are the same in OOD prediction tasks.' but no reference about OOD is given. (2) In Sec.3 :'However, the student trained with LKD seems to consider the TLD order more than that of LCE as each quantile is more clearly separated.' sentence like this seems pervasive in this paper, which is difficult to read. (3) Perhaps interestingly, our results indicate that the test accuracy does not depend on the different bundles, while KD is much better than CE.\n\nOverall, while the TLD measure is interesting, I believe the paper lacks central contributions, and the writing of major parts are not easy to understand (the MSE equivalence analysis part and TLD part), thus I would recommend rejection.\n\n[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015).\n[2] Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06, pages 535–541, New York, NY, USA, 2006. ACM\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}