{
    "Decision": "",
    "Reviews": [
        {
            "title": "Effectiveness and efficiency are the reviewer's main concerns.",
            "review": "-----------------------------------\nSummary\n\nThis paper introduces a new method for open-set domain adaptation by complementing the source data by generating samples from the unknown target classes. This idea turns open-set DA into a closed-set problem and can take advantage of a lots of techniques in closed-set DA. \n\n-----------------------------------\nPros\n- The paper presents a novel method that turns an open-set domain adaptation problem into a close-set domain adaptation problem by training the network to generate source data for the unknown classes. \nThe idea is interesting and helpful to the domain adaptation area for exchanging open-set/close-set sub-fields.\n- Experiments on various datasets and ablation studies are conducted to verify the performance of the proposed method.\n\n-----------------------------------\nCons\n\nThe reviewer's concerns are mainly on the effectiveness and efficiency of the proposed method :  \n\nEffectiveness : \n- Though the proposed method outperforms the existing ones in average accuracy, the proposed method didn't show dominant performance in class-wise comparison. Especially, a) In Syn2Real-O, over half of the class-wise accuracies are lower than existing methods. b) In VisDA-17 and Syn2Real-O, around/over 10% of accuracy drop is observed when classifying the unknown classes.\n- Methods for universal domain adaptation are also applicable in open-set domain adaptation (such as [a]). It will be better if the paper also compares with these methods in the experiments.\n\nEfficiency : \n- The paper didn't discuss the effect of n_g, the number of generated source data, on the performance. By inferring from Figure 2., n_g seems to be around the scale of the given source/target dataset, which is not efficient when applying to large-scale datasets.\n\n-----------------------------------\nReasons for Score \n\nAlthough the idea presented in the paper is interesting, effectiveness and efficiency are the reviewer's main concerns.\n\n-----------------------------------\n[a] You, K., Long, M., Cao, Z., Wang, J., & Jordan, M. I. (2019). Universal domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2720-2729).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising results, but lacking comparison with SOTA",
            "review": "The paper proposes to generate unknown source samples for open-set domain adaptation. To generate unknown samples, the paper uses adversarial learning with a discriminator motivated by the existing work. While there are generation methods for domain adaptation, but this work only generates unknown samples. In the end, closed-set domain adaptation can be used. \n\n\nStrengths:\n1. The paper is well-written and easy to follow\n2. The paper shows improvements by a large margin compared to the baselines.\n3. The method is able to leverage existing closed-set domain adaptation methods, which makes the proposed method more applicable.\n \n\nWeakness:\n1. The key concern of the paper is that SOTA open-set DA methods are missing in the paper [1, 2, 3] even in the related work.  The method should be compared with the SOTA methods [1, 2, 3].\n2. Some details on the generation are not clarified. How many unknown samples are generated? And there is no visualization of the unknown samples. \n3. In Fig. 2(a), target known samples should be also visualized for complete analysis.\n4. The paper does not show the number of runs and standard deviations.\n5. The experiment will be more thorough with other common benchmarks such as Office-31 than having the two synthetic datasets.\n6. OS* accuracy on Office-Home is missing.\n\nOverall, the method obtains promising results but some experimental details should be clarified and more analysis should be done.\n\n[1] Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation, CVPR'20\n[2] Towards Inheritable Models for Open-Set Domain Adaptation, CVPR'20\n[3] On the Effectiveness of Image Rotation for Open Set Domain Adaptation, ECCV'20",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Borderline paper with limited novelty",
            "review": "This paper tackles the problem of open-set domain adaptation, where the target domain may contain unknown classes. They proposed a method that utilizes a generator to directly generate images corresponding to these unknown classes. By doing so, the generated images can complement existing source training data such that the new joint distribution is matched. Several other ingredients including adversarial training and distribution matching are also considered in the proposed method. While the method does show some improvement over baselines, there is no obvious novelty of the paper and little evidence has been provided to illustrate the effectiveness of the proposed generator.\n\nMain Pros:\n1. The results show some improvement on the open-set domain adaptation (DA) setting.\n\nMain Cons:\n1. The method combines several techniques proposed in prior work with no obvious novelty. For example, the idea of using a generator in DA has been explored in [1] while exploiting dummy labels for joint distribution matching has been studied in [2]. In addition, these techniques are not well motivated. For instance, it is not clear to me why two distribution matching methods (MMD and discriminator) are needed at the same time. A better explanation is needed.\n2. I consider the application of the generator to open-set DA as the main contribution of this work. However, its effectiveness is not well supported by neither theoretical analysis nor empirical results. Theoretically, how could you guarantee that the dummy class C correctly represents the unknown? There is no explicit training signal for it and thus, in theory, it can represent anything. More justification would be helpful. Empirically, does the generator actually generate images from unknowns? In Figure 2(b), the generated images are mixed with source images, and thus it is hard to tell. Can the author actually show some example images generated, compared to unknown classed in the target? Do they look similar? Besides, Table 4 shows an ablation result using noise images instead. This baseline is too weak and clearly more variants can be compared, such as using images from the target (probably with some selection rules based on similarities to the source), or by adding noise to the feature space directly. More careful analysis is required to verify the efficacy of the method.\n\nMissing Reference:\nThe idea of incorporating a generator has been explored in [1]. Also, \"negative transfer\" is not a new term and has been studied extensively in [2] and [3]. It's good to mention these related work.\n\nTypo:\nEquation 4: L_b -> L_d\n\n[1] Generate to adapt: Aligning domains using generative adversarial networks. CVPR 2018.\n\n[2] Characterizing and avoiding negative transfer. CVPR 2019.\n\n[3] To transfer or not to transfer. NIPS workshop on transfer learning 2015.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}