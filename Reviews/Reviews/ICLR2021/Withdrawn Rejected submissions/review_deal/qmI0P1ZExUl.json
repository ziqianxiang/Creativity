{
    "Decision": "",
    "Reviews": [
        {
            "title": "Learning Encoder for StyleGAN",
            "review": "This paper provides a way to learn an encoder for a pre-trained StyleGAN. Given the structure of StyleGAN, the encoder is also designed to output different style codes in different layers. With a fixed StyleGAN as an image decoder, the author train the encoder with image reconstruction loss, perceptual loss, and face identity loss. Multiple experiments are performed on human face reconstruction and generation. \n\nPros:\n\n+ The method is simple and the qualitative results also look good. \n+ The authors also show that the encoder is able to use for training other image-to-image translation tasks besides reconstructing the original image. \n+ The method shows improvement over state-of-the-art approaches including SPADE and pix2pixHD quantitatively and qualitatively. \n\nCons:\n- Although the paper provides a simple method for training the StyleGAN encoder, the training processes of the encoder and StyleGAN are separated, there is no discussion on what happens if we train the StyleGAN and encoder together from scratch. This makes the comparison also a little unfair since pix2pix is an auto-encoder that can be trained end2end. \n- For a more fair comparison, there should be a baseline that also uses a pre-trained GAN and only train an encoder for the pre-trained GAN. \n- The authors only show results on human faces, which is a bit restricted. The loss also contains an identity loss which is designed for faces, which makes the method less generalizable. The authors should show results on other categories, for example, cars and birds. The author should also analyze how important the identity loss is.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Useful approach, good results, limited domain",
            "review": "The paper proposes a new architecture for StyleGAN encoders. The proposed encoders output the high-dimensional representation in W+ vector space. The main encoder takes as an input a face image and outputs its latent code. There are additional encoders that take segmentation map, sketches, and a frontalizing encoder.\n\nPros:\n-- The paper is well written, it has a thorough evaluation section.\n-- A large number of applications show the versatility of the translation approach within the face editing domain\n-- The results are really good, clearly an improvement over two strong competitors (ALAE and IDInvert). \n-- While the idea of latent-space editing is well-explored, the multimodal translation image translation ideas are new and interesting to me.\n-- The proposed architecture of the encoder makes sense and is apparently working well.\n\nCons:\n-- A single domain (near frontal faces) is considered\n-- Most ideas behind the encoding part (the idea of training an encoder, the use of W+ space, the losses) have been proposed before.\n-- As the authors rightfully admit in the discussion section, the whole approach works well whenever the target domain can be well covered by StyleGAN (e.g. near frontal faces). For most domains this is not true, and the proposed approach will not be competitive with more local image translation approaches (unless optimization and generator adjustment are added on top).\n-- The idea behind the unsupervised frontalization is a bit hacky. The frontalization results are worse than for other applications. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An okay paper but the novlety is not sufficient. Experimental results are not convincing, either. ",
            "review": "Strength:\n- This work proposes to train an encoder to cope with the well-trained StyleGAN. Neither tuning the latent nor optimizing the StyleGAN is required during testing, making the model inference efficient and elegant.\n\n- Style mixing is naturally used to achieve diverse generations. Hierarchal variations are achieved by manipulating the coarse and fine features in W+ space.\n\n- The generated results of conditional face synthesis are appealing due to the capacity of StyleGAN. The faces from sketch and semantic maps are photo-realistic.\n\nWeakness:\n- The proposed method uses an identity loss is induced by a face recognition network, namely AcrFace. Hence, this method is designed solely for face generation but not general image-to-image translation, which conflicts with the claim that pSp is \"a generic image-to-image translation framework\".\n\n- This paper claims that \"a novel StyleGAN encoder\" is proposed. However, the architecture of the encoder is based on the Feature Pyramid Network. The used units are convolutions and LeakyReLU, which are both routine choices. Most importantly, the idea of using an encoder has been explored by ALAE. Hence, using an encoder for StyleGAN is not novel and cannot be regarded as a contribution.\n\n- The encoder is optimized by minimizing the combination of the pixel-wise loss, perceptual loss, identity loss, and regularization that keeps latent vectors close to the average value. This setting simply puts four known losses together, but the weights differ significantly for different tasks (Supp A.1). It seems that significant efforts are needed to choose these weights. I suggest the authors provide more details on how to determine the hyper-parameters. Does a complete grid search is required?\n\n- The experimental results are not convincing to demonstrate the advantages of the proposed method. In the task of face frontalization, R&R significantly outperforms the proposed method, at least for the current metrics. Besides, neither mainstream evaluation metrics (e.g., rank-1 accuracy and true accept rate) nor datasets (e.g., LFW, IJBA, and MegaFace) are used.\n\n- The overall visual quality is not significantly better than that of the competing methods. For instance, in Fig. 2, It seems that the results of IDInvert look more realistic. In Fig. 5, I prefer the results of R&R, especially when I zoom in to check the details.\n\n\nNovelty: \nThe novelty of this paper is marginal. ALAE has explored the idea of introducing an encoder to copy with StyleGAN. While  directly embedding real images into W+ space makes the inference process efficient, the encoder settings, e.g., the loss function and network architecture, are simply borrowed from prior works.\n\nPresentation:\nThe writing is OK but still needs some improvements. For instance, Figures 2-5 should be more self-contained. I suggest the authors proofread the manuscript.\n\n \nContribution:\nThis paper proposes to train an encoder to learn how to embed images into the W+ space of StyleGAN. Neither tuning the latent nor optimizing the StyleGAN is required during testing, so the inference is very efficient. Besides, diverse generations can be achieved by applying style mixing in the W+ space.\n\nHowever, the proposed framework lacks novelty in general. The claims like \"(we propose) a novel StyleGAN encoder\" and \"we present a generic image-to-image translation framework\" are not well supported.\n\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}