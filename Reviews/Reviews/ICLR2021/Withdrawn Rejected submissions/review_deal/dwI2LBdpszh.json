{
    "Decision": "",
    "Reviews": [
        {
            "title": "Recommendation to Reject",
            "review": "##########################################################################\n\nSummary:\n\nThis work targets at improving the efficiency of some GCNs. In particular, it discusses two card shuffling hypotheses, and proposed two shuffle methods based on the hypotheses to improve the efficiency. The first method targets at Dynamic Graph CNN (DGCNN) (Wang et al., 2019), which uses the KNN search to generate dynamic graphs layer by layer. As each layer of DGCNN uses the KNN search, the following KNN searches can be accelerated by shuffling the NN selection criterion based on the results of the first KNN search. The second method targets at Point2Mesh (Hanocka et al., 2020). As reasonable arrangement of the order of local feature aggregation and feature extraction may improve efficiency, shuffling the order may be beneficial. Experimental studies on three applications demonstrate the improvement brought by the proposed shuffle methods.\n\n##########################################################################\n\nReasons for score: \n\nOverall, I vote for rejection. My major concern lies in three aspects, as detailed in the Cons: 1) The motivation is not accurate; 2) The impact of this work is quite limited; 3) The experiments need more baselines.\n\n##########################################################################\n\nPros: \n\n1. Using the card shuffling hypotheses to improve DGCNN is quite interesting.\n\n2. The presentation is well organized. The proposed methods are described clearly.\n\n##########################################################################\n\nCons: \n\n1. The motivation is not accurate. The statement quoted from the abstract \"State-of-the-art GCNs adopt K-nearest neighbor (KNN) searches for local feature aggregation\" is invalid for GCNs in general. The KNN search only appears when using dynamic graphs over layers, as in DGCNN. The motivation has to discuss the certain applications and models to make statements accurate.\n\n2. Based on the current version, this work just aims at improving DGCNN and Point2Mesh only, which thus has a limited impact. The shuffle H1 can hardly be applied on general GCNs, where no dynamic graph is considered. The shuffle H2 may have a wider application scenario. However, it needs more discussion and experiments. It is closely related to studies like [1,2,3].\n\n[1] Wu et al. \"Simplifying Graph Convolutional Networks\", ICML 2019\n\n[2] Liu et al. \"Towards Deeper Graph Neural Networks\", KDD 2020\n\n[3] Chen et al. \"Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification\", ICLR 2019 RLGM workshop\n\n3. The experiments need more baselines. All the experiments only have one baseline, making it difficult to assess the flexibility of the propose method. Also, it would be good to see Shuffle H1 and Shuffle H2 in tables 2-4 if applicable.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Some major issues",
            "review": "This paper presents two methods, called card shuffling hypotheses, to reduce the computational complexity of graph-convolutional neural networks. Even if I believe that reducing the computational complexity is a very important issue in the research on GNNs, I think that this paper has a few major issues:\n\n- Despite the general claims in the introduction and in the abstract, the proposed methods focused only on a specific definition of graph convolution (i.e., the DGCNN) and a specific application scenario (i.e., 3D point clouds). However, in the literature there are many definitions of graph convolutions and many application scenarios other than 3D point clouds. I agree with the authors that DGCNN is one of the most popular definitions of graph convolution for point cloud processing. However, there are other definitions that should be considered (even more recent than DGCNN), see for example [1]. Moreover, point cloud processing in not the only application for GNNs, in other application fields there are other definitions of graph convolution that should be considered (e.g., GIN, GCN, GraphSage etc...). \n\n- The contribution of the paper to the research in this field seems limited. The first method proposed in the paper focused on reducing the complexity of the graph construction operation. Instead of performing the knn search at every graph-convolutional layer, the authors propose to perform the knn search only at the first layer selecting a larger neighborhood and then in the next layers the neighbors are randomly sampled from this larger neighborhood. However, this method is only useful  when we consider a dynamic construction of the graph. In other definitions of graph convolution, the graph is not dynamic (see, e.g. ECC, GIN, GCN...), especially when we consider applications different than point cloud processing. Instead, the second method proposed in the paper is just a rearrangement of the order of the operations used to perform the graph convolution. This rearrangement seems quite straightforward and it is valid only for this specific definition of the edge function (i.e., where the global information x_i and the local information x_i-x_j are combined using that specific function). However, many other popular definitions of graph convolution use different edge functions where the proposed method is not applicable, see [1], [2] and many others (GIN, GCN, GraphSage etc...).\n\n- The assumptions used to derive Theorem 1 are too strong. The definition of graph convolution introduced in (6) does not correspond to the DGCNN (the only definition of graph convolution considered in the paper), the local contribution (i.e., x_i-x_j) is not considered. Moreover, the second assumption basically defines the graph convolution as a random projection and it is well known that random projections preserve the distances (see the Johnson–Lindenstrauss lemma). Therefore, if we consider graph convolution as a random projection, the result of theorem 1 is quite straightforward.\n\nMinor comments:\n\n- Notation is confusing, some symbols are not defined (e.g. phi) and the notation is not coherent (e.g. the notation of the neighbors changes: in Sec. 2 a neighbor is defined as x_i^k and in Sec. 3.3 it becomes x_k).\n\n\n[1] Li et al., DeepGCNs: Can GCNs Go as Deep as CNNs?, ICCV 2019\n[2] Velickovic et al., Graph Attention Networks, ICLR 2018",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting approach and motivation to enhance efficiency in DGCNN-like scenarios, but not enough proof to support claims.",
            "review": "## Summary:\n\nThis paper presents alternative operations for Graph Convolutional Networks (GCNs) that are both time and memory efficient when compared to current art. The authors focus their work on the popular DGCNN framework - based on EdgeConv, and show results on point cloud and mesh applications. The authors propose 2 operations:\n\n1. Shuffling neighbor selection across different layers of the GCN. This is an alternative to re-computing neighbors at each layer, which is a computational expensive procedure.\n2. Shuffling the order of the convolutions in EdgeConv, therefore significantly reducing its memory footprint.\n\nThe authors back their new operations with a theoretical analysis, and show positive results on tasks of point cloud classification and segmentation, as well as mesh reconstruction. I believe memory and computational time are a genuine bottleneck in GCNs and addressing this issues is of importance.\n\n## Strengths:\n\nI believe tackling the efficiency problem in GCNs is important, and I thank the authors for looking into this space. This work has several interesting points worth highlighting:\n\n1. The authors present a theoretical framework for the analysis of distances across GCN layers. Building strong theoretical foundations is an important, yet rarely tackled aspect in this field. The derivations presented in the proofs seem solid.\n2. The paper has extensive experiments on different datasets, tasks, and data modalities (point clouds and meshes). The authors also present large ablations showing improved performance and lower computations and memory.\n3. The authors provide code as well as large numbers of qualitative results.\n\n## Weaknesses:\n\nAlthough the paper tackles an interesting problem (time/memory efficiency in GCNs), I see some major weaknesses in its current version. I'll describe them below:\n\n### Hypothesis 1:\n\n- Hypothesis 1 is based on 2 assumptions. First it assumes EdgeConv is based on inner product and summation. This omits the non-linearity, ReLU in most cases. Having a ReLU would break the equivalence between (15) and (16) in the upper bound proof in B.1 ($\\|max(w,0) - max(v,0)\\|^2 \\neq \\|max(w-v, 0)\\|^2$). Second, it treats weight parameters as *iid* Gaussian. While this is true at initialization, there are no guarantees of such behavior as the network trains [[1]](https://arxiv.org/pdf/1806.07572.pdf). \n\n- The authors do state that such assumptions could be violated and they indeed show decent empirical performance. However, there could be other reasons behind these results:\n  \n  1. **Dilations:** Recent works on point cloud analysis ([[2]](https://arxiv.org/pdf/2007.01294.pdf),[[3]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf),[[4]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Choy_4D_Spatio-Temporal_ConvNets_Minkowski_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)), show how encoder-decoder type architectures achieve significant performance gains. This would be accredited to enhancing the receptive field of the network through striding/pooling. Since in the current work the authors sample larger areas of points as depth of network increases, the gain could come from the mere dilation effect. As the ablations show, best results come from high values of P and K (10 and 30 respectively).\n  2. **Oversmoothing:** GCNs can suffer from oversmoothing, where the activations might collapse to similar values. If the network is in fact oversmoothing, this leads to activations being progressively closer to each other. Although the results do show performance, therefore hinting at discriminative features, there are no experiments to quantitatively show the actual distances between representations. If the activations are fairly similar then convolutions might not be too sensitive to the choice of neighbors. \n\n### Hypothesis 2:\n\n- The explanation of this hypothesis is valid, although fairly trivial. I would point out that a similar formulation is presented in [[5]](https://arxiv.org/pdf/1904.03751.pdf) under *ResMRGCN*, where the authors shuffle the max and concat operations to achieve similar memory gains. The authors should point out how their work differs from [5] and if they present any advantages.\n\n### Dynamic kNN: \n\n- The paper is limited to DGCNN scenarios, where kNN has to be recomputed at every layer (or every few layers). With this limitation, the authors only show results on 3D vision tasks, and can only compare against DGCNN. Compared against SOTA, their results still lag behind. One solution would be to look at other setups where DGCNN is also used. An example of this can be found in [[6]](https://arxiv.org/pdf/1911.11462.pdf).\n\n## Rating and Justification:\n\nI vote to reject this paper in its current form. I do believe it is important to analyze efficiency in GCNs, and I applaud the authors for presenting a theoretical analysis to their claims. I am rejecting this paper for the following reasons. First, given the assumptions on hypothesis 1 don't generally hold, the authors need to show where the performance is coming from. I suggest in the *Questions and Suggestions* section some useful experiments to validate the author's hypothesis. Second, hypothesis 2 is very trivial and a similar one was presented in the literature (see [5]). Finally, the results are limited to one scenario only (dynamic kNN computations), commonly used for 3D data. In this scenario the authors don't show competitive results against other methods. As I mentioned above, one way to overcome this limitation is to look at other DGCNN-like settings. \n\n## Questions and Suggestions:\n\nHere are some experiments I would encourage the authors to run for upcoming submissions:\n\n- Look at the distribution of feature distances in the final activations. Are they all similar within a threshold? Would the similarity still hold in a regular DGCNN scenario? If they're all very similar then performance from shuffling neighbors could be accredited to oversmoothing and not distance preservation.\n- Plot the actual weight distributions during training, see if the *iid* Gaussian assumption holds.\n- As a baseline, what happens if you randomly choose your neighbors at each layer (from the whole graph and not just the available pool)? Intuition would say this shouldn't perform well, but since this work suggest a way to increase the selection pool at deeper layers, it would be solid to show how this compares to a random baseline.\n- Compare your hypothesis 2 scenario with *ResMRGCN* in [5] and other methods that perform efficient graph convolutions.\n- Would your analysis still work in other DGCNN frameworks like [6]? An experiment here would also be beneficial.\n\n## References:\n\n[1] Jacot, Arthur, Franck Gabriel, and Clément Hongler. \"Neural tangent kernel: Convergence and generalization in neural networks.\" Advances in neural information processing systems. 2018.\n\n[2] Liu, Ze, et al. \"A Closer Look at Local Aggregation Operators in Point Cloud Analysis.\" ECCV (2020).\n\n[3] Thomas, Hugues, et al. \"Kpconv: Flexible and deformable convolution for point clouds.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n\n[4] Choy, Christopher, JunYoung Gwak, and Silvio Savarese. \"4d spatio-temporal convnets: Minkowski convolutional neural networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n\n[5] Li, Guohao, et al. \"Deepgcns: Can gcns go as deep as cnns?.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n\n[6] Xu, Mengmeng, et al. \"G-TAD: Sub-Graph Localization for Temporal Action Detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The direction is interesting, but the work lacks novelty.",
            "review": "Overall, the paper works in a very interesting direction of improving the efficiency of GCNs. The paper introduces two ways to reduce the time and memory consumption of GCNs. Experiments demonstrate the effectiveness of the proposed methods.\n\nHowever, there are several problems that need to be addressed:\n\n1. The work introduces two Card Shuffling Hypotheses and proposes two tricks to improve the efficiency of so-called “Graph Convolutional Network”. However, the paper mainly works on one particular kind of graph convolutional network--EdgeConv. \n\n2. In the paper, the authors use many words that should be explained in more details. For example, “To some extent” after Definition 1, “relatively closer” after Theorem 1. \n\n3. The authors should give more explanation about Assumption 2. For GCN, there are many ways to initialize the parameters and the parameters are not necessary following the same distribution as initialization after training. Hence, the authors should give more evidence to validate the Assumption 2 or give a reasonable explanation.\n\n4. One of the major contributions in DGCNN is the dynamic edge update. However, by using the Card Shuffling Hypothesis 1, the network would lose the dynamic edge property. That is probably one of the reasons why the shuffled model performance is worse than the DGCNN.\n\n5. The experiment results(92.10,92.56) in Table 1 are different from the one reported in the original DGCNN paper(92.9, 93.5). And, the running time of DGCNN is 74.7ms in this paper VS. 27.2ms in the original paper. It would be better that the authors could give some detailed analysis on that.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}