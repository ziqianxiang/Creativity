{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes an approach to training language instruction following agents that aims to improve their compositional generalization., by means of an entropy regularization method to reduce redundant dependency on input.\n\nAll four expert reviewers agreed that the paper is not ready for publication in its current form. Of biggest concern is the fact that the reviewers could not interpret the exposition of the method, so were unable to be sure exactly how the method worked. This can be addressed in a future submission by clearer presentation. \n\nAnother concern was that the authors only consider a single benchmark, and fail to situate the work relative to other grounded language learning tasks and datasets. Thus, reviewers were concerned about the generality of the method, and suspected it may be too specific to the gSCAN setup. \n\nThat said, the reviewers were all impressed by the strong results on the gSCAN benchmark. It strikes me that there is some interesting insight here that can be derived from this impressive performance, that may also be applicable to other grounded language learning settings. However, to make the paper acceptable for publication the authors must do a much better job of communicating how their method works, what that specific insight is and how it is relevant beyond the gSCAN dataset (ideally via direct experimentation in other settings)."
    },
    "Reviews": [
        {
            "title": "Poorly written, poorly discussed related papers, not enough novelty",
            "review": "Summary\n\nThis paper tries to address a very important problem, compositional generalization in grounded agent instruction learning. It proposes to use interactions between agent and the environment to define output components, and entropy regularization to reduce redundant dependency on input. It shows significant improvements in most of gSCAN tasks. The paper also has an ablation study that investigates the effectiveness of entropy regularization and other factors.\n\nStrengths\n\nGrounded compositional generalization is a key challenge to the AI community. This paper proposes entropy regularization and shows some good results compared with Seq2seq and GECA are from Ruis et al. (2020). Semantic is from Kuo et al. (2020). \n\nWeaknesses\n\nThe paper is very hard to read. Notations is Section 4 is very confusing. \n\n\nIt is not clear there is any novelty in the model architecture, command, grounding and prediction modules. If not, the paper should clearly cite relevant papers.\n\n\nThe paper claims to be \"the first work to enable accurate compositional generalization in grounded instruction learning problem, serving for analyzing and understanding the mechanism\". It is not clear this is true given Ruis et al. (2020). Semantic is from Kuo et al. (2020).\n\n\nThe paper fails to make a connection to the two papers. It only says \"we apply the prior knowledge for the interactions of agent and the environment.\".\n\n\nSome qualitative results in the main text or appendix would have helped to illustrate the proposed approach and offer insights on why the method is effective.\n\n\nDecision\n\nSince the paper is poorly written, not put in the proper context of the related papers, and the novelty seems to be limited to entropy regularization, my decision is rejection. I would be open to revise my decision if the authors make clear of their methods and contributions.\n\n=====POST-REBUTTAL COMMENTS========\n\nThe authors did not make a good effort to address my comments and failed to update the paper. Therefore, I maintain my original decision.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good results, but many questions about the system and experiments",
            "review": "Summary:\n\nThis paper proposes a new model for the gSCAN dataset (Ruis et al. 2020) which is a synthetically-generated dataset that challenges models to generalize to new compositions of attributes and objects in an instruction. The paper proposes to use \"entropy regularization\" as a way to enforce that spurious correlations between input tokens and output actions are not learned. The proposed model shows promising results on the gSCAN benchmark, achieving nearly 100% accuracy on all but one task.\n\n########\n\nReasons for score: \n\nWhile the paper achieves impressive performance on the synthetic gSCAN dataset, the paper's framing of the problem, discussion of related work, and description of the model are vague and confusing. As a reviewer, I am left with many questions and confusions about the setup and scope of this paper, and my review depends on clarifications from the authors during the discussion period.\n\n########\n\nStrengths:\n1. The paper achieves impressive performance on the gSCAN dataset across all but one task, and describes intuitions on why the remaining task failed.\n1. The paper also achieves impressive results on the target length task when compared to existing systems.\n1. The proposed model is relatively simple.\n\n########\n\nWeaknesses:\n\n1. My main concern about the paper is its framing of the problem and use of vague terms, especially in the introduction and abstract. See below for more suggestions as well. The paper should be more clear about the scope of compositional generalization it is targeting, especially early in the paper. Specifically, it seems to be targeting novel compositions of attributes and objects in an input instruction. However, there are other dimensions of compositional generalization which the paper does not cover. For example, generalization to new compositions of the environment itself (i.e., novel combinations of the objects, attributes, and spatial relations, as evaluated in datasets like ALFRED (Shridhar et la. 2020), Room to Room (Anderson et al. 2018)), or novel deep compositional structures in the output space (as in semantic parsing, e.g., Finegan-Dollak et al. 2018, in comparison to shallow output space compositions such as new manner/direction combinations). This paper should clarify the dimensions along which it is evaluating compositional generalization, and contrast that with other forms of compositional generalization. Some other related work which evaluates compositional generalization and are missed include VQA with Changing Priors (Agrawal et al. 2017) and CLEVR CoGenT (Johnson et al. 2017). Especially for CLEVR CoGenT, there are several systems which achieve near perfect accuracy on that compositional generalization task, such as NS-VQA (Yi et al. 2019), which are not compared against.\n1. The evaluation is only performed on a single synthetic dataset. While it performs well on this benchmark, evaluation on other grounded instruction following tasks, such as some of the examples above, or even on non-instruction tasks such as image question answering, would make the results more convincing. \n\n\n########\n\nMajor questions for the authors about the paper setup :\n\n1. I am confused about where \"entropy regularization\" comes into play in the model. As it is written it seems to be used in two places: (1) in the actual forward-pass, the only difference is that it adds noise to an input (and the noise is zero at evaluation time), and (2) during optimization, it also minimizes the L2 norm of these layers' activations. Is this correct? \n1. In the ablation setting, it's unclear to me what exactly is being ablated. If EntReg appears both in optimization as L2 regularization and in the forward pass as noise, then are these ablated independently? What is ERL replaced with when ablated? And what exactly is NodeOut -- it's not defined anywhere earlier in the paper, if it is the most critical component of the approach (as indicated by Table 2), this is concerning.\n\nOther clarification questions for the authors:\n1. What is the intuition of the EntReg component actually solving the problem of learning spurious correlations? How does it actually limit the correlations (if this is the intent) between components if it is not considering some relationship between two components?\n1. What does it mean for compositional generalization to require prior knowledge on distribution change? \n1. Am I correct in understanding that the command module will always result in the same output while the agent is acting in the environment? Nothing about the command module seems to change as it is not conditioned on an environment observation.\n1. How does the model know the types and values of the words in the command? \n1. How do \"yellow squares\" and \"red squares\" actually differ in the compositionality they require?\n1. Does \"relatively\" test new attributes that were never seen or referred to during training? Or just new ways of referring to these attributes?\n1. What's an example of \"class inference\"?\n1. How is attention evaluated? Automatically?\n\n########\n\nSuggestions to clarify the presentation of the paper and the approach:\n\n1. The related work should contrast the proposed approach or evaluation setup with the cited works. E.g., how does the proposed approach differ from the approaches used on SCAN?\n1. \"entropy regularization\" is a more general term than it is used in this paper, so I would suggest renaming it. E.g., it is often used to prevent entropy collapse in reinforcement learning by adding an additional term in the optimization that optimizes for higher entropy output distributions. \n1. I am confused about why the term \"entropy\" is used here, because I don't see the relationship between the function EntReg and entropy (i.e., H(x) where x is a distribution). EntReg here just seems to add some noise to an input x, which is not necessarily a distribution. \n1. Several terms are vague/undefined. The term \"component\" is vague without a formal definition of the task being studied. Later on, I am also confused on what \"a change of direction between steps\" means, and why there is a direction, action, and manner. The example in Figure 1 doesn't show the output sequences or examples of different manners, and I only have an idea of what \"manner\" means when I read the gSCAN paper or later on in this paper that manner might include spinning, being slow/fast, etc. So, I would suggest, e.g., showing the actual possible space of outputs.\n1. The proposed approach should be described more concretely in the introduction before the results of it are described.\n1. Several terms used in the model description are vague and should be defined: what is a \"node\" x_i (Section 4.1)? What is the difference between ERL and EntReg (my reading is that ERL is an MLP with EntReg between the layers)? What is a \"keyword\" (as opposed to just a \"word\")?\n\nSuggestions on the description of model / results:\n1. The model seems to be described formally three times: in Algorithm 1, Figure 2, and Section 4.2. Its description could be more concise (e.g., by removing Algorithm 1).\n1. The experiments section should contrast the proposed approach with the baselines/other systems in a meaningful way. \n1. I know this is coming from the gSCAN dataset, but the names of the 8 tasks are very confusing. If it's possible to augment these with a short keyword description that accurately describes the forms of generalization they are evaluating, this would make things a lot easier to read. Particularly, upon reading the results table, the \"yellow squares\" and \"red squares\" settings aren't meaningfully different, and I have no idea what they are testing differently.\n1. If Figure 3 is just a bar graph version of Table 1, it could probably be removed as it is doesn't add any information over Table 1. It's missing the numbers in the bars, the x-axis is hard to read, and it's unclear to me whether this is graphing the mean, max, etc.\n1. The meaning of $k$ should be defined before Figure 3 and Table 1 are presented.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposes a new regularization method that constrains the mapping between the inputs and output spaces for achieving compositional generalization in simple grounded environments like gSCAN. The problem is interesting and important and the paper is corroborated by good experiments with 25% accuracy increase and also generalization to longer commands. However, the paper has  clarity issues with descriptions that are sometimes vague or not precise enough and quite frequent language mistakes. It also doesn't discuss almost at all existing works and mention the only very briefly, making it harder to judge the strength of the new approach as there's not enough context.\n\nIn addition, the idea presented feels somewhat too specific to the particular gSCAN task. For instance, it considers particularly disentangling the representation of each step to direction, action and manner components. I would hope that a general approach will discover a disentangled representation that allows compositional generalization on its own, rather than being hand-engineered for the particular dataset, especially given its relative simplicity. \n\nFor the entropy regularization idea, while it may allow for compositional generalization, it may reduce the model ability to capture trends in the training data, and so it may produce too \"extreme\" representations that can’t account for correlations within the data, and therefore I suspect it may not work well for more complex problems beyond gSCAN.\n\nComments and questions\n- The related work sections gives background about the subject and used dataset but has only one short like about competing approaches. It will be good to move the general text from the related work section to the introduction section and instead add a bit more detailed description about prior approaches for the problem and how they differ from the new method.\n- The experiments section doesn’t provide any details about the baselines either, so overall a more detailed comparison to existing approaches or putting the paper in the context of prior work is really missing.\n- Page 2: \"Another related work is independent disentangled representation (Higgins et al., 2017; Locatello et al., 2019), but they do not address compositional generalization.\" -> Why aren’t they? The main advantage of disentangled representation is their ability to generalize to combinations of properties outside the training distribution. For instance, if you encode separate features for color and shape, you may learn to generalize to any combination of them even if the training data didn’t cover all of them, since your representation inherent compositional structure that separates out these two properties may prevent capturing spurious correlations of the two properties and encourage generalization to combinations of them.\n- The description of the task is not clear enough. A more formal/mathematical definition of the task will be useful, especially if letters are presented for e.g. the input command, output sequence etc it will be easier to refer back to them later in the paper. Also further description/ a couple of examples on what the actions and the manners are will be useful for those not familiar enough with gSCAN.\n- \"We also assume automatic collision prevention… This makes us focus on addressing grounded compositional generalization problem.\" - is it fair to assume that or does it simplify the problem? Are alternative approaches assume that? How is it useful?\n- Page 3: \"For example, when \"red\" and \"square\" do not appear together in any training sample, a model might learn that square is not red. However, this causes errors for compositional generalization in test. To avoid such case...\" - Do we want to avoid such cases? A model that will learns that there is no correlation at all between pairs of properties will not work in the real world. Rather than avoiding learning the correlations it will be useful if the model will be still able to learn them but at the same time allocate some smaller probability for the case of combinations that are less common.\n- The description of entropy regularization isn’t completely cleared to me. What are x_i and y_i. Does each y_i depend on all x or only xi?\n- It sounds like basically entropy regularization reduces the capacity of the representation by adding noise + l2 loss on the activations. This sounds like a quite general regularization technique but is unclear to me why this encourages compositionality in particular. Further explanation of that will be helpful.\n- A small comment, since the generalization to length work well but one-shot learning isn't and is still an open problem, it may make more sense to put the length subsection first and then the one-shot learning one.\n\nSome Typos\n- What are the components in output -> in the output\n- other changes with ablation study -> with an ablation study\n- to understand command and the environment -> the command and\n- redundant dependency on input -> on the input\n- Grounded SCAN (gSCAN) dataset -> the Grounded SCAN dataset\n- but agent needs -> but the agent need or but agents need \n- of agent and the environment -> of the agent\n- to change position -> to change its position\n- on addressing grounded compositional generalization problem -> addressing the problem\n- Input contains command -> the input \n- Output contains -> the output\n- Information of color -> of the color\n- design entropy regularization layer -> a/the layer\n- finds correct object -> find the correct object\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "the paper needs major clarity improvements; reject",
            "review": "The paper aims to improve compositional generalization of grounded language learning methods. As far as I understood, this is achieved by (a) crafting a task-specific architecture with noise addition in certain spots and (b) changing the output format. Unfortunately, I did not understand many details that would be crucial to properly review this paper. I found the paper to be very unclearly written, see details below.\n\n1. As far as I understand the key contribution of the paper is supposed to be using “interactions between the agent and the environment to find components in the output”. The place in the paper that seems to explain what this is about is Section 3. But since this section does not explain what the original action space in the task is and does not show example trajectories, I found it impossible to understand what the \"components in the output\" and \"interactions between the agent and the environment\" are. \n2. The entropy regularization is also explained poorly. I understand what EntReg is. But to explain ERL the paper refers to “nodes x_i”. I do not think the paper clearly explains what these “nodes” are, and how this is related to the possibility that “a representation can be fed to multiple networks”.\n3. I struggled to understand the specific architecture that is explained in Algorithm 1, but one thing that struck me is that unlike the baselines that the paper compares to, it uses an object-centric representation, whereas the baselines are taking images (albeit symbolic) as inputs. This distinction alone in principle might explain the difference in the results. \n4. To make Algorithm 1 more understandable it could be helpful to visualize what the N command modules are and what they are supposed to do. A figure with an example that explains the motivation for the architecture would be of great help. \n\nUnfortunately, this concludes my review: I was not able to understand enough in order to make more substantial comments. I would encourage the authors to improve the paper in the following aspects: \n- clear explanation of the task, including input and output format\n- clear presentation of the model, including a visual motivating example\n- clear comparison to the baselines that takes into account the difference in input representations. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}