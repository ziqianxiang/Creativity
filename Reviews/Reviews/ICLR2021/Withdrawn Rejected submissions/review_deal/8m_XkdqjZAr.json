{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good motivation but lack many details",
            "review": "This paper proposes a resampling method based on SMOTE to address data imbalance issue. It adopts an autoencoder to map original data on to a latent space and applies oversampling by interpolation in the latent space. It can be applied to both continuous and discrete features. Experiments on 35 multi-modal datasets show that the proposed autoencoder based SMOTE outperforms several existing methods.\n\nStrength:\n- It is an interesting idea to interpolate latent space inn autoencoder as a way to do resampling. Although it is not new to interpolate data point in latent space, this work shows how such interpolation can be plugged into standard resampling methods like SMOTE.\n- Experiments are conducted on 35 public datasets and have shown the simple autoencoder achieves good results compared to its more complicated rivals.\n\nWeakness:\nOverall, this is not a well-written paper. The problem is clearly stated but the motivation is kinda vague. I also have concerns on the evaluation part in terms of the reliability of the conclusion drawn from the results. Please see my comments below.\n\nDetailed comments:\n\nContribution:\n- First, interpolating data points on feature space is not new. Prior works have explored different ways to improve feature space interpolation to create more realistic samples or address under-represented data. For example,\nYin et al., Feature Transfer Learning for Face Recognition with Under-Represented Data\nBerthelot et al., Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer\nBoth have studied how to do better interpolation on feature space.  \nI understand this work is more on a combination of the two areas, but more discussions would be better to motivate this work.\n- In terms of novelty, I do not see any new insights besides applying interpolation on feature space to generate more realistic samples. Even for generating more realistic samples, there is no experiment to justify this other than a toy exmaple.\n\nTechnical:\n- In Figure 2, it seems blue + points are generated by interpolation. I wonder what does the gray dash line connecting interpolated samples and real samples mean? Looks like it illustrates the manifold only, but not to indicate any pairwise relationship is used?\n- In the paper, only simple FC autoencoder is used. However, the design resembles variational autoencoder (and its variants) that samples from a latent space to generate realistic samples. In this sense, VAE is also directly applicable (or with minor modifications). Why it is not included?\n- What is the physical meaning of the regularization loss *T*?  Seems it does not help in most cases, so what is the reason to add it?\n\nExperiments:\n- Although 35 datasets are evaluated, most of them only have a few thousands samples, while only 4 have 100k+ samples. With such a small amount of data, it is hard to conclude that the proposed method would work well on larger, more complicated data. \n- From Table 1 and 2, I get the feeling that the datasets used in this paper are too simple to really showcase the capacity of deeper models or more sophisticated algorithms. On such simple datasets, larger model can easily overfit and for GAN-based approaches, they do need a fairly larger amount of data to train. Thus, I do not think the comparison is fully fair. \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "Summary\n\nThis paper tackles the imbalanced binary classification problem where the training examples from positive and negative classes are not balanced. The authors extend the SMOTE algorithm to generate new samples for the minority class. The key idea is to learn an autoencoder that maps the input to a latent space and generate new samples by interpolating latent codes. \n\nCons \n-I find it hard to understand the technique details of this paper because of its poor writing. \n\n--The approach figure shown in Figure 2 is quite confusing. The notations in the Figure do not match the notations in the main paper. The SMOTE is represented with a cylinder and I cannot get what it means. \n--In Section 3.1, D is used to denote a dataset as well as discrete features. \n--The loss function $l_A$ and $l$ are never defined. It is unclear which classifier is used. \n--Equation 2 is never explained either. \n--In equation 5, $x_i^c$ and $x_i^D$ are not defined. \n\n-In the intro, the paper claims that a previous method \"generates synthetic samples that are continuous rather than discrete and thus not realistic \". However, if I understand correctly, the proposed approach also generates samples in a continuous latent space. How does the proposed approach improves over the previous methods.\n\n-The evaluation is not convincing. It seems that the authors fail to follow some existing benchmark and select 35 datasets on their own. I find it hard to judge if its results are significant. In addition, the authors only report the average ranking of each method and fail to report the exact AUC or F1 score, which makes me hard to see how significant the proposed approach improve the SOTA. \n\n\nJustification\nOverall, I think this paper is not well written and I do not get many technical details. The evaluation is not convincing either. Therefore, I would recommand a rejection.\n \n \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Mostly well written, though issues with evaluation - and incremental novelty",
            "review": "The authors propose to use AE to do SMOTE in the latent space, in order to help deal with class imbalanced datasets including a mix of discrete and numerical features. The idea of using AEs in this context is not novel, as the authors identify, but their approach differs from prior attempts mainly by training AEs on both the minority and the majority class, thus allowing for more depth in the model. This is then evaluated on a fairly large benchmark of datasets, and compared to multiple baselines. The paper seems well written, apart from some added bits that could have been removed to make space for additional results and discussion (see comments below).\n\nThe small dataset filtering seems arbitrary. Yes, 1000 is a round number, but other than that there is no particular justification behind it. The paper would benefit from experiments demonstrating the performance of the method with respect to sample size - taking a larger dataset and randomly subsampling in a stratified way, showing how much data AE needs to train - and also, whether layer size and depth ought to be fit separately to each / how it changes with size.\n\nThe authors compare their proposed approach to a series of baselines ; yet, what seems to be missing from the list are the baselines representing prior work at using SMOTE on a lower-dimensional feature spaced, arrived at via some simpler mapping ; for those that are not applicable to discrete/categorical variables the authors should explicitly state there (page 6) why they were not included (so that the reader doesn’t have to go back and guess)\n\nThe ablation study along with the discussion on page 7 could be shortened, as it feels like the added options do not add much to the paper SAE, as well as +T seem not to have worked out, based on the average rank in those comparisons - the paper would read better (IMHO) with dropping them from the discussion altogether. They simply distract from otherwise interesting results being presented. Additionally, the choice for the top ranked row there (AE-Poly+T vs AE-Poly) is strange, as the average rank appears to be equal, while AE-Poly has a better average rank in 3 out of 4 measures used. This part of the paper reads like the authors felt that the approach by itself wasn’t novel enough compared to prior art and were trying to add complexity ; which doesn’t seem to have worked out.\n\nI wonder whether it makes sense to be averaging the ROC-AUC with the other metric ranks equally - after all, PR AUC is a much better fit for class imbalanced data, I find it a bit odd that both should get equal vote in evaluation in method ranking.\n\nWhat the main evaluation results are lacking, making it hard to interpret the presented improvements, is a correspondence between method performance and both data size, dimensionality, as well as imbalance ratio. It may well be that some methods are systematically better on datasets of a give property, at which point the takeaway would change as the method superiority would not be universal, rather special cased. Similarly, corresponding to the number of discrete features (as that is the added value of this work). These should be simple to compute and present, and yet these ‘sliced’ aggregate summaries (or plots) are missing - so, the reader needs to go manually through the detailed result tables and ‘guess’.\n\nThe use of bold font in the detailed result tables is confusing - are these just meant to be top numbers? Not statistical significance? In addition, there are some datasets where one of the baselines has equal performance as one of the proposed models, yet only the proposed model is in bold, obscuring equal performance. This is highly misleading. - for instance, letters, ROC AUC.\n\nThis brings me to my next question - how was rank computed for the average rank computations? Many of these numbers are -really- close, so when you look at the ordering of methods, it’s partially arbitrary - many of the ‘neighboring’ methods in the ranking, for many of the datasets will actually have no statistically significant difference between them, which means that there they should NOT be ranked differently. This doesn’t seem like appropriate use of statistics. This makes the presentation of the main results - flawed.\n\nIn addition, giving just the average rank without also giving the number of ‘wins’ (only counting statistically significant ones), even under the assumption of an acceptable ranking (see comment above), is insufficient. In principle, a method could win the average rank, without ever ‘winning’ over all other methods on a single dataset. The average rank metric is insufficient.\n\nLooking at the results in detail, it seems that in many (maybe the majority?) of datasets there is unlikely to be statistically significant difference between even the basic SMOTE and the proposed AE-SMOTE and AE-Poly, in F1-score.\n\nIn conclusion, the presentation of the main results is highly misleading. Most ‘differences’ aren’t likely to be real, when a proper statistical test is applied, and most differences are fairly small in magnitude, compared to the baselines - at least when it comes to the F1 score and the two AUC-s. This also raises questions when it comes to training / tuning of other baselines, as even very minor changes would alter the perceived results quite a bit. G-score seems to be the metric in which some improvement can be seen, but then - is it more important than F1 and PR AUC? If the improvements are mostly there, what is their nature? Additional analysis is needed. Also, it would be prudent to include the average performance across different measures in the main paper as well - or the median alternatively.\n\nNit: Page 5, typo: deocder -> decoder",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}