{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper augments pre-trained language models by introducing “adapter”, where each adapter is another language model pre-trained for a specific knowledge source (e.g., Wikidata) and an objective (e.g., relation classification). The representation from each adapter is concatenated to the representation from the generic LM. Specifically, they introduce two adaptors, “factual” (mostly derived from Wikipedia), and “linguistic” (from dependency parser), and the experiment shows modest improvements over various benchmarks.\n\nThis is a borderline paper, as both methods and experiments are reasonable yet not very novel or strong. The clarity of the paper can be improved (as pointed by R1 and R4), without any mathematical notations, model details are to be interpolated from figures. The novelty is limited and experimental rigor can be improved (i.e., for many settings, gains are fairly small and no variance reported). "
    },
    "Reviews": [
        {
            "title": "A good paper in general",
            "review": "The paper proposes a new approach to inject knowledge into pre-trained language representation models (PLMs). Instead of tuning the original PLM parameters, the paper plugs in new adapters for knowledge injection to avoid catastrophic forgetting. \n\nPros:\n* Injecting knowledge into PLMs is an advanced topic. The authors focus on the catastrophic forgetting problem during knowledge injection. \n* Evaluation is solid. The authors evaluate their model on three downstream tasks and show that the adapters improve the performance.\n* The paper is well written and can be easily understood. \n\nCons:\n* The approach is simple but achieves good performance over a variety of tasks. I appreciate that the authors conduct the knowledge probing experiment but its P@1 is quite low and worse than BERT. Some more explanations are expected.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work but limited by analysis and applicability",
            "review": "Summary:\nThe paper proposes a novel approach of incorporating different types of world knowledge sources contained in texts such as facts or linguistic syntax. To do this, they introduce additional transformers layers between the layers of a pre-trained language model such as Roberta and term this model as \"K-Adapters\", where the K stands for K different streams of knowledge.\n\nPros: \n- Incorporating different sources of information into a pre-trained model such as Roberta is an interesting idea. \n- The proposed approach is simple and interesting and it scales to many different types of information as the different adapters can be trained in parallel with the weights of the pre-trained LM being fixed.\n- Performance gains on different classification tasks such as entity tying, question-answering, and relation classification highlights the utility of the approach.\n\nCons:\n- Sec 1: Introduction: In the introduction, there are multiple mentions of the phrase “rich knowledge” but it is unclear what do the authors mean by that in the context of pre-trained language models. Some recent works such as “https://arxiv.org/abs/2002.08910, https://arxiv.org/abs/1909.01066 ” suggest that pretrained language models do indeed contain a lot of world factual knowledge. Hence, the statement in the paper that pertained LMs lack world knowledge is contradicting these works.\n- There is also a frequent mention of catastrophic forgetting of knowledge during the finetuning step. I tend to disagree that this is necessarily bad for a pretrained model, because it has been shown that finetuning pre-trained LMs perform well in open-domain question answering where some degree of world knowledge is needed. \n- Furthermore, producing entangled representations may not necessarily be a negative thing, if multi-task learning approaches are able to show an increase in performance due to knowledge injection.\n- In Table 1, dependency parser doesn't really fall under the same class of knowledge sources such as Wordnet or Wikidata. A dependency parser may be able to provide some sort of syntactic structure of the underlying text. Moreover, such syntactic information is not always generalizable to different domains and thus has the limitation of not being accurate enough.\n- The Introduction section is not well-motivated and does not present convincing arguments as to why external knowledge infusion is really required in some tasks. It just states that knowledge infusion using k-adapter model outperforms Roberta models in different tasks.\n- In Section 3.1, not enough space has been allocated to explain the Adapter model in detail. If the authors had used mathematical notation or equations for explanation, then it would have been much more clear. \n- In Section 4, it is mentioned that they select three downstream tasks for evaluating their models. However, the paper doesn't provide justifications as to why these tasks were selected, how can these tasks highlight the importance of k-adapter model, etc. \n- In the results table 2 and table 4, as the performance improvements are somewhat marginal, it is important to know if these improvements are statistically significant or not. The paper doesn't report if the results are from single run or the mean of multiple runs.\n- I have concerns about data leakage during the pre-training step. As the factual adapter makes use of supervised relation classification dataset (T-REx), I feel that there might be some overlap between entity typing and relation classification datasets used for evaluating the performance of the model. The authors should present an analysis as to what degree of overlap if any is present during the pre-training and evaluation tasks.\n- The paper lacks a detailed analysis section that could explain as to which test examples are getting correctly classified when using k-adapter model in tasks like relation classification, entity typing compared to other baseline approaches such as Roberta, Roberta + multitask. Currently, the paper pays just too much emphasis on raw numbers or performance improvements in various tasks.\n- The results of the probing experiments suggests that BERT-large model vastly outperforms k-adapter model on Google-Re and T-REx datasets probing datasets. This raises an important question over the validity of the results in different downstream tasks. For a fair comparison with baselines, the authors should compare the performance of the k-adapter model with BERT-large + multitask across different tasks.\n- In almost all of the experiments, the authors use Roberta as the underlying pre-trained language model. For demonstrating generalization to different pre-trained LMs, the paper should also evaluate when k-adapter model is trained when BERT-large or T5-large are used as underlying models in place of Roberta.\n\nGrammar errors: \n- page 1, 3rd line from bottom: remains -> retains\n- section 3.3, information that concerned -> information that is concerned\n- section 3.4, father index is commonly referred to as head index of a word.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Modular knowledge injection",
            "review": "#### Summary\n\nThis submission proposes a general method (K-Adapter) for injecting knowledge (either factual or linguistic) into pre-trained language models.  The key architectural property of the approach is that K-Adapters are isolated from one another, allowing the use of multiple adapters without interference. These K-Adapter modules take hidden layer _inputs_ from the main pre-trained model (eg, BERT), and are pre-trained on their knowledge outputs before a fine-tuning phase where they feed into a joint downstream task-specific model along with the pre-trained model outputs.\n\n#### Strong and weak points\n\nThe set of baselines seem strong, and the experimental results consistently show that using _either_ factual or linguistic knowledge K-Adapters improves, while using _both_ yields the best results.\n\nThe LAMA probing experiment is a nice sanity or validation test that the knowledge injection is achieving the desired effect. Being able to \"hard-code\" knowledge into the model in this way could be useful in a variety of applications. It is overselling it a bit to say the model captures \"richer\" commonsense knowledge, however.\n\nThe basic architectural idea is well-motivated and simple, in a good way.\n\nThe supplemental materials mostly provide additional reproducibility details on architectures, hardware used, learning rates, etc.\n\n#### Recommendation (accept or reject) with one or two key reasons for this choice.\n\nI recommend to accept. The proposed approach yields strong quantitative performance against solid and relevant baselines, and the LAMA experiments give some support to the hypothesis that it is doing so by capturing knowledge as intended. The general design pattern could spur further innovations in modular network designs or knowledge capture strategies as well.\n\n#### Questions to clarify / additional evidence required\n\n\"BERT-MK inegrates fact triples from the knowledge graph.\" - how? I can follow the cite but this sentence provides little information.\n\n\"inject different types of knowledge independently\" - is it correct to say then, that, by design, there can be no _beneficial_ interactions or synergies among different types of knowledge? Alternatively, in the fine-tuning phase, could different adapters interact or affect each other via the downstream coupling in the task-specific layers? Is this observed in practice?\n\nHow should the reader think about the relative magnitude of the presented improvements? At one point I see \"K-ADAPTER (F+L) makes significant improvement of ...\" but I believe \"significance\" is only meant coloquially here. \n\nSection 3.1: how was this structure chosen, what was the motivation or intuition here?\n\nWhat limits, if any, do you foresee with the use of separate parallel \"knowledge modules\" like this? Could we use 10, 100, 1000 K-Adapters?\n\n#### Additional feedback to improve\n\nIt would be helpful to cite Ling and Weld 2012 (or similar) for the definition of \"loose\" micro/macro F1, or briefly explain it inline in the evaluation setup.  Likewise for the \"catastrophic forgetting\" phenomenon affecting other knowledge injection attempts - is there some previous work explicitly demonstrating this problem when using multiple knowledge sources? If not, it would have been interesting to have an experiment of this sort in this work.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "As fine tuning a pre-trained model for specific tasks updates the original weights of that model and can lead to catastrophic forgetting, the authors propose to keep the pre-trained model's weights fixed and inject knowledge via separate adapter network that are pre-trained independently for specific tasks using internal representations of the pre-trained model.  The work learns two such adapters on top of RoBERTa and shows improved model performances on 3 different tasks using them.",
            "review": "##########################################################################\nReasons for score: \n\nThe authors propose a plug-in based adapter approach to allow for task specific parameter settings without updating the original pre-trained model which prevents the potential for catastrophic forgetting while also removing the need for separate models for separate tasks.  The work seems to build off Houlsby 19  as briefly cited, but its in plug-in nature seems easier to adopt for multiple tasks.  There is however not direct comparison with it or Cooper et al 19 ( https://arxiv.org/pdf/1902.02671.pdf ) which makes it difficult to assess.  The way in which the adaptors were pretrained was a little unclear to me.  The experiments are extensive and well done.\n\n \n##########################################################################\nPros:\n1) The number of experiments run ( 3 tasks on 6 datasets total ) are extensive and shows the K-adaptor approach can benefit from the factual adaptor in particular in giving better performance over RoBERTa ( with or without multi-task learning ).\n\n2) The proposed adaptor seems concise and easily expanded to incorporate other knowledge sources ( though there are few details which could help clarify things see #2 in next section )\n\n3) The probing task using LAMA to show how much factual knowledge has been memorized by the K Adaptor ( RoBERTA + facAdapter ) was well done and its discussion was very interesting. \n \n##########################################################################\nCons: \n1)  The proposed adapter solution is somewhat similar in nature to that of Houlsby 19  ( and to a lesser extent Cooper 19 ( https://arxiv.org/pdf/1902.02671.pdf ) ) and it feels like an omission to not discuss Houlsby 19 and make experimental comparisons against it discussing pros/cons more thoroughly especially since in the extensive experiments done in this work it is shown the linguistic adapter usually only adds a tenth of a percentage point when using RoBERTa with a single Factual adapter.  In this single adapter case then its not immediately evident how these models would differ and what the advantage is.   Both Houlsby and Cooper are evaluated on the GLUE benchmark and provide code.   \n\n2) I was a little confused as to how the adapters were specifically pre-trained and it might be a question of Figure 1b, but also sections 3.3 and 3.4 could have been expanded to clarify it a bit.  It is my understanding that when pre-training the facAdapter on the relation classification task for instance in Section 3.3, for a given example in T-REx-rc,  two entities and context are passed into RoBERTA whose weights remain fixed while those of the KIA units of the facAdapter are updated and the final hidden representations of RoBERTA and the facAdapter are concatenated to form an input representation of the entities given their context and this used for the actual task.  Is my understanding correct?  If so I'm confused as to how the subsequent pooling and concatenation actions are done.   Clarifying this process for 3.3 and 3.4 would be beneficial for clarity purposes and its not discussed in the supplemental materials either.\n\n3) Your RoBERTa Large baseline already beats most of what you are comparing against which is fine as your adapters give gains ( again particularly the facAdapter), but it also would have been interesting to see what sort of gains would have been achieved using a different less powerful model as the base RoBERTa small or just plain BERT and additionally some sort of ablation testing or explanation on the choices made for the adapter networks themselves ( ie, N=2 Transformers etc , hidden layer size, etc ) though its possible this could be left for future work.  For clarity in Figure 2 where you show N x Transformer Layer ( and N=2), I'm assuming the first Transformer Layer feeds directly into the second Transformer Layer which then feeds into the Up Projection layer correct?  If so it might be better just to show two transformer layers like that instead and additionally, naming the projection layers Up and Down Projection Layer respectively.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above \n \n#########################################################################\nSmall typos:\nIn Abstract:  we propose K-ADAPTER, which remains the original parameters  .... \"remains\"  ->  \"keeps\" or \"retains\"\n\nIn Introduction: they fail to continual learning   .....  \"fail at continual learning\"\n                              It remains the original representation    ....   \"remains\"  ->  \"leaves\"\n                              (pg2) while remaining the original parameters of RoBERTa frozen...  \"remaining\"  ->   \"keeping\"\n\nSection 3:     It remains the original representation    ....   \"remains\"  ->  \"keeps\"\n              3.1:  Different from Houlsby et al. (2019) add adapter layers  ->  \"In contrast to Houlsby et al. (2019) who add adapter layers\"\n              3.3:  all relations having lees than .... \"lees\" -> \"less\"\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}