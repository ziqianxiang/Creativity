{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper the flip-side of an adversarial \"attack\" in that data may be perturbed to make it look like a model was performing well rather than the standard notion of adversarial attacks. The reviewers found this notion interesting and potentially worthy of investigation. However as it stands, the proposed applications and methods do not seem developed well enough as would be expected at a conference like this."
    },
    "Reviews": [
        {
            "title": "I am waiting for the author's response.",
            "review": "Summary\nThis paper presents a new kind of adversarial attacks, named hypocritical attack. It is a reverse version of the original adversarial attack. It tricks a model into classifying data correctly with a perturbation. This can be a problem since it can make people satisfy the model performance, but the model is not robust on the real test dataset. The authors review the adversarial attack and define the new hypocritical examples and risk. The authors also show the simple results why hypocritical attach is a critical issue by a Naive model that is initialized randomly. It shows high performance on the hypocritical examples but low on the clean test data. They also investigate the algorithms that improve model robustness, THRM, and TRADES. Experiments show a trade-off between original classification loss and hypocritical risks, and THRM is a tight upper bound against the TRADES.\n\nThe main strength of this paper is suggesting a sound and straightforward attack named hypocritical attack. I think almost machine learning (ML) researchers cheer with joy when they see the high-performance results on their model. But sometimes, we need to think about our mistakes or test data characteristics (i.e., easy test data). This paper suggests another consideration to the ML community. The authors also investigate minimization algorithms on the attack risk and show future directions such as transferability.\n\nI have several questions about this paper that I want to listen to the author's response.\n\n\nQuestions and Comments\nIs there any trade-off between adversarial risk and hypocritical risk like precision-recall trade-off?\nWhat if we apply the technique to make hypocritical examples in the training data? We can measure the training accuracy then we can make the examples from the training data. Do these examples boost the classifier?\nIt would be better to describe the toy example since it is hard to understand the text's experiment in this paper. Even this experiment is borrowed from Zhang et al. 2019, readers want to understand the experiment with this paper only.\nTypo: with large hypocritical hypocritical risk, while the optimal on page 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation to reject",
            "review": "#### Problem Statement\n\nThe research on adversarial examples typically focuses on labels which are correct but non-robust: eg. an image of a bus that is classified correctly but with a few changes to pixels can be misclassified. This paper suggests that we should also pay attention to robustness in misclassification. If a classifier is not robust, even on images where it makes a mistake, changing a few pixels might fool the model into giving it the correct label, but presumably  for the wrong reasons. \n\nThe authors envision a scenario where a \"false friend\" examines the output of a network, and attacks the mislabeled examples to make it look like they are labeled correctly. In such a scenario, not much training happens even though the network is far from optimal on noiseless examples. They suggest certain mitigations against such attacks by favoring networks whose labels on mistakes are robust. \n\nThey propose a new objective function called THM and compare it to TRADES. \n\n#### Pros\n\nThe paper makes a good point that (non)robustness for classifications sometimes goes hand in hand with (non)robustness for misclassifications.  \n\n#### Cons\n\n* The attack scenario which the paper envisions does not strike me as very realistic. \n\n* From what I can tell, TRADES would aim for robustness on all examples, whether rightly or wrongly misclassified, whereas THRM seems to focus only on robustness for misclassified instances.    It appears that adversarial risk on correctly classified examples is not being weighted. This might be a better choice for the task of avoiding the type of attacks proposed in this paper,  but overall I am not convinced it is a better objective than THRM, which aims for robustness everywhere. \n\n* One could imagine that in the real world, when one is not concerned about false friend attacks, one would like robustness for the correct labels, but perhaps not so much for the wrong labels, as this could be a sign that the network is unsure of its prediction, and could change its output, given more data or training. \n\n#### Summary of recommendation\n\nThe observation that non-robustness applies to correct and wrong labels is interesting. But I didn't find the attack scenario built on top of this observation and the proposed mitigations compelling. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A technique to alter test images so as to mislead classifiers into self-complacency",
            "review": "1. The premise of the paper is that the adversary can perturb the *test* set so that the model is shown to perform better that it really is capable of. And in Section 7 (Conclusion) the paper claims that it exposes this new risk. However, remember that this risk is already mitigated in practice by keeping the test data *independent* of the model/classifier (e.g., see Kaggle competitions where the test set is hidden). Therefore, the perceived risk is not even present. In the context that the technique has been introduced, it seems like the [malicious] actor would only be fooling him/her self rather than fooling the model/classifier.\n\nIt is hard to come up with an application for manipulating the *test* data in the manner proposed and I am curious to hear what the authors feel.\n\n2. What if instead of manipulating the images to conform to the classifier's predictions, we manipulate the labels, i.e., relabel the test data as per the classifier's predictions? Would that result in the same final objective being achieved?\n\n3. Equations 1 and 2: The technique is based on generating adversarial training examples and does not make any fundamental technical contributions.\n\n4. Abstract: \"Extensive experiments verify the theoretical results and the effectiveness of our proposed methods.\" -- Just a couple of standard simple image datasets (MNIST, CIFAR-10) have been employed. Harder and more number of datasets should be employed in order to justify the statement in the Abstract.\n\n5. Section 6 Discussion: \"What we can do at present is using a relatively more robust model as a surrogate of the true friend to improve the robustness of a weak model.\" -- It is not clear how that would help. The attacker might mislead the 'relatively more robust model' as well. Moreover, why not use the 'relatively more robust model' instead of the weaker model?\n\nOne simple mitigation strategy could be to digitally sign the test set to check for tampering. Why has that not been suggested?",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The main idea of the paper is decent, but there are many holes in the paper",
            "review": "This paper considers attacks that flip the label for those data points that are incorrectly classified. It proposes a risk metric to capture this and uses prior attacks for this type of attack.\n\nPositive points:\nThe main idea in the paper is, at first look, interesting but then on more thinking I have doubts (see below):\n\nNegative points:\n- It is not very surprising that mis-classified data points can be perturbed to be properly classified using exactly the same kind of attack as prior work (not sure why authors call the attack conceptually different).\n- I think the work is missing an important comparison to R_bdy (and R_rob) in Zhang et al. Is that in Propositon 2? Why call it R_rev?\n- Also, why is equation (4) a natural risk to minimize? In Zhang et al., the term to minimize is nicely motivated by bounding R_rob - R^*_nat, which naturally leads to the tradeoff. I do not see how the tradeoff arises here.\n- Also, the fact that TRADES would help here is obvious, isn't it? As TRADES handles two sided errors, it naturally also takes care of the one side of the error the authors care about. And that naturally raises the question that will only caring about one side of the error (in Eq 4) increase the other side (R_adv)? This R_adv is not shown in experiments. Also, shouldn't the experiment be done with larger and more variety of datasets - particularly, when for more complex datasets TRADES is doing reasonably well - so might be better off using TRADES for complex and real image datasets.\n- A lot of the technically machinery is not very novel with same attack as before.\n\nMinor points:\nAs a writing style, false friends is a good analogy but please do not use it throughout the paper in place of technical terms such as this line \"Better transferability is beneficial for black-box attacks but is not always desired by false friends\".\n\n----------------Post rebuttal-----------\nThe issues I raised still persist in my mind. Also, other reviewers have similar issues and also more issues other than what I point out, which seem valid issues to me. I will keep my score as is.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}