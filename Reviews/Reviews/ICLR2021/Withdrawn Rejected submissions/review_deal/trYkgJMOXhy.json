{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper addresses counterfactual fairness learning using generative approach. While acknowledging the importance and potential usefulness of generative approach, the reviewers and AC raised several important concerns that place this paper below the acceptance bar: \n\n(1) low degree of novelty – see multiple concerns and suggestions by R2, R3, R4;\n\n(2) the model is not justified by a causal mechanism (R4), and it remains unclear under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples (R2); \n\n (3) lack of technical rigor when presenting the model – see R4’s request to relate to the DAG models, see R1 multiple questions regarding the reinforced data sampler; \n\n(4) lack of empirical evidence (R3) and evaluation details, e.g. on cross validation and more recent methods (see R4’s recommendations); \n\n (5) related work is not discussed in sufficient details – see R4’s elaborate comment.\n\nAmong these, (4,5) did not have a substantial impact on the decision but would be helpful to address in a subsequent revision. However, (1), (2) and (3) make it very difficult to assess the benefits of the proposed approach and were viewed by AC as critical issues.\nIn the rebuttal it is stated that ‘Our counterfactual examples are generated using a powerful generator rather than a fixed synthesizer in Kusner et al. (2017)’ – more rigorous comparison has to be provided to support such statement. AC would urge the authors to contrast and compare their synthetic counterfactual examples with Kusner et al on the datasets where causal graph has been built. [Razieh Nabi and Ilya Shpitser. Fair inference on outcomes., AAAI2018], Figure 2 postulates causal graphs for the Compas and Adult Income datasets evaluated in this paper.\n\nA general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews and encouragements are useful for revising the paper.\n"
    },
    "Reviews": [
        {
            "title": "Interesting and novel proposal, surprised part of it works and seeking more details",
            "review": "The paper proposes to pair a GAN based model for generating counterfactual samples given protected attribute labels and a reinforced data sampler for choosing whether to let a model train on generated data or original data.   The generative model in of itself is interesting, combining a gan component, and VAE component for analyzing an input image and a mutual information penalty for the VAE hidden vector and protected attribute.  The key to the method seems to be a reinforced data sampler, which picks, when to use a counterfactual sample versus the original sample. Given that the core of what is making this work is the data sampler, I wish there were more details. \n\nHow is the policy parameterized beyond the inputs? \nHow many episodes run out to tune the sampler?\nThe paper says the reward was the fairness measure on the held out set. Do you mean a different held out set or the same final evaluation set?\nI am confused by Figure 6. Is the y-axis the % of samples that left unmodified? If so, then seems like the sampler is essentially saying always use a counterfactual? So I would expect results very similar to all-fake baseline, but in terms of EO, the results are very different. If only a sample number of samples are different than that baseline, how is this possible?\n\nPositives:\n+ the paper proposes an interesting way to generate counterfactual samples\n+ the results seem promising\n\nNegatives:\n- mostly I am left confused how the reinforced data selector is making the method work. Answering some of the above questions would help. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of GFT",
            "review": "This paper combines counterfactual modeling with adversarial training for fair machine learning tasks. For a given fairness metric chosen from a variety of canonical examples, the method ensures fairness by augmenting the data with counterfactual examples during training. The approach has potential, which is best demonstrated on examples where the counterfactual data generation is interesting, like the CelebA data.\n\nI believe the main weakness of the paper is a low degree of novelty. For example, the idea of training with counterfactual data is present already in the Kusner et al. (2017) reference. The current paper expands the uses of that technique and combines it with an adversarial training architecture. So the strength of this work depends on the suitability of the counterfactual model and training architecture.\n\nThe other data examples, Adult and COMPAS, do not contribute much additional value, particularly in light of my previous point. The paper could be improved by replacing these with one or more examples that better leverage the strength of adversarial training. For ICLR it might be best if these examples use types of data where representations can be useful, like text for example. \n\nThe Section 2.2 comments on in/pre/post-processing were confusing. The method is \"related\" to all of them? Does it not fall into any of the categories?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction, but more needs to  be done",
            "review": "The paper proposes a teacher-student framework to ensure fairness by letting the teacher choose examples for the student from either the training data or from a counterfactual distribution. The main contributions are a counterfactual generative model and an algorithm for learning the teacher policy.\n\nStrengths:\n========\n1. The idea seems interesting and the proposed teacher-student framework is novel in the area of fair learning.\n\n2. The authors have done a good job of modeling various aspects of their complex approach using neural networks.\n\n3. The fact that authors were able to make the complex optimization work is itself a good thing since the objective has a lot of moving parts.\n\n4. The presented evaluations also show some promise.\n\nThings to consider improving:\n=========================\n1. My basic question is regarding the motivation for such a framework. Why is this approach important? Is there any fundamentally new insight that can be obtained with a GFT framework that cannot be obtained using existing fair learning approaches.\n\n2. The argument that causal methods cannot benefit commonly used fairness metrics like demographic parity (DP) or equalized odds (EO) should be elaborated. Isn't it possible to create Structural causal models that subsume conditional independences like DP and EO?\n\n3. There are a number of issues with references:\n- There are a number of additional pre-, in-, and post-processing methods that can be cited. Looking into some recent fairness papers may give the authors an idea of the works.\n- Agarwal et al. 2018 is considered an in-processing approach and not post-processing or pre-processing since it imposes fairness criteria while training a fair model.\n- Demographic parity is a really old concept and has been discussed in many papers before Madras et al. 2018.\n- The authors should also consider citing and perhaps comparing with https://homes.cs.washington.edu/~suciu/sigmod-2019-fairness.pdf if it makes sense, since it also does \"database repair\" or pre-processing.\n- Barring some exceptions, many early works in fairness are not cited. For example https://ieeexplore.ieee.org/document/6175897 is a classic work that discusses a lot of concepts in fairness that we use today (they call it discrimination). I recommend the authors to do a thorough literature survey and include at least the important works.\n\n5. Many crucial algorithmic details are missing. While  the complex optimization is important to cover all the aspects of the framework for data generation, it should also be motivated and explained better. \n- In Sec. 3.2.1, how valid is it to assume U to be independent of A? Technically A is a part of the data that is generated so it may be reasonable to surmise that it should depend on the latent. In general the authors should provide a DAG which encodes their assumptions and justify them.\n- How would this architecture be modified for DP since the authors discuss both DP and EO in the beginning?\n- What is L_att trying to optimize?\n- What does it mean to have attribute labels as auxiliary tasks for D (in L_cls)? What are attribute labels?\n- It may make sense to summarize in a few sentences what each term in the objective does. If space  is less, the authors can move some contents to the appendix.\n- In Sec. 3.3, could the authors provide some insights on why such a training works?\n- Please also discuss what REINCFORCE is, and provide reference/more details.\n\n6. Generally in experiments, cross-validated results are needed. It is also crucial to provide sufficient pre-processing and hyper-parameter details to help reproduction of results.\n\n7. There are also a number of recent post-processing methods that can be compared with besides Hardt et al, 2016 (see http://auai.org/uai2019/proceedings/papers/315.pdf, http://proceedings.mlr.press/v108/wei20a.html)\n\n8. More details needed on how Agarwal et al., 2018 was run for example. In https://arxiv.org/pdf/1906.00066.pdf, pg. 30  Agarwal seems to be more competitive than what is shown here. Look at adult-gender-LR-EO and  adult-gender-GBM-EO, Agarwal et al. 2018 (named \"red\"\")  at EO=0.04 has higher  accuracies than shown here. Similar comments apply to COMPAS.\n\n9.There are  also a few other pre-processing approaches that use GANs. \nhttps://arxiv.org/pdf/1805.11202.pdf\nhttps://arxiv.org/pdf/1805.09910.pdf\n\n10. The authors must identify the caveats  of training a model on CelebA which has a \"western\" and \"celebrity\" bias. Models trained there may not transfer to other general settings.\n\n11. In page 8, the analysis of teacher model needs more details. How can we say original images dominate if only 7% image is chosen at max at any iteration? Why is the teacher behavior of choosing real samples in the beginning and synthesized samples later justified?\n\nIn summary, the paper has identified an interesting direction but this needs to be taken forward a bit more.\n\nPost-rebuttal\n===========\nThanks to the  authors for  their detailed response to  my  questions. Some of the answers are indeed satisfactory, but some questions remain - such as extensive comparisons to other methods (probably using more datasets), how  the method would  behave (practically)  with  a different fairness measure like DP, and more  carefully situating the method in  the  fairness literature. I encourage the authors  to keep pursuing this interesting  direction.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work",
            "review": "This paper describes a pre-processing method to reduce certain statistical disparities in the classifier obtained from the training data. The proposed approach involves learning a latent probability model that simulates the training data. The authors then manipulate the learned model to generate \"counterfactual\" samples that belong to the membership of underrepresented demography. A more \"fair\" classifier is trained on the manipulated data mixing with the \"counterfactual\" samples.\n\nThe proposed approach seems sensible. The experiment results support the claims that it reduces the statistical disparities (e.g., equalized odds) of the trained classifier. I like the simplicity of the proposed method while future work is needed to explicate the theoretical condition under which it is sufficient.\n\nAlso, I believe that the clarity of this paper could be improved. For instance, the definition of counterfactual fairness seems to be orthogonal to the \"counterfactual\" samples described in later sections. More specifically, the counterfactual fairness concerns with the potential outcome of prediction $\\hat Y_{a'}$ has the sensitive attribute $A$ been $a'$. On the other hand, the \"counterfactual\" samples is the potential outcomes of the feature $X_{a'}$ had $A = a'$. It does not seem that the concept of counterfactual fairness has be used in later sections. Therefore, I would suggest that the authors could remove the definition of counterfactual fairness and provide a formal introduction to structural causal models and interventions, e.g., see (Causality, Pearl, 2009). \n\n\n------------ Post Rebuttal ------------------\nI read other reviewers' comments and the authors' responses. I like the idea of applying the GAN framework to compute counterfactual distributions. However, I could also see why other reviewers are not particularly excited about it. The authors managed to apply the GAN approach to obtain counterfactual samples in some specific datasets. However, many questions regarding the proposed methods are left unanswered, e.g., under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples. With this being said, I think this paper could be most improved by further elaborating how it contributes to the existing causal inference literature, especially in computing counterfactual probabilities. Due to these reasons, I intend to keep my score but won't strongly champion for it.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}