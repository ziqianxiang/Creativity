{
    "Decision": "",
    "Reviews": [
        {
            "title": "The proposed model (in particular, the inference model) built based on VAE is not novel.",
            "review": "Strengths of the paper:\n1. The paper is well-written and easy to follow.\n2. Both simulated and real datasets are used for evaluation purposes in the experiments.\n\nWeaknesses of the paper:\n1. The proposed model is not novel. The proposed model is built based on VAE. The proposed inference model relies on a variational lower bound to approximate the objective, which is also similar to that in VAE.\n2. The paper aims at inferring two categories of entities, users and words, i.e., R and C. There are other techniques that build based on VAE and also aim at inferring two different categories of entities. E.g., the following paper:\na) Meng et al., Semi-supervised co-embedding attributed networks, in NeurIPS 2019.\nb) Dai et al., Deep coevolutionary network: embedding users and item features for commendation, in KDD 2017.\n3. As the inference of the model is built based on variational lower bound that is also used in many VAE variant algorithms, the authors are encouraged to make comparisons between the proposed model and those built based on VAE for recommendation or not, including a) and the following:\nc) Liang et al., Variational autoencoders for collaborative filtering, in arXiv, 2018.\nd) Li et al., Collaborative variational autoencoder for recommender systems, in KDD 2017.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Double submission?",
            "review": "This paper is basically a slightly extended version of the work \"Missing rating imputation based on product reviews via deep latent variable models\" that has been accepted in the Artemiss workshop of ICML 2020: https://artemiss-workshop.github.io/ \nThe acceptance decision has even appeared at OpenReview: https://openreview.net/forum?id=1kbizwzFNwb\n\nThis is basically the exact same paper (one new baseline is added to the comparison), it does not mention the Artemiss submission and does not discuss the differences. I suppose that this is either a mistake on the part of the authors (forgot to withdraw?) or a mistake on my part if the Artemiss workshop for some reason does not count as a publication. But since I have found no indication that it shouldn't, I believe that this is some kind of a mistaken double submission and the paper should not be accepted.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper presents a new generative method considering member-item engagement (e.g., ratings) and text-based meta-data (e.g., reviews) associated with the bipartite graph's edges. The paper can be improved via more guidance to potential users supported by experiments and further analysis of model.",
            "review": "The paper presents a new generative method considering member-item engagement (e.g., rating) and text-based metadata associated with the bipartite graph's edges. The parameters are estimated through variational inference. The final model can predict both engagement probability and the words related to it. The claim is that under the severe data sparsity conditions, the proposed method provides much superior predictive performance than models only considering engagement data. Here are some points:\n\n-The \"Deep\" notion in the proposed method seems to be a bit of an overstatement as my understanding the encode-decoder framework is fairly shallow and the impact of \"deep\"ness is not explored. It looks like the novelty mainly comes from the joint modeling of text and engagement data. \n\n-The paper provides several baselines to compare. Interpretability, effect of sparsity is nicely explored. However, some more basic principles are not explored well. For example, how much does the joint modeling help? How does the proposed model compare to disjointly learning topics from text and use it as covariates to predict engagement? Reviews are usually very noisy. It is possible that trying to predict words with the latent factors come with a cost of losing the predictive power of the responses/ratings (compared to a setup in which they are learned disjointly)\n\n-The paper leverages an important idea of latent variables generating multiple observables. This has been explored in the literature (as many cited already by the authors in the paper) in topic modeling (sLDA, RTM), matrix factorization, and completion. This is another useful example. Usually, such techniques can ben considered in two folds: \n\n1) Additional metadata from users or items come for an additional/external event (sometimes related but not directly associated). E.g. a social media user likes a post (user-item) and follows another person (user-user). Under data sparsity constraints, this additional metadata helps learn the latent factors better because a user's chance of having at least one of the observables is higher than having only one of them (e.g. a user might never like a post but might have been connected with a bunch of other users--which might expose information about \"like\" behavior).\n\n2) The metadata is directly associated with the event itself (e.g., a review about a rating). In this case, this metadata usually provides much more granular, rich, and insightful information about a discrete event, e.g., a like/share. This is more about the \"depth\" of the engagement. This paper falls into this second category. However, this aspect is not explored and discussed in the paper. It is not clear how sparsity helps (in the former example (e.g., user-item and user-user), it is more clear why sparsity helps). The paper lacks intuition and more analysis in this direction.\n ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach but weak experimental results",
            "review": "Summary of this paper:\n\nThis paper presents a Deep Latent Recommender System (deepLTRS) for predicting rating scores from user activities. DeepLTRS exploits user reviews to complement user preference and alleviate the sparsity issue of a user-item matrix. This paper also proposes a variational inference algorithm for performing mini-batch optimization. DeepLTRS was compared to several baseline methods with simulated and real datasets.\n\nPros:\n\nThe proposed method is novel to my knowledge. It's a fairly simple model but would be one of the effective approaches for the addressed problem. generating topic vectors via a neural network seems to be effective for modeling complex relationships between textual reviews and rating information. The proposed inference algorithm would be also a good contribution.\n\nCons:\n\nMy major concern is the experimental part. I think the presented results are somewhat weak to ensure the significance of the proposed method.\n\nFirst, the experimental setup for the simulated dataset seems quite arbitrary. Many parameters were set up in a predetermined way to generate the dataset. Compared existing methods could work better for other parameters and generative processes. It is also incomprehensible that TransNet was not included in the results of this simulated experiment.\n\nRegarding the experiment on the real dataset, it is not sufficient that the compared methods have been evaluated by only one dataset. Considering related studies, it should be validated on several real data sets; similar datasets published in the SNAP website could be candidates.\n\nComparisons with existing methods are not enough to support the effectiveness of the proposed method. Many existing methods have been proposed to deal with the same problem [1][2]. It would be better to compare them to the proposed method. Also, the existing methods used in the experiments were not explained in detail. How were the hyperparameters of the methods set?\n\nMinors:\n\n- deelLTRS -> deepLTRS on page 5?\n- On page 6, the simulation setup is not well explained and is somewhat confusing.\n\nReferences:\n\n[1] Neural Attentional Rating Regression with Review-level Explanations, WWW 2018.\n[2] Joint Deep Modeling of Users and Items Using Reviews for Recommendation, WSDM 2017.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}