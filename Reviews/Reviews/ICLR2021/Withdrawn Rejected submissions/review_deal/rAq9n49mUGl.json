{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official review AnonReviewer2",
            "review": "Summary:\nThis article describes how to use generative adversarial networks (GANs) based on the maximum mean discrepancy (MMD) to learn small stochastic corrections to physics models so as to minimize inference bias.  The authors propose a hybrid training procedure that encompasses both the MMD and the standard GAN objective functionals. The proposed method is first used to learn stochastic model corrections and eliminate inference bias on a toy problem wherein the true data distribution is known, and then applied to a mildly ill-posed inference problem in magnetic resonance imaging (MRI), showing improvement over an established inference method. Finally, the authors propose to train a conditional variational autoencoder (CVAE) network on the corrected MRI physics model to perform fast inference and make this approach practical in the case of 3D MRI images.\n\nMajor comments: \n- Overall, I find the paper is easy to follow and the experimental evaluation shows promising results. But my major concern is about the novelty of this work, given the fact that the basic methods, such as the MMD-GAN and CVAE, were already proposed in the literature.\n- The theoretical contribution to this conference appears to be limited. I would recommend resubmission to a workshop/conference in related areas.\n\nMinor comments:\n- The size of some figures appears too small, for example Fig. 2, which may hinder readability.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Generative process and problem setting are not clear",
            "review": "This work proposed leveraging GAN and MMD (maximum mean discrepancy) to improve Bayesian parameter estimation methods for quantifying properties of physical systems which cannot be observed directly.\n As far as I understand, this proposes to correct a physical model of the phenomenon to be studied with a learned noise to further match observed data from a real physical system. This refinement step makes sense because there is always a mismatch between physical model outputs and observed data. GAN + MMD is employed to train a neural net that produces augmented noises.\nThis paper tackles a real-world problem regarding an interesting application of machine learning and deep learning in real-world problems. However, in terms of modeling the novelty and contribution are limited because it is a simple combination of GAN and MMD. \nThe way the authors wrote the paper really makes it confusing. Although I tried my best to read the paper several times, it is still not very clear about the problem and the contribution of this paper. I suggest the authors should describe the problem formulation and the meaning of variables X,Y,  and \\theta in their specific problem setting. In addition,  the notions used in the section ” Generative adversarial networks and the maximum mean discrepancy” are really messy. For example, they use P_y to describe both a distribution (i.e., acting on the measurable set) and density function (i.e., a function from data space to R+) and also the generator G is a map from a point X to a distribution which is different from the original formulation of GAN. Certainly, in terms of modeling, we can do this, but it needs an explanation. \nBelow are some of my questions\n1.\tWhat are the meaning of variables X,Y, \\theta in your problem setting? Does \\theta contain parameters of a physical model of the phenomenon to be studied? Is X the output of this physical model? Is Y the observed data of the quantifying properties?\n2.\tThere is a mismatch between X and Y, hence we need to learn the correction amount g_delta(X), do not we?\n3.\tIs the Rician distribution Rice(nuy_i, eps_i) applicable to a reparameterization trick like Gaussian distribution? Otherwise, how can you backpropagate the signal from the loss function to update g_delta? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insufficient novelty, insufficient experimental support, and presentation open to improvement",
            "review": "The paper proposes performing parameter estimation using the training process of a vanilla GAN and MMD-GAN alternatingly. It then shows a few applications of this training algorithm on a synthetic setup and a 3D brain imaging application.\n\nThe proposed algorithm is a direct entanglement of vanilla GAN and Dziugaite's MMD-GAN by alternative updates. This is rather a straightforward engineering design that is part of the common sense, rather than a scientific finding from which the ML community can learn something new. \n\nWhen I look at the reported quantitative performance in Tables 1 and 2, I do not see a clear winner between GAN, MMD-GAN, and the proposed Hybrid GAN. All three methods work similarly. Hybrid GAN appears to give scores in between GAN and MMD-GAN, which is expected but also not useful for anything concrete.\n\nThe paper highlights the following sentence in the introduction section as the central research question: \"how can one improve a physics model for the purpose of parameter inference in a computationally inexpensive way?\" However, when I read the rest of the paper, I have hard time to find out what this paper proposes particularly to make GAN-based parameter inference more efficiently. Algorithm 1 is the existing and well-known GAN training procedure and it is as efficient as any GAN the ML community knows.\n\nThe paper misuses a few terms repetitively from the title downwards, which distracts the reader who is used to their proper meanings:\n\n i) The proposed method does not have any relation to Bayesian inference. It does not assume a prior on latent variables and does not perform posterior inference by combining an explicitly defined notion of a prior and a posterior. Instead, it performs maximum likelihood estimation in its standard meaning, which is actually the straight opposite of the Bayesian approach.\n\n ii) The word stochastic refers to the sub-family of probabilistic methods that build on a stochastic process, i.e. there should be a source of memoryless randomness whose variance grows and shrinks proportional to the time leap. For instance, Brownian motion, or Levy process. The proposed method only performs probabilistic modeling and does not explicitly handle any source of stochasticity.\n\nIn its current shape, this work does not contain sufficient novelty to qualify as a main-track paper. However, it may indeed be published initially as a workshop paper, and then be extended to the main track by providing thorough theoretical analysis on the convergence properties of the resulting hybrid solution or by showing groundbreaking performance on a much larger variety of applications. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "limited justification, limited novelty, and not that much of physics",
            "review": "Abstract:\nIn this paper, the authors suggest using parametric distribution as a generator and use the loss function of the MMD GAN to estimate the parameter of the model. The parametric distribution is originated from some physical model, but beyond that, there is no physics in this paper contrary to what the title states. In fact, the whole paper can be summarized as follow:\n\n$$\n\\max_{k} \\min_{\\phi}   MMD_k  ( G( \\phi( X ) ), Y )\n$$\n\nwhere $k$ is a kernel, $G$ is the generator parameterized by $\\phi(X)$, and the $X$ denotes observations and Y real samples from parametric distribution represented by $G$. \n\n- When the loss function is classical GAN (not MMD GAN), minimizing the loss is not the same as the Maximum Likelihood. I have not seen any reference proving such equivalence, so I don't understand why the authors referred to it as  MLE GAN.\n\n- GAN, in general, is useful when the exact likelihood is not known. However, in the examples presented in this paper (Rice model), the model is parametric, so not straightforward MLE?  \n\n- One of the main arguments of the paper is that their method results in less biased estimation of the parameters, but no empirical or theoretical proof is provided.\n\n- The experiments are not convincing, MSE of the signal is not necessarily a good metric in this case. Why not show how successful the model is in recovering the true model (in the simulated case)? This should be done by reporting r^2 of the scatter plot of the true vs estimated parameters and comparing it with straightforward MLE optimization. \n\n- The writing is unnecessarily confusing. Other than the first paragraph of the intro, the rest of the intro does not motivate the problem well. For example, it is not clear why authors chose to denote X as a function of parameters (X = F(\\theta)) rather than other way around. In fact, the actual model is the transforming of X to the parameters. \n\n- As is, the paper offers limited novelty and experimental validation.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}