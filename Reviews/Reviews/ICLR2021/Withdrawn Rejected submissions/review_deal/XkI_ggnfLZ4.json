{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining. Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network.\n\nThe pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. The rates can in fact be transferred to more poorly performing network configurations and improve performance.\n\nThe cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost-effect strategy to find these. At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model. \n\nThe stronger, forward-looking implication is, instead, the connection to layerwise pruning rates. Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (e.g., [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates.  Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (e.g., gradient flow, or capacity) that indicates the improved eventual performance.\n\nMy Recommendation is to Reject. The paper's core experiments are well-executed. However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component.  Once done, that will make for a very strong paper.\n\n[1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han. EECV, 2018\n\n"
    },
    "Reviews": [
        {
            "title": "Official blind review #3",
            "review": "##########################################################################\n\nSummary:\n\nIn the lottery ticket hypothesis's general framework, network pruning's two phases of training (before and after the network is actually pruned) use the same hyperparameters - in particular, the learning rate schedule is identical in both phases.  Using different learning rates for these two phases, and, unintuitively, preferring a learning rate in phase 1 (finding the mask) that results in a *worse* dense model, can result in a better final model (after training with the mask in place).  It is also shown that the layerwise pruning ratios may be the key to understanding this behavior: finding the proper LPR is best done with a small learning rate, but given an LPR, a large learning rate is better able to determine the specific mask.\n\n##########################################################################\n\nReasons for score: \n\nI rated this submission as marginally below the acceptance threshold mainly because I'm struggling to find the practical benefit of the findings.  On one hand, decoupling learning rates seems to work for the limited results contained in the submission, but this boils down to \"here's an extra hyperparameter we should be testing.\"  (This hyperparameter is also not new in all contexts, only in the LTH.)  On the other hand, these results pointed to the LPR as being crucial for a good pruned model.  Back to the first hand, though, what do we do with this information?  The submission doesn't offer any guidance about where to go next, except that perhaps other hyperparameters should be similarly decoupled, which isn't the most comforting thought for practitioners.  Finally, the claim that decoupling learning rates is brand new should be tempered somewhat (see below).\n\n##########################################################################\n\nPros:\n\n+ This paper is well-written and fairly easy to follow with a straightforward organization. \n+ Decoupling learning rates in the LTH framework seems novel and useful.\n+ Not argued by the authors, but I'll add/clarify: while decoupling learning rates might not be new for all pruning methods (traditionally pruning+fine-tuning, as opposed to rewinding), it *is* novel to use a lower learning rate during the mask finding phase, which may result in a worse dense model, from which a better sparse model is created.  (I don't think this has been validated empirically anywhere for prune+fine-tune approaches, so maybe it's best left as future work and the claim re-worded to not run afoul of past work that uses different learning rates.)\n+ The importance of a good LPR is clearly motivated, and the experiments seem sufficient to prove the causal relationship.\n\n##########################################################################\n\nCons:\n\n- It's hard to tell what the practical benefit of these findings are.  I'd have expected a new SOTA result for some task with the LTH framework and decoupled learning rates, or at least a clear comparison of \"the best baseline LTH vs. the best decoupled LRs LTH over a range of sparsities,\" but I can't find such a result anywhere.\n- The discussion suggests practitioners incorporate \"simple additional sweeps of decoupled learning rates.\"  These sweeps may be simple (they may not, depending on the training framework), but they will almost certainly be very expensive for most production data sets.  (The cost is further compounded when considering the early pruning results of section 3.4.)  Useful guidance for limiting costly hyperparameter sweeps, either from a theoretical grounding or wider empirical studies, would make this requirement less painful.\n- Decoupling learning rates before and after pruning in general is nothing new as claimed in the introduction; past work (e.g. Han et al., 2015, referenced in the submission) specifically use different learning rates before and after pruning, so novelty/scope takes a slight hit. That said, applying it to the LTH framework is new, and it uncovered the dependence of a good model on finding a good LPR set.  (Also, as I argued *for* the submission above, it's new that the goal of the mask finding stage isn't necessarily to find the best-performing dense model; a worse dense model can lead to a better pruned model.)\n\n##########################################################################\n\nQuestions:\n\n- In section 3.2's late rewinding experiments, is the LR warmed up for both the find and eval phases?\n- At the end of 3.2's structured pruning: \"... decoupling LRs does not just apply to lottery tickets, but rather applies to global magnitude pruning in general.\"  Isn't this experiment still essentially the LTH framework, but with structure?  Application to \"magnitude pruning in general\" is too a broad claim, given past work (again, Han et al., 2015).  Or, if there was some experimental setup other than train, prune (with structure), rewind, etc., that is decidedly *not*-LTH, please clarify.\n- The text description of Figure 5 describes different pruning iterations -- what are these iterations?\n\n##########################################################################\n\nMinor suggestions:\n\n- Please add the particular network/data set used for each figure or table in the captions.\n- There's a double \"linear\" in the LR warmup description (just one is enough).\n- Similarly, the first sentence of 3.3 has a double \"also.\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting observations about pruning but open to many conflicting interpretations",
            "review": "## Summary\n\nThe paper studies the effect of different learning rates on finding performant pruning masks that can be applied to train a sparsified neural network in a lottery-ticket-style framework (i.e. train the original random initialization with the resulting pruning mask applied). The study hereby focuses on a particular setting: find a pruning mask using iterative global magnitude pruning (IMP) applied to various ResNets trained on Tiny ImageNet. The paper finds that a small learning rate is beneficial in finding a performant pruning mask, although the resulting network's performance may be worse, while a large learning rate is better suited to then optimize the sparsified network in the subsequent lottery-ticket-style training procedure. The authors further hypothesize that the per-layer prune ratios (LPRs) found by small learning-rate IMP masks are the main driving factor in increasing the performance of the sparse network in the subsequent training process. This is corroborated with a range of experiments where the LPRs from small learning rate IMP pruning masks is used to find masks (with pre-specified LPRs) using larger learning rates. \n\n\n## Score\n\nI enjoyed reading the paper and found the resulting conclusions quite interesting. As pointed out by related work before [1, 2, 3] the learning rate can have a huge impact on the performance of lottery tickets and related experimental settings (such as lottery-ticket-style masks with random re-initialization of the weights). This paper provides additional experimental evidence for this phenomenon by separating the effects of training hyperparameter on the mask-finding procedure and the sparse-training procedure. \n\nHowever, I am not entirely convinced about the framing of the paper itself. I believe the experiments are valid and have merit on their own but in my opinion the authors mix two related but distinct concepts, namely: \n1.  Lottery Tickets (LTs): Pruning masks that occur (in the general setting as pointed out by [2, 3]) _early in training_ and _not at initialization_. Specifically, the LT hypothesis in the more general setting states that if the experiment is repeated _exactly the same_ starting from that early iteration in training but with the applied pruning mask, it does not harm the performance of the resulting network.\n2. Pruning at initialization: Inspired by LT [4, 5, 6] (but also concurred work [7] with LT), various authors have proposed pruning methods that sparsify the network at the time of random initialization and then train the resulting, sparsified network. These methods usually perform better than when the network is pruned uniformly at random. \n\nConsequently, the conclusions that are drawn in this paper are somewhat confusing, it is really hard to discern the generality of the results, and to understand what is actually observed. Are these observations related to LTs or are the authors proposing another method, i.e. small learning rate IMP, to find performant pruning masks at initialization? \n\nI think that must be clarified before this paper can be accepted in order to for the paper to be beneficial to the community as a whole and to ensure that the observations are not just spurious effects of the particular experimental settings. \n\n## Ways to Improve My Score\n\nMainly, I think the paper must be properly contextualized in a clear setting that either studies LTs in general or studies pruning at initialization. Right now it sits in the middle and thus it hards to draw appropriate conclusions as the reader of the paper. Specifically, I can see two avenues for the paper to improve its framing of the results: \n1. Show that the conclusions hold for a more _general_ LT setting. That is, use a version of Algorithm 1 that rewinds to a stable training phase, c.f. [3, 8], where it has been previously shown that valid LTs can be found using standard IMP. As a result that would naturally require the authors to repeat the experiments. \n2. Frame the procedure (small learning rate IMP) as another way to perform pruning at initialization, which performs well. That would require the authors to compare to other pruning at initialization methods [4-7] to understand the performance. Also this would raise the question of why one would use the method in the first place since it is computationally much more expensive than the methods of [4-7]. \n\nI have additional feedback in the \"Weaknesses\" section as discussed below. The minor feedback won't necessarily change my score but I believe can help you strengthen the paper upon publication. \n\n## Strengths\n\n* I commend the authors for a very clean and well-written paper. It is easy to follow, well-structured, and provides sufficient context. \n\n* The experimental study seems to be carried out at a high level. Hyperparameters are clearly summarized and the provided level of detail is sufficient to reproduce the experiments. \n\n* Each plot is clearly labeled and contains shaded error region (i.e. experiments were repeated multiple times). Plots are also all well-structured and easy to interpret.\n\n* The study on the effect of separate hyperparameters for sparse training and discovering sparse masks could be helpful in guiding future research on pruning. \n\n* The part about LPR being the main driven factor for the improved performance is really interesting. \n\n## Weaknesses\n\nOn top of the concerns I have previously raised, here are some additional points of feedback: \n\n* What is the reason to mostly rely on Tiny ImageNet for drawing the conclusions? Most pruning work considers CIFAR and ImageNet. Since the conclusions of the paper are dependent upon previous papers it would be helpful to consider the inclusion of at least ImageNet. \n\n* Why do the late rewinding experiments use epoch 2 as rewind epoch? I believe this is misleading. As stated in [3, 6] the \"stable phase\" of training where LTs can be observed in a general setting usually don't occur until later in the training. So these experiments don't really add value since epoch 2 is most likely not in the stable phase and so it might be confusing to the reader rather than helpful.\n\n* This is mostly related to my main points raised in the \"Score\" section. Since all of these experiments consider rewinding to an unstable phase of training (i.e. back to the random initialization or a very early in training), I am really uncertain whether these conclusions hold for LTs or pruning at initialization. To me personally, the main conclusion is that if I want to use IMP for finding a pruning mask at initialization, I should use a small learning rate. But then again. Why would I use IMP to find a pruning mask at initialization if there is more efficient methods [4-7] for that? \n\n* The work of [9] is drawing somewhat analogous conclusions about the importance of the distribution of LPRs for pruning at initialization. In particular, the authors conclude that mimicking the LPRs found during pruning at initialization with various methods is essentially sufficient to reproduce the accuracy of the resulting sparsely trained network. I couldn't find a version of [9] in a peer-reviewed venue, so it might count as concurrent submission but nonetheless I think it is crucial to compare the results. \n\n## Other Minor Feedback\n\n* Please clarify in the introduction that all observations are entirely limited to magnitude pruning. I don't think that any conclusions can be drawn for pruning in general and some of the text may hint at a more general phenomenon. \n\n* You could add the appendix directly to the main document. It will be easier to read and jump back and forth between the main body and the appendix. \n\n## References\n\n1. [Rethinking the Value of Network Pruning](https://openreview.net/forum?id=rJlnB3C5Ym)\n2. [The State of Sparsity in Deep Neural Networks](https://arxiv.org/abs/1902.09574)\n3. [Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://arxiv.org/abs/1912.05671)\n4. [Picking Winning Tickets Before Training by Preserving Gradient Flow](https://openreview.net/forum?id=SkgsACVKPH)\n5. [Pruning neural networks without any data by iteratively conserving synaptic flow](https://arxiv.org/abs/2006.05467)\n6. [Progressive Skeletonization: Trimming more fat from a network at initialization](https://arxiv.org/abs/2006.09081)\n7. [Snip: Single-shot network pruning based on connection sensitivity](https://openreview.net/forum?id=B1VZqjAcYX)\n8. [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://openreview.net/forum?id=S1gSj0NKvB)\n9. [Pruning Neural Networks at Initialization: Why are We Missing the Mark?](https://arxiv.org/abs/2009.08576)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper finding that hyperparameters that lead to more accurate networks do not necessarily lead to better pruned networks",
            "review": "# Summary\n\nThis paper evaluates explicitly decoupling the hyperparameters (specifically, learning rate) used to find pruning masks from those used to train pruned networks, finding that for global magnitude pruning lower learning rates (relative to the learning rate that results in the most accurate full-size network) result in masks that can train to higher accuracies; on the other hand, higher learning rates tend to train the pruned networks better. These results are shown to generalize across pruning and re-training techniques (including the standard lottery ticket regime, lottery tickets with warmup, weight rewinding, learning rate rewinding, and structured pruning). The paper then shows that these differences in performance are primarily due to differences in layerwise pruning rates that come from training with different learning rates.\n\n# Strengths\n\n- The paper proposes an interesting experiment, decoupling mask-finding hyperparameters from mask-training hyperparameters, with the potential to change how people think about network pruning, challenging the assumption that a higher-accuracy full network results in a higher-accuracy pruned network\n- The findings are well evaluated, convincingly showing that lower learning rates than those that result in the highest-accuracy full network result in better masks\n- The analysis is also strong, showing that the primary factor in the performance delta is the layerwise pruning rates\n\n# Weaknesses\n\n- Given that many of the findings are centered around performance when resetting the weights to the beginning of training, which is known to not work with large-scale networks, a significant amount more discussion of or replications of experiments of [1] is warranted.\n- Presentation of figures could also be significantly improved: for instance, by moving figures closer to text that references them, by collating high-level takeaways into tables, and otherwise making it easy to process the large amount of data in this paper\n- The early pruning subsection seems to refer to results that are not presented in the paper; given that the data is presented in the appendix, it'd be better to include this text in the appendix.\n\n# Overall recommendation\n\n7: Accept\n\n# Other comments and suggestions\n\n- The original training hyperparameters could be much more clearly presented in the main body of the paper\n- It would be helpful to show the original accuracy of the fully trained networks (at various different learning rates) on the plots\n\n# References used in review:\n\n[1] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. \"Linear Mode Connectivity and the Lottery Ticket Hypothesis\"\n\n\n# Update after author response\n\nThanks to the authors for their response. I appreciate the inclusion of Figure A13,  though more details (specifically, whether the plot for LR=0.2 is both LR_find and LR_eval, as I suspect but can't confirm) and more runs (specifically, the decoupled LR lines in Fig 4a that outperform LR=0.05, probably the LR_find=0.05 and LR_eval=0.2) would be appreciated. \n\nI don't agree with the authors' claims about standard LT (rewinding to iteration 0) being equally as valid of an object of study as rewinding to later iterations on these large-scale networks, because by the original definition of lottery ticket (a sparse randomly initialized subnetwork that matches the accuracy of the full network in the same amount of training time), there do not exist standard lottery tickets on ResNet-50 at nontrivial sparsities using the standard learning rate schedules, and the resultant \"lottery ticket\" network trains little better than a random subnetwork [1, Figure 10], implying that LR_find may not actually be a relevant hyper-parameter in this large-scale rewind-to-0 context. I still think more discussion of this point is also warranted when discussing results on ResNet-50 when rewinding to iteration 0.\n\nRegardless, I believe that the paper presents and thoroughly validates an interesting hypothesis, and I maintain that the paper should be accepted.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "I don't think the assumptions the authors made about current practices, which are the motivation for the propose method, are correct.",
            "review": "### Summary:\nThe authors propose to use a different learning rate (LR) when fine-tuning a pruned model (LR_eval) than the learning rate used for the original training (LR_find). They empirically demonstrate in a Lottery Ticket (LT) framework, using a ResNet50 on Tiny ImageNet, that the LR that produced the best model before pruning does not correspond to the LR that produces the best results after fine-tuning. They then further show this phenomenon on other LT setups (with late rewinding, LR warm-up, LR rewinding), architecture (ResNet18), and dataset (MiniPlaces), and for structured pruning as well. Finally, they empirically show how LR_find impacts the pruning ratio obtained at each layer, and that high LR_find seems to prune too aggressively the first layers.\n\n### Strengths:\n+ The paper tries to provide better understanding of pruning methods, rather than introducing a new heuristic, which is an extremely important research direction in network pruning.\n+ The results are presented on different architecture, datasets, LT variants, and on both structured and unstructured pruning setups.\n\n### Concerns:\n+  I have issues the motivation behind the proposed method. In particular, these few sentences (listed below) sound quite incorrect to me, at least in the LT framework, which is the framework used for in this empirical evaluation:\n   - _\"However, prior work has implicitly assumed that the best training configuration for model performance was also the best configuration for mask discovery.\"_ \n   - _\"Common practice rests on the assumption that models with the best performance will generate the best masks, such that the optimal hyper-parameters for mask generation and mask evaluation should be identical.\"_\n  - _\"If we had to use the same LR for everything, we would have to make a trade-off between a better unpruned performance and a better pruned model performance.\"_\n  - _\"If we had followed common practice in the past, we would have swept over some LR values over standard training, found that 0.5 performed the best, trained LT with 0.5 as the LR\\_find, and gotten poor performance.\"_\n\n  In a pruning scenario where we have to both train the network and then prune it, like in the LT framework, the hyper-parameters are optimized to produce the best performing models after the iterations of pruning and fine-tuning, not the best performing network before the first pruning!\n  Ideally, one would have to tune all the hyper-parameters, which means the hyper-parameters at each of the LT iterations, not only the ones of the last iteration as proposed, which is prohibitively expensive. However Renda et al. (2020) showed that using the same ones at each iteration is a good heuristic, and this is precisely what the experiments of this paper seems to show (the best overall results are: in Figure (2) LR_eval = LR_find = 0.2, in Figure (3) LR_find = LR_eval = 0.2, in Figure (A1) LR_find = LR_eval = 0.1, etc...). I do agree however that one should not first tune the hyper-parameters to obtain the best network, and then reuse these same hyper-parameters for the pruning stages, and your experiments indeed show that, but I don't think this is what practitioners do in practice.\n\n+ While the analysis of how the pruning ratio per layer changes depending on LR_find is quite interesting, it lacks a bit of insights. What is causing this? Are other pruning criteria also affected? One can imagine the neural networks trained with high learning rate have gradient propagation issues (Magnitude Pruning removes the smallest weights, so the ones that received the smallest gradient). On that note, how are the networks initialized (this information is missing from the document)?\n\n### Reasons for score:\nAs it stands, I don't think this paper brings much new insights, and I don't think the assumptions the authors made about current practices, which are the motivation for the propose method, are correct.\n\n### Questions:\n- At each iteration of LT, what is the model selected to compute the mask on? Is it the model obtained at the last iteration, or the model that was best performing on the validation set? I imagine this could have a big impact on the results, as a high learning rate might produce models drastically different.\n\n### Other comments:\nThere are a few inaccuracies in the paper:\n - (2.1) what are one shot and local pruning (it should be defined in the paper)?\n - What are error bars representing in the figure? Standard deviation? Standard error? How many seeds were used? This should be precised in the document.\n - (2.2) What does \"decaying the learning rate with a gamma of 0.2\" means? Again, the type of decay used should be precised.\n\nFinally, Footnote 1 is quite unnecessary.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}