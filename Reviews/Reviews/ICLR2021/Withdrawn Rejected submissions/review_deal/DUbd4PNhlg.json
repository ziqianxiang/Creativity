{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper provides a theoretical analysis on a specific case of teacher-student setting: the teacher has only a single neuron and the student is over-parameterized (with multiple neurons K > 1). Both teacher and student nodes use quadratic activations.  Based on this setting, the paper shows local linear convergence (Lemma. 1) and shows that in the over-parameterized case, since each student weight is initialized independently and naturally contribute a small components of the target weight, over-parameterization gives stronger overall components of the target weight and thus accelerate the training. Finally, the paper shows that in this setting, a simple scaling of the student neurons shows similar behavior as the over-parameterization, since the parallel component of each student weights on the target weight now becomes larger. \n\nWhile studying the theoretical justification of over-parameterization is important and analysis looks interesting, the analysis heavily depends on the specific network structure and quadratic activations (Eqn. 4 itself is tailored to quadratic activation, also see the paragraph after Eqn. 4). Are there any ways to extend it to more realistic cases (e.g., ReLU setting, multi-layer deep networks)? The analysis is also local and it is not clear what happens to the beginning of the training, where the weight loss (Eqn. 4) is still large. Finally, Sec. 4.4 seems to serve as a counter-argument of the single neuron setting: by a simple scaling of the student neurons, the effect of over-parameterization can be fully accounted. Maybe the authors should consider more general  cases like multiple teacher neurons. Then we might see interesting behaviors: e.g., during training students compete with each other, and with over-parameterization, there always exist a student node that might have an advantage for a teacher node. In this case, the diversity induced by over-parameterization starts to play a key role and might better reflect the real picture how the deep model works. \n\nOverall I think the submission is still in its early stage and maybe a bit pre-mature for top-tier publication. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "[Summary] This paper studies acceleration due to over-parameterization. Specifically, the authors consider a simple setting where there is a single teacher neuron with quadratic activation and learning over multiple student neurons, and prove that over-parameterization leads to faster convergence for gradient descent in the sense that it enters the neighborhood of a global optimization solution faster.\n\n[Pros] Understanding the effect of over-parameterization in optimization is an important topic and is far from being well understood. So the contribution to this topic is timely.\n\n[Cons] 1. The model considered in this paper is very restrictive: the teacher model consists of only a single neuron and a quadratic activation. Even in this simple setting, the main result (in Theorem 2) requires the additional assumption that all the iterates are small across all the iterations. \n\n2. The comparison of the convergence speed between the exact-parameterization and over-parameterization under the same step size is not that fair. It is not clear whether the advantage discovered in the paper still holds or disappears if one simply uses a larger step size for the exact-parameterization, or use optimal step sizes for both cases.\n\n3. As shown in section 4.4, the acceleration effect of over-parameterization can be obtained by simply scaling up the weight of the output. Is it equivalent to use a larger step size? I am also curious if this result also holds in a general setting. If not, then the acceleration phenomenon discussed in this paper may be restrictive. Overall, more experiments under general settings are needed to make the claims more convincing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple model to illustrate how over-parameterizations can accelerate the optimization, but the theoretical analysis seems wrong.",
            "review": "Summary:\n\nThis paper studies how over-parameterizations can lead to the acceleration in optimization for a simple model: learning a single neuron by using many neurons. The activation function is the quadratic function, so the theoretical analysis is relatively easy. The authors also show that simply scaling the output factor can also speed up the training without the need of over-parameterizations. \n\n---\n\nPros:\n\nThe observation of over-parameterizations leading to an acceleration in optimization for learning a single neuron seems new. It may provide a good model for further theoretical analysis. \n\n--- \n\nCons:\n\nThe theoretical analysis does not make sense to me.\n\n1) The simplified dynamics (10) suggests that all the neurons are independently updated. The interaction between neurons is ignored. Let's use $K$ denote the number of student neurons.  So basically, the simplified dynamics is equivalent to K independent copies of learning a single neuron with a single neuron.  Apparently, this kind of trivialized model won't accelerate the training.  So the picture behind this kind of simplification does not make sense to me. I anticipate that somehow the interaction must play a role in helping the student find the benign region. \n\n2) The analysis in Section 4.4 is also very misleading.  Denote by  \n$$F_C(w) = E_{x}[|C (w^Tx)^2-(w_*^Tx)|^2]$$      \n\n   the loss function of the scaled model.  Then the GD dynamics is given by \n  $$\n    \\dot{w} = - E[(C(w^Tx)^2-(w_*^Tx))2C w^Tx].\n  $$\n\n  Let $v = \\sqrt{C} w$, we have \n  $$\n  \\dot{v} = - \\sqrt{C}  E[((v^Tx)^2-(w_*^Tx)) 2v^Tx] = - \\sqrt{C} \\nabla F_1(v).\n  $$\n\n  Here $F_1(\\cdot)$ is the population loss of the unscaled model. So the scaling of the model is equivalent to a trivial scaling of the original GD dynamics. Apparently, this kind of speeding up cannot be transferred to the discrete updates.\n\n--- \nOther Comments:\n- Why is the quantity (4) instead of the population loss used as the metric to measure the convergence. The population loss has an analytic formula as well.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Thorough analysis of an oversimplified case",
            "review": "The paper is in the important field of finding theoretical underpinnings on how different network designs ultimately learn. There are plenty of experimentally found designs whose justifications are always somewhat anecdotal. Without proper theoretical analysis from first principles, the generalizability and applicability of various tricks of the trade are left only vaguely understood.\n\nHowever, the paper focuses on an oversimplified situation that does not properly reveal the consequences of overparameterization. Typically overparameterization is not employed to gain convergence speed. Instead, its appeal is in the apparent inductive bias that improves generalization at the cost of increased training time. In the case analyzed in the paper there is no notion of generalization, and thus the main rationale for overparameterization cannot be analyzed.\n\nEven worse, the paper does not consider the increased cost of training an overparameterized network at all. Using the proposed overparameterization scheme, overparameterizing by a factor of K increases the training cost approximately K-fold. When taking this into account, none of the overparameterized variants come close to decreasing the overall cost of training. Instead, the total cost is always greater. Combined with no possibility for improved generalization power, this yields a situation that would never actually occur - nobody would overparameterize a network so that there is no upside in better performance in a downstream task, but there is an increased training cost.\n\nThe analysis of multiplier C on page 8 is, in my view, actually more interesting than the rest of the findings. This kind of modification does not increase the cost of evaluating, and thus training, the network, but it appears to have an effect on convergence speed. As there is no downside, there is also no need to counterbalance it with an upside, making this type of optimization a clear net win. Therefore, the simple case of a single teaching neuron is sufficient for analyzing this phenomenon.\n\nI greatly appreciate efforts to theoretically understand the experimentally found designs that make neural networks better. This paper, however, essentially analyzes a case that at first glance looks to be worse by a factor of K by design, and shows that it is actually worse by only a factor less than K. I do not think this is a very fruitful result even as a stepping stone. Overparameterization is analyzed in a regime where it is best to not overparameterize at all, and as far as I can tell, the theoretical findings hold only in this regime.\n\nPros: important topic, thorough analysis, experimental runs that provide insight into the problem.\n\nCons: overly simplified setting, no analysis of total computational cost, no analysis of other consequences of overparameterization besides convergence speed, no analysis of other optimization methods.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and timely topic; theory can be improved",
            "review": "This paper studies the scenario of learning a single teacher neuron by a student network with, potentially, several neurons, where all activation functions are quadratic functions. In particular, the paper is interested in the question of how overparameterization can be helpful in order to learn the teacher neuron. The paper provides some experiments as well as a theoretical analysis.\n\nClearly, this paper studies an important and timely topic, which has potentially great practical implications. Clearly, this paper raises important questions and interesting phenomena are observed in the experiments. However, I, unfortunately, do not think that the theoretical results are strong enough to merit publication. \n\nIn my opinion, the main problem is that the theory only in the case of infinitely many samples, i.e. in the population scenario. I feel that this is somewhat insufficient, as there is a large literature for related problems, where the finite-sample scenario is studied. (For example, as the authors mention in their paper, the scenario that the student network consists of only one neuron is equivalent to the phase retrieval problem, where there  is a large literature studying the finite-sample scenario as cited by the authors in the paper.) Studying the finite-sample case is especially critical in the overparameterized scenario, where the number of parameters is larger than the number of training samples, as it is not clear which parts of the population analysis transfer to the finite sample scenario. \n(To be fair, one can make the point that in the scenario of quadratic activations is somewhat different since here, as there is a unique PSD minimizer, see, e.g., Candes, Li \"Solving quadratic equations via PhaseLift when there are about as many equations as unknowns\").\n\nIn addition, I feel that the theorems itself are not strong enough, even for the population case. For example,  Theorem 2 assumes that (10) and (11) hold. While the authors note that simulations indicate that this conditions hold, I rather would like to see a proof for this.\n\n Also the conclusion of the theorem is very hard to interpret as it depends on many implicit parameters such as  $ c_{1,t} $ and $ c_{2,t} $. Although the authors try to provide some interpretation below the statement, this is not enough in my opinion. In order to strengthen the paper the authors should try to simplify the statement and conclusion of Theorem 3, such that an easier interpretation is possible. \n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}