{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for Adaptive Gradient Method with Resilience and Momentum",
            "review": "Summary:\n\nThis paper proposes a new adaptive gradient method with resilience and momentum (AdaRem) for training deep neural networks to reduce the oscillation of the network parameters during training. This method adjusts the learning rate based on whether or not the current gradient is aligned with the exponential moving average of the past gradients (i.e. the momentum term). Convergence analysis of AdaRem using the online convex programming framework is provided. The paper also proposes a variant of AdaRem, namely AdaRem-S, for networks with scale invariance properties including those with batch normalization (BN). In particular, AdaRem-S constrains the optimization on a sphere. The authors empirically show that AdaRem outperforms the stochastic gradient descent with momentum (SGDM) and other adaptive optimizers on small networks trained for ImageNet classification tasks. Results on different deep architectures such as MobileNetV2 and ShuffleNetV2, as well as recurrent neural networks (RNNs) for a language modeling task on the Penn Treebank dataset, are also provided.\n\n###################################################################\n\nReason for the Score:\n\nOverall, this paper could be an interesting algorithmic contribution. However, my main concern is the lack of experiments to justify the advantage of the proposed method. ImageNet experiments are done only for training with large batches on small models without error bars provided. Experiments on Penn Treebank dataset are also done with a small network. \n\nCurrently, I am leaning toward rejecting the paper. However, I would be willing to increase the score if the authors address my concern about the lack of experiments.\n\n###################################################################\n\nStrong points:\n\n1. The idea of adjusting the learning rate according to the alignment of the current gradient and the history of the past gradients is interesting.\n\n2. The paper gives good motivation and analysis for the proposed method, addressing different aspects of AdaRem including how to use the optimizer with scale-invariant models. \n\n3. The paper is well-written. \n\n###################################################################\n\nWeak points:\n\n1. Experiments are done for small models, such as ResNet-18, and it seems that AdaRem(-S) only outperforms the baseline optimizers when the model is small.  In Table 1, when training on ResNet-50, which is a larger model, AdaRemS yields worse accuracy than SGDM. There is no comparison between AdaRem/AdaRemS and other baseline adaptive methods such as Adam, AdamW, AdamBound, and RMSProp for ResNet-50. \n\n2. For ImageNet experiments, the paper does not discuss the performance of AdaRem(-S) versus the baseline methods when the models are trained with small batches.\n\n3. Error bars are needed for the reported results.\n  \t\t\n###################################################################\n\nAdditional Concerns and Questions for the Authors:\n\n1. In Algorithm 1 and 2, what is the role of the projection operation? The author does not discuss it in the paper. Is it a projected gradient descent step? \n\n2. In section 3.2.2 and in Figures 1 and 2, what is the metric to compute the path length l and the shortest distance d? In particular, the authors mention that “path length l is the total distance a parameter travels from a starting point and displacement d is the shortest distance between the ending point and starting point of a parameter.”. Here, how is the distance computed? Is it based on the norm of the parameters? If so, the proposed quantitative index does not fully capture the oscillations since the parameters can still change significantly while their norms are preserved.\n\t\t\t\t\n3.  In the caption of Figure 4 and in Section 4.1.2, the authors claim that “AdaRem achieves the fastest training speed among all methods.”  Actually, in Figure 4, AdamW converges faster than AdaRem.\n\n4. Why are the training loss curves for AdamRem-S in comparison with other methods not provided in Figure 4?\n\n###################################################################\n\nA Minor Comment that did not Impact the Score: \n\nIt would be interesting to see the ablation study for the effect of the hyper-parameter \\lambda in AdaRem(-S).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Resilience term stabilizes the optimization trajectory",
            "review": "Summary: this paper proposes the AdaRem algorithm that reduces the oscillation of iterates. In particular, it introduces a resilience term that adjusts the update direction to be stable for each coordinate, thereby reducing the unwanted fluctuation. AdaRem also incorporates a weight decay as in AdamW. Under some restrictive assumptions (e.g. bounded domain, bounded stochastic gradient), AdaRem has a $O(\\sqrt{T})$ regret under the online convex programming framework. To handle the batch normalization layers, the paper introduces the AdaRem-S algorithm that enforces the scale invariance. Both algorithms have desirable empirical performances.\n\nPros:\n(1) Resilience is a novel technique to stabilize the optimization trajectory.\n\n(2) The algorithm performs well on the examples considered in the paper.\n\nConcerns:\n(1) The paper claims (page 5) that \"An important property of AdaRem is that the gradient does not change the sign in spite of a large momentum with the opposite sign\". I think this is an inaccurate statement. Since $\\lambda < 1$ and $b_{t} < 1$, $1 + \\lambda^{t}b_{t} > 0$. Thus, $\\eta_{i, t}g_{i, t}$ always has the same sign as $g_{i, t}$. If $g_{i, t}$ and $g_{i, (t-1)}$ have opposite signs, $\\eta_{i, t}g_{i, t}$ and $\\eta_{i, (t-1)}g_{i, (t-1)}$ have the opposite signs as well. Am I missing anything? If not, what is the real benefit of the resilience term?\n\n(2) The Q-value is not convincing to me. Suppose a coordinate has tiny fluctuation and gets back to the initial value frequently, $\\ell / d$ would be infinite or large. Is there a better measure of oscillation? Maybe the denominator $d$ can be replaced with the range of the trajectory?\n\n(3) The regret bound in Theorem 1 should be stated as $O(d\\sqrt{T})$ instead of $O(\\sqrt{T})$ since the dimension dependence is quite undesirable for deep neural networks. In practice, $d$ often dominates $\\sqrt{T}$.\n\n(4) In Section 3.2.1, what does the sentence \"We argue that directly using the momentum instead of the gradient to reduce the oscillation in SGDM does not give full play to the effect of momentum\" mean?\n\n(5) In the experiments, which step size scheme is used for AdaRem? The theorem requires the step sizes to decay as $1 / \\sqrt{t}$. By \"learning rate\" do you mean the multiplier of $1 / \\sqrt{t}$? It should be clarified if there is a gap between the theory and the implementation.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Meta optimization of learning rates",
            "review": "Authors propose meta optimization of learning rates, Also proposes a metric of measuring oscillation of parameters during training which is quite interesting.  They show momentum helps reduce the oscilations, and their method AdaRem helps even further.  Authors also show a variant of their method for optimization for norm invariant functions.\n\nMy reason to score is primarily due (a) lack of details of how effective the learning rate adaption is, for eg: visualization the learning rate adapation can help resolve my questions (b) how well does it generalize across domains, the LSTM pentreebank results are concerning and (c) missing citation to existing work one of which explain challenges with meta-optimization and lack of comparions to other meta optimization of learning rates.\n\nI have listed my concerns below and hopefully authors can address them during the rebuttal period.\n\nQuestions/Comments\n\nAuthors use λ ** t in equation 2 without ever defining lambda or t, it would aid the reader if it was defined in the section.  Also, tuning details did not seem to include what lambda was used.\n\nSince this method actually auto-tunes the learning rate, a section on how learning rate is adapted is needed -- This could be plotting for a subset of coordinates how the lr evolves due to methodology (taking out external schedles)\n\nCould authors add couple more benchmarks; say a translation model on WMT'14 en-fr or any other benchmarks from: https://mlperf.org/\n\nFor learning rate tuning on a sphere, could authors indicate how this is applied in context of batch norm variables where a scalar and bias is learned?\n\nFigure 2. authors comment Adam has smaller Q due to adaptive changing of learning rate, -- could that be just because Adam also uses momentum?\n\nPlease add comparisions to other meta optimization strategies such as Delta-bar-delta and its variants.\n\nMissing citations on related work:\nJacobs, R. A. Increased rates of convergence through learning rate adaptation. Neural networks, 1(4):295–307,\n1988.\n\"Understanding Short-Horizon Bias in Stochastic Meta-Optimization\",  Wu et al https://openreview.net/forum?id=H1MczcgR-\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper may present several new ideas.",
            "review": "This paper wants to proposes a novel algorithm that is faster than Ada but enjoys almost same generalization property as SGD.\n\nMany places of this paper are unclear for me. For example, the authors write \"We argue that directly using the momentum instead of the gradient to reduce the oscillation in SGDM does not give full play to the effect of momentum.\"  Actually, I do not find what the experiment they conducted in Sec. 3.2. With just 1 Figure, how to get such a conlusion. \n\nThe  MOTIVATION part just helps to claim that \"us of the gradient to reduce the oscillation\". However, in the latter, how to motivate the scheme of the algorithm?\n\nDue to that DNN training is always non-convex , the theoretical results shall use a nonconvex  setting.\n\n\nA  major obvious technique problem  is that the proposed aglrotihm may perform bad if $g_t$ is very small  relative to $\\theta_t$. That means such algorithm may work bad when the network layer is large. And I also observe that the numerical results on deep network is unreported.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A new adaptive algorithm",
            "review": "To summarize, the authors proposed a new adaptive algorithm named AdaRem and its spherical variant AdaRem-S. They claimed that AdaRem reduces oscillations during the training process. The authors provided a convergence result along with experiments to support their two new algorithms.\n\nThe good points are:\n- The authors proposed a new update rule for a class of adaptive methods, adjusting the learning rate based on whether the current gradient aligns with the direction of changing in the past. They claimed that AdaRem reduces oscillation more effectively compared to Adam and SGD with Momentum.\n- They provided a convergence analysis for AdaRem and proposed a spherical version AdaRem-S for networks with Batch Normalization layer.\n- They conducted experiments to show the benefit of AdaRem compared to SGD with Momentum and other adaptive methods.\n\nThe weak points are: \n- The authors used an online convex programming framework to analyze AdaRem. However, they evaluated their algorithms on various deep neural network models, which are highly nonconvex. Therefore, their theoretical analysis cannot support the empirical results. \n- The experiment in Table 2 shows that AdaRem has the final test accuracy as well as SGD-M, and similar performance as Adam-W in Figure 4. This result still doesn’t show the significant difference of AdaRem compared to other algorithms. In Figure 5 and Table 1, AdaRem-S is better than SGD-M, however, it lacks a comparison with the other  algorithms such as Adam, RMSProp,... as in Figure 4. \n- AdaRem requires an additional hyper-parameter ($\\lambda$) compared to SGD with momentum, that is also additional tuning time. \n\nAlthough the update idea is nice, the theoretical aspect and practical results of this paper are not quite convincing to me. I think at the moment this paper needs more improvements.\n\nMy additional concerns and comments are below: \n\n- The update rule involves an important hyper-parameter $\\lambda$, however, I can’t find a discussion on the value of this parameter in the description of AdaRem. I think it is very important to specify the possible range of $\\lambda$ (e.g in the statement of Theorem 1). If $\\lambda > 1$, the results of Theorem 1 could be incorrect. Similarly, it is better if the authors indicate the learning rate $\\hat{\\eta_t}$ in Theorem 1 clearly and the range of $\\eta$ as well. \n- The learning rate notation for AdaRem and AdaRem-S is not consistent. AdaRem used $\\hat{\\eta_t}$ while the latter used $\\eta_t$, which is quite confusing because the authors also used the notation $\\eta_t$ on subsection 3.4.1. Moreover, I think this subsection is confusing because it is not clear what learning rate scheme is used for AdaRem-S.\n\nMinor comments:\n\n- There is no rigorous explanation for the optimization problem in Figure 1. I would assume that the algorithm is SGD, but the overall setting is not clear. \n- There are several typos in Subsection 3.1 Notations and Preliminaries. First, the two vectors at the beginning seem identical. The following sentence may need a comma. The gradient at timestep $t$ shows no relation to $t$. In the third line of the third paragraph, iteration $t$ should be italic.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}