{
    "Decision": "",
    "Reviews": [
        {
            "title": "needs a lot of re-writing",
            "review": "This paper proposes a dictionary-based method to enhance word representations with visual knowledge. First, using a paired data of images and captions a dictionary is created. For a downstream task, this dictionary is used to retrieve relevant images for tokens. Text and image representations are fused to enhance each token with a visual context. Experiments on several NLP benchmarks show the effectiveness of the proposed method. I suggest the rejection of this work since 1) it lacks clarity 2) it requires parallel data for images and text but claims otherwise 3) lacks a depth of exploration in terms of generalization.\n\nI list my questions (Q) and suggestions (S) below.\n\nS1 Abstract: 'leveraging the whole sentence-level' delete whole. 'more accurate contextualized' accurate for what please clarify. 'stiffing disambiguation' please define and clarify. 'active polysemy' please define and clarify. 'word-aware' does not make any sense since a representation for each word is induced, please consider changing this term. 'task-specific layer for prediction' prediction for what?, please clarify. '12 remarkable' please delete all instances of remarkable, use more objective language. 'generality' --> 'generalization capability'\n\nS2 Introduction: First sentence, please cite from all decades mentioned. In the second sentence, please cite brown clusters. Define and clarify and give examples for 'stiffing disambiguation over polysemy'. Please delete 'actually'. 'sentence expressions' --> 'sentence meanings'. 'each concerned context' please delete concerned.  'to release the inconvenient constraint' please rephrase. 'irrelevant noise' please delete irrelevant. Please give a convincing example for 'sentence-level clues may not always help word-level disambiguation'. \n\nS3 Figure1: please find a better example like 'the girl saw the man on the hill with a telescope'\n\nS4 Section 2.1. 'these approaches blindly create word vectors without taking into account the shared dependencies and context in which each word entails' Simply not true. The whole premise of distributed representations based on the idea that shared dependencies and the context determine the meaning of a word. Please be more precise and accurate in your criticism of these methods. 'capturing the semantic, contextual, and syntactic meaning' simply not true -- please support these claims with citations. \n\nS5 Section 3 Algorithm 1 please define zip()\n\nS6 Figure 2: Following this figure is like following a maze. Please simplify where the flow is natural (left to right or top to down)\n\nS7 Section 4: Please add a random baseline where an image is randomly paired with a word. Please add experiments for other languages such as German and French. \n\nS8 Table2,3 please add a column for methods using paired vision-and-language data.\n\nS9 Figure4 please clarify which task\n\nQ1 Table2: how do you explain the significant difference between your model and previous models, baseline, and your models on SST2l, MRPC, and QQP?\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Using paired text-image data improves NLU and NMT tasks; concerns about novelty and presentation",
            "review": "In this work, the authors explore techniques to use multimodal paired image-text data to learn a “image-word dictionary” which can be used to augment text-based embeddings with information derived from images. The authors evaluate the proposed techniques on a number of tasks. The main novelty over the previously published work of Zhang et al. (2020) appears to be the use of sub-words instead of word tokens for some tasks, and the evaluation of the proposed techniques on various NLU tasks.\n\nMy main concerns about this work are related to it’s novelty -- in particular, the similarities to the previous work of Zhang et al. (2020); and the presentation of the work -- in particular, the english usage, as well as the fact that many important details are missing or hard to understand (for example, the analysis section). My main comments appear below:\n\nMain comments:\n - The presentation of the paper, and in particular the usage of english makes it somewhat hard to read. I’ve included some examples below, but I would suggest that the authors examine the entire paper and correct similar inconsistencies throughout:\n  * Some sentences contain somewhat unusual word choices:\n“12 *remarkable* natural language understanding and machine translation tasks”;\n“*stiffing* disambiguation” (could the authors clarify what this is); \n“Multimodal information might provide *auxiliary instructions* to help ...”; \n“... we first perform experiments on a disambiguation task as *appetizer* ...”\n  * Some sentences contain awkward phrasing: \n“ … therefore, it is essential to release the inconvenient constraint on the annotation prerequisite when applying visual information … “; \n“... so that it easily suffers from the noise of irrelevant images.”\n  * Some contain grammatical errors: \n“Meanwhile, multimodality derived from word representations can also be more helpful to provide as *more* images as possible … “ --> “... as many images as possible”; \n“However, these approaches blindly create word vectors without taking into account the shared dependencies and context *in which each word entails*”; \n“... based on the *lingual* corpus”; \n“... which ensures the token overlap with task datasets to the maximum *extend*” --> “... extent”\n - In Section 3.1, the authors mention that they build a word-image dictionary “inspired by Zhang et al. (2020)”. To the best of my understanding, the only difference between the authors’ proposed method and the one described in (Zhang et al., 2020) is that in the present work the authors propose to tokenize the sentence using a “tokenizer that depends on downstream tasks”. In my view, this is not very novel. Also, if this is one of the claims of the paper, then I think then authors should conduct experiments to demonstrate that using sub-word embeddings is indeed better than using word-embeddings as in (Zhang et al., 2020). Does this experiment correspond to the results in Table 3? I would suggest that the authors re-write the paper to more clearly communicate the relationship of the proposed work to the work described in Zhang et al. (2020). \n - The description of Algorithm 1, and Section 3.1 / 3.2 is not very clear. For example, in Section 3.1, the authors mention that “At the end of processing the whole multimodal corpus, we form a word-image dictionary where each subword contains diverse images sorted by the number of image occurrence, which are later converted to the image representation as visual hints.” I did not follow how this sorting was done. Based on reading (Zhang et al., 2020) I assume that the authors mean that they sort the retrieved images for a new input sentence based on the counts, but I’m not sure if the authors mean something else. Also, even in that setting, I assume that there are many images in the list of retrieved images which have the same count. The authors do not mention how they break ties in this case when sorting the images. Also, a minor point: in the description, the authors use “m” to represent both the number of subwords in the tokenization as well as the number of images that are retrieved (section 3.1 and 3.2) which is a bit confusing.\n - Something that I missed after reading this paper is the difference in the results between the work proposed in this paper by the authors and the previous work of (Zhang et al., 2020) on the NMT tasks (i.e., the results in Table 3). Could the authors clarify what the differences between UVR and “ours” in that table are?\n - In the results reported in Table 2, the authors show results for the proposed method against the Baseline which is described in Section 4.2.1 to be BERT. The table also contains a separate results for BERT from (Devlin et al., 2019). What are the differences between these two systems?\n - The analysis presented in Section 5 is extremely brief, and at least in my view, is not very clear. The graphs in Figure 4 and Figure 5 are not easy to read because it is not clear what the values that correspond to each of the points/bars in the graph are. Also, I did not understand the “Image coverage analysis” based on the description. Finally, the description in the section titled “The influence of images corpus” was extremely brief. Also, I think this analysis would be more interesting if it were conducted on multiple tasks instead of on just the EN-DE task. \n - The authors mention at the end of section 4.4 that the proposed systems have more parameters than the baselines (6.3%). Given that the gains on some tasks appear to be relatively small over the baseline [example on MNLI and QNLI (0.3% absolute); SNLI (0.2% absolute)] I think it is necessary to conduct experiments with exactly the same number of parameters to demonstrate an improvement. Also, while it is great that the authors conduct significance testing on Table 3, I was wondering if the differences in Table 2 relative to the baseline are significant.”\n\nMinor Comments:\n- Section 3.3: Could the authors clarify what they mean by “SAN-based encoder”?\n- Section 4.2.1: Please cite (Sennrich et al., 2016) or another appropriate reference for SentencePiece\n- A detail which I might have missed was how many images, m, were used as inputs to the “image encoder” in the experiments that appear in Section 4.3 and 4.4\n\n************************\nUpdate after author comments\n************************\n\nI appreciate the authors efforts to revise the paper based on the reviewers' comments. In particular, the new experimental baselines address some of the concerns that I and other reviewers raised. However, I still feel that the work feels incremental relative to the previous work of Zhang et al. As I mentioned in the review, I think the work could be significantly strengthened by expanding on the analysis section to demonstrate the benefits of the finer-grained representation. For these reasons, I would like to retain my score as before.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting concept, but the experimental results are not convincing re: the model is meaningfully integrating visual information.",
            "review": "The authors propose a multimodal model that augments text-only tasks\nwith visual information at the word level (versus most prior works\nwhich augment at the sentence-level). Specifically, for each token in\nan input sentence, a dictionary lookup is performed and several\nrelevant images are obtained. A joint encoding is computed using a\ntransformer model, which is evaluated on GLUE and MT tasks. The\nauthors report modest performance gains when integrating their\nmultimodal components.\n\nThe motivation for this work is strong: it definitely seems like\nintegrating multimodal information should be helpful for language\nunderstanding tasks: after all, when humans address these tasks, our\nreasoning isn't limited to text-only inferences. The model itself is\nappropriate, and the idea of retrieving relevant images for text\n(though not new) is interesting. The experiments cover an appropriate\nswath of language processing tasks.\n\nHowever, my primary technical concern is that it's difficult to tell\nif the performance improvements in MT and NLU are due to the\nintegration of the multimodal information, or if they are due to some\nother factor (hyperparameters, number of parameters, number of model\ncomparisons, etc.). One missing baseline: the author's model, but\ntrained with random images retrieved. This baseline would isolate the\nimpact of semantically meaningful images (versus, e.g., simply having\naccess to more parameters relative to the baselines). In fact, the\nauthors NLU BERT baseline, which was trained \"without any\nmodification\" from the original procedure, illustrates the amount of\nvariability one might expect under multiple runs of finetuning BERT\n(compare: BERT vs. \"baseline\" rows in Table 2). Given the small\nmagnitude of difference in performance, I really would have liked to\nhave seem some estimate of confidence (e.g., train 5x and report the\nmean). Looking in the appendix, I have even more concerns: Table 4\nshows that the authors tried 3 versions of their model, and switching\nto WWM (vs. base or large) seems to have a much greater impact than\nincorporating visual information.\n\nMy concern that the performance gains might be due to a factor other\nthan meaningful semantic integration of visual information is\nexacerbated by the lack of error analysis. If the multimodal model\noutperforms the single modal one, I would have expected to see an\nanalysis of the instances for which the multimodal model does better:\nwhere does the visual information help, and does it make sense that it\nhelps there? The appendix contains some examples of images that are\nretrieved, but analyses stop there.\n\nOverall, I think there are some interesting ideas in this work:\naugmenting individual tokens (vs. whole sentences) with visual\ninformation is a new-enough idea, and it would be interesting to see\nif such an augmentation could help with text-only tasks. However, both\nthe experimental results and lack of model interpretation lead me to\nlead strongly towards rejection at this time: I simply am not\nsufficiently convinced that the model is meaningfully integrating\nvisual information to improve performance. I really do like the idea\nof this work, and hope that the authors update with baselines and\nanalyses (similar to those suggested here) that more directly\ninterrogate the question of multimodal integration.\n\nOther technical concerns:\n\n- The authors (e.g., in the abstract) claim that pretrained LMs only\n  have access to 1 sentence of context. But this isn't the case:\n  RoBERTa, for example, is pretrained with 512 token contexts. (see\n  S4.2 of the RoBERTa paper)\n\n- I think that presenting the performance of other methods (e.g.,\n  MT-DNN, GPT) detracts from the author's point. For these\n  experiments, adding in additional baselines seemingly serves almost\n  no purpose: the main comparison that \"matters\" is \"baseline\"\n  vs. \"ours\". Including, e.g., GPT performance invites the question:\n  why wasn't RoBERTa included? Why not T5? Why not other baselines for\n  MT that outperform UVR? Simply removing them probably makes the most\n  sense. Including the set of baselines that happen to just be\n  slightly worse than the author's proposed model seems both\n  distracting and misleading to me.\n\n\nPresentation suggestions:\n\n- There are a few words that seem to overclaim. E.g., in the abstract,\n  I think \"paradise\" could possibly be swapped to something more like\n  \"performant method\"; and, I think \"remarkable\" could be removed in\n  reference to tasks.\n\n- The authors motivate their multimodal model by invoking\n  polysemy. But it wasn't clear to me how multimodality explicitly\n  solves the single-sentence polysemy problem. Couldn't text\n  co-occurences similarly disambiguate? And if the full context is a\n  single sentence, is perfect disambiguation even possible?\n\n- \"For natural language processing (NLP) tasks, most text is\n  unlabeled\" --> \"For natural language processing (NLP) tasks, most\n  text isn't distributed with visual information.\" ?\n\n- Mikolov et al. 2013 is a good citation for distributed\n  representations, but there are many prior works that also deal with\n  vector representations for words (e.g., the ones cited in that\n  paper).\n\n- The authors tout that their method \"does not rely on large-scale\n  aligned text-image corpora\" but it does: specifically, Multi30K.  I\n  understand the distinction: that word-image associations are not\n  given explicitly (only sentence-image associations are). But that\n  setting has been redily examined in prior work (e.g., Karpathy et\n  al. 2014), so this factor doesn't distinguish from prior work.\n\n\nKarpathy, Andrej, Armand Joulin, and Li F. Fei-Fei. \"Deep fragment\nembeddings for bidirectional image sentence mapping.\" Advances in\nneural information processing systems. 2014.\n\n\n~ UPDATE RE: AUTHOR RESPONSE ~\n\nThank you for adding these baselines! I appreciate your additional experiments, and I did update my score a bit. But, my feelings are summarized in my response to the AC, which I have copied here. I think this work asks interesting questions, and points towards a potentially exciting result. But because the implication of the work /is/ so exciting, I can't bring myself to raise my score without additional concrete evidence the model is really leveraging visual information to solve language-only tasks better. The \"Random\" baseline is a good step, but, IMO, is still not quite enough. For more about my thoughts:\n\n~\n\nI appreciate that the authors have updated their work to include a \"random\" baseline (which pairs random images with words) and also a \"params\" baseline (which has the same number of parameters as the proposed method); these baselines were sorely missing in the original work and I think the updated version is much better. The +VG method indeed seems to outperform these baselines by a small absolute margin. I have updated my score accordingly.\n\nMy main concern remains: the claims the authors are making are, in view, still unexplored. The authors hint at \"grammatical ambiguity\" (which I think is the same as syntactic ambiguity) being resolved via multimodality, but provide no evidence that their model is actually doing that: as I mentioned in my original review, \"I would have expected to see an analysis of the instances for which the multimodal model does better: where does the visual information help, and does it make sense that it helps there?\"\n\nIf indeed the model is leveraging semantically-meaningful visual reference to accomplish language processing tasks (as the authors claim) that would represent a significant advance in vision+language research. But because such an advance would be such a significant milestone, relying solely on accuracy measurements on GLUE/MT tasks doesn't seem sufficient: more concrete evidence that it really is the vision+language grounding that matters is needed beyond high-level intuition. This evidence is not provided in this draft, but I am hopeful the authors can examine more closely why their model is working in future submissions, because I do think this is an interesting direction.\n~",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The idea is interesting, but there are flaws in experiments and model design. ",
            "review": "\n1.Objective of the work: This work proposes to integrate external visual knowledge into the contextualized word representations (CWP). The goal is to equip CWP with the ability to disambiguate and make CWP \"more contextualized.\" \n\n2.Metrits of the paper: Most multimodal researches focus on solving multimodal tasks, like cross-modal retrieval, VQA, etc. In comparison, this paper (along with some others) use multimodal information to address mono-modal tasks (language understanding, NMT). And the proposed method is general that can be applied to many downstream tasks. From this perspective, this paper is interesting and will benefit the community.\n\n3.Concerns:\n\n3.1 One of the authors' major claims/motivation is that their model is better in disambiguating by leveraging word-level multimodal information. But I did not see any analysis or case studies to substantiate this claim (except section 4.3 which we will discuss later). This makes it unclear to me where the gains on NLU and NMT tasks come from. For instance, Table 2 reports a large improvement(>10%) on RTE over BERT, which is impressive. But personally, I didn't find any ambiguous instances in RTE dataset that can be resolved by introducing visual knowledge. \n\n3.2 Another key concern about the paper is the lack of novelty in the methodology. The proposed method seems like a straightforward extension of [1], with two minor changes: (a) replace the decoder with MLP to solve NLU tasks. (b) consider images on word-level instead of sentence-level (see 3.3 below). The rest of the model is exactly the same as in [1], but this strong connection has not been discussed when describing section 3.2 \n\n3.3 The reason for doing word-level lookup, according to the authors, is that the sentence-level lookup suffers from irrelevant noise. However, by inspecting the examples in Appendix A.1, it seems that although word-level lookup injects more diversity, it also brings more noise. For example, in the second \"glass\" picture, the glasses is hard to notice. And since the author is using pooled representation from ResNet, the extracted vector can be very noisy (i.e. the extracted features will be related to more \"noticeable\" objects in the picture like \"man\", \"black suit\", rather than the desired \"glass\"). One possible fix I can imagine is to use visual grounding toolkits like [2] to detect visual objects before feature extraction (as an example, see [3]). \n\nAlso, as reported in Table 3, sentence-level lookup is compatible with the proposed method on WMT'16 EN-RO and even superior on WMT'14 EN-DE, which somehow supports my observation. \n\n3.4 The results of NLU tasks (Table 2) are encouraging, but there is a substantial lack of details to help appreciate the results. For instance, it's not clear how the NLU model is trained. Did you use pre-trained BERT as your model's text encoder, or train the whole model from scratch? My understanding is that you are using the former approach?\n\n3.5 For Table 1, it seems that only the challenge set was created with ambiguous words, but not the training set. Thus is not clear how the model that is trained on a \"not-ambiguous\" set learns to disambiguate. My guess is that the training set might also contain some ambiguous sentences. It would be great if the authors can add a few case studies to show how vision can help disambiguate translation. \n\n\n4.Other suggestions:\n\nThe example in Figure 1 is actually not a very good one. I understand the point the authors try to make here, but use \"apple\" to represent \"iPhone\" is kind of far-fetched. \n\nTypo in page 7. \"(described in Section 3 below)\" should be above? \n\n\n\n[1] Neural machine translation with universal visual representation. In International Confer- ence on Learning Representations,  Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao. ICLR 2020\n\n[2] A fast and accurate one-stage approach to visual grounding. Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. ICCV 2019\n\n[3] A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation. Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo. ACL 2020\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}