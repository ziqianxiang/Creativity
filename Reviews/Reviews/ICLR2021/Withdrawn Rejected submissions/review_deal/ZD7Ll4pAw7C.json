{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper works towards analysis to understand the difference -- and primarily the lack thereof -- between different pruning methods. The central observation is that the convolutional filters in a layer are not strongly correlated and -- if the weights of the layer are taken as a matrix -- then the covariance matrix is block diagonal. \n\nExtending this objective the regime of a large number of filters, then the matrix is approximately diagonal and all weights are -- approximately Gaussian and i.i.d. The point of this analysis is that under this assumption, norm-based metrics, particluarly $\\ell_1$ and $\\ell_2$, behave quite similarly.\n\nThe pros of this paper are the extensive evaluation and -- after revisions -- relatively clear text. The core analysis is nice to have elaborated in detail in the community.\n\nThe primary con of this paper is, as the reviewers point out, that there are limited conclusions to take away from this work. Specifically, a plausible default hypothesis is that different pruning criteria result in different pruning decisions. From the results in this paper, that still seems to hold with -- exception of the norm-based metrics. So, while this work does demonstrate that these norm-based metrics are relatively similar -- a nice clarification to see in the community -- the work offers limited comment on the broader space of pruning metrics.\n\nMy recommendation is Reject. Despite the strong empirical evaluation, the ultimate results offer limited clarification on the similarity of pruning metrics.\n"
    },
    "Reviews": [
        {
            "title": "Raises an important aspect, but there are still some reservations",
            "review": "**Summary:** The goal of the paper is to bring into attention that many norm-based pruning criteria used for structured pruning are very similar, in that their ranking of the redundant filters is highly correlated. The key ingredient is the CWDA assumption that filters in a particular convolutional layer are iid and approximately follow a Gaussian distribution, which is shown based on extensive statistical hypothesis testing. Based on this assumption, they prove that these pruning criteria are roughly the same. \n\n**Pros:**\nI like the nature of the question asked in this paper. These are fundamental questions, like how does the distribution of weights in a particular layer evolve and look like, etc, which do not get as much attention. So in that sense, this paper will be important for the pruning community (where there is a deluge of papers proposing a new criterion) to be aware of, and understanding this aspect they would be able to design a pruning criterion that actually matters. The paper is backed with extensive results testing their CWDA assumption and proving how the criterion are overlapping in their functionality. \n\n\n**Cons:**\nThere is an over-selling of claims, as this \"rethinking\" of criteria is only for norm-based methods, and there are lot of other established methods as well. Then this all holds for layers which are wide enough, but in the end for pruning these layers would not be as sensitive either (and layers like input, last fully-connected, or where there is a transition). \nAlso, at some other places like in the results of different pruning criteria, the experimental methodology and presentation is rather loose. I would have also liked to see at least the empirical results for global pruning because there even some out of these norm criteria might be better than others. \n\n**Detailed comments**:\n\n*Similarity between pruning criteria*:\n\n-First of all, Molchanov et al 2019 https://openaccess.thecvf.com/content_CVPR_2019/papers/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.pdf does look at the spearman rank correlations, so I guess you should also mention this in the context of \"blind-spot\". \n\n-Next the criteria that you compare are only based on (absolute/relative) norm of the filters. And, I understand that your main point is showing that many norm based methods are ranking filters in more or less the same way. However, there are several other established and widely used measures (such as Network slimming: Liu et al 2017; EigenDamage: Wang et al 2019). Hence, using the title “Rethinking the pruning criteria…” is too broad of a claim, and should be qualified to include ”norm-based”. \n\n-Also, the paper could have been made stronger by comparing these other measures, empirically at the least.\n\n*Empirical analysis*:\n- Table 4: It is not clear what is the pruning level here?  If you are pruning to small levels, then it does not matter as much of course (also known from results of other papers).  Here, all the methods recover performance after fine-tuning, which shows that this is not yet a good-test bed. The testing should be done precisely in the regime of high pruning ratios (> 90%, 95%), so that you actually see the difference. Can you perhaps provide results in such settings?\n\n- Further, can you share some correlation numbers for global pruning? I know that your focus is layer-wise pruning, which is perfectly fine. However, global pruning methods outperform layer-wise methods (at least this is known for unstructured pruning), and my intuition is that it might also hold for structured pruning (also, global methods would be better from the large minimum norm requirement of He et al 2019).\n\n- Table 1: I do not see much worth of showing that top 8 filters (out of > 100s) are similar for all criterions, as the few most redundant filters will be identified almost equally well by any sensible criterion. \n\n*Other important questions:*\nHow do you choose \\sigma_0 in testing? What is the smallest value that you can set, such that most of the tests pass? \n\"Need to be trained well enough\": Why is this a concern?\n\n*Related work discussion*:\n-Pruning: References go to as back as LeCun et al 1990, Hassibi & Stork 1994, etc. Otherwise it gives an impression that this whole area has just started.\n\n-\"one-shot pruning”: The definition of one-shot does not include fine-tuning. I think saying, \"one-shot pruning + fine-tuning”, is more accurate \n\n*On writing:*\n- There are actually quite a few typos, but still not too bad yet something not be proud of. Towards the last couple of pages, there are many more typos.\n\"about the pruning criteria as follow”; \n\"when the network exists too many filters” -> \"when the network has too many filters;\n“First, two theorems are shown” -> “First, we show the following two theorems\"\n\n- At many places, you have subscripts instead of superscript: \"In ith layer”\n\n- \"In CVPR 2019 Oral”: I don’t think this is a good way of writing or emphasising a paper. Otherwise, we will have things like, “The best paper award winner XYZ et al says, ….”, “The paper XYZ et al, which features a Nobel prize winner,  ….”. I hope you get the point. \n\n**Score**: Right now, owing to the above points, I have some reservations. If you can answer these questions better, I would happily increase my score towards acceptance. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments to validate two limitations of magnitude based pruning methods",
            "review": "This paper analyzes the current limitations of existing magnitude-based pruning methods. First, the paper focuses on the similarities between three methods and then focuses on the redundancy in large networks. The paper also analyzes the weight distribution for a well-trained network and propose CDWA as a way to prove this distribution.  My main concern with this paper is the contribution to the field. \n\nTwo main comments:\n1. If I understand correctly, the paper focuses on the distribution at the parameter level and shows it follows a Gaussian Distribution. If that is the case, that is a well-known result especially when the network is trained with a weight decay to regularize.\n\nThen, the paper shows the similarities between three magnitude based methods (L1, L2 and GM). I guess I am not very surprised the first two are similar in the structure and slightly different in performance as that is expected. The rank correlation is interesting tho. Would be more interesting to show the same comparison with importance based methods or other methods not using the magnitude only. \n\n2. In terms of redundancy, I am not sure I understand the point there. It is also known that training of neural networks leads to redundancy and that is the main reason for some particular postprocessing or compression algorithms based on that (see Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation -- Denton et al; or Compression aware training of Neural Networks -- Alvarez and Salzmann; and some others in the literature). Magnitude based approaches do not consider the interaction between parameters and therefore still lead to redundancy. Methods aiming at reducing redundancy usually do not provide good compression ratios as the architecture is harder to prune. \n\nThe paper also suggests l2 is regarded as importance. I think that should be reconsidered as L2 is the magnitude and there are many works using other criteria referred as importance (for instance gradient *magnitude). \n\nAll in all, not sure the real contribution to the community. \n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting insights into norm-based filter pruning but it lacks a broader discussion",
            "review": "## Summary\n \nThe paper discusses various baseline scoring mechanisms used for filter pruning that are norm-based and finds that none of the scoring mechanisms are particularly effective at pruning filters from CNNs. Moreover, all methods seem to perform very similar to each other. These conclusions are based on a theoretical (and experimental) analysis of the various scoring mechanisms under the assumption that trained filter weights follow a Gaussian-like distribution, which reveals that in this case the scoring mechanisms are insufficient to reliably discern the importance of filters since the resulting scores are very similar. The Gaussian-like assumption, which is termed \"Convolution Weight Distribution Assumption\" (CWDA), is validated through a large range of experiments on different architectures and data sets.\n\n## Score\n\nI enjoyed reading the paper and I found the resulting conclusions really interesting since it sheds light on some of the mechanisms behind filter pruning methods, which are not as well investigated as mechanism behind unstructured (weight) pruning. \n\nNonetheless, I think there a is couple of things that the authors should consider before the paper is ready for publication, namely the inclusion of more advanced filter pruning techniques [1, 2, 3] in the discussion and also a set of recommendations for the community that could help design more informative scoring mechanisms for filter pruning in future work. \n\nMore details are provided below. \n\n## Ways to Improve My Score\n\nTo improve the quality of the paper, I believe the authors should include the following two aspects: \n1. Additional filter pruning methods: There has been significant progress in recent years in developing more advanced filter pruning techniques, such as [1, 2, 3] including techniques that are data-dependent, i.e., consider the layer activation in computing the filter score. Do these methods suffer from the same inability to discern the importance of filters? Or are these methods already a stepping stone in the right direction towards building more meaningful pruning criteria? Having the baseline methods alone is not sufficient in order for the community to build on top of this work in a meaningful way. \n2. Recommendations for improved filter pruning methods in the future: As it stands now, the paper provides little guidance what future research into filter pruning should consider when designing novel pruning methods. I think this would be crucial though in order for the community to appreciate the paper. This is also linked to the first point I mentioned since some of the existing advanced filter pruning methods potentially already contain at least partial solutions to the problem presented here and could thus guide future research as well. \n\nI mention additional feedback that affects my score in the \"Weaknesses\" section. The \"minor feedback\" contains feedback that did not significantly affect my score.\n\n## Strengths\n\n* I think the conclusions are very interesting and confirm my own experience with filter pruning. A lot of the baseline approaches (i.e. the norm-based approaches) seem to work more or less the same, especially in a standardized prune+retrain setting.\n\n* The CWDA assumption is verified using a large set of experiments and the authors also discuss the limitations of the assumptions. This is really encouraging to see and I appreciate the rigor. \n\n* The theoretical analysis based on the CWDA assumption seems technically sound and is also back up by appropriate experiments. Figure 1 is really interesting, in particular, as it shows how small the variance of norm distribution is.\n\n## Weaknesses\n\n* The lack of additional filter pruning methods as stated above. The inclusion of those is crucial I believe as it may lead to some really interesting additional conclusions.\n\n* The authors discuss the limitations when CWDA does not hold. I wonder if they could dig a little deeper to see whether in those cases norm-based filter pruning is more effective at identifying important filters. \n\n* The writing in parts is somewhat confusing and as a results it's sometimes hard to discern the meaning. I believe the paper would benefit from an additional revision to ensure that it is accessible to a broader audience.\n\n## Other Minor Feedback\n\n* Typos:\n  * in (2) of the contribution list: \"...when the network exists too many filters.\"\"\"\n  * Page 7, bottom: \"..., which shows that the GM method also exist the same problem ...\"\n  * Page 8, top: \"In CVPR 2019 Oral, ....\"\n  * Page 8, Section 5.1, (1): \"...converges to a bad local minimal ...\"\n  * Page 8, Section 5.1, (2): ...are almost the layers that are ...\"\n  * Please check for other typos as well.\n\n* Is there a difference between $F_{i,j}$ and $F_{ij}$? I saw both notations in the paper and wasn't sure.\n\n* I didn't carefully check all the math but it seems to be reasonable. I think it would help potential readers to include high-level summaries of each theorem in the main part of the paper. That way the paper could be made more accessible to a larger audience.\n\n* Maybe it would be more useful to include the results of the statistical tests as part of a code repo that will be released upon publication. It feels quite difficult and slow to navigate many pages of tabulated results.\n\n * Maybe sort the indices in Table 8 differently. Then it might be easier to see the overlap between the chosen filters for each method.\n\n * What about fully-connected layers? Could the authors observe something similar in those cases? Note that \"filter pruning\" can also be applied to fully-connected layers where neurons corresponds to filters. \n\n## References\n\n1. [ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression](http://openaccess.thecvf.com/content_iccv_2017/html/Luo_ThiNet_A_Filter_ICCV_2017_paper.html)\n2. [Provable Filter Pruning for Efficient Neural Networks](https://openreview.net/forum?id=BJxkOlSYDH)\n3. [Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection](https://proceedings.icml.cc/paper/2020/hash/8b6a80c3cf2cbd5f967063618dc54f39-Abstract.html)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting but not good enough paper",
            "review": "### About weight distribution assumption\n\nThe authors assume that all well-trained filters are i.i.d and follow a Gaussian-alike distribution, plus the co-variance matrix is a diagonal matrix. In other words, each weight within the filter follows a Gaussian distribution and is i.i.d because their co-variance equals to 0. **PS: cov(x,y) = 0 is equivalent to independence for Gaussian distribution, though the conclusion does not hold for general cases.** \n\nHowever, CNN works because it takes advantage of the locality of images. In other words, these weights in filters should be highly correlated to extract features (though may not be linearly correlated), which seems to contradicte the assumption.\n\n\n\n### About contributions of the paper\n\nAs I understand, the contributions of the paper are mainly two folded: 1) Weight distribution assumption and its correponding conclusions. 2) The paper reveals the similarity of some previous pruning algorithms and finds they may not be good criteria.\n\nI am kind of worry about the reasonability of the first contribution (Please see last section for details). The second contribution does not seem to be sufficient for a formal paper. We may prefer some more materials. For example, why these findings are important? How do they inspire subsequent research? What should we do to avoid the problem in pruning? etc.\n\nTotally speaking, I think the authors analyze the problem from a very interesting perspective. It will become a good paper with some more improvement, but I do not think it has been good enough for publication now.\n\n### Updates after rebuttal\n\nAbout the contributions of the paper, it does provide some interesting points of view.\n\nThe authors revise the paper and modify their basic assumption. However, it seems to be a hollow assumption without any guarantee. The concerns are as follows: First of all, why the authors claim it as a diagonal matrix when actually observing it as a block-diagonal matrix? Secondly, considering the updated assumption is only tested on one layer of VGG and ResNet, I am really worried about its generality and reproducibility.\n\nTotally, the weight distribution assumption is the cornerstone of the paper, but the assumption seems to be not convicing. Specifically, the assumption in the first version of the paper is a paradox to some extent, and the revised version casually modifies the assumption without too much verfication. I have to decrease my rating to 4.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}