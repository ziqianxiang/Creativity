{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper gives a new algorithm for the CCA problem. The main idea of the new algorithm is to reformulate the matrices in the CCA problem as a product of three matrices: one orthonormal matrix, one rotation and one upper-diagonal matrix. The algorithm then performs remannian gradient descent to these components. The per-iteration complexity of the algorithm is O(d^2k) while the (local) convergence rate is O(1/t). Overall the reformulation is interesting and the algorithm seems effective in practice. On the other hand the convergence rate proof relies on local strong convexity and it's not clear why the algorithm converges globally (or even what is the radius of convergence locally)."
    },
    "Reviews": [
        {
            "title": "A stochastic linear CCA method for high dimensional data",
            "review": "1. Paper summary:\n\nThis paper proposes a method for solving linear CCA on high dimensional data. Linear CCA has a closed form solution. The solution requires a whitening step that costs O(d^3). This makes it not applicable to data in high dimensional spaces, e.g. representations learnt by deep networks. \n\nTo resolve the issue, the authors propose a reformulation of linear CCA which decomposes the transformation matrix U (V) into a product of three matrices. Those three matrices have the following properties:\n\n- Their initial values can be obtained by efficient streaming PCA on original view matrix X (Y). Streaming PCA costs O(d^2 * k) only where k is the top k eigenvectors.\n\n- They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold.\n\n2. Strong points of the paper:\n\nThe new linear CCA formulation justifies the rationale of batch CCA training.\n\nUnder Gaussian distribution assumption:\n- The absolute difference between correlation found by original linear CCA and stochastic one is bounded.\n- The convergence of the training process is proven.\n\nThe experiments are performed on different aspects:\n- Recovering groundtruth transformations on MNIST, CIFAR and Mediamill data sets.\n- Learning deep features on MNIST data set.\n- Improving fairness in deep learning by adding CCA term to the loss function.\n\n3. Weak points of the paper:\n\nThe theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution.\n\nMost propositions are from other papers.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The major concern is clarity",
            "review": "main contribution\n\nThis work offers a stochastic CCA algorithm based on Riemannian optimization approach\n\nStrength\n\n- The paper offers a theory-backed algorithm for CCA under the assumption that the two views are sub-Gaussian. The complexity-accuracy tradeoff of the algorithm seems to be appealing according to the experiments.\n\nWeakness\n\nOverall, the biggest concern is readability. The paper is very packed and many treatments seem to be unbalanced. Important details are missing, and proofs seem to be hastily. The paper may need some re-packaging and re-organization before its core technical contents could be easily followed.\n\n- readability. The biggest concern of the work is that it is very hard to read. This creates a lot of barriers in understanding key aspects of the paper, e.g., contributions, formulations, novelty of the proof, just to name a few. The key formulation and the definitions of the manifolds were not clearly defined. The equivalence of (1) and (3) was not clearly shown. The theorems were presented in a bit abrupt way. Even the algorithm does not appear in the maintext but the appendix (which is also hard to read). The color code is used in a way that is a bit confusing (does ICLR allow writing in different colors?).\n\n- Clarify about the contribution. It is hard to clearly see how much is the contribution. The proofs seem to be short and most of the proofs are presented using “propositions” cited from existing papers. If the authors think the contributions lie in reformulating the CCA problem as a PCA problem, then the reformulating part is perhaps the contribution. But from the current writing it is hard to follow how the reformulation comes through and how the reformulation enables using these existing propositions to prove convergence of the proposed algorithm.\n\n- It is unclear why the reformulation in (3) is a good approximation for CCA. Some attempts for justifying this were offered in Theorem 1, but it was based on the assumption X and Y are sub-Gaussian, which is at best a special case, even if the proof is correct (which, due to the current organization of the paper, is hard to read and fully understand).\n\n- “SO(n): group/manifold of nxn special orthogonal matrices.” what is the definition of “special” here? are they the commonly understood orthogonal matrices?\n\n- The paper has this upper triangular structure of the S matrices but this point seems to have no detailed explanation. If one understands U = \\tilde{U}S as the QR decomposition, then indeed S is upper triangular, but why is there a Q in the middle? Why is this Q useful?\n\n- the “high-level” description of the algorithm says the idea is to connect CCA with PCA using this reformulation, but this point was not clearly explained.\n\n- Complexity. From the current writing, it is very hard to see how the complexity of the algorithm is calculated. The gradients needed are tabulated in the supplementary materials, but it is very hard for a reader to directly see why the algorithm saves computational complexity.\n\n- It is also unclear how the convergence and convergence rate analyses come together. These parts may need to be elaborated.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices, a stochastic gradient-based optimization on a Riemannian manifold is provided reducing the computational complexity from $d^3$ to $d^2k$.\n\n\nStrength:\nCCA is a classic and still important method, especially in combination with deep neural networks (e.g., multi-view learnings). \nThe proposed method enables the applications of CCA to high-dimensional vectors with small memory.\nThis makes it easier to use CCA as an objective function of deep neural networks, which is trained on GPUs.\nExperiments show the benefits of their proposed method, in particular, the computational speed is 5-10 times faster than the existing method (MSG).\n\nWeakness:\nAlthough the authors claim that their proposed method captures more correlation than MSG, two of the three datasets in which their method is superior (MNIST and CIFAR) are not realistic setting (i.e., correlation between left/right half of images).\n\nQuestion:\nIs it possible to make a (numerical) comparison with Yger+2012, which reformulates CCA as an optimization on the generalized Stiefel manifold?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A successful approach to streaming CCA",
            "review": "The paper presents an approach to find canonical directions in a streaming fashion, i.e. without direct calculation of covariance matrices (which becomes hard when the number of examples is large). This solution to that task is not obvious, because the objective function of CCA, together with whitening constraints, does not allow simple additive decomposition.\n\nFirst, the optimization task is reformulated as a task over certain Riemannian manifolds, and a natural initialization is suggested. It is shown that under a certain assumption, this initialization is already a solution of good quality. Then, a natural minimization algorithm is presented, which is based on stochastic gradient descent on a Riemannian manifold. The key aspect of the algorithm is a combination of 2 types of gradient, the gradient for top-k principal vectors and standard gradient.\n\nThe experimental part shows that the algorithm successfully solves CCA in a streaming fashion. Also, it can be effectively combined with deep feature learning (Andrew, 2013) to find common features for multi-view representation learning tasks. Experiments look convincing.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}