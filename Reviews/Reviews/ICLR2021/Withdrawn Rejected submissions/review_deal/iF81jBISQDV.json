{
    "Decision": "",
    "Reviews": [
        {
            "title": "Needs additional work",
            "review": "Summary:\n\nThis paper proposes Sparta, a new activation function for neural networks. Sparta is designed to improve the robustness of models to adversarial attacks when using adversarial training. It aims to alleviate adversarial attacks by introducing attention, being dynamic and treating pixels differently depending on their position in images. Sparta takes the form of a neural network itself, thus requiring training. Experiments performed on CIFAR-10 and SVHN compare the proposed activation against different variants of ReLU and other previous methods, for both natural and adversarial training.\n\nPositive points:\n- Sparta seems to obtain similar or better results to state-of-the-art for the considered setups.\n- The proposed method also seems to transfer to some extent from one model architecture to another, as well as between datasets. One could thus potentially avoid retraining Sparta by using this technique.\n\nConcerns:\n- The idea behind this new activation and the concrete architecture trained seem a bit arbitrary and are not well motivated in the paper.\n- The experiments cover a limited setup. The evaluation should also include untargeted attacks, as these are harder to defend. Only small datasets are used; it is unclear how Sparta would scale to larger datasets and architectures. The paper focuses strongly on variations of ReLU, but other activations should also be considered (e.g., tanh). Similarly, all architectures are ResNets, thus failing to prove the generality of the proposed approach.\n- The paper is sometimes hard to follow, and some of the aspects are not clear. It is unclear how the proposed activation is trained. Some of the experimental setup and training procedures are also unclear (see questions below).\n- The computation complexity of the proposed activation function is not discussed. Since the cost of Sparta for both training and prediction seems elevated, this topic should be addressed in the paper.\n\nQuestions / comments:\n- While Sparta seems to be transferable across architectures, its performance does seem to  degrade in this specific case to the extent where it would lose its competitive edge over previous methods.\n- Does Sparta also transfer between architectures that are more distinct than two model from the same family (i.e., ResNet)?\n- In view of the computational cost, can Sparta scale efficiently to larger datasets and architectures?\n- Which one of the Sparta setups is used in the experiments presented in the main paper? How many Sparta layers are used and where in the architectures are they placed? Same question for transferability experiments.\n- Why does Tab. 5 not compare Sparta against the same baselines as Tab. 6?\n- Attention has been used previously for defending against adversarial samples [Goodman et al., 2019]; are these previous results relevant w.r.t. the proposed method?\n\nMinor comments:\n- The notations DPNet, CANet and SANet do not seem to be introduced.\n- Additional proofreading could also help.\n- Tab. 3 caption does not seem to reflect the content of the table.\n- Using bold fonts for the best results in each table would improve readability.\n\nReferences:\n- [Goodman et al., 2019] Improving Adversarial Robustness via Attention and Adversarial Logit Pairing, https://arxiv.org/abs/1908.11435.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Needs Improvements",
            "review": "The paper proposes a spatially attentive and dynamic activation function for Convolutional Neural Networks (CNNs), to improve CNNs' robustness against adversarial examples and classification accuracy on clean examples. Experiments also show that the trained activation function can be transferred across different models and datasets.\n\nStrengths:\n1. The idea is insightful and well motivated for improving the robustness of CNNs against adversarial examples.\n2. The experiments validate the transferability of the trained activation functions.\n\nWeakness:\n1. There are considerable extra learnable parameters in the proposed activation function.\n\n2. The number/percentage of parameter overhead is not reported in the experiments, which is very important when compared with the baseline activation functions. The extra model parameters may make most experimental comparisons unfair and hence less convincing.\n\n3. The technical presentation of the paper is relatively weak. Here are two examples.\n(a). Section 2.2.2 is one of the most important sections of the paper. However, it is poorly written and most details are vague and unclear. For example, how does the Softmax generate the parameters in Conv5? What's the meaning of the multiplication in Eq(5)? The authors should consider giving the precise formulations of the computations in this section.\n(b). By looking at Figure 1, I cannot draw any conclusion about noise removal as the authors have stated. How do the authors confirm that the pixels removed by SPARTA are noise?\n\n4. The proposed activation function is for CNNs only, not for \"general DNNs\". The authors should consider making it clear in the Abstract and Introduction sections, to avoid the over-claim of its usage in general DNNs. If on the contrary the authors do want to claim its superiority in general DNNs, then they would need to provide extra experimental results to support the claim.\n\n5. The number 23.84% in Table-7 is different from the number 14.47% in Table-6, but they are exactly the same experiment. Why? (Note that for ResNet-18, the number 15.48% in Table-7 and Table-6 are consistent, so I believe for ResNet-34 they should be consistent as well.)\n\n6. From the top1 error on clean images in Table-7 and Table-8, we can see that Sparta with transferred and fixed parameters performs even better than Sparta trained on the target model/dataset. Why? It seems unreasonable to me.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new activation subnetwork to improve adversarial robustness but more analysis and justifications may be needed",
            "review": "This paper argues that existing ReLU activation makes adversarial noise easily propagated and also enables white-box attacks to easily optimize the solution with gradients. To address this problem, this paper proposes to design a new activation function that learns spatial attentive maps across all channels and channel attentive maps across all pixels, in order to achieve higher robustness and accuracy. \n\nPros:\n* This paper proposes a new activation function consisting of a subnetwork to improve adversarial robustness. \n* The paper demonstrated a reduced error on adversarial images with adversarial training, while the clean error remains similar. And it is also shown that the trained activation has an ability to transfer between different networks and datasets. \n\nCons:\n* The motivation of the new activation function is not empirically justified beyond test accuracy, and deeper analysis on its effect may be needed. Can you empirically analyse if you add some noise to the input and it is now harder to be propagated to deeper layers? Can you also analyse how the behavior of gradients changes under white-box attacks when the model has different activations?\n* It is not convincing enough whether the improvement comes from the activation design or just extra parameters in the activation subnetwork. In Table 2, the model with the new activation has an improvement on clean accuracy with standard training. This raises a concern whether the advantage of the new activation really targets robustness.\n* In terms of writing, some arguments in the paper do not seem to be accurate. In Sec. 3.2, it is said that “On the other hand, the feature denoising method needs to add new blocks to existing DNNs, requiring extra adaption costs, which are often non-trivial and expensive”, but it looks like the new activation proposed in this paper is also a new block/subnetwork. In Sec 2.3.2, it is said that “SPARTA achieves lower top-1 errors under all PGD attacks in the cases of adversarial and standard training” compared to SPARTA-w/o-DPNet, but this is not the case according to Table 2 (SPARTA achieved higher error on PGD-50, 72.55% v.s. 72.35%).\n\nAdditional comments:\n* How are $\\phi_{\\theta_s}$ and $\\phi_{\\theta_c}$ computed respectively? It seems that they are not clearly stated in Sec. 2.2.2 and not labeled in Figure 2. \n* In Table 7, why are the errors on PGD-30 and PGD-50 so low in the last row (42.23% and 46.40%)? Meanwhile the clean error is much higher than those in other rows. Is it an issue in experiments? \n* Can you also include results on different perturbation radius, e.g., 8/255 on CIFAR-10?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good Submission overall. Details could be improved.",
            "review": "## Overall\nThis work presents a novel SPARTA activation function to replace the vanilla ReLU to make the existing neural networks more robust to the white box adversarial attacks. The SPARTA takes the advantage of dynamic conv units to utilize the channel, spatial attentive and dynamic predictive properties. Experiments show that the SPARTA activation helps to improve the robustness of ResNet18 on different datasets and it also shows the validness of trained SPARTA on information tranferring across datasets. \n\n## Pros\nFor readers, this work claims the main idea of SPARTA with mathematical description and straight-forward figures. Beyond the existing dynamic activation function, the SRARTA includes the attentive strategy to reduce the impact of minor noise caused by the defenseless attribute of ReLU. The experiment results validate such idea with ResNet18 and ResNet34 on CIFAR-10 and SVHN under various adversarial attacks settings.\n\n## Cons\n1) Since the main idea of SPARTA is to design the dynamic and learnable activation structures to avoid the attack from the white-box adversarial attacks, such approach must involve more learnable parameters and make the original neural network deeper. It is possible that a deeper neural network strategy will make the neural network more robust against the attacks. It is shown by the paper that a deeper ResNet34 performs more robust compared to ResNet18. Therefore, I would suggest the authors to make the comparison between SRARTA included DNNs with the similar parameter-size or layer-depth vanilla ReLU or other activation function included neural networks. Such comparison will make the work more clear about how much the SPARTA improves the robustness besides the impact of depth of DNNs.\n2) The claim of the SPARTA stands on the defenseless property of ReLU. The readers would expect the SPARTA performs well with standard training for DNNs. However, the later results still show that the adversarial training is needed for a meaningful defense against the adversarial attacks. Some additional analysis about this part would avoid the misleading for readers. \n3) Since the analysis only relies on the PGD attacks, how about the attacks results for other gradient-based white-box adversarial attacks? How about other non-gradient based white-box adversarial attacks? More adversarial senarios will make the results more convincible since PGD is just one of them.\n4) Both SVHN and CIFAR-10 only involves the small size images. Will the image size affects the results of defense? For larger images, the adversrial noise will hide in the image with more details. How does the SPARTA perform on images with more visual features? How about the performance of SPARTA on ImageNet?\n5) Others: Highlight the best and the second best results in each table would help the analysis to be more intuitive.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}