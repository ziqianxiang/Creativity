{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper was discussed by the reviewers that acknowledged the rebuttal and the authorsâ€™ responses. In particular, they appreciated the fact that some of their concerns were alleviated (e.g., going beyond the single ImageNet evaluation).\n \nMore generally, while all the reviewers thought that the problem tackled by the paper was of clear interest (i.e., full end-to-end auto-ML encompassing DA, NAS and HPO), they still expressed concerns (even after the rebuttal), in particular:\n\n* _Clarity of the methodology_: None of the reviewers could clearly and fully understand the mathematical formulation of the joint optimization, leading to a series of questions regarding the confusing usage of the training/validation set in the experimental setup. This unfortunately made the assessment of (some aspects of) the paper speculative for the reviewers.\n* _Comparison with AutoHAS_: AutoHAS and DiffAutoML are obviously related methods. Even if AutoHAS has weaknesses compared to the proposed approach DiffAutoML, e.g., discretization of the continuous hyperparameters and no tuning of DA, it is still meaningful to carry out an actual comparison (possibly normalized by the different costs at play since the authors have highlighted the different memory overheads). Though the listed weaknesses of AutoHAS _should_ play in favor of DiffAutoML,  a proper experimental comparison would better support that claim.\n\nGiven those remaining concerns and the overall mixed scores, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission.\n"
    },
    "Reviews": [
        {
            "title": "Overall this paper is clearly written and easy to follow. The motivation makes sense and result support the claims. ",
            "review": "Summary\nThis paper focuses on achieving automated \"from data to model\" including different components in modeling, namely data augmentation, Neural Architecture Search, Hyper Parameter Optimization. The proposed approach first use data augmentation to select the data argumentation transformation. It tries to select examples which incurs higher training loss for the model to address hard examples. Then use the DAG for neural architecture search. Given the data and architecture, it then alternatively update the model parameter and hyper parameter. The overall proposed framework is end-to-end. Experiment on ImageNet shows slight performance improvement over existing approaches. The authors also conduct ablation study to show the effectiveness of jointly modeling the three components (data augmentation, neural architecture search, hyper-parameter optimization).\n\nStrengths:\n1) This work considers three important components in modeling process including data argumentation, HPO, and NAS. The different components may interact with each other to impact the performance.\n2) This paper is clearly written and easy to follow.\n3) The experiment shows performance improvement with less time needed. \n\nWeakness\n1) The experiment is limited with only 1 task and data. Is it possible to show the results on more than 1 dataset? Otherwise, the result might still be dataset specific. However, the proposed approach should not be \n\nQuestions\nWill the proposed approach generalize beyond computer vision task?\nWhy are the data augmentation and neural architecture search grouped together? I was wondering what will happen if you group neural architecture search and hyper parameter optimization first?\nHow should I interpret figure 3? How could you demonstrate that the selected architecture by the proposed approach is better or make sense?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Incremental paper with limited results",
            "review": "Some of the choices that have to be made when training a neural net based image model are: type of data augmentation, architecture of the neural network, and other hyperparameters such as regularization and optimization hyperparameters (e.g. learning rate). Optimizing all of these is a challenging problem, NAS deals with architecture but ignores the others. More general hyperparameter optimization techniques such as Bayesian Optimization struggle with the dimensionality of the architecture parameters. And optimizing them independently might lead to local minima, and/or be slow.\n\nOne previous work (AutoHAS) already combined NAS with the ability to add differentiable hyperparameter. In this work data augmentation is incorporated as well and tuned jointly with the rest of the architecture and the other hyperparameters.\n\n**Their main contributions** of this paper are: 1. The authors describe a NAS+differentialble hyperparameter tuning technique that also include data augmentation, thus extending the setting somewhat compared to previous results 2. They provide experimental results on ImageNet\n\nThe paper is reasonably clear, and while some of the ideas are interesting (such as how to incorporate data augmentation as a differentiable NAS hyperparameter) I don't think this work should be accepted mainly because:\n* They describe only incremental improvements. Thus no major contribution from the theory/new tecniques part of the paper.\n* Limited experimental results, definitely not strong enough to compensate for the previous point. There is no comparison to AutoHAS, which appears to be the closest method, according to the paper itself. And most results are on ImageNet only. If the paper wants to provide a technique for automatically generating vision pipelines, stronger arguments and experimental results should be brought forward for why this will work on new problems and not just on ImageNet (for witch we can just take one of the existing pretrained models).\n\n\n##### Questions/comments: \n\nIn the results, it seems that the competing method use no data augmentation at all, is that correct? If so it would be more fair if the competing method use untuned but reasonable data augmentation. \n\nDoes baseline in table 2 contain any data augmentation? It's not really clear?\n\nThe results are the average over how many repetitions? It would be nice to know the number of repetitions and standard dev/error, or if a single one at least have the source code available for better reproducibility.\n\nThe validation set is used for selecting hyperparameters, thus should not be used for comparing methods, was a separate test set used for accuracy? It would be good to describe this more clearly in the paper.\n\nI would remove the epoch based plot, what we care about at the end is time.\n\n\n##### Typos:\n\nSection 1\n\"We first jointly optimize three different AutoML components, including\" -> you might want to remove \"including\" if you list all three\nSection 3.1\nFor t-th iteration -> for the t-th iteration",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "DiffAutoML: Differentiable Joint Optimization for Efficient End-to-End Automated Machine Learning ",
            "review": "Summary and contributions: \nThis paper proposes a joint differential search method to optimize data augmentation, discrete architecture choices, and hyperparameters. \n\n\nStrengths:\n1). This paper proposes an automated method to search for data augmentation, hyperparameters, and architectures using gradient descent, which is simple and easy to integrate. \n2). It achieves co-optimization on different components in one-stage.\n3). A modest improvement upon SOTA mobile models.\n\nWeaknesses:\nIn the optimization, the proposed method naively uses validation data to optimize the hyperparameters, and use training data to optimize architecture weights, data augmentation weights, and network weights. Wouldn't the architecture choices and data augmentation overfit to the training data in such case? Note that there exists a discrepancy in the optimization direction between NAS and  DAS, where NAS aims to minimize the training loss (e.g. cross entropy) while DAS tends to increase the training loss. Therefore, trying to optimize NAS and DAS under a single objective is intractable. The objectives for the jointly optimization needs to be carefully designed so that the optimization for NAS and DAS can contribute collaboratively to the final testing performance.\n\n\nCorrectness: Are the claims and methods correct? Is the empirical methodology correct? \nI am uncertain the method appears to be correct.\n\nClarity: Is the paper well written? \nYes, although a few aspects could be improved (see feedback).\n\nRelation to prior work: Is it clearly discussed how this work differs from previous contributions? \nYes.\n\nReproducibility: Are there enough details to reproduce the major results of this work? \nNo code is provided.\n\nAdditional feedback, comments, suggestions for improvement, and questions for the authors: \nClearly describe mathematically how the proposed method contribute collaboratively to the final testing performance including its generalizability. The mutual dependency between NAS and DAS arises from that NAS is based on the training data modified by the augmentation policy generated by DAS and the DAS is based on the network generated by NAS. Additionally, provide ablation studies on changing one single component while fixing others. \n\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper for fully differentiable autoML pipeline as long as some concerns on empirical evaluations are resolved.",
            "review": "**Summary**\nThis paper shows how the complex autoML pipeline for neural networks can be trained in an end-to-end manner by combining existing methods. By using backpropagatable discrete sampling methods (Gumbel softmax), input transformed by data augmentation is seamlessly embedded in full backpropagation flow. And a differentiable architecture search algorithm is used, which also incorporates architecture search in full backpropagation flow. On top of this differentiable procedure, an alternating optimization is introduced to train network parameters and hyperparameters.\n\n\n**Strengths**\n1. The paper shows that end-to-end automl is possible in a fully differential way which allows better models with fewer resources.\n2. Even though, intuitively, this joint optimization should perform better than non-joint ones, making such training stable does not seem trivial. The authors appear to successfully have taken care of it.\n\n\n**Weaknesses**\n1. Even though 2 hyperparameters are considered in the ablation study, its capability and stability with a moderate number of hyperparameters are not well-addressed (in the main experiment only 1 hyperparameter is considered). And the use of a validation set only in hyperparameter update step make me question that this hyperparameter optimization component may cause some issues such as training instability. Can the authors share some thought, experience, and intuition on this?\n2. It is questionable whether the comparison in Figure 2 is fair since DiffAutoML utilizes validation data in its hyperparameter optimization step whereas DSNAS seems to not use validation data.\n3. Similar to above, the numbers for DSNAS in Table 1 seem to be taken from DSNAS paper (Table 3) which are numbers from the validation set.\n\n\n**Recommendation**\nThe paper tackles interesting and practical problems and shows the proposed method outperforms baselines. My main concern is whether DiffAutoML uses validation data that is not used by other baselines, which could be a reason for better performance. The concern on considering on a small number of hyperparameters is a minor one if some explanation can be added. As long as the concern on the validation data access is resolved then I will be willing to increase my score favoring acceptance. However, in its current form, I cannot stand for it.\n\n\n**Questions**\n- The numbers reported in Table 1 and Table 2 are test performances?\n\n\n**Additional feedback** (Irrelevant to the decision assessment)\n- Since the most of NAS component in the full pipeline is from DSNAS, it would be better for readers if this is detailed in Related Works or Appendix.\n- At 4th line in Algorithm 1, using the word 'parent' network is more consistent?\n- In the last line in Abstract, en-to-end -> end-to-end\n- At 3 lines above eq. 3. ont-hot -> one-hot\n- In the last paragraph in Introduction, in the 4th line from the last, Differentiable -> differentiable",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}