{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper introduces the new task of few-shot semantic edge detection by adapting existing datasets. It proposes a new method which is compared to a baseline.\n\nPros:\n- Clear writing. \n- Extensive ablation experiments.\n- Good architectural choices.\n\nMixed:\n- The value of the new task raises a mix of opinions. For example R1 sees it as a \"relevant problem, and is well suited for few shot tasks\", but R2 finds is very similar to few-shot segmentation. I think a more interesting version of the problem (that would also create more separation to few-shot segmentation) would be to also consider internal edges, not just \"semantic boundaries\". For example the original BSDS dataset has pure edge annotations. \n- Besides the task, another novelty of the paper is the proposed multi-split matching technique, but while it is well demonstrated empirically (as backed by additional results given by authors in rebuttal), R3 would like to have seen \"theoretical or analytical reasoning\" and R1 says it is an \"ad-hoc technique\".\n\nCons:\n- the PANet+Sobel baseline. All 4 reviewers are unhappy with this baseline: 3 of them find it unfair because of the non-standard edge thickening employed and 2 think there would be more recent and better baselines. The authors provided a rebuttal arguing that their GT edges are \"not too thick to be unfair\" but two of the reviewers mentioned they remained unconvinced -- R1 hopes \"the authors will work on cleaner evaluation of the baseline\" and R4 find the baseline \"still unconvincing in the revised version\".\n\nOverall the paper would benefit from one more iteration focusing on the evaluation procedure to be convincing and impactful.\n\n"
    },
    "Reviews": [
        {
            "title": "A straghitfoward combination of few shot segmentation+edge detection ",
            "review": "\nA. Summary:\nThis paper works on few-shot semantic edge detection. Instead of dealing with the problem in a single stage, the authors decompose the problem into two stages.  First, a few-shot segmentation stage, where the foreground and the background probability are estimated via attention with the foreground and the background prototype (averaged feature vector on the foreground and the background region). Second, the feature maps from the encoder are masked by the attention map and sent to the decoder to generate the final edge-map.\nIn the experiments, the authors modify two existing datasets for evaluation.\n\nB. Strength:\n1.  The method is clearly described and easy to reproduce.\n2. Enforcing the group-wise similarity in sec.4.2 is a reasonable regularization.\n3. The two-stage decomposition separates the high-level sub-task (semantics) from the low-level sub-task (edge), which makes the training easier. \n\nC. Weakness:\n1. The attention map preserves the internal region of a class. However, this paper is about semantic edge detection, where the internal region does not contain any edge. So the author can try to process the final attention map by distance transform, which only focus on the region around the edge of the attention map and potentially further improve the results.\n2. The comparison with previous methods, especially the PANet+Sobel is unfair. The semantic response map corresponds to the attention map in the proposed method, so the author should also use sobel operator on their attention map to have a fair comparison, which can also serve as an ablation study to verify whether the attention map is one of the major contributions to the edge quality.\n3. Another ablation study worth to be investigated is to apply the attention map directly on the edge map, which will further sperate the high-level sub-task(semantic attention maps) from the low-level sub-task  (edge-detection). \n\nSome minor issues:\n1.  The attention map in Fig.2 (positive is black)is inconsistent with the rest of the paper (positive is white).\n\nD. Justification of the score:\nI vote for the current score because this paper lacks sufficient insights and novelty both algorithmically and conceptually. It also contains some issues in evaluation.\n\nE. Expectation for the rebuttal:\nI hope the authors could address my questions in C.weakness.\n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This work poses a new few-shot learning task, i.e., few-shot semantic edge detection, and proposes a Class-Agnostic Few-shot Edge detection Network to tackle this problem. In addition, two new datasets, FSE-1000 and SBD-5i, were introduced to evaluate the performance.",
            "review": "Pros:\n1 This work is well written and easy to understand.\n2 Extensive ablation experiments are performed to verify the effectiveness of the proposed modules in the CAFENet.\n\nCons:\n1 This new task is very similar to the few-shot segmentation. What are the advantages of this task over few-shot segmentation? The significance of this new research is doubtful. \n\n2 The proposed method was named “class-agnostic”. However,  it seems the basic setting is exactly the same as few-shot segmentation, i.e., if I want to segment the horse in the given query, I need to provide a hose image and its mask. So, I think the method is class-aware rather than class-agnostic.\n\n3 In my opinion, the main technical contribution of this work lies in the split-wise matching, which is employed to produce better segmentation masks. It is hard to find novel/specific designs in the edge detector part. So, I consider that the performance will significantly rely on the predicted segmentation masks, making this new task back to few-shot segmentation. \n\n4 Using PANet + Sobel as a baseline is unfair. Authors should replace the few-shot segmentation branch with PANet and more latest advanced few-shot segmentation methods for comparison. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "a problem of few-shot semantic edge detection introduced",
            "review": "Summary:\nThis paper introduces a novel problem of \"few-shot semantic edge detection\" where semantic boundaries are to be learned/detected with a few labeled samples. In order to remedy the issue of label sparsity within the few-shot scenario, the authors have leveraged the use of the segmentation process which provides the semantic information to the edge detector. They have also incorporated a meta-learning approach, namely Multi-Split Matching Regularization (MSMR), to avoid the overfitting when high-dimensional embeddings are used for feature matching.\n\nPros:\n- The motivation was effectively described and the problem was well-defined which made it easy to read. \n- Proposed architecture smartly inherits many previously introduced components (e.g., prototype-based metric learner)and combines them into a well-working framework to train a few-shot edge detector.\n- Quantitative results show reasonable edge detection performance\n\nCons/Questions:\n- The authors constructed the baseline by combining PANet (ICCV'19) with the Sobel edge detector while disregarding some of the better/recent approaches shown below:\n*  Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. (CVPR'19)\n** Prototype Mixture Models for Few-shot Semantic Segmentation (ECCV '20)\n*** Part-aware Prototype Network for Few-shot Semantic Segmentation (ECCV '20)\nWould the proposed approach still be effective than the combined version of Segmentators+Sobel if the segmentors were updated to more recent models?\n\n- Where does the proposed approach fit w.r.t the previous approaches mentioned in Section 2.2 and 2.3?\n- Theoretical or analytical reasoning to support the effectiveness of MSMR would better convey the authors' claim\n\nMinor comments:\n- There might be some incorrect labels in Figure 3. In this figure, the diagram on the left is not supposed to be split-wise matching, right?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New task of Few-Shot Edge Detection along with a proposed method CAFENet",
            "review": "This paper proposes the few-shot edge detection task, which is similar to few-shot segmentation but for the dual task of detecting semantic edges. For the task, the authors construct datasets and experimental settings constructed from existing edge detection dataset (BSD) and a few shot segmentation dataset (FSS). For the proposed method, the authors:\n\n1) Propose a meta-learning strategy based on prototypical learning and using intermediate low-res segmentation prediction to generate attention maps for few shot edge detection\n2) Propose using a multi-split matching which performs prototypical matching (for fg and bg) using many splits of the high dimensional feature\n\nThis paper is the first to propose the task of few shot semantic edge detection (dual task of few shot segmentation, which has been explored before). This is a relevant problem, and is well suited for few shot tasks instead of segmentation since learning a boundary can be easier (modelling high frequency info) as compared to modelling interior pixels (often containing low frequency info) that has to be done in few shot segmentation. The method section is also clearly written. \n\nI have **two major issues** with the paper in its current form:\n\n1. Results of PANet + Sobel filter: \n- The dataset constructed in this paper thicken edges by a certain radius (as explained in the paper and in the supplementary material) to reduce the time complexity of computing ODS. Quote - \"Moreover, evaluation with non-zero distance tolerance requires additional heavy computation. This becomes more problematic under few-shot setting where the performance should be measured on the same test image multiple times due to the variation in the support set\". \n- However, PANet + Sobel filter would only generate thin edges and there is no mention in the paper of the authors thickening its output edges according to the dataset (whereas thick edges are used to train CAFENet, so it has learned to produce thicker edges). Quote - \"For each split of SBD-5i, we meta-train the PANet on training classes using the segmentation labels. In evaluation, we obtain the edge predictions of test classes by applying the Sobel edge detector on the segmentation masks as done in (Acuna et al., 2019), and compare the predictions with the edge labels of test classes\"\n\nThis evaluation mismatch is a serious flaw and I would love to either get an explanation for why this is actually correct or updated results on these.\n\nGenerally, my view on this is that the evaluation should follow previous methods and should be done only on *exact* ground truth edges. Since exact ground truth edges are unavailable, using a error threshold makes sense. Evaluating with thicker edges as ground truth unfairly up-scores methods that generate thicker edges themselves, which are definitely not desirable. Therefore, I don't think the dataset with thickened edges as proposed is viable to benchmark improvements in the future for few-shot edge detection. \n\n2. Multi-Split Matching Regularization - The proposed MSMR technique is well ablated for the different choices of its hyperparameters, but at the end of the day is still an ad-hoc technique. What other design choices could be used? The proposed splitting of the vectors into smaller vectors is basically a fixed set of projections from the original feature space to a lower dimension where the different projections discard a lot of dimensions. Is this the best choice? There is a lot of theory behind random projections -- how would they work here? Instead of using K splits into N/K size, would K random projections into N/K size perform better? I believe this needs to be ablated more, and MSMR needs to be grounded more in existing work. \n\nOverall, I believe 1. is a serious flaw and 2. is something that would make the paper much more attractive to a reader and provide more insight into why MSMR results in additional improvements. I am rating this paper at a 4 now and hoping that my assessment of issue 1 is a mistake. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}