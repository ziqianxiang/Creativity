{
    "Decision": "",
    "Reviews": [
        {
            "title": "a more comprehensive study is needed",
            "review": "This paper proposes to jointly learn image and caption embeddings where the embeddings are in the form of Gaussian distributions rather than vectors.  The presentation is easy to follow, yet some parts are missing. \n\nSection 2 talks only about the tecnnical details of the proposed method, without much description about motivations behind different designs. Moreover, I think a more comprehensive study is needed to better understand the motivation and verify the effectiveness of proposed method. Below are detailed concerns:\n\n1. how do you use the learned Gaussian distributions of different images/captions to conduct retrieval in the experiments? Do you use the mean \\mu and conduct retrieval as in the vector case? Or do you directly use the distributions and compute the similarity as suggested in the paper? Or do you sample from the distributions?\n2. one of the motivations of using Gaussian distributions instead of vectors is handling the issue of polysemy. There is no corresponding analysis in the experiments to support this motivation.\n3. The proposed method has only compared to VSE++, which is a method proposed in 2017. More recent baselines should be compared.\n4. More qualitative analysis is needed to indicate what is improved in the learned representations compared to the baseline, besides quantitative results in Table 1&2.\n5. More discussion on the motivation is needed in Section 2. For example, why in the Gaussian Caption Embedding, you need to caption encoders with no shared parameters, while in the Gaussian Image Embedding, you need two fully connnected layers? ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Embedding images and captions as Gaussians in latent space for retrieval",
            "review": "\nSummary\nThis paper considers the embedding of images and captions in a joint space, and assess the effectiveness of using embeddings in the form of Gaussians densities rather than points. \nThis work builds upon Gaussian embeddings that have been used for word-embedding, see eg (Vilnis & McCallum, 2014).\nExperiments compare using the Gaussian embedding for either captions, images, or both. To compare points and Gaussians Mahalanobis distances are used. \nTo compare two Gaussian distributions three metrics are compared: Kl-divergence, minimum KL-divergence, and 2-Wasserstein distance.\nThe proposed retrieval approach is compared to a cosine-similarity baseline, which uses the same image and caption encoder.\nA pre-trained ResNet is used to encode the image, and one additional layer is used to map this to the mean and variances of the Gaussian embedding.\nFor the captions the same procedure is followed, but using a recurrent GRU-based encoder. \nExperiments are conducted in with and without fine-tuning the image encoder using the MS-COCO dataset. \nOverall, the best retrieval results are obtained using the Gaussian embedding for both the image and the caption, and using the 2-Wasserstein distance.\nIn many cases, the improvements over the baseline are fairly small (under 1% improvement in recall in 8 of the 12 reported recall levels). \n\nComments\n\n1 - Currently I find that the paper does not clearly explain how it differs from the cited previous work\n(Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017; 2018; Athiwaratkun et al., 2018; Mukherjee & Hospedales (2016)).\nThe paper would clearly benefit from a dedicated \"Related work\" section, to place the presented work in context.\n\n2 - The specific choice of metrics in Section 2.3 was not clear for me. They're fine, but why were specifically these three metrics used? How about Jensen-Shannon, or symmetric KL-divergence, for example? \n\n3 - In Table 1 and 2 the differences with VSE++ are not that big in many cases. Has there been a correction to compensate for the additional layers/parameters in the proposed approach? Are extra layers added and learned also specifically for VSE++? If not then the gain might be due to training for retrieval (in Table 1), rather than the Gaussian embeddings and Wasserstein distances. For table 2 part of the improvement might be coming from additional capacity.\n\n4 - The qualitative examples in Figure 1 are surprising. How is it that the cropped image yields a retrieval of the detailed description of the full image? Is this expected behaviour? Is the retrieval performed over the ground-truth captions of the test images? In that case the test set seems too small to make sense, given such matches come up. \n\n5 - Using one or two additional datasets would strengthen the empirical data supporting the success of the proposed approach.\n\n6 - To me, Image-Caption retrieval is not the most convincing testbed. For captioning, text generation methods are stronger than retrieval-based ones, and are able to generalise to captions not seen during training. So why focus on this particular task? Perhaps a document-retrieval task would be more convincing? \n\nOverall rating\nAlthough the topic of probabilistic embeddings fits well with ICLR, I believe the current paper has several limitations that make it not suitable for publication at ICLR in its current form. (i) It is not clear what the precise contribution of this paper is with respect to previous work. (ii) The paper has limited novelty: it combines a number of pre-existing elements: Gaussian embeddings, triplet-based  loss for retrieval, and evaluation of a number of similarity metrics. (ii) The baseline seems not to have been corrected for increased capacity in the proposed approach, making experimental results unclear to interpret.\n\n\nTypos: \n- 3 lines under eq 14, f_{\\sigma^2} should be W_{f_{\\sigma^2}}\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommend rejection",
            "review": "This paper introduces multimodal representations based on Gaussian embeddings, that encode variance and uncertainty. \n\nThe method is very similar to [1], which they extend, adapted to multimodal representations. While this idea could result in interesting representations, the paper does not motivate the extension to more modalities, and also does not add anything that makes it interesting for multiple modalities (e.g. a joint or cross-modal uncertainty is not presented, all uncertainty is mono-modal).\n\n### Strengths:\n- Successfully adapts a method that works for word representations to a multimodal setting (images and text).\n\n### Weaknesses:\n- In the paper there is no motivation for multimodality. Why is multimodality an interesting extension of [1]? In terms of motivation and method, it looks like this paper could have been single-modal (but then it is almost the same as [1]). For example, in Table 3, the entropies measured are mono-modal. The paper does not provide any measure of uncertainty for a match between modalities, just uncertainty about the representation for a specific modality. Actually, the sentence \"Recall that our uncertainty measure in equation 18 estimates the uncertainty or entropy associated with retrieving an image given a query caption (or vice versa)\" is not correct. Equation 18 estimates the uncertainty of a representation, not of the retrieval. \n- As a main point (e.g abstract, introduction, and Figure 1), the authors claim that their method can \"capture the level of details through the covariance matrix\", but it is not clear how the model can do that. The variance measures uncertainty in a specific dimension, but it does not allow the method to predict more fine-grained representations of an image. \n- The results are limited to a single experiment on a single dataset, and the only baseline is the same method without the probabilistic setting. It is hard to evaluate how well this method works in terms of quantitative results.\n- There is no intuition or reasoning about the results. Why do some metrics perform better than others? Why some perform better from text to image and vice versa? Why are there some variants that perform worse than the baseline?\n- The paper misses comparisons to other methods that model uncertainty, such as box embeddings [2] or hyperbolic embeddings [3].\n\n### Additional questions to authors:\n- Why the text mean and variance networks are completely separate? Wouldn't it make more sense that they shared a common backbone?\n\n### Final recommendation\nOverall, I believe this paper as it stands is not ready to be presented at ICLR and I recommend a rejection.\n\n### References\n[1] Luke Vilnis and Andrew McCallum. Word Representations via Gaussian Embedding. In ICLR, 2014.  \n[2] Xiang Li, Luke Vilnis, Dongxu Zhang, Michael Boratko, Andrew McCallum. Smoothing the Geometry of Probabilistic Box Embeddings. In ICLR, 2019.  \n[3] Maximilian Nickel, Douwe Kiela. Poincar√© Embeddings for Learning Hierarchical Representations. In NIPS, 2017. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting augmentation of VSE++, but experiments aren't definitive, nor is the qualitative analysis convincing",
            "review": "The authors augment the shared embedding space of a standard\ncross-modal retrieval algorithm with a probabilistic interpretation.\nSpecifically, they parameterize the mean and variance of a normal\ndistribution with GRUs (versus VSE++, which uses point estimate\nencodings) and explore various probabilistically-motivated similarity\nfunctions to substitute into the hinge loss. After exploring several\nmodel variants, they find that 1) representing both images and\ncaptions distributionally slightly improves performance; and 2) the\nmodel's confidence in embedding placement increases upon\ncropping/truncation.\n\nThe paper is straightforward, and conducts a (mostly) reasonable set\nof experiments to test the hypothesis that imparting a probabilistic\ninterpretation on the shared embedding space of VSE++ is meaningful.\nI appreciated that the authors qualitatively compared the performance\nof their probabilistic VSE++ versus the vanilla version: the idea of\nusing truncated sentences and image crops was a clever way to test the\nhypothesis that the model assigns higher uncertainty in cases when\nless confidence is justified (indeed, at least for the given examples,\nit does***).\n\nMy main concerns are twofold: first, because VSE++ isn't the most\nperformant model for cross-modal retrieval on the MSCOCO dataset,** I\nam somewhat less convinced that qualitative analyses regarding the\nmodel's behavior are telling us anything about the /task itself/\n(vs. the performance of one model that is two years old). I am a fan\nof error analyses, in general, but the authors both didn't examine\nnewer models, nor did they justify why they were looking at VSE++\ninstead of them. So, while the recommendations the authors make about\ninserting a probabilistic interpretation might be true and helpful for\nVSE++, beyond applying to this one model, the findings have limited\npractical implications.\n\nSecond, I'm not entirely convinced that performance differences\nbetween VSE++ and the proposed models are due to meaningful\nprobabilistic interpretations of the shared embedding space.  Because\nthe differences between the best-of-10-variants and VSE++ are on the\norder of 1-2%, could it simply be the case that performance\ndifferences are due to hyperparameter configurations of VSE++? A\nfairer model comparison, in my view, would have tried at least 10\ndifferent hyperparameter settings for VSE++ (e.g., learning rate,\nhinge size, batch size, etc.)  to compare to the 10 different\nprobabilistic model variants. Compounding my concerns: the model that\ndoes the best when fine-tuning the CNN doesn't match the model that\ndoes the best when not fine-tuning the CNN (Table 1 vs. Table\n2). While not a \"deal breaker,\" so to speak, it doesn't give me much\nconfidence that some general/interesting/meaningful probabilistic\nreshaping of the shared embedding space is causing the performance\ndifferences.\n\nI had several concerns with the qualitative evaluations, too. I\nunderstand the intuition that representing uncertainty explicitly\nshould be helpful, allowing the model to \"punt\" in cases where it\ndoesn't exactly know where to embed an instance. But I would have\nliked to have seen a more systematic comparison of which instances the\nproposed model outperforms VSE++ on --- are there consistent patterns?\nAnd while I liked the idea of the crop/truncate test from ***, I had\ntwo concerns with it. First, I could see an equally strong argument\nthat the entropy should actually go the other direction --- if images\nare cropped, less information is available, and thus what\ncaption/image should be retrieved should arguably be *more*\nuncertain. But this counter-intuition was not addressed. Furthermore,\nI wasn't clear why this cropping/truncating test wasn't applied more\nsystematically, i.e., as a quantitative evaluation.\n\nOverall, while the idea of making the shared embedding space of VSE++\nhave a probabilistic interpretation is somewhat interesting 1) there\nare better models out there, so the practical implications of this\nwork are limited; 2) the performance gains are small in magnitude, and\ncould easily be due to differential hyperparameter search; and 3) the\nqualitative analysis, while appreciated, could have been more thorough\nre: systematizing the crop/truncate test and comparing success/failure\ncases quantitatively.\n\n** for example, for im->txt/txt->im R@1/5/10, the following work\nachieves: 76.2/94.8/98.2 and 62.8/89.7/95.1 for the MSCOCO 1K task.\n\nLi, Kunpeng, et al. \"Visual semantic reasoning for image-text\nmatching.\" Proceedings of the IEEE International Conference on\nComputer Vision. 2019.\n\nFor R@1, this is ~10% absolute points better in both directions versus\nthe results in Table 2.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}