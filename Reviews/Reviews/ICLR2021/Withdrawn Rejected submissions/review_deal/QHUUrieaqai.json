{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nThe authors propose a pretraining strategy learning inductive biases in transformers for deduction, induction, and abduction.  Further, the claims and results seem to indicate that such pretraining is more successful in transformers which provide a more malleable architecture for learning inductive (structural) biases.  There are open questions that remain, specifically surrounding disentangling high performance from structural bias learning (i.e. is pretraining doing what we think it is) and whether datasets are the \"correct\" mechanism for imparting such biases/knowledge."
    },
    "Reviews": [
        {
            "title": "Interesting perspective on designing pre-training datasets for automated theorem proving. ",
            "review": "Summary: \nThis paper proposes a pretraining method, LIME, to improve Transformer’s performance on mathematical reasoning benchmarks. Specifically, this paper design three synthetic tasks to teach the transformer model to first learn three primitive reasoning steps: deduction, abduction, and induction respectively. The three datasets are used in the pre-training step, and the paper shows empirical performance gains on large mathematical reasoning tasks in the context of automated theorem proving.\n\nReasons for the score: \nThis paper provides an interesting direction in the field of automated theorem proving. In particular, it proposes a novel way to teach the model to learn primitive reasoning steps via designs of datasets. The paper provides good motivations and intuitions for their proposed method, but it would be nice for the authors to also formalize these intuitions and provide rigorous definitions as an academic paper (e.g. what is inductive bias and so on; details see below). The proposed method provides some ablation studies in the experiment section, but it is still worthwhile to conduct the following studies to enhance the quality of the paper: 1) how the positive transfer from the proposed pretraining change as we increase the size of the model? 2) It seems unclear from the current text how the LIME pretraining compares with large-scale unsupervised pretraining (e.g. BERT) in terms of convergence rate and the number of samples needed on the downstream tasks. For example, would pretraining on BERT lead to faster convergence and a smaller number of samples compared to LIME pretraining?\n\nPros:\n1.The idea of designing pretraining datasets to help mathematical reasoning is novel. \n2.Good empirical performance gains on large mathematical reasoning tasks in the context of automated theorem proving\n\nCons:\n1.Though the proposed method provides some ablation studies in the experiment section, but it is still worthwhile to conduct the following studies to enhance the quality of the paper:\n\na. How the positive transfer from the proposed pretraining change as we increase the size of the model? \n\nb. It seems unclear from the current text how the LIME pretraining compares with large-scale unsupervised pretraining (e.g. BERT) in terms of convergence rate and the number of samples needed on the downstream tasks. For example, would pretraining on BERT lead to faster convergence and a smaller sample complexity comparing to LIME pretraining?\n\nc. Would the proposed method have similar improvements when using different architectures (e.g. LSTM, LSTM+attention, GPT2)?\n\n2.The writing provides good intuitions in general, but it would be nice for the authors to also formalize these intuitions and provide rigorous definitions as an academic paper:\n\na. The word “inductive bias” may have different connotations across different contexts and it would be nice for the authors to give a more formal definition of what they mean by “inductive bias” in the context of this work. For example, inductive bias may refer to the structural bias imposed by the architecture. It may also refer to the prior knowledge of the target tasks or to the preference of the training scheme (e.g. what kind of functions are learned first by an SGD optimizer). It seems to me that the usage of inductive bias in this work is close to the prior knowledge of the target tasks, but this is not clear from the current writing.\n\nb. Similarly, it would be nice for the authors to clearly state what they mean by ‘‘knowledge’’ in the context of this work. Is it referring to knowledge distillation, learned representations of the context, or something else? For example, in this sentence “we focus on the latter and design pre-training tasks that are intentionally devoid of knowledge and only allow the model to learn inductive bias for reasoning”, it is unclear why specific reasoning procedure cannot be treated as a form of knowledge.\n\n3.The design of the three pre-training tasks is based on the three primitive rules for logical reasoning, which requires nontrivial human insights to formalize. Thus, the proposed approach may not be easily transferrable to other fields.\n\n4.It would be nice for the authors to provide discussions with relevant works on the reasoning abilities of neural networks such as [1] and [2]. The proposed dataset in [1] could be of interest to this work as a pretraining dataset. [2] provides a theoretical framework to formalize the inductive bias in graph neural networks, which demonstrate the limitations of graph neural networks in terms of its reasoning capacity.\n\n[1] Saxton, David, et al. \"Analysing mathematical reasoning abilities of neural models.\" arXiv preprint arXiv:1904.01557 (2019).\n\n[2] Xu, Keyulu, et al. \"What Can Neural Networks Reason About?.\" arXiv preprint arXiv:1905.13211 (2019).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for \"LIME: LEARNING INDUCTIVE BIAS FOR PRIMITIVES OF MATHEMATICAL REASONING\"",
            "review": "\n## Summary\n\nThe authors propose LIME: a pretraining strategy for learning inductive biases for mathematical reasoning. They construct 3 synthetic datasets corresponding to 3 basic reasoning patterns: deduction, induction, and abduction. Each dataset is designed to be devoid of concrete mathematical knowledge but encodes inductive biases of the reasoning pattern. In experiments, LIME pretraining improves the performance of a generic transformer model on 3 benchmarks for mathematical reasoning: IsarStep, HOList Skip-tree, and MetaMathStep. \n\n\n## Strengths\n\n+ I really like the idea of designing the synthetic datasets inspired by 3 basic patterns of mathematical reasoning: deduction, induction, and abduction. Though the basic reasoning patterns were discovered a while ago, it is novel to apply the idea to synthetic data for theorem proving. Traditional automated theorem proving focuses on deduction, but as the authors have explained in the paper, induction and abduction also play significant roles in conjecturing and framing new definitions. \n\n+ The proposed LIME pretraining lead to descent performance improvements on 3 benchmarks. It is surprising since the pretraining task is a very simple string rewriting task without any mathematical knowledge. It is interesting if it actually works.\n\n+ The paper is well-written and very easy to read.\n\n\n## Weaknesses\n\n- The authors acknowledged that there are numerous alternative designs for the pretraining datasets and presented a few alternatives in the appendix. Are there empirical comparisons between these alternatives? Why did the authors decide to use the current ones?\n\n- It would be great to have ablations that only include 2 of the 3 pretraining datasets. Then we can know if the 3 reasoning patterns are truly irreducible. \n\n- In Sec. 3.2, I'm confused about how the authors define \"Rule\" and \"Case\". To me, it looks like A * A + B = C is more like a \"Case\", whereas {A : a; B : b; C : d + e} is more like a \"Rule\"; since it describes how to map rule symbols to math symbols.\n\n- I'm confused about framing \"pretraining\" as \"learning inductive biases.\" Technically, I believe it's correct. Suppose \"inductive biases\" is defined as the learning algorithm's preferences when choosing one hypothesis among many equally valid ones. In that case, the initialization of model parameters is a kind of inductive bias.  Then, any pretraining is \"learning inductive bias.\" It would be great if the authors clarify more about \"pretraining\" and \"inductive biases\" in the next revision.\n\n\n## Conclusion\n\nI like the idea of constructing synthetic datasets inspired by basic reasoning patterns. And it is surprising that pretraining on these simple tasks can work. However, I have a few questions, and the paper lacks ablations. I could potentially raise my score if the authors are able to address these questions.\n\n## Minor Comments\n\nThese are minor points that did not account for my score. The authors do not have to address them in the rebuttal.\n\n1. There should be more margin between Table 1 and the caption of Table 2.\n\n2. Use \"K\" for thousand consistently, not \"k.\"\n\n3. \"We set the size of the math symbol set 44 and rule symbol set of size 24.\" This sentence is difficult to parse correctly. I thought there was a typo before reading it a few more times.\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, creative, and well-motivated approach for giving mathematical inductive biases to a model.",
            "review": "In this work, the authors introduce a method called LIME for imparting certain mathematical inductive biases into a model. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction. Each of these pretraining tasks is a sequence-to-sequence mapping involving 3 basic components of reasoning (Rule, Case, and Result), where two of these three components are provided as input and the third component is the target output. After pretraining on these tasks, the authors fine-tune on 3 different proof datasets, and find that the pretraining almost always improves performance, sometimes by a large margin.\n\nStrengths:\n1. This approach is creative and thought-provoking; pretraining is an important topic in ML nowadays, and this paper gives several interesting insights about how to structure pretraining. Therefore, publishing this paper at ICLR could help inspire others to use and develop improved variations of pretraining.\n\n2. One aspect of the pretraining that I found particularly impressive was how the authors found such clear improvements from such small amounts of pretraining. This is in stark contrast to the usually massive pretraining datasets that are used, and stands as an especially strong piece of evidence for the model’s usefulness.\n\n3. The experimental setup is well-motivated, drawing on a principled analysis of the problem domain. \n\n4. The paper is overall clearly written and clearly structured.\n\n5. There were some interesting discussion points and ablation studies analyzing the approach in more detail. I particularly liked the discussion about how loading the vocabulary weights had little effect, showing that the inductive biases that were imparted were abstract in nature. It was also useful to see that LIME was more useful than other pretraining tasks, ruling out the possibility that you could get similar improvements from just any pretraining task.\n\nWeaknesses:\n\n1. Part of the paper’s motivation for imparting inductive bias through a dataset, rather than through an architecture, is that designing an architecture “strongly requires human insight.” This is true, but LIME also seems to strongly rely on human insight, so this point is not a benefit for LIME over architectural approaches. This is not a huge problem, but it does not seem like a great motivation for LIME. \n\n2. Related to the previous point, it would be good to discuss the fact that the usefulness of LIME may be limited by the need to design the right pretraining task(s). As Table 4 shows, the nature of the pretraining task is very important; and although the authors were able to create some successful pretraining tasks for mathematical reasoning, it might be harder to create similarly useful tasks for larger-scale tasks in, e.g., language or vision. Again, this is not a huge problem, but I think it at least deserves some discussion.\n\n3. Though the goal of the approach (if I am understanding correctly) is to give inductive biases for induction, deduction, and abduction, the paper gives no direct evidence that it has done so: The authors create an approach *intended* to impart certain inductive biases, and this approach improves performance on 3 tasks that plausibly would benefit from those biases. But this result does not necessarily mean that the model has the inductive biases that were intended to be imparted; it’s possible that LIME imparted some other inductive biases that are also useful for mathematical reasoning but that are not related to induction, deduction, and abduction. Thus, there is a bit of a gap between the motivation and the actual experiments.\n\n4. It’s not entirely clear to me that the specific tasks (Deduct, Induct, Abduct) will necessarily enforce the types of reasoning that they are intended to enforce. For instance, consider the following input/output example: {A : a, B: b, C: d+e} <s> A A + B = C -> a a + b = d + e. Such an example is intended to show deduction, but it could instead be viewed as induction (where A A + B = C is the Result, a a + b = d + e is the Rule, and the Case dictionary should be read in reverse, treating the values as keys and the keys as values). Thus, related to the previous point, I think there is some concern that the LIME tasks may not necessarily encode the intended primitives. The results show that the LIME tasks clearly encode something useful, but it’s not clear exactly what useful things they encode. \n\nRecommended citations: (you definitely don’t need to include all of these or even any of these, but I’m pointing to them just in case they’re useful):\n\n1. You already cite the GPT-3 paper (Brown et al.), But it might make sense to cite it in a second place as well, for the sentence where you say “However, there is another potential advantage of pre-training--it may distill inductive biases into the model that are helpful for training on downstream tasks.” Another paper you can cite for this point is this one: Can neural networks acquire a structural bias from raw linguistic data? https://arxiv.org/pdf/2007.06761.pdf\n\n2. Like your approach, the following paper also uses carefully-constructed synthetic datasets as a way to impart targeted inductive biases into a model. (However, they use these tasks for meta-training, not pre-training): Universal LInguistic Inductive Biases via Meta-Learning. https://arxiv.org/pdf/2006.16324.pdf. This paper might also be useful as an example of how you can address the last two points I listed under weaknesses, as this paper gives examples of how to test whether a model has some specific inductive biases; the paper I linked to in the previous bullet (Warstadt and Bowman) also does this. (However, adding such analyses might be more work than would be doable for a camera-ready).\n\n3. It might be good to cite Peirce when first mentioned in the intro; right now, the citation to Peirce is buried deep in the paper, after he has already been discussed at length.\n\n4. Some more potentially-relevant examples of architecturally encoding inductive biases for math: https://arxiv.org/pdf/1910.02339.pdf, https://arxiv.org/pdf/1910.06611.pdf \n\nOther comments (these are not things that have affected my assessment. Instead, they are just comments that I think might be helpful in revising):\n\n1. Note that there is another approach in ML called LIME, which could potentially cause confusion. It’s completely up to you, but I would consider renaming to avoid confusion. Here is the other LIME by Ribeiro, Singh, and Guestrin: https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?casa_token=VrGSeKoqOnkAAAAA:tmzXq2uCWkUVyPdd9ytCNK4LSdRfIwsIeX4hd8EMkjnjevZ4d-rCeIIM7acIRWGtQlQemUqDlAJx-Q \n\n2. Abstract: “neural architecture” should be “neural architectures”\n\n3. Abstract: “on three large very different mathematical reasoning benchmarks” should be “on three very different large mathematical reasoning benchmarks”\n\n4. Abstract: I did not understand what “dominating the computation” meant until I read the rest of the paper.\nThe intro says “It is commonly believed that the benefit of pre-training is that the model can learn world knowledge by memorizing the contents of the natural language corpus.” This statement seems strong - I am more inclined to think that much of the benefit comes from learning linguistic structure, not world knowledge. So it might be safer to reword as saying “One plausible explanation for the benefit of pretraining is…”\n\n5. Page 3 says “the BERT pretraining objective,” which suggests that BERT is the objective. But BERT is a model, not an objective; the objectives are masked language modeling and next-sentence prediction.\n\n6. Table 1: The formatting of the table makes it look like the first two rows are numbers copied from Li et al. But from the prose of your paper, and from looking at Li et al, I’m pretty sure that these numbers are from your own re-implementation. Is that correct? If so, it might be best to format the table different - using the citation within the body of the table gives a strong suggestion that the numbers come from Li et al., in my opinion.\n\n7. Table 4 and Table 5: In the caption, say what task these results are for, so that the table can be understood on its own.\n\n8. Please double check the references: Several of them seem to only list authors, title, and year when there is at least an arXiv version that could be listed as well. E.g., “Mathematical reasoning via self-supervised skip-tree training”, “Enhancing sat solvers with glue variable predictions”, “transformers generalize to the semantics of logics”. Also, where possible, cite a paper’s actual publication venue instead of arXiv - e.g., the Raffel et al. T5 paper appeared in JMLR, not just arXiv. \n\nSummary: Overall, I am rating this an 8 because I find the strengths compelling but think that the weaknesses in framing hold the paper back from an even higher score. I would consider increasing the score if those weaknesses were addressed, though those weaknesses are deep enough that it would be hard to properly address them in time.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review: LIME",
            "review": "With the aim of learning inductive biases for deep neural net architectures, this paper presents three synthetic experiments for learning primitive forms of mathematical reasoning in theorem provers. The overall idea is inspired by Pierce’s view that these primitives are deduction, abduction, and induction. The synthetic tasks are built upon a simple arithmetic language with a source and a target. Using those tasks for pretraining a transformer model, the authors show the merit of this methodology in several mathematical reasoning experiments.\n\nI am not an expert in transformer models and pre-training techniques, so it is possible that I did not understand some parts in Section 4 & 5. Overall, I think that the idea of training a learner with primitive forms of inference is interesting for improving its performance in mathematical reasoning. The experimental results corroborate the relevance of this approach. \n\nMy main comment lies in the specification of synthetic tasks (Section 3.2). Here, the authors are using an ad-hoc arithmetic language upon which deduction, induction, and abduction tasks are defined. But this ad-hoc language has no formal semantics, so we cannot formally capture the primitive forms of reasoning. \n\nContrastingly, in Section 3.1, the three reasoning primitives identified by Pierce are well-defined because they are captured by the semantics of first-order logic. For example, take the abductive mode of reasoning:\n\nRule: $\\forall x, Bag(x) \\rightarrow White(x)$ (All the beans from this bag are white)\n\nResult: $White(o)$ (These beans are white, where $o$ is an object in the Herbrand universe) \n\nCase: $Bag(o)$ (These beans are from this bag) \n\nFrom the rule and the result, we can indeed infer by abduction that the case is an explanation of the result since:\n\n(i) $Bag(o) \\wedge \\forall x, Bag(x) \\rightarrow White(x) \\not\\models \\bot$ (consistency), and\n\n(ii) $Bag(o) \\wedge \\forall x, Bag(x) \\rightarrow White(x) \\models White(o)$ (consequence). \n\nBut in Section 3.2, the arithmetic language is given without any semantics, and hence, we cannot define a clear, unambiguous form of logical consequence ($\\models$). Therefore, the notion of abduction is here very unclear. \n\nTo sum up, the synthetic tasks proposed by the authors might indeed help in learning an inductive bias capable of improving theorem provers, but there is a discrepancy between the logical notions of deduction, abduction, and induction defined by Pierce (and more generally in the Knowledge Representation literature), and the reasoning primitives (essentially some forms of pattern matching) presented in the synthetic tasks. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}