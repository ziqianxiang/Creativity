{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper was evaluated by 3 knowledgeable reviewers. All reviewers raised concerns about the motivation of the contribution of the paper. It is unclear why the use of an additional discriminator should reduce the variance of the log density ratio estimate. Also, the derivations were found to be not convincing or intuitive. These concerns have also not been alleviated after a rather extensive discussion of the reviewers with the authors. Moreover, the transfer setting to a new environment was unclear as it does not show how the  reward function transfers to new dynamics, so the transfer experiments rather evaluate how well the algorithm can imitate a policy on a different dynamics, but it does not tell that the extracted reward function is valid. While the experimental results seem promising, the authors are encouraged to improve the motivation of contribution, check which of the \"incremental\" contributions are very necessary and improve their evaluation on the transfer scenario."
    },
    "Reviews": [
        {
            "title": "Initial review for CAIRL",
            "review": "This paper builds on a recent inverse-RL method, AIRL. The authors argue that the rewards learned by AIRL are potentially inefficient since they depend on the ratio of state-action visitation distributions of the expert and the policy. To resolve this, CAIRL derives rewards that excludes these visitation distributions; this is realized in practice by employing another discriminator to approximate the state-visitation distribution ratio. The paper further proposes a mechanism to handle the reward-bias issue in IRL. Experiments with the MuJoCo locomotion tasks show that CAIRL is a competitive algorithm for imitation learning and can also handle dynamics mismatch between the expert and the learner.\n\nI would like the authors to address the following concerns:\n\n1.\tBetter motivation: It is argued that since AIRL rewards depend on the ratio of state-action visitations, it suffers due to the “high variance” of this ratio. Is there an analytic argument for this, or prior work to support the claim? Note that the density ratio is implicitly obtained with adversarial training, which has found empirical success across many ML domains. Moreover, CAIRL requires estimation of a similar ratio (of state-visitation densities), and this estimation feeds into the IRL reward computation. So why would CAIRL not be plagued with the same “high variance” issue?\n\n2.\tReward-bias handling: The paper needs to provide more empirical evidence that the proposed reward-bias handling method is useful. I find the episode timesteps in Table 2 to be insufficient (especially with the variance in those numbers). The matter is made quite confusing by using a softplus on the IRL rewards – this manually enforces a positive bias on the IRL rewards. These are coupled with the negative rewards from the KL-penalty, and “alpha” now becomes a tunable knob to manage the overall reward-bias (as evident by “alpha” values in Table 3.). Note that DAC, which appears to be a much more principled way of handling the reward bias, does not add a modifier function (e.g. softplus) on the IRL rewards. I would encourage the authors to add more tasks where survival bias is detrimental (e.g. lunar-lander, etc.), and also comment on the role of softplus on their IRL rewards.\n\n3.\tTransfer-learning experiments: how was the algorithm (Algo 1.) changed for these experiments? The main section mentions the need for the transition-dynamics ratio in the IRL rewards. How do you obtain this ratio for these experiments? \n\n4.\tThere is an L2-norm penalty added to the loss function (termed as REG). The paper says it is required to “make the algorithm well-defined”. Could you expand on this statement? Also, it would be interesting to do an ablation on this to measure its contribution to the overall performance of CAIRL. Connected question – are the baselines regularized with the REG and GP losses?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A mostly sound work though with somehow poorly justified design choices",
            "review": "The line of reasoning and analysis followed in the paper is mostly sound. The paper claims that the use of (state-action) occupancy measure make IL and IRL methods brittle due to the high variance of these measures and their inability to transfer to other domains. These two claims are neither properly defined and grounded in the literature, nor are they isolated experimentally. It is important to show clearly that a) these are problems, b) hitherto unaddressed in the research landscape, and that the proposed methods and techniques address these problems specifically. \n\nThe paper also claims to address  the reward bias in imitation learning due to mis-specified termination cases. Since this claim is not tied to the previous claim, there should be clear experiments showing the advantage of proposed methods addressing each claim in isolation. This would show a) whether either of the proposed solutions can be used alone with benefits, and b) what is the individual impact of each contribution. Besides the solution claiming to address the reward bias needs to be directly compared against the method introduced in Kostrikov2019 (cited in paper). On another note, the method addressing the bias seem to suffer from a logic flaw. The paper states that in a “self-looping” terminal state, the expert policy is equivalent to a the uniform action distribution, and designs the reward (unrelated yet to reward shaping strictly-speaking) accordingly (0 reward when the expert coincides in action distribution with the uniform one over actions). Since, according to Figure 1, such a KL penalty seem to be applied at every transition, it is not difficult to imagine scenarios (highly multi-modal expert, expert in exploratory phase, etc.) where the designed reward will be zero in non-terminal state due to a close-to-uniform behavior displayed by the expert. The designed reward will then fatally fail to allow the policy to imitate such expert properly.\n\nAt the end of page 4, the paper states: “Compared to the AIRL, […] a reward function r [proposed in the paper] can be universally applied for training arbitrary policies since the agent-specific occupancy measure […] is detached.”. In the absence of empirical or theoretical results showcasing the flaws of learning a metric such as the one in AIRL, the claim remains unclear and unjustified. It must be made clear why the reward formulated by AIRL is flawed. Using results from the “state-only” adversarial imitation learning literature might contribute positively to such a ustification and will echo the first claim, however such results are not mentioned nor cited in the paper. Besides, since AIRL uses state-action occupancy measures in its reward design while the proposed reward involves the policy’s action probability directly, the proposed approach is close in essence to behavioral cloning. It differs from BC as it also involves reward shaping, as introduced in Ng1999 (cited in paper). The similarities in reward formulation appears clearly in the table from Ghasemipour2020 (cited in paper), from which Table 1 is heavily inspired. The BC line should be kept to highlight how similar the formulations are. Moreover, while the introduction of potential-based shaping is justified in AIRL, it is not motivated here, and it remains unclear why it was introduced.\n\nIt should be made clearer in the text that the method builds on AIRL, considering it involves of the functional elements introduced in AIRL (shaping, EBM formulation, MaxEnt IRL, derivations, etc.).\n\nAbout the experiments that are presented: the results of CAIRL are encouraging. As said earlier, it is difficult to disentangle the contribution of each component without ablation of the introduced techniques. It would also be interesting to evaluate the proposed IRL method on IRL tasks (reward recovery, as in AIRL), not only on IL tasks (perform well in environment). \nThe transfer learning experiments show high robustness, which (probably, no ablation) show how also using state occupancy measures on top of the rest of the usual AIL machinery makes the agent transfer well to environment sharing the same state spaces, and differing only by dynamics. This observation directly echoes the main result of AIRL, which is that rewards robust to dynamics changes are only function of the state (not action). Discussing this result is a piece that is missing from the submission unfortunately.\n\nThe paper could use a good editing to address language errors.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Some interesting ideas, but questionable algorithmic choices and flawed evaluation",
            "review": "Summary\n========\nThe paper proposes several modifications to adversarial inverse reinforcement learning (AIRL) that aim to improve transfer to different environments, to address termination bias, and to learn more multimodal policies. The most notable change is probably the introduction of an additional discriminator, $D_\\phi(s)$, that is trained on the state marginal. By adding the logits of the state-discriminator to the logits of the state-action(-nextState)-discriminator, $D_{\\theta,\\psi}(s,a,s')$, the latter supposedly estimates the expert's advantage function better. Apart from adding a state-discriminator (1), I could identify the following modifications to AIRL: (2) adding a constant offset to the optimization problem given by the log-density of a uniform action distribution, (3) enforcing positive rewards through a soft-plus, (4) disregarding the gradient of the next states Value $h_{\\bar{\\psi}}(s')$ when updating the discriminator and (5) penalizing the temporal value differences $||\\gamma h_{\\bar{\\psi}}(s') - h(\\psi)||_{2}^2$ via regularization.\nThe resulting algorithm, CAIRL, is evaluated (a) on a 2D and 3D toy problem for testing the multi-modality of the learned policy (b) on MuJoCo locomotion tasks to evaluate imitation learning performance, and (c) on MuJoCo tasks with modified dynamics to test transferability. The transfer setting is different from the setting used by AIRL because the evaluation learns the policy and reward function directly in the test domain and it is not assumed that the original domain can be simulated. Hence, the evaluation tests the robustness of the imitation learning rather than the robustness of the reward function. Especially at the transfer experiments, CAIRL performed better than AIRL.\n\nStrong points\n===========\n- The paper is well-written, tables and figures are helpful\n- Evaluations on different environments with respect to different properties (multimodality, imitation learning, transfer learning)\n- The paper comes with source code\n\nWeak Points\n===========\n- (I) I don't see the motivation for most of the modifications to AIRL. I do not see how they are supposed to improve multimodality or transferability\n- (II) The evaluations seem to be termination-biased by using positive rewards for CAIRL. Hence, the results provide little value\n- (III) the changes seem to be incremental at best (and quite possibly detrimental, esp. (3-5) )\n- (IV) Provided source code only contains demonstrations for the \"Hopper\" imitation learning experiment. There is no straightforward way to reproduce the other evaluations\n\nRecommendation\n===========\nI recommend rejection because there seems to be little logic behind the proposed modification. I don't see the theoretical basis for the claimed benefits and the empirical analysis is possibly dominated by reward bias that was introduced by restricting CAIRL to positive rewards. \n\nSupporting Arguments\n==================\nMy recommendation for rejecting is primarily due to the weak point (I) and secondarily due to the weak point (II). Hence, I will now focus on these points.\n\nI\n---\n\nThe motivations for most of the motivations are either missing or dubious. \n- __Adding a state discriminator (1):__ The paper claims that the additional discriminator improves transferability (based on Eq. 9). AIRL minimizes the reverse KL of the state-action distributions using a reward function that for an optimal discriminator is given by $r^\\star(s,a) \\approx \\log \\frac{\\rho_{E}(s)}{\\rho_{\\pi}(s)} \\log \\pi_{E}(a|s)$. CAIRL minimizes the KL between the policies using a reward function that for an optimal discriminator is given by $r^\\star(s,a) \\approx \\log \\pi_{E}(a|s)$. Intuitively, the term $\\log \\frac{\\rho_{E}(s)}{\\rho_{\\pi}(s)}$ that was dropped by CAIRL, seems to be very useful by pushing the policy towards the demonstrated states and away from those states that are not encountered by the expert. This additional reward to bring the policy close to demonstrated states seems to be a major advantage of AIRL compared to direct behavioral cloning that is known to suffer from covariate shift due to compounding policy errors [1]. It is not clear to me why removing this term improves imitation learning when learning under different dynamics. Based on Eq. 9 the expected unshaped cost corresponds to the KL $D_{\\text{KL}}\\left[ p_{\\pi_\\phi}(a,s'|s) || p_{E}(a,s'|s) \\right]$ and thus minimizes the divergence between these distributions that are obtained under different dynamics. However, wouldn't AIRL analogously be penalized based on the KL $D_{\\text{KL}}\\left[ p_{\\pi_\\phi}(a,s',s) || p_{E}(a,s',s) \\right]$? Why is a policy that puts less emphasis on matching the demonstrated states expected to perform better in a different environment? I would argue that it is more likely that good states remain good under different dynamics than that good actions remain good, but of course, it always depends on the concrete setting. \n\n- __Adding a constant term to the objective (2)__: I am fine with modification (2) since I see how the constant offset of $u = \\log \\pi_{\\text{uniform}}$ can tackle termination bias for CAIRL. However, the paper motivates this modification as a general, simpler way of tackling termination bias compared to the approach by Kostrikov et al. (2018), which is highly misleading. I think Section 4.2. needs to be rewritten.\n\n- __Enforcing positive rewards (3)__: This modification is in my opinion a crude hack that invalidates much of the evaluation based on a dubious motivation by arguing that the expected reward is positive. First of all, the fact that the expectation of a function is positive does not imply that the function itself is positive. Furthermore, the $\\ge$ sign on page 6 is wrong; it should be $E_{\\pi} \\left[ \\log \\pi_E \\right] \\le E_{\\pi} \\left[ \\log \\pi \\right]$ and not the other way around. On a side note, there is also a typo in that equation, because I suppose the log should be in front of the fraction.\n\n- __Disregarding the gradient of the next state's value (4)__: The paper argues that this is analogous to target networks in deep RL. However, (C)AIRL is not training the value net by minimizing the mean squared TD error but minimizes cross-entropy loss based on a special discriminator architecture, so I do not see the connection here.   \n\n- __Penalizing the temporal value differences (5)__: The regularizer would make sense to me if it was penalizing the TD error. However, I don't see the reason for penalizing $||\\gamma h_{\\bar{\\psi}}(s') - h(\\psi)||_{2}^2$. The paper just argues that this regularizer makes \"the overall algorithm well-defined\", without providing any reasoning.\n\nII\n----\nModification (3) adds a reward bias. Enforcing that all state-action pairs produce positive reward introduces survival bias. Entering unhealthy states in the locomotion tasks would enter an absorbing state with a hard-coded reward of 0. Hence, the agent will avoid entering such states. The survival bias might be a major reason, why CAIRL performs better than AIRL when training in a modified environment.\nI also think that evaluation not fair by not addressing reward bias for AIRL. CAIRL makes use of information about environment resets due to unhealthy states by assigning a value of zero to these states. AIRL could also make use of this information by addressing termination bias as proposed by Kostrikov et al. (2018).\n\n\n\nQuestions\n----------\n- At the bottom of page 4 you argue that compared to AIRL, the CAIRL reward can be \"applied for training arbitrary policies since the agent-specific occupancy measure $\\rho_\\pi(s,a)$ is detached\". Why can't the AIRL reward be used for training arbitrary policies?\n\n- Why is CAIRL assumed to be better at learning multi-modal policies? The connection between CAIRL and MaxEnt-IRL is based on the AIRL derivations, so I do not see the reasoning here.\n\nAdditional Feedback\n--------------------\nI think more details should be provided for the multimodal experiment. Can you plot the demonstrations as well? \n\nReferences\n----------\n[1] S. Ross and D. Bagnell.  Efficient reductions for imitation learning.  AISTATS. 2010.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}