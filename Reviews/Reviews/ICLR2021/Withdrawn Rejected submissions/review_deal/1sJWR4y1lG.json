{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper provides a new perspective on deep networks by showing that NPK is composed of base kernels and their dependence on the architecture is explicitized. It is further shown that learning the gates can perform better than random gates.\n\nWhile the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it. On the architectures considered such as FC, ResNet and CNN (btw, it seems restricted to 1-D), it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning (or get very close to).  It is debatable whether drawing such a nontrivial insight alone warrants publication at ICLR, while \"nontrivial\" itself is a subjective judgement.  I understand people differ in their opinions, and the NTK paper has been impactful.  Unfortunately since there are quite a few other papers that are stronger, I have to recommend not accepting this paper to ICLR this time."
    },
    "Reviews": [
        {
            "title": "Ok paper, but needs better exposition and model details",
            "review": "Overview: The authors examine deep learning from the perspective of kernel methods and demonstrate that convolution layers in these architectures can make DNNs a form of composite kernel learning.\n\nSignificance: Understanding and interpreting neural networks is an important problem in general; similarly extracting good features key to downstream performance of a neural network. Hence the paper tries to address some important and relevant problems in the field, however, I'm not fully convinced as to whether their procedure is  any more interpretable than existing methods or extracts features optimally.\n\nQuality and Clarity: While the work provides sufficient details to understand prior work and the method itself, the key contribution section of the paper needs more work. \n\nNovelty: There are many works that focus on understanding neural networks and learning features for downstream prediction from the perspective of kernels. The novelty in this work is limited to allowing gating functions to adapt during training, such that the learnt gates can perform better than random gates.\n\nPros:\n1) Paper presents a potential solution to a relevant problem\n2) Paper provides good overview of an existing method that form the basis of the approach.\n\nCons:\n\n1) The most significant weakness of this paper is lack of thorough discussion about what the kernels actually mean in terms of understanding what the neural network is doing. How are these kernels learnt? For this, I think the authors need to make concrete comparisons with methods that are deeply rooted in kernels such as GPs or BNNs. For instance, does using a particular composite kernel structure give you the same predictive performance as when using a GP? Can we directly interpret such models as forms of BNNs or GPs? How does this work compare to more classic work that uses composite kernels in support vector machines? \n\n2) I would have also liked to have seen a better exposition of the interpretability of the method. How does using these composite kernels together compare to other approaches for interpreting deep neural networks like for instance layerwise relevance propagation?\n\n3) There is hardly any reference material which suggests the authors need to include a more thorough description and comparison of related work.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper proposes an extension of the neural path framework to include composite kernel (sum, product-sum and CNN pooling) learning and learnt gates to show for infinite kernel width gates are important than weights.",
            "review": "Paper proposes and extension of neural path framework to include composite kernels which comprise of a) FC networks (Hadamard product of gram matrices), b) residual networks (sum of products of base kernels), and c) CNN max-pooling layer. Furthermore, they also include learnt gates instead of static initialized random gates and show learnt gates perform better.  \n\nPaper is well written with main technical contribution being theorem 5.1 which shows for infinite width case $w \\rightarrow \\infty $ the NTK is independent of the weights. It also presents experimental result on MNIST and CIFAR for four proposes regimes of (Definition 5.1) that models are robust to combinatorial variations in layers and inputs.  This results in novel makes an important theoretical contribution towards understanding of why DNN with composite kernels perform well in practice.\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting work, bringing a new view on the neural tangent kernel",
            "review": "This paper builds on recent work characterising deep neural networks in terms of Neural Tangent Kernels and Neural Path Features. Over the past few years, a number of papers have developed the theory of Neural Tangent Kernels, which can be used to interpret infinite width deep neural networks in the context of a particular type of kernel. A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths. For a fixed set of network weights, we can consider each path to give rise to a feature, corresponding to whether this path is active (i.e., is not switched off by one of the gates on the path). Then, the output of the neural network can be viewed as a weighted sum of active paths, equivalently the dot product of the neural path feature vector and a neural path value vector. Lakshminarayanan and Singh showed that under certain assumptions, a kernel defined in terms of the neural path feature is approximately equal to the neural tangent kernel (up to a constant). Specifically, they show that the value of the neural tangent kernel matrix tends to a constant multiple of the neural path kernel matrix as the width of the network goes to infinity. This suggests that the key component in a deep neural network with RELU activations is the gating structure, which defines active subnetworks, as opposed to the values. \n\nAs far as I can see, this work extends the analysis of (Lakshminarayanan and Singh, NeurIPS 2020) in two ways. Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). Secondly, they provide an interpretation of the neural path kernel as a composite kernel composed of layer-wise kernels, giving rise to the title of the paper.\n\nI was not very familiar with the work on neural tangent kernels and encountered (Lakshminarayanan and Singh, NeurIPS 2020) for the first time when reviewing this paper. As such, there were things which I didn't fully understand and may have misunderstood in my review.  \n\nI have one question regarding the theoretical results in both papers. Theorem 5.1 (in both papers) relates the neural path kernel to the neural tangent kernel by showing that the neural tangent kernel for a network in which the gates have been fixed tends to a constant multiple of the neural path kernel as the width of the nerwork goes to infinity. This felt counter-intuitive to me at first reading, as fixing the gates and growing the width to infinity seem to be mutually exclusive. Is it correct to interpret the result as follows? For any fixed gating structure, there is a relationship between the neural path kernel matrix and the neural tangent kernel matrix for a network with that gating structure (i.e., one in which we are only learning the neural path values). As we allow the width go to infinity this relationship tends to one of equality (up to a constant multiple).\n\nI also have a number of questions regarding the empirical results in this paper.\n1. In the experiments where we have fixed the weights, is the model learning parameters in a network in which the gating structure is fixed or is it learning the neural path value vector as part of a linear model?\n2. The discussion mentions performance when we fixed the input gram matrix to be a constant in the definition of the neural path kernel (and hence define the neural path kernel in terms of the gating structure only), but does not include numerical results for this case. I understand that this may be due to a desire to keep the paper within the recommended 8 pages, but don't see why these results could not have been added in the appendix.\n3. The discussion mentions performance when we permute the layers of the model and claims this is robust to permutation of the layers, but but does not include numerical results for this case. As above, I understand that this may be due to a desire to keep the paper within the recommended 8 pages, but don't see why these results could not have been added in the appendix. Moreover, I wasn't not sure what being robust to permutation of the layers means for the case where we are learning the both components of the DGN. Is this claiming that the results do not change if we permute the layers after training in the DL regime?\n\nAdditional comments\n1. LS2020 is used in several places to refer to (Lakshminarayanan and Singh, NeurIPS 2020), the recent paper on which this builds. These should be corrected to match the format of the other citations.\n2. Table 1 contains the information flow for the FCNN. Arguably, this is the simplest of the three architectures and an illustration of the information flow for a CNN could be more useful here. At the very least, the authors should direct the reader to Appendix A, where the CNN is described in more detail.\n3. The authors refer to this work and the previous work of (Lakshminarayanan and Singh, NeurIPS 2020) as a \"paradigm shift in understanding deep learning\". While this work seems to be an interesting and promising line of research, I think it is fair to say that we will need to wait to see if it really does provide a paradigm shift in how we understand deep learning.\n\nIn gener`al, I think the neural tangent kernel is an interesting and promising line of research in the study of deep neural networks. The recent work of Lakshminarayanan and Singh (NeurIPS 2020) seems to add to this and this paper provides a relevant follow-up up that and as such is likely to be of interest to the ICLR community. However, I was did not check the proofs in the appendix or the appendix of (Lakshminarayanan and Singh, NeurIPS 2020), on which the results in this paper depend, and hope another reviewer more familiar with this line of work was able to do so.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Comments to \"Deep Learning Is Composite Kernel Learning \"",
            "review": "#### General Comments\nThis paper establishes close relationship between CNN and FC-DNN with a composite kernel method. Specially,  this paper shows that architectural choices such as convolutional layers with pooling, skip\nconnections, make deep learning a composite kernel learning method, where the\nkernel is a (architecture dependent) composition of base kernels. This interestingly indicates that  standard deep networks have in-built structural properties that may explain their success before training them. \nMoreover, this paper develops neural path framework to characterize the role of gates/ masks in FC-DNN. \n#### Specific Comments\n(1) It would be more interesting if some superiority of deep learning relative to kernel methods can be provided. \n(2) Lakshminarayanan and Singh (2020) has developed a neural path framework in the NTK regime.  Are there additional challenges when establishing   these similar conclusions for DNNs with Relu activation? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}