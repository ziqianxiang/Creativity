{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper uses adversarial data to improve generalization in Programming By Example (PBE). The reviews were somewhat mixed with some people finding this useful and interesting while others finding it straightforward and unsurprsing. The reviewers were not convinced of the ultimate usefulness of the approach since it is evaluated on toy or synthetic datasets. The clarity of the presentation could also be improved."
    },
    "Reviews": [
        {
            "title": "Light and specific",
            "review": "The paper is proposing to evolve datasets of (inputs, outputs) in the context of programming by example (PBE), where the goal is to infer a computer program that is consistent with the association of (inputs, outputs). The justification of the approach is to figure out instances that would allow to learn a model with PBE that would generalize better, compared to learning from synthetic datasets that are randomly generated. The approach followed consists to use an evolutionary algorithm to generate this new dataset, using the learn model as a guide, by picking the points where the model at hand performs poorly. Such adversarial approach proceed iteratively, adding extra pairs of (inputs, outputs) to the training set, to get it to perform better in situations where it has trouble.\n\nThe proposal is relatively straightforward, using an adversarial optimization loop to enhance the training set for better (at generalization) PBE approach. Such solution is in line with many other adversarial approaches. The proposal is quite specific to the context of PBE, and has been tested in a rather synthetic setting (toy problems / synthetic problems). Performance shows good general improvement of the performance. However, the overhead of generating the adversarial samples is not provided, nor the overhead caused by adding more samples to the training set. If we have trained the model with more random samples (same size for all approaches), would the results be the same -- I mean is there any correction made to ensure that we are always comparing approaches with the same number of samples?\n\nThe overall proposition is analogous to different proposals made with coevolution in evolutionary algorithms in the years 2000s. Here again, the training set was \"co-evolved\" with the program inference task in a competitive (adversarial) way, with a population of samples vs. a population of programs, trying to obtain the samples that would mislead the most the programs, in order to infer more robust solutions. See for instance\n\nhttps://doi.org/10.1162/evco.1997.5.1.1\nhttps://doi.org/10.1162/106365604773955139\n\nAlthough we are not talking of the same kind of PBE -- evolutionary algorithms vs. PCCoder -- I think linking the current work with this older one would be required, as the idea is not new (but applied to a different PBE approach).\n\nThe quality of writing is not as sharp as we would expect for an ICLR paper. The writing stay somewhat vague at times, we would expect more formal and precise presentation. The algorithms presentation in Appendices A and B are not very clear and not very useful to grasp all the details. The presentation of the methods in Sec. 4.1 and 4.2 are very textual, I am far from being convinced that someone who what to reproduce the approach will be able with the information provided. For instance, how is exactly computed the fitness function is not given explicitly, from what I get I would have a hard time to implement it, or at least valid it in its correctness. It appears strange of proposing an optimization solution as the main contribution without explaining in detail the objective function optimized.\n\nIn brief, the proposal does not pass the threshold in terms of significance of the contribution, overall quality of the paper and how exhaustive the experimental evaluation has been made.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A valuable contribution to the synthetic data generation",
            "review": "## Summary:\n\nSynthesis models trained on synthetic datasets of randomly generated programs and corresponding IO pairs often fail to generalize to real-world PBE problems. The models often learn specific aspects of the synthetic data distribution rather than the semantics of the programming language. This work proposes a more principled adversarial approach to generate synthetic data. The training set is built iteratively by finding data distributions on which a given model performs poorly and adding data drawn from these distributions to the training set. The model and the training dataset are built in tandem. The candidate distributions are generated by mutating the current population of distributions. Models trained using this method are shown to generalize to a variety of distributions.\n\n## Positives:\n\n* The problem is this work is addressing an important one and is well situated. \n\n* Novelty: Although the idea of adversarial data augmentation to improve generalization across domains has already been explored in the literature (eg. Volpi et. al.), I haven’t seen this applied in the PBE setting. The current work is agnostic to the architecture of the synthesis model. I would, however, like the authors to contrast their approach with these approaches. \n\n* Clarity: The paper is well written and well motivated. The algorithm is explained well and seems reproducible. \n\n* Results: The experiments are inline with the claim. The proposed method is evaluated on two domains (DeepCoder and Karel) and compared with two state-of-the-art methods. Although the method is not a winner for all the datasets, it generalizes well and has best worst-accuracy across different datasets for various program lengths. \n\n* The work proposes to use adversarially generated test sets for evaluating performance of models. I believe this is an important contribution and will help in avoiding cherry-picking (often unintentional) of test datasets.\n\n## Negatives\n\n* Defining the space of distributions to be explored may be difficult if the DSL involves APIs with the complex semantics. For example, it will be difficult to use the simplistic method of defining distributions for all the literals for the Sygus String PBE domain. Most of the random IO pairs will not have a solution (or a very long solution). I think it will be difficult to adapt the current method to such a highly constrained domain. I would like the authors to comment on this and qualify the results if they agree.\n\n* To bolster the claim, I would have liked additional results on a complex domain (eg. SYGUS String PBE).\n\n* I believe readers will benefit from some discussion on contrasting the current approach to conventional adversarial data augmentation methods (even though the setting is quite different).\n\n## Overall Remark\nI believe the method will be a valuable contribution towards the important problem of generating synthetic datasets to train robust models. Despite my apprehension of the applicability to more complex domains, I believe the method can still be employed in conjunction with the other methods.\n\n## Minor comments\n* Although the paper claims adversarial training data generation as the main contribution, the title of the paper focuses on the “evolution” aspect of the method. \n* As I understand, in section 3, $(\\phi, i, o)$, “i” and “o” are input and output vectors and not a single input output pair. Would like the authors to clarify this.\n* “We sample n/m -1 new distributions”. Shouldn’t it be “n/m”\n\n## References\n* Volpi, Riccardo, et al. \"Generalizing to unseen domains via adversarial data augmentation.\" Advances in neural information processing systems. 2018.\n* Alur, Rajeev, et. al. Syntax-guided synthesis. IEEE, 2013.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Too narrow in scope",
            "review": "The paper proposes an approach to develop synthetic datasets aimed at training DeepCoder-alike models. They claim that current approaches to generate synthetic datasets do not help coding algorithms to obtain a good generalization. The reviewed literature and the experiments presented back such claim. The approach consists in an _adversarial_ Evolutionary Computation (EC) methodology.\n\nPros:\n- The paper is, in general, well written. The language is clear, and overall structure of the paper is OK.\n- The method is sounding, as it departs from proven concepts such as using evolutionary methods as adversarial sample generators for deep models.\n\nCons:\n-  Sec. 3 could be improved. Authors rely too much on formal notation to express their main contribution, with almost no explanation in simple words, or the intuition behind it.\n- Although the paper may be solid enough, from a scientific point of view, I consider the scope of the paper too narrow. Authors are proposing a tool to help another tool without too much use other than competitions on synthetic datasets. As the authors state themselves, there are no enough real-life cases for these types of models to perform, and therefore everything is reduced to benchmark tests on toy, artificially generated, datasets.\n- i.e., their contribution is minimal from a wider point of view. This is evidenced in their own list of stated key contributions, where they expand a unique contribution (the proposed approach) into three redundant achievements (experimental evaluation of the approach in two different settings).\n\nOther comments:\nA major problem with the entire field of Program synthesis within the field of deep learning, is that sometimes overlaps alarmingly with the field of Genetic Programming (GP), and it becomes difficult to really put in perspective if a contribution is original enough, or just a re-painted version of an older idea. This paper is a clear example of such problem. The method the authors are proposing, bears some resemblance to the (competitive) Co-Evolutionary approach of subset sampling used in GP [1,2,3]. But these methods are never mentioned in the related work area, becoming impossible to perform a side by side comparison of the proposed approach against what already exists in other areas; in other words, the paper fails by being too focused on the area of deep learning, while being oblivious to very similar developments in parallel areas.\n\n1. Doucette, J. A., Mcintyre, A. R., Lichodzijewski, P., & Heywood, M. I. (2012). Symbiotic coevolutionary genetic programming: a benchmarking study under large attribute spaces. Genetic Programming and Evolvable Machines, 13(1), 71-101.\n2. Chong, S. Y., Tino, P., & Yao, X. (2008). Measuring generalization performance in coevolutionary learning. IEEE Transactions on Evolutionary Computation, 12(4), 479-505.\n3. Pagie, L., & Hogeweg, P. (1997). Evolutionary consequences of coevolving targets. Evolutionary computation, 5(4), 401-418.\n\nRate updated 30 Nov.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An incremental, adversarial construction of a PBE dataset with the property that synthesis model M trained on that set generalize better",
            "review": "- quality : good\n- clarity : good\n- originality : okay\n- significance : okay\n\nThis work mainly deal with the issue of I-O selection during training. Typical program synthesis training collects its dataset as follows:\n1) select a random program \\phi\n2) select a set of random inputs I\n3) run \\phi(x) to get a set of outputs O\n4) add the tuple (\\phi, I, O) as training data \n5) train a model M using the training data\n\nGiven a program \\phi, there are infinitely many input-outputs that can be used to specify it. Which one should you pick? This work suggests that instead of picking a random set of I, this set of input should be chosen adversarially, conditioned on the current synthesis model M. Under this scheme, one alternates in generating adversarial training datasets D' and appending it to D, and each time we re-fit a new synthesis model M over this dataset. Experiments show this is good in practice.\n\nI especially liked the passage in this paper that uses the adversarial I-O as test set and measure performance with it. It is a very good lower bound on just how bad your synthesis algorithm is.\n\nSome comments and questions for the authors:\n\n1) why stop at being adversarial w.r.t. input I? why can't we also pick \\phi adversarially? Imagine we have a PCFG (or any language model for that matter) capable of generating programs, we can easily evolve a PCFG that targets more \"tricky\" programs to have them in the grammar. in this sense, this paper title is inaccurate in saying the evolution is evolving how to construct synthetic datasets, as we do not evolve over \\phi, but only over I. maybe a more appropriate title would be something along the line of evolving adversarial inputs for synthesis.\n\n2) how expensive is this algorithm? presumably this algorithm needs to cycle between fitting M and generating D. One can imagine this is costly if too many rounds happen? Understandibly, this cost is only paid in training time, but it should not be too high – for instance, re-training M after every single (\\phi, I, O) addition. If the time takes to fit M linear in the size of D, Is the time complexity quadratic in the number of iterations? Is 20 iterations (for your experiments) chosen for any particular reason?\n\n3) one line of work that is highly relevant is \"selecting representative examples for program synthesis (Pu et.al 2018)\", where they start with a big set of input-output examples, and iteratively select from the big set the most \"surprising\" examples, where surprising is estimated using a neural network trained on the task of I-O infilling. \nThey used representative examples as a pre-process step to program synthesis, but one can imagine adopting it as a baseline in the following sense:\n- first train the neural-network that can measure surprisal using I-O infilling tasks\n- sample a random program \\phi\n- sample a large set of input-outputs I-O_large\n- start with I-O_repr = {}\n- iteratively add i-o from I-O_large that is surprising – conditioned on I-O_repr – to I-O_repr until I-O_repr is of a desired size\n- return (\\phi, I-O_repr) as training data point\n\nI-O_repr is representative in a sense that it maximally disambiguate equivalent programs that satisfies the previous set of I-O. It would be interesting to see the relationship between the adversarial examples (optimized for hardness) and the representative examples (optimized for minimizing ambiguity).\n\n4) A question just out of curiosity, how can this approach be scaled to an interactive setting, where the size of I-O can vary? Imagine in an interactive synthesis setting, after giving 3 I-O to the synthesizer, the synthesizer M returns a program that satisfies these 3 I-Os, but isn't what the user want. The user adds a 4th I-O as a result. Would you evolve different sets of adversarial training data one for each I-O size? Is the user being adversarial here though? Or is the user being cooperative, how would you model that?\n\n\n\nOverall I enjoyed reading this paper. I find the reasoning of using test sets that are off-distribution compelling, and the usage of an adversarially constructed test-set well justified. This work can be made stronger by considering also evolving a good distribution of \\phi (or at least some comments on why this was not attempted), and comparing against the work that selects representative set of I-O.\n\nfinal recommendation:\nafter looking at the other reviewer's scores and exchanging some more information with the authors (thanks for the great exchange btw), I would maintain my score of 6. I feel this line of work is definitely useful for program synthesis, especially in the analytical aspects (using adversarially generated examples as benchmarking tools and as representative explanations). the comparisons against other genetic programming approaches, I feel, is not too necessary once the paper is framed as \"using adversarial examples to instrument / analyze synthesizer behaviours\". ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}