{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper addresses an distribution shift and biased Q-values that happens when offline agents are finetuned in an online manner. The final revision of the paper is very well written and easy to understand. The proposed method in the paper is interesting, and aiming to address an important issue in RL. The proposed method involves a combination of two well-known methods in RL to tackle the distribution shift issue, the paper first suggests to use a balanced replay mechanism a replay for online experiences and another one for the offline. The second improvement is coming from the ensemble distillation.\n\nIt seems like in the light of the reviews, the authors have improved manuscript. However, I would like to recommend the paper for rejection. I would like the authors to do further experiments on the individual components of the algorithms, for example what if we run all the experiments only with BR or only using ED how would the performance change. How much improvement is coming from each one of those individual components? As it stands, it is not clear to me right now, and the proposed solution looks a bit complicated and hacky. \n\nThe balanced replay mechanism is very similar to the replay approaches that are used for learning from demos methods like R2D3 [1] and DQfD. Also the ensemble distillation approach is very akin to RAND [2] and distillation approaches that are used in lifelong learning algorithms. It is not clear, why it is that important for offline RL. It should potentially improve online RL as well, perhaps some experiments on online RL would be interesting.\n\nNevertheless, I think the paper is very interesting and attempting to address a very important problem in RL. I would recommend the authors to resubmit the paper to a different venue after doing some small changes on it.\n\n[1] Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., ... & Barth-Maron, G. (2019, September). Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. In International Conference on Learning Representations.\n\n[2] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\n\n\n\n"
    },
    "Reviews": [
        {
            "title": "Inconclusive empirical evidence for BRED's performance and whether it directly addresses distribution shift",
            "review": "Summary\n-------\n\nOffline RL allows agents to be trained on static, offline datasets. If\nthe agent has already undergone offline training however, distribution\nshift makes further online training difficult. The authors propose to\naddress this problem by keeping separate offline and online replay\nbuffers, and sampling different proportions of each. In addition, they\npropose to ensemble a distillation policy. The proposed method is\nevaluated on logged MuJoCo datasets answering whether BRED improves over\nother fine-tuning methods and whether balancing or replay contribute\nmore to BRED's success.\n\nDecision\n--------\n\nBRED is an interesting approach, but the experiments do not demonstrate\nthat it is better than the baseline. As such, my preliminary rating of\nthis paper is rejection. I appreciate that the sampling scheme proposed\nis simple and well motivated to address the problem of distribution\nshift. It is not clear whether the sampling scheme actually addresses\nthe problem of distribution shift however, as it gives the agent access\nto additional experience. The distillation scheme also does not seem to\naddress distribution shift directly and the experiments do not\ndemonstrate that it helps performance outside of the Walker2d-random\ntask. Moreover, it is not clear whether the baselines are also\nensembles, which would make the experiment results less compelling.\n\nOriginality\n-----------\n\nThe contributions are: a sampling scheme combining offline and online\nsamples, as well as a distillation scheme. The sampling scheme is\nincremental and simple, but seems like a novel approach. Distillation on\nthe other hand, is well-explored in RL (Czarnecki et al. 2019) and does\nnot seem to contribute much to the problem addressed in the paper. Taken\ntogether, the paper has limited novelty.\n\nQuality and Clarity\n-------------------\n\nThe paper is easy-to-follow and well-written. However, I have some\nissues with the title. The title suggests that your aim is to address\ndistribution shift in online RL. However, distribution shift is a\nproblem due to offline training. Of course, the problem you are\naddressing is further online training after offline training. Perhaps\nthe title should be reworded to make this more clear.\n\nStrengths\n---------\n\n-   There is a clear exposition of the problem addressed, namely\n    distribution shift. This is well motivated by the t-SNE\n    visualization. Section 3 in particular highlights the correlation\n    between distribution shift and the performance of offline RL\n    algorithms.\n-   BRED, the proposed method, is a straightforward solution via\n    balancing offline and online replay buffers. In addition, the\n    schedule for updating the proportion sampled of each replay buffer\n    is well argued and also simple.\n-   The results are clearly motivated by questions in the beginning of\n    the section. Sensible baselines are used to compare the proposed\n    method (BRED), and these are discussed and reasoned in detail.\n    Furthermore, there is a good coverage of tasks (3 MuJoCo) and\n    difficulties (random, medium, medium-replay, medium-expert).\n\nWeaknesses\n----------\n\n-   While fine-tuning an offline RL agent can pose challenges, similar\n    challenges can arise in supervised learning. For example, Ash and\n    Adams (2019) note that in different regimes, warm-starting neural\n    network training can hamper generalization. Overall, I feel that the\n    correlation between distribution shift and online performance after\n    offline training is not shown to be causal, but correlative.\n\n-   It is unclear why distillation is included, as it does not directly\n    address the problem of distribution shift in offline RL, outside of\n    the intuition that it mitigates bad $Q$ estimates. This is not\n    enough motivation however, and the experiment results (Figure 5\n    and 6) does not show any significant benefit of ensemble\n    distillation over independent ensembles (besides Walker2d-random).\n    Furthermore, this may not be a fair comparison to other baseline\n    methods which do not seem to be ensembles.\n\n-   Despite a very well-motivated experiment section, the experimental\n    conclusions are far too strong. The results do not back this up,\n    when there is high overlap in the confidence intervals for many\n    Figures (in each section). Furthermore, there are unexplained\n    peculiarities in the results, such as very big drops in performance\n    or the fact that the agent trained with the offline dataset is\n    substantially better than the medium one.\n\nDetailed Comments\n-----------------\n\n-   \"pointed out that offline RL algorithms… are not amenable to\n    fine-tuning, due to the difficulty of modeling the\n    dataset-generating policy in the online setup.\"\n\n    This is not clear to me, how does fine-tuning connect to modelling\n    the dataset-generating policy? Furthermore, how does the\n    dataset-generating policy (I assume this is the behavior policy)\n    connect to online RL?\n\n-   Figure 1b: the agent is trained on the offline dataset and then that\n    dataset is thrown away for the online agent, whereas the agent in\n    red gets to keep using it. This does show the difficulty of\n    fine-tuning but does not provide conclusive evidence that\n    distributional shift is the cause. Fine-tuning can be difficult even\n    in supervised learning (Ash and Adams 2019).\n\n-   \"On the other hand, when using online samples exclusively, the agent\n    is exposed to unseen samples only, for which Q function does not\n    provide a reliable value estimate. This may lead to bootstrapping\n    error, and hence a dip in performance as seen in Figure 1b.\"\n\n    Can't this be shown experimentally in an environment to provide\n    conclusive evidence for distribution shift being the cause of the\n    dip in performance?\n\n-   Figure 3: It seems that halfcheetah-random results in a better\n    policy than halfcheetah-medium. This does not make sense to me. In\n    addition, there is a strange dip in performance for CQL-ft in Figure\n    3g. Many of the figures have overlapping confidence intervals\n    (although the shaded region is only the standard deviation, the\n    corresponding confidence interval would be approximately the\n    standard deviation. Anyway these should be confidence intervals\n    rather than standard deviation). While BRED does perform\n    statistically significantly better in some tasks, I don't think the\n    comparison is fair due to ensembling used only for BRED.\n\n-   Figure 4: Going from figure 4a to 4b, balanced replay is shown to be\n    more beneficial for a dataset generated by a random behavior policy.\n    This seems to suggest that the main benefit of BRED is artificial\n    exploration. Counter-intuitvely, this trend is reversed in Figure\n    4c. Moreover, the online-only curve is very different in Figure 4c\n    than 4a and 4b, and to my understanding there should not be any\n    difference. Due to the noisiness of the curves, its hard to make any\n    straightforward conclusion from these results.\n\n-   Figure 5 and 6: Again, there is no statistical difference in medium\n    and medium-replay between independent ensembles and a distillation\n    ensemble.\n\nMinor Comments\n--------------\n\n-   Equation 3: missing brackets on the left side.\n\n\nAsh, Jordan T., and Ryan P. Adams. 2019. “On Warm-Starting Neural Network Training.” *arXiv:1910.08475*.\n<http://arxiv.org/abs/1910.08475v2>.\n\nCzarnecki, Wojciech Marian, Razvan Pascanu, Simon Osindero, Siddhant M. Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. 2019. “Distilling Policy Distillation.” *arXiv:1902.02186*.\n<http://arxiv.org/abs/1902.02186v1>.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The idea is good, but more work is needed.",
            "review": "Summary: \nThis paper proposes to deal with distribution shift problem between online and offline samples when the agent trained by offline data is fine-tuned with online interactions. Two mechanisms are introduced: (1) using two replay buffers for offline and online data respectively, and training the agent with data sampled from these two buffers with a certain ratio (the ratio changes in a way that more online data is used in later epochs); (2) learning an ensemble of independent agents in the offline phase, and distilling them into a mean policy to overcome bootstrapping error. Empirical results demonstrate that the proposed method perform well during fine-tuning when there is distribution shift.\n \n\nStrengths:\n1. The paper is overall well-written and easy to follow.\n2. The problem studied by this paper is well-motivated, and the proposed methods are simple yet effective, making intuitive sense.\n3. The experiment section shows that the proposed method (BRED) outperforms baselines on 3 mujoco tasks with multiple types of offline data. Further ablation study verifies the effectiveness of the two proposed mechanisms.\n\n \nWeakness: \n1. Although empirical results are good, my main concern is that the novelty of this paper is limited. The main contribution of this paper is combining CQL with two tricks: balanced replay and ensemble distillation. As already discussed in the related work section, these two tricks are commonly used in the literature, even though the concrete setting varies.\n2. The specific way of combining the offline buffer and the online buffer (online fraction grows linearly in timestep) could be effective in some cases, but might not always be the best choice. I am curious whether the authors have tried other ways. And especially, can the agent chooses the ratio adaptively based on the degree of distribution shift and the learning performance?\n3. The definition of distribution shift in this paper is descriptive and informal. It will be nice to provide deeper analysis or insights w.r.t. the distribution shift problem. For example, is there a way to rigorously characterize the distribution shift? Is there a measurement for how large the gap is? This will be relevant to addressing point 2 above. \n4. Although policy ensemble can make the learned Q-function more accurate and stable, it is not very clear to me why it tackles distribution shift. Even the distilled policy has a more accurate estimation of the Q values, the estimation is still w.r.t. the offline data distribution. It still suffers from the bootstrapping error when using out-of-distribution samples. \n\n \nMinor comments: \n1. Figure 1 is a good visualization. However, the figure is not very informative since details are not provided (e.g., what is contained in a sample, in what sense / which dimension the distribution is different). \n2. In the experiment, I am wondering how an “ensemble-only” method would work and compare to baselines, i.e., use multiple pertained CQL agents, and fine-tune the distilled policy. The “Effects of ensemble distillation” paragraph shows a comparison between the proposed ensemble distillation and the ensemble of independent policies. However, the result does not reflect how ensemble compares with non-ensemble.\n3. The paper focuses on the distribution shift problem in offline RL. However, there could be multiple interpretations of distribution shift, e.g., state distribution changes due to the change of behavior policy, or dynamics distribution changes due to the slight change of the environment, etc. I think this paper is mainly dealing with the state distribution change case. But it will be better if the authors can make it explicit.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper but some issues need to be addressed",
            "review": "##### Summary\nThe paper proposes a fine-tuning method for an offline RL algorithm, CQL. The method incorporates a balanced replay scheme for both online and offline samples, and an ensemble distillation to stabilize policy learning. The proposed method is evaluated on benchmark environments. \n\nThe paper is clearly written and easy to read. However, the problem setting in the paper is not well-motivated and the goal is unclear. The paper makes some claims without supporting them (details come later). Moreover, some important experiments details are missing which makes it hard to assess the empirical results. I think all of these issues needs to be addressed before publication.  Therefore, I recommend to reject the paper.\n\n##### Supporting arguments and clarification questions\nFirst of all, I think it is not motivated why online fine-tuning procedure is necessary (see first paragraph in the introduction). If online simulation/interaction with the real environment is practical, why do we need to train an offline RL agent first and perform online fine-tuning (given that the paper claims that fine-tuning is challenging)? If online simulation is not practical, how could we perform online fine-tuning? \n\nIt is also unclear to me what is the goal we want the proposed method to achieve  (e.g., what are the evaluation metrics)? Do we want the fine-tunning method achieve the best sample efficiency compared to purely online algorithm, or achieve the best asymptotic performance compared to other fine-tuning methods? Without a clear goal, it is hard to assess the significance or soundness of the proposed method.  \n\nIn section 3, what is the reason for fine-tuning a CQL agent by SAC updates? Have you tried fine-tuning by DQN updates or its variant? I think SAC is learning a value function with an extra entropy term and CQL is learning the true value function (with some regularization during the optimization procedure), so these algorithm are essentially learning two different targets. It makes sense that we would see instability during fine-tuning since they are optimizing towards different targets. Therefore, I don’t think the clam “this instability occurs due to the shift between offline and online data distribution” is supported in the experiment. It might be due to the fact that we are optimizing different objectives. \n\nIn the experiment, how were the hyper-parameters selected? The hyper-parameters should be selected based on the offline dataset, not the fine-tuning performance. I think the paper should also include a baseline, which is the performance achieved the offline RL agents (i.e., horizontal lines in the learning curve). In Figure 3, it seems like BRED is much more stable compared to other methods in Hopper medium, so I wonder if the reason is that BRED already learns a good policy with offline dataset and it does not change much during the fine-tuning procedures. \n\nIn section 5, the paper provides a nice discussion on the related works. However, the paper claims that “this method (Nair et al., 2020) relies on regression, hence the learned policy seldom surpasses the best data-generating policy”. Can you elaborate more on this?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work",
            "review": "This paper considers the problem of policy learning in Markov Decision Process (MDP) from the combination of online and offline samples. The offline samples are generated by a behavior policy in the same MDP model, i.e., the behavior agent and the learning agent share the same state-action space. The learning procedure goes as follows. One first trains a MDP policy from the offline data; the online samples are then used to fine-tune the learned policy.\n\nThe authors propose a simple yet effective approach. First, the authors keep separate offline and online replay buffers, and carefully balance the number of samples from each buffer during updates. Then,  multiple actor-critic offline RL policies are trained, and a single policy is distilled from these policies using ensemble methods. Experiment results show that the proposed method consistently outperforms state-of-art algorithms.\n\nThis paper is clearly written and well organized. I am not sure about the novelty of the proposed method, since it seems to follow the line of carefully reweight online and offline samples. However, the experimental results show a significant improvement over existing methods.\n\nQuestion for the authors:\n1. In practice, what is a good heuristic for selecting the initial fraction p0 of online samples? How sensitive is the learned policy w.r.t. the initial fraction p0?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}