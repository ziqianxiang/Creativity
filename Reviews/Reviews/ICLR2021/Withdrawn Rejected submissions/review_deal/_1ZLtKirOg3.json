{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice practical problem. The paper is a decent focused contribution. However, some claims are not substantiated. The experimentation and the writing can be significantly improved.",
            "review": "This paper presents a neural encoder-decoder architecture for mathematical word problem generation (MWP). MWP has applications in educational applications. The approach combines representations of an \"equation based\" symbolic graph and a \"commonsense\" knowledge graph in the encoder. The decoder is mostly similar to a C-VAE. Experimental results on a newly created educational MWP dataset demonstrated that the method outperforms other simpler problem generation approaches in terms of automatic evaluation metrics and human evaluation metrics.\n\nStrengths:\nThe paper solves a very interesting problem. Mathematical word problem generation is a very practical problem, and also one that requires modeling improvements as one needs commonsense knowledge and a neuro-symbolic generation mechanism - both of which which we don't know how to do well. The idea of using a \"equation based\" symbolic graph and a \"commonsense\" knowledge graph is interesting and very reasonable. The experiments are good, the results are strong and convincing.\n\nWeaknesses:\nIn my opinion, many claims in the paper are not substantiated. For example, the paper says that the method can generate diverse problems. I am not sure what parts of the model explicitly promotes diversity (except the use of a conditional VAE). There are also places where words like personalization have been thrown in - this is not what the paper addresses as well.\n\nSome key information is lacking. For example, this is a newly created dataset, Why not evaluate on a more standard math word problem dataset like MAWPS? There is comparison with previous approaches like Koncel-Kedziorski et al., 2016. Also, there is no mention of how the commonsense graphs were created or obtained? Is this a realistic assumption? Can we use ConceptNet instead?\n\nSome more careful ablation studies need to be added. For example, how important is the symbolic graph? What if I remove some things in the architecture? For example, we use a standard VAE, use different pooling mechanisms, remove the introduction of the Levi graph?\n\nI am also uncomfortable with the use of the phrase \"planning\" in the paper. What is the planning mechanism? What plan is being followed in the generation?\n\nSome examples where the method does not do well should be included.\n\nWhere are the problems crawled from? Will the dataset be released?\n\nHow are the baselines implemented? The baselines are not properly described. Some details will help.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good paper, but suspicious of the usefulness of generated questions",
            "review": "This paper tackles the generation task of math word problems. The authors propose to construct an edge-enhanced Levi graph for equations and a commonsense knowledge graph and fuse them into a CVAE-based decoder. The experimental results demonstrate that their method outperforms existing methods including template-based and Transformer-based seq-to-seq models both in automatic and human evaluation metrics. Qualitative analysis also shows the effectiveness of the proposed approach.\n\nPros:\n\n- The paper is well written and organized. The figures effectively help describe the proposed method. Related Work Section has a good flow, briefly summarizing the trends and recent attempts of natural language generation and math word problem generation.\n- The evaluation is performed with both multiple automatic metrics and human evaluation metrics. Though the gold-standard test is small (196 samples), the authors augment unseen 15,000 samples and perform the human evaluation on them as well, which also shows the proposed method is better than existing methods.\n- An qualitative analysis illustrates that their CVAE-based model can generate diverse questions, which is also supported by the self-BLEU score.\n\nCons:\n\n- My biggest concern is that the generated questions do not have sufficient quality to use for an educational purpose. First, these questions seem to be unanswerable (only given a query text without the corresponding equation). In Table 3, MaKE does not present information on 2x - 4y (= 35), which is not implied by nouns either (like chickens and rabbits in Table 1). Second, their answers are not necessarily natural numbers, which may be inconsistent with generated queries. For example, again in Table 3, the answers are non-natural numbers which are invalid for representing the numbers of boats.\n- Likewise, as the authors mention in the ablation study, the question diversity reduces the question validity. Overall, the proposed model does not address the diversity challenge (raised in the third point in the instruction) while maintaining the question validity.\n- Therefore, I would expect the authors to discuss limitations of the current model and task formulation in order to generate more useful questions. If the above issues remain, it might not be useful to extend the proposed method for variables with high-order operations.\n\nQuestions:\n\n- In the gold-standard and generated questions, how often commonsense knowledge is required? I think Table 4 shows special cases where coefficients in the equation and the numbers of legs of animals fortunately correspond. If most of the crawled questions require commonsense reasoning, can you ensure the validity of generated queries that include different nouns?\n- For Table 5, how many workers did annotate 15,000 examples?\n\nTypos:\n\n- unsee -> unseen?\n- Some variables are not defined in the equations (e.g., W and U).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Commonsense knowledge graph is not described and the dataset is too small.",
            "review": "Summary:\nThe paper uses GGNN to incorporate equations and commonsense knowledge into math problem generation. It transforms equations and commonsense KG to Levi graph, and use the GGNN embedding as inputs to Variational Auto Encoder. It collects a new dataset that contains 1k+ problems. BLEU, ROUGH and human evaluation show it's superior to other methods.\n\nStrength:\nThe paper is a first effort to incorporate commonsense knowledge into math problem generation.\n\nWeakness:\n1.Commonsense Knowledge\n1.1 Commonsense KG not provided, not analyzed\nThe novelty of this paper lies in the commonsense knowledge incorporated in problem generation. However, there's little introduction to the commonsense knowledge graph in the paper. Where does the external knowledge graph come from? Is it collected by the authors or derived from other sources? What is the format of the commonsense knowledge graph? What are some examples of the knowledge graphs of different topics? \n\n1.2 Confusion about the number in commonsense knowledge\nAccording to the authors, the equation is transferred to a \"template\" by number mapping. For example, 2x+4y=60 can be transferred to Ax+By=M. However, there are also numbers in commonsense knowledge. For example, in the chicken and rabbit problem, each chicken has 2 feet and each rabbit has 4 feet. So the commonsense numbers are 2 and 4. However, the numbers are all input by the equation. And since it's transferred to Ax+By=M, it's the same as the model whether it's 2x+4y or 3x+6y. In this sense, it does not incorporate commonsense numbers at all. So what's the use of the commonsense knowledge graph?\n\n2.Data\n2.1 Insufficient analysis of the collected dataset\nThe paper proposes a dataset. However, it does not provide any analysis of the dataset, such as what topics are covered by the dataset, how many problems in a topic, problem lengths, number of templates, etc. \n\n2.2 Dataset too small\nThe dataset only contains 1k+ problems, which is far smaller than the existing math word problems dataset (e.g., Math23k). Since the model is rather complicated, it's doubtful that such a small dataset can be trained to get an ideal result. \n\n2.3 No data file in the code appendix\n\n3.Experiment\n3.1 Performance only slightly better than the template\nThe performances are only slightly better than the template.\n\n3.2 Human Evaluation \nThe paper does not give the standard deviation of the human evaluation, which is not convincing.\n\n3.3 Qualitative study shows poor performance\n3.3.1 In Table 3, MaKE generates a bad problem. How do we know that the small boat has 2 people and the larger boat has 4 people? The equation 2x-4y=35 is not used at all.\n3.3.2 In Table 4, the model generates several examples. But it only changes \"equal\" to \"0 less than\" and \"0 more than\", which does not show diversity.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Summary: The paper proposed a mechanism to automatically generate natural and well formed math word problems (MWPs). The goal is to understand mathematical variables, numbers etc from the given equation and generate coherent and diverse MWPs. Authors propose to use a combination of GGNN and VAE to generate MWPs. Authors consider generating MWP from linear equations with two variables.",
            "review": "\nStrengths:  The problem of automatically generating MWPs is novel and has myriad of application including solving MWPs. Using CSKG/ conditioned MWP generation is a good idea. \nWeaknesses: -  The idea of generating MWP’s from equations using external CSKG is not supported/backed by extensive experiments.  The idea of using VAE to generate diverse question is not novel, It has been widely used in question generation from text. Details about extracting subtree/sub-graph from CSKG is completely missing.\n\nOverall:  I think some more experiments on the standard datasets like MathQA, Math23k etc are needed to support the claims of the paper.  Also, It would complete the story if authors can augmented lots of automatically generated MWPs to improve performance of models trying to solve MWPs.\n\nQuestion: Is there any specific reason for not using standard MWP datasets( which also have equations) like MATHQA, AQUARAT etc for training and testing your model? Can you explain motivation behind self planning module i.e why you need to dynamically fuse information from equation and CSKG? What effect does self planning have on the final generated MWPs qualitatively? Any reason for huge drop in BLEU-4  after removing CSKG ( It needs some more explanations)?\nTypo/formatting: References require proper formatting.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}