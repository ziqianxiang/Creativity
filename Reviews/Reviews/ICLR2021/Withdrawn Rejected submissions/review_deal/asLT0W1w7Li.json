{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a model-based posterior sampling algorithm in continuous state-action spaces theoretically and empirically. The work is interesting and the authors provide numerical evaluations of the proposed method. But the reviewers find the contribution of the work limited.\n"
    },
    "Reviews": [
        {
            "title": "Interesting idea, incremental contribution",
            "review": "This paper studies the model-based reinforcement learning. They propose a posterior sampling algorithm and provide a Bayesian regret guarantee. Under the assumption that the reward and transition functions are Gaussian processes with linear kernels, the regret bound is in the order of H^1.5 d sqrt{T} where H is the episode length. The dependence that regret is in polynomial of H is a nice property. However, this advantage seems to be obtained by the assumption of Gaussian processes with linear kernels. Besides, I have the following comments.\n\n1) It seems that the result in Theorem 1 is quite straightforward from the results in Osband & Van Roy 2014. Could you justify your contribution in this result.\n\n2) What is MPC? Suppose to be model predictive control? It is not officially introduced in the context.\n\n3) In terms of computational complexity, could you elaborate more on the computational complexity of each line (component) of the algorithm. For general cases, posterior sampling could also be expensive as there are no closed-form solutions, one may need to use MCMC method.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Restricted contribution with no fundamental limit",
            "review": "The paper proposes a model-based posterior sampling algorithms with regret guarantees when the model is assumed to be drawn from a distribution randomly. The authors also provide numerical evaluations of the proposed method.\n\n- The contribution of this work as theoretical work is limited. There is no study on fundamental limit. In addition, the performance guarantee seems worse than existing ones, although a fair comparison might be unavailable due to different technical assumes. However, the authors do not provide numerical comparison to existing algorithms with performance guarantee.\n\n- It is not possible to assess the contribution from numerical comparison. There is no description on the hyperparameter selection of other algorithms (PETS, MDPO, SAC, ...). Hence, it is not reproducible as well.\n\n- The definition of $BayesRegret$ seems incorrect as it takes $M^*$ as input argument. The authors need to describe what they mean by the expectation in eq. (3). In my understanding, $BayesRegret$ should take hyper-parameter to generate $M^*$ as input, while $Regret$ taking a random incidence of $M^*$ as input.\n\n- I don't understand the meaning of regret bound $\\tilde{O} (H^{3/2} d_\\phi T)$ for non-linear case as regret of any algorithm is upper-bounded by $R_{max} T$.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unclear theoretical novelty and missing baselines in the experiments.",
            "review": "Pros\n--------\n* The paper proposes a method to balance exploration and exploitation in reinforcement learning problems whose transitions and rewards are assumed to be sampled from Gaussian processes, and provide a Bayesian regret bound. Also, the paper shows how the proposed approach can be implemented in practice using model predictive control.\n \nCons\n--------\n* It is not clear what is the novelty of the theoretical results in this paper when compared to the regret bounds by Chowdhury & Gopalan (2019), who provide both frequentist and Bayesian bounds when the transitions and rewards are in an RKHS or sampled according to a GP. The bound of Chowdhury & Gopalan (2019) seem to be polynomial in the horizon H (their paper mention a $HSA\\sqrt{T}$ bound in the particular case of finite MDPs), whereas the current paper says that \"H is still unbounded\" in their result. Hence, further clarification is needed regarding this point. \n* The method is claimed to be computationally tractable: “it can be easily implemented by only optimizing a single sampled MDP”. However, the sampled MDP is continuous, and solving a continuous MDP is hard in general. Experimentally, the paper proposes the cross entropy method (CEM) for planning: in this case, planning is not exact and the regret bound does not hold anymore. I believe this issue should be made clear in the introduction/related work section. \n* Although my main concern is the theoretical novelty, the experimental section can be improved: it would be interesting to compare the proposed approach to other strategies for exploration in deep RL, for instance\n    * Bellemare et al. (2016), Unifying Count-Based Exploration and Intrinsic Motivation\n    * Tang et al. (2017), #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning\n    * Azizzadenesheli et al. (2018), Efficient Exploration through Bayesian Deep Q-Networks;\nand also perform experiments in small simple environments that satisfy the assumptions to check if the regret is sublinear (e.g. a continuous “grid world” with noisy transitions). \n* In the nonlinear case (Section 4.2) it is not clear how to follow the steps of Yang & Wang (2019) to derive a Bayesian regret bound with feature representation, since their assumption is related to low-rank MDPs instead of Gaussian processes. In addition, it would be interesting to discuss how the model would be sampled (in Algorithm 1) in this case.\n \n \nSuggestions & remarks\n------------------------------------------------------\n* Introduction: mention which of the cited papers proves the $H \\sqrt{SAT}$ upper bound on the Bayesian regret, clarify whether $T$ is the number of episodes, or $H$ times the number of episodes. \n* Some definitions are missing:\n    * The linear kernel should be defined before Theorem 1\n    * $M^k$ is not defined before appearing in Eq. 5\n* Increase the font size of the text in Figure 1. \n* In Algorithm 1, include what are the input parameters (e.g. $\\sigma_r, \\sigma_f$). \n* Some suggestions for the proof:\n     * Write the relation between $\\Delta_k$ and $\\tilde{\\Delta}_k$.\n    * Include (possibly in the appendix) more details about the arguments in the paragraph below Eq. 9. For instance, there is an argument about a bound on the information gain, which is not defined in the paper.  Also, it might be useful (for the reader) to restate (in the appendix) the results by Williams & Vivarelli (2000), Srinivas et al. (2012) and Russo & Van Roy (2014) required for the proof.\n \nTypos\n---------\n* Abstract: $T$ instead of $\\sqrt{T}$ in the regret bound with feature representation \n* Page 9: Definition of MBPO, it should be “Model-Based Policy...” \n* Typo in integration limits in Eq. 12   (the $\\mu'-\\mu$ at the bottom should be $\\mu-\\mu'$).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good approach but not clear contribution in model-based RL",
            "review": "Review\nThis paper proposes a new model-based reinforcement learning algorithm named MPC-PSRL. Theoretically, the authors provide regret analysis of the proposed algorithm. The authors also provide empirical results that MPC-PSRL outperforms other previous model-based RL algorithms, such as PETS or MBPO.\nHowever, it is not clear that the main contribution of this paper. Osband & Van Roy (2014) already provides the algorithm posterior sampling RL algorithm, named PSRL for continuous domain. If posterior distribution of MDP is modeled as GP and optimal policy is computed by MPC, then it is the same as the method in this paper. They also provide regret analysis for continuous domain with sub-Gaussian noise model. What are the major difficulties / differences compared to the previous work?\nQuestions:\nIs there any reason there are no empirical results on Pendulum and Cartpole with oracle rewards?\nTo emphasize the effect of posterior sampling (maybe exploration), would you provide the results that using just mean instead of sampling? It is not clear whether both GP modeling and exploration via posterior sampling have a significant impact on performance.\nTypos:\nUse \\sigma_R with \\sigma_r / \\sigma_P with \\sigma_f\nTheorem 1 in page 3, d_\\phi should be d.\nIn proof of theorem 1, what is the definition of \\delta_k(r) and \\delta_k(f)?\n\nscore: 5 -> 6",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}