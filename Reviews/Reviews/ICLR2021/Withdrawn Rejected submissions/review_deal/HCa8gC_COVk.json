{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents \"stein bridge\", a joint training framework that connects an explicit (unnormalized) density estimator and an implicit sample generator via Stein discrepancy. The idea and methodology are valid and of interest. But the raised concerns were not properly addressed. "
    },
    "Reviews": [
        {
            "title": "Stein bridge is proposed to facilitate training both implicit and explicit models",
            "review": "In this paper, the task is to train an implicit and an explicit model simultaneously via GAN setting and a new regularizer called \"stein bridge\", which is constructed from the kernel Stein discrepancy between the implicit and explicit models. The idea of adding such regularization, with the notion of mutual regularization of two models, is interesting. The proposed regularization term is clearly presented, the illustration of stablizing the training procedure, and the empirical results are clearly shown and discussed. The sample quality from the generative models are compared.\n\nThere are some parts that remain unclear or can be further emphasized. \n It is said that training both explicit and implicit densities are more helpful to the whole procedure. Despite the cited literature reviews, it is unclear to me, in this paper presentation, why is this so?\nIn the paper, the implicit network is parameterized by \\theta as G_{\\theta} while the explicit EBM is parametrized by \\phi, as p_{\\phi}. \nBefore the Stein bridge is introduced:\nHow do \\theta and \\phi interact? From figure1 it seems they do not interact during training but only coupled via the objective. In addition, which of the model (implicit or explicit) is used as the final outcome?\nAfter the Stein bridge is introduced:\nstein bridge tries to minimize the Stein discrepancy between implicit and explict models. How is the EBM chosen so that the density class is rich enough? How are \\lambda_1 and \\lambda_2 chosen to balance three terms?\n\nHow does the training of generative model compared to the learning procedure in \"Deep energy estimator networks.\" Saremi, Saeed, et al.  2018, which learns a generative model from score-matching based criterion?\n\nThanks for the presentation.  ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper gives a solid and comprehensive analysis and implementation of the idea of Stein bridge, but the motivation may need further discussion.",
            "review": "Pros:\n* The idea to bind an explicit and implicit generative model and study the effect on both models is a valid research topic.\n* The method seems novel and inspiring, and the paper also shows a theoretical understanding of the proposed method, which is technically nontrivial.\n* The presentation is clear and pleasing (e.g., content organization, background, Fig. 1). The paper also includes a detailed review on existing works.\n* The paper presents comprehensive experiments, and the results are promising.\n\nCons:\n* On the motivation.\n  - Would it be too costly to train two models for one task just to alleviate the problem of one model? It seems to hide the problems of each model and serves as a black-box solution. The theoretical analysis makes things better, but the explanations may seem to be like \"side effects\" but not a direct solution targeting on the problems. Moreover, both the explicit and implicit models have the same amount of knowledge from data: one model cannot provide more information to the other model beyond the training dataset. How to understand the improvement under this perspective?\n  - For training an explicit model, the mode-collapse behavior may be due to the usage of the Stein discrepancy. Training by maximizing likelihood (i.e., minimizing forward KL divergence) via classical methods e.g. contrastive divergence may already circumvent this problem.\n* On the theory. It may be better to explain why \"By smoothing the Stein critic, the Stein bridge encourages the energy model\nto seek more modes in data instead of focusing on some dominated modes\". How does it make the Stein discrepancy more picky on missing a mode?\n* On the experiments.\n  - I see in the supplement that a hyperparameter searching is conducted, but I did not find the metric to select them. Is it done by AUC / IS / FID / MMD / HSR / manual visualization evaluation? Results of \"WGAN + something\" may be sensitive to hyperparameters and maybe they should not be worse than the vanilla WGAN (taking zero regularization).\n  - In Figs. 4 and 5, how are the density estimation of implicit models GAN/WGAN and samples of explicit models DEM/DGM got visualized? Do they rely on techniques like kernel density estimation or MCMC? If yes, how to make sure these techniques do not affect the outcome?\n  - The definition of High-quality Sample Rate (HSR) may value mode collapse, since it gives a high score if all generated samples are on the center of one mode. For the result in Fig. 8(a), may be the HSR needs to drop to be faithful to data.\n  - The experiment is comprehensive and shows the desired improvements. But maybe a comparison with other explicit model training methods (contrastive divergence, annealed Langevin dynamics, etc.) that avoid mode-collapse is also needed.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very similar to a NeurIPS-19 paper",
            "review": "This paper adopted Stein's method to connect an explicit density estimator and an implicit sample generator to propose an objective function for deep generative learning.\nThis paper is written well and the organization is very clear.\nHowever, this paper is very similar to a NeurIPS-19 paper (Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test).\nBesides, the authors didn't cite this paper in the current submission.\n\nFirst, the top-level idea is the same.\n1. They all adopted two generative models. One is explicit and the other is implicit.\n2. They all adopted Stein's method to connect these two generative models.\n\nSecond, important technical details are similar.\n1. They all used energy-based model in the explicit part.\nThe energy-based model is used to mimic the underlying distribution of the real data.\n2. They all used Stein's method to avoid solving the normalization constant.\nStein's method is a likelihood-free method that depends on the distribution only through logarithmic derivatives.\nWhen taking derivatives, the normalization constant will be eliminated.\n\nThird, these two papers have the same target.\nThey hope the explicit generative model characterize the formulation of the distribution and\nthe implicit one produces vivid or genuine-looking images.\n\nThe novel part of this submission is the introduction of kernel Sobolev dual norm and Moreau-Yosida regularization.\n\nThere is a big gap between the optimization formulations in Equation (3), Theorem 1 and Theorem 2 and the experimental results shown in Section 5.\nBesides, there are no open source codes provided.\nIt is very hard for me to figure out the details of the experiments and meantime to check the reproducibility of this paper.\n\nIn summary, I hope that the authors will correctly cite the closely related NeurIPS-19 paper,\nand clearly demonstrate their completely new contributions as compared to the NeurIPS-19 paper.\n\nSince ICLR is a highly selective conference,\nthe originality and significance of one submission will always be in the first priority.\nAlthough the writing quality of this paper is good, I cannot accept this paper in current state.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting paper with some gaps in theoretical analysis",
            "review": "Summary of the paper:\n\nThe author proposed a novel regularization technique for jointly training of implicit model (IM) and explicit model (EM). This is achieved by connecting the training of IM and EM via Stein discrepancy (SD), which is called Stein bridging. \n\nThe author claimed this regularization can (i) smooth the Wasserstein critic by kernel Sobolev dual norm (ii) smooth the Stein critic by Moreau-Yoshida regularization (iii) stabilize the training of IM. \n\nThe author also theoretically proves the (i), (ii) under Wasserstein distance, and (iii) for a simple toy case.  The author empirically evaluated the EM by inspecting the mode coverage of toy example, ranking digit and OOD detection on more complex data sets. \n\n-----\nReviews:\n\nClarity: The main text of this paper is in general clearly written and easy to follow. However, I have some concerns related to the theoretical analysis in the Appendix. \n\nNovelty: This regularization technique seems to be novel to the best of my knowledge. Although the idea is simple, the author provided some theoretical analysis to back it up. However, it is not enough to fully back the claims made in the main text. Details later.\n\nTechnical soundness and concerns: I have some concerns related to the SD and proof for theorem 1 and 2. \n1. For SD, the author mentioned that for Stein critic $f_s(\\pmb{x})$, it is not necessary $\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. One can specify a lower dimension $d'<d$ and make $f_s:\\mathbb{R}^d\\rightarrow \\mathbb{R}^{d'}$ as long as $f_s$ belongs to the Stein class. Indeed, this is true for Stein's identity (see Def 2.1 in Liu's paper). However, this does not mean it defines a valid discrepancy measure. The original SD in Gorham's paper assumes $d'=d$ and the trace operator is used to transform $d\\times d$ Stein identity to the scalar value. In that case, Gorham proves its validity by investigating its weak convergence property. My concern is I cannot see the direct generalization from the trace operator to other matrix operators like the ones used in this paper with $d'=1$. In other words, I agree that for two distributions $p$,$q$, when $p=q$, the SD defined in this paper is $0$, but not vice versa. Could the author point out any references or provides any details on the validity of the proposed SD?\n2. I do not fully follow the derivations in Appendix C.1. In page 15, how do you introduce the auxiliary variable $r^2$? Why there are two $\\min$ operations instead of one $\\min$ with jointly optimizing $h$ and $r$? Could you elaborate more on this and also how do you get rid of $r^2$ in the constraints in the second equality?\n3. I am also a bit confused about derivations in Appendix C.2. How do you combine the $\\mathbb{P}$ and $\\gamma$ in one $\\min$ operation instead of $\\min_{\\mathbb{P}}\\min_{\\gamma}$? (In page 15)\n4. In the main text, the author claimed that other objectives can be used for training implicit the model such as JS divergence. However, the theoretical analysis (theorem 1 and 2) only shows the regularization effect of Stein bridging is only for Wasserstein-1. So the analysis won't hold for other objectives like JS divergence. \n5. It is known that Stein based divergence is a weak objective for training EBM (see Liu 2016). Therefore the regularization technique may help a lot, like the mode coverage demonstrated in the experiment. I wonder if SOTA training method for EBM is used (like SSM in Song 2020, Song 2019), does this regularization help the training, because this regularization is not cheap to compute (higher than the SOTA method for EBM).\n6. In figure 4, I cannot find DEM and EGAN in the density plot.\n7. In table 2, it seems that the training of DEM is failing as the AUC is closed to 0.5. Any guess on why it fails? How do you pre-process the data set? Do you add different scales of noise in the images to smooth it for the EBM to learn the distribution (like the trick used in Song 2019)?  \n\nSummary:\nI am quite interested in this approach. But I am a bit concerned about the theoretical analysis and the true advantage of training EBM with Stein bridging compared to the cheaper SOTA EBM method.\n--------\nLiu, Qiang, and Yihao Feng. \"Two methods for wild variational inference.\" arXiv preprint arXiv:1612.00081 (2016).\n\nSong, Yang, et al. \"Sliced score matching: A scalable approach to density and score estimation.\" Uncertainty in Artificial Intelligence. PMLR, 2020.\n\nSong, Yang, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in Neural Information Processing Systems. 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}