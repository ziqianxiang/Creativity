{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers unanimously raised concerns over the clarity and technical correctness of the theory and the Imagenet experiments during the first round. The authors submitted a highly revised version during the rebuttal which allayed concerns for multiple reviewers, however all the reviewers raised the concern that the paper has gone through a very significant change, almost becoming \"a new paper\" in the rebuttal phase and should go through another cycle of resubmission and review to correctly judge the contributions and claims.  The reviewers were aligned in their judgement that the original manuscript with minor changes was not ready for publication. The authors are strongly encouraged to polish the version submitted during rebuttal and resubmit.  "
    },
    "Reviews": [
        {
            "title": "Interesting connection between quantization-aware training and natural gradient, but the empirical evaluations require substantial changes",
            "review": "The authors propose an interesting method to train a quantized neural network with a natural gradient, which considers the curvature information of the model. They also provide an efficient way to approximate Fisher Information Matrix using the connection between quantized and full-precision numbers. Although this idea is interesting and novel, I am concerned about some assumptions made by the authors and the empirical performance of the proposed method. \n\n1. The authors assume that the connection between w and \\hat{w} on the manifold can partially reflect the curvature distribution. Because this assumption is curial for the approximation of Fisher Information Matrix, it would be great if authors could provide more justifications on why such an assumption holds.\n\n2. The results for ImageNet seems misleading. As shown in Table 2, The authors didn't report the same number of baselines for each model. Also, the bitwidth of the baselines are different, which is hard to evaluate the performance fairly. For example, they show the results of proposed ONGD-large with 8 bits weight and 8 bits activation for ResNet50, but they report some other baselines with the setting of 4 bits weight and 4 bits activation. \n\nIn general, I think this paper provides an interesting connection between quantization-aware training and natural gradient optimization. I would champion this paper if the authors can provide fair and detailed empirical results.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The final method seems to be not based on natural gradient and the claimed connection is not explained",
            "review": "## Summary \n\nThe paper proposes a neural network quantization approach with a tanh-based quantization function and claims that it is a good approximation to the natural gradient method. In addition, they augment the cross-entorpy loss with a knowledge distillation type loss (between the logits of the real network and quantized ones while training) to improve the performance. The results seem to show marginal improvements over compared methods in cifar and imagenet datasets.\n\n## Strengths\n\n1. The idea of using a knowledge distillation type loss between the logits of the real network and quantized ones while training is interesting and could be one of the reasons for performance improvement.\n2. Overall the paper is clearly written.\n3. Reasonable experiments are conducted (even though comparison with some recent methods are missing) and the results show marginal improvements over compared methods.\n\n## Weaknesses\n\nThe main weaknesses of the manuscript in my opinion are as follows:\n1. The final method seems to be not based on natural gradient and the claimed connection is not explained: \n\t- In fact, the final method simply uses a tanh-based quantization function (Eq. 10) and simply uses SGD with STE approximation. There is no explanation of why this approach closely approximates the natural gradient method, given that this the main claim of the paper and even the title contains \"natural gradient\". Definition 2 might be coming from a textbook (even though no reference is given) and it simply provides an existence condition and it does not say anything about the proposed exponential map corresponds to the manifold induced by the Fisher Information Matrix. This is critical for the paper and without this connection the whole discussion about natural gradient is redundant and could be deemed as \"decorative maths\". Please explain (with supporting theory) how Eq. 10 relates to FIM.\n\t- Given that the method is not based on natural gradient, there are many unsubstantiated claims. Examples include: 1) \"unique limitaions of the QNNs to construct curvature information\" 2nd last para in the introduction, 2) \"demonstrated feasibility of training low-precision models using natural gradients\" first para of Sec. 3.2, and 3) \"our method .. avoids the risk of neural networks falling into the local minimum\" in conclusion.\n\n2. Misinterepretations and inconsistencies in mathematical writing/notations:\n\t- Eq. 8 has many issues. First, what is $vec(dL/dw_i)$? If $w_i$ is a scalar, $dL/dw_i$ is a scalar. So what does it mean to vectorize it?\n\t- To my understanding $F_{ij} = g_i g_j$ where $g_i = dL/dw_i$. There is no vector representation and no need for Kronecker product etc. Please clarify.\n\t- Please clearly mention the dimensions of each variable and define them appropriately for all the equations. Also, do not use same symbol to denote two things. Examples include $Q$ (Eq. 5, 10) and $\\alpha$ (Eq. 10, 14).\n\n3. No references given for the statements related to Riemannian geometry:\n\t- For both definitions 1 and 2 no reference is given. It seems like there are existing results and they should be cited so that someone can check/verify their applicability in the stated problem setting.\n\n4. Recent quantization techniques are not compared:\n\t- For cifar experiments only comparison is done against the first quantization method BinaryConnect (Courbariaux, 2015) which is very old. Especially, comparison against other relaxed quantization methods [a,b] are required.\n\t- Even for imagenet experiments important comparisons are missing such as [c,d].\n\n## Minor Comments\n\n1. Please provide the equation for FIM before Eq. 4.\n2. Second para in page 4: what is meant by \"closely related\"?\n3. Please provide all the experimental details required for reproduction (eg, any layers kept floating point in imagenet experiment?, what initialization used?, starting from scratch or pretrained network, etc.)\n4. After fig. 2, please tone down. The figures only show a marginal improvement in test error not \"significantly preventing overfitting\".\n\n## References\n\n- [a] Bai, Y., Wang, Y.X. and Liberty, E., 2019. Proxquant: Quantized neural networks via proximal operators. ICLR.\n- [b] Ajanthan, T., Dokania, P.K., Hartley, R. and Torr, P.H., 2019. Proximal mean-field for neural network quantization. ICCV.\n- [c] Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B., Huang, J. and Hua, X.S., 2019. Quantization networks. CVPR.\n- [d] Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W. and Cheng, K.T., 2018. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. ECCV.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Technical correctness and clarity might need work",
            "review": "In this paper, the authors proposed a new method to train DNN (partially) with fixed point weights and approximated natural gradients. The contribution of this paper comes in three folds. First, they propose to use an exponential map to encode the curvature information instead of using the inverse of Fisher information matrix. Secondly they propose to use this technique to compute an approximation of the natural gradient. Lastly, the authors proposed to train low precision DNN using a combination of this approximated natural gradient and an additional loss term which enforces consistency across a full precision and the low precision model weights. Empirically, the authors demonstrate that the proposed method can attain better validation accuracy than plain SGD when training fixed point low precision convolutional models.\n\nStrong points: empirically the proposed partially low precision training method in general attains meaningfully higher validation accuracy than plain SGD and other baseline methods (the plain SGD and baseline methods use a pure low precision fixed point model representation).\n\nConcerns: I have 3 major technical concerns and questions which lead to my initial rating to reject.\n\n1. In the high level, is the proposed method targeting at low precision training with less training compute resource, or is it just aiming at to attain a low precision model that can be efficiently deployed? This is not clearly elaborated in the paper. \n     \nIf it is for training with low compute resource, the proposed training algorithm will involve forward and backward evaluation of a full precision model. The additional compute from the low precision model weights can only increase the training overhead compared to a pure full precision training algorithm. \n\nIf the proposed method is just for attaining a low precision model for efficient inference, then the experiment protocol is not convincing to me. Specifically, the authors does not compare to post-training quantization after full precision training methods. Instead, the major baselines in the paper is training methods with low precision compute during training directly.\n\n2. The definition of straight through estimator in this paper is not correct for general low precision fixed point training. The definition in the paper is only for 1 bit (binary) model weights; however in the experiment the authors considers 4 bit fixed point number. The straight through estimator for non binary model weights will involve indicator functions depending on where the nearest quantization step is.\n\n3. In the ImageNet experiment, the authors use an already well trained full precision model to jump start the proposed partially low precision training algorithm. Is the full precision validation accuracy numbers directly fro them model weights which jump-starts the proposed method? If yes, then the fairness of the comparison needs to be enhanced. This is because the proposed methods is using additional training steps to build on top of the well trained full precision model weights.\n\n\nComments for improving the paper:\n1. The vector notations in equation (8) needs to be clarified in the paper.\n2. Citation formatting needs to switch to \\citep instead of \\cite in the second paragraph in intro section.\n4. In equation (14) and (15), it might need to explicitly define which weights is updated in each training phase.\n\n\n\n\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Modification of Gradient in Quantization Training",
            "review": "This paper proposed to incorporate local curvature information in the parameter space to assist the training of quantized model. Specially, it utilized natural gradient, with the complex Finsher Information Matrix replaced by a smooth quantization. It further used distillation from full-precision model and an alternative update between quantized/full-precision model to improve performance.\n\nCons:\n1. Experiments comparison in Table.2 is strange: comparsion should be made based on same bit width of weights/activation.\n\nI have the following questions:\n1. Author spent the whole Sec.3.1 to describe usage of natural gradient in quantized training. However, it replaced the core part (FIM) in Eq.(8) with a smooth quantization (Eq.10), which is published before by another work. Is there any intrinsic connection between smooth quantization and natural gradient? Author mention that such replacement is a hypothesis, which quite confused me, since it seems that Eq.(10) can hardly capture curvature information as claimed by the author.\n2. This work used distillation and training of full-precision model. It is better to conduct ablation study to validate the effectiveness of proposed gradient modification and distillation.\n3. ONGD is compared against SGD. However, quantized model can also be trained via other optimization method, such as Adam, especially in large model. How the performance of ONGD compared to Adam?\n4. According to Eq.(11), gradient of full-precision parameters is a multiplication of gradient of quantized parameter and a tanh function. If so, author should compare with other quantization training method which also modify the graident, such as [1]\n5. Since performance gap in CIFAR is narrow. Is repeated experiments conducted to reduce variance?\n\n[1] https://papers.nips.cc/paper/8647-metaquant-learning-to-quantize-by-learning-to-penetrate-non-differentiable-quantization",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}