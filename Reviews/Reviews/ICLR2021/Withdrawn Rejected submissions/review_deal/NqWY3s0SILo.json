{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work was deemed interesting by the reviewers, but they highlighted the following weaknesses in this version of the paper:\n\n- Lack of comparison to other methods.\n\n- Lack of novelty compared to previous work.\n\n- Fundamental problem with training only on one dataset (MNIST), issue with possible overfitting."
    },
    "Reviews": [
        {
            "title": "Reasonable approach with limited novelty",
            "review": "The paper proposed (1) to use a graph neural net to predict the performance of network architectures, (2) to query new architecture proposals to try next from the predictor, and (3) to iteratively refine the predictor using the collected dataset using gradient descent. Overall, the method is well-motivated and the paper is easy to follow.\n\nMy main concerns are as follows:\n* In section 2.1 the authors claim that \"In contrast, our method directly optimizes the discrete architectures, avoids the drawbacks of continuous relaxation\". This is not trueâ€”the method still relies on continuous relaxation because of \\bar{\\alpha} in equation (4), and is still subject to discretization discrepancies due to the quantization function q.\n* Predictive methods with GCNs are not new, especially considering that Wen et al., 2019 also trained GCN using gradient descent (although in an offline fashion). The only additional ingredients here seem to be (a) interleaving GCN with the data collection process in an online manner, and (b) weight sharing. However, the advantage of (a) has not been empirically verified with ablation studies and (b) is already common nowadays.\n* It seems a bit unconventional that the authors reported the best architecture in the main paper but the mean & variance in the appendix. IMO Figure 5 in Appendix C provides a much more accurate picture of the usefulness of different algorithms than Figure 4. In practice, it can be tricky to tell the \"best\" architecture for new tasks where test sets are not available.\n* The improvements are within the range of variance as compared to SPOS. While the results are still overall positive, it may not well justify the additional implementation complexity of the method.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple method with promising performance; experiments not fully convincing",
            "review": "The authors address the Neural Architecture Search problem. At the core of their contribution is an architectural improvement; performance prediction of a considered architecture is much better when using a particular graph neural network on (softened) architecture topology. The rest of the NAS pipeline is naturally built around this observation and the final performance looks quite strong.\n\nI think the topic is of sufficient significance to the community and I find the proposed method well-tailored to the problem and sufficiently original. Also, the writeup is easy to follow (up to minor issues listed below).\n\nSince the contribution is mainly in finding a good architecture for the surrogate model, I find it very unsatisfying that all experiments are carried out on CIFAR-10. The ML community has had some bad experience with architectures overfitting to datasets and more thorough evaluation is needed. Also, the prime purpose of the NAS line of work isn't to find a good architecture for CIFAR-10, and if this method aspires to have a broader impact in the NAS community (which I think it should), the experimental section needs to be expanded.\n\nMinor points:\nEq 3: I don't understand. Due to the $\\textrm{sign}$ function, the loss does not seem differentiable w.r.t $\\tilde{y}_i$. Or do the brackets need some regrouping?\nThe end of Sec 5.1: Currently $\\tau$ cancels in the definition of $q$. I guess it should be inside the brackets. Also, what does it mean to \"practically\" take a limit? The point seems to be that softmax with very low temperature reduces to a hard one-hot vector but I do not understand how the authors use this precisely. Why not \"round\" to the one-hot vector right away?\nSec 2.2: models -> model\nSec 4.1 donated -> denoted?\nExperiments: One ablation that I think would be great for giving insight to inner working of the method would be the following: In Figure 4, include also the performance curve of GOAL with the MLP architecture for rank prediction. This should nicely demonstrate where your main technical point is.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel but should compare with NAO and evaluate on ImageNet or CIFAR10.",
            "review": "#### Summary:\nThis work propose Graph Optimized Neural Architecture Learning, that uses a differentiable surrogate model to directly optimize the graph structures. More specifically, the surrogate model takes a graph structure as the neural architecture embedding and predicts a relative ranking, then applies gradient descent on the input graph structure to optimize the neural architecture. GOAL demonstrates superior performance compared to SoTAs.\n\n#### Weakness:\n\n-First, the method is quite similar to NAO, where an encoder and decoder approach the maps neural architectures into a continuous space and builds a predictor based on the latent representation. The only difference is that NAO is use a decoder to decode the optimized latent representation back to architecture representation while here GOAL applies gradient descent on a graph neural network. Also, how to decode the parameters back to the neural architecture discrete representation is not clearly explained in the paper.\n\n-Second,  this method can work well for small models and small search spaces, but can be hardly applied to larger models. Training the surrogate function for a larger search space or larger models can take more samples and more training time (e.g. large models takes much longer time to train, thus even a proxy accuracy should take more time to evaluate). A parameter sharing scheme can be very inaccurate in the beginning therefore results in suboptimal architecture selection. The inaccuracy is compounded when using the same model for both a surrogate function and neural architecture search. The evaluation on only NAS-bench partially verifies the reviewers concerns. \n\n#### Detailed feedback:\nThe reviewer would like to suggest several fixes to this paper:\n-First, try to compare to better baselines (e.g. NAO, more recent differentiable search work) on not only NASBench, but on real CIFAR10 or ImageNet workloads. \n\n-Second, compare a non graph neural network based approach with GOAL and show the necessity of using a graph neural network.\n\n-Third, be more clear about decoding back the graph neural network parameterizations to the neural architecture representation. Include more details on the number of samples used to train the surrogate function and hyperparameteers used in algorithm 1. \n\n\n[1] \"Neural Architecture Optimization\", https://arxiv.org/abs/1808.07233",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}