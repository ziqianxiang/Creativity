{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for Knapsack Pruning with Inner Distillation",
            "review": "### Overview\nIn this paper, the authors proposed a novel pruning method by formulating the network pruning as a Knapsack Problem, optimizing between accuracy and the cost. They also proposed Inner Knowledge Distillation to recover the accuracy of the pruned methods.  The proposed methods can be applied to non-sequential models like EfficientNet and MobileNetV3.\n\n### Clarity:\nThe paper is generally well written. The methodology is explained clearly with equations and figures. The format of Table 3 and 4 do not fit the template. It would be better to adjust it narrower. \n\n### Pros:\n1. The paper explicitly explains how to compress non-sequential networks with residual connections or SE layers. \n2. The compression results in Table 2 are impressive. It outperforms already efficient network designs like MobileNetV3 and EfficientNet.\n\n### Cons:\n1. My first concern is about novelty:\n- Optimizing the trade-off between accuracy and cost (FLOPs or latency) in model compression has been widely studied before (e.g., [He et al., 2018b] and [a]). Whether modeling the problem as a Knapsack problem or using evolution search/RL to directly optimize the trade-off are just different optimization methods on the same problem. \n- Existing work (like [He et al., 2018b] and [a]) also studied the pruning of non-sequential models like ResNets and MobileNetV2, where the convolutions with residual connection are also pruned. Actually, to prune such structures, we just need to remove the corresponding channels in multiple layers at the same time, which seems quite straight-forward and was not considered a novelty in prior work.\n- Using Taylor expansion to estimate the importance of kernels for the final loss is widely performed in prior work (e.g., Molchanov et al., 2017), which is also acknowledged by the authors. \n-  The IKD method by distilling from the inner feature maps is also well explored (e.g., [Romero et al., 2015] and [b]). Simply apply it to pruning is hardly a novelty. Even in the pruning scenario, using feature map distillation has been explored in [c] (they also use M_l to match the activation, so the formulation is almost identical). \n2. Consider 1, there are multiple overclaim in this paper. For example:\n- \"we are the first to suggest a pruning method that applies effectively to a non-sequential architecture such as EfficientNet.\"\n- \"we are the first to use a feature maps distillation for pruning\".\n3. The original ResNet-50 accuracy in this paper is about 78%, which is 1-2% higher than results in the existing literature. This advantage may affect the accuracy-FLOPs trade-off when compared to existing work, making Figure 2 biased. If we just consider the accuracy difference (i.e., accuracy drop) in Table 3, the advantage is much smaller.\n4. It would be better to also compare with recent SOTA based on NAS like [b].\n\n[a] Liu et al., MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning\n\n[b] Zagoruyko and Komodakis, Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\n\n[c] Li et al., GAN Compression: Efficient Architectures for Interactive Conditional GANs\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Knapsack seems intuitive and novel, inner distillation is similar to a previous work [1]",
            "review": "The paper proposes to formulate the pruning problem as knapsack problem, with the item weight being flops and item values being approximated loss. The inner distillation fine-tuning process matches student and teacher's features. The results are promising.\n\nStrengths:\n1. The formulation of the pruning problem into a Knapsack problem is a novel and intuitive approach. The paper develops the approximation approach one step at a time and states the reasons clearly. It also has the strength of easy incorporation with skip connections through grouping of channels across layers within a block.\n\n2. The results are promising. The approach seems to outperform compared methods by a large margin. It can also improve the already very high-performing EfficientNet.\n\nWeaknesses:\n1. The paper \"Few Sample Knowledge Distillation for Efficient Network Compression\" [1] is highly related, and can be argued to be very similar to the inner knowledge distillation part. They both essentially try to match transformed students' features with teachers' features, if I'm understanding it right. This hurts the novelty and the quality of related work of the paper. I suggest including a short discussion of this.\n\n2. There are two places that refer to other works that might be a bit exaggerated, despite I acknowledge these are advantages of the method: 1. Section 6. \"even though most of the other methods fine-tune for more than 100 epochs\". I think there are also many works that do not fine-tune for more than 100 epochs. 2. In related work, \"To date, most pruning methods are restricted to sequential connections as non-sequential connections.\" I think there are a number of approaches that are capable of pruning ResNets, not in such a restricted way the text later described. At both places, I suggest the authors include references, including both positive and negative ones.\n\n3. The pipeline of the whole implementation seems not very straightforward. It would be great if the code can be released.\n\nOverall I still think Knapsack formulation is a valid development of pruning method, but the inner distillation part could be better acknowledged as using/borrowing from a previous method [1] in this context.\n\n[1] Few Sample Knowledge Distillation for Efficient Network Compression. Li et al. CVPR 2020\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple idea but lack of novelty. ",
            "review": "This paper formulates the pruning as a Knapsack problem by regarding the selection of channels as the target 0-1 variable. To do so, this paper takes the change of loss as the value for each selection, and the FLOPs as the capacity constraint. Moreover, this paper adopts feature-based KD to boost the pruned network. Experiments validate the effectiveness of the proposed method. \n\nPros: \n+ Formulation of Knapsack problem for pruning is natural and makes sense. \n+ This paper is readable and makes some mathematical analysis. \n+ Experimental results seem convincing. \n\nCons:\n- The idea is basically sensible. However, the formulation of value in Knapsack problem for pruning needs justification. For example, the change of loss may vary during the end-to-end training. \n- The idea of inner KD is simple and plain. It is simply the FitNets (ICLR2016).  I do not think it is actually a new method. \n- In Eq(6), authors say it is observed that approximation leads to better performance. However, no ablation study is given. \n- In recent pruning papers, pruning usually leverage training from scratch to report the final accuracy instead of the fine-tuning used in this paper.\n- Compact neural networks such as MobileNetV2 are not included in the experiment. \n\nSome claims need justification. \n\n-  This is only possible due to our knapsack formulation. \n- To the best of our knowledge, we are the ﬁrst to suggest a pruning method that applies effectively to a non-sequential architecture such as EfﬁcientNet. \n------ Pruning papers deal with MobileNetV2 also similarly, which is a routine for channel pruning. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Necessary discussions and comparisons are required",
            "review": "This paper presents a novel network pruning algorithm that formulates network pruning as a Knapsack Problem and applies an Inner Knowledge Distillation mechanism to improve the performance of the pruned network. Though the motivation of this paper might be interesting, the proposed method is not described explicitly, and the approximation applied in this paper remains to be proved. Besides, the experiments in this paper are not convincing and thorough, one of my major concerns is that additional training technique, i.e. knowledge distillation, is applied in the finetune stage which makes the comparisons controversial. Considering these factors, I don’t think this paper meets the standard of this conference.\n\n#Comments in detail:\n1.\tTo cast pruning as a Knapsack Problem, this paper proposes an approximation to estimate the importance of every channel in Eq.6. However, this approximation is intuitional without necessary theoretical proof. More discussion should be given about this approximation.\n2.\tThe novelty of Inner Knowledge Distillation (IKD) is not strong enough, feature maps distillation is not a novel idea although you may be the first one to incorporate it with pruning. IKD might be regarded as a training technique rather than a major contribution of this paper. \n3.\tThe experiments are not convincing enough. Obviously, this paper takes advantage of the knowledge distillation technique in the finetune stage, which makes the comparison with other pruning methods meaningless. Results without the IKD technique should be given to prove the effectiveness of the proposed method. \n4.\tIn Sec.4, it is claimed that ONLY the proposed method is capable of pruning non-sequential convolutions, I doubt it. In fact, the proposed method can not prune this kind of structure directly unless some modification of structure is applied. More detailed information and visualizations should be given to provide a better understanding of how the proposed method solves this problem and why other methods are not qualified.\n5.\tIn Table 2, what does “High” represents? In Sec.6.1, “We took ResNet-50 as backbone and experimented with two variants: (i) With and without IKD, and (ii) our baseline training vs. PyTorch baseline.” What does “our baseline training” means? Does it mean how you train your baseline network? If so, you should give more detail about how the baseline network is trained.\n6.\tIn Sec.5, it’s demonstrated that only 0.1 GPU hours are used to prune the network, which is impressive. However, it is recommended to specify that the cost of training the baseline network is not included.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}