{
    "Decision": "",
    "Reviews": [
        {
            "title": "Critical flaws in assumptions and experiments; important descriptions and experiments are missing.",
            "review": "This manuscript proposes a method called incremental graph learning (IGL) that aims to apply a lightweight learning on growing graphs. The key idea is to selectively sample nodes and edges from the previous graph and merge this subgraph with the newly generated nodes/edges. The resulting graph is the main target where a graph learning algorithm (e.g., GCN and GraphSAGE) is applied. The target graph is generated under the assumption that time and space are restricted. Two strategies (sample-based and cluster-based methods) are proposed to select nodes/edges from the previous graph.\n\nThe manuscript tries to solve an incremental learning problem on graphs, which is an important problem in practice. Unfortunately, I found some critical flaws and limitations of the proposed method. I reject this manuscript mainly because (1) the main assumption about growing graphs is too restrictive, and thus the proposed method cannot handle many realistic situations, (2) empirical simulations do not appropriately simulate growing graphs, (3) important details about the proposed method are missing and unclear, (4) the contributions of theoretical analysis are not clear and the analysis does not match with the proposed method well, and (5) important experiments are missing or not shown. My main concerns are described below.\n\n1. The main assumption about \"growing graphs\" is too restrictive and unrealistic. According to the authors' assumption, no edge can be created between any pair of existing nodes and also no node/edge can be removed from the initial graph. Obviously, this cannot be considered as a realistic situation. In many real applications, edges are created between existing nodes, which can change the overall structure of the graph a lot meaning that new representations should be learned. Also, nodes and edges can be removed from a graph over time. The proposed method cannot handle these realistic situations because all the steps require $\\mathcal{V}^{new}$ and the structure of the previous graph is assumed to be static and fixed.\n\n2. There are critical flaws in the experiments. The authors' experiments fail to correctly simulate the growth of real graphs. In their experiments, it is assumed that the number of nodes in a graph linearly grows such as $n, 2n, 3n$, and so on where $n$ is the number of nodes in the initial graph. This setting is too artificial and a real graph will never grow in that way. As I also described in the first point, some nodes can be removed from a graph in many real situations. Given this, just counting the number of nodes does not correctly reflect the dynamics of graphs. For example, starting with $n$ nodes, assume that $n/2$ nodes are removed and $n/2$ nodes are newly joined. Then, the structure of the graph can be significantly changed while still having $n$ nodes. In practice, one should apply the incremental learning on graphs with a fixed time interval, which results in varying sizes of newly added nodes and edges. The entire experiments should be redesigned using some real-world graphs with time stamps.\n\n3. For cluster-based strategy, many important details and important experiments are missing. The clustering procedure is not described well. It is hard to understand how the authors performed clustering, how they initialized clustering, and how they controlled randomness. Also, how to select the number of clusters is never described. In the experiments, the authors should show how the number of clusters affects the performance of the proposed method.\n\n4. According to Theorem 4.3, the unbiased and edge-preserved conditions do not hold on either sample-based graph or cluster-based graph. Those conditions hold only on the entire graph that occurs in $\\textit{joint training}$. But this point is not clearly stated in the manuscript, and some explanations are misleading. I'm wondering if those theoretical analyses (Theorem 4.1~4.3) are essential parts of this manuscript given that those theories cannot say much about the proposed method. Section 3.3 is poorly written. I guess the authors wanted to argue that the cluster-based strategy partially satisfies the aforementioned conditions; but this point is not clear.\n\n5. Instead of sample-based and cluster-based methods, one can choose all the nodes and edges (from the previous graph) that are connected to $\\mathcal{V}^{new}_t$.Â As discussed in the manuscript, simply taking $(\\mathcal{V}^{new}_t, \\mathcal{E}^{intra}_t)$ drops the inter edges and loses important information (this strategy corresponds to $\\textit{finetuning}$ in the manuscript). Thus, to add the inter edges $\\mathcal{E}^{inter}_t$, sample-based and cluster-based methods are proposed. However, instead of selecting some nodes/edges using these strategies, one can simply take all the nodes connected to $\\mathcal{V}^{new}_t$ and corresponding edges from the previous graph to introduce $\\mathcal{E}^{inter}_t$ in the target graph $\\mathcal{G}^{L}_t$. If the experiments are correctly designed, the number of newly generated nodes should be much smaller than the total number of nodes (this is why the \"incremental\" learning works). Since real-world graphs tend to be sparsely connected, taking all the nodes connected to the newly generated nodes from the previous graph does not require too much space. The authors should consider this obvious solution as a baseline and compare it with their proposed strategies.\n\n6. In Section 4.2 & 4.3, how the memory constraints are determined should be explained. The authors set specific $V_{max}$ values for node classification and link prediction. Also, in link prediction, $M_{all}$ is differently set for different datasets. The authors should show the performance of the proposed method by varying memory constraints, i.e., different $V_{max}$ values on all the datasets. Currently, only Figure 3(b) is presented -- by the way, the result of Figure 3(b) is not consistent with Table 1. Since the performance of the proposed method can be largely affected by the memory constraints, some guidelines about how to set the memory constraints should be suggested.\n\n7. What does the y axis mean in Figure 3(b) and Figure 3(c)? If the y axis means the average accuracy, the result of Figure 3(b) is not consistent with the results presented in Table 1. The average accuracy of the cluster-based method is around 86% in Table 1, but in Figure 3(b), the highest value does not reach 70. \n\n8. In Table 3, other incremental learning methods (EWC, LwF, iCaRL, TEM, and A-GEM) should be also compared. It is important to check whether the proposed method can significantly save time and space compared to existing incremental learning methods. Also, this comparison should be made for both node classification and link prediction tasks on all the datasets.\n\n9. In the experiments, the proposed method should be tested on much larger graphs. Currently, all the graphs are too small to verify the benefit of incremental learning.\n\n10. I guess the manuscript is written in a rush. It requires much effort to get the main points of the manuscript. Some parts are poorly written. There are many English mistakes. (e.g., \"an soft-unbiased\", \"we aims at\", \"tend to the bias\", \"to mediately satisfies\")",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Incremental learning on growing graphs",
            "review": "The authors propose an incremental learning method that takes any GNN model as backbone model and learn the node embeddings efficiently and incrementally on growing graphs. The paper first prove the condition of unbiased estimation (i.e., uniform sampling) and the motivation of edge preservation. With these two conditions, the authors further propose a sample-based method and cluster-based method. The experiments can show the effectiveness of the proposed incremental learning method.\n\nPros: \n 1. The method is well motivated by the theoretically proved conditions (i.e., uniform sampling for unbiased estimation and preserved edges for meaningful representation learning). \n 2. The proposed variants of methods are clearly introduced with novel ideas (e.g., select or sample from existing nodes and keep all new nodes, etc.).\n 3. The experiments are solid from the perspective of (1) demonstrating the effectiveness in node classification and link prediction, (2) ablation studies of different hyperparameter settings and backbone models, (3) efficiency of the proposed incremental strategy.\n\nCons:\n 1. The notations of using nodes and their corresponding embeddings should be carefully distinguished. Bold u, v are used as both nodes (e.g., u\\in V) and embeddings (e.g., aggregations among neighbors' embeddings). \n 2. The details of the baseline methods are still missing. It's not clear how effective the baselines are for learning on graphs. Are they based on the same backbone models?\n\nOverall, the paper is in a good shape with nice ideas of dealing with incremental learning of GNNs.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work about a timely topic, but limited and not well-motivated solution.",
            "review": "The authors present a solution for learning over graphs that grow over time. It is important to distinguish the contribution of this paper (learning from graph data that change over time) vs the existing work on time-varying graphs, where the input is given by the sequence of graphs. The authors present different strategies for learning and evaluate their results experimentally.\n\n\nStrengths:\n\n- This paper is about a timely topic and the problem of scaling up graph algorithms is a compelling one.\n\n- The type of problem considered by the authors is well defined by the authors.\n\nWeaknesses\n\n- The actual motivation of the techniques proposed by the authors is unclear. In fact, it seems that the authors argue for an unbiased sampling, which is something expected if you are not considered the entire graphs. There are several existing existing techniques for unbiased sampling (see for example reference below).\n\n- The method based on clustering is not fully justified, compared to that based on sampling. \n\n- The theoretical foundations of the proposed approach, especially with respect to the clustering part is not easy to understand. In particular, the reviewer was not able to understand the need of the proofs reported by the authors for sampling. The results about clustering are not fully supported by experimental evidence in my opinion.\n\n- The practical implementation of the algorithm in terms of complexity and information to be kept should be clarified and described in more details by the authors.\n\nUnfortunately the reviewer was not able to understand how the sampling method proposed by the authors is implemented in practice.   The idea is that the authors give in input to the model new \"unbiased\" samples contained old and new samples. However, isn't a typical unbiased sampling of the graph?\n\n The actual contribution of this work is unclear. I don't want to diminish the actual results presented in this paper, but it seems to me that the essentially the paper suggests that unbiased sampling is needed without actually considering the fact that the solution of the selection of the training dataset can be based on existing methods for sampling graph properties.\n\nA good reference in this space is for example:\n\nKolaczyk ED, CsÃ¡rdi G. Statistical analysis of network data with R. New York, NY: Springer. 2014.\n\nTo summarize, this is a paper about a timely topic but its significance and contribution with respect to the state of the state appears rather limited. In general, the complexity of the approach with respect to classic graph sampling algorithm is not fully justified and experimentally validated in my opinion.\n\nQuestions for the authors:\n\n1. The reviewer wonders how the authors plan to implement the proposed solution in practice. \n\n2. The reviewer wonders if an actual basic sampling of the graph using one of the techniques presented in Section 3.2 would be sufficient. The modification of the framework presented in the paper is unfortunately not completely convincing.\n\n3. The reviewer was not able to understand Section 3.3. \"Cluster based strategy\". Why do you need to construct the random grouping. What is the theoretical foundational of this approach? Why is sampling not sufficient?\n\n4. What is the practical implications of the analysis presented in Section 3.4? It seems that this is essentially the entropy of the graph.\n\n5. From a practical point of view, which information do you need to keep at each step? For example, you will need to consider an existing \"representative\" sample of the existing graph. Moreover, it seems that you need to check all the nodes and it is connection. What is the advantage with respect to misalign the entire graph? Can you quantify this advantage?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting but lacks clarity and novelty to an extent",
            "review": "The paper proposes a solution to deal with the problem of incrementally re-training models on growing graphs. They suggest two approaches - one with sampling parts of the older graph and effectively providing higher representation to the new parts of the graph; the second approach is based on constructing and maintaining cluster nodes corresponding to cluster center computations.\n\nGrowing or dynamic graphs are a very real scenario and retraining on graphs that in large part overlap to prior versions on which models have already been trained, seems wasteful. There are two approaches to this problem. One is model adaptation, where the previous model is modified to reflect the updates in data. The other, as suggested in this paper is to make re-training somewhat more efficient by reducing the complexity of the problem.\n\nThe first approach suggested by the paper, of under-sampling the older parts of the graph makes sense. However, there isn't any novelty or exciting contributions when it comes to the graph sampling approaches itself. The second approach of clustering is not clear to me. I am not convinced of its merits or how it works. For example, if the authors can explain how link prediction works well on this cluster graph, it would help the reader understand the idea.\n\nRegarding experiments, more details on the methodology of evaluation would help interpret the results better. What was the train-test split on data? In temporal data, it becomes particularly important.\n\nLast but not the least, the paper considers a scenario of a growing only graph, which is little detached from reality.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}