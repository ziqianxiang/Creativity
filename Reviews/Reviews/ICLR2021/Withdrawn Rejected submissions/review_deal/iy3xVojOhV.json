{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs) for graph-based semi-supervised learning.\n\n**Strengths:**\n  * It is a reasonable attempt to combine GCN with GAN for semi-supervised node classification.\n  * The proposed method is general in that it can work with different graph neural networks.\n\n**Weaknesses:**\n  * The novelty of this work is limited.\n  * The proposed method, GraphCGAN, has no significant performance improvement over state-of-the-art methods.\n  * The writing has much room to improve in terms of both clarity and the linguistic quality.\n\nSince both the novelty and the significance of this paper in its current form are limited, it is premature for publication. There is consensus among all the reviewers that this paper is not up to the acceptance standard of ICLR.\n"
    },
    "Reviews": [
        {
            "title": "ICLR 2021 Review of paper 2790: GraphCGAN: Convolutional Graph Neural Network with Generative Adversarial Networks",
            "review": "##### 1. Summary\nThe paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs). The authors focus on the problem of semi-supervised learning on graphs and propose an end-to-end framework in which the generative model is followed by direct convolutions on the graph nodes. Experiments are conducted on standard benchmark datasets and the proposed method, GraphCGAN is compared against several state-of-the-art approaches.\n \n##### 2. Rationale for the score\nAs stated by the authors, the proposed method is an extension of GraphSGAN. In GraphSGAN a similar process of generating fake nodes is performed. As in GraphSGAN, the fake nodes in GraphCGAN are also linked to real nodes in the graph. The difference is that in GraphSGAN a graph Laplacian regularization is done while GraphCGAN applies a convolution on the graph. This could be considered a novel change if the authors were to show that the proposed approach outperforms GraphSGAN. Yet, if there exists an increase in classification performance, it is modest at best. The accuracy and margins of error in Table 1 do not show any actual increase in performance between GraphCGAN and GraphSGAN.\n\n##### 3. Positive aspects\n- The plots included in section 5.3 are interesting and match the behavior previously reported in GraphSGAN.\n- In the analysis, the authors show how their method compares to GCN and GAT when these two are included as classifiers in the end-to-end learning process of GraphCGAN. This is a good idea and the aim is to determine if there are advantages in using the generative model. Although the idea is good, there are issues with the reported values (see section 4 of this review).\n- The authors included the code associated with the method. This is always appreciated.\n\n##### 4. Negative aspects\n- The experiments are not comprehensive and it is not clear if the authors performed the experiments from scratch. For example, the performance values reported for GAT and GCN are the same as the ones listed in Table 3 of [R1].\n- The claim that GraphCGAN outperforms GCN and GAT is not supported by all the results in Table 1. For example, the performance on Pubmed is within the margin of error for GCN. Similarly for GAT, on all three datasets.\n- When comparing against state-of-the-art methods, the authors omitted the comparison against Graph U-Nets [R1]. This is, in my opinion, the main weakness of this manuscript. If we have a look at the results obtained by Graph U-Nets (reported in Table 3 of [R1]), GraphCGAN does not seem to outperform Graph U-Nets in any dataset, including margin of errors. \n- The authors refer to an ablation study but only a couple of parameters are analyzed, e.g., number of fake nodes in Figure 1 and loss function of the generator in Table 2. The many other parameters of the model are listed in section 5.2 without details about how they were derived.\n\n##### 5. Questions to be addressed during rebuttal period\n* How does GraphCGAN compare to Graph U-Nets on the Cora, Citeseer and Pubmed datasets?\n* The margin of errors are quite high, similarly to those reported for GraphSGAN. Can the authors elaborate on why this is the case?\n* Can the authors confirm on what set (hopefully a partition of the training set) were the plots in Figure 1 obtained? There is no margin of error associated to each point in the plots and it gives the impression of overfitting to the test set.\n\n##### 6. Additional references\n[R1] Gao, Hongyang, and Shuiwang Ji. \"Graph U-Nets.\" ICML 2019. PMLR 97:2083-2092",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The motivation is not strong enough that the proposed approach looks like a patchwork of two models",
            "review": "This paper proposes a novel framework to incorporate adversarial learning with convolution-based graph neural network, to operate on graph-structured data.\nThe proposed method is inspiring. However, some problems remain with the proposed technique.\n\n1.\tThis paper announces that it proposes a novel approach called GraphCGAN to deal with the three challenges of constructing a general framework. However, the motivation is not strong enough that the proposed approach looks like a patchwork of two models.\n\n2.\tMore experiments involving different scale networks are needed to prove the effectiveness of the proposed method. \n\nThis paper is globally well organized and clearly written. However, some important details are missing.\n\n1\tThe details about generator are unclear.\n2\tThe paper lacks of analysis on the experimental result.\n3\tThe details about networks are unclear.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "incremental work combining GAN and GNN",
            "review": "This paper combines adversarial learning with graph neural networks to improve the performance of GNN for semi-supervised node classification. To generate fake nodes, the authors designed generator G1 to generate node attributes of fake nodes and G2 to generate the links of the fake nodes to existing nodes.\n\n+ Positive\n1. The idea of combining GNN with GAN for semi-supervised node classification makes sense\n2. The authors provides visualization of the generated fake nodes to help understand the proposed method.\n3. The proposed method is flexible, which can be used for various GNNs\n\n- Negative\n1. The novelty of the paper is limited. It is simple extension of GAN for semi-supervised learning to GNNs. The proposed method heavily relies on existing techniques with little novelty.\n2. It is unclear why the generated fake nodes can only link to existing nodes, not to fake nodes. Intuitively, the fake nots are more realistic if they can also link to each other. The authors may need to give some explanations on such design.\n3. Equations (9) and L_G1 are difficult to understand. For \\tilde{H}_i^{L-1}, it aggregates the information of the (L-1)-hop neighborhood of x_i.  It is unclear to me why \\tilde{H}_i^{L-1} can be written as g(x_i, a_i; \\theta_C), i.e., only relevant to x_i and a_i. In L_G1, why do we have I_i for x_i while 0 for G_1(z). Shouldn’t both be \\tilde{I}_i as we treat the adjacency matrix as I.\n\nQuestion for rebuttal:\nPlease answer 2 and 3 above.\n\nJustification for score：\nThe paper studies an important problem. However, it lacks novelty. Some part of the paper is unclear. I will change the rating if the authors can clarify the contribution and novelty, and address my questions.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting work yet with limited significance.",
            "review": "Pros: \n1.\tThis paper proposes the first combination of GNN with GAN for semi-supervised learning.\n2.\tThe structure of the paper is clear and easy to follow.\nCons:\n1.\tSome sentences are hard to parse and many grammar errors.\n2.\tThe contribution and novelty are limited.\n\nThis paper deals with semi-supervised learning on graphs based on GANs by proposing a framework named GraphCGAN.  The proposed framework can be easily extended to include other GNN methods. However, the novelty of the proposed model is limited, and the motivation of this paper is not strong. Also, the only motivation stated by the authors is to improve the performance on semi-supervised learning, while the improvement of the performance in the experiments is limited (i.e. ~1% improvement). The followings are the details of comments regarding this paper from three aspects.\n\n1.\tMotivation and significance are not clear.\n(1)\tThe author(s) claim(s) that combining GCN with GANs could boost the performance of semi-supervised learning, which however is not solidly validated by the experiment results. \n(2)\tThe author claims that GANs have never been applied to the SSL task in graphs. However, the existing method GraphSGAN [1] has already done this and GraphSGAN shows better scalability than GCN. The only difference between the proposed GraphCGAN and GraphSGAN is the selection of classifiers (MLP vs. GCN). Thus, the significance of the proposed GraphCGAN is not clear compared to the existing GCN and GraphSGAN. \nAs a summary, considering the limited improvement on performance, I would like to see more other motivations why we need graph-convolution-based GANs for semi-supervised learning on graphs.\n\n       [1] Ding, M., Tang, J., & Zhang, J. (2018, October). Semi-supervised learning on graphs with generative adversarial nets. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 913-922).\n\n2.\tThere are some technical issues.\n(1)\tThe generator seems very weak. As stated in Section 3.1, the generator generates each node and edge only based on randomized latent representation “z”. If so, each new node (and edge) is generated independently without considering the dependency on the existing nodes and the dependency between the nodes and its adjacent relations.  \n(2)\tEquation 5 is confusing. As expressed in Equation 5, the adjacent matrix of all the newly generated nodes is defined as a unit diagonal matrix I_B. It is not clear why the adjacent relation between each pair of the newly generated nodes is set to zero. \n(3)\tThe author mentions “pull-away item” and “complementary loss” without giving any explanations on these two items in the loss function.  An ablation study is better to be conducted to validate the necessity of these two terms in loss function.\n\n3\tThe presentation of the paper has many grammar errors, typos, and hard-parsing sentences.\n(1)\tIt is hard to parse “…as the map from feature vector and adjacency vector to the space of second to the last layer in convolution-based GNN...”. It is not clear what “the space of second to the last layer in convolution-based GNN” refers to.\n(2)\tIt is hard to parse “….and extract the map to the intermediate layer g(:; :) ….” In Algorithm 1. The term “map” is confusing. \n(3)\tIt is not clear what \"i\" refers to in the equation of L_{G_2}. If it refers to the index of one node, then there may lost a “sum” symbol to sum up all the nodes into the loss.\n(4)\tThere are many grammar errors and typos, e.g., \n-----“existed methods”\n                 -----the font of word “softmax” is not unified, see Section 2.1 on Page 2.\n                 -----“…Later on, Dai et al. provided a theoretical….” lacks the hyper-reference.\n----- “: =”  in Equation 4.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}