{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting connection between optimal transport and LLP but more explanations are needed.",
            "review": "Learning from label proportions is one of the important setting in the weakly supervised classifications. Existing methods suffer from issues  due to unconstrained optimization related to bag-level proportional information which is insufficient to solve LLP. They also had to rely on probabilistic classifier, which will further degrade the performance of instance-level classification. \nThis paper tackles these issues by introducing a novel approach using optimal transport. A series of experiments are conducted to demonstrate\nthe proposed method performs better.\n\n\nOverall, their approach is interesting and empirical improvement looks good.\n\n1. However, writing is somehow poor: important formulations (adaption of optimal transport to LLP setting ) are just put in Appendix without explaining enough in the main paper. \nFor classic binary classification settings, are there any similar methods as the presented method?\n\n2. Also, the current paper is missing some important references:\n\n-Learning from Label Proportions: A Mutual Contamination Framework\nBy Clayton Scott and Jianxin Zhang\nhttps://arxiv.org/pdf/2006.07330.pdf\n\n-Estimating labels from label proportions\n by Novi Quadrianto, Alex J. Smola, Tibério S. Caetano, and Quoc V. Le. \nJournal of Machine Learning Research, 10:2349–2374, 2009.\n\n-Almost) No Label No Cry\nPatrini, Giorgio and Nock, Richard and Rivera, Paul and Caetano, Tiberio\nAdvances in Neural Information Processing Systems 2014\n\n\nThe authors should compare them and  discuss the improvement over this work.\nThe above work also  provided Rademacher-style generalization bounds, while the current paper has not provide any theoretical guarantee.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes to solve the learning from label proportions (LLP) problem with a framework to combine instance-level classification and pseudo labeling, and apply the OT algorithm to obtain integer solutions for the labels. The idea in this paper is interesting, but the innovation is not enough.",
            "review": "This paper proposes to solve the learning from label proportions (LLP) problem with a framework to combine instance-level classification and pseudo labeling, and apply the OT algorithm to obtain integer solutions for the labels. To further improve the performance of existing deep LLP models, this paper builds a model-agnostic LLP framework, which is useful in practice. The idea in this paper is interesting, and the experimental results show the proposed method has achieved satisfactory performance. However, I have the following concerns:\n1. The main contribution in this paper seems to modify the constraint in (18) or the admissible couplings in (19) to fit in the proportional information, and then convert (3) to a typical OT problem, which can be solved relatively efficiently. So I think the innovation is not enough. The author should add a more detailed description of the innovation.\n2. Why the modification in (3) can have improvements of former LLP methods? The author should make some elaboration.\n3. What’s the meaning of “model-agnostic”, the author should give an explanation in the context.\n4. The author should clearly state which formula is the objective function of this article, otherwise it will be difficult for the readers to grasp the focus of this article.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "Summary:\n\nThis paper presents a new method for learning from label proportions using optimal transport theory. The proposed algorithm is an alternating optimization where the self-labeling step is achieved through the Sinkhorn algorithm.\n\nReasons for score:\n\nThe method using optimal transport for learning from label proportions is interesting. However, the improvement over existing method seem to be marginal. Also, theoretical analysis is lacking. \n\nPros:\n\n1. The paper is overall well-written.\n\n2. The research idea of applying Sinkhorn algorithm in learning from label proportions is interesting.\n\nCons:\n\n1. The main results section is not presented properly. For instance, the meaning of p_\\phi in (3) is not explained in the main paper; one has to go to the appendix. \n\n2. Theoretical analysis of the proposed algorithm is missing. Is the algorithm guaranteed to converge? What's the computational complexity?\n\n3. The comparison with existing methods is not well presented. \n\n4. The improvement over existing method seem to be marginal.\n\nQuestions:\n\nPlease address and clarify the cons above.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but I have concerns about fairness of using unsupervised self-labeling for LLP",
            "review": "The paper considers the problem of learning from label proportions, an example of weak supervision where instead of labeling each individual samples, \na group of samples is labeled with the class proportions.  The authors consider a formulation based on optimal transport,  which attempts to learn a classifier\nfor each individual samples based from this weak supervision, such that the constraints on the groups are satisfied exactly.  As part of the solution, they leverage a recent related formulation to do self-supervised clustering, using OT ideas. \n\nI find the overall idea interesting, and the use of optimal transport to satisfy exact group label proportions constraints a good idea.  However, I have a couple \noverall concerns with the paper: \n1) The approach seems to require using another LLP approach as a warm-start, after which OT-LLP is used as a refinement step.  Can it be used stand-alone, \nor using another initialization based on first-principles, instead of piggy-backing on already strong solutions? \n2) The method produces improvements in accuracy most of the cases, but they're usually fairly incremental over the starting point. \n3)  Using self-supervision can have adverse effects:  suppose that the data is composed of multiple cohorts (say male/female,  or high-income / low-income), which have nothing to do with the label.  Self-supervision will postulate labels consistent with this natural data-clustering.  However,  using these labels in a OT-LLP framework may naturally encourage biased solutions,  which will attempt  to align the target labels with the cohorts in the group, and produce solutions that violate fairness.  Is this indeed an issue, and if so,  how can it be controlled?  \n4) I am not convinced by the argument that KL-divergence penalties are unsatisfactory and it's significantly better to impose exact label proportion constraints.  Isn't it possible to increase the weight on the penalty, to make constraint violations arbitrarily small (maybe by sequentially increasing the weights)?\n5)  A lot of information is presented in the Appendix,  and referenced in the paper without being introduced.  I assume not all readers will have access to the appendix, so the paper should be self-contained. \n\nAdditional comments: \n6) 'it probably results in high-entropy conditional class distributions' -- is it based on your observations, or just a general worry that it might happen?  Add some supporting evidence. \n7) It's worth mentioning / summarizing conditions under which LLP is not ill-posed.  It sounds like if groups are small and diverse, it should work better.  Is there some condition on some rank of a mixing matrix or something like that to ensure identifiability. \n8) Unsupervised learning helps to discover the data clusters specifically corresponding to the classes.  It may or may not happen as I mentioned earlier -- may datasets contain multiple cohorts (e.g. data from hospitals in different areas, or history of driving during the day and at night) -- and these clusters may be unrelated to the final target label.   So one has to be very careful to assume that such self-discovered clusters are informative about the targets. \n9) Section 3.3.  you seem to use m instead of N in the first paragraph? \n10)  A lot of information is presented in the Appendix,  and referenced in the paper without being introduced. For example: \nsection 4.1.  It's worth introducing all the symbols before presenting the equation,  and not to refer to (18) in the appendix, as some readers\nmay not have access to the appendix. \nAfter equation (4) you use lambda without introducing what it is.  Is it implicitly introduced in eqn (5)? \nSection 4.32. phi = (psi, theta) appear out of the blue. Are they also defined first in the appendix? \n11) Do you have any intuition why hard-labeling is preferred for CNNs,  and soft-labeling for fully-connected nets?  Is the conclusion statistically significant, \nor maybe explained by noise, and changes with different datasets?\n12) Section 5.1.1.  Is the assignment to bags random?  Is that a favorable case, is it more difficult to learn from LLP if the bags are chosen non-randomly in some systematic way, or even adversarially? \n13)  Our model is 'orthogonal' to previous KL-based LLP algorithms... What do you mean by orthogonality here, this is used informally, or has some mathematical implication like lack of correlation?\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}