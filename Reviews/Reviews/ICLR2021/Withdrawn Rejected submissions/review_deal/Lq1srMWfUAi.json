{
    "Decision": "",
    "Reviews": [
        {
            "title": "Weak evaluation, limited novelty",
            "review": "This paper proposes a variant of evolutionary strategies, which estimate a gradient of a black-box objective function by a limited number of Gaussian samples around the current search point. The proposed approaches replace the vanilla gradient with the natural gradient, the inverse Fisher information matrix times the vanilla gradient. Such an idea is introduced in natural evolution strategies (NES) and CMA-ES, where the inverse Fisher information matrix times the vanilla gradient can be computed directly (without computing the inverse Fisher information matrix). \n\nAs I mentioned above, the multiplication of the inverse Fisher information matrix is known to be for free for Gaussian models. Indeed, different ES variants uses the natural gradient, such as eNES, xNES, CMA-ES, and their diagonal covariance matrix versions such as sep-CMA-ES and other covariance models such as R1-NES, VD-CMA. Strangely enough, the authors say in Section 5 that it is difficult to scale to very high-dimensional problems because the computational cost for the inversion of the Fisher information matrix, and they propose to use a numerical optimization to derive the natural gradient approximately. I couldn't find any reason why the authors prefer to numerically derive the natural gradient. I do not see much difference from separable-NES (JMLR 2014, 949-980), which uses the natural gradient and uses a diagonal covariance matrix. The separable-CMA-ES also compute the same. \n\nThe authors compare the proposed approach with other ES variants. However, the authors didn't compare the performance with separable-NES nor separable-CMA-ES. What could be the possible reason that the proposed approach would be better than these approaches that compute the exact natural gradient on the same Gaussian models?\n",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Many problems, flawed experimental results",
            "review": "SUMMARY\n\nThis paper proposes a variant of the NES algorithm for convex problems. However, it is based on a gradient estimator that differs significantly from NES, and it solves a problem that has a solution since 10 years.\n\n\nCRITICISM\n\nThe paper cites [Salimans 2017] right next to [Rechenberg 1973]. There are hundreds of better references on ES than the OpenAI paper, for example all the original work on CMA-ES. Indeed, Salimans used a very much reduced algorithm that remotely resembles and ES, even lacking step size control, without making any contribution to ES research. Citing it as an original paper on ES reveals a serious lack of knowledge of the field.\n\nFor CMA-ES, the standard reference is:\nHansen, Nikolaus, and Andreas Ostermeier. \"Completely derandomized self-adaptation in evolution strategies.\" Evolutionary computation 9.2 (2001): 159-195.\nThis paper cites a benchmark study from 2016 instead.\n\nIt is claimed in the introduction that NES would need to estimate the Fisher matrix. That is wrong in many ways. First of all, Gaussian distributions are used all the time, and then the matrix is known in analytic form. Second, the xNES implementation of NES provides a neat way of getting around computing the matrix at all:\nGlasmachers et al. \"Exponential natural evolution strategies.\" Proceedings of the 12th annual conference on Genetic and evolutionary computation. 2010.\n\nSection 4.2: \"This gradient estimate is then supplied to a gradient-based optimizer to update the belief distribution.\" Wrong. All practical NES implementations rely on ranking of solutions, not on the plain gradient estimator. This is absolutely essential, see\nBeyer. \"Convergence analysis of evolutionary algorithms that are based on the paradigm of information geometry.\" Evolutionary Computation 22.4 (2014): 679-709.\n\nFirst sentence of section 5: \"the computationally expensive estimation of the Fisher information matrix $F (\\theta)$ and its inverse makes it difficult to scale to very high-dimensional problems.\" This is a long-solved problem, see [Glasmachers et al., 2010] above.\n\nThe paper contains two unsurprising theorems which do not really touch core questions like convergence properties. Therefore the main claim to fame of the proposed method is empirical performance. Sadly, the experiments are not convincing at all.\n\n\nEXPERIMENTS\n\nI will focus on the results on analytic problems. What I see here is as wrong as it gets. It is obvious that the results are entirely dominated by wrong initial step size settings. They are completely meaningless. I don't know what goes wrong here, but I see only two options: either the authors use bad initial parameters for their competitor methods on purpose, or they don't know how to properly use an ES.\n\nThe only way to make a non-elitist ES like CMA-ES get worse on a convex function is to start it with a too large initial step size. This happens on all four analytic problems. It is well known that CMA-ES vastly outperforms NES on sphere due to better step size control. Also, how can the step size of an ES increase on sphere after it already decreased? For all the ES in the field, if properly implemented and properly initialized, the log-distance to the optimum will decay at a fixed multiplicative rate (linear convergence), and so does the step size. This was proven rigorously for the (1+1)-ES, for all convex quadratic problems:\nJägersküpper, Jens. \"How the (1+1) ES using isotropic mutations minimizes positive definite quadratic forms.\" Theoretical Computer Science 361.1 (2006): 38-56.\nThe same holds true empirically for all ES, including NES and CMA-ES. For all of these reasons the results are entirely implausible and reveal serious problems with the paper. I can only speculate about the reasons. One option is a wrong implementation.\n\nI checked the code. Whatever is referred to here as an ES or a NES algorithm has not much to do with it. I refuse to call these algorithms ES. There is no selection, there is not even any ranking. What I see here are cheaply estimated stochastic (natural) gradients at work. Interestingly, the authors cite the NES paper but apparently ignore its contents; instead they prefer to roll their own reduced algorithm and call it NES. The resulting algorithms do not achieve linear convergence, see [Beyer, 2014] above.\n\nPlease ALWAYS plot objective function regrets and step sizes on a log-scale. That will make sense as soon as the ES algorithms are implemented correctly so that linear convergence can be obtained. Refer to figure 3 of the cited NES paper\nhttps://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf\nfor an example.\n\nPlease provide a comparison on the well-established BBOB/COCO benchmark suite, possibly restricted to convex functions. There is no need to rerun competitor algorithms since that data is publicly available:\nhttps://github.com/numbbo/coco\n\n\nRECOMMENDATION\n\nThe papar has far too many weak spots. Some of the experimental results are ridiculous. I strongly recommend to reject the paper.\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea for Gaussian distribution case",
            "review": "### **Summary and Contributions of Paper**\nThis paper proposes a new method (CoNES) to compute a relaxed version of the Natural Gradient, via convex programming due to the convexity of the relaxed problem when using Gaussian distribution parameterizations. Experiments comparing CoNES with ES, NES, and CMA with are shown on standard blackbox functions as well as Mujoco benchmarks.\n\n### **Strengths**\n- The writing quality is clear and understandable for a broad set of readers.\n- The approach is novel and theoretically sound. \n- Tackles second-order optimization in ES problems, an under-studied case.\n\n### **Weaknesses**\n- In the experimental section, the Adam Optimizer is used, which can confound the results. In general, it is uncommon to do this with ES techniques, and I'm not sure if, for instance, CoNES is better only when in combination with the Adam Optimizer (unlikely, but can be a possibility). It would be better if the authors used raw gradient descent in the experimental section.\n- Experiments could be stronger for the RL case. Right now, there's not a large improvement for the Mujoco benchmarks, while it can be complicated to implement CoNES due to the convex programming solver. Part of the reason seems to be that NES does not highly boost performance on these benchmarks. I suggest certain modifications to these benchmarks to see if NES/CoNES works better, such as: 1. Projecting the parameter space to higher dimensions using a random orthogonal matrix (which is also a reparameterization of the search space, to test parametrization invariance), 2. Stacking more Tanh layers in the policy, which would justify the claim about natural gradients avoiding vanishing gradients due to Tanh.\n- It would be good if the authors could also provide some sort of study (theoretical or empirical ablation) on how much the CoNES gradient matches with the true natural gradient, compared to using an estimate of the natural gradient, given the same number of perturbations used for both cases. This is especially true since the authors state that they use the same hyperparameters for all methods (ES, CoNES, NES), which means that the quality of the optimization method is most likely dependent on the quality/variance of the natural gradient estimation.\n\nOverall, I think the submission is fine but could be polished more, and thus my score is marginally above the acceptance threshold.\nIf the authors could fix the mentioned weaknesses, I would be happy to raise my score.\n\n### **Clarity Remarks**\n- Can you please clarify how you're computing the NES benchmark? e.g. is it Algorithm 3 from (https://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf)?\n- It would be better to not use \\delta\\theta to describe the perturbation, since \\theta is already vector with a fixed direction, while the intention is to convey that \\delta\\theta can be any direction within a ball.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}