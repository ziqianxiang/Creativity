{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "\nThe paper proposes to generate human-like question for a image by using additional information (hints) such as the textual answer and visual regions of interest (ROIs).  The visual regions are used to guide the question generation so that the model can generate relevant and informative question.  The question generation problem is formulated as a graph-to-sequence problem, starting from an object graph, and using GCNs with attention to align text and visual regions and to generate an appropriate question.\n\nReview Summary: The submission initially mixed reviews (scores from 3 to 6).  While reviewers find the problem interesting and work to be mostly solid, reviewers felt that the novelty of the work was limited (R1, R3) and that some of the presentation was unclear (R1, R3) with some missing details (R2).  R2 was not sure if the VQG was a useful task, and R4 felt that the initial submission was missing a key experiment on whether the generated questions can be useful as data augmentation for VQA. The reviewers were impressed by the extra experiments performed by the authors in the rebuttal and the revised draft, and indicated that most of their concerns were addressed, In particular,  the generated questions were shown to be useful as data augmentation.  Many reviewers increased their scores, ending with 3 scores of 6 (R1,R2,R4) and 1 score of 5 (R3).  \n\nPros:\n- The use of generated questions as data augmentation for VQA is an interesting direction\n- Strong empirical results with thorough experiments (and user study)\n\nCons:\n- The technical novelty of the work is still rather limited (R1)\n- The paper was difficult to follow (R1,R3) with a lot of moving parts, making it potentially difficult reproduce (R1) - The authors indicated that they will open-source the code.\n- The grammar and wording of the writing is poor even after revision and should be improved\n\nExample of poor writing (a full proofreading pass is recommended):\n- Section 2.1: \"Mora et al. (2016) firstly makes an attempt to adapt\" => \"Mora et al. (2016) adapted\",\n\"abstract and general results\" => \"imprecise and generic questions\"\n- Section 3.2: \"The most important point of our first issue located in how to effectively find...\" (it's unclear what this means)\n- Section 4.1: \"standfordcoreNLP\" => \"Stanford CoreNLP\"\n- Section 4.5: \"shown in 3\" => \"shown in Figure 3\"\n\nRecommendation:\nThe AC agrees that the work is addresses an interesting area of generating questions as data augmentation for VQA.  Despite the improved reviewer scores, the AC agrees with the initial assessment that the work has limited technical novelty.  The AC also found the writing of the paper to be poor and difficult to follow at places even after revision.  Due the limited novelty, the issues with exposition, and the many changes to paper, the AC believe that the work would benefit from another round of review and should not be accepted at ICLR in its current form.  Given the positive response, the authors are encourage to improve their work and writing and resubmit to an appropriate venue (the AC believes the work would be more appreciated in a vision or language venue)."
    },
    "Reviews": [
        {
            "title": "New model on Visual Question Generation",
            "review": "The paper introduces a new model on the task of Visual Question Generation. The model uses cross-modal alignment between the object features, position features and answer hints to find the right subset of relevant visual hints to be used to generate the relevant question. The model also ensures that the latent space features capture the answer and position information by predicting them back from it. Informed by the visual hints, the object and image features are passed to a GCN network that is used to get the final hidden state which is passed to an attention and language lstm similar to BUTD model to generate the final question.\n\nThe cycle consistency used for answer and position features seems to provide grounded question generation for the answer and image. The model is tested on VQA2.0 and COCOQA and achieves better performance compared to the baseline model on automated metrics as well as human evaluation. \n\nOverall, the paper is strong and provides a solid foundation for the intuition and framework behind the model supported with detailed ablation analysis and case studies. What I find missing, is the actual test of how good these questions actually are by using them to train on the actual VQA task. Understanding that and how it performs as extra data on VQA 2.0 would give us a better understanding on how good the generation actually is where it matters. \n\nSome other questions:\n- Is there any specific reason why BERT wasn’t used instead of GloVe for word embeddings?\n- Please add citation for VQA 2.0\n\nOverall, I would like to recommend the paper for acceptance but it is hard to understand the actual value of this work without downstream application on the task of VQA 2.0.\n\nEdit after rebuttal: I have read the author response and I thank the authors for their valuable insights and answers to my questions. It is exciting that this does help in improving the performance on downstream VQA2.0 task though I would have expected the results to be conducted on one of the recent state-of-the-art models instead of very old BUTD model where achieving performance gains is trivial. I would like to keep my rating as it is.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good results on existing benchmarks; Concerns around novelty and key take-away messages. ",
            "review": "Summary: \nThe paper proposes a model for the task of Visual Question Generation (VQG) which uses the answer as well as object regions to generate the question. The paper models interactions between various visual entities and the answer tokens using a graph and then use it to generate the question. The proposed approach outperforms existing methods on the VQA and COCO-QA task. \n\nStrengths: \n- The paper emphasizes on using both image-regions for the VQG task. While some of the proposed techniques borrows from existing works, they showed how to combine it to improve over exisitng methods. \n\n- On the VQA and COCO benchmarks, the models outperforms existing methods. On the small scale human study, the proposed approach was rated higher than existing approaches. \n\nWeaknesses: \n- Overall, the novely of the paper is low. The paper is a collection of already popular ideas (use of attented region features for V+L tasks (Shah et al.), use of position embeddings to model spatial relations (ViLBERT, VL-BERT etc). Because, the paper is written as a collection of ideas, the take-away message isn't clear. \n\n- The ablation tables has very subtle performance changes across design choices which are hard to understand. The ablation table suggests that all choices in isolation leads to a drop when compared to the full model. But what happens when you combine these choices one by one. More importantly, what are the most critical components of the proposed method? \n\n- Some of the statements made in the paper are not backed by citations or lack explanation. For instance, the authors claim that the \"amount of objects which are visual hints are much smaller than ones not\" and \"when humans ask questions from the image, we will infer whether the object is important clues by looking around in the image\". \n\n- The paper was hard to follow and it seems like there are a lot of moving parts. In general, this makes reproducing the paper and adopting ideas from the paper difficult. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Limited novelty and the poor writing makes this paper a rejected one",
            "review": "In this paper, the authors aim at generating the right questions based on the textual answers and corresponding visual regions of interest (ROIs). The core innovation of the proposed method is leveraging the region information to supervise the question generation, which helps to mitigate the ambiguity of the answers. Correspondingly, a simple method is designed to generate the noisy annotations of the ROIs using the pretrained Masked-RCNN and the questions. \n\nThis core part is mainly divided into two components: 1) aligning the object features with the answer word embeddings using attention mechanism; 2) constructing the object graphs and getting the GCN-refined object features; 3) refining the question embeddings by attending the refined object features and image ones. Experimental results show that the proposed method outperforms the existing approaches significantly. In addition, ablation studies prove the effectiveness of the proposed components. \n\nAlthough the visual regions of interest can help to guide the question generation, the one-to-many mapping issues still exist when generating the visual regions of interest (no additional information is given), which may lead to the same problem in the question generation stage. Nevertheless, the proposed approach is still valuable. Although it helps little in mitigating the one-to-many issues, it helps to learn a better question generation network, as the additional region information can mitigate the issue during the training stage, which will help to avoid learning a generator that prefers to give general questions. Therefore, I think authors should make that clear in the paper that it helps in learning a better question generator instead of generating the specific question because the ROIs are also selected based on the answers only. \n\nAlthough great improvements are achieved, the proposed method is not novel. The main novelty is leveraging the object features in generating questions. However, neither the feature alignment nor the GCN part is new. In addition, the graph construction part does not make sense to me, and no ablation study show that pruning the non-hints objects can help to improve the performance.  \n\nIn addition, the confusing equations and poor writing of the paper cannot make this paper an unaccepted one:\n* In Eq. (2), \\beta X^{a} seems like a matrix, which cannot be concatenated with a vector. \n* In Eq. (3), \\beta_{j} should be \\beta_{i, j}.\n* In Eq. (3), what I_{j} means? Why the image feature has a subscript?\n* In Eq. (3), what does the operation “*” means? The product signs are different in (2) (3) and (4)\n* In Sec. 3.4, it lacks detailed explanations of how the model will attend the image and graph to get a better representation of the question embedding between the two LSTMs, which is one of the core components of the proposed methods. \n* Typos like “size information” should be “side information” in the top paragraph on Page 2, “question tokens” should be “word tokens” in the bottom paragraph on Page 3. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An OK submission, but the motivation is questionable. And some details need to be clarified. ",
            "review": "Overview: \nThis submission focuses on the problem of visual question generation and proposes to use two hints: answer and visual regions. With the help of these two hints, a graph representation is built and then a GCN-based graph2sequence generator is applied to generate questions based on given inputs. Experiments on two datasets show its efficiency on question generation quality.\n\nStrengths:\n+ The description of the method (model) is easy to follow.\n+ Improvements of the question generation quality (according to the BLEU and Cider score) is big.\n+ Good ablation studies and human study.\n\nWeakness:\n- The motivation of VQG: I understand VQG is a recently raised task in vision-and-language research, but I can not get any ideas that why this is an important task, at least from this paper. According to my understanding, one reason that we need VQG is that we can use it as a data-augmentation tool to train a better VQA system (which is a more important task). Another reason is that it may lead to a better  (goal-oriented) visual dialogue system, such as GuessWhat?!.  This paper mentioned these two directions very briefly but failed to explain it well and there are no experiments presented in this paper to show whether their question generation model can benefit these two areas. I don't think a better BLEU score can show its potential since it only means this model can generate similar questions as the training data. It is worth to check whether these augmented questions can boost VQA performance.\n\n- The motivation of two hints: It looks like many previous works only use answer types as the hint. I think this is more reasonable since it gives more freedom to the model to generate questions. And I don't think it is an issue that 'one answer/image can be potentially mapped to many different questions'. Instead, I think this provides a diversity of generated questions and further improves the VQA generalisation ability, from a data augmentation view. In this submission, authors choose to prove the answer directly as a hint. There might be two issues: at first, it seems not fair since it requires more annotation. Secondly, how can you get the answer during the inference? Say if one downloads an image from the Internet  (not from a well-annotated VQA dataset) and there are three bears in the image, it might be easy if the answer is 'bear' since you can use a detector, but how can one know the answer is 'three' if we want to generate a question about 'how many?'. In this case, the answer type is more practical.\n\nAccording to the visual hints, if I am right, from section 4.1, they are mined from the questions and answers directly, for both training and testing. This suggests you (authors) already got some info from the target questions (even in the testing!). I can't say it is a fair setting, even it happens in pre-processing step. Please fix me if I am wrong.\n\n- The 3.2.1 is basically an attention mechanism?\n\n- What is the accuracy of the tasks listed in 3.2.2?\n\n- In section 3.3, the \\epsilon seems quite important. It decides the sparsity of the graph. Any ablation studies?\n\n- In section 4.1, another \\epsilon, which decides the quality of the visual hints. Any ablation studies?\n\n--------------------------- After Rebuttal --------------------------------\n\nThe authors did a good job in the rebuttal. Most of my concerns have been addressed so I am happy to raise my score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}