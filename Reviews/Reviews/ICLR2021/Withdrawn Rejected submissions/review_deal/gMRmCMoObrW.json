{
    "Decision": "",
    "Reviews": [
        {
            "title": "Extension of identifiability results in nonlinear ICA from supervised to self-supervised learning.",
            "review": "This work primarily extends the identifiability results of non-linear ICA to self-supervised learning. Using theorem 1, the work proves that the latent variables for augmentation are identifiable. This paper demonstrates the advantage of the proposed method on synthetic data and EMNIST data as per the experiment. In specific, the experiment demonstrates that the method can identify more latent variables than those for augmentation. \n\nComments:\n1. This work considers two datasets: synthetic and EMIST. This is the biggest weakness of this work as the considered datasets are not enough or standard. Can the author demonstrate the efficacy of their models on other standard datasets? dSprites (Matthey et. al., 2017) and 3dShapes (Burgess & Kim, 2018) are some examples in the disentanglement literature. \n\nMatthey L., Higgins I., Hassabis D., & Lerchner A. (2017). dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/.\n\nBurgess, C, & Kim, H. (2018). 3D Shapes Dataset. https://github.com/deepmind/3dshapes-dataset/.\n\n2. The contribution of this work primarily seems to be Theorem 1. The contribution seems limited. Can the authors rebut this? \n\n3. Although the title mentions disentanglement learning, minimal arguments and experiments support if this work is really about disentanglement learning. For instance, after the success of contrastive learning in recent times, there has been a surge in interest in using such a self-supervised approach for disentanglement learning. One example could be (Bhagat et. al., 2020). Can authors discuss this or related works on how their current work is different from them? \n\nBhagat, S., Udandarao, V., & Uppal, S. (2020). DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors. arXiv preprint arXiv:2006.05895. \n\n4. Authors point out in this work that generating augmented data using a latent model as \"key insight\". However, using a latent model for generating data augmentation has been well explored in the literature. Some examples could be (Gyawali et. al., 2019) and (Bowles et. al., 2018). Can the authors discuss them in their literature review, pointing out the differences and specific insights generated by this work? \n\nGyawali, P. K., Li, Z., Ghimire, S., & Wang, L. (2019, October). Semi-supervised learning by disentangling and self-ensembling over stochastic latent space. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 766-774). Springer, Cham.\n\nBowles, C., Chen, L., Guerrero, R., Bentley, P., Gunn, R., Hammers, A., ... & Rueckert, D. (2018). Gan augmentation: Augmenting training data using generative adversarial networks. arXiv preprint arXiv:1810.10863.\n\n5. There are many grammatical errors in the current manuscript. Please carefully proofread your paper. Some examples:\n- In generating progress, 'u' is written instead of 'a'.\n- \"we first introduce the our method\"\n- \"data augmented data\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Weak presentation and incremental contributions",
            "review": "The paper extends the identifiability results in non-linear ICA from supervised learning to self-supervised learning with augmented data. Experimental results show some interesting findings in the context of augmented data.\n\nStrengths:\nThe paper seems theoretically sound.\n\nWeaknesses:\n1. The qualitative results in Figure-2 are not convincing to me, as most of so-called interpretable factors of variation are not very clear, especially when compared with the GIN (Sorrenson et al., 2020) paper.\n\n2. The presentation of the paper is relatively weak. Here are some suggestions for improving the paper.\n(a). Consider giving a technical introduction about the relevant previous works (e.g. the supervised learning theory of non-linear ICA, GIN) in section 3 before presenting the self-supervised theory of non-linear ICA, to help the readers better understand the paper.\n(b). I cannot understand how the rightmost column of Figure-2 is obtained.\n \n3. The contribution of the paper is relatively incremental.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Not enough experiments to support the authors claims",
            "review": "Summary: \nThe authors of the paper present the method for learning disentangled representations based on semi-supervised learning. \n\nPros:\n- The paper is well-written and easy to follow. The experimental setting is described in detail.\n\nConcerns:\n- The experiments were conducted only on the synthetic data and EMNIST dataset; I would suggest adding several datasets to the experimental setup. There exist datasets constructed specifically to study learned disentangled representations, e.g., Shapes 3D, MPI 3D, etc. \n- The degree of disentanglement is not measured. There exist several disentanglement metrics (see [1]); even though they were originally constructed for VAEs, you can adapt it for your model since you have an encoder. \n- Also, the paper lacks comparisons with other methods. \n\nComments/questions:\n- According to Figure 2, the factors of variation are not very noticeable. Have you tried to increase the shift magnitude from [-2,2] to, say, [-3,3]? \n- Did you try to vary different noise magnitudes for stabilization? \n- (minor typo) Inconsistent writing: in some places in the text, you write \"nonlinear\", while in other -- \"non-linear\".\n\nOverall, while the paper presents a novel method for the identification of variation factors, the key concern it that the paper lacks the experiments on different datasets and comparison to other methods, making it hard to evaluate the method's performance.\n\nReferences:\n\n[1] Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Schölkopf, B., & Bachem, O. (2019, May). Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning (pp. 4114-4124).\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "#### Summary and contributions\nThis paper builds upon the recent work (Khemakhem et al., 2020) in the identifiability of latent factors in deep latent-variable models using the theory of non-linear ICA. Khemakhem et al. had shown that latent-variable models can be made identifiable using  a conditionally factorized prior distribution $p(z|u)$ of the latent variables. The conditioning $u$ can come from a variety of sources such as (noisy) class information, history in a time-series, etc. This paper extends the idea to the self-supervised setting by constructing augmentations of the data $a$ and treating them as the variable of interest with the original datapoint $x$ as the conditioning variable $u$. Theoretical results of identifiability of latent variables similar to Khemakhem et al. have been presented for this setting. Qualitative experimental results have been presented on a synthetic dataset and EMNIST to showcase the disentangling property of the proposed method.\n\n#### Strengths\nThe construction of augmented data and then using the original datapoint for conditioning is an interesting idea which is relevant to the research community working on disentanglement and self-supervision in general. That said, this paper suffers from several weaknesses that have been described below.\n\n#### Weaknesses\nClarity: The manuscript lacks clarity and is riddled with grammatical errors. The paper is not self-contained and it's difficult to understand what's going on without reading Khemakhem et al., 2020 in detail.\n\nSignificance of Contribution: Although the paper presents an interesting idea, the total contribution of the paper does not go beyond that. In particular, the theoretical results are slight modifications of Khemakhem et al. and experimental results are unconvincing and lack rigor. \n\nEmpirical Evaluation: The synthetic data experiment is arguably simplistic. The authors only present a qualitative analysis on the EMNIST dataset which may be subjective. Since the factors of variations have been manually constructed for training conditioned on the real data, it is unclear how this method would disentangle factors of variations that are already present in the data. Just saying that augmenting certain variables leads to the discovery of other inherent factors too without a proper justification looks like magic. This makes me suspect that something very different is at play here. For example, invertible models with a Gaussian latent space are naturally able to disentangle certain factors (Kingma and Dhariwal, 2018.). Another thing missing from the paper is any kind of comparison (qualitative or quantitative) against previous methods. Various datasets and metrics have been presented in the literature (e.g., see Locatello et al. 2019) to study disentanglement which could have been adapted in this setting. \n\nKingma, D.P. and Dhariwal, P., 2018. Glow: Generative flow with invertible 1x1 convolutions. In Advances in neural information processing systems (pp. 10215-10224).    \nLocatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Schölkopf, B. and Bachem, O., 2019, May. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning (pp. 4114-4124).\n\n#### Minor Comments\n\n- Before Eq. (1), I think the graphical model should be x —> z —> a.\n- In Theorem 1 (i), I think it's better to use a different notation than \"a\" for the characteristic function (CF) to avoid confusion since the CF takes in a frequency as the input.\n- In the proof of Theorem 1, the matrix volume (\"vol\") is missing in Eq. (11-14). I believe it's a typo.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}