{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting work with good results",
            "review": "Summary: \nThe author proposed a new method on NAS (DOTS) by decoupling the operation and topology search in DARTS during optimization. By updating the operation weights and edge combination weight respectively, the authors claim that their method can get much better performance. Further, the operation group anneal and edge combination anneal technique is applied in the SoftMax operation in operation and topology search.\n\n\nThe proposed decouple strategy is more like an edge selection refinement after the operation search. While the original DARTS retains the operation with the largest weight for each graph node, the proposed DOTS can learn the final topology automatically in one-level optimization. The motivation for this paper is interesting.\n\nHowever, the reviewer has several concerns.\n1. The division of groups according to whether have learnable parameters is not so convincing. The observation or motivation in Fair DARTS is the domination of skip connection during searching and ‘other operations are suppressed and steadily decline’. Thereby, it is unclear why the authors split the pooling operations and convolution operations into different groups since they may not suffer from the fairness issue as the skip connection. A comprehensive study on the division of groups is encouraged. Besides, it is noteworthy that the group search scheme is not new and has been proposed in STACNAS: TOWARDS STABLE AND CONSISTENT OPTIMIZATION FOR DIFFERENTIABLE NEURAL ARCHITECTURE SEARCH.\n\n2. The edge search phase is still coupled with the searched operation. The author mentioned that \"we observe that the operation weight merely indicates the importance of the operation on the speciﬁc edge, but not the evidence that the edge should be retained in the searched cell.\" However, we can further believe that the full topology (14*2=28 edges before edge search) after operation search is still coupled with the following edge search, because there is an underlying assumption in the edge search phase that the operations on the searched edges (the subset of the full topology) share the same operations of those on the full topology. In a sense, the edge search phase acts like \"learning to prune\". Thereby, it will be interesting to see the results of other pruning methods. More explanations and discussions are encouraged.\n\n3. The authors apply the decayed temperature SoftMax (anneal in the paper). However, the annealing operation in SoftMax has been widely used in NAS. For example: \"Searching for A Robust Neural Architecture in Four GPU Hours\", \"Neural Architecture Search in A Proxy Validation Loss Landscape\", \"DATA: Differentiable Architecture Approximation\".\n\n4. The results in Table 4 are a little weird. For example, the bilevel method achieves higher accuracy than the one-level method, which conflicts with results in DARTS. The original DARTS paper has ever studied the possibility of one-level optimization while it may have some failures. Is this a general phenomenon in NAS or owing to the specific setting of authors? More discussions on this phenomenon are required.\n\n5. The used strategies are straightforward and heuristic. While they bring some good results, the technical contribution is a bit incremental given there is no theoretical analysis.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A timely solution to address DARTS's ineffectiveness problem but needs improvements",
            "review": "\n+ This paper separates the cell topology representation from the operation weights to make an explicit topology search. (+)\n\n\n- By saying, \"However, DARTS mainly focuses on the operation search, leaving the cell topology implicitly depending on the searched operation weights\", the authors ignore the topology search in DARTS. Specifically, the topology is also searched in DARTS by deciding the skip connection and deciding the previous nodes to be connected. (-)\n\n\n- The authors define a topology search space as the pairwise combinations of edges. What is the advantage of such a search space? Is it possible to define the topology search space as the ways to select an edge from the whole edge set? My understanding is that the latter is better than the one proposed by the authors. (-)\n\n\n+ The following observation by the authors is believed to very important: \"Figure 1a shows that the stand-alone model accuracy has no clear ranking correlation with edge importance indicated by the operation weight. It implies that DARTS' searched cell is sub-optimal because it cannot converge to the best cell topology, which is consistent with the finding that DARTS cannot surpass random search in some cases\". This observation is important. Although some works have noticed the importance of architecture rating in NAS, some works still neglect this critical problem and insist on a black-box architecture search. I appreciate that this paper can recognize the architecture rating problem in NAS. (+)\n\n\n- The following observation by the authors is also critical: \"Figure 1b demonstrates that the edge combination weight has a better ranking correlation with the stand-alone model accuracy.\" I appreciate this important observation, which provides an insight into the timely reproducible NAS. Could the authors offer theoretical proof for this observation? (+-)\n\n\n- Dividing operations into two groups based on whether they have learnable parameters are not novel. Please discuss it with previous work like StackNAS to show the difference in operation grouping ways. (-)\n\n\n- The idea of annealing the operation weights to reduce the optimization gap between the searched over-parameterized and derived child networks is neither new nor novel. Please discuss it with SNAS, ProxylessNAS, and GOLD-NAS. Moreover, recent works in the NAS community have employed single-path one-shot methods to avoid the risk of discretization error. (-)\n\n\n+ The proposed method is simple and straightforward so that there are little flaws in the method. (+)\n\n\n+ The measure of CORRELATION BETWEEN EDGE IMPORTANCE AND MODEL ACCURACY is appropriate. Especially, the way to calculate the edge importance is proper. As we know, ineffective architecture rating is the drawback of existing NAS methods, with two ICLR 2020 papers and many ICLR 2021 submissions suggesting that most of the current NAS methods are not better than random architecture selection. I appreciate the authors recognize the importance of architecture rating and propose a metric to measure the CORRELATION BETWEEN EDGE IMPORTANCE AND MODEL ACCURACY. (+)\n\n\n- Although the method is simple and a re-implementation is easy, it is useful if the authors provide codes as a supplementary so that the readers can directly run the code without additional efforts. (-)\n\n- More recent works aiming at improving architecture rating (e.g., PCNAS, DNA, and CDAS) should be compared. (-)\n\nPCNAS: Improving one-shot nas by suppressing the posterior fading\n\nDNA: Block-wisely Supervised Neural Architecture Search with Knowledge Distillation\n\nCDAS: Cyclic differentiable architecture search.\n\n\nOverall, I think this paper is a timely solution to address DARTS's ineffectiveness problem, which provides a direction to solve the core problem of existing NAS, i.e., the inaccurate architecture rating problem. I recommend an acceptance for this paper.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "suggest a rejection",
            "review": "This study aims to decouple operation and topology in current differentiable NAS. It points out two problems in the DARTS framework, the edge importance coupled with operation weight, and the optimization gap between search and evaluation. To this end, the distribution among all candidate edge combinations for each intermediate node is explicitly modeled. Two annealing schemes are introduced to bridge the optimization gap. Experiments on CIFAR-10, CIFAR-100, and ImageNet are conducted. The paper is well-organized in general. However, I have the following concerns. \n\n1.\tThe motivation of grouping is unclear. The paper points out two problems in the DARTS framework. But what is the relation between grouping operations and the problems to solve? I do not see the motivations. The authors claim that the purpose is to avoid unfair competition among operations. Why do the authors group based on whether they have learnable parameters? How does this grouping solve the unfair competition? I think the unfair competition still exists within each group, such as conv 3x3 and conv 5x5. Besides, in DARTS, the zero operation is introduced to make probabilities in different edges comparable. Why do the authors consider zero as a valid operation in the non-parameterized group?\n2.\tThe contribution of decoupling operation and topology search is somewhat over-claimed. The study explicitly introduces a distribution among all candidate edge combinations for each intermediate node. It derives the topology based on the weight of edge combination, instead of operation weight that does not correlate well with topology. So it is actually a better topology derivation. But the topology is still limited by constraining 2 input connections for each intermediate node. The topology search is still not enabled. \n3.\tSome descriptions are not clear. How do the authors determine the searched operation from $o^{(i,j)}_p$ and $o^{(i,j)}_f$? How to derive the final architecture from $\\{\\beta, \\alpha_{o_n}, \\alpha_{o_p}, \\alpha_{o_f}\\}$? The corresponding descriptions are lacking. \n4.\tThe edge weights $\\gamma^{(i,j)}$ seem not normalized. \n5.\tNovelties are limited. The grouping operation scheme has been proposed in previous studies, such as [1]. The annealing scheme to bridge the gap between search and evaluation has been widely adopted in many studies, such as [2, 3]. \n6.\tExperiment result on CIFAR-10 is not competitive. \n\n[1] StacNAS: Towards stable and consistent optimization for differentiable Neural Architecture Search, arXiv: 1909.11926;\n\n[2] SNAS: stochastic neural architecture search, arXiv:1812.09926;\n\n[3] Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search, CVPR 2019;\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A paper that combines several existing methods",
            "review": "This paper presents a differentiable NAS method that aims to decouple topology and operation in the search space. It shows promising search results on standard NAS benchmarks including CIFAR and ImageNet.\n\nOverall, I think the paper overclaims the contribution. The goal of \"decoupling topology and operation\" is not achieved by the proposed approach. This paper seems to deliver the message that once the operations and edges are individually annealed and pruned to contain fewer candidates, the search space is well decomposed and the drawbacks listed in Section 2.1.1 (the inconsistency between the rankings of the predicted weights and the true performance) can be largely solved. But, this is not true.\n\nThe biggest issue of NAS, in particular, weight-sharing NAS, lies in that we are using one super-network to estimate the performance of exponentially many sub-architectures. The essence of the optimization gap is that when we try to reuse the weights in the super-network to evaluate the sub-architectures, the weights are actually optimized in the scenario that many unnecessary edges and operations exist, not only for the sub-architectures. So, the estimation is inaccurate.\n\nTo alleviate this problem, we have two choices, either pruning some operations or edges during the super-network training process, or preserving all of them till the end and using a standalone sampling process for evaluation. Both of them suffer inaccuracy, because, in many scenarios, we are not sure if an edge/operator needs to be pruned. If it is pruned, the super-network becomes closer to the final sub-architecture, but the pruning is based on the current status and we are not sure if the pruned elements will be useful later. If it is not pruned, then the optimization gap always exists. In other words, the first choice can easily lead the search procedure to a local optimum - this paper facilitates the local optimum by annealing but does not analyze how this method harms the search procedure. That is what I said, this paper seems to overclaim the contribution, but does not focus on the critical part of NAS (this is very difficult indeed).\n\nTechnically, the proposed approach is a combination of three existing works. First, it follows PC-DARTS to represent the edge weights with a set of \\beta parameters. Second, it follows StacNAS to partition parametric and non-parametric operations into two groups. Third, it follows ASAP to perform annealing on the operations. The technical contribution is limited. I agree that the results on CIFAR and ImageNet are good, but simply having these good results is insufficient for an ICLR submission.\n\nSo, I will give a score of 5: Being technically less novelty + good results -> marginally below the threshold. The research direction is not good, so no credit is added on this point.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}