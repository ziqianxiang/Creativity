{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposed two variants of the SELU activation function, termed the leaky SELU (lSELU) and scaled SELU (sSELU), respectively, in order to yield a stronger self-normalization property. The review process and the discussion find the following issues:\n\n- The hyperparameter tuning for the baselines is insufficient for the baselines so that the comparison may be unfair. \n- The experiment results (Table 2) do not show superiority of the proposed activation functions. In addition, the results appear to be unrelated to each other. (see Reviewer 3's detailed update)\n- Reviewer 2 pointed out that the architecture that the authors used was far from the SOTA. I read the authors' response. This paper may benefit from adding some even naive workaround and making fair comparisons under the SOTA architecture. \n\nI do not think (6) is a good way to present this equation. The authors may want to perform change of variable and replace $\\sqrt{q}z$ by $z$ in the integral and add this form to the right-hand side of (6). In addition, $\\epsilon$ appears in Definition 2. However, when the authors mention the self-normalization property in Definition 2, they omit $\\epsilon$. It might be better to call it $\\epsilon$-self-normalization property to stress that this definition depends on $\\epsilon$.  \n\nOther minor issues:\n- The line right below (15) on page 11, the authors did not need to capitalize \"orthogonal\".\n- In eq (16), $W_l^{,T}$, the comma is unnecessary. "
    },
    "Reviews": [
        {
            "title": "There seem to be multiple gaps in the theory and the modifications do not mitigate gradient explosion",
            "review": "## Summary \n\nThe paper proposes two modifications to SELU activation function to improve it with regards to preserving forward-backward signal propagation in neural networks. The work builds on top of the mean-field theory literature and provides a modified self-normalization property (additional constraints compared to SELU). Further, it discusses some heuristics (mixup, weight centralization) to improve performance in practice.\n\n## Strengths\n\n1. The problem is interesting and important to improve trainability in neural networks. It is shown recently that SELU suffers from gradient explosion and this work attempts to circumvent it by redefining the self-normalization condition. This condition leads to one more scalar parameter in SELU function and discusses a method to obtain this scalar.\n\n2. Experiments are conducted on a 56-layer CNN for cifar and tinyimagenet datasets and modified mobilenetv1 for imagenet. These proposed modifications to SELU seem to result in improved accuracy even though the improvement is minimal on imagenet.\n\n3. Overall the paper is clearly written.\n\n## Weaknesses\n\nThe main weaknesses of the manuscript in my opinion are as follows:\n\n1. There seem to be multiple gaps in the theory and the modifications do not mitigate gradient explosion:\n\t- The theoretical analysis lacks rigor and the main purpose of mitigating the gradient explosion issue of SELU is not sufficiently addressed. In fact, if I understand correctly, even with the redefined self-normalization property gradient vanishing/explosion can occur. Note, according to Eq. 15, when $q_l <1$, the numerator can grow arbitrarily. Please clarify.\n\t- Related to the above point, the 3rd condition in Eq. 12 and the subsequent argument of gradient norm converges to the fixed point is unsubstantiated. To my understanding, Eq. 15 only gives a lower/upper bound depending on the value of $q$ and it does not convey anything regarding convergence to such a fixed point. If it does a rigorous proof would be required.\n\t- The assumption in Eq. 4 is unjustified. In Fig. 3 there is a plot provided but it has several underlying assumptions (eg, small $\\epsilon$ and $E[x^2]$ etc.) and it is not clear what functions would satisfy this assumption. I recommend the authors to look at Gaussian-Poincare inequality for connecting a function and its derivative under the Gaussian distribution. In fact, GP inequality is recently used to define a class of activation functions to solve gradient vanishing/explosion in neural networks [a].\n\t- The condition on $\\lambda$ (Eq. 19) is derived without any theoretical basis and it seems like it is found empirically. This also weakens the theoretical contribution of the paper.\n\n2. Contradicting arguments compared to dynamical isometry and mean-field theory literature:\n\t- In dynamical isometry and mean-field theory literature it is shown that dynamical isometry is sufficient to ensure stable gradients and for RELU the scalar is computed to be $\\sqrt{2}$. Refer [b]. To this end, it is confusing to me when it is mentioned that this condition \"will lose self-normalization\" in the second last para of page 4. Please precisely define what is meant by self-normalization (including its purpose) in this paper and also clarify this confusion.\n\t- In the literature it is known that when the width is large, the preactivations will be close to a 0 mean Gaussian distribution due to the central limit theorem (Assumption 2 in this paper). However, in 3rd last para of page 7, it is mentioned that \"networks with large fan-in are more likely to lose the self-normalization effect\". This seems to be contradicting as well.\n\n3. Discussion about the mean of activations:\n\t- The discussion about the mean of activations exploding in Sec. 6 seems irrelevant to me. In the theoretical derivation of this paper, I could not find any discussion on the mean of activations affecting any of the desired quantities such as $q$ or $\\phi(q)$. Please improve the clarity on how this is connected to the theory of this paper.\n\n4. No experiments on gradient vanishing or exploding behaviour:\n\t- As far as I understood, the main motivation is to fix the gradient exploding issue of SELU. However, there is no experiment showing this effect and this seems to reinforce my concern that the proposed modification does not solve the gradient vanishing/exploding issue (also mentioned briefly in the para before conclusion). This questions the significance of the contributions.\n\t- I understand that generalization error is the quantity that we mostly care about. However, self-normalization or the work in mean-field theory literature concerns on trainability. Therefore, I believe, it is important to show the trainability behaviour and the propagation of gradients. Note, the theory does not convey anything about the generalization error directly but the signal propagation in the forward and backward directions.\n\nIn retrospect, I just want to say that in the current form theory and heuristics are mixed together making it difficult to see where the benefit is coming from and I think if theory is tightened this could be a good paper. \n\n## Minor Comments\n\n1. It seems the modifications could be done to other activation functions as well such as tanh etc. Please consider.\n2. Please explain what is the meaning of $\\epsilon$ in Eq. 10. This is not clear from this paper.\n3. First para in Sec. 7.1 : \"considerably higher\"\n\n## References\n\n- [a] Lu, Y., Gould, S. and Ajanthan, T., 2020. Bidirectional Self-Normalizing Neural Networks. arXiv.\n- [b] Pennington, J., Schoenholz, S. and Ganguli, S., 2017. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. NeurIPS.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting results but needs better experiments",
            "review": "In this paper, the authors propose a new definition for the self-normalization property of a network. Using this definition, the authors propose two new activation functions: sSELU and lSELU. Performance of the new activation functions is tested on benchmarks like CNNs on CIFAR10/CIFAR100/Tiny-ImageNet and MobileNet on ImageNet.\n\nThe new definition of self-normalization is interesting and could potentially be useful. The paper is also written reasonably clearly (see below for minor comments on how I think the writing could be improved). However, I think the experimental section of the paper using the two proposed activation functions have a number of limitations (which I describe below) that explain my rating. I am happy to raise my score if the authors address some of my concerns.\n\nMain comments/questions:\n- In the end, it seems like solving the problem of the shift in the mean is the main thing that makes SELU, sSELU and lSELU work on MobileNets. Is there a problem of gradient explosion in SELU apart from growing means that sSELU or lSELU solves, and if so, what is an experiment that shows this? Without this, it is not clear to me whether the slightly better results of sSELU and lSELU is simply a result of sub-optimal hyperparameter tuning of the baselines (see comment below).\n- The BN+ReLU baseline run on MobileNet seems a bit suboptimal to me. The original MobileNet paper reports 71.7% accuracy, and other work reports even higher numbers, e.g., 72% in https://arxiv.org/pdf/1710.05941.pdf. This could be a result of the learning rate not being tuned for any of the activation functions. It is not obvious to me that the optimal learning rate should be the same across all activation functions, so ideally optimal performance should be shown over a learning rate sweep for each method.\n- The authors mention that neural networks with larger fan-in are more likely to lose the self-normalization effect on the mean based on the observation that if the mean is less than 1/N, then multiplication with the weight decreases the mean of the pre-activations. I do not however follow this argument (and don't think it is true), since the weights are typically initialized from N(0, 1/N), and therefore the probability that the mean is less than 1/N does not change with larger N?\n- How sensitive is performance to epsilon? The fact that epsilon needs to be tuned seems to be a major disadvantage of the proposed activation functions.\n- It would be interesting to see the following ablations as well for the MobileNet problem:\n    - SELU + Weight Centralization\n    - dSELU + Mixup\n    - dSELU + Weight Centralization\n\nOther minor comments:\n- It would have been good to report the values of the parameters alpha, beta and lambda of sSELU and lSELU for at least one problem (such as the MobileNet experiment), and compare them with SELU.\n- Would be good to add SELU in figures 1b, 1d and 3.\n- The authors mention in definition 2 that the self-normalization property is stronger when phi(q) is closer to 1/q. Can the authors expand on why this is? It is not obvious to me from the definition.\n\nAdditional comments on the writing:\n- It is not clear to me how the new definition is \"easier to use both analytically and numerically\".\n- Explanation of related prior work should be improved, as it is not quite clear right now. In particular, the explanation of the prior work in mean field theory, as well as the explanation of \"gradient norm equality\".\n- The first paragraph on the second page needs to be checked for a number of typos (Forbenius -> Frobenius, etc) and grammatical errors.\n- Typo at the end of page 2: boarder -> border\n- Equation 13 and 15 need to be made clearer by adding \"if q_l > 1\", etc.\n\n----------\n\nUpdate after rebuttal: I thank the authors for the detailed response. Some of my comments have been addressed. However, some of my major concerns remain such as the insufficiency of the hyperparameter tuning for the baselines to do a fair comparison. I am keeping my score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid effort, but theoretical motivation and experimental evidence aren't convincing enough",
            "review": "# Update\n\nI thank the authors for extensive replies and updates to the paper. Most of my questions are answered, and the paper quality is substantially improved. I would not be opposed if other reviewers recommends to accept it. Unfortunately I still can't raise the score and advocate for it myself, since:\n\n1) Table 2 doesn't really show superiority of new nonlinearities, since the bolded (bottom) entry has both trainable $\\lambda$, centralization and Mixup, while the baseline of dSELU only has mixup and trainable $\\lambda$, no centralization. Without centralization (Mixup + trainable $\\lambda$), lSELU/sSELU/dSELU perform comparatively. Without trainable $\\lambda$ (only Mixup), they perform a bit worse than dSELU. With only centralization, BN is still better. Further, even after rebuttal, I still believe that making $\\lambda$ trainable effectively cancels the preceding theoretical discussion, and makes this subset of results somewhat unrelated to the main idea of the paper. Finally, Table 1 shows a more robust benefit over dSELU on CIFAR-100, but not on CIFAR-10/TinyImageNet (where s/lSELU can be both better and worse than dSELU), which is in my opinion underwhelming given the added implementation complexity.\n\n2) Figure 4 is very promising, but I find that SELU doesn't look that bad on it, which again makes me wonder whether the new improved self-normalization actually matters in Tables 1/2, especially given gradient clipping and other heuristics in Table 2.\n\nSo at the moment I find that the proposed nonlinearities are promising in terms of both self-normalization (Figure 4), and generalization (Table 1/2/6), but these results appear to be largely unrelated to each other, and neither of them in separation is strong enough to convince me to try s/lSELU over dSELU (given additional implementation complexity + the boolean hyper-parameter of whether to use lSELU or sSELU) or over BN (given additional hyper-parameter $\\epsilon$). I wish the paper either showed clear use-cases where one can't train BN/SELU/dSELU networks at all in reasonable time (but new nonlinearities allowed it due to superior normalization), or more robust generalization results.\n\nOriginal review below:\n\n# Outline \nThe paper proposes two new nonlinearities designed to normalize the second moment of activations and the Frobenius norm of gradients, and therefore avoid exploding/vanishing activations/gradients. The nonlinearities are evaluated on multiple image datasets. \n\n# Review\n\nOn one hand, I find the theoretical motivation and the idea of adding the new minimization constraint compelling, and empirical performance of nonlinearities promising (notably on CIFAR-100). I appreciate evaluation on multiple datasets, and providing native CUDA code. Further, several decisions along the design process appear well-motivated and backed by experiments (e.g Figure 2, 3, 4).\n\nOn the other hand, I find the theoretical contribution to be relatively incremental from Chen et al (2020b), and unrelated to the uptick in generalization performance in Tables 1 and 2 (motivation for deriving the nonlinearities concern improving trainability, which is not studied experimentally in the paper; better generalization is a nice bonus, but is not predicted by theory). \n\nOn the empirical side, I believe there are important gaps in the experimental results that leave me uncertain of how robust the improved generalization is, and hence whether new nonlinearities justify the added complexity (I believe the claim that no new hyper-paramaters are introduced is false) over SELU or dSELU from Chen et al (2020b). \n\nAs such, at the time of submission neither theory nor experiments appear sufficiently convincing for me to accept the paper, but I am open for discussion.\n\n\nBelow are my specific questions/concerns.\n\n\n## Motivation/Theory:\n1. Do I understand correctly that Definition 2, “new self-normalization definition”, and what follows within section 4 is effectively the same as “partial normalization” in Chen et al (2020b), section 5.3? In either case, I suggest drawing a more explicit connection with Chen et al (2020b) in this section, highlighting precisely what is novel relative to Chen et al (2020b), and potentially softening/clarifying the first bullet point of main theoretical results in the Introduction respectively.\n2. One important weakness of this work is that the paper does not evaluate whether the designed nonlinearity ends up doing what it was designed to do. Namely, we know that in finite networks Assumptions 1-3 don’t hold, Eq. (4) is not exact, and, for $\\lambda \\geq 1$ $\\frac{d \\phi(q)}{d q}|_{q=1}$ is clearly far from $-1$ in Figure 1, notably even deviating further from it than dSELU in Figure 1.c. As such I am not persuaded that s/lSELU has better gradient behavior than dSELU in practice. One could demonstrate such benefits in different ways, perhaps by showing that s/lSELU allows higher learning rate in deep networks on some toy task, or comparing how the gradient norms evolve with depth in deep random networks compared to dSELU etc. Further, even in generalization experiments, as mentioned in appendix B, both this paper and Chen et al. (2020b) clip the gradients by $[-2; 2]$, which, from my understanding, further conceals whether these nonlinearities help normalize gradients or not.\n3. Making $\\lambda$ trainable in s/lSELU is a somewhat disappointing decision/necessity since there is no theoretical reason to do so, and from a quick look at the referenced Zhang et al. (2019) I could not find the justification there - could you please clarify which part of the paper you were referencing? Does $\\lambda$ remain constrained to $\\geq 1$ when trained? \n4. I would like the discussion about “large fan-in networks are likely to lose self-normalization on the mean” (section 6, 3rd paragraph) to be either elaborated, referenced, or confirmed empirically (e.g. on a toy task with networks of increasing fan-ins). Precisely, the conclusion is reached under the assumption that the mean of weights $\\Delta \\mu$ (during training) is independent of fan-in $N_{l-1}$, but isn’t this clearly not true? I.e. weights with larger fan-in are sampled with smaller variance, and I expect them to move less and less during training as the network gets wider, hence increasing fan-in could proportionally decrease $\\Delta \\mu$. In this case these changes cancel out, and the conclusion does not appear justified.\n\n\n## Experiments:\n1. As presented, experiments do not support the introductory claim that “no additional hyper-parameter is introduced\". Firstly, I would prefer if the claim was more explicit, i.e. “no additional hyper-paramater relative to dSELU”. Secondly, even apart from $\\epsilon$, there are still hyper-parameters of a) which among s/lSELU to use; b) whether to have $\\lambda$ trainable or not; c) whether to use Weight Centering, or mixup, or nothing. From reading the abstract, I was expecting a drop-in, no-hyper-parameter activation, but unfortunately there are quite a few knobs to tune.\n2. In both Tables, I believe it’s important to have entries with hyper-parameter-free d/s/lSELU as a baseline, i.e. with $\\epsilon = 1 / L$, to get an understanding of what order of improvement is gained from the additional $\\epsilon$ hyper-parameter. I’m concerned that If $\\epsilon$ is necessary, the relative performance of d/l/sSELU against BN/SELU may be less compelling, especially if the best-performing models were selected on the test set (see next question). \n3. In both tables, was a validation set used to select best performing numbers, or were the numbers selected on the test set (please expand appendix B with this detail)? \n4. Table 2: what $\\epsilon$ was used?\n5. In both Tables, it would be useful to see how sensitive different nonlinearities are to $\\epsilon$, so I suggest adding respective tables for all values in the appendix (also noting the specific values of $\\lambda, \\alpha, \\beta$).\n6. In Table 2: since Chen et al. (2020b) is the primary point of reference for this work, I would argue that for this Table to be convincing, it should also include “dSELU + Mixup” and “dSELU + Mixup + trainable $\\lambda$” (and, ideally,  “dSELU + Weight Centralization”).\n7. In Table 2, could you please include results with non-trainable $\\lambda$ as well?\n8. Was trainable $\\lambda$ a single shared parameter for the whole network, or one per layer?\n\n## Mentioned references from the paper:\n* Chen et al. (2020b): [A Comprehensive and Modularized Statistical Framework for Gradient Norm Equality in Deep Neural Networks](https://arxiv.org/abs/2001.00254)\n* Zhang et al. (2019): [Fixup Initialization: Residual Learning Without Normalization]( https://arxiv.org/abs/1901.09321)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper with lacking experiments",
            "review": "Summary: The authors propose variants of the SELU activation \nfunction that yield a stronger self-normalization property. The \nanalysis in done using mean field theory with several\nassumptions on the randomness of quantities in a neural\nnetwork. \n\nPros:\na) The paper is clearly written and the line of thought can \neasily be followed. \nb) The notation is clean. \nc) The mathematical formulations are sound.\nd) The connections between SNNs and the paper series\nby Poole et al. and Schoenholz et al. has been made\nquite clear. \n\nCons:\na) The work has limited relevance due to the \nfact that it is unclear whether the proposed property is \nindeed crucial for learning and that the empirical results\nare biased and far from state-of-the art. \nAs stated by Goodfellow et al. in their \"Deep Learning Book\"\n(Section 6.3.3), \"New hidden unit types that perform roughly comparably to known types are so common\nas to be uninteresting.\" They provide an example using cos() as activation function reaches a test error\nbelow 1% on MNIST. This implies that the machine learning community should be rigorous in the\nassessment whether a new activation function is worth publishing in order to avoid drowning literature\nabout activation functions. Furthermore, the number of potential activation functions for neural networks is\ninfinite. Therefore, activation function research should be focused around those activation functions with\ninteresting theoretical properties or -- if no theoretical properties are given -- the new activation\nfunctions should increase the state-of-the-art of predictive performance.\ni) The mathematical derivations use several assumptions on the quantities, such as weights and\nactivations. These assumptions would require commenting on how strong they constrain\nthe applicability of this theory. The author should comment on how strong those assumptions are\nand how they can be leveraged.\nii) It is unclear why a stronger normalization property should improve learning. The authors\nshould back this more.\niii) Overall, the derivations are rather related to the backward pass. The authors should \ninclude a view on their activation functions in terms of vanishing/exploding gradients (see also [1]).\niv) The main concern of the reviewer are the experiments. Firstly, SNNs were introduced with\nrelatively large-scale experiments on fully-connected networks. In order to demonstrate \nimprovements, the suggested activation functions shoudl be compared in that set of experiments.\nThe  presented experiments are done with architectures that are relatively far \nfrom the current SOTA on CIFAR and Imagenet [2], such that their relevance cannot be judged well. \nThe authors should introduce their suggested activation functions into SOTA architectures.\nv) The presented performance metrics suffer from a hyperparameter selection bias, since \nthe best epsilon-parameters are selected (section 7.1.). The authors should select \nhyperparameters of their method on a validation set. \n\nb) The novely of this could be stated clearer. Large parts of the derivations are \nsimilar to those by Poole et al, 2016. The authors should make their theoretical contributions\nclearer. \n\n\nQuestions: \nQuestions are implicitly contained in the suggestions above.\n- Can you elaborate more on the connection to mixup?\n- Skip/shortcut connections typically pose a problem both theoretically and practically [3]. How are they handled in this work?\n\n\n\nMinor: \n1) English editing might be required. Should it say \"Redefining THE self-normlization property\"?\n\n\nReferences:\n[1] Hoedt, P.J., Hochreiter, S. and Klambauer, G. Characterising activation functions by their backward dynamics around forward fixed points. Critiquing and Correcting Trends in Machine Learning workshop at NeurIPS 2018.\n[2] Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n[3] Huang, Z., Ng, T., Liu, L., Mason, H., Zhuang, X., & Liu, D. (2020, May). SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units for speech recognition. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6854-6858). IEEE.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}