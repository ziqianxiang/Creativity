{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The reviewers generally appreciated the problem statement and topic of the paper, but raised concerns across the board about the empirical evaluation. Since the paper is largely experimental in nature, a compelling experimental evaluation is important. Reviewer concerns generally centered around: (1) the relative simplicity of the evaluated tasks; (2) somewhat questionable baselines. While the authors provided responses to some of these concerns, after discussion the reviewers generally still felt that these issues were rather severe, and preclude publication at this time. I would encourage the authors to take this feedback into account in improving the empirical evaluation in the future."
    },
    "Reviews": [
        {
            "title": "The paper explores cross modal adaptation to map system state (available in sim) to observations (available in real world) in RL setting",
            "review": "This paper explores learning the mapping from state to sensor observations as part of RL policy learning. This is a relevant problem to enable sim-to-real transfer.  The paper explores cost terms that have been used in GANs to align distributions between the state and observation. The paper shows experiments in simulation, where the observation is images of the robot in the environment. I have some concerns  which are listed below,\n\n1. The paper only shows transfer from the robot's state to images of the robot in the environment. The state of a system might not sufficiently capture high fidelity information from observation. E.g., can this method reconstruct uneven terrain from only observing the robot's pose and other low dimensional representations in state? It's unclear to me how the method can transfer to work with observations on real systems. \n\n2. All the chosen environments do not involve a second object to interact with. E.g., can the method work on the OpenAI cube rotation task where the state could contain the object pose and the observation might not. It's also shown by other researchers that walking doesn't require high fidelity observations~[1] such as vision so the effectiveness of the method to transfer policies trained on state to observations is unclear. Is the method overfitting to recorded trajectories? one good test would be to deploy the policy in a different environment(increased slope of the floor?) that wasn't part of the recorded trajectories but was part of the full state policy.\n\n3. It would be interesting to discuss how this work could leverage or compare against multi-modal representation learning in robotics~[2]. \n\n\nReferences:\n[1] Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. Learning quadrupedal locomotion over challenging terrain. Science Robotics. 2020 Oct 21;5(47).\n[2] Lee MA, Zhu Y, Srinivasan K, Shah P, Savarese S, Fei-Fei L, Garg A, Bohg J. Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. In2019 International Conference on Robotics and Automation (ICRA) 2019 May 20 (pp. 8943-8950). IEEE.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New topic in RL, but experimental results are missing",
            "review": "## Cross-modal Domain Adaptation for Reinforcement Learning\n### Summary\nThe authors propose CODAS, a domain adaptation algorithm for transferring policies learned in state-space to the image domain. They learn a variational RNN to map image inputs to state space with an adversarial loss to ensure that the distributions of the mapped states and the original states are matched. Their experiment results show improvement over one behavior cloning baseline and one GAN domain adaptation baseline.\n\nOverall, the paper is clearly written, and the topic is new and relevant to the field.\n\n### Strength\n- The first cross-modal adaptation for the RL agent to the best of my knowledge.\n- No paired data is needed.\n- Interesting DM model.\n- Good improvement over the baselines considered in the paper.\n\n\n### Weakness\n- Insufficient experimental results. Specifically:\n   1. What's the performance of the policies trained in the original state-space?\n   2. What's the performance of the policies trained directly in the image space?\n   Knowing these numbers gives us a better idea about how well the adaptation works.\n\n- Unclear description of GAN baselines. Did I miss something? Can't find a description in the appendix as well.\n\n- Need human expert policies to collect image input. It is unclear how this component may affect the final performance of CODAS.\n\n- \"In this paper, we propose Cross-mOdal Domain Adaptation with Sequential structure (CODAS) that learns a mapping function from images in the target domain (real world)\" (page 2): this is misleading. The target images are not really from the real-world. The images are rendered in simulation, with no realistic perturbations.\n\n### Minor Comments\n- Notation: why refer to Eq in the appendix instead of the main paper? ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting problem formulation; more clarity needed",
            "review": "Summary:\n\nThe authors pose a problem of learning a mapping when when a low-dimensional state simulation is given along with target image tragectories. The goal of the paper is to learn a mapping from image to state such that at test time the agent can directly use this mapping with a trained policy from the simulator to perform in the target domain. The contribution of this paper lies in its problem formulation and an algorithm named Cross-mOdal Domain Adaptation with Sequential structure (CODAS).\n\nPros:\n\nThis paper proposes a novel and interesting problem for cross-modal transfer.\n\nI appreciate the paper shows the results when the dynamics of the source is slightly different from the target dynamics. \n\nCons:\n\nAre there any assumption about the target data? Does data collection policy in the target domain resemble the pretrained source policy in the simulator in some way?\nIt seems to me that without this assumption q_\\phi can be different from the groundtruth posterior p(s_t|o_t).\nI am not sure if this is related to this sentence, \"The policy \\pi in source domains included in the second term does introduce a new assumption that real-world data are collected by a known behavioral policy.\" Do we need the policy in the target domain? If so, please clarify where this is used and mention the assumption in the beggining.\n\nThe experimental section needs clarity in baseline details. The GAN baseline is not explained in the paper making it difficult to interpret the comparative performance. Potentially, there should be a sequential version of GAN that generates tupples/sequences rather than just a single image [1].\n\nWhat is the reward ratio? Please elaborate.\nDoes the groundtruth state-based (source) policy achieve the reward ratio of 1?\n\nThe dynamic mismatch should be investigated more. How differ can the dynamics be before things break? This is very important to understand whether this formulation can **actually** be applied to the real world settings.\nI am curious to see what happen the states contain irrelevant information or be represented in a different way?\n\nIn equation 7, where does log p(\\hat{o}_t|o_t) come from? Why do we not use the first term from l_D in equation 5 here?\n\nConclusion:\n\nThis paper proposes an interesting problem of cross-modality domain adaptation. However, there are a few questions to be addressed above to substantiate the potential of the proposed low-cost sim2real.\n\n\nReferences:\n\n[1] Learning Plannable Representations with Causal InfoGAN (http://papers.nips.cc/paper/8090-learning-plannable-representations-with-causal-infogan)\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, but further details required",
            "review": "## Summary\nThe paper proposes a new approach for performing cross-modal domain adaptation, i.e. adapting a policy trained with inputs from modality A (eg low-dimensional environment state) to work with inputs from domain B (eg images). The main use case demonstrated in the paper is the adaptation of policies trained on states in a simulator to work on image inputs, which can be useful for eg real world deployment where states might not be available. While it is a very classic approach to separately train a perception module images --> state (eg in robotics), the main novelty of the presented method is, that this mapping can be learned without the need for paired [image, state] data.\n\n## Strengths\n- the discussed problem of cross-modal domain adaptation is very relevant, since it can allow for the transfer of policies from training in simulation to deployment in real environments\n- being able to learn the image --> state mapping without paired data can be impactful since it is often tedious / impossible to get exact state annotations for real world data\n- the proposed technical approach is novel to the best of my knowledge\n- the method is evaluated on multiple environments and some of the design decisions are ablated + (preliminary) experiments on the robustness of the method are conducted\n\n## Weaknesses\n- **baselines are not described in detail**: the experimental section only mentions that \"We modify state-of-the-art methods in same-modal domain adaptation for comparison\" -- it is important to describe which method for same-modal adaptation was used and how it was modified for the cross-modal case to properly judge the results. \n- **no investigation into why baseline does not work**: figure 1 provides one possible explanation (because of not taking dynamics into account), but later the paper mentions it could be because the necessary biases present in image-to-image translation are not present in image-to-state translation, finally it could be because the method was not tuned sufficiently. It would be good to show a more detailed investigation of this, particularly since the paper claims to be the first trying to apply domain adaptation techniques to the cross-modal case\n- **only tested on visually clean environments**: the environments used for testing the approach are visually clean -- in particular: the state information is sufficient to fully render / reconstruct the scene. This is likely not true for more realistic scenarios (eg think about autonomous driving where the commonly used state representations are certainly not sufficient to render all details in the environment -- the whole point is to reduce the amount of information in the policy's input). I wonder whether the proposed method would struggle with such environments since it constrains the \"latent\" variable of the prediction model to be equal to the pre-defined state while training it to reconstruct the full scene. If there is lots of detail in the scene that is not covered by the information in the state the model might struggle to reconstruct the scene properly.\n- **requires action trajectories in the image data domain**: at least the version of the model that was experimentally validated requires access to action trajectories in the image domain (for the action reconstruction loss). Such action annotations might be hard to obtain in the real world -- baselines like CycleGAN do not require these since they learn the mapping purely from state and image data.\n\n## Questions\n- does the proposed approach need access to a differentiable behavior policy? I would think that just action samples would be enough and it would only need a differentiable policy on states (which is anyways available) --> however, the formulation in the paragraph before eq6 talks about a \"differentiable behavior policy\" so it would be good if the authors could clarify whether it is indeed needed\n- how stable is the training of the approach? (Cycle)GAN approaches can be notoriously hard to train (see Appendix Sec D5) -- the proposed method also uses a GAN to minimize divergence from the prior. Some discussion on stability of this training or even a quantitative analysis of performance over a range of hyperparameters could help to show that this method might be easier to train than the GAN-based alternatives?\n- when training the dynamics model online the simulator needs to be reset to the predicted state of the model --> how can value ranges be handled? ie what to do if the model predicts an invalid output state?\n- what are the \"numerical instabilities in Mujoco\" mentioned in Section 4? maybe add a little more explanation?\n\n## Suggestions to improve the paper\nAddressing the weaknesses I listed above can help to improve the paper. In particular I would suggest to:\n- clearly describe the baselines used \n- show *why* the baseline is failing to make clear that this is not an issue of tuning\n- include a GAN baseline that operates on short trajectory snippets instead of single states to see whether it is truly an issue of cross-modality or whether merely including dynamics information can help the simpler GAN baselines\n- test on environments with (non-static) visual components that are not captured in the state information (eg add moving visual distractors to the Mujoco scenes which are not part of the state)\n- ablate the action reconstruction component of the mapping loss to show that the method can work without the need for action annotations in the image domain\n- tone down the contribution claims that talk about \"transfer to real-world images\" since the tested scenarios are far from real-world images\n\nSome further, optional improvements:\n- add a baseline that shows RL trained on images from scratch --> this can show how much performance we gain / loose by doing the domain adaptation vs training from scratch (even if we loose some performance it is okay since training from scratch might not be feasible in the real world)\n- it seems that some of the differences to prior work require a better understanding of the technical details of the proposed method (particularly paragraph 4), it might be worth considering to move the related work section after approach before experiments\n- as mentioned in my summary, it is a quite classic approach eg in robotics to separately train a perception module that maps images to states and then use it to work in the real world with policies that operate on state input (eg motion planners etc), ie perform cross-modal domain adaptation. However, they assume access to a paired dataset of [image, state] tuples, which is potentially hard to obtain. I think the related work section could benefit from adding a discussion about this.\n- this sentence is not clear: \"We first formulate the generation process of the real world and simulation\" -- does this talk about how observations are actually generated in the real world or about the generation process of the model used in this paper? maybe reformulate for better clarity?\n- Section 3.3 is a bit confusing, it only later became clear to me that training the separate dynamics model is optional, this could be emphasized a bit more.\n- the dynamics mismatch experiment should be trained to full convergence (which seems to be ~10k steps for hopper), now it is only trained for 5k steps which equals only half-converged performance so it is unclear whether the mismatched runs will actually reach full performance\n\n## Overall Recommendation\nThe proposed method is interesting and the paper certainly has merit. My main concern is that it is currently hard to judge the thoroughness of the experimental evaluation since it remains unclear how the baseline is implemented and why it fails. Therefore I cannot recommend acceptance at this point. If the authors can more convincingly show why prior work fails in the cross-modal case and how their method fixes that I am willing to increase my score.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}