{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting idea and results, but too preliminary yet.",
            "review": "# Smooth adversarial training\n\n### Summary\n\nThe paper studies the effect of replacing the ReLU activation by a smoother\napproximation on the accuracy and adversarial robustness of SOTA neural network\narchitectures (ResNet-50 and EfficientNet) on the ImageNet dataset. The authors find\nthat all the different smooth approximations tested improve _both_ accuracy and\nrobustness, when applied to the backward path only, and even more when\napplied to both the forward and backward path. They confirm previous empirical\nresults according to which increasing width, depth and input resolution boosts\naccuracy and robustness, whatever the activation function. Combining all these\ntechniques, they claim to beat the state-of-the-art on ImageNet by a high\nmargin, both in terms of accuracy and of adversarial robustness.\n\n\n### Overall evaluation\n\nThe results are interesting and promising, but still in a preliminary state.\n\n1. _Standard deviations missing_: All the paper is about relative improvements\n  over the standard ResNet-50 architecture but does not report a single\n  standard deviation. That makes it difficult to evaluate the significance of\n  the improvements.\n2. _Only one dataset_: The results are tested on ImageNet only: what about other\n  standard datasets, such as MNIST, CIFAR10/100, SVHN, CelebA? Since the paper\n  is all about empirical results with almost no theory, several datasets +\n  standard deviations would seem a least.\n3. _Only one attack_: Everything is tested for one single attack type (PGD),\n  attack norm ($\\ell_\\infty$) and attack size ($\\epsilon = 4 / 255$). I would\n  at least expect that the findings be confirmed with other attack types (in\n  particular attacks that do not explicitly use the network's backward path,\n  such as the boundary or HopSkipJump attack), attack norms, and for different\n  attack sizes (e.g. plot robust acc vs attack-size). I also miss an analysis\n  of the influence of the attack/regularization size $\\epsilon_{train}$ used\n  during adversarial training: your proposed architecture could be more\n  efficient for some values of $\\epsilon_{train}$ than for others.\n4. _Readability & Tables_: Overall, I find the paper difficult to follow. I\n  recommend putting **all results in one or two big tables** (not just small\n  parts). That's what tables are made for: a concise presentation of many\n  results to simplify comparisons.  The text itself could be more concise.\n  (Also, current column names of Table 1 are misleading. I suggest making them\n  shorter and explain them in the caption. More generally, I would advice\n  making tables and figures self-contained: it should be fully understandable\n  from reading the caption with as little as possible reference to the main\n  text, and if it can't be avoided, then use clear references. Figures are\n  there to simplify the reader's work, not harden it by letting him search all\n  the info in the text.)\n\nFor these reasons, I _currently_ clearly suggest to reject the paper.\n\n\n### Additional comments/questions\n\n5. _Dependence on network architecture_: Results were only tested on\n  architectures with residual connections. What about more classical,\n  feed-forward architectures (without skips)? I am especially curious, since\n  the fact that input dimension helps robustness contradicts\n  [(1)](https://arxiv.org/abs/1802.01421) where it is shown that adversarial\n  vulnerability increases with the input dimension.  However, their results\n  only hold at initialization (with He init) and for classical feed-forward\n  nets without skips. So I'd be curious to know whether the difference comes\n  from the architecture, or a better training.\n6. _Initialization_: On the same note, how do you initialize? He initialization?\n  In particular, He initialization uses a factor 2 to account to for the ReLU\n  activation. Do you adjust this factor to account for the different activation\n  functions (as in pytorch's `torch.nn.init.calculate_gain(nonlinearity)`)?\n7. How do you modify the architecture in higher dimensions? (It can't be exactly\n  the same architecture.)\n8. Sec 4.2: I disagree with the last part of the last sentence \"... rules out\n  the effect from having responses when $x < 0$\". It might suggest it, but it\n  does not rule it out.\n\n(1) First-order adversarial vulnerability of neural networks and input\ndimension, ICML 2019, [link](https://arxiv.org/abs/1802.01421).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Impressive empirical results, but has some weaknesses",
            "review": "The paper improves the state-of-the-art of adversarial training on ImageNet by using smooth activation functions and optimizing architectures via neural architecture search. Compared to prior art (Qin et al. [1]), the paper reports clean and adversarial accuracy of 82.2% (+9.5) and 58.6% (+11.6), respectively.\n\nIf those results withstand scrutiny, they are pretty impressive and would in my opinion deserve publication in ICLR. The paper also reports on a number of insightful supporting experiments, e.g. exploring different types of smooth activation functions, studying the effect of scaling up the architectures etc., which are systematically designed and carefully described.\n\nHowever, the paper has several weaknesses:\n- It does not follow established protocols for evaluating adversarial robustness as put forward, e.g. in [2]. The authors state that 200 PGD iterations were sufficient for attack convergence - it would be good to corroborate this e.g. by showing plots of attack success rates over the number of PGD iterations. Did the authors use random restarts in PGD (same as Qin et al.)? Plotting the loss landscape around test samples (same as in Qin et al.) and reporting adversarial accuracy for PGD with larger epsilon would be other valuable sanity checks. I don’t want to create unnecessary work for the authors, but there are just too many examples of supposedly great advances in defending against adversarial examples which did not withstand scrutiny (e.g. consider [3]). Related to this: are you planning to release your trained models?\n- The paper talks extensively about “better gradients” being the reason for the improved performance of adversarial training, however, it doesn’t give a proper definition of what “good” and “bad” really means, except - inherently - that “good” gradients yield better results. This makes the notion of “better gradients” a rather tautological explanation. Moreover, Figure 1 does not provide sufficient evidence to substantiate the authors’ very general claim that, because “ReLU’s gradient takes an abrupt change, when its input is 0” it “significantly degrades the gradient quality”. Moreover, I find it misleading to denote the columns in Table 1 as\n“Improving Gradient Quality”. A priori, this table reports the effects of using smooth activation functions; the conclusion that this may yield improved gradient quality can only be drawn a posteriori. \n- Finally, the paper lacks discussion of related work and prior art, such as the usage of smooth activations in other contexts, but most importantly the relation to Qin et al. [1] who actually seem to be the first ones using smooth activations in adversarial training. Comparing the results with Qin et al., one could arrive actually at the impression that performance gains mostly stem from the neural architecture search rather than from the smooth activations. This needs to be clarified, and proper emphasis should be put on the relative merits of smooth activations vs neural architecture search throughout the paper.\n\nTwo minor comments on the “no-free-lunch” in Adversarial Training:\n- It is not just “generally believed” that there is a fundamental trade-off between accuracy and robustness; Tsipras et al. (2019) (and others) actually proved that such trade-offs exist.\n- There may be a misunderstanding of what this trade-off entails; the final model trained by the authors still incurs degraded clean accuracy compared to a conventionally trained model, which actually confirms the “no-free-lunch” trade-off.\n\n[1] Qin et al., Adversarial robustness through local linearization. In NeurIPS, 2019\n[2] N. Carlini et al. https://arxiv.org/abs/1902.06705\n[3] L. Engstrom et al. https://arxiv.org/abs/1807.10272\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting finding, but needs further analysis",
            "review": "############################## Summary ##################################\n\nThe paper proposes the use of smooth activation functions for better gradient propagation during attack generation and adversarial training. The authors demonstrate improvement in both adversarial robustness and clean accuracy by replacing ReLU activations with smooth functions during PGD-1 adversarial training on ImageNet dataset, for an $\\ell_\\infty$ constraint of 4/255. \n\n############################### Pros #####################################\n\n-  Interesting observation, can potentially give an additional boost in performance\n-  Adversarial robustness and clean accuracy improve at no additional computational cost\n-  Easy to integrate with any network architecture\n-  Scales well to networks of larger depth, width and larger image resolutions\n-  Achieves improvement over LLR-2 for ImageNet dataset\n\n############################### Cons #####################################\n\n  -  The paper considers a PGD-1 step (or R-FGSM) defense as baseline. The original PGD-1 step defense with ReLU activation itself might be suffering from gradient masking. In this case, even the proposed defense could also be a result of gradient masking, although the smooth activation function helps with better gradient computation. Absence of gradient masking in single-step defenses needs to be justified using thorough validation as discussed by Carlini et al. [1] using gradient-free attacks such as Square attack and SPSA, black-box transfer based attacks, attacks with multiple steps and multiple random restarts. Also, the sanity checks proposed by Athalye et al. [2] need to be demonstrated. \n  -  Evaluation on PGD-200 step attack is not sufficient. It might happen that there is a large benefit while evaluating against this attack, but only a marginal benefit with stronger attacks such as MultiTargeted attack [3] and AutoAttack [4]. \n  -  Since the paper is an empirical finding, it would require more experimental justification, to validate that it actually works across other datasets and conditions as well. ImageNet has many closely related classes and findings on ImageNet may not generalize well to other datasets. For example, LLR-2 [5] achieves very good results on ImageNet (better than TRADES), however the robust accuracy with their 2-step defense on CIFAR-10 is significantly lesser than TRADES. \n  -  For $\\ell_\\infty$ robustness, the most widely used benchmark is a threat model of 8/255 on CIFAR-10 dataset. Evaluation on this dataset would enable the reader to get a sense of where the proposed defense stands with respect to all other existing defenses. \n  -  The paper does not mention the impact of using smooth activation functions on multi-step defenses like TRADES.\n  -  The key novelty of the paper is the use of smooth activation functions for robustness. However this is also seen in prior works [5].\n\n[1] Carlini et al., On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.06705\n\n[2] Athalye et al., Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, ICML 2018\n\n[3] Gowal et al., An Alternative Surrogate Loss for PGD-based Adversarial Testing, https://arxiv.org/pdf/1910.09338.pdf\n\n[4] Croce et al., Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks, ICML 2020\n\n[5] Qin et al., Adversarial Robustness through Local Linearization, NeurIPS 2019\n\n########################## Reasons for score #################################\n\nAlthough the observation is very interesting, and can potentially be useful for adversarial training, the paper needs more thorough evaluation. For example, demonstration that it works on other datasets, better evaluation of the defense and impact on multi-step training. Also the use of smooth activation functions for robustness is seen in prior work. Hence, I vote for rejecting the paper. \n\n##################### Questions during rebuttal period ###########################\n\n  -  Please address and clarify the cons above \n  -  Could the authors also report the results across multiple reruns, since R-FGSM training is known to be very sensitive to initialization.\n  -  While R-FGSM training can give good results for small values of $\\varepsilon$, it may not work well with larger $\\varepsilon$ values [6]. It would be useful to know whether the proposed solution works well for larger $\\varepsilon$ values such as 8/255 \n  -  Could the authors share details of the base PGD-1 (or R-FGSM) training (optimizer, learning rate schedule, number of epochs, initial random noise added for R-FGSM attack, training loss, validation split, use of early stopping)? The baseline results seem to be much better than the results reported in FBF [7].\n\n[6] Andriushchenko et al., Understanding and Improving Fast Adversarial Training, NeurIPS 2020\n\n[7] Wong et al. Fast is Better than Free: Revisiting Adversarial Training, ICLR 2020\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good work, but incomplete experiments",
            "review": "### Main idea\nThis work points out an potential cause of the adversarial samples: the discontinuity of gradients for Relu activation. It further investigate a more smooth substitute, the soft-plus function, and show incredible improvements on ImageNet dataset.\n\n### Clarity\nThe writing of this paper is clear and straightforward\n\n### Originality\nThe argument that the Relu function worsen adversarial robustness has been suggested in some past works. Yet, according to my knowledge, this work is the first one to seriously investigate this problem and propose a solution. Also ,I notice that there is a similar work, who also proposes a new activation for robustness, termed as [Sparta](https://openreview.net/forum?id=iF81jBISQDV) in the submission of ICLR 2021.\n\n### Quality and significance\n\n#### Pros\n1. The motivation of this work aligns with the common sense in the domain of adversarial robustness. I myself also believe that, to fully solve the issue of adversarial samples, we need new designs and insights about the architecture of DNN. The proposal of using soft-plus function seems straightforward and reasonable. \n2. The experimental results on ImageNet is really awesome and convincing. The setting of empirical evaluations is also clear and suitable.\n\n#### Cons \n1. This work is short for the most important empirical evaluation setting: defense against PGD20 on CIFAR10, as most of the leading works all present their results of this setting as their main entry. The author claimed the complexity of training ImageNet forces them to adopt FGSM in the adversarial training procedure, yet this problem does not exist in CIFAR10. In fact, I've read [a recent work](https://arxiv.org/abs/2010.03593) that points out that the soft-plus function may not work on CIFAR10.\n2. As stated in the paper, the author believe the soft-plus function can smoothen the gradients and therefore lead to better results. I've seen the improved results in the paper, but I haven't seen any empirical analysis about the gradients of models with or without soft-plus.\n3. The content in Section 5 seems somewhat redundant and irrelevant to the main idea. It simply uses the framework of EfficientNet and does not offer any special understandings for adversarial learning. It is more suitable to be presented in the appendix.\n\n### What I expect the author to respond:\n1. Verify the effectiveness of your work on CIFAR10 like TRADES and MART.\n2. I have a question: Why only using soft-plus in the back-propagation? Is this the key design? Can you explain its motivation or has anyone else tested this scheme before? Can you provide results when using soft-plus in both forward and backward? \n3. Please provide visualization or other statistics that can prove that your method did smoothen gradients. Maybe replace Section 5 with this analysis.\n\nAs long as the author can answer these three questions, I will raise my score to 7 or 8.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}