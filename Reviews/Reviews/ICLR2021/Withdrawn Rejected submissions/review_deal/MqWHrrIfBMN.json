{
    "Decision": "",
    "Reviews": [
        {
            "title": "good study but lack of novelty ",
            "review": "This paper focuses on federated learning in which the goal is to train/build a core/centralized model that can effectively aggregate models from various clients where each client accesses a different dataset, without forgetting the core model. In order to accomplish this, they propose to use a form of knowledge distillation. In addition, they utilize ensemble and copies of past edge models together to further improve core model performance.\n\nWhile I found this paper is easy to follow and understand, it lacks novelty in the proposed approach. Specifically, the core idea of this paper is to utilize knowledge distillation between core and edge models without proposing new ways for knowledge distillation or improving the current knowledge distillation techniques. Moreover, as mentioned in the paper, cloned distillation has been well-studied before, like Xie et al. (2020),  which is the main contribution of this paper. \n\nFurthermore, CIFAR-100 has been used for the experiments in this paper which seems to be far from a real-world scenario considering the fact that FDL is very important in real-world applications and the authors assume each edge device has access to the same amount of data with the same quality and same sample size. Since this is an application paper and if the goal is to showcase the performance of this model with this kind of dataset, then large-scale evaluations and experiments should have been done with more realistic assumptions such as i.e. more edges, larger datasets, more realistic lagged edge model, etc. \n\nAnother important issue with this work is that it assumes that an edge device is as powerful as the core device in terms of computation (i.e. edge devices use the same network architecture as the core morel plus ensemble) which I found it another unrealistic assumption in this work.  My understanding is that in federated learning, the core model should use a very powerful model but at the same time, an edge device can only be able to handle a small and compact model\n\nIn summary, I don't find this paper to be a good fit for ICLR given the above reasons. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Idea is not new and empirical results are not strong.",
            "review": "#### Summary:\nThis paper proposes an asynchronous communications method with knowledge distillation on the task of federated learning. The method elastically handle the dynamic inflow and outflow of users, and adopt a concensus score for evaluate the learned quality to avoid noisy data at edge. \n\n#### Pros:\n1） The method is shown to be more effective than independent KD learning, and the experiments show that the proposed method is comparably more reliable. \n\n2） The temporal memory also makes sense by considering the trained model in the past. \n\n#### Cons:\n\n1) The novelty in this paper is limited,  sequential adding learning instance for federated learning was proposed in previous works such as FedFMC: Sequential Efficient Federated Learning on Non-iid Data. And sending gradient or weight back without knowledge is also very common in this field. The method needs to have more comparison, and in its related works, I would suggest the author to have more complete overview of federated learning, e.g. as discussed in Federated Machine Learning: Concept and Applications, and better distinguish the contributions.  \n\n2) The experimental scale is too small, only 9 clients with one core, which has limited ability to show its generalization ability. \n\nThe method has not compare against other SoTA algorithm on large-scale federated learning that is truly demonstrating its usage: e.g. Bonawitz et.al TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN, especially on real applications such as on-device item ranking or next word prediction. \n\nTherefore, it lacks a standard benchmark to understand the advancement of the proposed method. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Asynchronous Edge Learning using Cloned Knowledge Distillation\"",
            "review": "--------------------------------------------------------------------------------------------------------------------------------\nSummary:\n\nThis paper proposes a Federated Learning (FL) method. The goal of FL is to overcome the challenge of performing machine learning using vast amounts of decentralized data mainly for the purpose of preserving client privacy. The method in this paper is developed for the case where limited communication is available with clients and clients have a high inflow and outflow rate. Under this setting, the goal of this paper is to extract as much information from a given client as possible while defeating catastrophic forgetting. The paper proposes an asynchronous edge learning algorithm with knowledge distillation to enable this. The authors show that the ResNet-32 architecture achieves better accuracies on the CIFAR100 dataset when the proposed FL method is used compared with some baselines.\n\n--------------------------------------------------------------------------------------------------------------------------------\nOverall assessment:\n\nI am not an expert in this field, but I think this is a good paper. The studied problem is important and is gaining interest in the field. The method proposed in this paper is applicable to scenarios in which not only a limited number of clients are available (for example startups) but also it is not possible to have constant communication with clients for multiple rounds of model update. This is a realistic and reasonable assumption e.g. for clients that have limited resources. Another case where this assumption holds is when there is a high inflow and outflow of clients. The paper is well structured and written clearly. The paper proposes a knowledge distillation technique to transfer knowledge between a core model and models trained on client data. The main idea of the knowledge distillation technique is to train the client models using a pre-trained core model trained on a pre-prepared core dataset as opposed to the conventional techniques that train the client models from scratch using independent initializations. This proposed knowledge distillation model also addresses the problem of catastrophic forgetting. The presented evaluations, although done on a single dataset, seem comprehensive and satisfactory.\n\n--------------------------------------------------------------------------------------------------------------------------------\nPros:\n\n(1) This paper addresses an important problem and proposes a solution for it under a realistic scenario.\n\n(2) The paper is well-structured and well written\n\n(3) The proposed method is evaluated on the CIFAR100 dataset for ResNet-32. The authors evaluate their proposed method under several settings and show that their proposed knowledge distillation method is better compared with a knowledge distillation technique in which the client models are trained from scratch. They also present robustness to noise and potential malignant client behavior. The experiments look convincing to me.\n\n\n--------------------------------------------------------------------------------------------------------------------------------\nCons:\n\nI didn’t find any specific cons for this paper. I just have a few questions that I’ll list below\n\n\n--------------------------------------------------------------------------------------------------------------------------------\nQuestions:\n\n(1) In the experiments, if I understand correctly, CIFAR-100 is split such that all the classes are available in all subsets of the data. In this case, is it true that the proposed method is applicable to cases where the client data is similar to the core dataset?\n\n(2) Following up on the previous question, how would the proposed model perform if the client data has distribution shifts compared to the core data?\n\n(3) I think that perhaps one of the reasons for the gains observed in the performance of the cloned KD models and the independently initialized models is that the data distribution is very similar across clients and the core dataset. Is this a correct assessment? \n\n(4) If the client data is far from the core data, why would enforcing the teacher model to start from the pre-trained model be more desired?\n\n--------------------------------------------------------------------------------------------------------------------------------\nMinor comments and typos: \n\n(1) The entry names in Fig 5 (left) are very hard to interpret and remember.\n\n(2) Fix two instances of “researches” in intro.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}