{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The idea presented in the paper is interesting and has caught the attention of the reviewers. However there seem to be only a tepid support for acceptance with a reviewer championing rejection. \nThere is little novelty in the approach but empirical validation shows results that consistently improve over selected baselines. I am afraid that more evaluations would be needed at this stage to consider this work for acceptance."
    },
    "Reviews": [
        {
            "title": "A radical new architecture, but unfortunately the actual design is not fully convincing and based on strong theoretical foundations.",
            "review": "\nThe paper discusses a model for learning the architecure of a convolutional networks starting from a fully connected graph. The idea is to learn the adjacency graph of the model together with the weights of the networks.\n\nStrenghts:\n\n- The idea of thinking out-of-the-box by imagining new architectures is very attractive and interesting.\n\nWeaknesses:\n\n- The actual advantages in the model do not look apparent given the results in the evaluation section.\n\n- The theoretical foundation of this work is unclear. For example, it is unclear how the proposed solution will work in practice in terms of back-propagation.\n\n- The performance results show that the proposed method is characterized by performance close to those of existing methods.\n\n\nIn general, I really welcome this type of work: non-conventional, experimental and quite radical in terms of approach. However, unfortunately, the authors do not provide a convincing description of their approach. Unfortunately, it does not appear that the method is developed on a sufficiently strong theoretical basis. For example, it is unclear how back-propagation work in these circumstances when you don't have a stacked architecture.\n\nThe choice of the thresholds and the actual learning of the adjacency matrix is not described in sufficient detail.\n\nThe actual computation complexity and the trade-offs in terms of computational complexity/accuracy is unclear.\n\nIn the experimental results, the actual performance of the method appear very similar to the other methods. In some cases they might be the same since the confidence intervals are overlapping.\n\nQuestions:\n\n- What is the theoretical foundation of the method? Given the fully connected nature of the graph, how does back-propagation work in this case? In a sense, in fact, you have a DAG, how do you deal with cycles? When you do you stop the back-propagation if you do not have a stacked architecture?\n\n- Could you please provide the confidence intervals for all the results you presented? In fact, it seems that the values related to your approach are better than existing techniques in some cases but they look very close?\n\n- Is the computational complexity justified? Also note: you probably need a larger number of samples to learn the additional adjacency graph. What is the trade-off? Given the gain in terms of performance, the actual additional complexity might not be completely justified.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea, but limited novelty and experimental validation",
            "review": "This work proposes a novel method, called Dynamic Graph Network (DG-Net), for optimizing the architecture of a neural network. Building on the previous work introduced by (Xie et al., 2019), the authors propose to consider the network as a complete directed acyclic graph (DAG). Then, the edge weights of the DAG are generated dynamically for each input of the network. At each node of the network, the authors introduce an extra-module, called router, to estimate the edge weights as function of the input features.\n\nThe proposed method addresses the problem of optimizing the connectivity of neural networks in an interesting way, where the architecture is not fixed but it depends on the input instances. Moreover, I think that a strong advantage of the proposed technique is that the optimization of the architecture comes with a negligible extra cost both in terms of parameters and computational complexity. Overall, the paper is well written and easy to follow.\n\nMy only serious concern is the degree of novelty with respect to (Yuan et al., 2020), which was published at ECCV 2020. The main difference seems to be that in the proposed method the graph is dynamic (i.e., it depends on the input instances), instead  in (Yuan et al., 2018) the graph is learned but fixed for all the input samples. In the experimental results, I would have expected a deeper ablation study on the importance of the dynamic graph, since this is the main contribution of the paper. Instead, there is only one experiment in the appendix (Table 6). Therefore, the impact of the dynamic graph in the performance of the proposed method is not clear and it is difficult to evaluate the importance of this contribution.\n\nOther comments:\n\n- In Sec. 3.1 the authors say that the ResNet architecture can be represented with a DAG where the set of edges is defined as E={(i,j)|j=i+1,i+2}. This is not true: if you unroll the definition of the ResNet architecture, as done in Eq. (4)-(6) in [1], and compare it with what you obtain using Eq. (1), it is easy to see that the two resulting functions are different.\n\n- The definition of the convolutional block is not clear, is it a ReLU-conv-BN triplet as in (Xie et al., 2019)? \n\n- The use of a DAG with edge weights for representing the architecture is not novel, it was already introduced in (Xie et al., 2019).\n\n- In Sec. 4.3, Table 5 shows a comparison with state-of-the-art NAS-based methods. DG-Net is implemented using RegNet-X and RegNet-Y as the basic architecture, however in Table 5 the performance of the basic architectures (without the dynamic graph optimization) is not reported, this would be useful to evaluate the gain provided by the optimization of the architecture.  \n\n\n[1] Veit et al., Residual Networks Behave like Ensembles of Relatively Shallow Networks, NIPS 2016\n\n###############################################################################################\n\nAfter the discussion period:\n\nI thank the authors for their responses and for updating the paper. The authors have added a deeper analysis on the impact of the dynamic graph, however I still believe that the novelty of the paper is a bit limited. I have slightly increased my score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting extension of randomly wired architectures",
            "review": "Pros:\n- Interesting extension of RandWire to learn better architectures\n- Good experimental results\n\nCons:\n- Idea could be seen as minor modification of RandWire\n- Doubts about memory requirements\n\nThe paper proposes an improve over the idea of randomly wired architectures [1] by exploiting a complete graph where edges are weighted by dynamically computed weights. Dynamically computing edge weights allows the network to optimize its topology. While this idea could be seen as a relatively small modification of [1], it is still very interesting and the results prove its effectiveness in commonly used tasks.\n\nIt would have been interesting to show an analysis of what the method is actually learning, e.g. if a particular topology emerges from the adjacency matrix, in order to provide more insights on why the method is effective.\n\nI also have a concern about memory requirements that I would like the authors to address. It is true that DG-Net does not require significantly more FLOPS or trainable parameters. However, you use a complete graph which means that it requires to store in memory O(L^2) activation tensors, i.e. the activation of each convolutional layer weighted by edge weight. This could become a limitation for larger networks. Note that this is also a problem for [1] but, in that case, the sparse connectivity mitigates the issue. \n\n\n[1] Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He.  Exploring randomly wired neural networks for image recognition.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing related work",
            "review": "This paper presents a novel approach (DG-Net) to “generate” a dynamic structure for the neural network, by learning to predict and select the edges between computational nodes in an end-to-end manner. The method is based on a gating mechanism, applied on top of a fully connected graph (similar to the connectivity in a DenseNet), designed to control the quantity of information received from each previous layer. The experiments show consistent improvement in image classification (ImageNet) and object detection (COCO).\n\nPro:\n- I enjoy the interpretation of the “weighted edges” as a dynamic architecture, able not only to address a more general class of models but also to adapt to each input accordingly (using a second-order approach).\n- The method is simple, yet effective with the results on both tasks showing improvement at a low computational cost.\n- The paper is clearly written and easy to follow and understand.\n\nCons:\n- In my opinion, two approaches are very related to this paper: the Highway network [1] and the ablations performed in the ResNet paper [2]. In both cases, the intuition is a little bit different: improve the expressiveness by replacing the residual connection with something more powerful - an input-dependent gate. While I agree that this paper offers a more general framework, working on a densely connected graph instead of a limited subset of residual edges, (plus some other minor technical differences: map gates vs scalar gates), the connection to those papers should be clearly discussed in the related work. Also, it is interesting to notice that in [2] this kind of gating decreases the performance compared to the simpler summation, contrary to what we see in DG-Net.\n- This idea suggests an additional ablation study: constant-gating instead of dynamic ones (by setting the alpha scalar equals to 1). This is related to the static experiment in the Supplementary material, but more closely to the DenseNet architecture and it would clarify that the improvements come indeed from the learned structure and not from the ability to combine features from different levels.\n- It is not clear from the paper how the ResNet architecture is adapted to the dynamic setup. I guess the graph is fully connected on each stage (as long as the spatial dimension is preserved), but since the paragraph about the “multi-stage” architectures comes only in section 3.1, that speaks about classical architectures, not the DG-Net, it is not very clear what densely connected means in the experiments.\n- Is any particular initialisation scheme used for the routers? Could it be important for the optimization to start with some parameters that lead to the original version of the model (initialize the parameters such that the residual connections start with alpha=1 and non-existent connections are close to 0)?  For example in [3] all the skip connections are initialised such that the extra module is ignored in the first iterations.\n\nMinors and observations:\n- Even if the focus of this work is the dynamic of the edges, it would be interesting to observe if the architecture learns to use all the computational nodes and only connect them differently than what we used to do, or if it rather prefers to drop nodes, thus using a lighter model in the end. (maybe a regularization term that encourages that would be useful for computational efficiency).  - A statistical analysis in this direction could reveal interesting ideas.\n- The formatting of Table 1 is hard to follow. At least a vertical line between Baselines and DG-Net would improve the readability.\n\n[1] Highway Networks, Srivastava et. al\n[2] Identity mappings in deep residual networks, He et. al, ECCV 2016\n[3] Non-local Neural Networks, Wang et. al, CVPR 2018\n\nMy main concern regards the connection to previous works that use gating to aggregate information from previous layers, which I see very related to the current work. However, the proposed method is more general and seeing these gates as a routing mechanism that allows the model to learn its own structure in a differentiable way is interesting and could lead to further, more advanced ways of doing this. So, with a clearer discussion of those previous methods and how the current approach differs from them, I lean towards acceptance.\n\n############# UPDATE #############\n\nI thank the authors for their response and for updating the manuscript according to our questions. I agree with the other reviewers that the novelty of this paper is quite limited. However, the idea of using a dynamic, learned graph from a general large search space of models is interesting, provides good empirical results on both image classification and object detection and the authors provide ablation for the new components that motivate the paper. I maintain my initial score: 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}