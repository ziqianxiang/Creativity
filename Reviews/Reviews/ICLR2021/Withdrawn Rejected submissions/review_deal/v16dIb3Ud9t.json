{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "In order to tackle the issue of efficient exploration, the authors propose a self-supervised method for exploration, which maximize multisensory incongruity. The metrics include perception incongruity and action incongruity. There's an alignment predictor to produce perception incongruity. Different modality inputs go through sensory-wise dropout and measure the action incongruity. The authors evaluate their proposed method in OpenAI Robotics and Atari domains to show that they can improve exploration.\n\nPros:\n+ Experiments are very extensive, which cover OpenAI Robotics and Atari, experimenting with learning only with intrinsic rewards, combining the proposed intrinsic rewards with extrinsic reward, and combination of the proposed intrinsic rewards with other intrinsic rewards in the literatures;\n+ The proposed method uses a simple way to generate intrinsic rewards which can be generally plugged in different multi-modal environments;\n\nCons:\n- The proposed method should be hard exploration games like Montezuma’s Revenge in Atari, in addition to the already evaluated games in the paper;\n- There are quite significant lines of work on exploration and intrinsic reward should be compared with, for example, \"Exploration by Random Network Distillation\";\n- Personally, the reviewer feels that the paper uses combinations of the state-of-the-art techniques like self-supervised learning and contrast learning to see whether these techniques can improve RL performance, rather than, motivated from the RL exploration issue then started to tackle the issue. It leads the paper into a quite empirical (doing extensive experiments) but not much on why it works better. The authors should investigate whether it's due to auxiliary tasks training (like UNREAL paper) or it's due to dealing with exploration;\n- The proposed method is limited to multi-modal inputs to RL agents, which might not be common case.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but not properly executed",
            "review": "This paper introduces SEMI (Self supervised exploration via multisensory incongruity) a new algorithm that leverages multisensory incongruity to drive exploration. The algorithm derives a reward from multisensory perceptual incongruity and action incongruity that is then optimized by a reinforcement learning algorithm. Experiments on OpenAI robotics and the Atari 2600 games are provided to show the validity of the approach.\n\nI really liked the idea of using multi sensory data to drive exploration. Data like sound is often ignored by current algorithm and it is interesting to see how it can be leverage to build better algorithm.\n\nHowever I was overall a bit confused by the paper and wasn't able to fully understand it. My main is issue is due to the fact that incongruity between multisensory streams is never defined, this makes it hard for me to understand the link between multisensory incongruity and exploration. It is stated several times that “synchrony of multiple senses is a fundamental property of natural event perception”, however I do not understand how this relates to exploration. For example it is said\n“we humans are extremely sensitive to the incongruity between them, which is a strong signal of novelty”\nI do not understand the direct link between sensory incongruity and novelty.\nSame with\n“Congruity in actions is inspired from the fact that human perception is robust to the partly loss of senses, and humans have an exceptional ability to compensate for the loss with other senses.”\nWhich I understand and agree with, it is then directly followed by\n“Therefore in the setting of robot exploration, we use the incongruity with drop of senses as an indicator of novelty.”\nI also had a hard time understanding the complete algorithm through text, adding it in pseudo-code would be helpful.\nRegarding experiments more informations should be provided on the set of Atari games chosen. It makes sense to drop games without sound however I do not understand why games with background music are ignored. Currently it looks like games where SEMI performs well have been cherry picked. After stating that incongruity between senses is a signal of novelty it would have interesting to observe the performance of SEMI when video and sound do not match. We would hope that the algorithm would then ignore the sound and does not perform worse than an agent not doing any exploration but this is not discussed in the paper.\nThe appeal of SEMI becomes limited if it applies only to a few environment.\n\nOverall I did not see enough evidence to support the validity of the algorithm proposed, for this reason I recommend rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "An important problem, but execution could be better",
            "review": "**Summary**:\n\nThis work presents a self-supervised exploration method that makes use of multi-sensory, multimodal observations in RL. Specifically, (multisensory) perceptual and action incongruity are used as intrinsic rewards for exploration, which capture the uncertainty in multimodal model or the agent’s policy.\n\nThe alignment predictor measures misalignment across different modalities via contrastive learning, and its prediction error (called perceptual incongruity) is used as an intrinsic reward signal. The variance of actions from the policy network given different fusion/combinations of multimodal inputs, called action incongruity, also constitutes the intrinsic reward. The resulting algorithm SEMI is shown to make the agent explore better than some curiosity-like baselines.\n\n**Strengths**:\n\nThis work is well placed around the recent advances of contrastive representation learning in RL. Learning efficient multimodal representations would be one of the very significant and important research directions in RL these days.\n\nThe modal-wise dropout strategy is an interesting and novel idea.\n\nThe paper is in general easy to follow. I also like how Figure 1 was drawn, which provides a good overview of the key concept of the paper.\n\n\n**Weaknesses**\n\n(i) It is not clear or less explained why the action/perceptual incongruity can derive a good intrinsic reward signal of “novelty”. The paper is overall easy to follow, but the overall clarity (especially on the method section) could be improved.\n\n(ii) The core idea and the mathematical formulation on the multimodal contrastive learning (section 3.2) is exactly the same as Contrastive Multiview Coding (CMC), which the authors did not cite or compare. A novelty/contribution of this work would be to use such techniques for intrinsic-reward exploration, but the significance and novelty is a bit limited.\n\n(iii) Technical soundness: It is not convincing whether the action incongruity (the variance of actions across different combinations of multisensory inputs) might derive a useful exploration signal. (Please refer to the comments below)\n\n(iv) I do not think the multimodal setting in robotics control is convincing enough. RGB/Depth information are possible modalities, but only there are only two. It would have been much more significant if the modality consisted of proprioceptive, touch information.\n\n(v) Overall, the experiment studies the method on continuous and discrete environments, but the choice of environments is limited. I do not think the evaluation setup is strong enough (see below comments). Besides, on Atari, due to the nature of the environment, multimodal information might not help as much -- the benefit of audio signal might be not significant enough to derive a novel exploration signal. Also, qualitative examples and additional analysis (e.g. Appendix A.4) do not seem comprehensive enough.\n\n\n**Detailed/Additional comments**:\n\nTechnical soundness about the action variance: In most of the cases, the number of modalities are limited; for example, M=2 in the experiments which gives only 3 possible inputs in total. Is it enough for measuring variances? Also, in many cases it is likely that the policy is ignoring one modality and using another to make a decision despite the uncertainty. However, no in-depth justification was made. Also, how to measure the action variance in Atari domains?\n\nEmpirically, the paper does not have a diverse set of environments for evaluation. For continuous control, only the FetchPush environment was considered, and there are only 3-4 environments reported from Atari. Experiments like Section 4.3/Figure 7 need to be evaluated on all sets of environments but only one was provided. Also, the experiment should focus on “success rate” or learning control ability in Fetch environments rather than interaction rate --- I doubt this is a valid metric to measure the exploration quality.\n\nSome ablation studies are missing, such as balancing between perceptual incongruity and action incongruity. How critical is this balancing hyperparameter? From Table 1, I see SEMI(P) nearly approaches SEMI(PA) in performance, so my understanding is that the action incongruity itself is not contributing as much. What would be the performance of SEMI(A) where the action incongruity is used solely? \n\n\nTo me, the meaning of “alignment” in the “alignment predictor” sounds a bit unclear. It sounded like temporal alignment at first, but to my understanding, this appears to mean agreement between two multimodal observations.\n\nAtari: The details of the policy network is missing. Question: 4 hidden layers are used, but this looks deeper than usual. Also, why does the action space consist of 12 actions (in ALE, it is usually 18)?\n\n\n**Overall recommendation**\n\nOverall, the biggest weaknesses of the paper are the experiment and clarity/justification of the method. Although the proposed method achieves a better performance compared to the baseline, the paper would have benefited by having more diverse environments and more careful analysis.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting work, some missing details",
            "review": "This paper presents a method for intrinsic exploration using perception and action alignment to generate reward signals. The paper addresses the interesting problem of exploration using intrinsic reward and shows with experimental evaluations that intrinsic rewards improve learning in several domains.\nThe paper is well organised and clearly written, the proposed method is well framed within the relevant literature and the contribution is clearly presented.\nExperimental results support the claims pushed forward and demonstrate the advantages of using the proposed method.\n\nComments:\n- Are there notable differences (qualitatively) between behaviours obtained from extrinsic rewarded and from extrinsic+intrinsic rewarded agents?\n- Why are modalities fused in different ways for current and next step (referring to Fig. 1: for current observation \"mean\" is used, while for next observations fusion is realised with eq. 3)? Does this affect the representation?\n- Does the experiment on the robot arm involve continuous actions? If not, would your method work with continuous actions?\n- What is the main limitation of your proposed method?\n- What dropout rate is bearable by your method? Is there a limit to modality dropout to keep learning stable? How big is the effect of dropout during training? Do you have ablations on this?\n- How do you modulate/choose the weight \\gamma? What is the effect of tuning this parameter to higher/lower values? Do you have ablations on this?\n- Similar to previous question: what is the effect of choosing higher/lower values for \\beta for mixing different rewards?\n- How is r^a affected by the different sensor modalities? Does each of the three modalities contribute equally to determine var{a_t+1} or are some modality irrelevant?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}