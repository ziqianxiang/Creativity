{
    "Decision": "",
    "Reviews": [
        {
            "title": "Motivation is not clear",
            "review": "This paper proposes a novel architecture for vision-language pre-training to align cross-modal semantics at both high and low levels. This is motivated by the observations that the image descriptions could be either abstract (low-level semantics)  or concrete (high-level semantics). The authors achieve this goal with a shared Transformer that is trained iteratively for both single-stream and double-stream processing of image and text. A pluggable cross-modal attention is used at higher-level layers in two-stream cases. \n\nIn general, the paper is well written and easy to follow. The idea is clearly demonstrated. The authors have shown the effectiveness of this pre-training scheme by fine-tuning on four down-stream VL understanding tasks and achieved comparable results to prior works. Ablation study also shows the effectiveness of iterative training rather than single-stream or two-stream training alone. \n\nMy major concern is about the motivation. I do not fully understand the so-called high-level semantics and low-level semantics. From the examples in Fig. 1, it seems the authors refer to high-level semantics as image concepts of larger regions and low-level semantics as image concepts of smaller regions. Such scale variations is a challenge for object detection. However, in most VLP works, the object detector is pre-trained and fixed. The goal of the object detector is to extract region features that are hopefully scale-invariant. In VLP, it is essential to align the visual concepts with the text. First, I am not sure if it is necessary to separate the so-called high-level and low-level semantics. Second, I am not convinced that single-stream is to align low-level semantics while two-stream is to align high-level semantics. In fact, as analyzed in this paper (Behind the Scene: Revealing the Secrets of\nPre-trained Vision-and-Language Models), single-stream attentions also show some pattern of two-stream where some layers only have single-modality attentions. It is likely that single-stream also aligns the so-called high-level and low-level semantics at higher and lower layers of the Transformer. \n\nFrom another perspective to look at the framework itself, the current presentation of Figure 2 is quite confusing. It took me a while to understand this figure after reading all the text. Another way to present this work is to show the same architecture, but instead vary the attention mask alternatively. For the single-stream case, it will be cross-modality attention for all layers. For the two-stream case, it will be single-modality self-attention in lower layers (image can only attend to image, text can only attend to text) and cross-modality attention for higher layers. \n\nEven when presented in the way I mentioned above. It is not clear how this kind of pre-training could be helpful. I would only think of it from a regularization or augmentation perspective. \n\nFurther, most of the experimental results are not surpassing state-of-the-art methods. In some cases when it does, the performance gain is quite marginal. Therefore, it is hard to believe the effectiveness of this framework. \n\nBased on the above reasoning, I suggest to reject this paper. \n\nQuestions for the authors:\nI would like to know if the method can be structured in the way I described above. Besides, are there ways to validate your assumptions? For example, if only trained in the single-stream or two-stream, can we verify that the single-stream model is doing better in aligning the low-level semantics while the two-stream model is good at high-level semantics? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A unified vision-language pre-trained model combining single-stream and two-steam architectures",
            "review": "This paper proposed a vision-language pre-trained model, which uses a single-stream Transformer architecture and a two-stream Transformer architecture (with a cross-modal attention module) to learn low-level and high-level semantic alignments of image and text representations, respectively. Comparing to previous VL pre-trained models, the pre-training tasks used in the proposed model are not new, but it introduces a new pluggable cross-modal attention module to learn high-level semantic alignments, which are integrated with the Transformer blocks at top levels. The evaluation results on V+L understanding tasks like VQA 2.0, NLVR2, GQA and image-text retrieval demonstrate the effectiveness of the proposed model.\n\nStrengths: This paper combines two typical VL pre-training architectures (i.e. single-stream and two-stream) into a unified framework, and proposes a shared Transformer-based encoder with a pluggable cross-attention module to bridge the semantic gap between two modalities and learn representations of images and texts with different semantic granularities. The ablation study verifies that the learned low-level and high-level semantic alignments play important roles in various VL downstream tasks. The paper is well written and experiment results are comprehensive. \n\nWeaknesses: (1) The main idea of this paper is introducing a cross-modal attention module at the upper parts of transformer blocks to learn high-level semantic alignments between images and texts. This module increases the amount of parameters, which makes the comparison between the proposed method and \nexisting VLP models unfair. I suggest the paper can provide the model size comparison in the result table. (2) Based on Table 1, it seems that the proposed method cannot outperform ERNIE-ViL-base. I suggest the paper can provide some analysis on this. (3) Important details are missing. How was the model finetuned on downstream tasks? Are the low-level semantic alignments and high-level semantic alignments maintained during the finetuning stage or only one of them? \n\n- Additional question. Is it possible to apply the proposed model to single modality tasks, such as image captioning?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors proposed a method for vision-language pre-training (VLP) that combines two existing training regimens via training iteratively. I am leaning towards rejection because of the lack of novelty. ",
            "review": "**[Summary]**\n\nThe authors proposed a method for vision-language pre-training (VLP). In the past, there were mainly two ways of performing VLP: (1) concatenate image representation and text representation at a feature level as input to a single-stream Transformer, or (2) use a two-stream cross-modal Transformer to align the image-text representation at the higher level. The proposed SemVLP is very straightforward. It combines the two training regimens via training iteratively, i.e., (1) --> (2) --> (1) --> (2) ... The Transformer network is shared between the two training regimens to leverage the best of both worlds. \n\n**[Pros]**\n- The writing is easy to understand. \n- Ablation study shows combining the two previous training methods indeed improves the performance.\n\n**[Cons/Questions]**\n- Lack of novelty. It seems to me that the novelty of this proposed work is simply put two types of training VLP methods together by 50% of time training with one method and 50% of time training with another. \n- The authors state that 50% of the time, the network is trained with either low- or high-level semantic alignment. Why iteratively pre-train, instead of actual \"jointly\" pre-train for both low-level and high-level semantic alignments? Was memory consumption the issue preventing the authors from doing so?\n- In P.2, \"tying all the parameters of self-attention module and FFN layer,\" what do the authors mean by \"tying\" here? \n- The submission is over eight pages. \n- I am hoping to see that the authors can present any other particular benefits of SemVLP, besides performance improvements. For example, how long was the training? Did it take shorter to train compared to the existing methods? \n\n**[Reason for rating]**\nThe proposed method indeed seems to be superior compared to the existing work with a similar network size (using \"base\"). My major concern and the reason for the rating is the novelty of the proposed method. There does not seem to be any particular part of the method that is particularly novel. This lack of novelty could be fine, if there are other insights bring to the table, but there is no such discussion as well. I am thus leaving my rating to 4, i.e., leaning towards a rejection. \n\n**[Minor comments]**\n- The font size is too small for Figure 2.\n- P. 2, \"we introduce a novel two-stream\": I am not sure why this two-stream Transformer network is novel.\n- The abbreviation FFN is used without formally introduce its original form.\n- Please re-format Table 1 to 3 within the text width.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiment is insufficient ",
            "review": "This paper proposes a V+L pretraining framework, called SemVLP, to align cross-modal semantics at multiple levels. The idea is simple and straightforward -- by iteratively pretraining the model to align semantics at low- and high-levels, the pre-trained model will be more robust toward diverse image-text pairs. Experiments show that the proposed method achieves comparable performance to previous SOTA methods on VQA and NLVR2. Yet experimental results on more tasks are needed to demonstrate the advantage and generality of the proposed method.\n\nStrengths:\n+ The method is simple and easy to understand.\n+ Comparison experiments show its effectiveness on VQA (GQA) and NLVR2.\n+ Ablation study shows the importance of iterative pretraining.\n\nWeaknesses:\n- Experimental results on multimodal tasks are not rich enough. It cannot be seen from the shown results that the proposed method works for a variety of V+L tasks.\n- It is suggested to show the performance of SemVLP-large, for both completeness and fairness of comparison against the listed large models.  \n- In section 2 SemVLP Pre-training, a description of semantic alignment and pretraining strategy can be more detailed, to help the reader understand Figure 2.\n\nOverall, the SemVLP framework seems to be an effective VLP method. Yet the effectiveness needs to be confirmed with more experimental results. If the aforementioned concerns are addressed clearly in the rebuttal, I am willing to adjust my rating.\n\nQuestion:\nIn the left part of Figure 2, the Cross-modal attention module $\\varphi^l$ takes $h^L$ as input. How is the output of the last layer passed back to intermediate layers? Or is it a typo?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}