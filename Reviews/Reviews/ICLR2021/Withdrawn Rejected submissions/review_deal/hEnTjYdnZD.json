{
    "Decision": "",
    "Reviews": [
        {
            "title": "An ok paper, but nothing special.",
            "review": "This paper proposes modifying the activation function of a ReLU/PReLU so that it includes clipping. The motivation is that the clipping restricts the range of the PReLU, and therefore one can use a finite-range quantizer without worrying about saturation of the quantizer (effectively the saturation level is learned and enforced by the network). The paper goes through a lot of mathematical gymnastics to prove that this modification does not hamper the performance of the network. \n\nThe authors further demonstrate the evident result that as the quantization bit-depth increases, the quantized version converges to the unquantized one. For some reason they assume a laplacian distribution in the input, although the result should really hold for any input distribution. When considering the quantization noise, the fact is that a trained network is bounded and has some Lipschitz constant. Since the quantization error decreases exponentially with the number of bits, the total error of the network, compared with the unquantized version, decreases.\n\nAs a side note, I should add that Figure 2 really suggests the sigmoid as a solution to this problem, as the figure is very similar to a piece-wise linear sigmoid approximation. The sigmoid has fallen out of favor because of very valid reasons, including vanishing gradients at infinity. The authors should comment on why this is not an issue in their approach.\n\nWhile the paper has some interesting nuggets, I don't see the benefits of this contribution. Even the experimental results show a very marginal improvement over existing methods, which might not even be statistically significant. Overall, I don't see something truly innovative or ground-breaking in the paper, but I could be convinced otherwise.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice reading, however of limited interest",
            "review": "This paper proposes a ReLU variant called BCPReLU; the authors propose a theoretical anlysis of its properties and experimentally evaluate the proposed nonlinearity in a quantized activation function scenario over a few different datsets and network architectures.\nThe topic adderessed by the paper (quantized activation functions) is timely and fit for this venue, the paper is well organized and well written, the theoretical analysis is appreciable.\nHowever, this work ha a few glitches that prevent me from recommending for publication.\nFirst, the proposed BCPReLU can be seen as a generalization of the so-called HardTanH: the authors fail however to acknowledge such similarity. The proposed nonlinearity shall be compared with the HardTanH at least from an experimental percpective.\nSecond, the experimental evaluation is lacking in some extents. First, it is defined as extensive in the conclusions, however it just considers two ResNet sizes and two small datasets (CIFAR and SVHN): this is far from an extensive experimental evaluation. The results with the full precision activation in Fig 3 show no gain over the standard PReLU despite the increased complexity. The experiments with the quantized network in Tables 1 and 2 show some gains, however such gains are quite limited.\nAll in all, this paper was a nice reading, however I doubt the level of novelty and the experimental results are such to draw interest from the audience of this conference. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Incremental improvement with limited experiments",
            "review": "This paper proposes an interesting improvement on the bounded Relu activation function, whose slope and bounds are trainable parameters. This approach is supposed to be equally expressive as the unbounded ones, and to be more robust in quantized models. The paper is very easy to follow and the approach is presented in a concise fashion. My major concern is twofold with respect to the experiments: i) the choice of model architectures and datasets seems quite limited: there are practically only two models tuned on datasets. It is thus difficult to claim that the proposed activation function can generalize. ii) The authors provided results of training quantized models. In practice, however, it is more often to quantize a trained model and evaluate its performance. The authors might want to also include experiments thereof. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Adding LPreLU to 2-sided clamped activations",
            "review": "Recent work (e.g. PACT) on training quantized models has proposed clamping the (slope-1, -0-clamped ReLU) activation function from the right. However, previous work for non-quantized models has argued that LPreLU  (slope estimated by training, clamped at -$\\mu$/+$\\alpha$) may be a superior activation function. This work proposes therefore that a 2-sided clamped version of LPreLU (instead of ReLU) be used as the activation function of choice for training quantized models.\n\nThe conceptual advanced is very small here, so the paper really lives and dies by its empirical results. I found it hard to get excited by the empirical results because unlike the standard in this field, (a) it does not show numbers for imagenet (b) it does not show 1-bit numbers (c) even the advantages on CIFAR compared to an old SOTA (PACT) seem minimal (d) Although the theoretical analysis seems to say that the advantage of the technique may be in faster convergence, the author's don't provide measurements of convergence speed. Note that the state of the art for imagenet is already very impressive now, see e.g. ReactNet:\nhttps://arxiv.org/pdf/2003.03488.pdf\n\nGiven the simplicity of the technique, unless the authors are able to show noteworthy empirical speedup, I don't see this work clearing the bar for conference acceptance.\n ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}