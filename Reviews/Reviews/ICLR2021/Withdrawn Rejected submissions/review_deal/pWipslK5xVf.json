{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper proposes a potentially very interesting and original approach to handle label noise.  The numerical experiments suggest that the method works very well. But the paper  itself has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and  Markov theory. \n\nNote that due to the low confidence in several review an additional emergency review by an expert was asked and it confirmed the global opinion from  other reviewers that the paper is interesting but needs a major rewriting before acceptance in a ML conference. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation of the method before resubmitting.  "
    },
    "Reviews": [
        {
            "title": "A good idea but the paper needs to be extremely more polished",
            "review": "\nThis paper aims to deal with the learning of noisy/corrupted labels based on the small loss criterion. If I understood well the idea is to consider a new loss function on the Wasserstein space to learn the certain and uncertain data distributions. This loss function is based on kind of penalty term ensuring that the uncertain labels lies in a Wasserstein ball for which the radius is automatically tuned to get the best possible result. This construction crucially relies on the use of the Wasserstein gradient flow associated with Gaussian distributions. To conclude, a series of experiments show that the new methodology proposed in the paper leads to state-of-art results.\n\nI think that the idea is nice and was very impressed by the numerical results reported by the authors. However, it took me a while to just understand the problem and the setting considered by the authors! My second main criticism concerns the theoretical results presented in this paper. While I think that it is important that methods are justified by rigorous results, the one presented in this paper are just not understandable by most people. Two of the main underlying issues in my opinion are that the notion and objects which are used in the paper are not introduced very much rigor (or even not at all) and the result lack of clarity. To be honest, I did not catch half of the sentences of the paper.\n\nI advise the authors to make a in-depth revision of their paper, introducing more carefully their method so it can be understand by a broader audience. One solution in my opinion, is to reduce the theoretical notion and results  to their strict minimum. I think that a lot of them are unnecessary for the introduction of the proposed methodology.   ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Obscure motivation of proposed objective ",
            "review": "Summary\n\nThe paper introduces a novel objective function by imposing geometric constraints on the logits of uncertain samples. The authors' approach is to map the distribution logits of uncertain samples onto the 2-Wasserstein ball centered on the measure of certain samples. To overcome the dilemma of selecting the ball radius, the authors propose a surrogate objective, namely Wasserstein Normalization. An SDE grad flow is proposed for solving the Wasserstein normalization. The paper also keeps the Gaussian parameters as moving average during training in light of batch normalization. The paper both theoretically and empirically validate their method.\n\nContributions\n\ni) Proposal of a novel Wassersterin Normalization objective for uncertain samples. \n\nii) Theoretically giving a verifiable upper bound of the constrain term.\n\niii) Extensive experiments on synthetic and real-world datasets.\n\nIssues:\n\ni) The motivation of normalizing the uncertain measures into the 2-Wasserstein ball of certain measure is unclear to me. And it seems irrelevant to the practical problem. Yes, I agree that we should prevent the over-parameterized network to overfit the uncertain samples since the majority of them are noisy samples (network tends to fit noisy samples slowly). Based on this observation, the idea of discarding/relabeled uncertain samples is widely adopted in this field. The paper's approach belongs to this category. But the motivation of Wasserstein Normalization for uncertain samples is weak. I cannot directly see its benefits. Imagining the network has high confidence for certain samples (which is generally true in practice), then the mean of gaussian $m$ is uniform categorical distribution.  The paper's approach is just injecting noise onto the network output of uncertain samples.\n\nii) Does the simulation of the SDE makes the training much slower? Since we need the calculate the grad flow for every single sample during every iteration of the inner loop. \n\niii) Could the authors better explain the dilemma of selecting the ball radius $\\epsilon$ in Sec 3? I cannot see why the benefits decrease when $\\epsilon\\approx 0$.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially a very good paper but writing could be drastically improved.",
            "review": "The paper is a contribution that aims at solving the label noise problem. In this setting, the labels are possibly corrupted, this yielding a potentially significant underperformance of a (neural network) classifier when minimizing the empirical risk. This problem is ubiquitous and important in real life scenarii. The paper builds on the idea of small loss criteria, which favors learning on certain samples in the beginning of the learning process, and gradually incorporate uncertain samples along iterations. The paper proposes a novel type of distributional normalization based on Wasserstein distance. It projects uncertain samples on a Wasserstein ball defined wrt. the certain samples. This process is done with a particle based stochastic dynamics, based on a Ornstein-Ulenbeck process. A theoretical Analysis is given, along with results on classical datasets in the symmetric noise setting, open noise and a real world dataset (clothing 1M), for which it achieves very good performances compared to state of the art competing methods. \n\nWhile the overall idea is interesting, and the results described in the paper seem impressive and state-of-the-art on many of the tasks considered, I believe that the paper is badly written and very hard to follow. In the reviewer’s opinion, the major problems arise from:\n - a lack of introduction on the label noise problem, and a more comprehensive overview of the so called small loss criterion and its rationales;\n - despite an apparently rigorous mathematical framework, there are a lot of vague assertions (see minor remarks) and arbitrary choices (two examples: certain and uncertain distributions are looked upon in the pre-softmax logins, why ? Is the continuity equation of proposition 7 the only choice to model \\mathcal{F} ? Some parts are really unclear (Why taking a detour ? what is the role of the Wasserstein moving average ? Is solving the SDE corresponding to the OU process the only solution to perform the normalization ? There are many algorithms in the literature that allows to estimate an OT mapping without solving this SDE)\n \nFinally, whereas symmetric noise is considered in the experiment, I wonder how much the method is amenable to the more realistic setting of asymmetric noise, which is more likely to occur in the real life scenarii.\n \nAs a conclusion, I think there is a lot of interesting ideas and material in this paper, but the writing should be improved and clarified. I am willing to revise my note positively given that those aspects are taken into account in a revision of the paper. \n\n\nMinor comments:\n- why taking the pre-softmax activations to build \\ mu and \\epsilon ?\n- some sentences are rather vague : ‘a functional that can manipulate \\mu to induce geometric properties...’ ‘the pivot measure can be represented as a mini-batch containing the best combination of certain samples’ ‘the other iteration ‘ of which process ?\n- Définition 1: ‘d’ is used for the distance symbol and the dimensions of the input signal\n- Why Eq. 7 is a proposition ? It is rather a modeling choice \n- Typo p8 CIAFAR-10 -> CIFAR-10\n\n### After response to authors\nI thank sincerely the authors for providing a detailed answer to my concerns. I changed my note to 6. I believe the paper is interesting and show strong empirical evidences that the method is worth considering. I am not giving a higher grade because in my opinion the writing of the paper could be signifcantly improved. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation to Accept",
            "review": "This paper propose a computationally efficient Wasserstein distributional normalization algorithm for accurate classification of noisy labels. An explicit upper bound for the Wasserstein-2 distance is derived and such a bound can be used as an estimator to determine if a network is over-parameterized. Empirical results on CIFAR-10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches. \n\nOverall, this paper is very well-written and easy to follow. The problem is well-motivated, the proofs looks solid, and the claims are well supported by experiments.\n\nI am positive with respect to acceptance of this paper. First, compare to previous works, the proposed method (WDN) is fully non-parametric, sufficiently leverage the batches of training datasets, and do not assume any prior knowledge on class dependency. Hence WDN is flexible and potential has a smaller generalization error (compare to existing method based on sample selection). Second, I would like to emphasize the technical quality of the paper.  The concentration results in this paper are by themselves quite  elegance.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Related work, clarity, and rationale.",
            "review": "Note: initial score was 4; updated to 5 after taking a brief look at the experimental sections; will check again later to take a closer look at the updated \"motivation\" part.\n\n---\n\n__0. disclaimer.__\nIt is possible that I failed to understand a significant portion of this manuscript; I had a very hard time trying to understand the notations and the writing overall. Please correct me if I am wrong.\n\n__1. paper summary.__\nThe paper aims to give a better way of handling the noisy label data, by providing a way to better utilize the information given by _uncertain_ (or high-loss) samples, which are often simply filtered out in previous works. The proposed method is as follows: (1) select the \"lowest-loss mini-batch\" among the _certain samples_ (i.e., low-loss) $Y$, with some empirical measure $\\nu$. (2) Construct an approximate optimal transport map $\\mathcal{F}$ from the measure of uncertain samples $\\mu$ to a Gaussian measure $\\mathcal{N}(\\mathbf{m}_\\nu,\\Sigma_\\nu)$ in the feature space, having the same first and second moments as $\\nu$. (3) Minimize the sum of losses on the certain samples and transported uncertain samples. Authors also introduce several techniques to circumvent\n\n__2. review summary.__\nThe proposed method seems to be advantageous over the considered baselines, but I am not sure if the method would outperform other baselines as well. Also, the clarity of the manuscript is not quite good (in my opinion).\n\n__3. missing baselines.__\nA wide range of algorithms has been proposed recently to make use of the estimated-to-be-mislabeled subset of samples (or uncertain sample, as authors put it) data. For instance, the ICLR 2020 paper by Li et al. ([\"DivideMix: Learning with noisy labels as semi-supervised learning\"](https://openreview.net/pdf?id=HJgExaVtwr)) proposes to drop the labels from the uncertain samples and use semi-supervised learning approaches. The paper also refers to several related approaches, which could/should be compared or discussed against the optimal transport based method proposed by the authors. Other popular baselines would be [INCV](https://arxiv.org/abs/1905.05040) and the beta-mixture modeling by [Arazo et al.](https://arxiv.org/pdf/1904.11238.pdf), which are slightly different in spirit. A recent [NeurIPS 2020 paper](https://arxiv.org/pdf/2001.10528.pdf) (which should be considered as a concurrent work) is worth discussing, for the benefit of the readers.\n\nI strongly recommend adding (at least) an empirical comparison to the Li et al., as it shares a larger goal (in the sense that utilizing the information from mislabeled data) yet implemented with a different philosophy. Understanding when/how one approach outperforms the other should be fantastic.\n\n__4. clarity and (potential) errors.__\nSome parts of the manuscript was not clear enough to me, or perhaps is written in an overly convoluted manner. Clarifying the following bits may help the readers (and me) follow the content more easily.\n- Eq. (2) follows the description \"Specifically, with a probability $1-\\delta$, the generalization bound for clean datasets is defined as follows:\". However, I am not sure whether it is a \"definition of the generalization bound.\" Rather, it should be one of many generalization bounds that one can prove.\n- Eq. (3) contains the expression $\\mathbb{E}_{X\\sim \\mu,\\hat{r}|T} [l(X;\\hat{r})]$. I am not sure if the definition of $T$ appears in the main text. I guess that it is the label transition matrix? Also, on which space is the minimum taken over? Is it over any distribution $\\mu$ on $\\mathbb{R}^d$, or is it more like over any distribution that can be generated by partitioning $\\mathbb{P}$? Any vagueness must be removed, as the authors also provide theoretical results.\n- In definition 1, what distance measure $d(\\cdot,\\cdot)$ are authors using? (also, using same $d$ for distance and dimensions add unnecessary difficulty). If the authors are not using more than two different notions of distances, why don't you just write it directly, like $\\mathbb{E}_{\\mu}\\lVert x_0-x\\rVert_2^2$?\n- In Eq. (5), again it is not clear over what the minimum is being taken over. Is it over all partitioned-distribution $\\mu, \\xi$, or also over all possible transport maps $\\mathcal{F}$?\n\n__5. Rationale__\nI am curious what the general intuition behind the proposed approach is. Why is the \"minimum transportation cost\" map $\\mathcal{F}$ a sensible method to transport the high-loss samples? Are we making any implicit assumptions on the nature of label-flipping operations, or perhaps the learning dynamics itself?\n\n__minor suggestions.__\n- The title can be more informative. I am not sure if the current title \"Wasserstein distributional normalization\" gives any information about the task under consideration, or \n- The notation $\\vartheta$ looks like the model parameter at a first glance. How about changing it to other greek letters?\n- In proposition 1, there should be a typo: $\\mathcal{F}: \\mathbb{R}^+ \\times \\mathcal{P}_2$. The mapping $\\mathcal{F}$ maps to where?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}