{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not mentioned/compared to prior work",
            "review": "The paper proposed an algorithm for decentralized learning and provides its experimental evaluation. The algorithm is very similar to already existing works on decentralized optimization (e.g. [1]), however, none of the papers were cited nor compared to. I don't see what are the differences between the proposed algorithm and these works. \n\nThe paper overall is hard to read and has grammatical mistakes. \n\n[1] X. Lian et al, Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent.\n[2] X. Lian et al, Asynchronous Decentralized Parallel Stochastic Gradient Descent. \n",
            "rating": "1: Trivial or wrong",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Not very new algorithm, lacking theoretical justification and comprehensive experiments",
            "review": "The paper proposed a decentralized training algorithm, where each worker reach consensus with a gossipy algorithm. This kind of algorithms is not new, and similar algorithms have been proposed in many existing literatures.\n\nTo list a few: \n\n\n* S. S. Ram, A. Nedi ́c, and V. V. Veeravalli. Asynchronous gossip algorithm for stochastic optimization: Constant step size analysis. In Recent Advances in Optimization and its Applications in Engineering. Springer, 2010.\n* Lian, X., Zhang, W., Zhang, C., & Liu, J. (2018, July). Asynchronous decentralized parallel stochastic gradient descent. In International Conference on Machine Learning (pp. 3043-3052). PMLR.\n* B. Sirb and X. Ye. Consensus optimization with delayed and stochastic gradients on decentralized networks. In Big Data, 2016\n\n\nThis paper failed to compare with existing similar algorithms, and does not provide more insights for dencentralized algorithms (for example providing new theoretical results). The experiments are on relatively small datasets (MNIST, CIFAR10). For above reasons I feel like the paper should be rejected.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments on distributed SGD for deep networks",
            "review": "There has been ample work to distribute learning among multiple units (workers) of a distributed network, such as a wireless network, both for convex and non-convex problems. The paper runs a few experiments of such a standard asynchronous algorithm of a small multilayer perceptron on MNIST data. They also give some results for a slightly larger CNN, run on CIFAR10.\n\nThe last decade has seen hundreds of proposals of distributed optimization algorithms inspired by consensus algorithms, which typically combine local gradient computations with a push phase for gradient aggregation and a final broadcast phase to  infrequent or sporadic communications and gradient quantization to compress the amount of the data exchanged between workers, see e.g. X. Lian et al, \"Asynchronous decentralized parallel stochastic gradient descent\", ICML 2018; A. Koloskova et al, \"Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication.\", ICML 2019; M. Amiri and D. Gündüz, \"Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air,\" IEEE Transactions on Signal Processing 68, pp. 2155-2169, 2020, etc. + all the references therein.\n\nThe CL algorithm described in the current paper is only a simplified consensus algorithm with the same local gradient computation, push and broadcast phases are most distributed SGD algorithms, but without gradient quantization, sparsification etc. It is evaluated in a simulated environment that is also simplistic (no real wireless network although the goal of the paper is to tailor the algorithm to mobile teams of robots). Obviously, its performance is weaker than a centralized algorithm, but the authors find in their experiments that the algorithm is insensitive to the frequency of gradient exchanges, bias in local datasets and losses in the communications affect the performance, which is strange. They even find sometimes higher accuracy for infrequent updates and/or larger loss rates, which does not make sense (for instance for CIFAR10 the accuracy decreases when the frequency of updates is increased from M_i = 2 to 5, or the droprate is increased from 25% to 50%). There is not a single comparison with any algorithm proposed in the hundreds of papers published on this topic in the last decade. Needless to say, this would be a minimal requirement for such a study. ",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to Reject: No theory and numerical experiments are not convincing.",
            "review": "This paper proposes consensus driven learning, which combines the Distributed Averaging Consensus (DAC) algorithm and local stochastic updates. \n\nStrength: \n\n- The DAC algorithm can be implemented in the asynchronous framework, which is very important in the decentralized environment. The combination of DAC and local updates may be able to solve many decentralized machine learning problems.\n\nWeakness:\n\n- The theory is missing. This paper does not have any theoretical results on the proposed algorithm, even for a special case. In addition, in Sec. 3.3, though the algorithm works without the reply message, the authors still keep it just because it comes from DAC. It seems from Sec. 3.3, if the reply is dropped, the communication can be reduced almost by half. So it is important to provide a theoretical guarantee for it. \n- Numerical experiments are not convincing. The number of nodes in the numerical section is too small. Only 8-10 nodes are used. The types of problems are limited as well. It applies to deep neural networks only. Since this can be applied to many other types of learning models, it would be interesting to see the performance in other cases. In sum, the numerical experiments are limited and not convincing at all. Is this algorithm sensitive to the learning rata and the \\gamma? \n- Writing needs improvement. E.g., in numerical experiments, many important details are missing. What is M_i for Sec. 3.2? What is the hardware for the experiment? What is the communication time for each experiment? There are many other unclear parts.\n- In Section 3.2., when the mixing rate r_m=0, the learning failed. What is the reason for this failure? Decentralized algorithms can handle this type of heterogeneous cases without any problem. So is it because a large learning rate is chosen? Did you try a small learning rate? Also, what happens when a different \\gamma is chosen. ",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}