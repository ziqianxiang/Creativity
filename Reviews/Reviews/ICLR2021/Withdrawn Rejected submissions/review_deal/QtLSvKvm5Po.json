{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for Backdoor Attacks to Graph Neural Networks",
            "review": "\n### Summary\n\nThis paper studied the backdoor attack problem in graph classification tasks. Specifically, the paper aims at injecting a subgraph structure into some of the graphs in the training data, such that the trained classifier would predict the malicious graphs as intended by the attacker. The paper has studied several random graph models to inject the subgraph, across several datasets with several different graph neural network parameterizations. The results show that injecting the subgraph for training would be able to lead the desired hacks during the test. \n\n### Pros\n\n1. The backdoor attack concept for GNN sounds interesting.\n2. The paper is written clearly. \n3. The experimental results are good apparently. \n\n### Cons\n\n1. The problem might not be quite well defined/motivated. \n2. The technique used in the paper might be a bit straightforward, without much of the insights into the graph distribution, or the problem/domain adaptive designs. \n3. There might be some flaws in the experimental design. \n\n\n### Details\n\nOverall I lean towards the rejection of the paper. There are several concerns:\n\n1. It is not very clear to me how one can modify the large portion of training data for some security sensitive systems. I understand that one can create some transactions to construct the subgraph, so as to inject the graphs into training inputs. However it would require huge effort to adversarially label those graphs, and finally get them used in the training system. Typically the classical adversarial attack only requires modification to the inputs, but here the problem setup might not be quite realistic.\n\n2. The injected graph is simply a random graph with predefined density/size. There could be problems for this non-data driven distribution of random graphs. For example, one should be able to easily distinguish the two distributions of graphs (i.e., the vanilla graphs, and the graphs after subgraph injection). If this is the case, one can easily detect the malicious graphs, and there’s no need to further run the GNN predictor. The author should verify the distribution mismatch. For example, one can use some test functions of graphs to plot the histogram, and visually judge the mismatch. Or one can also use MMD to measure the distribution discrepancy.\n\n3. Continue with 2., my hypothesis of why the clean accuracy is similar to backdoor accuracy is that, the neural network is able to easily distinguish between the vanilla graphs and the malicious ones in most cases, such that it is able to make multi-modal predictions (one for vanilla graphs and one for malicious ones) -- if this is the case, one should even be able to visually separate the graph embeddings of these two distributions (e.g., using t-SNE) -- and this may not be the ‘bug’ of GNN, but a desirable ‘feature’ of GNN.\n\n\n### Questions:\n\nI’d like to see the reply for the above questions. \n\n### Improvement:\n\nI’d suggest to focus on a more practical setting of the problem. The problem studied might not be the problem of GNN, or we probably don’t need to worry about the deep learning side. If one is able to attack the training dataset, then most models would behave like this. The main difficulty is instead to convince the system (e.g., twitter, bitcoin) to use your malicious dataset, which might not be a focus for the GNN community. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Applying backdoor attack to GNNs",
            "review": "Summary and contributions: \nIn this work, the authors propose the first backdoor attack to graph neural networks (GNN), which is a subgraph based backdoor attack to GNN for graph classification. The proposed algorithm uses a subgraph pattern as a backdoor trigger, and the results of empirical evaluation show that the proposed backdoor attack algorithm is effective with only a small impact on GNN’s prediction accuracy.\n\nStrength (significance and novelty):\n1. The proposed algorithm is the first backdoor attack algorithm to GNN.\n2. The algorithm characterizes the backdoor attack using four parameters: trigger size, trigger density, trigger synthesis method, and poisoning intensity.\n3. In the empirical evaluation, the proposed attack algorithm is effective in the graph classification task, with only a small drop in GNN’s prediction accuracy. Moreover, the authors perform careful analysis on the impact of choosing different parameters and different trigger synthesis methods.\n4. The empirical evaluation results show that the attack algorithm are all effective for GIN, SAGPool, and HGP-SL, which indicates it does not rely on the architecture of GNN.\n\nWeakness:\n1. It seems that this paper only transfer the basic type of backdoor attack on images to graphs, using the random graph generation model to synthesis the subgraph trigger. Although the authors mention the key difference between images and graphs (i.e., images can be repre- sented in Euclidean space whereas graphs cannot.), they do not explicitly discuss the unique challenges in performing backdoor attacks to GNNs. For example, since the adjacency ma- trix of a graph is in a discrete domain, when we synthesis the trigger by perturbing edges in the graph, how can we ensure that the perturbation is unnoticeable? It seems that when the trigger size becomes larger and larger, injecting it to the graph is likely to alter the graph’s basic properties (e.g., degree distribution, sparsity), and thus the injection can be easily de- tected. [1] discussed the unnoticeable constraints for perturbing graph structure by preserv- ing the node degree distribution, and I would like the authors to discuss in more details about whether their work can generate unnoticeable trigger.\n2. This work only performs backdoor attacks on vanilla GNNs, while there have been many proposed backdoor defense methods. Although most of them are dealing with backdoor attacks on images, there are still several methods that maybe can be used for graphs (e.g., [2]). Besides, many methods for training robust GNNs have also been appeared recently. Therefore, I would recommend the authors to perform backdoor attacks on GNNs with de- fense/robust training in order to further demonstrate the effectiveness of the proposed attack algorithm.\n3. In addition, it seems that the algorithm is largely heuristic, and I would have appreciated an inclusion of some discussion about the algorithm through a more theoretical perspective.\n\n\nAdditional feedback: \nIn the part ‘Impact of trigger size, trigger density, and poisoning density’ of Section 4.2: Figure 5a, Figure 5b, and Figure 5c respectively show the impact of trigger size, trigger density, and poisoning intensity on the Twitter dataset. →− Figure 2a, Figure 2b, and Figure 2c respectively show the impact of trigger size, trigger density, and poisoning intensity on the Twitter dataset.\n\n[1] Zu ̈gner, Daniel, Amir Akbarnejad, and Stephan Gu ̈nnemann. ”Adversarial attacks on neural networks for graph data.” Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery Data Mining. 2018.\n[2] Liu, Kang, Brendan Dolan-Gavitt, and Siddharth Garg. ”Fine-pruning: Defending against backdooring attacks on deep neural networks.” International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, Cham, 2018.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Novel idea but weak experiments and discussions",
            "review": "Motivated by the recent backdoor attacks on images, the authors propose a type of backdoor attacks on graphs. They  poison the training set by partially replacing a graph with a subgraph backdoor trigger and show that the graph neural networks (GNNs) can be successfully injected backdoors after training.\n\nS1: This paper demonstrates that graph neural networks are vulnerable to backdoor attacks.\n\nW1: Weak experiments. For example, the trigger size is too large (20% of the original graph), making the attack impractical. \nW2: Lack of in-depth discussions. For example, how the interaction between the topology of the trigger and that of the original graph affects the effectiveness of the attack.\n\nDetailed comments and questions:\n\nThe default trigger size is 20% of the average number of nodes per graph in the dataset. How does the attack perform when the trigger size varies? The 20% is way higher than that used in the image domains.\n\nHow does the interaction between the topology of the trigger and that of the original graph affect the effectiveness of your attack? Please conduct more experiments with varying graph-specific properties (e.g. Closeness Centrality, Betweenness Centrality, network diameter, etc.) and discuss your results.\n\nWhy do you place the trigger at a random position of a graph? Will placing the trigger at the center or edge (based on a network centrality measure) of the graph improve performance?\n\nTypos:\n* Table (2), the first row should be named “Error Rate-Baseline\" instead of “Attack Success Rate-Baseline” because the original model is not under a backdoor attack. \n* In section 4.2, the “Figure 5a, Figure 5b, and Figure 5C” should be “Figure 2a, Figure 2b, and Figure 2c.”\n\nOverall, it is nice to see a paper showing the vulnerability of GNNs against backdoor attacks. However, the backdoor attack is not a new technique and this paper lacks discussions on the particularities of backdoor attacks on graphs. With more experiments with varying graph-specific properties and some in-depth discussions on how these properties affect attack efficiency, this paper could be a strong work.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Inconclusive evaluation",
            "review": "Summary:\nA backdoor attack for GNNs is proposed. The goal is for a GNN to predict a target label chosen by the attacker if a specific (randomly generated) subgraph is injected in the test graph. Using sufficiently large and dense backdoor subgraphs the attacker can achieve a high success rate. \n\nReasons for score: \nOverall I am recommending a weak reject. While various aspects of the proposed attack are extensively evaluated, the evaluation is performed only on undefended graph neural networks. Moreover, given that the trigger size and trigger density need to be relatively large, and given that the subgraph is generated based on simple random graph models, it is likely that the defender can easily detect the backdoor (see weak points).\n\nStrong points:\n* The study of backdoor attacks for GNNs is timely, relevant and interesting.\n* The proposed method is simple and easy to implement.\n* The effect of different hyperparameters is extensively evaluated.\n\nWeak points:\n* As stated in the paper (\"We suspect the reason is that the random triggers are structurally similar\") the generated subgraphs are structurally similar since they are based on a simple random graph model. While this has the desired effect of random triggers being as effective as fixed triggers as shown in the paper, it also means that defender might be able to easily detect them. For example, [1] shows that embedded Erdos-Renyi subgraphs inside a larger ER random graph can be easily detected. Similarly, [2] shows that anomalous subgraphs can be easily detected using eigenvector L1 norms. Given the power law nature of real graphs and the non-power law degree distribution of ER graphs it is likely that these subgraphs can be detected as anomalies.\n* In most experiment the default value for the density is rho=0.8 making the density of the injected subgraphs significantly larger than the average density of the clean graphs, e.g. 0.34 for Bitcoin or 0.52 for Twitter, or even below 0.05 for other commonly used datasets for graph classification such as Reddit. This again would make it easier for the defender to detect the backdoor subgraphs and makes the experimental results inconclusive.\n* The attack is evaluated only against vanilla (non-robust) GNNs. It would be more informative to also evaluate the attack against robust GNNs such as e.g. the method proposed in [3], or even simple defenses such randomly dropping out edges as in [4].\n\nQuestion for the authors:\n1. Are the injected subgraphs detectable by existing methods for detecting anomalous subgraphs? (see weak points)\n2. How large is the success rate if the density of the injected subgraphs is roughly the same as the density of train/test graphs?\n3. What is the attack success rate if the backdoored testing dataset consists of graph whose *predicted* label is not the target label? There is no need to insert a backdoor if the prediction already matches the target. We can roughly estimate this rate based on the attack success rate and the clean accuracy, but this would assume that errors are uniformly distributed which might not be true, especially given the backdoored training set.\n4. How does the proposed method compare to other graph backdoor attacks such as [5]?\n6. Why is the subgraph injected by replacing nodes rather than adding new nodes?\n\nAdditional feedback that did not affect the decision:\n* The claim \"such trigger could be easily detected especially when the number of nodes in the subgraph is large\" for constructing a complete subgraph as a trigger is misleading. Most versions of the clique detection problem are hard, e.g. the problem of finding a maximum clique is NP-hard. While in practice approximately detecting cliques might be feasible for the defender it is not clear whether this is \"easy\".\n* The claim \"These defenses cannot be directly applied to defend against our subgraph based backdoor attacks because graphs do not have the coordinates like images\" is misleading. The randomized smoothing certificate from [6] or [7] and the certificate from [8] is applicable to the graph classification problem that is studied here. Using a smoothed classifier for example might be a viable defense against the proposed attack.\n* Even though this is mentioned in the text, it would be helpful if Figure 2 is annotated with the exact fixed values of t, \\rho and \\gamma while one of them is varied.\n\nTypos:\n* \"and graphs to an GNN can have different sizes\" \n\nReferences:\n1. Kadavankandy, Arun, Laura Cottatellucci, and Konstantin Avrachenkov. \"Characterization of L 1-norm statistic for anomaly detection in Erdős Rényi graphs.\"\n2. Miller, Benjamin, Nadya Bliss, and Patrick J. Wolfe. \"Subgraph detection using eigenvector L1 norms.\" \n3. Zhang, Li, and Haiping Lu. \"A Feature-Importance-Aware and Robust Aggregator for GCN.\"\n4. Rong, Yu, Wenbing Huang, Tingyang Xu, and Junzhou Huang. \"Dropedge: Towards deep graph convolutional networks on node classification.\"\n5. Xi, Zhaohan, Ren Pang, Shouling Ji, and Ting Wang. \"Graph backdoor.\"\n6. Bojchevski, Aleksandar, Johannes Klicpera, and Stephan Günnemann. \"Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more.\"\n7. Gao, Zhidong, Rui Hu, and Yanmin Gong. \"Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing.\"\n8. Jin, Hongwei, Zhan Shi, Ashish Peruri, and Xinhua Zhang. \"Certified Robustness of Graph Convolution Networks for Graph Classification under Topological Attacks\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}