{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work brings improvement to contrastive learning method for text data by\ncombining a Wasserstein objective with a \"memory bank\" strategy\nfor getting (and updating) hard negative samples.\nThe approach leads to small but consistent improvements across a variety of representation learning tasks in both supervised and unsupervised settings. While the paper makes a useful contribution and evaluates with some success on downstream tasks, the reviewers would like to see some intrinsic, qualitative discussion of the representation learning itself, and in comparison to more powerful contrastive learning methods. Overall the work falls below the acceptance threshold in a very competitive venue, so I cannot recommend acceptance.\n\nEven after discarding one uninformative review, the consensus remains borderline. The reviewers are, however, appreciative of the additional clarifying experiments provided by the authors. In the internal discussion, a concern was raised that the BERT baselines may not have a fair chance to compete, as they are not fine-tuned on the unsupervised data in the same way that the proposed method is, leading to possibly overestimating the improvement.  I encourage the authors to consider this.\n\nFinally, while some concerns about clarity have been addressed, some remain in the current version. In particular, I spot at least three duplicate and inconsistent entries, for Hjelm et al, Lin et al, and McAllester and Stratos.)"
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "The paper aims to improve contrastive learning for text representation by tackling two challenges: (1) The conventional mutual-information (MI) based objective can results in unstable training due to the sensitivity of KL to changes of representation; (2) contrastive learning could require a large number of negative samples, which is inefficient. For (1), the paper replaces the KL loss with Wasserstain distance; while for (2), the paper selects top-K most difficult negative samples (defined as the nearest neiborhors to the current representations).\n\nThe contrastive learning loss is then used to augment the standard task-specific loss to enhance learning and enable semi-supervised training. Experiments on benchmarks for text classification (GLUE), retrieval, and language modeling show the proposed method improve over standard supervised/semi-supervised learning.\n\nThe proposed method is intuitive and simple, largely stitching together previous successful techniques (e.g., Wasserstein distance for robustness, hard negative samples, momentum update).\nSection 2.2 gives a nice summary of several core challenges of contrastive learning.\n\nQuestions / weaknesses:\n\n1) In section 2.2, is it necessary to introduce v? -- Constrative learning is usually seen as maximizing the MI between input w and representation u, i.e., MI(w, u). Why here MI(v, u), e.g., Eq.(4), is used?\n\n2) I'm a bit surprised that the experiments do not include any comparison with previous contrastive learning methods, but only compared with models without contrastive learning. \nSimilarly, the ablation study only compares results of negative sample sizes (K). It's necessary to include more comparsions to show how much improvement each of the two proposed techniques (Wasserstein distance, hard negative samples) reaches. \n\n3) The caption of Table.2 says \"Results for QNLI, MRPC, and QQP are in the Supplementary Material (SM). \". Yet those results are not included in the suppementary.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Limited novelty, unfair comparisons",
            "review": "The paper argues that there are some challenges in learning text representations with vanilla contrastive learning techniques, due to the metric-aware property of  KL divergence as well as the difficulties in selecting good negative examples. It then presents two remedies: employ a Wasserstein constraint to the critic function and a simple active sampling strategy for negative examples.\n\nI think the paper is not strong enough to get into ICLR. Firstly, the Wasserstein constraint has been proposed to improve general representation learning in Ozair et al. (2020). The novelty of this work is very limited compared to Ozair et al. (2020). The active sampling contribution is a simple heuristic. Secondly, the authors only compare the proposed method with the non-CL baseline. A vanilla CL baseline should be included in all the experiments. Finally, the improvement margins are also pretty small.\n\nThe writing can be much better if the authors can use examples for concrete tasks. For instance, I am not sure the distinction between u and v representations after reading sec. 2.2.",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Contrastive learning for NLP with Wasserstein constraints and fast/dynamic selection of high-quality negative samples ",
            "review": "This paper presents a new contrastive learning methods based on intermediate textual feature pairs, which stabilizes training and overcomes vanishing contrasting signals by imposing Wasserstein constraints on the critic via spectral regularization, and leverages a fast and dynamic selection of high-quality negative samples. \n\nOverall, this is an interesting work on Contrastive learning for NLP, which\n-\tswitches from the MI-based NCE to a Wasserstein-based NCE for more robust and stable training\n-\tcreates a fast (with sub-linear Asymmetric Locality Sensitivity Hashing) and dynamically (with momentum update) select the most challenging negative examples on-the-fly.\n-\tseems to get solid performance gains on wide range of downstream applications.\n\nMy major concern is about the clarity of the paper and some detail information of model and experimental design.  It seems not very easy to reproduce corresponding codes and experiments only based on the current paper content. At least, there is quite some space to improve readability. For instance:\n-\tThe concrete u and v in each experiment should be more clearly explained, especially for u.\n-\tDetail design and Implementation of g\n-\t\"random sample 80% of features\" seems not clear enough for understanding\n-\tMore explanation for “Stop gradient for…” in Algorithm 1\n-\tOrganic integration of section 3.2/3.3/3.4 based on holistic and detail explanation of Figure 1 \n-\tMore clear illustrations on unsupervised/supervised/semi-supervised experimental designs and processes\n\nMinor issues:\n-\tSome notations are not self-contained. For example, “sup_x” in section 3.2 is not defined.\n-\tReference section should be more carefully checked. For instance, Hjelm et al. 2019a and 2019b are pointing to the same paper, and Lagler et al. 2013 seems to point to a wrong GPT2 paper which is not in the NLP domain.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Sound empirical results, unclear about goal of paper (representation learning vs regularization)",
            "review": "-----------------------------------------\nSummary\n-----------------------------------------\nThis paper proposes an approach to improve (supervised and unsupervised) representation learning for text using constrastive learning. The proposed approach augments standard contrastive learning with: (1) Spectral-norm regularization of the critic to estimate the Wasserstein distance instead of the KL (as in the Wasserstein GAN-style approach), (2) Active negative sampling to select hard negative examples, and (3) momentum-based updates of intermediate features. The resulting contrastive learning objective can be combined with standard supervised and unsupervised objectives to improve downstream tasks.\n\nNone of these techniques is novel on its own to my knowledge, so the main contribution here is to show that combining these techniques for text representation learning yields strong empirical results. This combined approach, called RECLAIM, is applied to a suite of standard NLP tasks: transfer learning, image-text retrieval, and language modeling, where it is found to outperform standard baselines.\n\n-----------------------------------------\nStrengths\n-----------------------------------------\n- Sound improvements on top of existing finetuning methods for GLUE  (though I have questions about the semi-supervised setup; see questions).\n\n- Evaluation across a wide range of setups (though language modeling results aren't really valid in my opinion due to the iVAE perplexity numbers being not exact; see weaknesses).\n\n- Ablation studies across various augmentations, though it would have been interesting to also ablate on Wasserstein vs KL as well.\n\n-----------------------------------------\nWeaknesses\n-----------------------------------------\n- My biggest gripe is that the paper is presented as a \"representation learning\" work, yet much of the focus is on performance on underlying tasks. That is, the contrastive learning objective is simply used as an additional regularizer on top of existing objectives. Implicit in this is the idea that \"good representations should lead to good downstream performance\", but it would have been great to do closer analyses of the learned representations to see how this contrastive learning objective results in different representations (see some example analyses that could be conducted in the questions section).\n\n- Given that RECLAIM is really a source of regulariazation, I would have liked to seen it compared to existing regularization methods such as MixOut on GLUE (https://arxiv.org/abs/1909.11299).\n\n- Results on language modeling are not really valid since the iVAE codebase (on which this work is based) does not actually calculate perplexity (they only estimate perplexity, which is a drawback of the iVAE work). The language modeling results would be much more compelling if the authors showed improvements on top of works that calculate perplexity more properly, e.g. with a variational lower bound or an importance-sampled estimate (e.g. as done in works such as Cyc-VAE).\n\n- It seems like this objective can be combined with self-supervision (e.g. masked LM objective) to aid in better BERT-like models (which would have been a very interesting result), but this seems to have been left unexplored.\n\n-----------------------------------------\nQuestions/Comments\n-----------------------------------------\n- How do representations learned this way differ from the standard approach? There are many interesting analyses that could be conducted for the different tasks. For example: on GLUE, which layers of BERT are finetuned more (or less) with this method; on image-text retrieval, what are some mistakes made by the standard approach that this method is able to fix?; on language modeling, how do the learned latent codes differ compared to iVAE (i.e. is it more sensitive to nouns/verbs, etc?)\n\n- Does the semi-supervised experiments utilize the full unlabeled data? (i.e. do you train with the contrastive objective on unlabeled data, and the supervised + contrastive objective on the labeled data?)\n\n- Does the switch from KL to Wasserstein distance make the lower bound on MI (equation 4) invalid?\n\n- The results on GLUE would be much easier to read if the average results across tasks are also presented\n---------\nAfter rebuttal: Thank you for the response. I have decided to leave my score unchanged.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}