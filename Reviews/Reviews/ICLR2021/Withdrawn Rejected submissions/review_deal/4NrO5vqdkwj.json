{
    "Decision": "",
    "Reviews": [
        {
            "title": "reasonable work, but limited contribution",
            "review": "This paper proposes a semantic inference network (SIN) to solve few-shot streaming label learning (FSSL) problem. The FSSL problem stems from streaming label learning, but with more consideration on the few-shot properties of the emerging labeled samples. SIN learns the semantic correlation between past and new labels through gradient-based meta learning, in order to solve the new few shot learning problem. Experimental results verify the effectiveness of the proposed approach for FSLL problem.\n\nThis paper is well organized and the overall representation is clear. The mathematical formulations are generally correct, but I didn’t check them all in detail. The experimental results are somewhat convincing.\n\nThe main weaknesses of this paper lie in the following aspects:\n(1)\tThe overall contribution is trivial. The main point is considering streaming label learning as a few-shot learning problem. Then, the contribution is limited in borrowing meta-learning strategy for FSLL. \n(2)\tSome methodological details are not very clearly explained. The feature-level, label-level and attention-level semantic inference take similar mechanisms, though the exact forms are different (say MLP or linear transformation). All of them compute the correlations between past and new semantic vectors. So they are abundant, instead of complementary. Besides the ablation study from empirical results, any theoretical analysis is helpful to clear this point.\n(3)\tThe setting of FSLL is somewhat doubtful to me. Why the past label set and the new label set are disjoint? A more realistic setting may be different but overlapped. \n(4)\tThe presentation can be greatly improved. Some typos exist. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The paper proposes a semantic inference network for a novel few-shot stream label learning problem with convincing empirical results but handwavy theoretical analysis",
            "review": "**Summary:** The paper investigates the few-shot variant of the stream label learning problem, Few-shot Stream Label Learning (FSLL), wherein new labels are introduced on the fly and the goal is to learn a model for the new labels using few examples. Towards this goal, the paper proposes a meta-learning framework, Semantic Inference Network (SIN), which exploits the semantic correlation between new and past labels to efficiently adapt the features from a few examples. Extensive experimentation on two existing multi-label classification datasets, Delicious (text-domain) and Mir-Flickr (image-domain) showcase the effectiveness of the proposed framework. While the paper provides a generalization analysis of the proposed framework, it lacks theoretical rigor and is handwavy, thereby undermining the claim about a theoretical understanding of the proposed method (for more details see cons subsection).\n\n**Pros:**\nThe paper introduces a novel setup for a few-shot learning problem: streaming label learning (multi-label classification tasks) where new labels associated with a few examples are added over time.\n\nThe proposed framework, Semantic Inference Network, is intuitive and successfully builds upon the two lines of the existing work: stream label learning (exploits the knowledge about the semantic relationships between old and new labels) and few-shot learning (focuses on single-label learning from a few examples).   \n\nEmpirically results are convincing with reasonable baselines and relevant ablation. \n\n**Cons:**\nThe paper presents Theorem 1 which is a standard result relating the generalization error of ERM to the Rademacher complexity of the hypotheses space $\\mathcal{H}$ (for more details see Chapter 26 [1]). The paper argues that the SIN uses the prior semantic knowledge to constrain $\\mathcal{H}$ to a smaller hypothesis space $\\mathcal{\\tilde{H}}$ which in turn helps in generalization. They provide an intuitive justification for this argument using Figures 2 & 3. Apart from the empirical results, the paper claims theoretical analysis as one of the contributions of their work. However, there is a disconnect between the presented theoretical analysis and the conclusion about the generalization of SIN derived from that.\nThe paper claims that SIN constrains to a smaller hypothesis space. They do not provide any formal results to justify this claim except for Figures 2 & 3 (which provides a visual depiction of what authors hypothesize about their method). A more formal statement is required to justify this claim if the paper wants to shed light on the theoretical perspective of SIN. For e.g., given two hypotheses spaces $\\mathcal{\\tilde{H}}$ (constrained because of SIN) and $\\mathcal{H}$, can they show if $\\mathcal{\\tilde{H}} \\subset \\mathcal{H}$, then $\\forall h \\in \\mathcal{H}$, $h \\in \\mathcal{H}$ is true and $\\exists \\in \\mathcal{H}$ s.t. $h \\notin \\mathcal{\\tilde{H}}$. Another way would be to reason in terms of the bound on Rademacher complexity (of the hypothesis space) and show that the method strictly improves upon this bound which in turn improves the generalization upper bound. Further, it is unclear under what assumptions $h_{\\mathcal{H}}^*$ will be included in the constrained space $\\mathcal{\\tilde{H}}$.\n\nThere is no denying the fact that the paper demonstrates the effectiveness of SIN empirically. However, under what assumptions/ conditions “the proposed semantic inference mechanism could constrain the complexity of hypotheses space” will translate to “would constrain” is not addressed. Without this, the current analysis looks more handwavy and falls short of convincing that SIN achieves better generalizability.\n\n**Questions to the authors:**\n1. The proposed SIN $\\mathcal{I}$ consists of different networks- $\\mathcal{W}_Z, \\mathcal{W}_I, W_A, \\phi(.)$. What are the configurations of these networks? Please add some details to the main text/ appendix. These details are essential to understand that all methods have access to similar model capacity.\n2. In the training procedure (section 3.3), it is mentioned that feature extractor $F$ is frozen. Further, in the ablation study, the paper mentions that SIN $\\mathcal{I}$ would reduce SIN to a MAML algorithm with semantic embedding. But it is unclear then what are the trainable parameters apart from the SIN? Meta-threshold parameters $\\phi(.)$?\n3. Please look at the cons section for more questions around the theoretical analysis.\n\n[1] Shalev-Shwartz, Shai, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed model is naive. Trivial theoretical analysis.",
            "review": "This work studies the few-shot streaming label learning problem and proposes a new deep model. Motivated by the sematic correlation among labels, the authors involve three branches of networks to predict the label probability. The first one transforms the features to a scalar value $\\mathcal{I}_f$. The second one transforms the past labels to $\\mathcal{I}_c$. The last one performs the attention operation from both the feature and past label weights to obtain a $\\mathcal{I}_a$. Finally, they are weighted averaged to be the final prediction. The authors also propose a meta-learning method to determine the threshold for each label. Empirical results demonstrate the proposed method is better.\n\nPros:\n1. The authors formalized the few-shot streaming problem, which makes this learning setting more practical.\n2. The proposed meta-learning method is novel in the multi-label learning community.\n\nCons:\n1. While the authors describe their model in an unusual way, I find that the proposed method is in general trivial except the attention network. First, $\\mathcal{I}_f$ is just a common NN model with the normalized feature $W_Z(z)$ as the feature and $W_{new}^k$ as the fully-connected layer. For $I_c$, actually $zW_{past}$ is the predicted past labels, which is directly transformed by linear mapping $W_IW_{new}^k$. For the attention one $I_a$, similar ideas can be commonly found in the data-mining community, such as recommendation systems, which adopts the classifier as the label representation. In general, I think this paper does not bring new results on the model level.\n2. Theorem 1 a very simple Corollary of classical theoretical results of structural empirical minimization and regularization. By restricting the hypothesis space to a smaller one, the generalization error can be naturally decreased. But, the true problem is that whether the hypothesis is indeed (theoretically) restricted to a (better) sub-space, which requires an in-depth exploration. However, the authors just intuitively discussed it. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper studies a novel and practical setting, i.e., few-shot streaming learning. A meta-learning appraoch with theoritical guarantee is presented, which achieves state-of-the-art preformance.",
            "review": "This paper studies a novel and practical setting, i.e., few-shot streaming learning. A meta-learning approach with theoretical guarantee is presented, which achieves state-of-the-art performance.\n\n### Pros\n1. The paper is well-structured and well-motivated: the problem statement is clearly laid out from the outset, with appropriate context, and explanations supported well diagrammatically. The idea, and perhaps more so the applications thereof, is seemingly novel and its explanation is given straightforwardly while avoiding getting bogged down in technical details.\n2. The experiments are logically ordered with the initial set covering the standard multi-label learning benchmarks with appropriate baselines. The proposal is shown to yield a significant boost in performance on few-shot streaming multi-label learning tasks.\n3. Particularly impressive is the depth into which the Appendices regarding the experiments, both elaborating on the details given in the main text as well as additional ablation studies.\n\n### Concerns\n1. Regarding the semantic inference mechanism, specifically in Equation (4), the coefficients of three components are learned from data, while it is not demonstrated in the experiments to show the importance of each of them.\n2. Regarding the consistency between Figure 1 and Equation (1)-(3), Figure 1 shows that $W_{new}^k$ is incorporated after combining three components, while Equation (1)-(3) separately uses $W_{new}^k$  and then do the combination.\n3. In Theorem 1, It seems to be incorrect regarding the definition of Rademacher complexity w.r.t. $\\mathcal{H}$.\n4.  In Table 2, the ablations show that the label-level, i.e., $I_c$, leads to large variance of the performance, I am interested in the rationale behind this.\n5. Regarding comparison baselines, it seems these methods are designed for few-shot multi-class classification, does this contribute to their inferior performance on multi-label tasks?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}