{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": " The paper attempts at controllable summarization in two dimensions: Length, and content. Authors try to achieve this through training data generation approach, where they provide a standard BART model with additional keywords (extracted using a BERT model) in training.\n\nThe paper's main motivation on controllable summarization is important and interesting, and despite simplicity, the results are generally positive on multiple datasets.\nHowever, despite positive results, reviewers raised several critical concerns, some of which remained unresolved after reviewer/author discussion period. Examples include concerns regarding lack of methodological novelty over prior work (R1, R2, R4), unfair/incomplete comparisons with prior work (R2, R4, R5), and not evaluating on a real user controlled setting instead of automatic keywords (R1, R4). Although the authors tried addressing human evaluation in their revision, some reviewers remained unconvinced. \nSome quotes from reviewer discussions:\n\n> I'm not convinced the human eval was done properly.\n\n> My concerns are not completely addressed and the score remains unchanged. For human evaluation, I agreed with Reviewer X.\n"
    },
    "Reviews": [
        {
            "title": "review",
            "review": "This paper proposes a two-stage summarization system where a document is provided along with (optionally) keywords or a prompt. This supplemental information helps to guide the summarization and possibly make it more user-specific. The keywords and prompt can also be guessed automatically by a BERT-base model, which seems to improve automatic metrics on CNN/daily mail.\n\nStrengths:\n* Moving beyond the conventional 'document/summary' framework of existing summarization approaches is a strength of this paper. This paper studies a few different ways that the summaries can be controlled: through prompts, entities, or one-sentence summaries of summaries (contribution and purpose summarization). These seem novel at least to this reviewer and could be helpful for future work.\n* When using oracle guidance, performance increases on several different datasets for slightly different forms of summarization (CNN/DM, arxiv, bigpatent).\n* The idea of using a two-stage approach (with a BERT-Base extractor to guess keywords to guide the summary) seems novel to this reviewer, and it seems to enable this approach to perform well even in an unconditional setting.\n\nWeaknesses:\n* The main weakness to this reviewer is that the evaluation might not be sufficiently convincing to test the key hypothesis: that these keywords/prompts can enable users to get summaries that are closer to their intent (like Figure 1). To this reviewer, this necessitates a human evaluation. Though testing factual correctness in Table 3 seems like a good start to this reviewer, measuring overall summarization quality (both conditional and unconditional on user intent) through a human evaluation seems necessary.\n* (minor) one possible reason why the two-stage approach might perform better on unconditional summarization is because there are more parameters when ensembling BERT-Base and BART. Possibly doing something multitask within a single BART model might be cleaner and could clearly test whether the gains come from more parameters/computation, or the keyword approach.\n\n\nOverall, to this reviewer, this paper seems like it would be strong if it had human evaluations of summarization quality. I would be willing to raise my score if those were provided.\n\n----\n\nUpdate: thanks for the additional human evaluation results! These help and the results on excluding unimportant entities seem strong to this reviewer. Perhaps it might be more helpful for the annotators themselves to try to interact with the summarizer in some way, but that's a more minor point.\n\nAnyways, I bumped up my score from 5->7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting techniques for some summarization tasks, but unclear contribution over Fan et al.",
            "review": "# Summary:\n\nBuilds/extends on Controllable Abstractive Summarization (Fan et al) using keywords and other prompts. There’s two phases, and both phases are independent:\n\n1. Extract keywords, z,  using a BERT classifier/sequence-tagger trained to predict keywords\n2. Fine-tune BART (Lewis et al) to learn p(summary | document, z).\n\nOne can use automatic keywords using (1) and get uncontrolled generation, for which they present SOTA results on some summarization tasks. Two datasets are collected: (a) intro->contributions from arxiv papers; (b) patent->one-sentence summary, which are used to measure performance of prompts specific to those tasks.\n\n# Pros:\n1. Improves state-of-the-art results on some summarization benchmarks.\n2. Provides BERTScore results in addition to ROUGE.\n3. Results provided across multiple summarization datasets.\n4. Interesting new datasets for measuring document+prompt->summary performance\n5. Interesting zero-shot/transfer results from summarization to Question-answering.\n\n# Cons: \n1. Contribution in methods over Fan et al + BART (Lewis et al) is minimal. Results in Fan et al were weak because the underlying model was much weaker than BART, so it is unclear how much results here improved from simply using BART and adding control tokens from Fan et al, which would be a useful baseline to have that is omitted.\n2. Since the focus of the paper is on controlling generation, more results on this would be informative. It is unclear how well control works when not using oracle words or automatically extracted keywords, i.e. user-controlled. An MTurk experiment evaluating how well control works would be useful in assessing this.\n3. Comparing BART/PEGASUS to BART+BERT-based model is a little unfair since BERT is another large model in the system, i.e. the total amount of compute and number of parameters is much greater. A more fair comparison would be to compare using a smaller BART or PEGASUS model such that the model sizes are comparable. It is unclear whether the proposed system would do better than BART/PEGASUS scaled to the same amount of total parameters.\n4. For the prompt tests, how well does BART/PEGASUS do if the decoder is prompted? This baseline would be useful to have. That is, it’s unclear how the p(y | x, z) improves over using p(y | x) by simply prompting the decoder.\n\n# Clarifications/questions:\n1. For \"CONTRIBUTION AND PURPOSE SUMMARIZATION\", what is the fine-tuning process? Are the models fine-tuned on (paper/patent, keywords)->abstract task before testing on intro->contribution generation? \n2. How are entities randomly selected in the example decodes in the Appendix?\n3. Are any of the prompts used in training or is it zero-shot?\n4. What is zero-shot state-of-the-art on the QA tasks? Please add to Table 5. GPT-3 zero-shot results would also be informative.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A simple but effective method of focusing abstractive summarization models.",
            "review": "The authors propose an abstractive document summarization model that can\ngenerate summaries that target a specific set of keywords or prompts. This is\nin contrast to generic summarization models that learn to summarize a document\nbut are difficult to control or direct. The authors propose a straightforward\nway of obtaining keywords from an article similar in spirit to Gerhmann et al.\n2018. Alternatively, \"ground truth\" keywords can be found using a reference\nsummary. In either case, the keywords are prepended to the input document and\na BART model is fine-tuned to generate summaries using both the document and\nkeyword content.\n\nThe authors go on to show how a model trained in such a way, which they refer\nto as CTRLsum, can generate entity-focused summaries, by using an entity name\nas the keyword prefix. Additionally, providing differing numbers of keywords\ncan be used to control the length of the generated summary.  The authors also\nshow that CTRLsum can respond sensibly to prompts, i.e. instead of providing\nkeywords, a question or initial phrase is provided.  Useful summarization\nbehavior can be achieved including zero shot question answering, or\nenumeration of a research paper's contributions or the purpose of an\ninvention.\n\nWhile the paper feels largely like an extension of Keskar et al. 2019, the\nevaluation of the proposed methods is very thorough on a variety of settings\nand domains. I especially enjoyed the break out of entity targeted summaries\nbased on whether the entity occurred in the lead and/or reference summary. The\nuse of prompts to obtain question answering and more focused\ncontributions/purpose summarization was also very interesting.\n\nI would be happy to see this paper accepted to ICLR. This paper offers a\nsimple method of obtaining a variety of focused or targeted summarization\nbehaviors from a BART summarization model. In general, I would like to see\nmore work like this exploring methods of controlling pretrained language\nmodels.  The evaluation of the correctness of the generated utterances\nsuggests that this method provides fairly reliable control.\n\nThere several areas where the paper could improve. The explanation of how\nlength control is achieved was not very clear. It would help to have examples\nlike those shown in the appendix present in the section introducing length\ncontrol. \n\nComparisons are to the standard BART model or to Fan et al. (2018) which\nsimilarly prepend important control information to the input. It would be\ninteresting to see an evaluation that compared CTRLsum to BART with a\nconstrained decoding method, such as dynamic beam allocation [1].\n\nAdditionally, the authors should say more about the differences in entity\ncontrol of their method and Fan et al. 2018, which seem on their face to be\nsimilar.\n\nDid the authors experiment with pairs of entities as entity controls? It would\nbe especially interesting to see whether the model preserves the correct\nrelationship between entities, especially for entities that didn't occur in\nthe same sentences in the original document, e.g. one important entity and one\nunimportant entity.\n\n[1] Matt Post and David Vilar. Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation. ACL. 2018.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "Paper Summary:\n* This paper proposes a framework for controllable summarization, CTRLsum.  It is different from standard summarization models that CTRLsum uses a set of keywords extracted from the source text automatically or descriptive prompts to control the summary.  Experiments with three domains of summarization datasets and five control aspects.\n\nStrengthes:\n* The authors investigated the effectiveness of the proposed model through extensive experiments.\n\nWeaknesses:\n* The proposed method that uses keywords as an additional input text is almost the same as CIT (Saito et al., 2020), and the scores of CTRLsum on the CNNDM dataset reported in Table 7 does not outperformed those of CIT.  Also, it is not novel to use descriptive prompts to control natural language generation.\n   * Saito et al.: Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models. CoRR abs/2003.13028 (2020)\n* I think that the author's claim, \"keywords and prompts are complementary\", is not evaluated fully.\n\nQuestions:\n\n* With respect to contribution summarization, did you evaluate CTRLsum(keyword without prompt) and CTRLsum(prompt without keywords)?  The control tokens \"the main contributions of this paper are : ( 1 )\" is far from the keywords used during training, and so I think that the keywords are not effective for contribution summarization.  In fact, BART that uses prompt worked well for contribution summarization.\n\n* Did you evaluate the ablation tests with respect to the special token \"|\" and keyword dropout?\n\n* Can CTRLsum control the generation with multiple aspects (length and entity control, length and QA control, etc.) simultaneously?  The length of  summaries generated by CTRLsum is strongly dependent the number of keywords, and so I think it is difficult to simultaneously control multiple aspects including length control.\n\nUpdate:\nThank you for the answers to my questions and additional experiments. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}