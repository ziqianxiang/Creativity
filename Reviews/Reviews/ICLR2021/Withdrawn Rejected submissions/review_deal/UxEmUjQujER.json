{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of Spatial Frequency Bias in Convolutional Generative Adversarial Networks ",
            "review": "\nThis work aims at highlighting a spatial frequency biais in convolutional GANs, and proposes FSG, a method to circumvent this bias. More specifically, it states that high frequencies in a generative convolution architecture are harder to produce as the higher the frequency, the more correlation there is between adjacent frequencies, making it difficult for the network to adjust to specific high frequencies in the data without affecting adjacent ones. This is shown both theoretically and empirically, and is corroborated by experiments that highlight the fact that GANs yield larger errors on the higher frequencies. To overcome this issue, they propose to learn generators whose frequencies are shifted around specific high frequencies. \n\nThe attacked problem is interesting and the research questions have potential implications in various scientific fields where GANs are applied.\n\nThe paper is written clearly and reads well. The experimental analysis is interesting and seems to be conducted rigorously. If shifting the frequencies does not hinder the GAN training and introduce additional instabilities, shifting the frequencies as proposed in the paper could be a useful ‘hack’ for specific problems.\n\nHowever, up to the best of my understanding, the theoretical results seem a little unrelated with the narrative. The paper states that it is difficult for the network to adjust itself to high frequencies, a consequence of the high correlation between adjacent frequencies. However, correlation is computed with respect to the randomness due from different initialisations (this is what I have understood reading in between the lines in the proof, but should be clearly stated in the assumptions, see below). A priori, this correlation during initialisation is by no means linked to the difficulty to adjust the network, which is a dynamic property that should hold at all times. If there indeed is a link, I suggest the authors make it explicit. In the contrary, a different quantity should be considered, e.g. a measure of the impact of a small perturbation applied to a high frequency of the output with respect to adjacent frequencies would better correspond to the narrative. This is for me the main issue that should be addressed.\n\nThe independence assumption of the unrestricted filter component G, for the spatial frequency components only seems to hold during initialization. As a consequence, Thm 1 only applies during initialization. If this is the case, this should be clearly stated.\n\n\nIt would have been interesting to propose a real world application where this work, specifically FSGs could be motivated, or at least state specific cases where modifying a prior on the frequencies could be useful. \n\nHow exactly are the multiple FSGs combined? Are these trained jointly? Additional details are needed. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting viewpoint, but theory and experiments leave too many questions",
            "review": "The paper argues that CNN's have a built-in bias that makes them more adept at generating low frequency than high frequency content. This is an interesting hypothesis that is worthy of studying, however I am not quite convinced by the reasoning and experiments in the paper.\n\nFirst, about the theory.\n\nOverall the math notation is somewhat confusing, and does not do what notation ideally does, which is to unambiguously clarify what is meant in the text. I'll mention this on a couple of occassions below. \n\nIt appears that a key assumption is made that CNN's can be studied as linear functions. The argument is that ReLU nonlinearities are locally linear. I am rather skeptical about whether this is valid in this context, and whether dropping the relus in Eq. 3 is mathematically valid. It is not clear what is meant by \"neighborhood of H^1\" or \"locality of any single sample\". When linearizing, one obtains a first-order Taylor expansion, which contains the original formula (relus and all) evaluated at a fixed point, and then a linear perturbation that's based on the derivative at that point (but generally does not correspond to a sequence of linear layers). This function will indeed be linear in the perturbation, but it is certainly not the function in Eq. 3 where the relus are simply dropped. In more practical terms, imagine taking a trained neural network and then dropping all the relus from it. The result is not a \"linearized network\" but rather a network that returns completely arbitrary results and has little relation to the original one. \n\nThis is not just a question of whether the formal notation is correct, but to me it calls to question whether the overall reasoning ignores some critically important factors. I can buy the argument that a _linear_ CNN suffers from this kind of a bias, for the reasons stated in the paper, but linear networks are extremely poor generators for a variety of reasons, and hardly relevant in practice. I suspect that the nonlinearities play a key role in shaping the spectrum, and this is ignored by the analysis. For example, imagine something as simple as a spatial sine wave before and after applying a relu on it. The spectra are completely different and such effects likely have significant consequences on how the spectrum is built by the network.\n\nThe notation about absorbing the upsampling operator seems questionable. As far as I see it could be absorebed into the convolution kernel with suitable care. But doing it this way is basically just saying that the operation is still secretly done somewhere, but now the notation no longer describes it, which is not very helpful.\n\nAs for the overall thesis, I do find it plausible that CNN's do better with low frequencies, but I am not fully convinced that these derivations are the fundamental reason for such behavior.\n\nExperiments.\n\nI am also somewhat skeptical about whether the experiments support the conclusions of the paper, at least as strongly as suggested.\n\nFor the FID experiments, a potential problem is that the metric is not comparable between different datasets. For example good performance on FFHQ results in different FID values than good performance on CelebA, and the performance in these datasets cannot be meaningfully compared based on these values. Similarly, it might be that the change of FID in figure 3 is mostly explained by the underlying datasets being different at each point in the x-axis, due to the drastically changed image content in the training data. One possible way to study this would be to show a baseline where a subset of e.g. FFHQ is compared by FID to another disjoint subset of FFHQ (simulating a \"perfect\" learning result), and seeing whether the curve stays flat as a function of the high pass (as is implicitly assumed in the paper now).\n\nAs for the high-frequency shifting experiments, I like some parts of the reasoning behind it in principle. However given all the reservations about the analysis above, I am not sure if there is a strong logical connection between the paper's arguments and the results of this experiment. In the end it boils down to multiplying all images in the dataset by a single-pixel-pitch checkerboard pattern, which is unnatural for most CNNs for any number of reasons (though this heavily depends on design details of the final layers) regardless of its spectral interpretation.\n\nFinally, the proposed FSG architecture. \n\nFirst of all, the vastly improved results for 0.5-frequency shift (changing DC to Nyquist) seem unsurprising when the data is similarly spectrum-shifted. This again boils down to manually multiplying the generated image by a checkerboard image before feeding it to the generator. The layers of the generator are then unaware of the the need to draw a checkerboard, and do what they always do. The task of the discriminator is marginally harder than before, effectively it just needs to learn to remove the checerboard in the first layer, which is easy with convolutions. So almost nothing changes from the usual case of \"clean\" data, and training goes well. So problem solved? Unfortunately the setup is extremely contrived, and not applicable to any dataset that wasn't multiplied with a checkerboard pattern -- indeed, applying this particular FSG-shift with any regular dataset would probably degrade the results greatly, like in the shifted-data experiments.\n\nThe somewhat more interesting variant is where less drastic shifts are made. It's interesting (if not overtly surprising) to see that the network learns to naturally latch onto the easiest way available to generate the particular frequencies. This could have some potential for further study and development. At the moment its usefulness is limited by the fact that the shifts are specified as fixed hyperparameters that are probably dataset-specific. Would there be sense in learning them on the fly instead? The argument that there is no increase in computational resources is also questionable in this case, as additional layers are introduced. The paper currently keeps these low-capacity, but one has to wonder if insisting on keeping them low capacity limits the quality of the results.\n\nIn summary, I think the authors are right to be studying questions like this, but in the present form of the paper, I am not enough convinced about the reasoning or the experiments to recommend acceptance.\n\n_POST-REBUTTAL UPDATE_\n\nI unfortunately missed the time window to discuss the authors' response further with them, but I still wish to address the linearization issue. I believe we agree up to a certain point, and I realize my explanation above was not the clearest. The existence of _some_ linear (well, affine) function around a point is indeed what I mean by the Taylor expansion. However, what I do not agree on is that such linear functions could be expressed as a CNN in general, because _convolutionality_ is a strong restriction on the space of linear functions. Quite simply, there is no guarantee that this locally linear piece could be expressed as a linear CNN of a given architecture with _any_ choice of weights -- indeed, I suspect vast majority could not, and those that can might need to use extremely contrived filters. With a full linear MLP instead of CNN they could of course -- that would simply be the Taylor expansion (that is, full linear + bias). For this reason I believe that the reasoning is not quite valid.\n\nMy bad for missing the True FID levels, this does indeed make the experiment somewhat more relevant. However, my overall opinion remains that there are too many issues to recommend acceptance at this time.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting exploration but left with more questions than answers",
            "review": "**Summary**: This paper examines the ability of GANs to learn distributions outside the typical natural image distributions for which they were originally developed. Specifically, the authors scrutinize the ability of GANs to model high frequencies, which are more scarce than low frequencies in natural image data. The authors provide evidence that GANs may be biased against producing high frequencies, and propose a solution to ensemble several GANs to (implicitly) model different frequency bands.\n\nOverall, I agree with the authors that the sensitivity of GANs to different frequency bands in data merits scrutiny. Additionally, the paper is well-written and reasonably clear. However, I feel that this paper unfortunately leaves me with more questions than answers. A few of them follow, in descending order of my perceived importance.\n\n*Why are the motivating applications not considered?* The motivation for this work is strong and clear: despite being designed for natural images, GANs are increasingly employed to generate data from other distributions such as medical data (Kazuhiro et al. 2018), satellite images (Ganguli et al. 2019), waveforms (Donahue et al. 2019), seismic data (Wang et al. 2019), etc., where high-frequency information may be vital. Despite this nice motivation, the authors choose to examine only artificial datasets consisting of frequency-shifted images. It makes sense to look at such data for initial experimentation, but it doesn't make sense to state the aforementioned motivation and never examine anything besides (manipulated) natural images.\n\n*Why blame GANs and not generative ConvNets?* I take issue with one particular aspect of the narrative: that the bias against high frequencies is attributed to \"GANs\" and not \"convolution-based generative image models\". In fact, there is no experiment in this paper which looks at the culpability of the GAN _learning algorithm_; only the architecture (convnets). There are two reasons this is problematic:\n1. The _inattentive_ reader is left with the impression that the learning algorithm rather than the architecture is to blame\n1. The _attentive_ reader is left wondering if the learning algorithm _should_ be to blame, or if it is exclusively an architectural problem\n\n*Can we _fix_ the inductive bias instead of hacking together a solution?* I think the frequency-shifted GAN idea is cute. It makes perfect sense on paper: GANs seemed to be biased towards a particular band (low frequencies), so let's just artificially shift other frequencies into this band as a pre-processing step, and merge the outputs of a \"bank\" (as in filter bank) of GANs which specialize in different bands. However, this strikes me as a hack and not something that we (as a research community) actually want to be doing down the road. Why not take a closer look at the inductive bias (convnets) and see what we can do about eliminating their bias against high frequencies while preserving their ability to effectively model lower frequencies?\n\n*Is the frequency-shifted GAN actually working?* In Figure 4, the \"shape\" of the scores from the frequency-shifted GAN matches that of the corresponding standard GAN. Why is that? Is it possible the improved performance is just coming from the additional capacity? Seems like you could run the same experiment without the frequency shifting inductive bias to ablate.\n\n*What of our metrics?* There is a small amount of discussion about metrics at the end of Section 5, but I would like to see more explanation of the bias of our *metrics* to different frequency bands.\n\n*What about high-frequency artifacts?* GANs (especially earlier convolutional GANs) are known to generate undesirable high-frequency artifacts, commonly referred to as _checkerboard_ artifacts (Odena et al. 2016). I find it difficult to reconcile the existence of these artifacts with the theoretical framework expressed in this paper. Can the authors clarify what these artifacts imply w.r.t. their hypotheses?\n\nLow-level detail(s):\n- I like the thought experiment in paragraph 1. So much that I would love to see it move from thought experiment to toy example :)\n- Axes are too small to be readable in Figures 3 and 4",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea but badly designed experiments",
            "review": "The paper looks into the inductive bias in generative networks that makes it easier for them to learn to output low frequencies than high frequencies. No completely satisfactory explanation to this apparent bias has been given so far. In this paper, both theoretical and experimental evidence is proposed for explaining this phenomenon, and an architecture is proposed to circumvent the bias.\n\nThe FID Levels measurement would benefit from further analysis. FID is based on a latent space of a network trained for classification, so any frequency biases of this classification network would be inherited by the FID metric itself. Hence its capability to resolve differences may vary between frequency bands for this reason alone. This effect should be somehow quantified before FID Levels can be considered a reliable metric.\n\nMy main concern about the paper is that the frequency shifting tests are so extreme. The construction of the high-frequency datasets in Section 3.2 effectively multiplies the images by a pixel-sized checkerboard of +1 and -1 values. The inability of GANs to reproduce such images is not at all surprising, because they are extremely unnatural and thus difficult to handle with  convolutional networks. I suppose this is partially the paper's point, but it would need to be shown with images that are not so extremely corrupted.\n\nEven worse, the FSG experiment in Table 2 uses parameters that are exactly the same as were used for frequency-shifting the datasets. In effect, the GAN is tasked to generate the original, natural image, which is then frequency-shifted, i.e., multiplied by a checkerboard, afterwards according to Eq. 8. As such, the GAN objective ends up being the same as in the vanilla case with unmodified data and unmodified generator. The chosen shift parameters of $(\\hat{u}_t,\\hat{v}_t) = (\\frac{1}{2},\\frac{1}{2})$ cause the weights of $G_i$ in Eq. 8 to be zero (the weight simplifies to $sin(\\pi(x+y))$ which is identically zero for integer values of $x$ and $y$), and the weights of $G_r$ to be the checkerboard. The discriminator still has a harder time deciding between real and generated images when data is frequency-shifted, which probably explains why the results do not match exactly with the results in Table 1 with normal generators and non-shifted data - although with WGAN-GP they are very close.\n\nFurther experiments in Table 4 show some improvement in FID thanks to using an ensemble of FSGs, which is a compelling result. I would still like to see an improvement to a state-of-the-art FID using a modern dataset like FFHQ or CelebA-HQ, which should be possible to obtain if the method addresses a real issue. The original CelebA dataset is more of a curiosity these days, and known to exhibit various artifacts due to bad resizing operations and low-quality source material. More importantly, the observed improvements are questionable because the baseline FIDs in Table 1 do not agree with previously published results. Karras et al. 2019 (IEEE TPAMI) shows FIDs of 2.65 and 6.81 for StyleGAN and PG-GAN, respectively, whereas the corresponding results in Table 1 are 22.49 and 36.65 for StyleGAN2 and PG-GAN. These are quite large discrepancies and raise a question if the networks were trained properly.\n\nTo summarize, I find the experimental results too inconclusive to properly bring to light the effects of the frequency bias, or to provide solutions to it. Unless I have misunderstood the experiment in Section 4 completely, it is a bizarre setup where the generator's optimum is restored to exactly where it was, and the extra imaginary term can be neglected. I don't see any merit in this experiment, and even with the additional experiments I am not convinced that FSGs are a good way to combat the bias, because the paper has baseline results that are far from what the used networks should achieve.\n\nPros: important topic, nicely illustrated observation in Figure 1, an interesting although not properly analyzed idea for a frequency band specific distribution measure\n\nCons: badly designed experiments, use of archaic CelebA dataset",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}