{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an approach to defining/tackling the question of separating \"style\" and \"content\" of images, and introduces a novel way to learn representation that disentangle these aspects of images. I think it offers some new ideas. The reviewers were split on the evaluation. Among the chief concerns with the initial submission were a problematic formulation of the objective, missing comparisons and analysis, and questions about novelty of the architecture (in particular w.r.t. AdaIn). I think the rebuttal/revision have addressed these fairly well. I do agree with R2 that some flaws remain, in particular the analysis could be more thorough/complete, and the paper could then be stronger. "
    },
    "Reviews": [
        {
            "title": "Unsupervised Content-Style Disentanglement for Image translation",
            "review": "In this paper, authors introduce a new approach to content-style (C-S) disentanglement for multimodal unsupervised image-to-image translation. The main idea behind the proposed method is that the content information is encoded into the latent space common for both source and target domains, while the domain-specific style information is used to shift and scale the points from content distribution so they are mapped by the generator to the target domain distribution.\n\nThere are a few issues in this paper that I would like to be addressed.\n\n1. In the first page of the paper you write: \"When group observation is not available, we define content includes the factors shared across the whole dataset, such as pose.  Take the human face dataset CelebA (Liu et al., 2015) as an example, the content encodes pose, and style encodes identity, and multi-views of the same identity have the same style embeddings, but different content embeddings, i.e., poses\". In this paper, authors view only pose information of the faces in CelebA dataset as content; but according to the definition above, shouldn't facial expression also be included in the content as shared information across all examples?\n\n2. The proposed method suggests using style embedding to shift the content embedding before translation in Single case and within translation in Multiple case. How to address the fact that the same idea was used in a few other papers for multimodal cross-domain translation, such as MUNIT by Huang et al. ECCV'2018 or FUNIT by Liu et al. ICCV'2019?\n\n3. In addition to the main C-S fusion block, two more losses were introduced in this method: Instance Discrimination (ID) and Information Bottleneck (IB). To see the effect of each component, it would be helpful to see the ablation study results.\n\nOn the other hand, the paper is well-written and well-structured ans easy to follow; the translation results look promising. In addition, the quantitative metrics used in this paper, in particular the content transfer metric, are very reasonable for evaluation of disentanglement quality.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice experimental results, but lack of thorough investigation and formalism limit impact",
            "review": "Summary: This paper presents a novel combination of losses to learn representations that disentangle content from style. An ablation study is done with a few variants of the model, and extensive quantitative and qualitative experimental results on content/style disentangling demonstrate the model’s performance. \n\nOverall the experimental results look very nice and seem to compare well to other methods both  qualitatively and quantitatively. However, the scope of the contribution seems limited since the losses have already been introduced in the literature. The novelty seems to be in the embedding combination method, which is not explored as thoroughly as I’d expect. Also, the grounding of the proposed loss as a probabilistic model seems lacking to me.\n\nStrengths:\n* Impressive looking results on style transfer in a few distinct domains, including faces (celeba), chairs, and Car3D.\n* Simple method that could in principle be applied to a variety of domains.\n\nWeaknesses:\n* This paper would be stronger if it dispensed with the incorrect formalism introduced in Section 3, and introduced the training objective as a set of standard loss functions as in Eq. 4.\n* If this paper is about methods for combining embeddings for content/style recombination, I would hope to see a more thorough exploration of the different options. For example,  Eq. 2 is close to a bilinear model, as used in [4]. It would be interesting to see this combination method compared to explicitly, as it is a well known method for linear content/style recombination. It seems like only two methods (concatenation and the proposed method) were tried.\n* The proposed method seems to be an ad-hoc combination of losses already introduced in the literature. \n* “most previous C-S disentanglement works rely on supervision, which is hard to obtain for real data” --- full supervision of both content and style is difficult to obtain, but supervision of one variable alone is extremely common and represents the basis for most modern content/style decomposition methods. For example, see generic content/style decomposition models based on GANs [1, 3], and VAEs [2].\n* Denton & Birodkar 2017 does not use full supervision of content and style, rather they assume that the static components in the video sequence represent content, and everything else represents style.\n* Page 1 Paragraph 3: This definition of content --- as  factors shared across the whole dataset --- doesn’t make sense to me. Each face image has both an identity and a pose, so why is one content and not the other? It seems to me that the distinction is somewhat arbitrary, and probably determined by a target downstream task (e.g., face classification).\n\nClarity:\n* Section 3.1: This needs a more complete explanation. A few detailed questions follow:\n** Section 3.1: Is Q conditioned on s as well as c?\n** Equation 1: Since P(x|c) is NOT conditioned on s, wouldn’t this objective function encourage Q to be independent of s, i.e. Q(x|c,s) = Q(x | c) ?\n** Where is \\Psi in Eq. 1 ?\n* Eq. 3 - I don’t follow how this is optimizing KL( P(x | c) || Q_{s, \\theta}(x | c ) ). Please give a derivation.\n* Eq. 2+3 s_i, c_i are free parameters that the network is allowed to optimize, so how are  images not in the training set dealt with? Does it require training a new s_i, c_i for each new sample?\n* Which loss is Eq. 3 part of? I don’t see it listed in Eq. 4. \n\nRating:\nReferences:\n[1] Xun Huang,  Ming-Yu Liu,  Serge Belongie,  and Jan Kautz.   Multimodal unsupervised image-to-image translation. InECCV, 2018.\n[2] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervisedlearning with deep generative models.  InAdvances in Neural Information Processing Systems,pp. 3581–3589, 2014\n[3] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and377Yann LeCun. Disentangling factors of variation in deep representation using adversarial train-378ing. InAdvances in Neural Information Processing Systems, pp. 5040–5048, 2016\n[4] Tenenbaum, J. B., & Freeman, W. T. (2000). Separating style and content with bilinear models. Neural computation, 12(6), 1247-1283.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting results but possible limited novelty",
            "review": "The paper addresses the problem of unsupervised content style disentanglement. To this end, a content vector and a style vector is sampled from a given prior distribution. The style vector is decomposed to mean and std which is applied on the content code in the same manner as in AdaIN (C-S Fusion block). \n\nPros:\n\nThe paper is well written and clear, providing a clear formulation of the problem and the proposed architecture. \n\nThe method provided is well motivated and clear.\n\nThe experiments demonstrate some improvement on state of the art. Some ablation is performed on the use of normalization layer, showing its effect on disentanglement. A variety of datasets are considered, and both generation quality (LPIPS) and disentanglement (classification accuracy) are numerically evaluated. Overall the experimental evaluation is extensive and show some improvement over a number of baselines as well as a new application in 3D generation. \n\nCons:\n\nTo me the C-S fusion block is essentially the application of AdaIn from the style vector to the content vector (hence applying a change of style). Modeling the content vector as a shared parameter c and the style vector as an image-specific has been introduced in MUNIT before (and in other works). Other than architectural specific choices, the difference to MUNIT is the application of this framework in conjunction with GLO instead of using and style encoder and a content encoder. Even though MUNIT was designed to work with class level supervision, the method suggested seems very similar (other then the use of GLO instead of encoders) and it might be the case that MUNIT would therefore perform similarly. I therefore believe a comparison to MUNIT when both A and B are the same dataset (e.g celebA) would provide a better insight.\n\nImage2StyleGAN [1] and StyleGANv2 [2] are also able to disentangle content and style. It would be interesting to compare their method to the one suggested. The very latest work tackling this problem is that of Swapping Autoencoders [3] but this has only been recently published. \n\nSome concerns regarding the experiments: In table 2, the results of disentanglement are worse that that FactorVAE. Why is this the case? In addition, no comparison to baselines is provided for the Figure 7 - visual analogies on the Market-1501 dataset.\n\n[1] Image2stylegan: How to embed images into the stylegan latent space? In: IEEE International Conference on Computer Vision (ICCV) (2019)\n\n[2] Analyzing and Improving the Image Quality of StyleGAN. Karras et al.,\n\n[3] Swapping Autoencoder for Deep Image Manipulation. NeurIPS 2020. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}