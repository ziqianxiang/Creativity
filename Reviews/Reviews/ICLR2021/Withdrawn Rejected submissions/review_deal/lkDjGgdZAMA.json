{
    "Decision": "",
    "Reviews": [
        {
            "title": "Nice idea, but lacking in novelty.",
            "review": "This paper proposes a metric, transferred discrepancy (TD), for measuring how different two feature representations are, based on the difference between the predictions made by downstream models trained using these representations.  Theoretically, it shows this metric is invariant to isotropic and orthogonal transformations, and analyzes the behavior of this metric in the limit of infinite data.  Empirically, it demonstrates that this metric is able to distinguish between features trained with similar vs. different training data, and that this metric can be used to choose between various training strategies (by choosing the training strategy that generally attains lower TD values across random network initializations).\n\nStrengths\n- The idea of measuring the difference between representations based on the downstream performance of these representations is good.\n- The metric definition is simple and intuitive, and can apply to both regression and classification tasks.\n- The theoretical results shed light on how this metric behaves in specific linear regression settings.\n- The experimental results, showing the important impact of different training strategies on the TD metric, are interesting, revealing that certain training methods lead to more stable feature representations than others (and that it seems these same methods are generally the ones that perform better on downstream tasks as well).\n\nWeaknesses\n- I think the primary weakness of this paper is novelty, and lack of proper comparison with existing baselines.  There exist several metrics, from both the kernel approximation literature, and from the embedding compression and stability literature, that also focus on comparing how similar two representations are in terms of their performance on downstream tasks.  For example, the kernel approximation literature is largely concerned with the question of: “If we replace an expensive/high-dimensional feature representation phi(x) with a much less expensive/lower-dimensional representation z(x), and train a model using z(x), how will this compare to the model trained using phi(x)?”.  See papers [1,2,3,4] cited below, which define/analyze notions of “spectral approximation” between different feature representations.  In the case of word embedding compression, the question of interest is “If I compress a word embedding matrix (e.g., using quantization), and train a model using the compressed embeddings, how will this compare to the model trained on the uncompressed embeddings?”.  See paper [5], which defines a metric called the “eigenspace overlap score”.  Lastly, in the case of embedding stability, the question is “If I routinely update my embedding matrices based on new data, how much should I expect the predictions of the model trained on the new embeddings to disagree with the previous model?”.  See paper [6] for further details about the corresponding metric (“eigenspace instability measure”, EIS).  This final EIS metric is defined in a **very similar** way to the TD metric, focusing on the _average_ disagreement across a set of tasks between models trained on two different representations, instead of _worst-case_ disagreement. \n- It has already been noted that training deep learning models from different random initializations can lead to very different models (e.g., [7]).\n- I don’t think the paper does a particularly good job at arguing why an optimization method being “TD robust” is necessarily better than one that isn’t.  Is “TD robustness” something a practitioner should care about independently of the model’s accuracy (it probably is, but this isn't argued in the paper)?  In Section 5, the paper says “Our experimental results demonstrate a strong connection between TD robustness and the quality of the representation”.  A couple issues here: (1) This is not validated empirically (none of the tables/figures include model accuracy results, or show that TD robustness is positively correlated with model accuracy across a wide range of settings). (2) This sentence suggests that TD robustness is simply a proxy for picking feature representations which are better in terms of downstream performance.  But if this is the case, why should the proxy be used instead of simply selecting based on accuracy?  Computing the TD metric for a pair of representations is just as expensive as computing the accuracy of these representations, so it doesn’t offer any obvious computational savings.\n\nGiven the amount of existing literature focusing on very similar questions to those investigated in this paper, and the fact that this literature is not cited or compared with, it is not clear to me that this paper provides enough additional insight to merit publication at this time.  Although some of the empirical results about TD robustness are interesting, they are also not particularly novel, and the argument that there is a tight connection between TD robustness and representation quality is not well-supported empirically.\n\n[1] H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, A. Zandieh.\n“Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees.” ICML 2017.\n\n[2] T. Erdélyi, C. Musco, C. Musco. “Fourier Sparse Leverage Scores and Approximate Kernel Learning.” NeurIPS 2020.\n\n[3] C. Musco, C. Musco. “Recursive Sampling for the Nystrom Method.” NeurIPS 2017.\n\n[4] J. Zhang, A. May, T. Dao, C. Ré.  Low-Precision Random Fourier Features for Memory-constrained Kernel Approximation. AISTATS 2019. \n\n[5] A. May, J. Zhang, T. Dao, C. Ré. On the Downstream Performance of Compressed Word Embeddings. NeurIPS 2019.\n\n[6] M. Leszczynski, A. May, J. Zhang, S. Wu, C. R. Aberger, C. Ré. Understanding the Downstream Instability of Word Embeddings. MLSys 2020.\n \n[7] Q. Cormier, M. M. Fard, K. Canini, M. R. Gupta.  Launch and Iterate: Reducing Prediction Churn.  NeurIPS 2016.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting metric, good theoretical analysis, but empirical experiments fail to study the most important property of the metric.",
            "review": "This work presents a measure of similarity between representations that is based on the corresponding transferability of two representations across tasks. It is claimed that the proposed metrics provide a more fine-grained evaluation on different tasks than other previous metrics, thanks to its use of transfer learning, and that it can be used to evaluate the effectiveness of training strategies.\n\n===============================\n\nPros:\n1. Proposed metric is grounded in tasks of interest.\n\n2. They present an analysis of the metric under the linear probing setting (limited to linear heads). First part of the analysis outlines the invariance of the metric to isotropic scalings and orthogonal transformations. Second part of the analysis shows that for a set of representative tasks (linearly realizable) the metric depends both on the tasks and the correlations between the representations. Previous metrics would ignore the statistics from the joint distribution over (Phi(x), y).\n\n3. They further present how TD is equivalent to CCA, highlighting the potential interest for TD to provide more fine-grained evaluation for specific groups of tasks. \n\nCons:\n1. The empirical part neglects the importance of intermediate representations which may not lend well to transfer learning with linear heads. These representations are nevertheless crucial within a neural network.\n\n2. There lacks clear guidelines/baselines to help conclude whether a metric value should be considered as significantly different or not. If we say two initializations lead to a TD of 0.2, on what basis can we determine whether 0.2 is a significant difference or not?\n\n3. Most of the trends in Table 3 are also observable for the other metrics (D_CCA, D_CKA) in most cases. Based on these results I would not say TD is more useful than the others to observe these effects as they all corroborate it.\n\n===============================\n\nReasons for score:\n\nI would vote for a weak rejection. I believe the proposed metric is an interesting contribution, but the claims are too strong and partially verified.\n\nTwo major modifications that would be required are 1) The empirical section should demonstrate clearly how including information on the joint distribution over (Phi(x), y) helps providing more fine-grained evaluation for specific tasks than other metrics. This should be the focus of the empirical section as it is the most important property of the proposed metric. 2) Section 5.3 should be revised as a sanity check instead. The other metrics seem to corroborate these findings. This is good as it suggests TD is consistent with other metrics, but there should be no strong claims that TD provides better evaluation than the other metrics, not based on these results.\n\n===============================\n\nAdditional observations\n\nThe focus on transfer learning neglects the importance of intermediate representations which may not lend well to transfer learning with linear heads. These representations are nevertheless crucial within a neural network. In the same spirit, previous works have shown (ex: Yosinski et al. 2014) that first layers transfer better than last ones. If we assume last layers to be less transferable, then the TD metrics on large corpus of tasks should be marginally informative. In other words, if we think from the perspective of the theoretical analysis in this work, a large corpus of tasks (even if they are qualitatively similar such as random splits in Yosinski et al. 2014) is bound to have many non linearly realizable tasks if we look at the representation of the last layers. I believe this may be an important limitation of the empirical section of this work. Studying representations at different levels in the neural networks would be an important sanity check. \n\nIn the theoretical part, the authors clearly assume that the tasks are linearly realizable. This is an obvious requirement otherwise the measure of TD on poor solutions with W/W’ would not be reliable. The empirical part of the paper has no guarantee with regards to this. Because of this, reporting the performance of the output heads is critical to assess the validity of the metric evaluations. If the heads cannot solve the task, the TD metric is uninformative. This also relates to my previous point about representations at different levels in neural networks.\n\nMost analysis of results are based on absolute numbers, but it is unclear whether different metrics should be compared directly as such. Looking at the many tables in appendix, there seems to be similar trends across the metrics. This suggests to me that a normalization of the metrics may lead to similar results across them.\n\nYosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014). How transferable are features in deep neural networks?. In Advances in neural information processing systems (pp. 3320-3328).\n\n===============================\n\nQuestions\n\nWhy do you take the max TD across tasks instead of mean+/-std? It seems to me the latter would be more descriptive. There are also few experiments probing the effect across many downstream tasks. Using only a small subset of downstream tasks only gives limited contextual evidence.\n\nJust to be sure, the 10 runs are trainings with different initializations, this means 10 times of pairs (Model 1, Model 2) with each different initializations? I believe the authors should provide the standard deviation of these measures. It is difficult to evaluate whether the differences of TD are significant without the standard deviation.\n\nTable 13:\nAre the results on different initializations of the same depth/width? Or comparison between ResNet20x20/32/44/56/110?\n\n===============================\n\nTypos\n\nPage 4, first paragraph: Shouldn’t the square distance be noted ||u - v||^2 instead of (u-v)^2?\nPage 5, Corollary 1. [...] the notations in Corollary 2 -> the notations in Theorem 2?\nPage 21, second paragraph: initialing -> initializing",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea; more focus on either theory or experiments could improve the work.",
            "review": "The paper proposes a new metric to measure the distance between the representations learned by two different models. A few others have been proposed in the past, but the authors argue that these most often directly depend on the feature values, and in practice it's hard to make sense of this. Accordingly, they propose a metric that explicitly depends on how representations behave on one or several downstream tasks of interest.\n\nInformally, and assuming the models are deep networks, the 'transferred discrepancy' metric between models M1 and M2 requires to replace their heads with randomly initialized ones for the new task, and train those heads on the downstream data. Then, we compare the predictions coming from M1 and M2 with their new trained heads on the downstream data, and quantify their similarity or lack of. In particular, a symmetric divergence is computed on the output distribution for each downstream input data point (note this doesn't explicitly account for performance, accuracy, etc. -- downstream labels are only used in the head training phase), and then averaged over all data points. When we care about several downstream tasks, the authors propose to take the largest transferred discrepancy among the set of tasks.\n\nSection 4 provides some theoretical analysis in a simple setting: linear regression, squared loss, and high data regime (n > p). By applying the closed formula for OLS, the authors present in (5) the analytical solution for TD. It is then straighforward to check that in this setting TD satisfies two desirable properties: invariance to isotropic scaling and orthogonal transformations of the input representations. This applies to other convex losses. I think this is an important check, and the results provide some value.\n\nLet Z and Z' be the representations from models M1 and M2 respectively. In 4.2, the authors try to study a number of convergence properties of TD under some assumptions (for example, that Y is indeed a linear combination of Z and Z' -- realizable setting). They show that if there's a linear mapping transforming Z to Z', then, with enough data (n \\to \\infty), then the TD converges to 0, something desirable and to be expected. Theorem 2 (and Corollary 1) extend to the multi-task case, and link TD to the singular values of matrix D. Theorem 3 shows a link between an alternative metric (Canonical Correlation Analysis) and TD; the former is task agnostic and can be related to the average TD over (linearly realizable) tasks.\n\nSection 5 describes a number of experiments. From this point on, the authors reduce the TD metric to be the fraction of datapoints where both models do not make an identical *argmax* prediction.\n\nThe first experiment is a simple check where three representations / models are trained. Two of them on very similar datasets (CIFAR 2, 5, or 10) and the third one on SVHN. Then, metrics are computed on Cifar 10's X (D_CCA and D_CKA) and Cifar 10's (X, Y) for TD_CLS. All the metrics state that the Cifar representations are closer to each other than to the SVHN one, as desired. TD_CLS scores are --in absolute value-- more far away from each other in both cases.\n\nThe second experiment focuses on models trained on the same data (with identical hparams, etc.) but different random initializations. Table 2 shows results for different downstream tasks. While D_CCA and D_CKA are task agnostic, TD_CLS provides a score per task. The TD_CLS results are what one would expect, the representations are further away when the target task is less related to the original task.\n\nTable 2 should include standard deviations wrt different independent trials. It should also include the transfer performance for each downstream task. The authors claim \"two feature extractors trained from different initializations do not learn the same features\". I think this is too broad (and inaccurate) of a statement given just Table 2.\n\nOne thing that stands out in Table 2 is that D_CKA ranking suggests Cifar 2 models are closer to each other than Cifar 5's. If you average over downstream tasks for TD_CLS, you reach the opposite conclusion. Is there any intuitive explanation for this?\n\nIn the last experiment, the authors further study the impact of random initializations. They say a training strategy is robust if finetuning a linear head on top of two (or more?) frozen model features coming from different initializations leads to consistent predictions (low TD_CLS) on a given downstream task. Table 3 summarizes the results obtained when applying different training strategies (data augmentation, LR schedule, and adversarial training). Unfortunately, the upstream and downstream datasets are extremely close (Cifar 5 and Cifar 10). Again, no standard deviations are shown. Table 3 should also show the accuracy obtained downstream for each type of models.\n\n---------------------------------------------------------------------------------------------------------------------------------------\n\nOverall, the paper proposes a simple metric (% of argmax prediction disagreement) to evaluate the distance of two representations. The novelty lies in explicitly using specific downstream tasks to provide a context to compare representations. However, I feel the paper is a half-baked mix of some theory and some small experiments. Honestly, I'm not sure how useful the theoretical section is indeed (beyond the checks in Section 4.1, which I appreciate). The experiments feel a bit toy-ish. Not necessarily due to the dataset size (Cifar is on the small side of the spectrum), but due to a lack of diversity and breadth: most experiments use downstream tasks that are really similar to the upstream tasks (is this the relevant setup in practice?). What happens when we train several models on the same upstream data, but then evaluate on a very different one (say, SVHN and Flowers)? Is everything \"equally close\" or far wrt TD? What if we take a randomly initialized deep network in this setting? Is it further from the models trained on the same upstream dataset --even though the task is quite different and the initial features may not be very useful in absolute terms? What if we take two randomly initialized deep models? How does their distance compare to the one of models trained on some \"non-relevant\" upstream data?\n\nAlso, a more common practical way to use deep representations for transfer consists in fine-tuning the whole model (not only its head). The metric can be trivially extended to this case: fine-tune each model (completely), and see the % of prediction disagreement. One may say the metric doesn't longer evaluate the representation themselves but their \"ease to be adapted\". Have the authors tried this? How does it relate and compare to the linear-head case?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes a novel feature comparison metric that simply measures the difference (or distance) in downstream performance between two feature encoders for a set of tasks. Theory is presented justifying the generality of the metric for linear tasks. Experiments are also presented to support the usefulness of the metric.\n\nStrengths:\n- The proposed method attempts to solve an important problem by shifting points of view, this insight is important\n- The proposed method resolves some contradictions of previous proposed metrics\n- The paper relates the proposed method to previous metrics\n- The paper is fairly well written\n\nPossible improvements:\n- This paper does not address the caveats of the proposed method, instead only focusing on what it improves [C]\n- This paper, despite being based on a notion of transfer, does not address how this problem is solved in transfer learning, nor does it refer to the transfer learning literature.\n- The base experiments of this paper could be improved, with more complete sanity checks.\n\nProblems:\n- Many experiments are run on a single seed, or appear to be, and variance is not reported [A]\n- The robustness claims appear unsubstantiated, as they are not measured [B]\n- The proposed metric shifts the problem of feature similarity onto downstream tasks, which seems to negate the very reason for the existence of feature similarity metrics [C]\n\n\nI rated this paper with a \"Clear Reject\", because as I *currently* understand the paper and results, many aspects are lacking. I'm honestly open (and hoping) to have my mind changed. I think the authors make otherwise interesting points.\n\n\nA:\n- For all results, Tables 1,2,3, the variance across seeds should be reported (in the main text or the Table), as well as how many seeds were used, for Tables 1 and 3. I could not find such details in the appendix, and one might assume that the authors did not run multiple seeds, simply judging by the number of significant digits reported. **This significantly undermines the scientific value of the reported results**.\n  - looking at Table 3, I assume that the row for \"Without augmentation\" and the row for \"Standard Training\" are actually the same setting, since they both are the baselines of their row-group. The first setting has a TD_cls of 0.28, while the second has a TD_cls of 0.21. If this is correct, then TD_cls can vary by at least as much as 0.07 between two random seeds. If this is correct, then it is fair to presume that all the configurations in Table 3 are not significantly different from one another, and it is fair to presume that most quantitative claims made in this section are inaccurate.\n\nB:\n- \"TD-robustness\" is defined informally as something quantitative, or at least, quantifiable, i.e. \"consistent predictions on the same downstream taskwhen trained from different initializations\", why then is TD_cls reported in Table 3 instead of some measure of robustness? How many different initializations were run for the same setting? As I understand it, Table 3 does not currently report on TD-robustness, and so claims such as \"[this metric] suggests a training strategy that leads to more robust representation\" have no empirical support and should be avoided.\n\nC:  \nThere are many possible reasons one may want to compare features, scientific inquiry, understanding generalization, interpretability, performance on downstream tasks.\n- I think it's ok for the authors to argue that downstream tasks are an important part of that. That being said, it seems like a more important use of comparing representations is to know how good they are _before_ knowing what the downstream tasks are. The authors make a great point of considering the class of linear problems of 4.2, but considering that we now live in the realm of non-linearity, it seems naive to hope that every downstream task can be solved with a linear finetuned layer. This seems like an important caveat that isn't addressed by the authors.\n  - In the disentanglement literature, linear combinations of disentangled features aren't expected to solve most tasks, but rather there is a desire to have interpretable features (possibly to plug them into other neural networks later).\n  - Relying on downstream tasks means that if we don't yet know the _real_ downstream tasks the quality of TD will depend on some other set of downstream tasks which have to be designed, but could be arbitrarily far from the real downstream tasks. This appears unsolvable, unless perhaps a metric also relies on generic task-agnostic characteristics of features.\n- An argument for this metric is that it resolves apparent contradictions between two prior approaches, yet it can just as easily introduce new contradictions. Here's an example: if a problem is overcomplete--and here I mean there are multiple possible (perhaps redundant) features that once captured generalize perfectly--then we should expect the ratio of #learnable features to #all overcomplete features to match the overlap in similar features between randomly initialized feature learners. It seems like something is lacking if we define a metric that doesn't take into account the nature/structure of the data distribution. I assume such problems are very likely to arise in computer vision (think of the causal failures of ImageNet classifiers, cows on a beach are misclassified because of the reliance on the presence of a confounder, grass, to classify cows).\n- There are cases, as above, where Z and Z' could be radically different and have no overlap in learned features yet solve all downstream tasks similarly. This raises a deeper question which the authors don't really address, if all we care about are downstream tasks, perhaps this doesn't matter at all, although I'd argue that if one cares about interpretability, which may be a crucial ingredient in integrating fair and ethical AI into society, then this is a problem.\n\nI don't really expect the authors to address the above criticisms _in depth_ in their paper, I realize space is limited, but in general I think papers which openly address their caveats do everyone a service by pointing out future research directions and honestly situating the paper in the field.\n\nAdditional comments:\n- \"In representation learning, the quality of the learned representations is often evaluated based on their performance on downstream task\", true, but this ignores a large amount of works on feature disentanglement and unsupervised learning in the absence of immediate downstream tasks (a prominent problem in e.g. exploration in Deep RL).\n- \"we think it is more reasonable to define whether two representations are similar based on their performances on downstream tasks\", I think it would be interesting for the authors to acknowledge the existence of/conjecture on scenarios where this is unreasonable.\n- Perhaps I'm reading this too fast, but the results of 4.1 don't seem too surprising, if not obvious, since (4) solves a linear least squares then by nature of the problem it should be invariant to scaling and to an orthogonal projection. I mean to say that the linear least squares is what's invariant, not TD per-se. That this applies to all strongly-convex losses also seems somewhat obvious, but I'm perhaps missing the complexity of that statement.\n- 4.2 typo, should be $\\alpha' \\in \\mathbb{R}^{p'}$ \n- On base experiments: \"For simplification, we use the Cifar-10 task as the downstream task\", it should be unsurprising that phi_1 and phi_2 would then be similar, this is indeed what is found. A much better sanity check would be to also test that when using SVHN as a downstream task, phi_1 and phi_2 are going to be similar, an even better sanity check would also include a totally different dataset, like fashion MNIST, and find that phi_1 and phi_2 are still more similar. If this is not the case and this similarity isn't found, then in a sense something useful is lost in the proposed metric. It would be important to know that.\n- I honestly worry that this paper is reinventing metrics that already exist in transfer learning, since measuring the performance on downstream tasks appears like a fairly obvious thing to measure, but I'm not familiar at all with that literature. It would probably be a good exercise (and valuable for this paper) for the authors to read and relate to that literature. Quickly scanning a 2019 survey paper suggests that this is exactly the kind of measure that people use to compare models.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}