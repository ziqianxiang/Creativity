{
    "Decision": "",
    "Reviews": [
        {
            "title": "Not strong enough",
            "review": "The paper considers the well-studied column subset selection problem, where the goal is to find a small subset of the columns that can be used to \"reconstruct\" all the remaining columns (using linear combinations). The goal is to minimize the error in reconstruction. The authors propose an algorithm called \"spectrum pursuit with residual descent\", which given K, starts with a set of K columns and in a round-robin manner, refines the chosen subset using a spectral algorithm. \n\nThe authors show a theoretical result, comparing the reconstruction error to the error of the best rank-K approximation (given by the SVD tail, which is a lower bound on the reconstruction error using any K columns). They prove a bound of $(N-K-1 + \\kappa(A)^2) / (N-K)$ * Rank-K SVD error, where $\\kappa(A)$ is the condition number of A. If the condition number is small (< N) this gives a pretty good approximation to the best-possible reconstruction.\n\nWhile the form of the bound is new, I believe it only applies to the somewhat uninteresting regime where the condition number (ratio of the largest to the smallest singular value) is \"small\". Often, matrix approximations are only used in cases where the top singular values are much larger than the bottom ones, so in this setting, the bound is not really useful. One question I have in the proof (on Page 13, eq. 12) is that the authors ignore the cross terms of the form u_i u_j^T. While orthogonality implies u_i^T u_j = 0, the outer product is not necessarily zero. Such terms also survive when multiplying by A. Can the authors comment on this?\n\nAs for the algorithm itself, it would be nice to say how the algorithm compares with local search (where one can try to swap one of the columns for any other column). It seems to me that apart from the computational advantage (due to P being < n), local search only does better. Can the authors comment on this?\n\nWhile the experiments show a slightly improvement over the considered previous results, things like naive local search are not considered. The greedy algorithm has also been used for CSS (Altschuler et al. 2016), which should also be a baseline. Also, the experiments compare the performance of K-subset-selection against the best rank-K approximation. It is useful to set a threshold of mass that one wishes to cover, and see how many columns different algorithms choose. (This seems to have been done only against random selection with synthetic data, which is not quite fair.)\n\nOverall the paper is a bit underwhelming, especially for ICLR. The theoretical results don’t show a convincing improvement over known algorithms, and also the experiments are restrictive. The problem also may not be too interesting to an ICLR audience (although it's a basic ML primitive, so it would be interesting if the results were a bit stronger).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review 1",
            "review": "This paper studies the column subset selection problem for low rank matrix approximation. The main contribution of the paper is an analysis to a (variant of) greedy-like heuristic proposed in Joneidi et al. 2020, which unlike many previous works, is a beyond-worst-case bound which adapts to the structral property of input matrix A. In particular, the authors show that this algorithm (Spectral Pursuit) achieves approximation ratio close to 1 when the condition number is small and number dimensionality of matrix is large. The experimental results look good too, although my expertise is more on the theoretical aspect of this problem so I cannot comment too much about them.\n\nHowever, there are a few crucial drawbacks of this paper that prevented me from recommending for acceptance. Detailed comments below:\n\n1. The strength/optimality of the theoretical results is questionable. \n\nI actually really like the idea of non-worst-case analysis in column subset selection and I think it is the right direction for future research. However, the theoretical analysis provided in this paper does not seem to be accurate enough. \n\nFirst, unlike many existing papers in this area, like Deshpande & Rademacher 2010a, Dan et al. 2019, and more references below [1,2,3], this paper did not provide any (nearly-matching) lower bound for the approximation ratio analysis, which is very useful for understanding the optimality of the analysis. Another more important reason for my concern is that the approximation ratio in theorem 2 scales super linearly with the condition number. This just doesn't look correct. Typically, the low rank approximation is only applied when the input matrix is nearly low rank (otherwise, the low rank approximation will be a poor approximation to the original matrix). This means that the matrix has a lot of singular values that are very close to zero, in which case the condition number is extremely large or even infinity. The theoretical results will be vacuous in such scenarios, which aren't unrealistic at all.\n\nFrom the reviewer's point of view, the right measure for the difficulty of the problem should not be the condition number. It should measure the tail behavior of the eigenvalues, like the eigen-gap condition in power methods literature, or the eigenvalue decay rate (inverse polynomial or exponential decay) in kernel methods literature. And indeed, there are existing papers that provide analysis in such measures, but this paper did not cite them. I will discuss the details in (2c).\n\n2. Mistakes in review of previous results/missing important references. \n\nAlthough I appreciate the effort that the authors made in the related work section, a few important references are missing and some of the claims made here are incorrect. \n\n(2a) For example, the highlight of previous results (Table 1) has a few mistakes and hence doesn't reflect the best progress made in existing literature. The results in Dan et al. 2019 did not improve over Volume sampling in Deshpande & Rademacher (2010a). In fact, Dan et al. 2019 explicitly mentioned that their result achieves the same approximation ratio with Deshpande & Rademacher. The presented upper bound here for Deshpande & Rademacher (2010a) is incorrect, it should be \\sqrt{K+1} instead since Deshpande & Rademacher (2010a) considered squared frobenious norm as objective. For the setting where oversampling is allowed (i.e. the number of selected columns can be larger than the rank k), the papers mentioned here (CUR and \"Near-optimal\") are not the best known result, see Guruswami, & Sinop's SODA 2012 paper [1].\n\n(2b) A related issue here is that the comparison in table 1 is not entirely fair and even somewhat misleading. All of the previous results are worst case analysis which don't require any extra assumptions, like the bounded conditional number assumption in this paper. Besides, it is misleading to claim that this paper achieves a (1+eps) approximation: both CUR and \"Near-optimal\" can achieve (1+eps) approximation with *arbitrarily* small epsilon for any input matrix A, while this paper cannot guarantee that. \n\n(2c) This part is a continuation of Section 1: this paper missed a few key references with similar ideas and some even achieved better (at least from the reviewer's point of view) results. \n\nAn ICML 2016 paper [2] (and a few references therein) analyzed a similar greedy heuristic and also provided instance-dependent approximation guarantees. There are some differences, for example, their theoretical results require oversampling, but their instance-dependence is in a more reasonable way (in the sense of the guarantee is better when the matrix is closer to low rank). Although the theoretical results are not directly comparable, it's reasonable to expect a comparison with their method in the experiments.\n\nA very recent paper [3] that appeared on arXiv in February and recently got accepted to NeurIPS provided a similar non-worst case analysis to column subset selection. They focused on the exact rank setting as well, like this paper did. Their non-worst case guarantees are stated in terms of stable rank and eigen-decay conditions, and always provably tighter than the worst-case \\sqrt{K+1} bound, which do not suffer from the problematic behavior I mentioned in section 1. \n\nIn summary, I find it difficult to recommend this work for acceptance given the current state of the paper.\n\nReferences:\n\n[1] Guruswami, V., & Sinop, A. K. Optimal column-based low-rank matrix reconstruction. SODA 2012.\n\n[2] Altschuler, J., Bhaskara, A., Fu, G., Mirrokni, V., Rostamizadeh, A., & Zadimoghaddam, M. Greedy column subset selection: New bounds and distributed algorithms. ICML 2016.\n\n[3] Improved guarantees and a multiple-descent curve for the Column Subset Selection Problem and the Nyström method\nM. Dereziński, R. Khanna, M. W. Mahoney, arXiv 2002.09073",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Self-rank and Column Subset Selection Problem",
            "review": "The authors propose a technique for finding representatives from a large high dimensional dataset based on the concept of self-rank. Self-rank is defined and it is closely related to the column subset selection problem under the Frobenius norm.\n\nGiven an input $N x M$ matrix A and parameter $K$, the authors propose an algorithm (Spectrum Pursuit with Residual Descent SP-RD) that selects $K$ columns from A so that $||A - \\Pi_{S_k}(A)||_F^2 \\leq (1+\\varepsilon) \\|A- A_k\\|_F^2$ for $\\varepsilon$ that is lower bounded by $(\\kappa(A)^2 -1 ) / (N- K)$ where $\\kappa(A)$ is the condition number of $A$. The authors also present a tighter bound for the special case of K=1 (Theorem 1). In addition, the authors experimentally evaluate the proposed method on synthetic data and real-world applications.\n\n#### Reasons for score: \n\nOverall, I vote for weak rejection that I elaborate below in a list of concerns. In high level, I believe the flow of the paper should be improved. In particular, in the introduction there was a discussion about self-rank (Eqn. 1) which was a regularized version of CSSP. Later, in Equation 2 the definition of self-rank is a different one, as well as in Eqn. 3. Theorem 1 and 2 present results about the CSSP problem. That said, it was difficult to make the connection back to self-rank. In addition, the presented bounds depend on the condition number of $A$ that could be arbitrarily large. The authors should compare / normalize their bound with state-of-the-art CSSP algorithms.\n\n\n#### Strong points:\n* Intuition on self-rank and reduction to CSSP problem.\n* Reference to prior work seems up-to-date (I am not an expert in representation learning and self-rank)\n\n\n#### Concerns: \n* Bounds on condition number seem incomparable with prior SOTA of CSSP problem. The authors should try to make a comparison.\n* Paper readability should be improved to make connection of CSSP and self-rank more clear. Avoid having several similar definitions of self-rank.\n* Theorem 2 for $K=1$ does not correspond to Theorem 1 due to an extra $(1-\\rho(A))$ factor. Why is this the case? Is Theorem 2 tight or not?\n* The statement of the main theorems should be improved. See my comments below.\n\n#### Minor comments: \n- Equation 1: Self-rank seems to depend on $\\lambda$ here. A better notation $S(\\lambda)$.\n- Font is small before related work section.\n- Table 1: $M^w$ is probably $M^\\omega$ and please define $\\omega$. Is this the matrix multiplication constant?\n- Table 1: $\\varepsilon$ is a big misleading here, since it depends on condition number. It might be better to use another letter to denote your bound.\n- \"Our proposed bounds is the first work .. approach to $0$ asymptotically.\" -> I think this statement is not true. If you increase $N$ the condition number will be increased as well.\n- If $\\Pi_S(A)$ is the projector operator then you must remove the extra $A$ in your definition and write $\\Pi_S(A)A$ in your theorems. Please fix the inconsistency.\n-Theorem 1: Please define $\\rho(A)$ in the text before introducing the theorem. Don't simply refer to another paper.\n- Remarks 1 and 2 have very small font.\n- Theorem 2. The first sentence of the theorem can be removed.\n- Theorem 2. what do you mean by \"for any\" in the statement? If it is true for any epsilon, make it an equality on the lower bound value.\n- \"for the first time\" -> I would recommend to remove this phrase.\n- Conclusion: \"Our algorithm delivers a tight ...\" -> Why is the algorithm tight?\n- References: \"Global optimality of local search for ..\" -> fix the reference.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good contribution but need more clarification",
            "review": "This paper discusses sparse representation of models. The main algorithm is based on selecting items from the dictionary that best matches with the residual, and its error is proven theoretically. Thm1 starts with the error in the rank-1 case, and Thm2 extends to low rank case. This problem is important and if this work can beat the complexity and error of the previous literature.\n\nHowever, after reading the paper and supplement, I have the following concerns. I'm glad to raise my rating if the questions are addressed.\n\n1. I'd like to see more discussion about the result, since the lower bound on $\\epsilon$ (resulting in a lower bound of error) is a bit unusual. It would be great to see whether this number, and specifically whether $\\kappa(A)$ is small enough.\n2. Following the last point, it would be great to compare with the regularization algorithm, which uses $\\ell_1$ norm to encourage sparsity. In regularization algorithm, the RIP, which is the condition number of dictionary operated on all sparse vectors, affects the error. How does RIP compare with the $\\kappa(A)$ dependence here?\n3. About the proof in the supplement, I think the rank-1 case is easy to understand, but the low rank case is a bit hard. Is it essentially an easy generalization from rank-1 case? I feel that the proof uses an iterative method, each time looking at the remaining error residual and dictionary, and use rank-1 result for this stage and move on. However, does $A_K$ follow a recursive structure, or, can the support of $A_K$ be totally different from $A_{K-1}$? The proof heavily depends on $E_K$ but I don't see $A_K$ there. I'd also suggest that the notation can be more clear. Now $E_{\\bar K}$, $\\tilde\\mathbf{E}_{K}$, $\\mathbf{E}_{\\bar \\mathbf{k}}$, $\\mathbf{E}_{\\bar k}$ are confusing.\n4. I suggest introducing fewer notations such $A, K, M, N, \\kappa$ in abstract. It's more clear after they are formally defined. ",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}