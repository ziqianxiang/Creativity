{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The discussion with the expert reviewers reached the consensus that the paper lacks in novel *technical* contributions, and as such it does not meet the bar for a theory-oriented paper at ICLR."
    },
    "Reviews": [
        {
            "title": "Review of Paper 2092",
            "review": "This paper studies online learning under adversarial corruptions in three settings in the regret minimization setup. The first setting is the classical stochastic multi-armed bandit. The second one is contextual stochastic bandit, and finally, the third is learning in MDPs. In each setting, it is assumed that the rewards (and transition distributions) are corrupted by an adversary with probability $\\epsilon$, for some fixed $\\epsilon$. For each setting, the paper presents a no-regret algorithm that is robust against such corruption. \n\nMain Comments:\n\nThe paper studies interesting online learning problems, which parallels several recent studies on bandits with corruptions. The considered corruption model is however different from the ones in the existing literature. \n\nFor each of the studied settings, the paper presents a variant of the classical algorithm for that setting, which is tailored to be robust against $\\epsilon$-corruption. For example, for classical stochastic bandits, the paper presents a variant of UCB that achieves a sublinear regret against $\\epsilon$-corruption. This is a strong aspect of the paper, as the presented algorithms have similar designs and complexities to their corruption-oblivious counterparts.  \n\nThe paper is overall well-written and is clearly presented. The paper, however, could still benefit from polishing, and in particular, by fixing typos listed below. \n\nOn the technical side, I have some comments:\n\n- For the three settings considered in the paper, it turns out that the price to pay for being robust against $\\epsilon$-corruption is an additive penalty of $O(\\epsilon T)$ to the regret. This further implies that the interesting regime happens when $\\epsilon$ is $O(1/\\sqrt{T})$, or decays as $O(1/\\sqrt{t})$, since otherwise all the reported regret bounds would scale as $O(T)$ no matter how small a “fixed” $\\epsilon$ is. This way, Lemma 1 renders useless as it assumes a fixed $\\epsilon$.\n\n- The presented algorithms mildly require some prior knowledge of $\\mu_{\\max}$. While for bounded rewards, this is no longer the case for, e.g., Gaussian distributed rewards, as $\\mu_{\\max}$ can be arbitrarily large. This is relevant in particular for the contextual stochastic bandit setting. In addition, for classical stochastic bandits, UCB is presented for Gaussian rewards.   \n\n\n- In the stochastic bandit setting, a median estimator for mean rewards is used instead of the empirical estimates. However, it is not crystal clear to me where in the proof you used the properties of such a median estimator. Is it only in the concentration of good events?\n\n- It would be also tempting to use median estimators in the MDP setting too. Why did you use there only empirical estimates for mean rewards (and transition probabilities?)\n\n- There is a mistake in the analysis for the MDP setup (which is straightforward to fix): In the right-hand side of (31) and (32), a term of $\\widetilde {\\mathcal O}(\\sqrt{T})$ is missing; see the analysis of UCRL2 in (Jaksch et al., 2010). \n\nOverall I do not see the presented results of enough significance to make the paper eligible for ICLR. \n\nSome typos:\n\np. 2: depend on logarithmically in $T$ => logarithmically depend on $T$ \n\np. 2: let $i^*$ represents => … represent\n\np. 3: agent start => agent starts \n\np. 3: from true MDP => … the true MDP\n\np. 3: confidence intervals build => … built\n\np. 4: Is it known => It is …\n\np. 11: A recent of => A recent work of \n\np. 11: foe => for\n\np. 12: time spend => time spent\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New or improved results; writing could be better.",
            "review": "This paper studies regret minimizing algorithms for stochastic MABs, linear stochastic bandits, and finite-SA MDPs where epsilon-fraction of rewards/transitions are corrupted by noise of unbounded magnitude. In each case, the resultant regret bound has a epsilon-dependent linear term in T, in addition to standard T^0.5 term. In the latter two settings, the results are the first of their kind; in the first, they improve (in the linear term) over a known result. The general algorithmic approach here is to replace parameter/mean estimation with robust estimators (median, robust regression).\n\nStrengths:\n+ The regret bounds are novel, and offer an improvement upon known.\n+ The algorithms are fairly natural & modular.\n\nNot ideal:\n+ Lemma 1 could be more precise. In the current form, it is unclear if it is better or worse than epsilon * sigma * T. Sigma needs to be large enough for the proof to hold -- so comparisons to c*T remain uncertain. Similarly, at other places, the  writing could be a bit more rigorous.\n+ For stochastic linear bandits, typically the regret bound does not scale with the size (or number of vectors) in the decision set. The finitude of the latter is only needed for computational tractability. Why is such a dependence on K necessary here?\n+ Finally, could algorithms with tighter regret bounds (as in S^0.5, particularly) be adapted?\n\nTo what extent the work here is suitable for ICLR (finite SA MDPs, no func approx); I leave such considerations to the discretion of the other reviewers and the AC. My scores are independent of such aspects.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical model and techniques, could use some clarification on MDP algorithm and simulations",
            "review": "Summary:\nAs the title of this paper suggests, the authors look at online learning in three settings of increased generality (stochastic multi-armed bandits, linear contextual bandits and general MDPs), with the added feature that an adversary may corrupt reward (and in the case of MDPs, transition probability) distributions. Corruption may occur on a given turn with probability $\\epsilon$, thoguh in general, the degree of corruption an adversary can incur is unbounded. In this model, the authors study regret as the performance metric of their online algorithms, and they provide algorithms for each setting where their regret guarantees can be decomposed into a component which is comporable to state-of-the-art solutions in the no-corruption model, and a second term which depends on the corruption rate, $\\epsilon$, and quantified the degree to which ``power'' of the adversary can increase worst-case regret. \n\nTo give a brief overview of the algorithmic techniques presented, for the stochastic MAB setting, the authors propose a variant of UCB, whereby the baseline average representing historical performance of an arm is its median rather than the mean of its rewards (which makes sense as a more robust statistic when faced with corruptions), as well as an upper confidence bound similar to UCB. For MDPs, the proposed algorithm maintains a list of feasible policies (those with transitions and rewards ``close'' to what has been observed), and acts optimally according to this feasible set of MDPs (these details are still a bit unclear to me) for a certain number of turns, thus updating the set of feasible MDPs in the process. \n\nFinally, the authors validate their models using simluations on real-world data (road traffic routing in NYC where an agent has to pick a travel route from point A to B, coresponding to arms in the MAB), and on simulated data for the MDP (Erdos Reny Random graphs of 20 nodes where an agent has to find the shortest path of the graph). \n\n\nReasons for Score:\n\nI agree with the authors that it is relevant to explore adversarial information production in ML, and this model furthers our knowledge in terms of how to adapt successful online algorithms to adversarial scenarios. Furthermore, the fact that the paper covers online settings of increasing generality is of use in terms of techniques, and indeed the theoretical contributions are interesting in their own right.\n\n\nQuestions / Points of Discussion:\n-In general I think it would be great to hear more details on the linear contextual bandits algorithm at the crux of Thm 2. I understand that space is limited, but more details expanding on the line: \"Instead we build upon the recent work of Diakonikolas...\" could also help transition between the MAB to MDP\n-I wonder whether there can be more of a discussion on the dependence of $\\mu_{max}$ in your regret bounds. \n-I am still a bit confused on the MDP algorithm on page 6. First of all, you mention that $M_h$ is the set of all \"close\" MDPs to what has been observed, and then $\\pi_h$ is called the best policy that \"lies in $M_h$\". What does this exactly mean? The policy is not an MDP, but I'm guessing it is the best policy against a worst-case MDP from that family of consistent MDPs? This could be clarified a bit\n-In relation to this MDP algorithm, I am also curious what its runtime is. It seems to me that it would be computationally expensive to find say the best policy against a worst-case consistent MDP from $M_h$, but this could be clarified as well. \n-A final question for the MDP algorithm is also with respect to the initial sampling of all state-action pairs. What does the algorithm do if this is not possible (in a degenerate MDP), or if a single $(s,a)$ is only attainable with extremely low probability?\n-As for the simulations, I am curious why such a simple model of adversarial noise was used?:\n    -I like that the power of the adversary is bounded by $\\delta$, this should be further studied in the theoretical framework\n    -On the other hand, the fact that corruptions are either unilateral for all actions, or all but the best action (with a uniform distribution of corruptions), seems to be very limited in the scope of the theoretical framework. Could it be that the algorithms are performing well empirically because of this limited adversarial framework?  \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An incremental work which leverages known knowledge and techniques to prove refined regret bounds.",
            "review": "The work studies three online learning problems with corrupted rewards as the feedback. The three problems are the stochastic multi-armed bandit, the linear contextual bandit, and the reinforcement learning of the Markov Decision Process optimization.\n\nThe major contributions are three improved regret bounds for each of the problems, where the key to success is to replace the empirical mean in the arm/action selection scoring function by the median. Then, robust estimation bounds in the literature are leveraged to achieve the regret analysis. Experiments also support the theoretical findings.\n\nConcerns\n\nBesides the above positive contributions, following are some concerns:\n\n1. The target of optimization. All analyses bound the regrets with respect to the best arm/policy in the uncorrupted situation. This requires more elaboration to connect with the routing example in the introduction. If the actual routing time is the signal to measure the performance, then one does not need to take any ETA estimation into learning. If the actual routing time for the performance evaluation is the time affected by some noise, the regret definition makes no sense. It seems a reasonable situation is that the observations are corrupted but the performance feedback is not. The concern becomes stronger when it comes to the MDPs, as the trajectory of a policy in the corrupted environment will be different from that in the non-corrupted environment. It would be great if the authors can elaborate on the target chosen. Why is regret compared to the uncorrupted situation instead of the corrupted one? How does the trajectory from the corrupted observations affect or not affect the MDP analysis?\n\n2. The key modifications in the algorithms and the key inequalities in the regret analyses are from the literature. The analyses also mainly follow the similar steps in the previous works. Without observing novel technical contributions, the work is considered incremental.\n\n3. There is another work (http://proceedings.mlr.press/v32/seldinb14.html) also dealing with with the mixture of stochastic and the adversarial rewards which is worth mentioning or comparing.\n\n=====================\n\nPost Rebuttal\n\nI went through the authors' reply. My first concern is resolved by the reply. Form the authors' replies to all reviewers, I believe this is an incremental work. It is technically sound, but the lack of involved and novel technical contributions makes it more belong to an incremental work. Thus, I will keep my score unchanged.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}