{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes an optimization framework that automatically adapts the learning rates at different levels of a neural network  based on hypergradient descent.  The AC and reviewers all found the approach interesting and promising and appreciate the author feedback.\n\nWe strongly encourage the authors to incorporate the points and additional results provided in their response to the reviewers.\n\nAdditionally, some concerns remain to be addressed regarding initialization of hyper-parameters of combination weights. Specifically it would be important to further investigate the impact of such initialization on the optimization performance. Furthermore, additional experiments with other network optimizers would be valuable as pointed out in the reviews."
    },
    "Reviews": [
        {
            "title": "Ambiguous analysis and results from a novel method",
            "review": "Quality\n\n+ The authors have published code for replicability.\n+ The baseline evaluations are of reasonable quality.\n+ The proofs and theorems around convergence and complexity are high quality.\n- The results on their metrics are poor and ambiguous. They only evaluate on validation accuracy / cross entropy, which do not improve substantially.\n- Combination ratio results look inconsistent across datasets & models. \n- The analysis is limited and there are few generalizable insights to be gleaned from the paper.\n- There’s no transfer analysis or generality analysis, implying that each task will have to have its hyper-parameters tuned independently.\n- It’s not demonstrated that overparameterization is a problem. Networks are already overparameterized without issue.\n\nClarity\nTheir combination ratios plot is unclear. What should be taken away from these changes in Gamma? The analysis is unclear. Convergence analysis is unclear. There are plenty of typos. \n\n\nOriginality\n\nThe regularization methodology is somewhat novel, to my knowledge. Hypergradients are known and this is an extension to more parameters which will interact with one another. Multiple levels of tree based interacting hyperparameters is novel, to my knowledge.\n\nSignificance\n\nThis result isn’t a substantial improvement over existing methods. There aren’t clear insights to be gleaned that will generalize to other work on hypergradients (other than that it’s possible to try to regularize with a hierarchical parameter scheme).\nThis may be evidence that trying to tune every parameter simultaneously is overkill / too challenging.\n\n\n\nPros\n\nThe authors present a novel method for learning hyperparameters of a model at multiple levels which typically are not manually tuned. The potential upside of a working method here is high, as the learning rate dramatically impacts model performance. They publish code for reproducibility. The authors propose and implement a novel regularization method for their new hyperparameters as well. There are proofs backing convergence claims made by the authors. The baselines have been tuned and the presentation is honest.\n\nCons\n\nThe paper’s results are ambiguous. The paper isn’t carefully written - it’s unclear what ‘levels of adaptations’ refer to until after the introduction. It’s not clear what conclusions should be drawn from the combination ratios graph as gamma changes (Figure 1). The paper’s analysis value is limited, it’s unclear what solid & general insights about hypergradients can be taken away from it.\n\n\n\nOther feedback:\n\nTypo in the abstract: combination -> combinations\nTypo in the first line of introduction: is gradient -> is the gradient\nTypo in the second sentence of the second paragraph of the introduction: function -> functions\nTypo in the third paragraph of the introduction: horizon -> horizons\nTypo in first paragraph of Experiments: lowercase p in ‘The Proposed -> The proposed’\nLikely better not to shorten ‘Feed Forward Neural Network’ to FFNN in the section heading (though this is fine in the body of the text).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Limited novelty and weak empirical justification",
            "review": "Update: I really appreciate the response from the authors. Some of my original concerns have been addressed, and additional experiments help to show the benefits of CAM-HD, so I have increased my score to 5. But, after reading other reviews and responses, I still believe that this work needs to be compared to advanced learning rate adaptation methods. Most reviewers have pointed out the presentation and insufficient experiments, so it's better to submit the improved version to one of upcoming conferences.\n\n**Summary**\nThis work proposes an optimizer that adaptively determines a learning rates from different levels (global, layer-wise, parameter-wise) based on the hypergradient framework.  The proposed optimizer introduces many additional hyperparameters, and empirical evidence is not strong compared to baselines.\n\n**Detailed comments**\n\nThe proposed method adaptively adjusts learning rates at different levels (parameter-wise, layer-wise, and global). This needs to be compared to previous learning rate schedulers, but I found that only a basic scheduler has been compared. How about the performance of CAM-HD compared to previous approaches?\n* Using Statistics to Automate Stochastic Optimization, NeurIPS’19. \n* Statistical Adaptive Stochastic Gradient Methods, Arxiv’20. (This work is an extension of the neurips paper above).\n* Large Batch Training of Convolutional Networks (Arxiv’17). This work also proposes a ayer-wise Adaptive Rate Scaling.\n* SGDR: Stochastic Gradient Descent with Warm Restarts, ICLR’17.\n\nThis seems to be very sensitive to newly introduced hyperparameters (beta and gammas). This method requires extensive grid search to find a suitable beta, because the optimal beta is different across tasks (as in Table 1). \n\nThe proposed optimizer has been validated only on small-scale datasets. It’s difficult to predict how to behave in training on a large-scale dataset (such as ImageNet). In addition, I’m not quite sure that this approach works well with strong data augmentation techniques (for instance, auto augmentation). Also, I guess that the proposed scheduler will not be working well on large-batch settings (for instance, batch size >= 1K). \n\n“For the learning tasks with recommended learning rate schedules, we will apply these schedules as well.” -> In experiments, just a simple step decay lr rule has been applied. What about using cosine annealing lr schedulers? It could improve the baseline consistently.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposed a hierarchical learning rate setting method for network training.  ",
            "review": "Setting appropriate learning rate for network optimization is an important task in deep learning applications. This paper investigates the setting of learning rates for network parameters in different levels, e.g., individual parameter, each layer and global levels. By setting the constraints on the learning rates at multiple scales, the paper derived a hierarchical learning rate setting approach, which is the combination of adaptive learning rates at different levels. \n\nOverall, this proposed learning rate setting method seems to be interesting, however, I have some concerns on the setting of the hyper-parameters of the proposed method. \n\n1. The paper proposed to set the constraints at different levels by Eqn. (7). I have concern on how to automatically set the hyper-parameters of combination weights.  The details on setting / learning these hyper-parameters should be clarified in a more clear way. \n\n2. What are the possible reasons that the proposed hierarchical learning rate can improve the baseline optimizer Adam and SGD?\n\n3. The proposed learning rates setting method can be combined with any gradient-based network optimizer. More combinations and corresponding results and comparisons should be given to show how much the proposed technique can improve the different baseline gradient-based optimization methods.  \n\n4. The comparisons with more network optimizers should be given in the experiments. \n\n\n----\nPost rebuttal comments：\nThanks for the responses from the authors. These responses partially solved my questions. I think that the initialization of hyper-parameters of combination weights seem to be heuristic, and it is unclear on the effects/robustness of its initialization on the optimization performance. My questions on more comparisons and more combinations with other optimizers are not well answered. \n\nI also read the other reviews and responses, I agree with other reviewers on the concerns of experiments, justifications, robustness, etc. Considering that it needs some additional works to solve all these concerns, I suggest the authors to improve the paper and submit it to one following conference. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Extending hyper-gradient descent to many levels of learning rates",
            "review": "### Summary of the paper\n\nThe paper investigates the setting of hyper-gradient descent in the context of adapting learning rates at different levels in a neural network (e.g. per layer). The paper derives an equivalence between regularization of such learning rates and a weighted combination of non-regularized adapted learning rates. Experiments on small datasets and small network architectures are performed and show an improvement over baselines including Adam and SGD.\n\n### Main review\n\nAfter the introduction (which is well-written) I found the paper relatively hard to follow. Sections 2.2 and 2.3 are long and introduce a large variety of options that *could* be used without giving the reader a clear perspective of where this discussion is going. In the experiments, a lot of different combinations of small networks and datasets are used, and while the learning curves look promising, it is hard to assess how robustly the improvements may transfer to other settings, because e.g. the parameters $\\gamma$ are initialized differently in the experiments. The experiment of Fig.1 shows that the variation in the measurements can be very high with respect to $\\gamma$, such that the effect of the algorithm as such is a bit hard to assess. (And e.g. Fig.7 seems to indicate that the $\\gamma$s sometimes do not move from their initial values.) This makes me wonder how much of an improvement it is to go from optimizing hyper-parameters of e.g. SGDN or Adam to optimizing \"hyper-hyper-parameters\". \n\nWhen reading the paper I also wondered if any relation between the behavior of the adapted learning rates and other quantities in training (e.g. momentum per parameter) can be established. Does the adaptation counteract or increase momentum effects? Or does this depend on other quantities? Any such connection could provide insight and/or help with intuitions about the adaptation behavior. I did not see any attempt into such a direction in the paper.\n\nThe relation between regularization and weighting of unregularized values (Theorem 1) seems interesting.\n\nAs the experiments go as far as ResNet-34 on CIFAR10, it would be interesting if a comparison to existing results from the literature could be made. E.g. the accuracies in Fig.4 seem to go into the area of 92-93%, which seems appropriate for this combination, but it is hard to tell from the graph alone. (It would be even more impressive if a clear statement could be made that shows that the final accuracy is better than for the optimizers used in a work that is (close to) state of the art (under given constraint like e.g. architecture=ResNet-18).\n\nA (brief) discussion of the additional computational cost (in terms of memory and operation count) in the main body text would be useful. \n\n* Pros: The paper takes hyper-gradient to many new multi-levels.\n* Cons: The paper is somewhat hard to follow, and it is not immediately clear how transferable the experimental results are to other settings.\n\n### Minor details and comments\n* The font in the figures is very small in some cases, e.g. Figure 5 is very hard to read even when zooming in considerably on the screen.\n* Typos/nits:\n  * p.3 \"leave nodes\" -> leaf\n  * p.4 \"use each of them to updated the learning rate\" -> update\n  * p.6 \"tunning\"\n  * p.8 \"in later stage\"; \"our methods achieves\" -> method\n  * in several places: CMA -> CAM\n  * Lowercasing in reference titles, e.g. \"Rmsprop\", \"smd gain\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}