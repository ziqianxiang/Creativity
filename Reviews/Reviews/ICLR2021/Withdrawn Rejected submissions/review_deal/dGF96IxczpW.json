{
    "Decision": "",
    "Reviews": [
        {
            "title": "Introducing knowledge distillation-based ensemble learning into NMT",
            "review": "This paper introduces an ensemble learning framework into teacher-student training, to aim at training a single student model with  multiple teachers' signals. The authors introduced two different ensemble learning such as 1) word-level ensemble learning and 2) sequence-level ensemble learning. Now that we can apply the different decoding strategies in the sequence-level ensemble learning, the student model is expected to learn knowledge from multiple diverse teacher models. Experimental results on several translation tasks show that the proposed approach achieves substantial improvements over the baseline systems. \n\n- The paper is well written, and it provides a simple idea of extending the existing knowledge distillation framework by using ensemble learning. \n- How do you choose a hyper parameter alpha with 0.75 for WEL and 0.5 for SWEL? How sensitive are these parameters in your proposed method?\n- Table 3 makes me unsure what are the key factors in the proposed approach. There is no much difference among them. Is ensemble effect very limited in this task?\n- in Table 1 and 2, do you have the results of SEL baseline using either of 3L2R or 3R2L?\n- What is a student model architecture? Is the student model size equivalent to the other work? It is unclear that your experiments are conducted for fair comparison with the other work. Do the teacher/student size equal to the others?\n- The paper reports BLEU score improvement in each task. In terms of what, do your model translate better? Can you report the analyses on the outputs as well?\n\nTypos:\n- p.6 in table 1. -> in Table 1.\n- p.6 In table 2 -> in Table 2\n\n## After Author response ###\nI've read the authors' response and the other reviews. I agree with the concerns that the contribution of the paper is limited. As Reviewer #1 mentioned, this work might be good submission to other conferences/workshop such as WMT. Since the authors clarified my questions, I'd like to keep my score of 6, but am okay with not accepting the paper to the conference. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Could be a nice technical report, but requires further work",
            "review": "The paper investigates NMT training strategies with ensemble distillation. It considers two strategies: word-level ensemble learning (WEL) and sequence ensemble learning (SEL), as well as their combination. WEL is a distillation of average over teachers probability distribution, SEL uses translations produced by either each of the teachers or ensembles of teachers with the same decoding strategy. Note that SEL allows using teachers with different decoding strategies.\n\nIn this approach, to train one model:\n1) train N teachers\n2) for each teacher, translate the training dataset\n3) for each group of teachers with the same decoding strategy, translate the training data using this set as an ensemble\n4) train M new teachers using the new data (original+individual teachers+ensembles, >N times larger than the original dataset)\n5) use the new M teachers for word-level ensemble distillation on the new larger dataset\n------------------------------------------------------------------------------------------------------\nStrengths\n\nThe paper provides empirical results:\n1) for some methods from previous work: ensemble distillation, l2r and r2l models, ensembles of these, etc.\n2) for some of the combinations of these strategies, including the proposed approach.\n3) for L2R and R2L models combined in a sequence-level distillation.\n------------------------------------------------------------------------------------------------------\nWeaknesses\n\n1) Novelty of the approach is limited. Namely, word-level and sequence distillation are the standard methods that have been known for quite a while. For example,\n(i) Word-level Ensemble Learning (WEL) duplicates the approach from “Ensemble Distillation for Neural Machine Translation” by Freitag et al, 2017.\n(ii) Sequence-level Ensemble Learning (SEL) is the “Sequence-level knowledge distillation” by Kim & Rush, 2016 applied to several models.\n\n2) In light of the above, the main contribution is the specific combination of these well-known approaches. However, the improvement is rather small, but the approach is very heavy.\nThe proposed approach (see above) requires several stages of training different sets of teachers, and some of them on the dataset which is several times larger than the original. Compared to simple ensemble distillation by Freitag et al 2018 (one stage with one set of teachers, without translating training data), the improvement is 0.8 SacreBLEU.\n\n3) If we consider this as an investigation of different ways to combine the methods, there are not enough comparisons showing the effects of particular choices. For example,\n(i) in SEL, you use translations of single models as well as ensembles of the groups with the same decoding strategy. Does the latter help?\n(ii) at the second stage if distillation with WEL (with the new teachers trained on the increased dataset), you distill on this new increased dataset. To what extent is this better compared to using the original dataset?\n(iii) results for SEL are shown only for SEL (3L2R+3R2L). How is this compared to SEL (6L2R)? (The setup with the same number of models.)\n\n(Overall, if positioned as an investigation of the effects of different combinations, the paper should be structured differently with clear questions of the kind “How important is this, that and that” with the experiments targeting these questions.)\n\n------------------------------------------------------------------------------------------------------\nOverall recommendation\n\nI think this can be a good technical report showing the effects of different combinations of word- and sequence-level distillation methods (after answering the questions above). In this case, it would be perfect for venues like WMT, but I can not recommend accepting it to this conference.\n\n------------------------------------------------------------------------------------------------------\nUpdate after author response\n------------------------------------------------------------------------------------------------------\n\n1(i) - thank you for clarifying! \n\n3. I appreciate you added the comparisons.\n\nNote that my suggestions in 3 were in case this paper is positioned differently: an empirical investigation of combinations of existing methods rather than a new approach. But to position the paper in this direction it would require much work, and this is not something that can be fixed with a couple of new results.\n\nOverall, my feelings have not changed and fully agree with those of Reviewer 4. Namely, the proposed approach is not a novel framework but something already used in WMT shared tasks. The empirical results can be of interest if the paper is rewritten as an investigation of different combinations of existing methods, but this would require a substantial amount of work.\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Knowledge distillation and model ensemble are both utilized in NMT model training. ",
            "review": "This paper introduces the approach of distilling the student model using the ensemble model as teacher model, and claims to integrate the two SEL and WEL distillation methods into the same framework. The paper is simple and easy to understand. There are some performance improvements in the baseline model of machine translation. However, the contribution of this paper is incremental, and there are several defects as follows:\n\n1. Large amount of computation and memory requirement: it is claimed in the abstract that the traditional model ensemble has a large amount of calculation and memory requirements, which does exist. However, in the framework proposed by this paper, this problem still exists, which only moves the requirement from the inference stage to the training stage. In the framework proposed in this paper, in the training phase, multiple teacher models need to decode for each example of the training set in both WEL and SEL, which will greatly extend the training time of the model and the demand for computation and memory. The only possible significance is that the inference consumption of the online system is lower than that of the ensemble model, which is equivalent to that of the baseline model.\n\n2. Teacher models with different decoding strategies: in SEL, the synthetic translation sequence obtained by ensemble models are used to form a new pseudo parallel corpus with the original training data input. Although this paper forcibly package it as knowledge distillation, in fact, it is only a data enhancement method relying on model ensemble, which has nothing to do with knowledge distillation in a narrow sense. Moreover, the problem of inconsistent decoding strategies discussed by the author has not been solved. It is also necessary to assign the teacher model with the same decoding strategy to the same group for integrated generation, so pseudo parallel corpus produces multiple repetitions.\n\n3. Neglect the important related work: the WEL mentioned in this paper has been a very common means in the knowledge distillation, using the ensemble teachers to guide the student model training; and SEL is a commonly used data enhancement method such as self-training/tri-training, and has been widely used in NMT, especially in WMT competition. And SEL as a data enhancement method needs to compare a large class of important work is back-translation, also as a very important data enhancement method. SEL takes synthetic sequence from the ensemble model as the target, while back translation takes it as the source.\n\n4. Inappropriate SOTA claims: on WMT14 en-de, SOTA results are obtained by (Edunov et al., 2018). As a data enhancement method, better results can be obtained by using model integration and knowledge distillation. Therefore, what needs to answer is how the performance will be compared with the additional corpus of the same size as (Edunov et al., 2018), back-translation or SEL.\n\nGenerally speaking, the paper overpackaged the extensive data enhancement (SEL) and model enhancement (WEL) methods, lacking novelty and ignoring some important works related in the machine translation field, and the result did not reach SOTA claimed (not important). The proposed SWEL is not a new framework. As a commonly used method in the WMT competition, it can not bring something new to the machine translation community. A large amount of computation and memory requirements in the model ensemble have not been alleviated, and different decoding strategy models are still unable to be integrated together. They are only used as real targets to alleviate the exposure bias problem. Therefore, this paper needs to further clarify the real contribution.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Generally good paper, some questions on experiments",
            "review": "This paper proposed to combine sequence-level distillation and word-level distillation to train student MT models. The sequence-level distillation is done by applying single-model approximation over L2R and R2L models. The authors further train three copies of the sequence-level distilled models with different random seeds. Then with these three sequence-level distilled models, the authors train the final student model with word-level distillation.\n\nBoth the sequence-level distillation and word-level distillation are established techniques and can be seen in many papers. The contribution of this paper is about the combination of sequence-level distillation of models with different decoding strategies, followed by word-level distillation. As the paper seems to be well written, my decision lean to an accept.\n\nOne concern for this paper is in the experiment. The main cost of ensemble distillation is the training time. For training the SWEL model, are you fixing the maximum iterations even when the dataset grows larger?\n\nAs the SWEL is distilled from 3 SEL models, the training data size shall be 9x of the original data size (According to Alg. 1). If the training time grows with the dataset, the total training time is 6 + 4 * 9x = 42x longer  (6 teacher models, 3 SEL models + 1 student model for distillation, assuming that you train them sequentially) than a vanilla model.\n\nPlease correct me if this analysis is wrong.\n\nTherefore, I feel it is necessary to clarify the time cost for training a SWEL model. If it is indeed 42x longer than a vanilla model, then the comparison is extremely unfair for WEL(3L2R) baseline and SEL (3L2R + 3R2L) baseline. Here, the training time of WEL (3L2R) shall be 4x of vanilla. If this concern about the experiments is solved, I will raise my score from 6 to 7.  \n\nComments:\n- May be the performance drop by including NAT models suggests the importance of adjusting the weights of teacher models.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}