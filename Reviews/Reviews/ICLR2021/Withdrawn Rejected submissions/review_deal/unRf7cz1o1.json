{
    "Decision": "",
    "Reviews": [
        {
            "title": "Blind Review",
            "review": "## Summary\n- The paper proposes a new approach for model inversion attack i.e., given a  a white-box model, to recover data representative of training instances.\n- They approach involves extending GMI (Zhang et al. 2020) with improvements (a) to their GAN training objective and (b) reformulating the data recovery step.\n- The approach is evaluated on a variety of datasets -- CelebA, MNIST, CIFAR10, Chest X-Ray and is also accompanied by ablation studies.\n\n---\n\n## Strengths\n\n**1. Significant improvement in performance**\n- Although the approach here is a sequence of simple extensions, the improvement quantitative results the paper demonstrates are remarkable e.g., 21% -> 72% accuracy on CelebA.\n- Furthermore, the ablation study further highlights the importance of each extension.\n\n**2. Insights connecting to other attacks**\n- I also like some of the insights used in the paper to improve results. For instance, training the discriminator on soft-labels from public data is connects to recent flavours of model stealing attacks, where this strategy has shown to be quite effective.\n\n**3. Experimental depth**\n- Kudos to the experimental depth of quantitative results in the paper: the authors demonstrate results on multiple challenging scenarios, multiple datasets, etc.\n\n**4. Writing**\n- The paper is well-written and easy to follow.\n\n---\n\n## Concerns\n\n### Major Concerns\n\n**1. Evaluation protocol**\n- I have a series of related concerns/questions on the evaluation protocol used in the paper.\n- (a) #generated images per class and diversity: What is the test set used to compute the attack accuracy? Would it be 5 images per class * $K$ classes? Wouldn't this be quite small for some of the datasets e.g., 50 test images for MNIST? I also wonder what is the diversity/consistency among these recovered instances per class?\n- (b) Evaluation classifier: Are all the evaluation classifiers trained only on the private classes, or private+public classes? For instance, does the CIFAR10 evaluation model to compute attack accuracy have 5 or 10 output classes? Because if it's only the private classes, I imagine the promising results might be an artifact of the approach generating instances from a closely-related, but not the same class.\n- (c) The previous point also makes me wonder if it is possible that the approach generates instances from public data (e.g., closely related identities) for a similar target identity, instead of truly recovering the person identity. After all, this appears easy, esp. for face images, given that public data is a sizeable pool of ~30k diverse faces. One way to verify this would be to use as baseline recovered images KNN instances of private classes from the public dataset. This would greatly help clarify whether the model indeed inverts the target class or just synthesizes similar classes encountered in the public dataset.\n\n**2. Performance drop with distribution shift**\n- My concern in the previous point is reinforced by observing the results in Table 3, where there is a significant performance drop when the public data is slightly different from the private data. I am surprised that in spite of both public/private data being face images there is a drastic drop in performance.\n- Could the authors comment on whether they noticed similar drops with MNIST/CIFAR10? A suggestion here would be to use Fashion/CIFAR100 as public images.\n\n**3. Qualitative results**\n- While the quantitative results are highly encouraging, I was disappointed that the qualitative results are restricted to a handful of images in Figure 2.\n- I am especially curious to know how does the distribution of recovered images look for a target class -- Are they diverse? Do the set of images display consistent statistics to the identity (in case of face images)?\n- I would really appreciate if the authors included qualitative results over randomly sampled target classes and if possible, nearest neighbour image from the public pool of images.\n- It would also help demonstrating qualitative results on MNIST/CIFAR where it is easier to assess whether the target class was recovered.\n\n### Minor Concerns\n\n**4. Distribution recovery**\n- I am slightly unclear in distribution recovery -- how was z sampled to produce images G(z) in the previous step? Was it from an isotropic Gaussian? Because if this is the case, I am surprised that the $\\mu, \\Sigma$ extracted in this step results in something meaningful.\n- Have the authors analyzed the distribution $\\mu, \\Sigma$ recovered for a particular target class?\n\n### Nitpicks\n\n**5. Entropy regularizer**\n- While I understand the reason behind the entropy regularizer, I am surprised this simple trick provides such a performance boost. However, in line with many works around confidence calibration/out-of-distribution detection, I imagine that the predictions probabilities are not uniform (i.e., with max entropy). A suggestion here would be to encourage G to produce similar prediction statistics as T on public data rather than max entropy predictions.\n\n**6. Writing**\n- Typos: \"datasete\", \"K-Nearesr\"\n- \"mislead target classifier\" (Page 7) - I found this confusing because the goal is to simply recover the correct target class, rather than \"mislead\"",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "review",
            "review": "Summary:\nThe paper aims to show that models memorize enough information about the training data that can be utilized to extract private information contained within the data. Through improved attack algorithms, the authors enhance the performance of model inversion to demonstrate it to be a realistic threat. \nThe paper builds on recent work (Zhang et al., 2020) and makes three modifications:\n(1) soft-label discrimination (SD), (discriminator that is able to differentiate not only real data from the fake but the class labels associated with the target network.)\n(2) entropy minimization (EM), which minimizes the prediction entropy of images produced by the generator, and \n(3) distribution recovery (DR), which explicitly models and estimates the private data distribution.\nThe authors show strong improvements on the 'accuracy' metric used for evaluation of model inversion.\n\nPros: \n1. The proposed approach helps bring significant improvements in runtime for the model inversion attack. \n2. Experiments are performed over multiple image domains: face recognition, digit classification, object classification, and disease prediction.\n3. Proposed techniques are well motivated and result in strong improvements on the attack accuracy metric.\n4. The public data, throughout the experiments, do not have class intersection with the private training data of the target network.\nIn short, this work will act as a good starting point for further research in model inversion, detailing good practices.\nHowever, I have my concerns w.r.t. the type of model inversion achieved by this work, detailed in Cons..\n\nCons: \nMy major concern is that it appears that the proposed methods effectively act as an adaptive technique to improve on an specific evaluation metric -- 'acc' and 'acc5' by specifically modelling class information within the discriminator. It is not clear however that this metric is a valid metric to optimize.\n1. What does it mean to be able to reveal private information? Will you call it a privacy infrignement if you get access to a photo of the celebrity that is 'private' to the dataset, or when you can generate 'similar' images. This work does not achieve the former. Consider scenarios such as medical record extraction (or the chest xray dataset) where privacy violation is a tangible threat: in such scenarios it is important to be able to extract the 'exact' record to have any meaningful privacy considerations. I would like to see some samples for images extracted from the disease prediction dataset.\n2. There are very large drops in case of distribution shifts. The only time that model inversion shows reasonable results is when the exact same data distribution (without overlapping classes) is fed to the generator during training. These results make it hard to foresee the utility of model inversion in practical scenarios. \n3. KNN distance is nearly the same. This suggests that the added techniques are not able to improve upon the idea of extracting 'memorized' training points. Seems like a good method to artificially generate images that can 'fool' the target network (or another classifier trained on the same dataset). But I am not sure if this is what the starting premise of the paper was: \"Evaluating the MI attack performance requires gauging the amount of private information about a target label leaked through the synthesize images.\" \n4. Insufficient details for reproducibility. (See questions..)\n\nQuestions and Suggestions during rebuttal period: \n1. Are the individual images in Figure 2 randomly picked? What is the diversity of images within a class? Say 100 images from the same private class.\n2. What are the training and test accuracies of the target models? It seems that they overfit since the subsets of datasets provided for their training are quite small. Is memorization necessary for this attack to be practical?\n3. Please define p_gen in the paper.\n4. There is no information about internal activation in GAN training and what is the motivation for the same. Was standard training unstable? I assume that since the discriminator is trained with a custom loss this may not be needed and coupling a given example with a random seed.\n5. Information about the optimization procedure for Step 2 is lacking. How is the optimization problem solved post monte carlo estimation of the loss? How many samples do you collect for monte carlo estimates (L)? How is this number chosen?\n6. What about the parameters for distribution recovery like $\\lambda_i$, did you perform a structured search to set it. Does the same parameter work across all datasets and domain shifts?\n7. Intro could be more descriptive of the proposed techniques and their motivation.\n\nTypos: Paragraph 1 on page 6, Bullet 2 page 6. Please use `` for inverted commas in the paper.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes an interesting extension to the previous state-of-the-art method. However, the quality of the manuscript needs substantial improvement.  ",
            "review": "This paper proposes extending a previous state-of-the-art method generative model-inversion (GMI) with additional constraints that help in making the attack stronger. This is done by including soft-label discrimination (SD), entropy minimization (EM) and distributional recovery (DR). \n\nStep 1 of the proposed attack algorithm involves training a generative model is trained to mimic a public dataset. Step 2 involves optimizing with respect to the input latent vector to generate target class specific images. \nWhile SD and EM in Step 1 are a natural extension for GMI, to bring the distribution of the generated images closer to the target private distribution. DR on the other hand is introduced in Step 2 to estimate mean and scale of the latent vector to generate the class specific images as a distribution.  \n\n\nWhile the ideas are certainly interesting, I have a few concerns with the quality of the paper. \n- There are mistakes in the paper, especially with defining equations. \n— In Page 3, Step 2 part of GMI (referred in this sentence as existing work), optimization is defined as $max_{x=G(z)} D(x) + log Ty(x)$. However, GMI paper optimizes with respect to z (latent space) and not x (image space).  \n— The line before Eq. (6) describes an optimization problem to estimate $\\mu$ and ${‎‎\\sum}$.\nHowever, in Eq. (9) it is written as $A$ and $b$ and it is not clear from from Eq. (9) how $A$ and $b$ are initialized. $A$, $b$ and $\\mu$, ${‎‎\\sum}$ are interchangeably used throughout the paper.\n— the target network is defined as $f$ in Section 3.1 and later referred to as $T$.   \n \n- The ablation study describes using SD alone vs using other combinations. However, it would be beneficial for the reader to also know the impact of using EM alone or DR alone. \n\n- There are several grammatical mistakes in paper. Here are a few (not limited to): \n— For ChestX-ray8, we 10,000 images with label \n— CIFAR10 datasete \n— K-Nearesr Neighbor Distance (KNN Dist).\n— One possible reason is that images in FaceScrub has much lower resolution (64 × 64), and there are quite a few images are under poor lighting condition or only show part of the face.\n\n- Any claim made in the paper should be validated in the form figures or tables. However, the below claim is made without any proofs. \n—“A specific example is that GMI generates ”7” when attacking digit ”1”, which the generated sample can achieve a very low identity loss under target network. Our method can overcome this problem to some extent and has better performance. “\n\n- It is generally a good practice while reporting performance in Tables to indicate for every metric used whether high is good (e.g. Accuracy) or  low is good (e.g. KNN distance). \n\n- The references are poorly formatted. Any publication should be assigned to the respective venue including page number if applicable. \n—Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium, 2018.\n—Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks, 2019.\n\nIn view of the above mentioned reasons, the quality of the manuscript has to be substantially improved to warrant a publication. Hence, I vote to reject the paper.   \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper with some evaluation concerns",
            "review": "This paper proposes better techniques for model inversion attacks on computer vision models. The authors propose to train a GAN to mimic the distribution of inputs of a targeted classifier, with the help of public data from a related domain. The proposed approach is shown to outperform a prior approach along a number of metrics over multiple datasets.\n\nThe goal of model inversion, in particular in the context of facial recognition models, is well motivated. One aspect of the threat model that I would maybe express somewhat differently is the white-box access to the classifier: if a model is indeed trained on private data, it is unlikely that an attacker would get full white-box access to it. But assuming white-box access can still be useful to show an \"upper-bound\" o the kind of information that can be extracted.\n\nThe idea of tailoring the GAN to the specific model to be inverted is interesting. But I had a harder time following the intuition and motivation for the exact approach proposed in this paper. \nFor example, it isn't clear to me why encouraging the discriminator to predict the target model's soft labels should necessarily help with model inversion.\nMoreover, isn't the entropy regularization term in (5) essentially enforcing the same thing as the original MI optimization (max_x log T_y(x)), which is claimed to be inappropriate because of semantically meaningless local minima?\n(just to be sure, shouldn't p_disc in (5) be replaced by the target network T?. As written, the entropy regularization is over the discriminator's outputs rather than the target network's).\n\nI have a number of concerns about the evaluation procedure. First, while model inversion is well-motivated for facial recognition, where every class corresponds to a distinct individual, it isn't clear what one should make of model inversion attacks on e.g., MNIST, CIFAR-10, or ChestX-ray8 (for the latter, the classes are different diseases rather than different individuals).\nIt also isn't clear to me why the proposed evaluation metrics are necessarily the right ones, from a privacy perspective:\n- Attack accuracy measures whether reconstructed images from each class have some predictive features of that class. But this does not mean that these features are necessarily semantically meaningful and thus indicative of a serious privacy threat. Since you use a different classifier for this, you make sure that these features are \"transferable\". But we know from the adversarial examples literature that such features are not necessarily semantically meaningful either.\n- Similarly, KNN and FID measure whether reconstructed images have similar features as real images. But it isn't clear what private information is necessarily leaked if these metrics are low. It is well-known that \"inverting\" a representation (i.e., finding an input x' that has a similar representation as x) typically leads to semantically meaningless outputs.\n\nTable 2 and Figure 2 illustrate this point quite well: while the proposed approach always outperforms GMI across all metrics, I have a hard time assessing, from Figure 2, which one of the two approaches results in qualitatively worse privacy breach. The attacks seem able to reconstruct the gender of each class, but beyond that it's hard to conclusively say whether the attack is supposed to be successful or not.\nIt seems to me that some form of human study is necessary to appropriately evaluate these type of attacks. E.g., given real pictures from different individuals, and one reconstructed image, could human users recognize the correct user (i.e., some sort of \"police sketch artist\" experiment).\nSuch a study would be extremely valuable to get a better sense for the actual threat posed by these types of attacks, and would also make comparisons between attacks more meaningful. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}