{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review",
            "review": "## Overall summary\nThis paper aims to produce a good neural representation for source code that can be used in downstream tasks such as summarizing source code into natural language (comment generation and method name prediction). The authors construct a directed graph from an AST, by taking the nodes from the AST and adding syntactic role of each node as its label (statement, argument, etc), and adding edges which from the tree as well as \"NextToken\" edges in the order of the program text. The Heterogeneous Graph Transformer from existing work is applied to this graph. The paper reports experimental results on two existing datasets and tasks (comment generation with CoNaLa, method name prediction from the Open Graph Benchmark).\n\n## Strengths\n- The paper demonstrates stronger empirical results compared to baseline methods.\n- The paper provides an ablation study which shows that the empirical results decline when certain features, like node types in the graphs, are removed from the method.\n\n## Weaknesses\n- The paper is a straightforward application of the Heterogeneous Graph Transformer to the code-related experimental tasks. The way that the graphs are constructed for HGT also does not seem particularly enlightening, in that it takes the existing Python ASTs output by the official [ast module](https://docs.python.org/3/library/ast.html) and adds NextToken that were proposed in prior work. This seems like less than the expected amount of technical content for a typical paper at ICLR.\n- One of the prior work comparisons (against TransCodeSum) is based on a reimplementation, a different dataset, and a different preprocessing method for identifiers, which is not as indicative as comparing to the prior published result. It would be best if it were possible to compare using the same dataset and experimental setting as the prior work.\n\n## Recommendation\nI vote to reject as I believe the paper does not contain enough contributions for an ICLR paper.\n\n## Comments\n- I don't think it's true that existing works \"only consider node type in the initial node embedding and neglect their differences in the message passing (Gilmer et al., 2017) step\".  For example, in Equation 2 of https://arxiv.org/pdf/1805.08490.pdf, both the node's type and the edge's type is used in message propagation. \n- $\\mu_{\\langle \\phi(e) \\rangle}$ didn't seem to be defined in the paper. I read the HGT reference to figure out what it was. If the goal of the paper is to be self-contained with respect to HGT, then all such functions should be defined.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "Summary\n-------------\nA method to learn program representations using heterogeneous graphs is presented. Programs are first transformed into heterogeneous graphs, where node types are determined by the language parser and edges come from the syntax tree and a \"NextToken\" relationship, with fine-grained distinction of different edge types. These are then processed using the existing Heterogeneous Graph Transformer (HGT) model. Experiments on two tasks (code description generation & method naming) indicate that the approach outperforms baselines.\n\nStrong/Weak Points\n-------------\n* (+) The idea of using more readily available information about the input programs (such as node types) is natural and well-introduced.\n* (+) The experiments show some improvements and test some reasonable model ablations.\n* (-) Very limited novelty: as programs-as-graphs+GNNs comes from Allamanis et al. and the HGT model comes from Hu et al., the contribution of this paper is limited to using node types (and more edge types) than Allamanis et al.\n* (-) CoNaLa is a very noisy dataset for experiments on models of code, as the program snippets are very short and not representative and the dataset is small. It was designed as a _code generation_ benchmark task. The value of ablation experiments on this dataset for a code representation model is hence questionable. Comparing with Allamanis et al. on the VarMisuse task use would be much more meaningful.\n* (-) The experiments do not analyse if results improve because of the additional information in the graphs (node types / more edge types) or because of the higher capacity of the HGT compared to baseline models. This could be tested using further experiments:\n * A run using the same (K/Q/V/M, A)_Linear for all node types in Eqs. (3), (4), (5), (8) would provide information if the additional capacity (and how much) helps.\n * Similarly, replacing W^(ATT/msg)_\\phi(e) in Eqs. (2), (6) would provide similar information about the edge types.\n\nRecommendation\n-------------\nGiven the very incremental nature of the submission, I would only recommend acceptance for publication given strong and substantial experimental evaluation. As I am missing ablation experiments on a more meaningful dataset, I feel this paper is currently not ready for acceptance.\n\nQuestions\n-------------\n* Did you run any other experiments / ablations (see above for some suggestions)?\n\nDetail Feedback\n-------------\n* I found the description of HTG very confusing:\n - Eq. (1): What is the softmax performed over? Not the different heads, I assume?\n - Eq. (1): What is $\\mu_{\\langle\\phi(e)\\rangle}$?\n - Eq. (7): What is the shape of the results of Attention / Message? I can kind of guess how things are supposed to fit together, but this is not clear\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Good empirical results but minor machine learning contributions",
            "review": "This work presents an application of heterogeneous graph transformers (HGT) to program graphs. By encoding fine-grained node and edge-type information over syntax tree child-parent node relationships, HGTs perform better compared to baselines on summarization tasks (method naming, descriptive comment generation).\n\n(+) A reasonable combination of HGTs with program graphs based on ASDL.\n(-) Minimal machine learning contributions. HGT is a contribution of previous work and the tree representation is quite similar to others. Novelty is using fine-grained edge types available in programming language grammars.\n(+) Good empirical evaluation results, with noticeable improvements.\n(-) Evaluation only on one kind of task (summarization-style)\n\nAlthough the work presented in this paper is technically correct, the contributions do not seem entirely relevant to ICLR. This work tweaks a graph representation of code, and uses a pre-existing GNN (HGT) for a known task. Arguably, this work would be better submitted to a conference relevant to the target application (e.g. software engineering, programming languages), where its contributions would be better evaluated. Despite this, I would be okay with this paper to be accepted, with a heavily rewritten Sec 3.2.\n\n\n### Other comments & Clarifications\n\n* The description of HGTs (Sec 3.2) is confusing with undefined symbols and notation. Completely rewriting this would be necessary.\n* What is the performance (computational speed/memory) penalty for HGTs compared to other models?\n* How/where were hyperparameters for HGT/baselines tuned? Some of the differences are small enough that _could_ be explained by such hyperparameter differences.\n* How are the initial node representations computed? Is an embedding computed per-identifier? Are the identifiers split into subtokens/BPEd as in Fernandes 2019/Ahmad 2020?\n\n##### Typos\n* \"Ablatioin Study\" (p7)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting, but the main claims are unclear and ignore previous work",
            "review": "## Summary ##\nThe paper proposes an approach for learning and modeling programs.\nThe authors argue that existing work on modeling source code \"neglect an important aspect ... the different types of nodes and edges\", and propose heterogeneous graphs, to consider node and edge types.\nThe approach is evaluated on method naming and code comment generation.\nThe use of heterogeneous graphs is an interesting direction, but it is unclear if it really provides any benefit.\n\nOverall, I think that the claim that \"existing work neglects the different types of nodes and edges\" is quite harsh, as detailed below. Since this is the main motivation and claim of the paper, and this claim is not properly empirically evaluated, I currently vote for rejection.\n\n### Details ###\n1. The main motivation of the paper is to use node and edge types for representing programs. Please correct me if I didn't understand this correctly, but don't *all* existing work already leverage node and edge types (even works that this paper already cites)? \nFor example:\n* Allamanis et al. (ICLR 2018) proposed 8 syntactic and semantic edge types such as \"LastWrite\" and \"LastLexicalUse\".\n* Alon et al. (ICLR 2019) used node embeddings for different AST node types.\n* Brockschmidt (ICML 2020) represented node types using character-level convolutions and used edge types in Relational GNNs (like Schlichtkrull 2018).\n* Alon et al. (ICML 2020) used the order of a child node among its siblings (i.e., 1st, 2nd, 3rd, etc.) to distinguish different child nodes of the same parent.\n* Hellendoorn et al. (ICLR'2020) used semantic edge types as relative embeddings in transformers (like the relative positional embeddings of Shaw et al., NAACL 2018).\n\nSpecifically, the paper also argues that \"previous GNN-for-code works (Allamanis et al., 2018, Fernandes et al., 2019) did not use AST edge types\". I don't think this is correct. A large part of Allamanis's 2018 paper is about edge types, including an ablation study on the subsets of included types.\n\n2. Evaluation - the evaluation is mostly presented as ablations of the main model, instead of directly comparing empirically to any of the above baselines. If I understand correctly, the authors did not use the original implementations of any of the above papers. \n\n3. When proposing new *structural* models of code, it's also very important to compare them with strong *sequential* models, i.e., Transformers and LSTMs (with attention, copy mechanism, and all possible improvements) that work on the sequence of tokens. These are required baselines, to verify that the structured approach provides any benefit. \n\n### Questions for Authors ###\n1. When considering only the neural architecture and ignoring the actual choice of node and edge types, how do heterogeneous graphs differ from the relational GNNs of Schlichtkrull 2018, with a GAT as the GNN type, instead of GCN?\n\n2. When considering only the node and edge types (ignoring the neural architecture) - how do the edge and node types used in this paper differ from the above previous work? Does this model use new types that previous work didn't?\n\n### Improving the Paper ###\nTo improve the paper, I advise the authors to:\n1. Use the remaining page and the 9th extra page (the paper currently has 7.25 pages) to directly compare their model to existing models.\n2. Elaborate on the conceptual differences between heterogeneous graphs and R-GNNs (like Schlichtkrull's).\n3. Provide examples for predictions made by their model and the baselines, to provide intuition for when and how their approach provides a benefit over existing models.\n\n### Minor questions and comments (did not affect score) ###\nTypo in Page 7, \"Ablatioin Study\" -> \"Ablation Study\"\n",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}