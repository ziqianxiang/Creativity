{
    "Decision": "",
    "Reviews": [
        {
            "title": "This paper is a little out of my area but it is well written and the technical contributions are clear.  I am not convinced by the final results as the performance seems to have a large variation in Figure 6 and the extra computation may not be worth the improved performance.",
            "review": "The authors propose an ensemble approach to the problem of neural network predictions in distributions that differ from the training distribution.  The ensemble is combined via a principled manner based on model uncertainty.\n\nThe paper is generally well written and the approach is clear.  The problem of building robust models to differing application and training domains is important and well motivated.\n\nUsing ensemble methods to improve robustness is not new, but the methods of combining the predictions based on uncertainty estimates is interesting.\n\nIt would be interesting to see the approach extended to classification problems as well since changing viewpoints and lighting in application that differ from the training set can lead to poor classifications at times.\n\nThe error bars in Figure 6 suggest the performance of the proposed approach is not consistently better than deep ensembles or the other baselines.  Can the authors comment on this?  This may preclude the adoption of this approach.\n\nThe additional computation needed to alter the images and process the different paths would require the results to be significantly improved over previous approaches that require less computation.  It would be helpful if the computation speed compared to other models is provided, possible in fps.\n\nIt would be nice to see an ablation study showing different combinations of the pathways to have an understanding of the contributions made from each path to the final performance.\n\nrecommendation and reasoning\n\nThis paper is a little out of my area but it is well written and the technical contributions are clear.  I am not convinced by the final results as the performance seems to have a large variation in Figure 6 and the extra computation may not be worth the improved performance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Restricted novelty and setup combined with inconclusive experiments",
            "review": "**Summary and contributions**:\n- The authors propose a test time ensemble more robust to domain shifts in the input, by using multiple different intermediary domains, which reacts in a different way to the variations in the input distribution. They test over natural and synthetic datasets.\n- The direct and 1hop pathways are combined using a weighted average ensemble, where weights are based on each pathway uncertainty. The uncertainty is learned, using synthetic corruptions for an input image. \n- The claims that the uncertainty generalizes over domain shifts and that this way the final ensemble improves the results under this problem are not experimentally proved. The differences between the baselines and the proposed method are minor.\n\n\n**Strengths**:\n- The work discusses a very relevant (mostly neglected) aspect in cross-tasks setups: the domain shift problems when using multiple tasks (learned or fine-tuned on different input distributions).\n\n**Weaknesses**:\n- The setup is very restricted, it is not proven that outside the common corruption set (on which the uncertainty is trained on), this will generalize over other domain shifts.\n- The idea is very simple, mainly proposing an ensemble over a prior work X-TC (Zamir et al.), where the weights are learned based on a set of augmentations (sigma train). The authors train those uncertainty weights against some corruptions and afterward prove that their model is resilient to the same ones, on a different dataset, compared with methods that were not trained to understand those corruptions. \n- The results do not sustain the claims, but rather a very restricted task setup (to a common set of augmentations).\nFor all baselines (except for the “Blind Guess” one), the direct loss l1 (both on Taskonomy and Replica dataset) are almost the same, proving that even with a more general setup, the results wouldn’t have been conclusive.\n\n**Quality**:\nThe method is not sound for the above-mentioned problems.\n\n**Clarity**: \nThe paper is hard to read. The ideas are not clearly expressed and the phrases seem rather hard to follow. The ideas could be more distilled, rephrased.\n\n**Novelty**:\nThe novelty is very limited. The work is similar to X-TC from content organization, experiments, examples in the figures, to principles/ideas and conclusions point of view. [A] also shows an ensemble point of view in the cross-task formulation.\n\n**Significance of this work**:\nVery simple idea (to learn an uncertainty for guiding a weighted average ensemble), mostly using existing results from X-TC. The results are not convincing even for the given setup.\n\n**Typos/Others**:\n- Figures have very small/hard to read the text.\n- Table 1 is hard to read (should have top1 entries bolded, or top3 entries colored)\n- “Notations:” Z and Y intersection must not be void. Does this mean they should have common elements? I think this is a typo.\n- ‘constraints”Zamir’ missing a space\n\n\n[A] Semi-Supervised Learning for Multi-Task Scene Understanding by Neural Graph Consensus, Marius Leordeanu, Mihai Pirvu, Dragos Costea, Alina Marcu, Emil Slusanschi, Rahul Sukthankar\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review (Reviewer2)",
            "review": "**Summary:** The work proposes a method that allows to build ensembles that are more robust to a dataset shift, comparing to conventional ensembles. The main idea of the proposed method is simple, but yet elegant; The (corrupted) input x is mapped to a set of middle representations (e.g., \"wavelet, 2D edges, greyscale, and emboss\"). Then, different models are used to predict the target of the original problem and uncertainty in the predictions based on the middle representations. The core hypothesis is that different corruptions affect middle representations with different strengths (e.g., color corruptions will have less influence on greyscale middle representation). The predictions then are aggregated based on uncertainties---the predictions that are based on less affected middle representations will be accounted for with higher weights. The empirical results are shown on depth estimation problem, the comparison with deep ensembles looks convincing.\n\n**Highlight:** I want to highlight the especially interesting point that is demonstrated in the work. Many works in uncertainty estimations improve the ability of a model to estimate uncertainty in its predictions. However, it almost never goes beyond the estimate of uncertainty. Certain questions still remain unanswered: Are our uncertainties estimations adequate *? Are they adequate enough to be useful to improve predictions of the model? Is it possible to improve predictions based on uncertainty estimation at all? etc. Uncertainties rarely (if not newer) are used to improve the predictive performance of a model, that however is done in this paper, and I see it very interesting. \n\n'* the evaluation prosses is a weak point at the UE research.\n\n**The major concerns**: \n\nThe only concern I have is related to the area of applicability, and it is more like a question rather than a concern. \n\nThe paper clearly demonstrates that the method makes the difference in depth estimation. Do you think the method will transfer to other problems (e.g., detection, classification, ...) and domains (e.g., sound, ...)? I certainly understand that it is impossible to pack everything into one paper, but at least I would like to hear some comments. \n\nPerhaps we are bounded to corruption where lack of certainty is the kind of obvious --- if the image is blurred then X transformation suffers and it is easy to notice (for instance via pure pixel-vise classification). But for example, if we see a completely sharp but out-of-domain object we may not recognize uncertainty at all.\n\n**Minor comments:**\n\n1. Use `\\citep` for multi-citations \\citep{work1, work2} and use `\\citep` for (Boyd et al., 2004) style instead of manually adding redundant brackets like (Boyd et al., (2004)).\n\n2. An unclear sentence \"Similar to (Hafner et al. (2020)), we  ... \"\n\n3. There is a relation to test-time data augmentation [Ashukha2020, Kim2020].\n\n4. Add colorbar(s) to uncertainties images (Figure 2, ...).\n\n5. Maybe it worth to add some analysis like which corruptions break which transformations?   \n\n[Ashukha2020] Ashukha A, Lyzhov A, Molchanov D, Vetrov D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. ICLR, 2020.\n\n[Kim2020] Kim I, Kim Y, Kim S. Learning Loss for Test-Time Augmentation. arXiv preprint arXiv:2010.11422. NeurIPS 2020.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Cross-task ensembles",
            "review": "This paper suggests a method for producing diverse ensembles with meaningful uncertainty estimates by having each ensemble method use a different intermediate task from which to predict the output. The paper applies the method to dense-prediction problems for images, so each ensemble member first translates the image to some intermediate domain (like greyscale, or 2d edge maps) before predicting a per pixel laplacian distribution. Each model is trained in a supervised fashion with an NLL objective, and uses a perceptual loss to further refine it. To improve the uncertainty estimates under distribution shift, the model is trained in a 2nd stage to produce high uncertainty for distorted data. The final ensemble output for each pixel can either be produced using a trained mixture model or just using inverse variance mapping. Experiments show improved performance on corrupted inputs vs a number of baselines, both qualitatively and quantitatively.\n\nStrengths:\n+ I like the idea of auxiliary tasks being used to force networks to find diverse solutions\n+ The results generally look good\n\nWeaknesses:\n- I'm totally unclear from the paper whether F_xy is a learned function or a procedurally applied transformation. There is no loss provided for it, but it is repeatedly referred to as \"self-supervised.\" If it's just a procedurally applied transformation, it seems like it may be roughly equivalent to just using different data augmentation strategies for each model.\n- The sigma training procedure is extremely similar to \"Deep Anomaly Detection with Outlier Exposure\" (Hendrycks, et al ICLR 2019). That should be cited at the very least. Furthermore, as that paper showed, as well as many others, training with specific out-of-distribution datasets to increase uncertainty does not generalize to all other forms of out-of-distribution samples. The fact that this paper always uses the same corruptions for this stage concerns me that they may have been handpicked with knowledge of the downstream task. I'd at least have liked to see cross-validation over the corruptions used for training and testing.\n- I'm further concerned about using corruptions from ImageNet-C as part of the training procedure. ImageNet-C was released as just a test set and the paper says \"networks should not be trained on these images.\" I recognize that what's being done here is applying their corruptions to a different dataset, and not literally using their dataset, but the point is that that these corruptions should not be used to train models to be more robust, since then we need a new set of corruptions to evaluate robustness against. It's good that you didn't evaluate on the same corruptions that you trained with, but these corruptions should not be used for training at all.\n- The paper is framed as a general method for making predictions from neural networks that is robust to distribution shift. Since only one task is used throughout the paper, it's unclear how many of the contributions are generally applicable as opposed to specific to the domain of dense prediction from images(or some subset thereof). For example, is the Laplace prior generally applicable? Certainly the cross-task consistency loss is not. This needs to be discussed at a minimum and ideally also demonstrated experimentally. It seems like much of this could be directly used to train an ImageNet classification ensemble, for example.\n- I'd like to see better ablations. How much of the improvement over DeepEnsembles comes from the separate Y domains, the Laplacian prior, the sigma training, the cross-task consistency loss, or the merging function? The baselines don't capture this, although the description of the baselines isn't clear enough to know in some cases. For example, is \"Baseline UNET\" also using the Laplacian prior, or is it Gaussian? What about the DeepEnsembles implementation?\n- Are the architectures of each model (whether or not they are members of a larger ensemble) the same across the proposed models and the baselines?\n- The \"cross-task consistency loss\" seems misleadingly named, since there doesn't seem to be anything cross-task about it. It seems to just be a perceptual loss (which also needs a citation - Johnson et al, ECCV2016) applied to each F_yz model independently. What am I missing?\n- I can't tell from Section 3.3 whether Eq 4 was actually used, or whether the \"alternative approach\" of just using a standard perceptual loss on the predicted mean was used. If Eq 4 was used, how was g_jk instantiated?\n- I find it really hard to understand what the overall training procedure is. It seems that there are at least 2 stages, where Eq 1 is optimized in stage 1 and Eq 2 is optimized in stage 2. When is Eq 3 optimized? Is there a 3rd stage? As mentioned above, I have no idea what version of cross-task consistency loss was actually used, but whatever it was, what stage was it in?\n- None of the qualitative results are presented with groundtruth. That would make it much easier to evaluate quality.\n- I don't understand what the different subcolumns of Table 1 mean. From what I can tell, the results are all over the place, with DeepEnsembles consistently winning the \"Direct\" subcolumns. The 2 \"Ours\" methods only seem to win a combined 50% of the time, but maybe I'm misunderstanding the table.\n\nMinor details:\n* In the Notations paragraph at the beginning of section 3, it says  the intersection of Z and Y does not equal the null set. Should that be \"equals the null set?\"\n* The Training Details paragraph of section 4 references the \"merging network.\" This term was never used before in the paper. I assume it's referring to the mixture weights discussed in section 3.2. It would be helpful to use consistent nomenclature. \n\nReasons for score:\nThe general concept is good, but it is presented very poorly. It's not clear what is addressing the general problem of robustness to distribution shift vs just improving the single specific instantiation of the problem that is experimented with. The method is not clearly presented and is therefore hard to follow. There are also a number of important missed citations that make it seem like this work is claiming novelty that it doesn't have. A number of best practices are also violated here.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}