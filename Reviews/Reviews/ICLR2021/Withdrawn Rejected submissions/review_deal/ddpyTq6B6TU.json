{
    "Decision": "",
    "Reviews": [
        {
            "title": "Experiments could have been more comprehensive",
            "review": "This paper presents a method to develop contextual language models which capture different granularities of linguistic information. The approach is similar to BERT, but instead of masking only tokens, the authors select ngrams based on high PMI score. The masking objective is then to predict the full ngram given the context. The method is evaluated on the CLUE dataset, and shows improvements across a wide range of tasks requiring different granularities of information. The paper also introduces two new datasets to evaluate retrieval performance using their contextual language model.\n\nI am not convinced with certain parts of the evaluation. The objective mentioned is very similar to SpanBERT, and I was interested in an explicit comparison with SpanBERT. Probably the \"span\" row in Table 7 is showing that, but it was not clear. Even then, I would be interested to see if the trend holds in English. Its important to know if the advantages are due to Chinese segmentation challenges or are more general. \n\nThe retrieval tasks do not have a strong baseline. A very relevant baseline will be LASER (https://github.com/facebookresearch/LASER), which can be directly applied to Chinese text. \n\nMinor comments:\n1.  Certain statements are not well supported with evidence, better to remove : \"which is not applicable when multiple levels of linguistic units are involved at the same time\", \" to encode different levels of linguistic unit into the same vector space\", \"achieving encouraging performance at a certain level of linguistic unit but less satisfactory results at other levels.\"\n\n2. Why remove the longest n-gram ? You can create different masking examples out of the overlapping ngrams.\n\n3. It was tricky for me to review since I dont know much about CLUE. I will advise introducing relevant tasks of CLUE with an example.\n\n4. Same with BERT-wwm-ext. It is not common knowledge, so better to introduce it briefly with citation. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental paper; less than satisfactory experiments",
            "review": "This paper introduces the idea of universal representation learning and presents a pre-trained language model that maps different granular linguistic units into the same vector space. The model is a simple and intuitive extension to BERT. Experiments are performed on a Chinese retrieval and language generation task.\n\nStrengths:\nThe ides of mapping different linguistic units into the same vector space is interesting and potentially impactful.\nThe application of the pre-trained model to the retrieval and generation task where linguistic units of different granularity are retrieved or generated (instead of paragraph retrieval or token level generation) is interesting and also potentially useful.\n\nCons:\nThe paper is clearly an engineering effort with some simple modifications to BERT pre-training. I do not see how this is scientifically very interesting. The comparison with BERT on Chinese retrieval and language generation task is not appropriate. Instead we should consider Chinese BERT or multilingual BERT as comparison. We could also consider comparing this to SpanBERT.\nMore clear comparison with token level masked pre-training strategies on established benchmarks would help make the contribution more apparent. As the moment, it is hard to judge the impact of the pre-trained model.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Limited technical contribution and weak baselines",
            "review": "The paper proposes to perform n-gram masking and predicting rather than token-level masking. n-grams with lengths up to N are first extracted and point-wise mutual information is used to filter out meaningless n-grams. Then pre-training with masking on these more meaningful n-gram/phrases are performed.\n\n1) The idea of performing mask on filtered n-gram is quite incremental and the technical contribution is very limited.\n2) Baseline is weak: there have been many pretrained language models (e.g., RoBERTa, XLNet, spanBERT) proposed in the past year which show improved performance over BERT. However, the author only uses BERT as baseline and failed to compare with more recent BERT-variants.\n\nTo sum up, given the limited novelty and weak experiments, this paper is below the bar of acceptance for ICLR.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experimental results are basically convincing, but the novelty is not sufficiently strong",
            "review": "    The authors proposed BURT to capture features of different levels of linguistic unit by extracting and masking n-gram segmentations. In particular, they first (a) extract a large number of n-gram spans from the large corpus and use PMI to filter out meaningless n-gram spans, and second (b) apply masking strategy on those matched n-gram spans and employ the Transformer encoder to predict the tokens in the masked n-gram spans. However, (a) is just a unsupervised method for word segmentation, (b) has been developed in spanBERT, BERT-wwm, ERNIE 2.0, etc. Besides, it is not appropriate to call BURT \"universal representation\". It did not learn unique embedding for each granularity of linguistic units, nor did it simultaneously learn multiple levels of linguistic units in a single sentence.\n\n    The pros & cons are as follows:\n\n    Pros:\n\n    1. The overall presentation of this paper is clear and the reported experimental results are convincing. The concrete examples and illustrations are also good to tell the methods and experiments.\n    2. With the provided implementation details including the used third-party libraries for pre-processing the data and the hyper-parameters for training the model, the reproducibility of this work should not be a problem.\n    3. The improvements of BURT compared with its baselines on WSC and the FAQ are significant and convincing.\n\n    Cons:\n\n    1.  As aforementioned, the contribution and novelty of this paper do not seem to be sufficient to be accepted as a ICLR paper.\n    2. The \"universal representation\" is somewhat overclaimed.\n    3. According to the experimental results on the test set of CLUE, the improvement of BURT is not that significant except for WSC. Besides, can you explain why BURT achieves great improvement on WSC but not on other tasks in CLUE?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Initial Review",
            "review": "[General Review]  \nIn this paper, the authors propose to improve the BERT by utilizing different levels of linguistic units, which is similar to SpanBERT or BERT with N-gram masking. They propose an N-gram pruning strategy based on PMI calculation to ensure the extracted N-grams are informative. Then a traditional N-gram masking is applied with several minor modifications (some of them are known), such as dynamic masking, etc. The pre-training was performed based on the BERT or BERT-wwm-ext checkpoints with the processed Chinese Wikipedia data. The experiments are carried out on 1) a subset of CLUE, including 6 classification tasks; 2) retrieval-based FAQ; 3) natural language generation. The results show that the proposed BURT model shows marginal improvements over BERT and BERT-wwm-ext.\n\nOverall, the writing of the paper is clear and easy to follow. However, I have several main concerns about this paper, which stop me from recommending acceptance (also see weaknesses in next). \n1) The main results on CLUE is not significant, considering the baselines are not also further pre-trained.\n2) The n-gram masking is not a new idea, though the authors try to improve it in some ways.\n\nAlso, considering ICLR is one of the top-tier conferences, after evaluating the novelty and impact of the paper, I regrettably decided to recommend to reject this paper.\n\n\n[Strengths]\n1. The authors propose an n-gram refinement strategy for extracting many meaningful n-grams.\n2. The writing of the paper is clear and easy to follow.\n\n[Weaknesses]\n1. There is a major weakness in the experimental setting, which I think is fatal. The proposed BURT is trained based on the BERT or BERT-wwm-ext checkpoints, but these counterparts were NOT also trained with additional steps for a fair comparison.\n2. Besides the major flaws in the experimental setting, the overall gains in the subset of CLUE is limited. Considering the experiments are only performed on a base-level pre-trained language model, I wouldn't call the results are significant.\n3. N-gram masking strategy is not a novel idea though the authors try to improve it some way, which I think the novelty is marginal.\n\n\n[Questions for Authors]\n1. Why the evaluations are only performed on a subset of CLUE? I assume that the reading comprehension task (such as CMRC 2018, DRCD) will benefit more from n-gram masking, but there is no result for that. The authors should clarify this.\n2. How was the PMI threshold chosen? In Section 2.3.2, it only shows the threshold is -12 but does not describe how it was set.\n3. The experiment of SOP in section 3.4 seems weird to me. It is not related to the central idea of this work.\n\n\n[Minor Comments]\n1. page 1: it is better to include citations right after the model names. For e.g., \"ELMo, GPT,... (Peters et al., 2018; Radford et al., 2018;...)\" -> \"ELMo (Peters et al., 2018), GPT (Radford et al., 2018),...\"\n\n\n[Missing Reference]\n1. Adam: [Kingma and Ba, 2014] Adam: A method for stochastic optimization. In arXiv.\n2. SRILM: [Stolcke, 2002] SRILM - An extensible language modeling toolkit. In ICSLP 2002.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}