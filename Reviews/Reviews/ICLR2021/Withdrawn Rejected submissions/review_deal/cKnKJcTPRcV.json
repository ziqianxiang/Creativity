{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a learning framework for Hypergraphs. The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs. Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required. However, the proposed methods are not significantly different than that used for Graphs. Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance.\n"
    },
    "Reviews": [
        {
            "title": "This paper showed the limitations of existing graph neural networks on hyper graphs, and presented a novel inductive hypergraph learning framework to encode the intra-relations (within a hyperedge) as well as inter-relations (across hyperedges) from the hypergraph. The extensive experiments confirm its effectiveness and robustness.",
            "review": "Overall, this paper is well-written. However, the intuition of the proposed framework is not very clear, and the model efficiency needs to be analyzed and evaluated compared to other baselines.\n\nThis paper analyzed the key issues of the existing message-passing graph convolutional networks. That is, the multiply stacked layer might be over-fitting and over-smoothing. Thus it proposed to choose the neighbors from the entire graph based on the structure-aware and feature-aware relatedness rather than simply choosing the local neighborhood. However, the motivations of the proposed structure-aware and feature-aware teleport functions are not very convincing, and Table 3 shows that the performance improvement of TeleGCN might largely be induced by model architecture rather than the proposed TeleGCL. \n\nSpecifically, the pros and cons of this paper are summarized as follows.\nPros:\n[1] It proposed a two-level message-passing strategy for neighborhood aggregation in the hypergraph.\n[2] The proposed HyperSAGE framework is inductive.\n[3] The experiments demonstrate its effectiveness and robustness compared to baselines.\n\nCons:\n[1] It required the message aggregation scheme had the property of hypergraph isomorphic equivariance. Two question are that can the examples of Figure 2(b) be distinguished by the proposed scheme in Eq. (3), and can the proposed be guaranteed to distinguish the hypergraphs which not isomorphic?\n[2] It is not clear what MLP+HLR is in Table 3?\n[3] In Section 4.3, it is confusing why to split each of the datasets into a train-test ratio of 1:4. Figure 3 has shown that with such a train-test ratio, HyperSAGE might work just like the HyperGCN.\n[4] The efficiency of HyperSAGE needs to be confirmed empirically compared to baselines as it argued that the condensed neighborhood could help improve the training efficiency.\n[5] There is a typo in Eq. (3). Should it be |N(v_i, e)|/|N(v_i)|?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A simple and flexible message passing method for hypergraphs",
            "review": "pros.\n-The paper proposed a simple and flexible message passing method for hypergraphs.\n-By performing semi-supervised node classification on 4 popular datasets, authors show their method outperforms existing methods.\n-The paper is well-organized and clearly written. To the best of my knowledge, the method is technically sound.\n\ncons\n-The novelty of the paper seems limited. The proposed method defined in Equ. (1) and (2) is a straightforward extension of the message-passing method for ordinary graphs. \n- In Equ. (3): N(v, e) -> |N(v_i, e)|, N(v_i) -> |N(v_i)|\n\ncomments\nFrom Tab. (1),  it seems the reported performance of hypergraph-based methods is not better than the results of GAT, GCN. So, why should we use hypergraph-based methods?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Better and more formal presentation is required; recent literature needs to be propertly cited",
            "review": "This paper introduces HyperSAGE, a method to address the problem of representation learning for hypergraphs.\n\nThe chosen approach consists in defining aggregation functions adapted to this specific type of data. It allows inductive learning while methods based on reduction to regular graphs are inherently transductive.\n\nThe algorithm introduced in this manuscript achieves good accuracy scores compared to other recent methods for hypergraphs on several real datasets. It is also more stable.\n\n\nMain concerns:\n\n1/ The paper deserves a better presentation: the authors should be more precise and also more formal at some places of the paper, in particular to describe the problem under consideration and the model they develop.\n\nFor example:\n- The reader is expected to guess that nodes have features that are stored in a matrix (which supposes that all the nodes have the same features and are real-valued). Indeed, hypergraphs are only defined through their topology (see Def. 1). The feature matrix is then introduced without any detail and explanation on what it models (see Subsection 3.2). \n\n- The concept of aggregation function should be introduced in the paper (for example at the beginning of Subsection 3.2). The two aggregation functions are not defined. In eq. (1) and (2), the reader can understand that the input of an aggregation function\nis a subset of features. In Prop. 1, the input is a hypergraph (which has no feature if one reads Def. 1 to the letter).  In eq. (3), the input is a node.\n\n- Property 2 should be written in a formal way. In particular, I do not fully understand the relation between what is claimed to be proven in Appendix B and Prop. 2. In addition, if I understand correctly, the proof does not show that p_1 is equal to p_2:\n\"we first assume p_1=p_2\" under eq. (10), while the case p_1\\neq p_2 is not dealt with. Finally, an additional condition seems to be assumed in the proof while not mentioned in the main document (see sentence under eq. (11)).\n\nFor these reasons, Sections 2 and 3 (as well as Appendix B) are difficult to read and understand.\n\n2/ It seems to me that the recent literature on hypergraph-based learning approaches is not properly cited. The authors should have a look at the following papers, in particular at the second one that proposes inductive (and transductive) learning methods for hypergraphs.\n- \"Hyperedge2vec: Distributed representations for hyperedges\", A Sharma, S Joty, H Kharkwal, J Srivastava (Preprint, 2018)\n- \"Deep Hyperedges: a Framework for Transductive and Inductive Learning on Hypergraphs\", J Payne, published in Sets & Partitions Workshop at NeurIPS 19.\n\n\nQuestions:\n- What is a uniform hypergraph as mentioned in Section 2?\n- What is the global neighborhood of a node?\nIs that the union of intra-edge and inter-edge neighborhoods?\n\n\nFew typos:\n- page 3: space missing\nactive learning etc.(Li \n- page 5 (property 1)\nisomporphism\n- page 5\nThe two functions (...) is defined",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An extension of GraphSage towards Hypergraphs ",
            "review": "Summary \nIn this paper, the authors study the problem of learning node embeddings for hypergraphs. While most of the existing studies consider reducing hyper-graphs into graphs, this paper studies learning embeddings directly on the hypergraphs using two stages of aggregations / sampling. The efficacy of the proposed method is illustrated in a semi-supervised as well as inductive setting, where the method achieves better performances than the baselines. \n\nOriginality. \nThis paper may be interpreted as extension of GraphSage towards hypergraphs. The main contribution is in the aggregation function, which has two levels: one over intra-edge neighbourhood, and other over inter-edge neighbourhood. This method is thus suitable for both transductive as well as inductive settings. \n\nSignificance.\nThe experimental results show that HyperSage achieves better performance than HyperGCN in the semi-supervised tasks, while also extends to inductive settings and compares favourably against the MLP+HLR baseline.  In this aspect, the results are significant and relevant to the community. \n\nClarity: \nThis paper is well written and easy to follow. The relevant baselines and related methods are discussed appropriately. There are quite a few typos and some lack of clear notations, which are listed below:\n\n\t- Intra edge neighborhood - definition unclear - How is v_i used in the definition of N(v_i, e). \n\n\t- Typo in eqn 2, should have been e \\in E(v_i)\n\n\t- Typo in eqn 3, the second equation has ratio of two sets, which ideally should have been ratio of cardinality of two sets. \n\n\t- Algorithm 1, line 4, initialization of h_i^l from h_i^{l-1} should be outside the for loop, otherwise it may get reset again repeatedly. \n\n\nQuestions to authors:\n\n\t- HyperSage has less deviation in the results than other methods such as HyperGCN. Any reasons ?\n\n\t- In the stability analysis paragraph, it is shown that as the train to test ratio improves to 1/3, HyperGCN and HyperSage have nearly same performances. Would be interesting to see how it behaves as we increase the training ratio further. Also, any insights into why this behaviour happens would be useful.\n\n\t- Also, what is the specific train-test ratio which is used in reporting Table 1? It would be better to include that in the main paper, since there is a discussion on train-test already present.\n\n        - What are the other possible baselines for comparison in the inductive setting (Bai.et.al (Pattern Recognition, 2020) ? )\n\n        - From the results presented in Tables 1,2,3, it appears that the best performance is shared between p = 0.001 (equivalent to that  of Geometric mean aggregator) and p = 1(arithmetic mean aggregator). Any explanations of this would be useful. \n\n\nPros:\n\n       - An effective and simple extension of GraphSage towards HyperGraphs, which is suitable for inductive and transductive settings.\n\n       - Experiment results illustrating better performances than the baselines.\n\nCons:\n\n       - The novelty of the proposed method is limited to that of the aggregation functions.\n\n       - Some of the results mentioned lack explanation. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}