{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review for adaptive dataset sampling paper",
            "review": "In this paper, the authors propose an interesting approach that advocates the use of RL methods for adaptive sampling of the samples being used for training models for deep learning. The approach is based on the use of policy gradient for training models. The method is compared against a variety of models trained on CIFAR 10 and CIFAR 100 and compared against a random sampling model.\n\nPros:\n1) The idea of adaptively sampling the mini-batch to be used for training is interesting and the results show varying improvements over random sampling suggesting that the choice of samples matters\n2) The use of an RL framework suggests that the approach can consider varying reward functions that could be used to optimize the choice of samples \n\nCons:\n1) The method is compared with a random sampling function and pre-trained RL sampler. However, the paper does not provide comparison against other valid baselines that have been previously proposed for sampling such as the active sampling or curriculum learning approaches. It is not clear whether the RL learned approach would be competitve against such approaches.\n2) The method needs to also be compared in terms of the additional computational effort and whether the additional computational effort and reduction in training due to additional need for validation merits such an effort\n3) It would also be interesting to consider generalization and transferability efforts. For instance, perhaps a sampler trained on one dataset could be directly applied on another dataset\n\nAdditional analysis\nIt would be useful to also know how the behaviour of the sampler changes over multiple epochs. Is it sufficient to train the sampler for one epoch or does it need to be trained for each epoch?\nThe baseline used in policy gradient is the exponential moving average over the reward. It is not clear how the use of other baselines could have an effect on the proposed method.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning RL sampler is unnecessarily harder than original problem, infeasible complexity of the RL problem, lack of many baselines and benchmarks, experimental improvement is small. ",
            "review": "This paper proposes to train a sampler as an RL agent to generate indices for each mini-batch used to train a model and claims the generated sequence of mini-batches (i.e., a curriculum) performs better than a uniform random sampler. In particular, they follow a similar strategy as the first NAS paper: for the RL sampler, they use an LSTM to generate the current state from historical logs (composed of output logits for previous training batches), then a policy network takes the LSTM output state as input and produces actions of sample selection, i.e., the probability of selecting each sample from the training set. The RL sampler gets a reward as the validation loss on a held-out set periodically but less frequently due to the extra computational cost for inference on the validation set. The policy gradient method is used to train the sampler. In experiments, they show that the proposed method outperforms naive random sampler by a small margin on the two CIFAR datasets when used to train models of different architectures. They also show that the sampler and its generated curriculum can be transferred across different model initializations and architectures. \n\nThe advantages/disadvantages of the RL formulation for curriculum learning in this paper are not well justified and discussed thoroughly, resulting in several major concerns about the unnecessary cost, complexity, and necessity of training an RL sampler. The paper lacks comparison to any related works from curriculum learning and active learning, which target exactly the same problem but are usually simpler in problem formulation and more efficient in computation. The experimental improvements reported are too small and the final comparison to existing data selection criteria (Figure 4) also worries me.\n\nI list my major concerns below, hopefully, the authors can provide a convincing response:\n\n(1) Formulating the curriculum learning problem as an RL problem is straightforward but needs to be more careful since it should not be even harder or more expensive to solve than the original supervised learning problem. However, the paper fails to convince me so. Specifically, even after the simplification and approximation in Section 3.1, the RL problem they aim to solve is still too challenging and much harder/more complicated than the original classification problem. For example, (i) their action space is too large (n possible actions where n is the number of all training samples), most RL algorithms can only handle tens to hundreds of actions; (ii) the output dimension of the RL sampler is n, much larger than then input dimension, which is the batch size; (iii) the feedback reward is very sparse since they can only afford infrequent evaluation of the validation loss; (iv) not to mention the high variance of the reward due to the limited size of validation set and the original REINFORCE algorithm. It is hard to imagine that the training of RL sampler in this scenario can learn any meaningful knowledge or even converge, and the authors did not provide any justification about why solving such a big RL challenge is feasible.\n\n(2) In experiments, there is only one baseline to compare and it is the naive random sampling. The paper ignores a great number of works in curriculum learning and active learning, which target exactly the same problem but many of them do not formulate it as a complicated and challenging RL problem. They show much more improvements in experiments than the paper and are usually more efficient in computation. In experiments, no comparison to any of them is presented. To name a few of them:\n\nFan et al., \"Learning to Teach\", ICLR 2018.\nZhou and Bilmes, \"Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity\", ICLR 2018.\nJiang et al., \"MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels\", ICML 2018.\nSaxena et al., \"Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum\", NeurIPS 2019.\n\n(3) The ideas proposed in this paper for data selection is not novel. Similar Ideas of using RNN to extract features for training history and training a teacher model on a held-out validation set have been widely used/studied in these existing works (and in many ones not mentioned above). A comparison and discussion of them are expected.\n\n(4) The input to the history logger is the logits of the previous batch, which are not aligned across different time steps since the samples selected into the batch in each step are different. However, the output of the RL sampler is sample-specific, i.e., each dimension corresponds to a specific sample. This is problematic since the input cannot distinguish different samples.\n\n(5) The improvements shown in the experiments over random sampler is not significant at all, especially when considering the reported variance. In contrast, many curriculum learning methods can show much larger improvements. \n\n(6) The paper only reported results on CIFAR. Experiments on more and larger datasets such as ImageNet1k, MS COCO, or FGVC datasets should be considered.\n\n(7) Figure 4 shows that the learned sampler does not contain any information about the loss or gradient norm, which has been demonstrated effective in selecting more informative samples for training in several existing works. While I agree that there might exist other useful information orthogonal to the two criteria, a well-learned sampler should at least cover partial information of the loss or gradient norm since they are the easiest pattern to capture. Hence, it worries me again that the RL training fails and the sampler does not learn any meaningful thing.\n\n(8) The computational complexity and time cost of training RL sampler should be reported and compared to those for training the classifier model.",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Some questions about empirical evaluation",
            "review": "Summary\n\nThe paper proposes to treat minibatch selection as a standalone agent (RLSampler) that picks the next minibatch considering previous performance of the trained classifier. RLsampler is trained through a policy gradient where the reward is the classifier's performance on a held out dataset.\n\n\nStrong points\n+ I find casting the problem of minibatch sampling as an RL problem an interesting idea.\n+ Empirical evaluation shows that RLsampler helps with many standard image classification architectures.\n\nWeak points\n- The method is compared only against random minibatch sampling and not against any method mentioned in Related Work.\n- It isn't clear whether the design follows Occam's razor: e.g. is the recurrent component of policy important? There aren't ablation studies showing that it is needed.\n\n\nRecommendation\n\nWhile I like the general idea of learning the example sampling process I am not convinced yet by empirical evaluation of the approach. Therefore I recommend rejecting the paper in its current form.\n\n\nQuestions\n\n* What is the number of parameters of RLsampler compared to the image classification model? Since RL sampler produces probability distribution over all training examples the last layer might be quite parameter heavy.\n\n* What is the computational overhead of training and applying RLSampler, does RLsampler help when the training time isn't measured by number of epochs but by wall time (compared to random sampling)? \n\n* Is the probability of examples context dependent? E.g. are there some examples that are sampled more often at the beginning of the training than towards the end? In other words: does the algorithm learn just fixed weight for each example (that would be a nice and easy to implement baseline) or is the LSTM part also important? Fig 3 (b) shows the average distribution, some visualization of sampling distribution shift during training would be helpful. \n\n\nPossible improvements\n\n* Compare learning performance against baselines (e.g. those mentioned in Sec 6.2). \n\n* Include learning curves comparing different approaches. At the moment only numbers at fixed final iteration of training are reported. However what if when trained till convergence both approaches perform similarly? \n\n* Show that recurrent policy is needed and that the weights of examples change over time, right now the paper claims that there is an optimal sequence of training examples (and I believe that there is such in spirit of curriculum learning). However none of the experiments show whether properties of the sequence change over time or whether it can be summarized just by a distribution like in Fig 3 (b).\n\n[* As always: more datasets from different domains, e.g. text classification would make the results stronger. While it would be nice in my view it isn't as important as previous possible improvements.]",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}