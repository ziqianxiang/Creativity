{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper presents an extension of recent implicit representations for view synthesis, such as NeRF. The presented formulation accepts an image set as input at test time, and can thus in principle be applied to new scenes. The idea is sound, but reviewers had concerns with the presentation and the experimental results. The work is primarily evaluated on the simplistic ShapeNet domain, which a number of reviewers found unconvincing. Concerns remain even after the authors' responses, and the AC agrees that the work can benefit from further investment before it is published."
    },
    "Reviews": [
        {
            "title": "Accept",
            "review": "This paper presents an extension of NeRF. The key idea is to represent a 3D\nscene as a collection of K \"posed\" images. A network is trained to take these\nimages, viewing information and a query point as input and output density (soft\noccupancy) and RGB reflectance color. In contrast to NeRF, this methods learns a\na mapping from the input images to feature vectors and thus promises to\ngeneralize across inputs (whereas NeRF uses weight-encoding). On a detailed\nlevel, this method also uses multiview consistency during training, though there\nis not an ablation I could find that demonstrates the effectiveness of this\ndelta.\n\nI recommend accepting this paper to ICLR. I like the idea of using the K input\nimages as parameters controling the scene. My main criticisms are in the\nexposition of the paper and the experiments. I hope the exposition can be\nimproved in revisions and am comfortable leaving the experimental shortcomings\nto future work.\n\nMy main source of confusion reading this paper centers around the input\n\"viewpoint\". The following comments are made in order of the paper's exposition\nand hopefully highlight the path of my confusion:\n\nPlease define what's meant by \"posed 2D images\".\n\nAre the viewpoints V1, ... Vk corresponding to the K input images really just\nthe 3D position of the camera focal point? Or does this also include the view\ndirection and camera intrinsics? If it does include other information then\nperhaps use a different symbol for the query viewpoint.\n\nNear Figure 2, the discussion of \"viewpoint\" for the image is really confusing.\nDo all pixels of an input share the same viewpoint position (seems to be the\ncase. This seems awkward, but best matches the written explanation.\n\nOr is the image \"placed\" into the 3D scene according to a pinhole camera's\ntransformation matrix, so that each pixel gets a unique 3d position associated\nto it? This seems more appropriate, but would not match the text or explanation.\n\nThe reprojection step appears to assume access to more than the camera center.\n\nMuch later the paper says Vk \"including extrinsics and intrinsics\". This is a\nfairly abusive notation. This should be clarified early on and the confusion\nissue with the image augmentation with viewpoint position remains.\n\nIf the intro/abstract made it clear that the input is K images and full camera\ninformation and a different symbol was used for camera info and query viewpoint,\nmuch of this confusion would go away.\n\nIn the motivation of this paper, there is special emphasis on representing\n\"scenes\", yet the experiments are on \"single objects\", just as other methods are\ncriticized for being limited to. Can the proposed method be used to represented\na full indoor room (not just a 360° video; but view from anywhere in any\ndirection)?\n\nThe examples in the appendix are more like photos+depth type examples than what\nI would consider a full \"scene\". Perhaps tone down/clarify what is claimed in\nthe introduction.\n\nI would also like to get a sense of how stable / gracefully degrading this\nmethod is to the K input images. For example, if I only input images of the\nfront of an object, what will happen when viewing the back? What if most of the\nimages are from one direction, will this bias affect views elsewhere?\n\nDoes the method generalize to scenarios with ≠ K input images? I suppose one\ncould use the zero vectors trick for the <K images, but I wonder about\ndegradation. What about >K?\n\nIt is not really fair to write that \"mose methods ... require ground truth 3D\ngeometry for supervision\". NERF/IDR/SIREN and when considering noisy point\nclouds SAL/SAL++ do not need ground truth 3D geometry.\n\nThe paper writes \"This simple design of GRF follows the principle of classic\nmulti-view geometry (Hartley & Zisserman, 2004), therefore guaranteeing the\nlearned implicit representations meaningful and multi-view consistent.\" I do not\nsee how this setup provides any formal guarantee. In the appendix it appears\nthat following NeRF this paper predicts the density from the query position and\nnot the viewing direction. Is that simply inherited here? The multiview aspects\nduring training simply harmonize with this choice but don't provide any\nguarantee that I can understand.\n\n\"for very query\" --> \"for every query\"\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a neural rendering model named General Radiance Field (GRF) to achieve 3D representation learning and novel view synthesis. Different from the previous neural rendering methods NeRF and SRN that directly learn the mapping from pose information to color, GRF learns to aggregate the 2D features of different observations via attention mechanism given a 3D location and view pose. ",
            "review": "Pros:\n+ This paper proposes a method for synthesizing 3D scenes in novel view by treating the network as a general radiance field. It seems that the method is more like traditional manner based on multiple view geometry. It enables the generalization ability of rendering unseen test data in contrast to the most related work, NeRF (Mildenhall et al., 2020).\n+ The proposed GRF can empirically generalize to novel scenes. Experimental results show that the proposed method achieves better performance on several large-scale datasets.\n+  In general, this paper is well-written and easy to read.\n\nConcerns:\n- The novelty is limited as the concept of this paper is highly similar to NeRF, despite a significant improvement is the general feature for novel view rendering. Yet I have few questions about general features for 3D points.\n1.  As depicted, the aggregator that assembles the 2D features of each 3D point from multiple views handles the visual occlusion implicitly via an attention process without depth scans. How does it work on unseen scenes? The network may have no knowledge about the structure of the novel scenes. Are there any failure examples of such cases?\n2.  Rendering a query 3D point p requires the (r_p, g_p, b_p) and the density d_p. As described in Eq. (6), the color channels are estimated through MLPs with learned feature \\bar{F}_p and the query viewpoint \\mathcal{V}_p as input. Nonetheless, the density function in Eq. (5) does not require the query viewpoint as input. I think the density of 3D points for volume rendering should be dependent on the viewing ray as well. How to explain the difference between Eq. (5) and Eq. (6)?\n\n- The evaluation of generalization for the novel scene of the proposed model is still not convincing enough, which needs improvements.\n1.  In section 4.1, the performance of SRN is better than the proposed methods in almost all situations. The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code. I suggest that the authors should evaluate the performance of SRN on novel scenes without retraining process to validate the argument.\n2.  The experiments only conduct on the novel scene of the same category which share the similar features. I consider that SRN can also handle this situation by the learned latent code and hypernetwork. The authors are suggested to train the proposed GRF on a large amount of data with different categories of objects to learn the general feature for attention mechanism, and evaluate the model on the new object to validate the generalization.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A generic scene radiance field neural estimator with better generalization ability than NeRF.   Well motivated idea but poorly executed. ",
            "review": "This paper presents a new neural learner for learning generic scene radiance function. Compared with existing methods such as NeRf which are scene-specific requiring per scene based training, the proposed new method is potentially able to generalize to novel unseen scenes or objects.   Specifically, the method represents scene as a neural radiance filed in terms of spatial coordinates of camera centre and a query point. An attention model is used for aggregating back-projected image features at every query location, and the aggregated feature vectors are then used for predicting its RGB-alpha radiance.  \n\nThe work is well motivated, aiming to relax the restriction of existing per-scene based radiance field learning methods  (NeRF, for example).  Experiments show some improved performance in novel view/radiance field synthesis , however I do no f find there are convincing tests provided or conducted to validate the \"better generalizability\" claim.   Overall, the current results are insufficient to validate the method's generality to novel scenes .More comments are given below:\n\nPros:  \n+ The work is well motivated.  \n+ The idea of using attention model to address multi-view consistency issue is interesting,  which appears to be sound and promising.  \n+ extensive experimental study, lots of ablation studies and comparisons. \n\nCons: \n-One key intention of this paper is to improve the network's generalibility to unseen, novel scenes.   However, throughout the reported experiments,   I do not find  adequate experimental evidences to  support this claim.    From the paper it seems the proposed method has only been tested on some unseen scenes in the ShapeNet dataset, however with very similarly-looking objects in simple and similar poses.    What are the results on the other two datasets, and what are the training-testing split on unfamiliar scenes?  Since the major motivation of this paper is to handle complex novel unseen scenes, I would suggest the authors conduct experiments on outdoor scenarios, such as the 'Tanks' and 'Temples' dataset commonly used in recent related work on neural novel view scene rendering.\n\n- It is mentioned that two different attention models (AttSets and Slot Attention) are used depending on training dataset. However it is unclear which model was used for testing which datasets.  How did you compare in PSNR/SSIM metrics on each dataset?   Also, there is no description of how these attention mechanisms help the process.  In particular, since 3D point visibility of source view features under target view is one of the key problems for any novel view synthesis work.  How the employed attention module solves this issue is unclear.  In short,  I did not see how this attention module handles the visibility problem, other than (as the authors said) it helps to \" aggregate multiple source view features\"-- which seems to me to be an obvious outcome. \n- The authors  also claimed that the attention module is invariant to the input view number and orders;  Yet,  there is not any  evidence (either theoretic analysis,  or experimental) provided in the paper.   Is it an inherent property of the Attention modules ?  \n\n- From reading Table-1,  it seems SRNs sometimes outperforms GRF.   The advantage of the proposed GRF is not entirely clear.  \n\n- overall, a well motivated paper, but the execution of the ideas is not convincing. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Recommendation to weakly Reject",
            "review": "Summary:\n\nThe paper proposed an extension on Neural radiance field for better generalization across novel scenes by introducing multi-view pixel aligned features as additional input to NeRF. To fuse multi-view information and implicit reason about occlusion, an attention aggregation module is applied.\n\nStrengths:\n\n+ The idea of using pixel aligned feature for making NeRF generalize to novel scenes is interesting.  \n\nConcerns:\n\n1) the term of implicit representation is interchangeably used to describe implicit function and explicit 3D representation parameterized by MLPs.\n\n2) Comparison between GRF and GRAF: In GRAF, the radiance field is also conditioned on a shape code as well as an appearance code. But in the related work section, the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim.  \n\n3) Missing details about volumetric rendering: The paper did not talk about how rendering is performed(possibly volumetric rendering). And both in NeRF and NSVF, sampling strategy and volumetric rendering both play important roles on achieving high-fidelity rendering results. It is unclear here how ray marching is formulated. \n\n4) Experiments on ShapeNetV2: the author just reported SRN's result here, but didn't compare with some other method like NVS which is also directly applicable. Besides, other methods like NeRF could be modified to train on multiple objects like including a conditional embedding to NeRF jsut like GRAF or like in SRN use hyper-networks. Lack results here could be potentially undermining the claim of GRF being more general and robust.\n\n5) Experiments on real-world complex scenes: The training setup here is a bit unclear to me. From just the description there, it is hard for me to tell whether GRF is training one model on all those scenes or training separate models for different scenes. It would be great if the author could make that clear.\n\nMinors:\n\nIn the last paragraph of section 3.4, very query 3D point -> every query 3D  point?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}