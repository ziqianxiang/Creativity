{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "There was a consensus among all the reviewers that the methodological contribution is not significant enough for publication at ICLR. In short, the main contribution of the paper is to include spatial modeling into a deep temporal point process model. However, to do that, they just use a well-known method (KDE) on top of a methodology that is very similar to Du et al., KDD 2016. \n\nIn addition, in the original submission, the specific functional form for KDE was independent on time, as highlighted by the reviewers, which basically separates spatial and temporal modeling. Unfortunately, further experiments performed by the authors failed to show performance benefits on making it dependent on time.\n\nThe authors also claim that an additional contribution is the sampling method, however, this seems to thin of a contribution for a full paper."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper sets up a spatiotemporal neural network, claiming it hasn't been done before. It may however be similar to a spatiotemporal graph network (using a regular lattice graph connectivity structure). The authors should clarify this. \nThe paper is well written however and sound otherwise. \nSome edits are required:\n- Page 2, ln 1: 'hidden states'? Language needs to be improved here. \n- Section 2, ln 6: Zhao et al (2015) rather. \n- Section 3 refers to 'markers'. In spatial statistics these are marks. I suggest sticking to the terminology of the field? \n- Page 2, ln -4: cannot rather than can not \n- Where is Figure 1 referred to in the text? \n- Page 3, ln -4: What is 'firing'? \n- Section 4.1, ln 1: fix the language\n- Section 4.1: be consisten: 2(a) or 2 (a) - not both\n- Page 6, lns -7, -8: spaces are missing\n- Caption of Figure 4 needs editing. \n- Figure 5: remove the names from the graphics at the top - put into captions. \n- There are many capitals missing in the references and journal names that should be in full. \n- Park (2019) has an et al in it? \n- Is there a published version of Mozer (2017)?\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review of Reviewer #4",
            "review": "### Summary\nThis paper introduces a mechanism to learn spatio-temporal point processes using RNNs (for time estimation) and Kernel density estimation for spatial dependencies. They perform experiments on synthetic data generated by famous point processes such as Hawkes process. \n\n### Strong/Weak points\n- The paper is well-written (with a few exceptions)\n- It is very easy to read and understand\n- The model is very easy to understand, but very hard to reason about and to give guarantees\n- It is only tested on synthetic data and no real-world example is tested upon\n\nI have a tendency to reject this paper, as the idea is straightforward: if one is asked to mix RNNs with spatial data, the way to combine these two is very clear. There is nothing new about the theory of point processes either, hence, no new understanding of these types of processes is provided. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "a reasonable model but lack of novelty or careful enough justification: reject",
            "review": "The paper proposed a neural point process that learns to predict time and location of an event. The temporal component uses the parametrization of Du et al 2016 and the spatial component uses a kernel density function. \n\nPros: \n\nThe model design is reasonable and it performs well on multiple synthetic and real datasets, compared against several appropriate baseline models. \n\nThe paper presentation is fairly clear, with thoroughly documented appendices. \n\nCons: \n\nIt is a lack of novelty. Precisely, I don’t mean the model is not novel at all; it is indeed new. But its novelty and significance is not enough. \n\nThe key design is to factor $\\lambda(s, t)$ as $f(s|t) \\lambda(t)$, but this design is really not well-motivated. In general, coupling $s$ and $t$ inside $\\lambda$ would definitely increase the model expressiveness and, for a neural model, it often won’t increase the number of parameters. E.g., one might simply move $s$ into the power of exp in eqn-5 (and possibly allow $t$ and $s$ to interact with each other by feeding $h$ and $s$ through a neural net)---why and in what sense is the proposed factorization better than (say) this simple one? \n\nI am sure there must be reasons to favor the proposed design, but they need to be carefully thought through and clearly presented---that is missing in the current paper. \n\nThe regularization for $w$ (suppressing the largest past $w$) is a little hacky: if one would like to balance between past and background points, considering their weights sum to 1, the most straightforward way would be to smooth the $w$ distribution, namely aggregating past $w$ and background $w$ and then increasing the entropy of the resulting binary distribution.  \n\nThere are things the authors need to clarify: \n(1) citations for spatiotemporal Hawkes and self-correcting processes; \n(2) $\\lambda$ of eqn-5 is inherited from Du et al 2016 and the authors didn’t mention it. \n\nIt is a little too bold to claim as \"the first neural point process model that can jointly predict both the space and time of events\": many past models can, they are just not great (e.g., they may need discretization). So I suggest the authors revise their claim: if they’d like to be a little more specific (e.g., continuous space), then their claim might be more convincing. \n\nQuestions: \n\nThe list of baseline models has Mei & Eisner 2017 but I didn’t find this model in any presented table or figure. Is this a typo or is it missing? (The authors are not obligated to compare to every single published model but they should keep the paper consistent.)\n\nHow could multiple types of events be handled in this framework? The authors use the parametrization of Du et al where they use Du’s event-type-marker to handle the event-location---this is a pity: the authors propose a new model to handle some new features, but lose the capacity of handling old features. How could they do both? E.g., in the Taxi data, in principle, one might want to model (time, location, pickup/dropoff), but the current model only handles (time, location). \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "review for #980",
            "review": "This work studies the DNN-based spatiotemporal point process model. It points out the drawback of most existing DNN-based point process models: incapability to incorporate the spatio information. Although in statistics, the spatiotemporal point process is capable of capturing events in continuous space and time, such methods are computation expensive. The theoretical analysis is provided, and experimental comparisons are conducted on synthetic and real data. \n\nStrengths: The model setup is reasonable, and the paper writing is easy to follow. The work successfully includes the space information into the DNN-based point process model with a KDE method. \n\nBut I recommend rejection of the paper for the reasons below.\n\nWeakness: The major concern is the contribution of the work is not significant enough for publishing on ICLR, because most of the work (in fact, almost all the work on the temporal part) is same as the work “Recurrent marked temporal point processes: Embedding event history to vector. KDD, 2016.” as cited by the submission, e.g. the conditional intensity in Eq.(5). The improvement of the model on spatial information is implemented by introducing a KDE method to learn the spatial pdf. The KDE method has a major drawback that the current spatial pdf does not depend on the current time (eq.(3)) which is counter-intuitive, as in most cases it is natural to decide when it happens first and then locate where it happens, just as the definition of \\lambda_(s, t) = f_(s|t)*\\lambda__(t) in the submission. \n\nSome specific concerns: in abstract, the author stated “time-conditioned spatial density function”. This is not accurate as it is not conditioned on current time as mentioned above.\nThe example in the end of sec.3.1 is not appropriate. Because the earthquake observation in the example only contains time and magnitude (no location information), of course it has no advantage over RNN for location prediction. \nThe kernel for KDE is chosen to be Gaussian kernel, so how to choose the hyperparameters, e.g. \\Sigma, for the kernel function? I only saw it is fixed to identity matrix in the experiments, so why choose identity? Any validation experiments?\nIn sec.4.2, why use the ‘max’ regularizer to avoid w_j>>w_k? why not use the L2 regularizer? is the max regularizer better than L2? why set the hyperparameter \\gamma to 1e-4 in experiments? Any validation experiments?\nIn experiments, why fix w^t and b to 0.1, respectively? If my understanding is right, these two parameters should be learned in the training process. \nIn experiments, why are only 1 or 2 baseline models considered in Table 1 and 2? What is the performance of other baselines?\nIn experiments, what is the definition of joint RMSE? \n\n\nTypo: in “Relationship to TPP”, f^*(s|t)=f(s|t,H_t)\nIn Eq.(5), h_{i-1}-->h_{i}    H_{t_{i-1}}-->H_{t_{i}}   \nIn eq.(7), E[t_i|H_t_{i-1}]-->E[t_{i+1}|H_t_i]     E[s_i|H_t_{i-1}]-->E[s_{i+1}| H_t_i]\nSec.4.2, S={s_i-j,t_i-j}_{j=1}^L-->S={s_i,t_i}_{i=1}^N  the RNN input is J but the whole observation is N.\nIn “Baselines”, ofspatiotem --> of spatiotemporal ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}