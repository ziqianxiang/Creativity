{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces ICRL, where the RL agent is supposed to maximize the reward under unknown constraints, which should be inferred from the expert demonstration. Reviewers generally agreed that this is an interesting work, and potentially make RL to be applied to more general settings. However, they also would like to see more experimental results with baselines (e.g. agents based on IRL and also related constrained learning approaches) to make the motivation behind the approach more convincing. I hope these concerns are addressed in the future work."
    },
    "Reviews": [
        {
            "title": "Official Blind Review | Reviewer #4",
            "review": "#### Summary\n\nThe submission focuses on a variant of inverse reinforcement learning, where the learner knows the task reward but is unaware of hard constraints that need to be respected while completing the task. The authors provide an algorithm to recover these constraints from expert demonstrations. The proposed algorithm builds upon a recent technique (Scobee & Sastry 2020) and addresses problems with large and continuous state spaces.\n\n++++++++++++++++++++++++++++++++++\n#### Reasons for score\n\nStrengths:\n* The problem considered is interesting and relevant to the ICLR community.\n* The technique developed (Algorithm 1) is novel and well motivated.\n* The experiments provide adequate evidence to back the claims.\n* The paper is very well written and organized.\n\nWeaknesses:\n* Justification for the policy loss function (Equation 9) is unclear.\n* Comparison with prior art is lacking.\n* Discussion of related work is sparse and can be more detailed.\n\nBased on the above-mentioned strengths, I vote for accepting. My concerns (further detailed below) potentially can be addressed during the rebuttal phase.\n\n++++++++++++++++++++++++++++++++++\n#### Major Comments\n\n1. (page 2) The requirement of ‘ability to modify the environment’ is listed as a limitation of prior art (Scobee & Sastry 2020). However, like the current approach, the prior art adds the constraints / modifies the environments only conceptually (and not physically). Further, both the current and prior work focus on the case of hard constraints. Please clarify this limitation of the prior art vis-à-vis proposed approach.\n\n2. (page 2) The rationale behind the objective (Equation 7) of the prior art and the proposed approach is identical. Please clarify, then, if the current algorithm is also greedy.\n\n3. (Equation 9) Please provide additional details for the inclusion of the entropy term in the policy loss function. \n  - The principle of maximum entropy is used to arrive at Eq. 4, the loss function of theta (since Eq. 4 uses the term derived in Eq. 2, which in turn is obtained from the maximum entropy principle). Given this, it is unclear why the entropy term is also included in Eq. 9. Is it used as a regularizer?\n  - Alternatively put, consider the unconstrained version of Equation 9. In this unconstrained case, the problem is analogous to MaxEnt IRL (Ziebart et al.). In MaxEnt IRL, given the reward $\\theta$, the policy $\\phi$ is computed by value / policy iteration and without the extra entropy term.\n  - Further, adding both $J$ and $H$ in the loss seem counterintuitive as they have different ‘units’. J is cumulative reward, while H is dimensionless entropy. Why is the entropy term normalized by $\\beta$? How is the normalization constant chosen?\n\n4. (Section 4) While not all domains considered in the Experiments can be captured by the prior art (Scobee & Sastry 2020), the first three can be (as they have discrete state, action spaces). Please benchmark the proposed approach with prior art for these three domains. Time permitting, also consider utilizing one of the recent high-dimensional techniques (see below) as another baseline.\n\n5. (Section 6) Space permitting, please include a discussion of following related works.\n\n  - Constrained IRL for high-dimensional problems:\n\n    * Chou, Glen, Necmiye Ozay, and Dmitry Berenson. \"Learning parametric constraints in high dimensions from demonstrations.\" Conference on Robot Learning. PMLR, 2020. \n\n    * Park, Daehyung, et al. \"Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning.\" Conference on Robot Learning. PMLR, 2020. Notes: Extends beyond the proposed approach to consider constraints which may not be global (i.e., locally active constraints).\n\n    * Chou, Glen, Necmiye Ozay, and Dmitry Berenson. \"Learning constraints from locally-optimal demonstrations under cost function uncertainty.\" IEEE Robotics and Automation Letters 5.2 (2020): 3682-3690.\n\n  - Inverse reward / policy learning frameworks that incorporate prior knowledge of reward / policy:\n\n    * Ramachandran, Deepak, and Eyal Amir. \"Bayesian Inverse Reinforcement Learning.\" IJCAI. Vol. 7. 2007. \n\n    * Michini, Bernard, and Jonathan P. How. \"Bayesian nonparametric inverse reinforcement learning.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.\n\n    * Unhelkar, Vaibhav V., and Julie A. Shah. \"Learning models of sequential decision-making with partial specification of agent behavior.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n\n    * Jeon, Wonseok, Seokin Seo, and Kee-Eung Kim. \"A bayesian approach to generative adversarial imitation learning.\" Advances in Neural Information Processing Systems. 2018.\n\n  - Learning features (which can be in the form of logical constraints) for IRL:\n\n    * Choi, Jaedeug, and Kee-Eung Kim. \"Bayesian nonparametric feature construction for inverse reinforcement learning.\" Twenty-Third International Joint Conference on Artificial Intelligence. 2013.\n\n++++++++++++++++++++++++++++++++++\n#### Questions for Rebuttal Phase\n\nPlease address comments 1-4.\n\n++++++++++++++++++++++++++++++++++\n#### Minor Comments\n\n- (typo) In the Introduction, Scobee & Sastry is used as singular noun, where in fact it is plural.\n\n- (Equation 5) Beta is missing in the log exponential term.\n\n- (Page 4, below Equation 7) The statement ‘Notice that … essentially tries to match’ is ambiguous, since the gradient by itself does not try to match the two values. Please consider rephrasing to say that this matching occurs at the minima (where the gradient is zero).\n\n- (Page 5, Section 3.3) Please denote the range $[0,1]$ as $(0,1)$, since 0 and 1 are not in the range of $\\zeta$.\n\n- (Equation 9) Consider distinguishing the loss functions in Equation 5 and 9 (say through superscript or subscript). Due to $L$ being overloaded, at first glance, I misunderstood the loss function in Eq 9 as a continued derivation of Eq 5.\n\n- (Section 7, typo) (2) -> Eq. (2)\n\n++++++++++++++++++++++++++++++++++\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Constrained Reinforcement Learning with Learned Constraints",
            "review": "Summary of review:\nInteresting work, clearly described and well executed. But weak support for the overall motivation, and incomplete baselines to confirm the approach is useful.\n\nDescription:\nThis paper describes a new approach for inverse constraint learning, whereby an RL agent infers constraints (on state/action pairs) from expert trajectories, and uses this in combination for a given reward function to optimize an agent’s behavior.\n\nThe main novelty in the work is the ability to infer constraints with fewer assumptions than previous (incl. handling continuous state/action spaces). The work is still limited to deterministic domains.\n\nThe paper is clearly written, well organized, easy to follow.  The authors are transparent about their assumptions throughout.  Interesting ablation studies are included in the experiments. The characterization of the results & discussion are fair and not over-inflated.\n\nMy main concerns with the work are: (1) the proposed setting itself, and (2) some missing baselines in the experiments.  Regarding the setting, I don’t really buy the motivation.  There are claims (p.2) that rewards are easier to express than constraints, and therefore it is reasonable to assume that the reward is given but the constraints are inferred. But I don’t see the support for this (experimental, theoretical or from the literature) either in people or in AI.  Furthermore, it is not obvious to me that this is a better strategy than straight-forward IRL.  Under some (reasonably flexible) assumptions, constraints can be transformed into (negative) rewards.  So why not just apply IRL to infer a reward function that includes both the positive (goal) and the negative (constraints)?  At the very least, this should be considered as a baseline in the experiments, applying one of the recent IRL methods, such as GAIL, to see whether there is any advantage to this formulation.  If anything, the proposed method makes a stronger assumption on knowledge (having the reward function) than most IRL approaches, so those should perform worse in practice.\n\nAdditional questions:\n-\tSec.3.1: Is it necessary for D (the observed trajectories) to be drawn from \\pi*_C?  Could the trajectories be drawn from other domains?  The results near the end on transfer suggest so.  Why not embrace this in the initial formulation?\n-\tSec.3.1 / 3.2: Can you clarify how you use the given reward function?  I assume this is r(\\tau) in Eqn 2, but if that’s the case, it may not be clear why you need to know the reward function a priori, rather than be able to observe it during trajectories.\n-\tSec.3.2: How do you draw negative examples for your binary classifier?  It seems all the trajectory data would be considered as positive examples.\n-\tSec.3.4:  I’m somewhat surprised that the importance sampling helps that much. In many other instances, the variance is too high to see an improvement in practice, especially when the importance weights are calculated over a full trajectory (not one-step).  Any idea why it works so well (as per Fig.5) in this setting?\n-\tFig.3:  Is the x-axis the number of trajectories of constraint data (i.e. D)?   In Fig.3(b),  Why do you break the constraints here?  Is this indicative of not enough data?  Why don’t you show the constraints for Walk2d (Fig.4)?  I strong recommend some analysis of the results in the main text to make sure to explicitly state the findings, not leave it to the reader’s reading of the plots.\n-\tWhy do you use only 3 seeds?  That seems insufficient, as per recent discussion on reproducibility in RL.  How were those chosen?\n-\tYou briefly mention some directions for future work in the last paragraph of Sec.7.  Can you also comment on what is the difficulty in each?  Why is the current work limited to deterministic MDPs, or hard constraints, or on-policy trajectories?\n\n\nMinor comments:\n-\tIntro: “This is known as agent alignment” -> It is also known as value alignment, and there are many more references.\n-\tIf you need additional space, some (or all) of Fig.2 and Fig.6 could be moved to appendix.\n-\tThe references need to be cleaned up. Stick to a consistent style (full first name, or initial, not a mix).  A few references don’t say “where” the work was published. A few references have the arxiv link, even though the work since has been accepted at a peer-reviewed venue.  This type of sloppiness gives a bad impression (suggesting the authors might be as sloppy with their scientific work.)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Potentially interesting contribution, evaluation is the weakest aspect.",
            "review": "POST-REBUTTAL COMMENTS:\n=========================\n\nSeveral of the original concerns, mainly about the scope of the contribution, have been addressed by the rebuttal, so I've increased the overall score. See my responses to the rebuttal for the concerns that remain outstanding.\n\n=========================\n\n\nSUMMARY\n\nThis submission extends Scobee and Sastry's ICML-2020 work, which formulates the problem of learning (hard) constrains in RL as inverse constrained RL setting, by generalizing it from tabular to continuous settings and, empirically, increasing the latter's scalability.\n\n\nHIGH-LEVEL REMARKS\n\nI believe that the paper, while incremental, might be making a valuable contribution. At the same time, the magnitude of this contribution is difficult to assess, because paper makes a number of unsubstantiated claims about them, doesn't thoroughly analyze the limitations of its approach, doesn't provide an empirical comparison to existing alternatives, and has several other presentation issues as detailed below.\n\n\nDETAILED COMMENTS\n\n-- The paper claims, \"[Scobee and Sastry] assume ability to modify the environment (specifically, to add arbitrary constraints to it).\" I'm not sure what is meant here -- adding a constraint doesn't mean modifying an environment. The constraints apply to a policy.\n\n-- The paper also claims, \"our approach [...] does not suffer from the curse of dimensionality\". This claim is unsubstantiated and doesn't sound true. Is the approach really oblivious to observation space dimensionality? Do you have a theorem showing this?\n\n-- The problems in the experiments are too small to support the above claim about the approach being impervious to the curse of dimensionality empirically, if that was the intent. In particular, the paper doesn't experiment with environments involving visual observations.\n\n-- How many expert trajectories were used in the experiments in Figures 3 and 4? \n\n-- The paper claims that its approach can handle arbitrary constraints. This is a misleading claim, at best. There are rich temporal-logic based constraint languages such as LTL, CTL, etc. The paper doesn't discuss whether they are subsumed but CMDP's constraint form even theoretically. In the meantime, even the fragments of these constraint languages that can be compiled into CMDPs' constraints can cause exponential state space size explosion -- consider, e.g., constraints that specify that every trajectory must visit state A before state B before state C, etc. So in practice the paper's claim can't be true.\n\n-- The paper's title, \"CONSTRAINED REINFORCEMENT LEARNING WITH LEARNED CONSTRAINTS\" isn't descriptive of the proposed technique. The paper assumes that the entire nominal MDP (the MDP without constraints) is given, along with a set of demonstration trajectories, and the agent tries to learn *only* the constraints. This means that once the constraints are learned, there is no more learning going on, only optimization. Thus, a more appropriate title for the paper would be something like \"constraint learning via maxent inverse RL\".\n\n-- The paper's assumption of the nominal MDP being fully known, including the transition and reward function, is steep, stronger than IRL's. It also seems to require that the environment should be able to indicate constraint violations during training. All of this raises the question: why not solve the constrained MDP using one of existing forward methods for doing so (the paper mentions a few in the related work) without explicitly learning the constraints? Note that the paper's experiments don't just evaluate constraint learning, they evaluate solutions to the MDPs at hand, so a comparison to forward CMDP RL solvers is natural to ask for.\n\n-- One answer to the above question can be that the goal of learning constraints is transferring them to other settings. But if so, the paper ought to analyze the effect of expert quality on the quality of constraints. Note that this effect can be drastic: if the expert is suboptimal, then its trajectories don't necessarily imply constraints, which means that the proposed methods might learn fictitious constraints that may make other environments where they are applied unsolvable. \n\n-- In general, the paper doesn't compare its approach even to alternatives that also try to learn constraints explicitly. They may try to learn more constrained constraint types, but it would still be very useful to see a comparison to one of them (see the \"constraint inference\" part of the related work section).\n\n-- The paper proposes to use a sigmoid at the output. I'm wondering whether this will lead to vanishing gradient for DNNs.\n\n-- Related to the above point, note that while the paper claims to list all relevant hyperparameters for the experiments, it doesn't describe the network architecture (and the number of experts used in some of the experiments, as mentioned above) either in the main body or in the appendix, which makes its results unreproducible.\n\n-- The use of J(.), with or without superscripts, for a value function is non-standard and confusing. The RL and controls convention is to use J(.) for a cost function to be minimized and V(.) for a value function to be maximized. The paper uses J both for a value function (the paragraph right before section 2.2) and, with a superscript, for a cost function (the first paragraph of 2.2). I strongly suggest bringing the paper in agreement with the standard convention, which will also remove the need to use J with a superscript.\n\n-- In the related work is reasonably complete. A notable omission is Le, Voloshin, Yue, \"Batch Policy Learning under Constraints\", ICML-2019). Also, note that Tessler et al's and Subramani et al's papers' venue is listed in arXiv, but both have actually been published in refereed venues (ICLR-2019 for the former, CoRL-2018 for the latter). Please cite them as such. \n\n-- Figure 5's legends are barely legible, please increase their font.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good algorithm derivation but empirical evaluation is lacking",
            "review": "This paper extends a tabular method for constraint inference to work in high-dimensional environments, and demonstrates it on a few environments.\n\nOverall I liked the derivation of the algorithm (modulo some nitpicks described later). However, the experimental evaluation is sorely lacking: there are no baselines compared against, even though the obvious candidate to compare against is preference learning algorithms, e.g. imitation learning (such as GAIL [1]) and inverse reinforcement learning (such as AIRL [2]).\n\nIndeed, it is not clear even theoretically what benefit constraint inference gives over the preference inference framework. While it is not common for preference inference to learn what _not_ to do (as this paper highlights), it is possible -- [3] is an example that creates some toy environments where the agent must learn what not to do, and then solves it using a single-state version of Maximum Causal Entropy Inverse Reinforcement Learning. I would expect that high-dimensional preference learning algorithms like GAIL and AIRL would also be able to do this, and thus are important to compare against.\n\nIn fact, it seems to me that the proposed algorithm can be cast as a special case of preference inference. This is most easily seen from the gradient expression in [4], where the MaxEnt IRL gradient in high-dimensional environments is written as\n\n$$\\frac{\\delta r_{\\theta}}{\\delta \\theta} [\\mathbb{E} [ \\mu ] - \\mu_{D_{\\tau}}].$$\n\nThis is the negative of what you might expect because they are defining a loss function to be minimized while you are defining a log likelihood to be maximized. (Note that $\\mu$ is the state-occupancy measure.) If you set\n\n$$r_{\\theta}(\\tau) = \\log \\zeta_{\\theta}(\\tau)$$\n\nI believe you recover the proposed algorithm. So it seems that the main contribution relative to existing algorithms is to propose this particular structure on the reward function (which corresponds to a hard constraint because when $\\zeta$ is zero the reward is negative infinity). So it becomes even more important to show why this particular structure is an improvement over e.g. the GAN-like structure in AIRL.\n\n(Other minor contributions include the use of importance sampling and the regularization of $\\zeta$, though it is also hard to tell how important these contributions are.)\n\nQuality: As explained above, I have serious concerns about the experimental section.\n\nClarity: The paper was quite clear. I’ve listed a few minor cases below for improvement.\n\nOriginality: To my knowledge no other paper has inferred constraints based on the maximum entropy assumption in high-dimensional environments. However, there are both preference learning algorithms (discussed above) and algorithms that learn logic specifications [5], which seem similar in spirit to constraints (though are not the same).\n\nSignificance: Hard to judge without better experiments.\n\nNitpicks / typos:\n\n> Note that this is not particularly restrictive since, for example, safety constraints are often hard constraints as well are constraints imposed by physical laws.\n\nShouldn’t physical laws (things like F = ma) already be encoded in the transition dynamics T? Why do we need constraints for this?\n\nEquation (5) seems to have dropped a $\\beta$ compared to Equation (4).\n\nShould Equation (8) have an absolute value? Otherwise I don’t see how it incentivizes the classifier to predict values close to 1. Actually, I see, the network is constrained by the sigmoid to output values in [0, 1]. It could be worth clarifying this.\n\nWhen going from Equation (5) to Equation (6), a policy π has suddenly appeared -- please explain what the policy is; the reader should not have to look at the appendix to understand the notation. (I assume it is a Boltzmann rational policy since you are using the MaxEnt framework.)\n\nFirst line of Section 4: “TwoBrdiges” -> “TwoBridges”\n\nReferences:\n\n[1] Generative Adversarial Imitation Learning, https://arxiv.org/abs/1606.03476\n[2] Learning Robust Rewards with Adversarial Inverse Reinforcement Learning, https://arxiv.org/abs/1710.11248\n[3] Preferences Implicit in the State of the World, https://arxiv.org/abs/1902.04198\n[4] Learning a Prior over Intent via Meta-Inverse Reinforcement Learning, https://arxiv.org/abs/1805.12573\n[5] Maximum Causal Entropy Specification Inference from Demonstrations, https://arxiv.org/abs/1907.11792",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}