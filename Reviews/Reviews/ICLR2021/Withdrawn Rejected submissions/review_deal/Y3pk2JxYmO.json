{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper introduces modifications that allow to make the training of contrastive-learning-based models practical. The goal of the paper is very interesting, and the motivation clear. This paper tackles a very important issue with recent unsupervised feature learning methods.\nHowever, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work. As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude. In its current form, this paper unfortunately doesn’t meet the bar of acceptance.\nGiven the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary:\n\nThe paper proposes a clever trick to make instance contrastive learning faster by using the intermediate feature layers to perform the contrastive loss rather than just using the final 2048-d mean-pooled features as is typically done in MoCo or SimCLR. That way, the backprop costs are cheaper. The authors also use this intermediate representation based similarities to guide a better set of negatives to the layers on top. The authors demonstrate good results that show better time to accuracy (on the linear classifier) with both MoCo and SimCLR. A useful consquence of this paper is making contrastive unsupervised learning on ImageNet more accessible to people with less computation resources, ex PhD students in academic labs who may not have a DGX-1 or v3-TPU pods.\n\nPros:\n\nTackles an important problem - how can we make self-supervised learning even faster with clever engineering\nDelivers on the problem by proposing two sensible solutions - saving the cost of backprop by using intermediate layers for contrastive losses, and also better negative mining while doing the fprop in a computationally efficient manner.\nGood results in terms of time to accuracy on linear classifier - both with MoCo and SimCLR - two leading instance contrastive learning approaches.\nCons:\n\nNo code release as of yet. I believe the utility value of engineering driven papers is high but heavily relies of clean open source code that's usable by the community.\nNo results with fewer GPUs. If you can get results faster but still use 8 GPUs (though I do notice you have used 12 GB memory Titan V as opposed to 16 Gb memory GPUs used by MoCo, I don't think that's a deal breaker since MoCo ResNet-50s can be trained with the same HBM too). I think what's more important is to show you need fewer GPUs - can someone with a single GPU get results with the same amount of compute-time taken by Facebook to get MoCo's results.. That's what really increases accessibility to folks with less compute IMHO.\nReducing time to accuracy could be done in a host of other ways - eg - make everything run in fp16 or bfloat16 on the right kind of hardware - you would see a speedup of ~1.3x already. Use smaller image sizes while training, eg - instead of 224x224, use 192x192, use different temperatures (tune hyperparams) for training within as few epochs as possible. Use less unlabeled data to get the same accuracy. All these are orthogonal to your proposed method ofc, so not trying to compare them with yours. My point is that - why not push the broader goal to the extreme - can one combine all these engineering tricks in one piece, and design a way faster contrastive learning pipeline that can train with way less resources and still perform as well in much less wall clock time..\nThe Related work section can be improved - example, Contrastive Learning prior work should mention the CPCv2 work (Henaff et al 2019); and the idea of using intermediate layers and backprop. the losses through them should mention the Inception architecture (Szegedy et al. 2015) as well as Deep Classification work that did this few years ago for supervised learning.\n\n\nRating: Weak Reject - I would be open to reconsidering this if the authors commit to releasing their code, and also performing some benchmarking with respect to number of GPUs, and revising related work. I also encourage the authors to push further on making their pipeline even more efficient.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Adding intermediate losses and hard pair selection to speed up the training of contrastive learning",
            "review": "**Summary**: \nThis paper proposes a new pipeline to speed up the training of contrastive learning. Specifically, besides the contrastive loss placed at the very end of the network, it introduces several additional intermediate losses. During the training, only one of them is used to compute gradients. With these intermediate losses, it can be also used to filter out the easy pairs. These two strategies can significantly accelerate contrastive learning while matching the performance of the recent methods. Authors conduct experiments and ablation studies to show the effectiveness of the proposed method on ImageNet and downstream tasks.\n\n**Pros**:\n+ The whole idea makes sense.  It introduces the intermediate losses to reduce the computation costs, and filter out easy samples to  recover the performance. \n+ Authors conduct experiments on linear classification and downstream task transfer learning to show good performance.\n+ Overall, the paper is well written and well organized.\n\n**Concerns**: \n- As we add multiple intermediate contrastive losses to the network, can we also evaluate the linear classification and downstream transfer task using the intermediate representations (after the project head)? I am curious about whether they can also obtain good performance.\n- I wonder what if we update the network using all loss L, but not randomly sampling one of them? Will it improve the baseline performance?\n- Does the hyper-parameter M affect the performance a lot when doing the hard pair selection? Can we do the ablation study on it?\n- As mentioned in [1], the low- and mid-level representations, not high-level presentations, make the instance discrimination good for the detection task. This may also support the effectiveness of adding intermediate losses. \n\n[1] What makes instance discrimination good for transfer learning?\n\n**Minor Comments**:\n* Page 2 (middle), below eq(2), same lass -> same class\n\nOverall, I prefer the rating as above the threshold at the current stage. Hope the authors could address my concerns or questions in the rebuttal period.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Promising initial results, curious to see if/how it scales to more pre-training epochs",
            "review": "This paper proposes techniques to speedup contrastive self-supervised pre-training. The two key ideas are to back-prop only through a subset of layers in the network (from a random layer, back to the input), and to drop training instances which are not \"hard\" (in a way defined more precisely in the paper). By combining these techniques the paper shows that pre-training time can be reduced by ~30% without significant loss in performance on downstream tasks.\n\nThe proposed acceleration techniques are evaluated on multiple contrastive learning methods (SimCLR v1, MoCo v2, SwAV) and for multiple downstream tasks (image classification, segmentation, and object detection). The experiments convincingly demonstrate that the accelerated approach works well when pre-training for a relatively small number of epochs.\n\nMy main criticism is that the performance values reported in the paper do not always match what is reported in the literature. Presumably this is because the amount of pre-training is less than what was done in previous work. For example, SwAV reports achieving 75.3 top-1 on ImageNet with linear classification, while Table 1 in the paper only reports 70.1. Although the acceleration approach works well early in training, it is not clear whether the same benefits play out when one aims to achieve performance on par with that reported in the literature. More (longer) experiments are required to support this claim. \n\nOne would expect that, eventually, the approximation techniques introduced to speed up training, may have a negative effect on the ultimate performance. It would be useful to know at what point in training (or at what scale) this becomes apparent. And if it is not the case that the approximation techniques cause any loss in performance, it would be nice to have some deeper understanding of why that's the case.\n\nThe claim that the proposed approach reduces the (average) memory usage during training is accurate, although it should be clarified that this is the _average_ memory usage. The peak memory usage still occurs when all layers are used to produce the representation, and this is no different than the peak memory usage of standard methods. Anyways, I guess the main message is about reduction in training time, and memory savings could be down-played.\n\nFine-tuning the full architecture generally performs much better than just linear evaluation for image classification. Did you also investigate the performance of methods using fine-tuning? Also, the test accuracy reported in Fig 3(b) is much lower than that reported in the SimCLRv1 paper (Table 6). Is it clear how Accel-simCLR behaves when run for more pre-training epochs?\n\nMinor nit: Looking at the values in Table 1, I wonder why runtime isn't proportional to epochs for MoCo v2 (for both the standard and accelerated versions).\n\nHow many replicas are standard deviations computed over?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Slight acceleration achieved, but much more to be explored?",
            "review": "In this submission the authors suggest modifications to reduce the computational cost of contrastive learning. The authors propose to add constrastive loss heads at intermediate stages of the network. Using these intermediate losses as a proxy for the final contrastive loss they make two proposals for  reducing computational effort: 1) randomly choosing which contrastive loss to start from such that on average less of the network is actually applied. 2) randomly cropping the image at each intermediate loss and continuing only with hard patches for the positive samples. \n\nOverall, I am on the fence for this one. I think adding the intermediate contrastive losses is an interesting idea and the proposed changes already improve training times. However, the reductions in training times are not overwhelming and I believe there would be many more aspects to explore for earlier contrastive losses. Thus, I am not overly enthusiastic.\n\nPros:\n1) Earlier contrastive losses are an interesting idea!\n2) There is a reduction in training times at negligible cost in performance.\n3) The method is quite general over contrastive learning methods.\n\nCons:\n1) The reduction in training times is not overwhelming given the amount of changes.\n2) While the authors present a few analyses which parts of their method and network are important I don’t think these are particularly informative.\n3) Having losses for the individual parts of the network, the immediate idea would be to train the parts separately which would remove many more backpropagation operations and would allow parallelization of the network parts onto separate machines. This possibility is not explored here.\n4) Given that the stated aim is to reduce training times it seems that there is very little analysis on what actually takes time in this approach. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}