{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction.  The main theoretical result is that individual ensemble members should not be individually calibrated in order to have a well-calibrated ensemble prediction.  While other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results. \n\nPros:\n* Theoretical study of ensemble calibration with meaningful insights\n\nCons:\n* Contributions limited to theoretical study of known observation and dynamic temperature scaling.\n* Dynamic temperature scaling is not shown to outperform baseline methods.\n* Limited experimental validation: CIFAR-10/CIFAR-100.\n\nThe authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the SKCE calibration measure.\n\nOverall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular: 1. limited experimental evaluation (one type of task, one/two datasets only), and 2. given known literature the benefit of the derived theoretical results to practioners is not clear.  The discussions have been unable to resolve this disagreement.\n"
    },
    "Reviews": [
        {
            "title": "AnonReviewer1",
            "review": "- **Summary**:\nThis paper investigates the impact of different calibration strategy (pre-combination, post-combination and its dynamic variant) on the performance of a deep ensemble. It presents both theoretical and empirical proof to show that well-calibrated ensemble member does guarantee calibration in the final ensemble.\n\n- **Strength**:\n  * A coherent theoretical account for the issue of calibrating Deep ensembles. Accompanied by empirical evidence from CIFAR datasets.\n  * Although not stated explicitly, a new calibration approach (dynamic calibration) is introduced, which empirically leads to better performance.\n\n- **Weakness**\n  * Novelty may be limited: one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery (2020) and Wen et al. (2020).  Although I appreciate author's work on providing mathematical explanation for recent empirical findings, I'm not sure if the submission in its current form  is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence, since max of the mean probability is no greater than mean of the max probabilities. On the other hand, the empirical investigation is conducted on a single vision task (CIFAR-10/-100). This paper can be made stronger by investigating synthetic situation where the ground truth is known, or extend experiment to also other data modalities (like  Guo et al. (2017)). \n\n  * Organization: Given the place of the new approach (dynamic temperature scaling) in the experiment, it might be worthwhile to devote some paragraph to introduce the procedure in more detail.\n\n- **Recommendation**: Based on reason stated in weakness, I recommend rejection since the either theoretical or the empirical contribution of this paper does not seem to be substantive enough for ICLR.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Theoretically backed explanations for expected conclusions",
            "review": "The paper makes an analysis of calibration in ensembles of deep learning models. Through some theoretical developments, the paper supports that a given ensemble cannot be more confident than the average individual members for regions where the ensemble is well calibrated. Empirical results, on CIFAR-100 and three different deep models, report a comparison of ensemble calibration, where calibration is done over all members in order to achieved a calibrated ensemble decision, over individual calibration of members with no feedback from the ensemble decisions. Results show that individual member calibration does not lead to calibrated ensembles, and as such calibrating directly on the ensemble output is required for obtained a proper evaluation of its uncertainty. Different ensemble calibration approaches are also compared.\n\nPros:\n- Overall well-written paper.\n- Straightforward proposal, simple yet meaningful on several aspects for better understanding of the link between calibration and ensembles.\n- Rely on theory to support some claims, which strengthen the proposal.\n\nCons:\n- The proposal is somewhat trivial, although I do not have knowledge that it has been investigated in detail elsewhere. Before reading the paper, I expected the results (i.e. calibration of individual members will not lead to calibrated ensemble decisions; calibration at the ensemble level is required), the paper is somewhat confirming this in a more explicit manner.\n- Evaluation on only one dataset (CIFAR-100) in the main paper, with another dataset for the appendix (CIFAR-10).\n- Results on CIFAR-10 in the appendix are not very compelling.\n- It is hard to make sense of the results in Table 1 and similar. Differences are small and difficult to interpret.\n- The explanations and organization of the paper are hard to following in some specific part.\n\nAlthough the paper is making a well-founded analysis of a hot topic in the last few years (i.e.,  ensembles are a way to evaluate uncertainty on decisions), I found it having some relatively trivial developments. And the conclusion is intuitive and expected. However, it is the first time I see this point well articulated, and the authors have made a good effort to develop theoretically backed explanations to support this.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting observations, writing can be improved",
            "review": "Update after the author response: I've read the other reviews, and agree with R2 and R3. I think the paper is useful (emphasizes you need to calibrate the final ensemble, not enough to calibrate members), and has some nice conceptual contributions (explaining that if ensemble accuracy > average member accuracy (which is usually the case), and the ensemble is calibrated even in just a global/weak sense, then the members must be uncalibrated). This could spur more research into conceptually analyzing ensembles, and seems interesting. But I understand the other reviewer's concerns that it's not clear what practical impact this will have, so I'm keeping my score at a 6 (instead of raising to a 7).\n\n#########################################################################\n\nSummary:\n\nThis paper tackles the problem of calibrating an ensemble. They show experimentally that calibrating all members of an ensemble is often not enough to calibrate the combined ensemble, so instead we need to calibrate the final predictions of the ensemble. Additionally, they show that using a different temperature parameter for different regions of outputs can improve calibration. They explain why if the ensemble members are top-label calibrated (even in a very weak sense they call “global” calibration”), and the ensemble is calibrated, then the ensemble is less accurate than the average member of the ensemble.\n\n#########################################################################\n\nReasons for score:\n\nThey make interesting observations about calibration of ensembles that could guide practitioners. For example, that it’s not enough to calibrate the members of the ensemble. They also raise an intriguing connection between calibration of ensemble members and ensemble accuracy, one would not expect a priori that if both are calibrated the ensemble would do worse than the average member. I could see this result being interesting to people who study ensembles as well. There are some weaknesses in writing and execution, but overall this paper is probably worth publishing if edited.\n\n#########################################################################\n\nPros:\n\n- I think it’s a nice observation that calibrating the members of an ensemble may not yield a calibrated ensemble. It’s easy to come up with toy examples where this is the case, but it’s interesting that this seems to be the case in practice.\n\n- They make an intriguing observation that if the ensemble members are in fact calibrated and the ensemble is calibrated, then the ensemble accuracy is at most the average member accuracy\n\n\n#########################################################################\n\nCons:\n\n- I believe the writing can be substantially simplified. The core ideas are simple and nice, but it takes a lot of effort to get to them, and I believe the authors should put in more work into making this understandable.\n\n- Some of the results seem unrealistic and can be omitted. For example in the start of section 4.1, the first couple of results require that the ensemble member regions and ensemble regions are the same. This seems rather unrealistic. The assumptions in prop 1 seem too strong to me. I’d remove the mentions of regions and I’d instead mention the other results (prop 2, 3, 4) in the main paper, Section 4.1. You could just move the propositions, and give some intuition for why the results are true. Removing regions should also considerably simplify the notation and setup.\n\n- I’m not quite sure what you mean in the intro when you say “Eq. (1) doesn’t explicitly reflect the relation between … and the underlying data distribution p(x, y)”. The definition in Equation (1) uses p(x, y). I’m not sure why all the definitions in 3.1 and 3.3 are defined in a way different from the standard ways in the calibration literature e.g. in Kull et al 2019 or Kumar et al 2019.\n\n- Temperature scaling is performed on logits, not on the actual probabilities. From equations 24, 25, and 26 it looks like you might be doing temperature scaling on the probability space (in equation 24, 25 the first argument to f is the probability, not the logit), which looks a bit odd.\n\n- Prop 4 should also hold when K = 2 (2 ensemble members) I believe. Happy to provide an example.\n\n- Some symbols are undefined. For example, \\delta(y^{(i)}, \\omega_j), I don’t believe \\delta is defined. I think it should be 1 if they are equal and 0 otherwise?\n\n#########################################################################\n\nQuestions and things to improve:\n\n- Please answer the cons above.\n\n- Ensembles are particularly useful because they tend to be more calibrated out of domain (Lakshminarayanan et al 2017). It could be useful to see which of these methods (calibrating the members, or the entire ensemble) is better calibrated when we have domain shift (e.g. training data = CIFAR-10, test data = CIFAR-10C, Hendrycks et al 2019).\n\n- Having confidence intervals for the calibration errors would be nice (and also using more modern, debiased estimators to estimate the calibration error) e.g. in Kumar et al 2019. \n\n#########################################################################\n\nAll cites mentioned are already in the paper, except: \nBenchmarking Neural Network Robustness to Common Corruptions and Perturbations. Dan Hendrycks, Thomas Dietterich. ICLR 2019.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Review (Reviewer2)",
            "review": "- In general, my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient.  \n- I also agree with AnonReviewer3  and AnonReviewer4 but feel less excited about the prons and more worried about the cons. \n\nAt this point, I'm not against the acceptance of the paper, although I'm still staying on the rejection side. I'm increasing my score because we are at least talking about a borderline.\n\n--------------------------\n\nSummary:\n\nThe paper study calibration of ensembles of DNNs and its relation to the calibration of individual members of ensembles. The work demonstrates that i) members of an ensemble should not be calibrated, but the final ensemble may require calibration (especially if members of an ensemble are calibrated) ii) provide theoretical results to support the statement iii) propose an adaptive calibration scheme (dynamic temperature scaling) that uses different temperatures based on the confidence of a model. \n\n\nConcerns: \n\n1) The main question of the paper \"Should ensemble members be calibrated?\" feels trivial, because the community is aware of the simple example that provides an answer. The Deep Ensembles [Lakshminarayanan2017] have miscalibrated members---conventional DNNs, but the predictions of an ensemble are, in-most-cases, calibrated. Thus the answer is \"No\".\n\n2) The paper mostly is clearly written, but section 4.1 \"Theoretical Analysis\" is *extremely* hard to follow. Even though I re-read it many times, I'm still not sure if I understood it correctly. The most confusing part is the conclusion \"In practice, there is no constraint that the ensemble prediction should be calibrated, thus ensemble prediction calibration is required even for top-label calibrated members.\". It seems that no listed results were used to produce this statement.    \n\n3) The calibration of ensemble has been proposed in [Ashukha2020, 5 Discussion & Conclusion]. (\"The resulting ensemble predictions ..., requiring calibration functions to be optimized for the ensemble prediction, rather than ensemble members.\")\n\n4) The two main contributions (4.1 Theoretical Analysis, 4.2 Temperature Annealing for Ensemble Calibration) feels not related, they are basically two independent topics packed in the one paper.\n\n5) The empirical comparison exploits the calibrations score (e.g., ECE). ECE  is a biased estimate of true calibration with a different bias for each model, so it is not a valid metric to compare different models (see Vaicenavicius2019). The fact is even mentioned in the current paper (\"It should be noted that for finite number of samples ... \") but still is ignored in the empirical study.\n\nWhat I suggest is to use the squared kernel calibration error (SKCE) proposed in [Widmann2019] along with de facto standard, but biased ECE. The SKCE is an unbiased estimate of calibration. There might be some pitfalls of this metric that I'm not aware of, but the paper looks solid and convincing. Also, please put attention to Figure 83 in the arХiv version. Yes, ECE is the standard in the field, but it is the wrong standard that prevents us from meaningful scientific progress, so we should stop using it.\n\n6) The results provided in Table 1 seem to be close values (0.6119 vs 0.6129, etc.), so at least standard deviations need to be reported. Also, there is no mentioning of several runs per results in the text.\n\nThe paper toches the nice topics but, overall it feels like \"ok, but not enough\". The theory is interesting but it does not give us a lot of insides (maybe it's very subjective). The dynamic temperature scaling is not proofed to outperform the basslines. The contributions feel disconnected. The writing quality needs to be improved.  \n\nComments: \n\n1) As far as I can tell, the citation \"The weights assigned to the probabilities are either optimized using AUC as in (Ashukha et al., 2020) ...\" is incorrect, as there is no mentioning of optimizing weights using AUC in the paper.\n\n2) typo: It should be noted that for A finite number of sampleS\n\n[Lakshminarayanan2017] Lakshminarayanan B, Pritzel A, Blundell C. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems 2017 (pp. 6402-6413).\n\n[Ashukha2020] Ashukha A, Lyzhov A, Molchanov D, Vetrov D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. ICLR, 2020. \n\n[Vaicenavicius2019] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas B Schon. Evaluating model calibration in classification. AISTATS, 2019.\n\n[Widmann2019] Widmann D, Lindsten F, Zachariah D. Calibration tests in multi-class classification: A unifying framework. In Advances in Neural Information Processing Systems 2019 (pp. 12257-12267). https://arxiv.org/pdf/1910.11385.pdf",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}