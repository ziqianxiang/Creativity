{
    "Decision": "",
    "Reviews": [
        {
            "title": "an interesting work, but needs some clarifications ",
            "review": "This paper provides good analyze of why and how normalization can help the optimization procedure of GNNs as a Preconditioner and shows batch-level statistics are noisy for GNNs.  Based on these observations, it proposes GraphNorm, which normalizes each individual graph instead of batch-wise, and learns a parameter for each feature to automatically control how much the mean to preserve in the shift operation. \n\n1) First of all, I think authors might want to clarify what is the meaning of 'individual' graph even from the beginning of the paper. By reading through the paper, I still do not get what is the meaning of that term. Do you mean you partition the graph into many sub graphs? or partitioning the graph inside each batch? and how each subgraph is formed? is each node forming an individual graph? What is the relationship between each subgraph or individual graph with a batch in traditional GCN training? \n\n2) That is an additional parameter \\alpha learnt during normalization. What if you also learn such parameter in batch normalization? This will tell us whether the normalization over each 'individual' graph using graph mean is making improvement.\n\n3) Will this method be the same as we just use smaller batch size? how that compared with the proposed method? \n\n4) The graphs experimented in the paper are all small scale -- less than 10k nodes. It would be interesting to see how the proposed method works on larger scale dataset.\n\n5) It seems the only normalization method compared with is batch norm, it would be interesting to see how the proposed method compared with other normalization schemes.\n\n6) how much additional computation for the proposed scheme comparing with batch norm?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A normalization scheme for learning with graphs",
            "review": "Summary\nIn this paper, the authors propose a sample/instance-specific normalization scheme for learning with Graphs. This is in contrast to BatchNorm, which uses batch level statistics for normalization. The authors also suggest to use a learnable parameter while subtracting the mean, which avoids a possibility of losing graph information. Through the experiments over several datasets, GCN/GIN with the GraphNorm is shown to perform better than those with BatchNorm. \nThe paper is written clearly and easy to follow. Over the paper, the authors gradually motivate the proposed scheme using the observations made using Theorem 3.1 and Propositions 4.1, 4.2. \n\nPros\n1. The proposed scheme is simple and easy to implement. \n2. The advantage of the method is analysed well for graphs of different nature (regular vs non-regular)\n\nOpen questions requiring clarification.\n1. How is that the matrix QN has better condition number than Q ? It is shown by the author that the minimum singular value = 0. \n\n2. In Sec 3.2, it is mentioned that batch norm is applied using the batch level statistics, which may be different from the dataset level statistics, which in turn is used in testing. If that is the case, why can't one always use dataset level statistics even in the training phase, since it will anyway be used in the testing phase too ?\n\n3. Propositions 4.1, 4.2 show that the normalization (7) may lead to loss of graph information for regular / complete graphs, which inspires the authors to use control the mean shift using a learnable parameter. In such a case, how does Theorem 3.1 extend to the modified normalization scheme? \n\n4. In addition, does the property \"normalization decouples the optimization of direction and length of the parameters\" mentioned in Sec 1.1 hold for the modified normalization scheme ? \n\n5. In Table 1, the improvement in performance for some of the datasets is marginal. \n\n6. As another comparison, in Figure 4, it would be interesting to see the performance where the BatchNorm is done using the statistics computed using the entire training set (similar to the testing setting). It might also be interesting to see Figure 4 with x-axis chosen to be the time taken rather than the iterations. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper introduces a new normalization for graph neural network, GraphNorm, that experimentaly shows great efficiency. However the theoretical claims do not support the introduced algorithm, making the paper not good enough as it currently is.",
            "review": "The paper focus on understanding the shift operation when performed on the attributes of a graph during tranining, such as BatchNorm. The author lead a theoretical analysis pointing out some issues with classical BatchNorm for graph neural networks and to support the claim that normalization shift may bring some advantages for the training. They introduce a new normalization method called GraphNorm that use the individual graph statistics instead of the whole batch. The efficency of their method in term of speed of convergence is then illustrated on a set of classical graph benchmark datasets.\n\nPros:\n+ The paper is overwhole well written and easy to follow.\n+ The experimental analysis conducted in the main paper and in the appendix is quite interesting, especially when looking at the noise introduced in the normalization layer, shreding some light on BatchNorm.\n+ The experimental speed up observed (Figure 4) is quite impressive.\n\nCons:\n- The theoretical justifications are rather weak: to my understanding, theorems seem not to be much related with the claims.\n- Section 3.1\n    Theorem 3.1, to my understanding, this theorem do not prove any preconditionning or impact on the convergence, only that the eigenvalues are interlaced.\n- Section 4: study of GIN on regular graphs is dubious, as the obtained results are due to the structure of MPNNs and not of any particular shift.\n\nRemarks:\n- Section 2: Word Art in Figure 1 is definitely fun but I'm not sure it's required in an ICLR paper.\n- Table 1: the bold font is misleading as it does not take into account statistical signifiance. The accuracy of GIN+BatchNorm is not statistically different from GIN+GraphNorm on the first 7 datasets.\n\nQuestions:\n- It would be interesting to study the gradient flow for BatchNorm and GraphNorm, this might give much stronger arguments in favor of the proposed method.\n- How does graph norm perform when one adds more layers to the models?\n\nTypos:\n- sentence starting with a symbol (a few cases in the paper)\n- Theorem 3.1: $\\mu_0$ instead of $\\mu_n$.\n- many references are incomplete, e.g. Layer Normalization is NeurIPS'16, Spectral Networks and Deep Locally ConnectedNetworks on Graphs is ICLR'14, etc",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "# Summary\n\nThe paper presents a normalization method and its theoretical motivation. The main insight behind the proposed method is that normalization in graphs leads to enhancement in condition numbers. At the same time, the paper demonstrates that with regular connections and Graph Isomorphism Networks (GIN), there is degradation in the expressivity of Graph Neural Networks (GNN), and thus introduce a learnable parameter in the equation to avoid this. Experimental verification shows improved performance over Batch Normalization (BN).\n\nMy preliminary rating for this paper is leaning towards rejection, mainly because of the following factors: (1) the theoretical analysis seems to have logical gaps; (2) the most closely related work, Instance Norm (IN), is not compared against or mentioned in detail. I will detail these below, including other minor issues.\n\n# Strengths\n\nThe problem at hand is of large interest recently, especially with the attention that graph-representable data is receiving. The method provides an attempt at how one could analyze different normalization techniques theoretically to understand which design direction to take.\n\nThe method improves significantly over batch normalization, which shows some promise, albeit it is unclear if the change in the method compared to instance normalization is significant enough.\n\n# Weaknesses\n\n## Logical gaps\n\nThe paper does a great job in relating normalization and the condition number of QN in the paper but becomes handwavy in Section 3.2 when explaining the downside of BN. I may be missing something, but as the paper describes, the analysis for Theorem 3.1 can be applied in the same way for BN with a supergraph. If this is true, then BN also enhances the condition number, as in the case of the proposed method. \n\nThus, I find it hard to relate how the noisy batch statistics relate to the analysis in Section 3.1. This makes the theoretical analysis, while seems correct, less appealing.\n\nTheory in Section 4 is also less appealing given that they are provided only for the case of GIN. While it is stated that similar results can be easily derived, it is unclear how exactly. Since it's stated as easy, it would be great if the analysis is provided for a couple more architectures.\n\nFinally, the last logical gap is related to equation (9). It would be great if the paper presents that introducing alpha theoretically removes the problems highlighted in Propositions 4.1 and 4.2.\n\n## Comparison with Instance Norm (IN)\n\nThe proposed method, Graph Normalization, is extremely similar to what is called IN [a]. While [a] is mentioned, is clearly not distinguished from this work. Other versions of this have been also suggested in literature in the context of permutation equivariant data processing, namely Context Norm (CN) [b]. For [b], as it operates on point cloud data, if considered as a fully connected graph, what they do correspond to GCN. In [b], it is shown that CN is the critical component that enables reasoning on this point cloud data, which in terms of the terminology of this paper, is the proposed GN, but without the learnable parameters. CN also has a follow-up named Attentive Context Norm (ACN) [c], which has a weighing term on the aggregated statistics---the mean and the variance---closely resembling equation (9).\n\nGiven the similarity, there is a strong need for theoretical, and empirical comparison with these methods. Moreover, it has recently been shown how important IN is in the context of set functions---up to a level where IN alone, without any training, is able to provide state-of-the-art performance [d]. Given these insights, I find it hard to justify this absence.\n\n## Minor comments\n\nVertices of the graph are referred to as nodes. While this is fine, I would personally prefer vertices.\n\nThe definition of ReLU and MLP is never provided. I presume they are the rectified linear unit and multi-layer-perceptrons, but this is not explicitly provided. \n\nI am not a fan of Figure 1. It provides an architectural framework, but then also provides a K.O. message, but the most related work is actually Instance Norm.\n\nThere is no x axis explanation for Figure 2.\n\nProof of Theorem 3.1 is in the appendix, but never referred to in the main text.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}