{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting connection between RL and Operations Research, but technical novelty is insufficient for publication.",
            "review": "The authors consider the Flexibility Design Problem, which is an NP-hard stochastic combinatorial optimization problem with applications in supply chain management. The authors propose to cast the problem as an episodic RL problem, with state transition (which is deterministic) representing the insertion of an edge in the supply network. After that, the authors apply the policy gradient algorithm for solving the RL problem, and apply a meta-learning algorithm by Finn et al. 2017 for solving a related meta learning problem. Finally, the authors compare their algorithms with existing algorithms in the Operations Research domain and show that their algorithms have a better empirical performance. The authors also introduce a variance reduction procedure for mitigating the stochastic variation during the RL training.\n\nWhile I appreciate the authors' attempt to draw connection between RL and Operations Research, I find that the technical novelty of the work below the bar for publication. Therefore, I am on the side of rejection. Indeed, the paper's contribution is mainly translational, since the authors are applying well-known (policy gradient) or existing (meta-learning) algorithms for solving the stochastic optimization problem. To my understanding, the major parts in the paper that do not appear in the RL literature are the formulation of the edge insertion procedure as an MDP as well as the variance reduction procedure, which are rather straightforward and intuitive. The depth of these contributions are not sufficient compared to the publications in ICLR to my knowledge.\n\nApart from the contribution perspective, I also have doubt on the benchmarking of the proposed algorithms. While I do not research on stochastic combinatorial optimization, my understanding is that there are research (which could be classified as outside the machine learning community) that works on heuristics for NP-hard stochastic combinatorial optimization. For example, there are numerous well-known heuristics such as Ant Colony Optimization, Simulated Annealing, local search, Monte Carlo Tree Search. See this survey article for example:\n\nhttps://link.springer.com/article/10.1007/s11047-008-9098-4\n\n(There could certainly be more recent development, but it is the authors' responsibility to conduct the literature review). My main question is, why are these well known heuristics not considered? The authors should survey about major heuristics on stochastic combinatorial optimization, and investigate if they are applicable to the FDP, and then benchmark their proposed algorithm with those applicable heuristics. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A nice paper with lots of room for improvement",
            "review": "Summary of the paper:\nThe paper proposes a Deep Reinforcement Learning (DRL) approach to a classical problem in Operations Research (OR): flexible design or manufacturing. This is a 2-stage stochastic integer programming problem on a bipartite graph. On one side of the graph are resources (e.g. factories), and on the other are demand types (e.g. car models). The car manufacturer has the option of equipping factories to be able to produce a subset of the car models, each at some fixed cost. Factories have production capacities. Demand for each car model is assumed to be stochastic and the corresponding distribution can be sampled from. For each (factory, car model) pair, there is an associated known revenue. The overall objective is to maximize profit: (the expected revenue with a given network configuration F) minus (the cost of building F). The expected revenue term involves solving a resource allocation problem for a fixed F.\n\nThis flexible design problem has numerous application and is computationally challenging due to its combinatorial nature and stochastic demands. Even for a finite set of stochastic demands, converting the design problem to a deterministic single-stage integer program results in a very large formulation which is challenging even for the best integer programming solvers. \n\nThe paper contributes to this problem in the following ways:\n- An RL formulation of a sequential edge-addition algorithm is proposed;\n- Some enhancements to the RL training procedure are proposed. These include variance reduction for reward estimation by using a kind of baseline, and a meta-learning approach for adapt an existing learned policy to new tasks which are slightly different;\n- Experimental results on three different real-world problems that fit within the flexible design framework, showing that the learned policy is competitive with two heuristics from the OR literature;\n- Ablation studies to assess the impact of the proposed RL training enhancements.\n\n\nStrengths:\n- A very interesting stochastic combinatorial optimization problemm which is both practically important and not studied in the growing ML-for-combinatorial-optimization literature;\n- A clear, well-written paper which provides sufficient technical details on both the OR and DRL sides;\n- Promising experimental results, and a careful ablation analysis.\n\n\nWeaknesses:\n- Most of my concerns (see Questions below) have to do with the experimental setup and the running time comparison, which I believe can be improved substantially. \n- Besides experiments, I am concerned that the current policy/value networks have an immense number of parameters, and are not invariant to shuffling the ordering of nodes/edges in the bipartite graph.\n- Last, I am concerned that the authors may be training and testing on the same problem, which reduces the RL training phase to an (intelligent) search procedure.\n\n\nRecommendation: I am currently at (5: Marginally below acceptance threshold), but I do like this paper quite a bit and would be ready to go up to 6 or 7 if the authors can address my questions.\n\n\nQuestions to the authors (in no particular order):\n\n1- \"Finally, for each instance of the FDP, we perform training runs with 12 different seeds, and use the policy from each seed to extract 50 ﬂexibility networks via greedy action selection, producing a total of 600 ﬂexibility networks. Then, we choose the best network among the 600 using a set of 5000 randomly generated demand samples.\": I couldn't understand this sentence; could you please explain how you extract 50 flexibility networks with greedy action selection, which I assume is deterministic? Also, what do you use this \"best network\" for? Do you mean that this is the network whose objective value you report in Figure 2? \n\n2- Following up on 1-, if you are training and testing on the same problem, then you are essentially overfitting.\n\n3- Figure 2: Related to 1-2 above, you seem to be testing on (28 problems = 4 scenarios x 7 values of K). This is a relatively tiny set of testing problems. Could you grow this set by having multiple instances with different demand distributions?\n\n4- Appendix A: how many demand samples do you use for Greedy and SP? Is that value tuned on training data? If not, it should be for fair comparison with your method.\n\n5- Appendix A: \"For both greedy and SP heuristics, we choose S = 1000\"; what is S here?\n\n6- How do you represent the current state s_t and action a_t when you pass it as input to the value and policy networks? Is it just the adjacency matrix of the current network, or do you do some feature engineering? If you use the adjacency matrix, the neural network will not be invariant to matrix reordering, whereas it should be; graph neural networks and other similar architectures may allow for invariant representations. Thoughts on the issue of invariance?\n\n7- Runtime scalability: I appreciate your honesty about the running time of your method, which is much larger than the two heuristic baselines. Is this due to inference time in the policy and value network? Also, you give rough time estimates for m=n=10, but it's not clear how the running times of all three methods would grow as a function of m * n or m + n. I imagine that larger m and n might require larger policy/value networks, and thus further slowdown. \n\n8- Fair comparison with heuristics taking runtime into account: if your learned policy takes 50x longer than Greedy, then a modified Greedy can be given 50x more time than original Greedy so that you have a more fair baseline to compare against. For example, you could run many copies of Greedy, each of which uses different demand samples, after which the best of the Greedy solutions is returned and compared to your method.\n\n9- Related to 6-7, the value network seems to have > 2 million parameters, despite m and n being small. This seems like an overkill, affects invariance, and possibly results in a much slower method. Any thoughts on this issue?\n\nMinor:\n- Related work: Bengio, Yoshua, et al. \"A learning-based algorithm to quickly compute good primal solutions for Stochastic Integer Programs.\" International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research. Springer, Cham, 2020.\n- \"We denote S as the state space, which in our setting is the set of all ﬂexibility networks connecting supply and demand nodes, i.e., $\\mathcal{S}...\": Please fix the former S to \\mathcal{S}.\n- Appendix B, \"A psuedo-code for how\" --> \"A pseudo-code for how\"\n- \"[github url] for reproduction\" --> \"for reproducibility\"",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Customized RL framework for solving a specific combinatorial optimization problem, without convincing numerical experiments",
            "review": "Summary\n-------------\nThe paper proposes a reinforcement learning approach to the flexibility design problem (FDP), a two-stage stochastic combinatorial optimization problem. The authors formulate the FDP as a Markov Decision Process (MDP), with deterministic environment dynamics, discrete action space and stochastic rewards. They use proximal policy optimization to learn a policy for the MDP, which can be seen as a construction heuristic for the FDP. The solution of this approach is compared to that of two heuristic baselines on 4 problem instances from the literature.\n\n\nStrong points\n-------------------\n1. The FDP is a particular case of a larger family of two-stage stochastic programs and it’s interesting to see it addressed by a learning-based approach.\n2. The paper is well written, the problem well explained and illustrated with examples in the introduction and Figure 1. Overall, the equations and mathematical definitions are rigorous and precise.\n3. The proposed variance reduction technique is mathematically sound and proved to be effective. It is a particular baseline for policy gradient methods, that is customized to the problem.\n4.  It is interesting to use meta-learning here to adapt to different instances characteristics and avoid training from scratch for each. \n5. The experimental setup is well presented and the appendix gives useful complementary information\n6. The authors claim that the code will be released\n\n\nWeak points\n-----------------\nRegarding the experiments, although the authors show that their approach outperforms the baselines, I have several concerns:\n\n7. About the baselines: since the problems addressed are small (m and n around 10), an exact MILP solver such as Gurobi (that the authors already use for the LP subproblems) should be able to solve them, at least approximately for a limited number of demand scenarios (say a couple of hundreds or as many as possible while keeping the computation time reasonable).\n8. The authors mention (in the appendix) that there are more sophisticated heuristics but that they don’t address the general FDP. 2 out of 4 scenarios are not the general FDP. Could some of these heuristics be used on the 2 specific scenarios?\n9. To my understanding, for each problem instance, the authors generate 600 solutions then choose the best one based on 5,000 demand samples. To make the comparison fair with the other heuristics, they should do at least a few runs of each and select the best on the set of 5,000 demand samples (before the actual evaluation on the independently generated 10,000 samples). To get different results, they could for instance vary the random samples of demand used by the heuristic.\n10. Even against the two very simple baselines, the precise difference in the results, presented in Tables 2 to 5 seems negligible. Because the numbers are so big, it would be useful to report the improvement of the RL approach compared to the best baseline, in percentage:  100x(Profit_RL - Profit_best_baseline/Profit_best_baseline). I did this computation for a few random columns and got values between 0.01% to 0.3% or at most 1.3%. I know the authors claim that the problem is strategic and a few percentage improvement is already important. However, given the stochastic nature of the problem and the limitation of the baselines, these improvements do not look significant to me. \n\n\nRecommendation\n------------------------ \nI would vote for reject. In summary, the paper is a customized application of the RL framework for combinatorial optimization to a specific combinatorial problem. The problem itself does not seem very well-known nor does the approach significantly improve over simple baselines.\n\n\nArguments for recommendation\n---------------------------------------------\n11.\tAlthough a part of an important subclass of problems, the “flexibility design problem” does not seem very well-known, so I’ll be concerned about the impact and interest of the paper to the community\n12.\tThe MDP framework is similar to what have been proposed before (e.g. Abe et al 2019): sequentially selecting edges to build a solution from scratch, based on a reward signal.\n13.\tRegarding the other contributions: variance reduction is clever but quite specific to the problem, using meta-learning is interesting in this context, but it’s a straightforward application of the MAML algorithm here.\n14.\tThe numerical experiments are not convincing.: small problem instances (graphs of size at most 32), only 4 graphs considered, very simple baselines although stronger ones (such as a MILP solver) should be available for the problem size considered. Still the results of the proposed approach do not seem significantly better.\n\n\nQuestions to authors\n-----------------------------\n15.\tA google search for “flexibility design problem” does not give any clear definition of this problem. The references given in the paper treat the “process flexibility”, is it the same problem?\n16.\t“FDPs do not possess the property that solutions are naturally represented as a sequence of nodes ... Hence, it is not straightforward to apply the existing RL works in combinatorial”. I don’t understand what you mean. You formulate FDP solutions as sequence of arcs, as can be the case for the min-cut problem, or vertex cover problem. Although these problems seek a set of arcs or vertices, the solutions can still be built sequentially. So I’m not sure I understand. \n17.\tThe number K of potential arcs in a solution is used to parametrize the instances into a set of tasks and meta-learning is applied and demonstrates its effectiveness here. Could the same be done with the values of of m and n? It would make sense in the case of a new available resource or a new product type to avoid training from scratch and just adapt a pretrained policy. \n18.\tSec 5.1 “use the policy from each seed to extract 50 flexibility networks via greedy action selection”: how do you do that? With greedy action selection, I would expect you always get the same solution.\n19.\tThe greedy heuristic takes 6K seconds, do you mean 6000 seconds? It seems very long given the heuristic pseudo-code and the values of K. Can you tell us more? How many demand samples were used?\n\n\nFeedback to help improve the paper\n--------------------------------------------------\nMathematical typos:\n-\tSec 2: M_ij = min{c_i, d_j} --> I think it should be M_ij = max{c_i, d_j}\n-\tSec 4.1: The objective of RL is to maximize expected rewards --> maximize the expected return \n-\tSec 4.1: the transition fct or “environment dynamic” is called P in the MDP definition but then T is used in the definition of p_\\pi(\\tau). I think that T should be used since P is already used for the profit function.\n-\tSec 4.2: again expected rewards --> expected return\n-\tEquation 8: I believe there is a missing sum_t=0 to K before the advantage fct.\n-\tSec 4.2: set of tasks {T_i}_i=1^m, m is already used as the number of resources\n\nEnglish typos:\n- Section 1:\n    - Strategical decision --> strategic decision\n    -  where an arc (i, j) represent --> represents\n    - Secondly, the heuristics mostly relied on, and therefore, are also constrained by human intuitions --> needs reformulation\n- Section 2: \n    - M_ij = min{c_i, d_j} --> I think it should be M_ij = max{c_i, d_j}\n    - While P(d, F) is easy to solve --> to compute\n    - a NP-hard --> an NP-hard\n    - such as travelling salesman or vertex cover problem --> such as the travelling salesman or vertex cover problems \n- Section 3:\n    - combinatorial optimization problems that have been attempted using ML --> better: addressed using ML\n- Section 4:\n    - that a noisy rewards using a relative small value of --> that noisy rewards using a relatively small value of\n    - in similar number of training steps --> in a similar number of training steps\n    - details for our implementation --> details of our implementation\n    - does not provide better solution and renders much longer training time  does not provide better solutions and results in a much longer training time\n    - in the real-world --> in real-world applications\n- Section 5: Table 2 thru 5 --> 2 to 5\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The papers applies RL to FDP. However, the proposed algorithm lacks novelty, and baselines for solving combinatorial problems using RL are missing.",
            "review": "This paper proposes to use RL to solve Flexibility Design Problems(FDP), a class of stochastic combinatorial optimization problems. The authors propose an RL formulation for this problem and optimize the policy with the PPO algorithm. Some variance reduction tricks are applied and a fast adaption method is proposed. Experimental results show that the PPO algorithm outperforms heuristic methods and stochastic programming based heuristic methods on several benchmarks. \n\n+ves: \n+ The framework proposed in this paper is interesting as the FDP problem is important, and is the first one that applies RL to the problem.\n+ The paper does extensive ablation studies to validate its performance improvement.\n+ The idea that using MAML to learn a general solution firstly, and then fastly adapt to a new task is meaningful.\n+ I appreciate the theoretical upper bound is shown in Figure 2.\n\nConcerns:\n- The performance gap between RL and SP is very small with large K. But the optimal performance should increase with a larger value of K as the space of possible actions is enlarged. There is no clarification about the significance of performance improvement.\n- The main novelty of the paper is an application of current algorithms on a new domain. The authors did not propose a new RL algorithm, and the \"Variance Reduction for Reward Estimation\" trick seems quite common in modern DRL works.\n- Some important related works that applying RL to combinatorial problems are not clearly discussed and compared. That is, the paper does not discuss why previous works fail to solve FDP, and some comparison results are missing.\n\nMinor comments:\n* The details of the fast adaption approach is missing.\n* What is the variance of curves in Figure 2?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}