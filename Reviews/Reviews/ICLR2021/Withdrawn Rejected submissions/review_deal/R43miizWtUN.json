{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper focuses on the update step in Message-Passing Neural Networks, specifically for GNN. A series of sparse variants of the update step, say complete removal and expander graphs with varying density, are compared in empirical studies. The findings are quite useful for practice, and the paper is organized and written well.  As observed by the reviewers, there are several concerns regarding the novelty and contribution of the work. Besides, theoretical analysis of the sparsification approach is lacking. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. "
    },
    "Reviews": [
        {
            "title": "Some interesting empirical results but unclear impact",
            "review": "**Post response update:**\nI would like to thank the authors for their response and revision addressing my concerns, at least to some extent. However, after careful consideration as well as looking at the other reviews (and responses), I am still not convinced the case can be made for increasing the score beyond marginal leaning towards accept, which was the original score I have marked for the paper. The paper does provide some interesting empirical evidence showing that GNNs can be simplified to by sparsifying, or even removing, the learned weights for combining different channels between message passing steps. However, this kind of simplification is not particular to GNNs (indeed, many popular network architectures are overparameterized and can be significantly sparsified), and has little effect on the incorporation of graph structure in the network. The activation-only experiments are perhaps more specific to GNNs, but on the other had, they mostly establish the importance of message passing, which has already been pretty well established in previous work (together with certain well described limitations of current designs). As other reviewers indicated, the insights provided here are intriguing, but it is not clear whether they meet the level of significance and impact expected from an ICLR publication. \n\n---\n\nMost popular graph neural networks (GNN) can be formulated as combining learned transformations between node features on graphs, parametrized by GNN weights, and nonlearned aggregation of information within neighborhoods determined by the graph structure. While most work on GNN focuses on how to improve the aggregation of information (for example, with message passing, convolutional graph filters, etc.), this work examines the redundancy in overparameterized network weights and shows it can often be significantly sparsified with relatively minor effect on network task performance, such as classification or regression accuracy. This, in turn, can lead to simpler models, with significantly less trained parameters, which in turn also leads to more efficient training process. The study here establishes this trend via extensive empirical study, using expander graph based sparsification of neural network weights (via modeling of neuron connections in each layer as a bipartite graph) following the sampling procedure proposed in Prabhu et al. (2018).\n\nMy main criticism of the paper is that it offers little insight specific to GNNs. Indeed, the so-called \"Update\" step considered here is essentially a way to mix together different channels of information at a node-wise level without considering graph structure, which is mainly used and encoded by the so-called \"Aggregate\" step in GNNs. As such, the sparsification here is not much different from the typical sparsification approaches applied to feed forward (or other) neural networks. It is generally a well accepted understanding in deep learning that neural networks are often unnecessarily overparameterized, and most of them can easily be pruned or sparsified (at least to some extent) without significantly degrading their task-specific results. Moreover, while several architectures are considered here (GCN, GIN, and standard MLP to provide a non-GNN baseline), it is not clear how well optimized the hyperparameters are for these networks prior to pruning. Are these networks as compact as they can be? or perhaps further tuning could also achieve a smaller network with roughly the same performance that doesn't need sparsification. Furthermore, the authors only consider a specific sparsification approach, but it seems to me that pretty much any sparsification or pruning approach could be applied here. How does the sparsification strategy affect the results? Perhaps a comparison of several sparsifiers could help establish the main argument of the paper, or justify the use of expander-graph sparsification, even though the graph here has nothing to do with the underlying data graph.\n\nHowever, all that being said, the paper does provide evidence showing that with very few weights (or even no learned weights at all), GNNs are able to retain reasonable results, indicating some information is extracted solely by the message passing regardless of the trained network components. The discussion here is framed as following up on indications and ideas provided by Wu et al. (2019), and it is effective in this respect. Therefore, there may be some value in continuing to push the discussion along the research line here, to promote the study and better understanding of the impact different components have on the performance of GNNs. With this in mind, I consider this paper a borderline case, while slightly leaning towards acceptance in order to give the authors an opportunity to improve the presentation here by considering and studying the impact of sparsification strategies and presparsification hyperparameter tuning on presented results, in order to address my concerns mentioned above.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting, but novelty and theoretical justification is limited",
            "review": "Analysing the update step in graph neural networks via sparsification\n\nSummary:\n\nThe paper is focused on the Update step in a message passing (graph convolution) neural network. Several sparse variants of the update step, including its complete removal and expander graphs with varying density, are compared in a thorough and comprehensive empirical study. \n\nPositive:\n\n1. The paper is quite well written and easy to follow.\n2. The research hypotheses are clear, and the experiments are well designed with that in mind.\n3. Technical details are presented in sufficient mathematical detail.\n\nNegative:\n\n1. The authors do not appear to supply a software implementation, which would have been of great benefit to the reader.\n2. The technical novelty/contribution of the paper is limited (main contribution is empirical)\n3. The data sets in the study could perhaps be more fittingly chosen, so that the benefit of the graph neural network is more evident in all data sets. Not necessary to get the main message from the results, but would provide stronger motivation in my view.\n4. The findings are not strongly justified by theory. Of course, not all work has to provide theoretical results, but a more technical discussion of the potential benefites of the motivation for particular Update structures could be interesting, and would strengthen the paper.\n\nRecommendation:\n\nRejection. While the paper is well presented and technically sound, it would be much stronger if it could provide some theoretical analysis/justification of its findings.\n\nFurther comments:\n\nThe graph representation of MNIST and CIFAR10 was not clear to me.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper introduces the use of simplified Update steps in GNNs and shows on par performance with the simplified models. Discussion is lacking and contribution is of concern. ",
            "review": "#### **Post Rebuttal** \nI thank the authors for the elaborated answers and additional experiments. Unfortunately, I believe that although this work introduces some interesting insights it needs a little more work to achieve the ICLR publication bar. \nI therefore keep my original score.\n---\n\n#### **Summary of Contribution Claims** \nThe paper questions the necessity of complicated and computationally expensive  dense Update steps in message passing GNNs.\nTo answer the question two GNN variants with simple Update steps, expander based sparse and activation only, are suggested. \nThe paper presents a wide empirical evaluation on various datasets, demonstrating the abilities of the suggested models,  showing that in many cases, a simple Update step is good enough. From a practitioner point of view, the activation only model can serve as a simple \"test\" model to check whether on a specific task, a linear transformation in the Update step is necessary. \n \n**Strengths** \n- Clarity of ideas - the ideas behind the design of suggested Update steps are clearly explained and motivated. \n- Novel observation - in some cases, even the extreme case of an activation only Update step achieves good results.\n\n**Weaknesses** \n- Novelty and Contribution - I have some concerns regarding the novelty and contribution of this paper. As the use of expander graphs to sparsify NN layers has already been used in other NN architectures, I would say that Expander GNN is an extension. Furthermore, since the paper leans towards improving the practical side of GNNs, I would say that the hardware limitations mentioned in the paper are of concern. \n\n- Motivation vs. Evaluation - the paper motivates the design of sparse Update steps by the reduced computational costs. However, in the experiments, only relatively small graph datasets are evaluated on, leaving, in my opinion, the question of the effectivity of this approach in the desired setting open.\n\n- Experiments -  as mentioned above, I feel that the datasets over which the method has been evaluated do not provide a convincing evidence to the effectiveness of the approach. I would like to see performance on some datasets used in the SGC paper. \n\n- Discussion - the analysis and discussion of the results is rather a translation of the presented graphs without providing insights and possible explanations for the observed phenomena. From the results discussion in the paper I take that the success of the simplified Update steps is task dependent however, I would like to have some understanding and characterization as to when or why things work the way they do?\n\n#### **Decision Recommendation**\nAlthough presenting interesting empirical results, and potentially valuable simplifications of GNNs, I feel that this paper can be improved by performing evaluation on more relevant datasets, and acquiring stronger theoretical understanding and explanation of the observed results.  \nI would suggest to reject the paper.  \n\n#### **Other Comments**\nThe paper title is a bit misleading, I would suggest changing Analyzing to Simplifying, since the paper mainly proposes the method and shows on par performance,  but the analysis is lacking.  \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "REVIEW FOR \"ANALYSING THE UPDATE STEP IN GRAPH NEURAL NETWORKS VIA SPARSIFICATION\"",
            "review": "This paper proposed to sparsify the update step of MPNN (message passing neural network)-typed graph neural networks. Specifically, the authors borrowed the idea of upsampling from X-net in CNN and update step simplification from SGC. Although with some presentation issues, the general idea is easy to follow. The reported empirical prediction performance of the simplified models on several standard datasets is comparable to that of the baseline models. However, I have some concerns regarding the methodology and the experimental settings, which, at this stage.\n\n**Below listed are the main concerns or confusion regarding the paper. \n1. (Section 3.2) what’s the meaning of $|\\mathbb{S}_1|$ and $|\\mathbb{S}_2|$? By absolute, does it mean the number of nodes in the two sets? Also how is the size of $|\\mathbb{S}_1||\\mathbb{S}_2|$ defined? Do you mean $|\\mathbb{S}_1|*|\\mathbb{S}_2|$? \n2. (Section 3.2.1) When mentioning the pruning algorithms, it would be more appropriate to add related citations after the concept.\n3. (Section 3.2.1) As mentioned in the second point, the expander sparsifiers are claimed to 'avoid the expensive computational cost'. While there are no reputable citations over the statement, one would expect some specific evidence to support it. For example, what is the computational cost gap between the pruning methods and the expender sparsifiers? \n4. (Section 3.2.1) As $\\mathcal{B}$ is interpreted as a linear transform, it would be clearer to define from which space to which space it transforms the information.\n5. (Definition 2) What does it mean by 'without loss of generality'? \n6. (Definition 2) What is the size of the adjacency matrix $\\mathbf{A}' $? Do you define $\\mathbf{A}^{\n\\prime} \\in \\mathcal{R}^{d*d}$, $\\mathbf{A}^{\\prime} \\in \\mathcal{R}^{|\\mathbb{S}_1|*d}$, or some other specific dimensions? Also, are there any constraints on setting the value of $d$? \n7. (Section 3.3) It is not clear how Equation 4 is ended up with an 'activation-only' form when the density is 0, i.e., $d=0$. The $d=0$ would result in a zero matrix of $\\mathbf{M}$, which cuts off all the signal transformations within the network. As a result, the output $\\mathbf{H}^{L}$ should also be a zero matrix. If different update rules are used when $d=0$, it should be clearly stated, rather than 'deriving from the general case of Equation 4'.\n8. The figures are difficult to read or understand. First, the labels are ambiguous to me. For example, which 'simple' and 'vanilla' models are referred to exactly? Also, why the number of bars for each of the methods (GCN, GIN, MLP) are different? Why some of the datasets (ENZYMES, DD, etc.) include error bars while others do not? It is recommended for a more straightforward and informative presentation to substitute the figures with tables, which provide readable comparable numeric values.\n9. The experimental results, especially displayed in Figure 1, cannot suggest the proposed methods' power. According to Figure 1-(d) and 1-(f), the last 3 Expander methods (except expander-10\\%) require more time and parameters to obtain a 'comparable' accuracy. Adopting such a structure is then in question.\n10. The results in Figure 1 are surprising, where MLP based methods outperform other models significantly. What is the possible explanation of it?\n**It is also found that many sentences are difficult to understand due to the over-complicated expressions, expositional problems, and/or grammatical mistakes. Some examples which need improvement:\n1. (line1, Section 3.1.1) \"Given a graph…\": can be simplified to shorter sentences.\n2. (line 4, page 3) Maybe some grammar problems in \"The idea of … has found successful application\".\n3. (line 1, Section 3.1.2) \"Expander graphs…can be informally defined as being highly connected and sparse\". The meaning of \"sparse\" is not clear.\n4. (line 12, Section 3.1.2) \"Training each parameter … incurs some training and inference, computational\". This sentence is ambiguous.\n5. (line 13, Section 3.1.2) \"Using the expander graph concept … reducing thus cost, and it is the aim to utilise each parameter to its greatest efficiency\". The meaning of \"reducing thus cost\" and 'greatest efficiency' is not clear.\n\nGiven the problems stated above, the paper needs significant improvement and polishment before it can be considered acceptable. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}