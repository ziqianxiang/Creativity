{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Sequence generation models trained via maximum likelihood estimation (or variants of so called 'teacher-forcing') condition on *data* samples during training and on *model* samples for predictions. The susceptibility to this potential \"mismatch\" in input distribution is often referred to as exposure bias (EB). \n\nThis paper stresses that most research around EB is focused on addressing it, rather than defining and/or quantifying it. Thus the submission questions the severity of EB and attempts to operationalise a testable definition for it. Myself and all the reviewers strongly support the observations and the agenda, we find the question this paper asks an important one. \n\nDespite our appreciation for this paper's relevance, we have identified a number of problems that prevent me from recommending this paper. I will comment on the two most important points:\n\n1. The 'operational definition' of EB in this paper is not sufficiently precise to be testable. It builds on the somewhat commonly accepted view that the effects of EB accumulate as the conditioning context grows longer, and that this causes a model to generate badly distorted sentences. This definition still leaves quite some room for interpretation (without specifying reasonable expectation about how these effects 'accumulate' and what/how bad they are, it seems difficult to design tests). We acknowledge that the submission attempts to shed light onto some of these aspects by having some 'control groups' using gold data and shuffled strings, but we did not find those sufficient (mostly in light of the next point).\n\n2. MT evaluation metrics (essentially, string similarity metrics), most notably (but not exclusively) BLEU, are used in this work in a setting where we cannot easily grant that they have the discriminating power that the authors expect of them. See this is not a criticism about the imperfections of BLEU (or any other automatic metric), but about the lack of evidence supporting its use against unrelated sentences. We do not find it sufficient that some recent NLG papers have made similar use of it (I, for example, would have criticised those papers on similar grounds).\n\nOverall, we believe this submission asks a relevant question, the insight about dependence on prefix is nice and might lead to a first operational definition of EB (which might be only a few refinements away from the version proposed here). The current evaluation is unconvincing and I believe the authors should be able to find more credible strategies, especially, strategies that have already gone through some scrutiny (for example, in literature around OOD detection and tests for distribution shift). \n\nThough I do not recommend this paper for acceptance, I hope the authors will find valuable feedback in the expert reviews attached."
    },
    "Reviews": [
        {
            "title": "Not convinced by the evaluation",
            "review": "The paper studies the exposure bias in auto-regressive neural language models. This problem is known to cause incremental performance degradation, and attempts to mitigate this problem have received significant attention in the community (using, e.g., RL and GANs). The paper claims that prior work has mostly focused on addressing the problem rather than measuring how severe the exposure bias problem actually is. Despite extensive previous work on mitigating exposure bias, the paper suggests that the exposure bias is not “large enough [to] induce drastic performance loss during generation” (e.g., a human evaluation controlling for exposure bias show relative differences of only < 3%).\n\nOn the positive side, I agree with the paper that it is worthwhile to study the extent of the exposure bias problem, and the approach of the paper (comparing generation with model-based prefixes vs. data prefixes) is quite intriguing, as it attempts to compare generation with exposure bias again without such a bias. That said, I have several concerns that make me question some of the claims of the paper:\n\n1) The human evaluation shows very little performance difference between generation with data prefix (D-prefix) and model prefix (M-prefix), suggesting exposure bias is not a problem. But I would argue this is mostly an artifact of the experimental setup, as prefixing generation with D-string (of length L) only eliminates exposure bias up to position L, and then both evaluated systems (according to the generation process defined Section 4.1) use a standard auto-regressive LM generation process that makes them both subject to the exposure bias problem. Since these generated strings are of length 30, there is plenty of room for exposure bias to crop in. (Indeed, Zhang et al., (2019) and Holtzman et al. (2019) have shown concrete examples of exposure bias artifacts appearing in much shorter sequences). So, the small performance difference between a preference M-prefixed and D-prefixed generation may very well be due to both setups being almost identical (same model, same auto-regressive inference algorithm, and the only difference is the prefix – which human raters are not even asked to judge directly). If exposure bias is indeed a problem, then it would affect both systems almost the same way, so this human evaluation can’t be used to either affirm or deny exposure bias is a significant problem. \n\n2) I also have concerns regarding the automatic evaluation based on BLEU. What the authors call “corpus-BLEU” is actually not the standard version of BLEU (see “Other Comments” below), as the version of the paper looks at n-gram (n=1 to 3) matches between the generated sentences and a large set of references that is *not* specific to a particular context. As the paper’s automatic evaluation is done in a completely context-agnostic way and relative to a large pool of references, it essentially only measures whether the model (with or w/o exposure bias) is able to generate plausible trigrams, but that sets the bar very low as we already have plenty of evidence showing neural language models are quite capable of generating reasonable trigrams (whether there is exposure bias or not), and in fact often much longer n-grams. The authors claim that auto-regressive models appear to have “self-recovery ability” that mitigate any exposure bias, but I would say that it is hard to claim anything about self-recovery (at least in terms of automatic evaluation) when the evaluation metric operates over such a small window (<=3 words). \n\n3) Abstract: “Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is.” \nI find this claim a bit misguided, as the numerous papers addressing exposure bias are *empirical* ones. They have shown improvements in various tasks such as machine translation, image captioning, and other generation tasks thanks to techniques aimed at reducing exposure bias. Now, one could claim that significant improvements shown in these papers are due to reasons other than exposure bias, but if that is the case then the submission doesn’t do a much better job at isolating exposure bias from other factors (given my concerns in (1) and (2)). Note that Zhang et al. (2019) actually does include a short section attempting to quantify the effect of exposure bias. \n\n4) For a paper that attempts to challenge the current understanding of a problem that has received very significant attention (i.e., exposure bias), it is quite thin in terms of related work (2 paragraphs). The paper omitted important related work (e.g., Schmidt, 2019; Tan et al., 2019; Rennie et al., 2016), including an ACL best paper on the same topic and whose findings appear to be at odds with the current submission.\n\nIn sum, the paper tries to improve our understanding of exposure bias and its impact on open-ended language generation, and this is totally a worthy goal. I also recognize that isolating (i.e., ablating) the effect of exposure bias is difficult. That said, the paper makes rather strong claims (e.g., that “performance gain is minimal” if exposure bias is supposedly eliminated) that I find unfounded given my points in (1) and (2). In both evaluations, the setups evaluate a given model sequence W prefixed by a string sampled either from the data or the model, but that does not eliminate the fact that exposure bias is bound to appear within the generated sequence (length 30), which is the sequence that is evaluated in the end.\n\nOther comments:\n\n“Corpus BLEU”: Note that BLEU, as originally defined in (Papineni et al., 2002), is in fact a corpus-level metric, as it aggregates n-gram statistics over an entire (test) corpus. So “corpus” in “corpus BLEU” is redundant and possibly confusing. The term “corpus BLEU” or “corpus-level BLEU” is generally used to contrast with various versions of “sentence-level BLEU.” Now it appears that “corpus BLEU” in this submission refers to the version of BLEU used in SeqGAN (Yu et al.), which is therein not called “corpus BLEU.” That distinction should be made clearer, considering that the use of “a large number of sentences from ground-truth data as references” is a significant departure from how BLEU was originally designed to work. Indeed, (corpus-level) BLEU (Papineni et al., 2002) doesn’t allow matching hypotheses and references across test instances, as opposed to the submission. The authors justify their use of BLEU as it is a “well-established [metric] in the NLG literature,” but this is rather misleading as their specific version of BLEU is not well established and not what is commonly used in MT and NLG.\n\nTable 1 doesn’t actually illustrate what the experimental setup of the paper does (i.e., Section 4.1), as the latter doesn’t include a shared prompt of 20 words that is supposed to make the two generated strings “more comparable”. Since this prompt supposed to make the two strings more comparable is absent from the actual evaluation setup, I gather from the authors’ own words that they implicitly admit that string comparisons in their evaluation setup are not so comparable. \n\n* Zhang et al., 2019: https://arxiv.org/pdf/1906.02448\n* Schmidt, 2019: https://arxiv.org/abs/1910.00292\n* Rennie et al., 2016: https://arxiv.org/abs/1612.00563\n* Tan et al., 2019: https://arxiv.org/abs/1811.09740\n* Holtzman et al., 2019: https://arxiv.org/abs/1904.09751\n* Chen and Cherry, 2014: https://www.aclweb.org/anthology/W14-3346/\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid empirical study of an important question",
            "review": "This paper presents an empirical study of exposure bias, showing that it does not appear to be an especially significant issue.  Both automated metrics and human evaluations are presented, and I found the experiments to be fairly convincing.  While the paper could have more material and depth, I think it makes an important point that people will appreciate seeing in the conference.\n\nDesigning metrics for exposure bias is a novel task, and this paper invented appropriate approaches.  The experiments cover the two most important model classes (LSTMs and transformers) and use representative model settings and corpora.\n\nConcerns:\nThe experiments only look at pure sampling from the model---i.e., they don't use temperature, nucleus sampling, greedy or beam search.  These other generation settings are more important for applications, compared to pure generation.  Also, they have been reported to exhibit more deviation from the corpus distribution (especially with respect to particular pathologies like repetition).  Evaluating generation in these other settings would increase the impact of the paper's conclusions.\n\nThe human evaluation is helpful, but needs measures of inter-annotator agreement.  \n\nLess significant: The qualitative experiment that starts the paper is very anecdotal, and I think the paper would be better off without it.  The concrete examples in Table 1 are helpful for illustration, but I would not dedicate a section to them or call it an ‘experiment,’ as it is too limited in scope.  It detracts from the stronger experiments later in the paper.  Likewise, the model in Example 1 seems too inaccurate to be illustrative, and the associated discussion in Appendix D is already well known I believe (that the “Teacher forcing” MLE objective just aims to choose parameters that maximize the likelihood of the corpus, using the chain rule).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review for: Quantifying Exposure Bias for Open-ended Language Generation",
            "review": "Summary:\n\nThe so-called \"exposure bias problem\" (EBP) is often cited as a serious issue when training sequential model with MLE and teacher forcing. This paper attempts to quantify whether the problem is real on open-ended generation experiments, and concludes that it is actually minor.\n\nPositive aspects of the paper:\n\n- EBP is often accepted as an obvious problem in the generation community, despite the lack of experimental evidence. Thus, a paper such as this one that tackles such evidence head-on can be very useful.\n\n- Close to the end, the paper argues, I think correctly, that \"the original claim of exposure bias is not well defined, [and] our approaches can only act as a reasonable proxy to measure its seriousness\". The paper however attempts to design (1) experiments and (2) quantitative measures that correspond with our general intuition of the EBP.\n\n- According to these experiments and measures, and also to some human evaluations, the paper compares two situations: (D) the trained generator is provided with a prefix from (an unseen portion of) the dataset and asked to generate a continuation; (M) it is provided with a prefix that it generated itself and asked to generate a continuation. The conclusion from these experiments is that the continuations in the (M) case are only very slightly worse than in the (D) case. This contradicts the usual expectation about the EBP, namely that in the (M) case, the model would be \"lost\" in unknown territory and start to produce very poor text. The main conclusion of the authors is that: \"although the mismatch between the data and model prefix distribution exists, it is still in the model’s “comfortable zone”, and is not large enough induce drastic performance loss during generation\".\n\n- The authors are careful to avoid a possible misunderstanding of their results. They do *not* make the claim that MLE + teacher forcing is the best way to train a generation model, but only the *different* claim that exposure bias is not such a serious problem as is often assumed.\n\nSome issues and questions:\n\n- The fact that the generation model does not move away from its \"comfort zone\" when generating prefixes could be related to two different dimensions: First, the prefixes that you generate are only moderately long, thus alleviating the EBP issue. Second, you do not describe exactly how the generation is done. I gather that Transformer-XL uses a form of \"top-k\" sampling, that is, a generation mode more restricted than a standard (from the pure probabilistic viewpoint) \"ancestral sampling\". Such restricted sampling (as also beam-search) is known to improve the quality but limit the diversity of the generations. It would be interesting to see if you would obtain the same EBP conclusions with ancestral sampling.\n\n- The corpus-BLEU measure that you use is most of your experiments was designed for Machine Translation and appears to me to be a very weak measure of quality for open-ended generation. It would actually be informative to compute a baseline for that measure in terms of continuations not from the model, but from the training data itself, giving a measure of the average \"quality\" of a gold-standard continuation relative to all the other gold-standard continutations from the training data. How good would this \"gold corpus-BLEU\" be? Probably not so good, and it would be interesting to compare the (D) and (M) continuations to these gold continuations.\n \n\nMinor points:\n\n- The JS divergence is not a distance.\n\n- Please clarify exactly when \"prompts\" are used in your experiments. While they are used in the human evaluations, it was not clear to me whether they were used in the EB-M experiments, for example.\n\n- The paper https://arxiv.org/abs/1906.05664 \"Calibration, Entropy Rates, and Memory in Language Models\" might be relevant as related work: in that paper the authors argue that neural language models tend to suffer from \"entropy drift\", namely the tendency to entropy of the next token prediction to be higher when conditioned on an (M) type prefix than on a (D) prefix. However, if I am correct, they assume standard hierarchical sampling, and their empirical evaluations are not extensive.\n\n-----\n\nNov. 30th: On a second reading of the authors’ exchanges with the reviewers as well as of the updated paper, I am lowering my overall score. While I still believe that the *questions* that the paper raise are very worthwhile to the community, I agree with several reviewers that the *answers* provided in the paper are insufficiently supported by a convincing formalization and by experiments.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Problem & Intuition, but Unjustified Metrics & Claims",
            "review": "This paper makes a key observation: “exposure bias” is blamed for many of the issues with Neural Language Generation but it lacks both a concrete definition or any obvious evidence that it truly exists. The authors begin by defining exposure bias as the decrease in quality and relevancy (to the conditioning text) in generations as the model conditions on its own output. A limited qualitative study fails to find evidence of exposure bias, and the authors propose two metrics, EB-M and EB-C, to measure the quality and relevancy degradation, respectively. Quantitative results find that degradation on these two axes as the model conditions on itself are either minor or non-existent. To calibrate our understanding of these metrics, the authors do a human study as well as comparing two GAN frameworks. In the discussion the authors discuss limitations of the work, mostly that the given metrics are not a complete notion of evaluation and connect their work to related literature.\n\nStrong Points:\n- The authors correctly call out the lack of scientific work on actually defining and understanding exposure bias.\n- Their intuition that exposure bias should mean quality and relevancy are highly dependent on the prefix is good idea.\n\nWeak Points:\n- The quantification of exposure bias is largely unjustified.\n- The qualitative evaluation is ad-hoc.\n- The discussion section is rushed and does not make a strong argument that the author’s interpretation is supported.\n- The choice of wiki-103 as a testbed is strange.\n\n\nI recommend rejecting this paper, as the way exposure bias is quantified is dubious and the authors do not make a strong argument that they would have detected exposure bias if it is present.\n\nI want to start by saying I think that the authors correctly point out a deep flaw in the literature around text generation: exposure bias is a casually defined issue that is nowhere well-defined or proven to exist. Even more importantly, I think the intuition that this paper gives around sampling different kinds of prefixes and looking at the resulting generations is exactly right. The main issue I have is that the formalization of exposure bias is not well-founded and I do not believe a convincing argument is made the claims of relatively little (or even no) exposure bias are actually supported by the experiments.\n\n While the ways the authors try to quantify exposure bias are intuitive, there is really no evidence that these metrics measure what they claim to. Corpus-BLEU has been used in many papers, though I don’t know of any showing actual correlation with a human metric, but we can at least say that it has precedent being used cumulatively. However, using the corpus-BLEU of _individual_ samples in a fraction creates the potential for completely inaccurate estimates. Even assuming corpus-BLEU works, there is no guarantee that it acts as an estimate of individual samples, in the exact same way that BLEU is known not to be accurate for individual sentences (Burch, 2006). Furthermore the EB-C metric is basically proposed from thin air. It makes sense to compare the distributions, but how big of a difference should we expect? GANs do badly on EB-C, but they are known to be significantly worse even on normal metrics, so it’s not clear what this proves.\n\nThe qualitative evaluation is just the narrative of what the authors think. No intuition is given, simply “we didn’t find exposure bias when we looked at the data”. It especially shouldn’t be described as failing “to show the significance of exposure bias”. The authors refer back to these thoughts as experiments, e.g. on page 5 with “This agrees with our observations in the qualitative experiments.” This seems inappropriate.\n\nThe discussion is extremely rushed and it is not very clear what we are supposed to conclude from it. The authors show that their metric is not complete via a toy example, but talk about how MLE does not produce these kinds of solutions. That’s fine and I believe that part of their argument, but I still feel that there is not much evidence that the metric would actually show significant differences if the quality of the writing went down.  Quality is a tricky thing to measure. To the authors’ credit they conduct a human study.  However, it is unclear to me whether humans that are exposed to a generated prefix (which may already deviate somewhat from human language) would feel that the mistakes made due to exposure bias “match” the mistakes in the prefix.  If the generations were truly horrible, I agree we would see more deviation in this score, but the paper attempts to disprove the very existence of exposure bias. A more appropriate reframing would be to talk about the kinds of effects exposure bias couldn’t possibly be having, e.g. that exposure bias may only result in strange word choice but not total degeneration. This over-claim, and the lack of smaller claims to build-up to it that might have been more appropriate, make it difficult for me accept the described conclusions.\n\nFinally, previous works on open-ended text generation have used the portion of the corpus released by OpenAI. It’s hard to know how comparable these results are, and since both GPT-2 and a significant number of validation text examples from WebText are freely available this seems like a flaw.  Why was wiki103 chosen instead of WebText used alongside the pretrained GPT-2?\n\nCallison-Burch, Chris, Miles Osborne, and Philipp Koehn. \"Re-evaluation the role of bleu in machine translation research.\" 11th Conference of the European Chapter of the Association for Computational Linguistics. 2006.\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}