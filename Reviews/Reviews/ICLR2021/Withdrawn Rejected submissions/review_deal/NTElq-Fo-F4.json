{
    "Decision": "",
    "Reviews": [
        {
            "title": "Paper lacks depth and is missing important details in the experimental study",
            "review": "This paper analyses autoencoder feature representations trained on two types of code embedding aiming to discover if those representations are predictive of code features.\n\nOverall the paper lacks depth and is missing some details in the processing of the input corpus. In addition the experimental study is testing on the validation set used to select activation thresholds, so the results are not necessarily valid.\n\nMajor comments:\n\nThe experimental study trains autoencoders on vector representations of the code using a training set. Then the neurons whose activations most strongly correlate with one of two different code classification tasks are selected using a sample of the training set to determine the thresholds. It appears that these training set accuracies are reported. The re-use of the training set to determine the thresholds is dubious (it would be preferable to determine the thresholds on unseen validation data), but the accuracies of those thresholds on the training set are not representative of an accuracy calculated on a held out test set. Without knowing the accuracy on a test set it's tricky to know if the neurons have actually learned a transferable representation or if it's just overfit to the training data. Also the random baseline gets 58% accuracy on a balanced binary classification task, which indicates that this overfitting is occurring.\n\nThere is no description of the tokenization strategy used for the doc2vec embedding. As it's applied directly to the source code rather than the AST (as used in the code2vec embedding) doc2vec is sensitive to the tokenization choices, and may even be sensitive to semantically irrelevant formatting choices in Java code (e.g. presence or absence of spaces in statements like `if(p>5){System.out.println(\"hello\");}` which contains no spaces but could have them inserted between any keyword, around any parenthesis, curly brace or operator without semantic effect). In general the processing and hyperparameter choices in the embedding algorithms are not described, yet are important for the reader to be able to replicate the results.\n\nMinor comments:\n\nThe network diagram is probably unnecessary for the ICLR audience, as it could be stated as checking the decoder layers for useful representations along with a description of the network architecture.\n\nThe algorithmic description could be much shorter, it selects 10 thresholds evenly distributed between that neuron's minimum and maximum activation, and finds the best threshold per neuron for the task, and the best neuron overall.\n\nThe reference to Allamanis et al 2016 in the background suggests it's a convolutional neural network combined with a transformer (citing Vaswani et al 2017), when it's actually a CNN with standard attention. This could do with revision to make the architecture clear. Similarly the code2vec reference also states that that algorithm uses a transformer when it uses standard attention.\n\nThe definition of the random problem doesn't appear to have a source of randomness. If the list is a shuffled list of methods then that should be noted, but otherwise it's not a random task (as the ordering of methods in code could be meaningful as it is enforced by style guides, though it's not relevant to javac).",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The power of individual neurons of unsupervised models of code",
            "review": "# Summary \nThe goal of this work is to understand if individual neurons of a model trained in some unsupervised way on programs (Java in particular, but that shouldn't matter much) can be used as classifiers for code-related classification tasks.\n\n# Strengths\nInteresting question. I haven't seen this particular question answered for code.\n\n# Weaknesses\n\n* Novelty is limited. It's not exactly clear what to do with the conclusions here. It might be useful to think the next step beyond this work. What do you do know that you know? Would this allow you to prune models into much smaller embedding modules? Could you train faster? Make inference faster?\n\n* At this point, there are many published pre-trained embedding modules (e.g., CuBERT, CodeBERT, Graph CodeBERT, etc.) It would be far more impactful to talk about something that's already out there, rather than training your own embedding module and showing your results.\n\n* Methodologically, the work could use some more thinking. Using a validation dataset to choose good neurons and a test dataset to measure their accuracy would say more about generalizability of results.\n\n# Details\n\nIn general, I think there is something in the goal of your work, but this is only a first step. Given the vast scope for program-understanding models (from bug finding to code optimization to synthesis to code review), having cheaper powerful models is very very important, especially if you want to make predictions as a programmer is typing (e.g., in an IDE). What you're doing here is an early form of distillation: you're finding some neurons that may be powerful enough to tell you something useful. Showing the benefit of that (e.g., by pruning the embedding module down to only the neurons needed to feed your special neurons) might indicate that you can have models that run fast in inference mode. This is just one possible idea. I'd encourage you to think more about what to do with your first results.\n\nQ1: I'm a bit confused about equation 1. Is the vector subtraction in embedding space? In vocabulary space? The latter doesn't make sense for program inputs (two tokens may be very far apart or close together in vocabulary space, but would be equally icorrect), but the former isn't really an input or output. Could you please clarify? Section 3.2 seems to indicate that you mean $x$ and $y$ to be embeddings, not raw inputs, but the figure doesn't show the two embedding modules. Adding those in Figure 1 would help.\n\nTo make the differences between the two embedding methods easier to understand, it would help to use similar model sizes (for the embedding modules). That way we'd know that it's the representational power of the two embeddings that matters and not their capacity. Especially for your conclusions in Section 4, this \"fair\" comparison of the two embedding modules is important. Using something like a similar number of trainable weights, which you can control by modifying the hidden dimension, or adding extra layers where possible, should suffice.\n\nThe definition of the Semantic problem is a little odd. Unlike the Structural problem, which is a single classifier, your Semantic problem is a family of classifiers, one $h_t$ per word $t \\in T$. It would be good to point this out in the text explicitly.\n\nQ2. Why did you choose the particular neurons to test? It shouldn't be that much harder to test all neurons. What was the rationale?\n\nAlgorithm 1 is a little sloppy, with respect to information tracked and returned (e.g., storing the argmax neuron). Also, it would be good to mention the test dataset explicitly, rather than referring to \"activations\". The reader doesn't care about arbitrary activations, but only about activations obtained from the 3000 test examples.\n\nI would have expected to see a use of a validation set of examples to detect good candidate neurons and a test set of examples to measure the generalizability of your finding. Presence of neurons that predict your classes accurately is good, but showing that those particular neurons work on a different dataset is more useful in general.\n\nThe description of the experimental setup for the Semantic problem in Section 4 is not clear. How did you assemble your datasets and what exactly classifiers did you try to predict? It looks like you chose specific examples from the input datasets. How?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Known result",
            "review": "This paper presents an autoencoder trained on Java programs. The authors use code2vec and doc2vec embeddings as input. The authors then explore the individual neurons in the autoencoder, and finds that there are neurons that somewhat corresponds (~60-68%) to detecting the code's cyclomatic complexity, and detecting the presence/absense of a subword in the function's name.\n\nThat neurons in neural networks trained in an unsupervised can be classifiers is a well-known result. The authors cite Le et al. [1] which demonstrates the result in a convolutional setting. Radford et al. [2] shows similar results in a sequence/language model setting. Since these results are already known, the novelty of the work is extremely limited.\n\nMoreover, the author's choice of model is a straightforward autoencoder, and is also not novel. The results in table 1 and in the figures are not compelling or surprising. \n\nThe algorithm outlined in Algorithm 1 appears to be an approximation of an AUC (area under the ROC curve) computation. Would it not make sense to just use AUC metric instead?\n\nThere are several issues with the writing of the paper, and I found the writing repetitive and vague in many places:\n* In the abstract, for clarity, instead of writing \"For doing that...\" it's better to either repeat what \"that\" is, or remove the phrase altogether.\n* It is not necessary for a paper at this venue to include equation 1 for the Mean Square Error.\n* The name \"gitHub\"should be capitalized \"GitHub\". \n* Figure 2, 3, 4 x-axes are not labelled, so they took a while understand. \n* Problem and model names are not used consistently throughout the paper: for example, page 6 middle paragraph discusses the \"Structural problem\", a term that is not used anywhere else in the paper.\n*  I didn't understand the parenthetical in the last sentence, second paragraph of section 4.\n\n[1] Le et al. (2012) Building high-level features using large scale unsupervised learning https://arxiv.org/abs/1112.6209\n\n[2] Radford et al (2017) Learning to Generate Reviews and Discovering Sentiment https://arxiv.org/abs/1704.01444\n",
            "rating": "2: Strong rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper provides an analysis of the internal representation of a neural network model. More specifically, they analyzed the representations learned by an auto-encoder model which is trained on programs. They defined two binary classification tasks, identify best neurons that learn each of the task and reported the accuracy. Compared to random neurons, the identified best neuron performed better thus showing the efficacy of their method.",
            "review": "The main motivation or I would say goal of the paper is to establish a foundation for understanding and mining various parts of the prgrams in an unsupervised way. The authors make a strong assumption that there will be one single neuron learning the particular task. \n\nThourgh the premise of the paper is interesting, it requires substantial work in better presentation, more experiments, cite related work, and clearly state and justify assumptions taken in the study.\n\nSome of the points that I would like authors to address:\n- The auto-encoding is relatively simpler task and it is possible that the network learn to copy the input to output. The small hidden states did encourage some learning of the task but it is still an easier task. The paper is based on the assumption that neurons are very focused and they will find single neurons learning particular classification task. The authors are completely ignoring the fact that there can be a group of neurons working together to learn a task. Does the current setup bring those neurons?\n- What is the motivation of testing neurons of specific layers instead of including all layers?\n- Algorithm 1 needs more explanation in the text. From where do you get predLabels? similarly, where does variable vectors used which is introduced in the second step?\n- The procedure is not entirely clear. First an autoencoder is trained using the Java dataset. Then using Algorithm 1, authors find the best neurons with respect to two binary classification tasks. I did not get how this binary classification is happening. Are you fine-tuning the original model for the target binary classification task or you are doing training an external classification using a single neuron as input or you are simply assigning labels based on an activation threshold? If the last option is considered, \n- What is the x-axis in figure 3 and 4? what is the reason of showing x-axis limited to 50-68 only?\n\n\n- There has been some work done on analyzing neurons in deep NLP models. Authors should relate their work with them. A few examples are:\nWhat is one Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models\nIdentifying and Controlling Important Neurons in Neural Machine Translation\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}