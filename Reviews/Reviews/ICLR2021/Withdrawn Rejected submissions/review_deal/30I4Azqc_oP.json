{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The work proposed to learn causal structure of the environment and use the average causal effect of different categories of the environment, between the current and next state after performing an action as intrinsic reward to assist policy learning. While the reviewers find the ideas presented in the paper interesting and of potential, there are some concerns regarding proper introduction and comparison to related works, and clarity of the algorithm itself. While the two experimental results presented in the paper do show the potential of the work, it is missing an important baseline to disentangle the effectiveness of introducing the causal structure alone vs intrinsic reward. For example, how would A2C with curiosity or surprised based intrinsic reward, which also introduce the surprisingness of the next state as a result of performing an action as additional reward perform on these tasks? \n"
    },
    "Reviews": [
        {
            "title": "A novel approach to learn causal relations and used them to accelerate the learning process of deep reinforcement learning",
            "review": "The authors proposed to learn causal relations to accelerate the\nlearning process in deep reinforcement learning. The proposed approach\nfirst learns a graph to represent the causal structure of the\nenvironment calculating the Average Causal Effect (ACE) between different\ncategories. It uses an intrinsic reward to encourage interacting with\nthe most relevant entities of the causal graph which accelerates the\nlearning process.\n\nThe proposed approach first need to create a factored state space,\ngiven by the user. It is not clear how intuitive is this process and\nif it can be performed automatically.\n\nThey first learn a model of the environment, predicting the next state of\neach category of entities. For that they obtain data of the agent\ninteracting with the environment and use an encoder-decoder approach\nto obtain k categories. Then predict the next state based on the\ncurrent state and an action, where the decoder predicts the next state\nof each category. \n\nIt is not clear how to determine the number of categories? (how many\ndecoders?) If there is a single agent interacting with different\nobjects, a new category needs to be defined for each object?\n\nIt then evaluates the ACE between every pair of categories. Following\nChattopadhyay et al., they used the causal attribution method to infer\nthe causal relations between entities. The proposed approach then\nconstructs a DAG, where an edge between category \"i\" and \"j\" is placed\nif ACE i->j > ACE j->i. The DAG represents a causal ordering where\ncategories with higher ACE value are placed before. Each category is\ngiven an intrinsic reward, in addition to the extrinsic reward, based on\nits order.\n\nThe approach was tested on two domains and compared against a flat DRL\n(A2C) and Relational DRL with clearly better results.\n\nThe results should also include the time used to obtain the causal graphs.\n\nTypos:\n... analysis et, al (Ren et al. ...\n... deconvlutional ...\n... cause that the ewe ... => ... cause the ewe ...\n... the the ...\n\nPros:\n- The use of causal relations to accelerate the learning process\nCons:\n- The approach requires manual segmentation of the state space into\ncategories \n- Tested on two new domains and compared against a baseline an another\nalgorithm, so it is not completely clear the generality of the approach.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting method for deep reinforcement learning that learns causal structure of the environment and leverages it to assist policy learning.",
            "review": "In this paper, the authors propose a new deep reinforcement learning algorithm that learns the causal graph representation of the environment and leverages it to assist policy learning.\n\nThe authors should define a critical entity since they mentioned it many times in the article. Since the learned graph considers categories as nodes, I did not understand\nthe reference of critical entities as top-ranked nodes in the graph (Sec. Calculating ACE).\n\nI would like also to see a proper definition of the entities and how the learned causal graph of entities impacts the performance of the proposed method.\n\nThere is confusion about the nodes in the causal graph, sometimes the authors mention them as entities:\n\nAbstract: “critical entities of the causal graph”\nIntroduction:: ”The proposed algorithm learns a graph to encode the relation information between entities”.\n\nAnd they change to consider nodes as categories of entities:\n\nMethod: “we calculate ACE values between categories of entities, which are used for constructing a causal graph G”\nSec.3.1: “we focus on the causal relation among categories of entities.”\n\nFor my review, I am considering nodes as categories of entities, given that the authors discuss a restriction on learning the category causal graph\nrather than the entity causal graph to make their approach scalable. But this inconsistency and lack of definition lead to confusion.\n\nMy main concern about this paper is about the experimentation, specifically: baselines. \n\nI consider that the requirement of no prior knowledge is an interesting and valuable contribution of this paper. However, in scenarios in which we have such information, I do not consider that we should ignore it. Thus, I consider it is an open research question the application of the proposed method in these scenarios.\nIn this sense, I consider that the approaches proposed in [1,2] should be considered as baselines in an experiment where CARE receives the causal graph and does not\n“update\" it during the sequential decision-making process. The categories of the given entity causal graph could be inferred to build the causal relation among categories\nof entities as assumed as input to CARE. \n\nI did not understand why CARE is not appropriate to Text-Adventure Game [3].\n\n[1] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with graph neural networks. In Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.\n[2] Varun Kumar Vijay, Abhinav Ganesh, Hanlin Tang, and Arjun Bansal. Generalization to novel objects using prior relational knowledge. arXiv preprint arXiv:1906.11315, 2019.\n[3] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Deep reinforcement learning with relational inductive biases. In International Conference on Learning Representations, 2018.\n\nReferences should point to the published work rather than the arxiv entries, when the former is available (e..g. Neural Network Attributions: A Causal Perspective, ICML’19).\n\nTypo:\n\n - Sec.3.2: “of several deconvlutional …” -> “of several deconvolutional …”",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Marginally below acceptance threshold",
            "review": "Summary: \nThe paper proposes a deep reinforcement learning algorithm of advantage actor-critic (A2C), which ﬁrstly learns the causal structure of the environment and then leverages the learned causal information to assist policy learning. The causal structure is computed by calculating Average Causal Effect (ACE) between different categories of entities, and the authors assume that an intrinsic reward is given to encourage the agent to interact more with critical entities of the causal graph. Two experiments were conducted on simulation environments (Shepherd and Ant tasks) and demonstrated the effectiveness in obtaining the rewards and interpretable and accurate detection of the true graph.\n\nReasons for score: \nThe motivation and solutions were simple and clear, but the related work about deep reinforcement learning leveraging causal relation was not properly introduced. Furthermore,  there were unclear points in the implementation (without sharing codes) and the results. I think the idea is interesting and contributed to this community, but for the above reasons, it is difficult to provide a higher rating. \n\n\nPros:\n1. The authors designed causality-based intrinsic reward to an agent, to encourage it to interact with critical entities for accomplishing the task.\n2. Two experiments were conducted on simulation environments (Shepherd and Ant tasks with heterogeneous agents) and demonstrated the effectiveness in obtaining the rewards and interpretable and accurate detection of the true graph.\n\nCons:\n1. There were also other researches in deep reinforcement learning leveraging causal relation (e.g., [i] and [ii] below). The title and the first contribution of this paper should be more specific and the related work should be included in the main text.\n2. There were unclear points in the implementation (without sharing codes) and the results (below). \n\n[i] W. Shi et al.  Self-Supervised Discovering of Causal Features: Towards Interpretable Reinforcement Learning, arXiv, 2020 \n[ii] W. Agnew and P. Domingos. Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning, arXiv, 2020 \n\nOther comments:\n\nThere is little information about the algorithm of Relational DRL (Zambaldi et al., 2018) as one of the baseline. Relational DRL seems to be an attention-based A2C algorithm, but is the difference only related to the attention-based approach? At least, whether it is an A2C algorithm or not should be clarified. \n\nIn both experiments, is the number of categories K the number of agents? This was not specified.\n\nI did not understand Figure 5 right and the sentence (”The value of coordinate (x, y) in the heatmap corresponds to the effect of the agent on the ewe when it is on position (x, y) and taking action drive”). The correspondence of xy coordinates with Figure 5 left might help my understanding. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}