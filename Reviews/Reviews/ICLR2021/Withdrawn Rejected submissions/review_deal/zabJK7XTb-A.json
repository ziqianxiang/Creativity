{
    "Decision": "",
    "Reviews": [
        {
            "title": "Good experimental results, incremental technical contribution, additional discussion of related works and ablation study are needed.",
            "review": "This paper modifies the prototypical networks for transductive (semi-supervised) few-shot learning: it trains a meta-model to re-scale the distance/similarity metric (or \"confidence\") between each sample and prototypes. The proposed method runs a soft K-means (an EM-algorithm) that rescales the sample-prototype similarity (called \"confidence\") by using the meta-model and then updates the prototypes accordingly. They also show that computing the similarity under model and sample perturbations such as drop-path and data augmentations helps. They compare their method with other methods in this setting and on several benchmark datasets and show improvements over them. An ablation study is presented to show the improvements brought by each part of the proposed method.\n\nI find the experimental results are encouraging but the technical contribution might be incremental and limited without further clarification from the authors.\n\n(1) While training a meta-model to produce an input-adaptive metric and using it to modify the confidence and prototypes makes sense in the few-shot learning scenario, it is not novel and too incremental due to several existing works in few-shot learning, for example, a line of works in prototype propagation:\n\nLiu et al., \"Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot Learning on Category Graph\", IJCAI 2019.\nLiu et al., \"Learning to propagate for graph meta-learning\", NeurIPS 2019.\n\nThe above works use an attention module as the meta-model to produce the confidence in this paper, which is more general and expressive than the re-scaling function g() used in Eq.3 and Eq.4. Moreover, the presented method is also very related to the series of works that use graph neural networks for few-shot learning, especially the ones using graph attention networks. Discussion and comparison to these works are expected.\n\nThe usage of the term \"confidence\" and \"soft k-means\" is not rigorous. The alternative procedure between Eq.1 and Eq.2 is a standard procedure in the EM algorithm for the Gaussian Mixture Model, where the confidence in Eq.1 is called the responsibility of sample x w.r.t. cluster/component c. Confidence is usually used only for the predicted class with the largest probability. Given this connection to GMM, the authors should discuss their method's relationship to EM algorithm, metric learning for clustering, meta-learning for clustering (learn to clustering), etc.\n\n(2) The paper lacks a formulation of the meta-optimization problem they aim to solve, making some presented procedures hand-waving. For example, what is the objective of running Eq.1 and Eq.2 alternatively? It seems to be some maximum likelihood estimation of a mixture model, but then applying T=1 transductive step cannot guarantee convergence without theoretical justification.\n\n(3) Inconsistency for key steps in Algorithm 1: line-5 in Alg 1 is different from Eq.1 in that the prototype P'_c in line-5 is the initial one while the P^{t-1}_c in Eq.1 is from the previous update of the prototype. \n\n(4) I would like to see an ablation study of the proposed method without normalization to a_1 and a_2 in Eq.3 and Eq.4, as recent studies showing that such simple normalization is sufficient to produce remarkable improvements on few-shot learning tasks. For example, it is interesting to compare MCT with (i) MCT with the metric scaling g() but without the normalization, and (ii) MCT without g() but with the normalization. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "The results are superior, but the novelty is very weak.",
            "review": "The paper aims to address the few-shot learning problem using the scaling parameters and the meta-learning concept. The proposed method is called MCT and there are two proposed scaling strategies: 1). instance-wise  scaling applies to each feature and 2). pair-wise metric scaling applies to the distance between two features. Furthermore, model and data perturbation techniques are applied to boost the performance even more. The experiments show the effectiveness of the method with some improvements on few-shot learning benchmarks. \n\nStrengths:\n- The proposed approach shows in the experiments that it improves the results compared to prior few-shot learning methods on four datasets: miniImageNet, tieredImageNet, CIFAR-FS, and FC100. \n- The paper is well-structured and easy to understand the content in each section.\n\nWeaknesses:\n- The contribution is not very clear, especially the novelty of the proposed approach.  TPN (Liu et al., 2018) used a metric scaling also for each feature which looks similar to Eq. 3. Is there any argument about their difference?\n- The term 'confidence' is a bit ambiguous in this manuscript because it can be defined as the output prediction and tuning the confidence score should take into account all features from support and query instances rather than only using features from two instances. In my opinion, it should be called as a metric-scaling method. This is to minimize ambiguity to readers.\n- The reason why the proposed method is only applicable for transductive setting is not explained. MCT is also possible for the inductive setting.\n-  It is not very clear if the comparison is fair with other methods because stochastic data perturbation is involved in each episode. \n-  Is the number of transduction step affecting the overall results? There is no empirical investigation about this important hyperparameter.\n- The detail about $\\phi$ is not described well.  Algorithm1 does not have any description. For instance, how it is updated, how it affects the prototypes, how it affects the predictions.\n\nIn the current form, I not very positive it can be accepted because the contributions are not very significant and some prior methods (e.g. TADAM and TPN) also have proposed metric scaling in their works. The analysis to compare the proposed approach with similar approaches is also lacking.\n\nSome questions need clarification:\n- In semi-supervised few-shot classification, why pre-training the model with all training classes is necessary here? Because Ren et al. train the model from scratch and use some portions of data to follow the problem of semi-supervised learning. \n- Table 6 and Table 1 show different results. Is there any reason?\n\nSuggestions to improve the manuscript:\n- It would be better to have different number of unlabel or query data in each episode to show if the method really improves in most of the cases.\n- I believe there is a connection between the scaling method proposed in this paper and the Mahalonobis distance, it would sound more technical if this discussion can be added.\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "In this paper, authors propose to meta-learn the confidence for each query sample, to assign optimal weights to unlabeled queries such that they improve the model’s transductive inference performance on unseen tasks. ",
            "review": "Strength: In this paper, authors propose to meta-learn the confidence for the transductive few-shot learning to address the unreliability of the labels predicted on the unseen samples during transductive inference. Specifically, the model perturbation (block dropping) and data perturbation (horizontally flipping) is applied in a meta-learning fashion to model the uncertainty, and also an input-adaptive distance metric is learned with this perturbation. Experiments on few-shot and semi-supervised few-shot benchmarks validate the proposed model. \n\nWeakness: There are some concerns authors should further address:\n1)The transductive inference stage is essentially an ensemble of a serial of models. Especially, the proposed data perturbation can be considered as a common data augmentation. What if such an ensemble is applied to the existing transductive methods? And whether the flipping already is adopted in the data augmentation before the inputs fed to the network?\n2)During meta-training, only the selected single path is used in one transductive step, what about the performance of optimizing all paths simultaneously? Given during inference all paths are utilized.\n3)What about the performance of MCT (pair + instance)?\n4)Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics.\n5)Though this is not necessary, I'm curious about the performance of the SOTA method (e.g. LST) combined with the adaptive metric.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not clear why the proposed method improves confidence",
            "review": "This paper studies transductive few shot learning. Previous transductive or semi-supervised approaches used predicted labels on unlabeled data which are intrinsically unreliable. This paper proposed a confidence transductive inference scheme, which includes two new designs: (1) using model and data augmentation to generate an ensemble prediction, and (2) meta-learning the distance metric by adding instance-wise or pairwise scaling. They evaluated the proposed method in both transductive and semi-supervised settings and showed improved performances compared with pervious work.\n\nMy major question about this paper is whether it is appropriate to call it “meta-confidence”. It is true that a fundamental problem with many semi-supervised learning is the unreliability of the predicted labels, but I don’t see from the paper how it improves the confidence. Ensemble in general can lead to a more accurate prediction, but it is only used at inference time. During training, an encoder is randomly sampled from 4 options, which is essentially a data augmentation technique and is widely used in previous few-shot learning research. It would be however interesting to see how to consolidate the predictions from different views into a more confident prediction during meta-training. \n\nThe second technique, meta-learning the distance metric, is not a new idea. For example, Liu et al. (2018) used the same approach as the instance-wise scaling in this paper. The proposed method is more like a variant of previous work. It is not clear why it would lead to more “accurate and reliable” confidence as claimed by the authors. \n\nBased on the above assessment, I think the contribution of this paper is limited and vote for rejection.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}