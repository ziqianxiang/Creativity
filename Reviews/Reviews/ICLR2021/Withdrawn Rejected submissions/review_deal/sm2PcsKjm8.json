{
    "Decision": "",
    "Reviews": [
        {
            "title": "Meta-Aggregating Networks for Class-Incremental Learning",
            "review": "Summary: This paper proposes Meta-Aggregating Networks (MANets) for Class-Incremental Learning (CIL), which alleviates stability-plasticity issues between the learning of old and new classes. This model explicitly builds two residual blocks at each residual level: a stable block and a plastic block.  It aggregates the output feature maps from these two blocks and then feeds the next-level.   Meta-learner is used to aggregate weights to dynamically optimize and balance between two types of blocks: stability and plasticity. The experiments are conducted on three CIL benchmarks: CIFAR-100, ImageNet-Subset, and ImageNet.\n\nStrong Points: 1- This paper is well written and easy to follow.\n2- This model tries to maintain the balance between plasticity and stability using a meta-learner.\n3- A fine-tuning step uses a balanced subset of exemplar to reduce the biased toward new examples.\n4- It performs the experiments in a large scale imageNet dataset also.\n\n Weaknesses: 1-  This paper presents a Replay-based class incremental learning for image classification.  It integrates MANets (proposed architecture) with already proposed models such as TPCIL, Mnemonics, etc. and shows that it significantly improves the performance. The author's claim is trivial. Since meta-learning is well effective in few-shot learning, and due to the replay-based approach, this setting is more similar to few-shot learning where it has few examples for each class (old and new classes). Several methods have been proposed in few-shot learning. Those showed that the use of meta-learning in generative models (based on GAN and VAE) significantly improves the model performance. In my view, this model is weak in the sense of novelty or terms of significant contribution.\n2- A fair comparison is missing: In this paper, I found that very similar models are missed to compare, such as iTAML CVPR 2020 [b], it is also used meta-learning and replay based approach for incremental class learning. iTAML [b] is a very similar approach; therefore, it should be included in the comparison table.\n3- Replay-based methods for incremental learning during the last task  (t_k) converges to approximately few-shot learning since, in the memory buffer, a few examples from all classes are available. Hence this kind of approach is less preferable than approaches that are not using old class examples.  Several methods have been proposed to handle the model's plasticity and stability without using the previous class examples such as [a]. I recommend that the authors include results without using previous class examples in the ablation study section.\n[a]- Semantic Drift Compensation for Class-Incremental Learning, CVPR 2020.\n[b]- iTAML: An Incremental Task-Agnostic Meta-learning Approach, CVPR 2020.\n\nOverall:  This paper proposes incremental class learning using a meta-learning framework, which is well suitable for few-shot learning.  The proposed network is integrated with the proposed model and claim a significant model's performance improvement, but the individual contribution of the proposed model is not mentioned. The comparison with similar and recent state-of-the-art methods is missing. No experiments have not performed for a more interesting setting, which does not use previous class examples.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Paper Review",
            "review": "Summary\n----------------------------\n\nThe paper proposes a new architecture for vision-based Class-Incremental Learning (CIL) tasks. Starting from a Resnet-like architecture, each residual block is duplicated into two sets or parameters : a static and a plastic block. The authors further propose to learn the mixture weights to combine the stable and plastic representations.  They use Bilevel Optimization to learn the mixture weights and the model parameters respectively, i.e. one is fixed and the other is trained for a full epoch before alternating. The authors show the proposed method can be added to existing ones, yielding consistent gains. \n\nPros and Cons\n----------------------------\npros\n+ the method is easy to understand and well presented \n+ the ablation study is well designed\n+ the activation maps analysis complements well the paper\n\ncons\n- the experimental section seems incomplete : \n    --- there are no confidence intervals, suggesting that only a single run was executed. This is *especially* problematic, as for most of the observed results, the gains are quite small so it's quite hard to conclude that they are statistically significant.\n    --- What about hyperparameter search ? how was it conducted ? what measures did the authors take to ensure each method was given a similar hyperparameter budget ? \n- The explanation about how the \"neuron-level scaling weights\" are more efficient is unclear. If I understand this correctly, given a (channels_in, channels_out, kernel_size, kernel_size) 4D convolutional weight tensor, the scaling weight has shape (channels_in, channels_out, 1, 1) ? If that it not the case please provide an explanation in the rebuttal. \n\nMy main criticism of the paper is the following : the authors operate under a supposedly strict memory budget, yet propose an architecture which more than doubles the amount of papers. If I understand correctly, you have\n\na) the set of original weights (say N parameters)\nb) the set of scaling weights (say N/9 parameters)\nc) the set of learned weights (say N parameters)\n\nTaking a Resnet-18 for example, you have about 11M parameters before the last fully connected layer. The proposed approach would bring the parameter count to ~23M params, about 12M added parameters. In the case of say CIFAR-100, 12M parameters takes about 12M floats * 4bytes / float = 48Mb which is 48Mb / (32 * 32 * 3 * 1b) ~= 15K raw samples. In my opinion, an essential baseline for this paper has to consider allocating this extra space to simply storing more samples. I can only consider accepting this paper if the authors can show that the proposed approach has a better memory usage than this baseline.\n\nOther Notes\n----------------------------\n- it is mentioned in the text that \"our work is the first one proposing a new architecture for CIL\". This is not true. Here are a few of them\n(1) Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n(2) Xu, Ju, and Zhanxing Zhu. \"Reinforced continual learning.\" Advances in Neural Information Processing Systems. 2018.\n- table 2 caption seems to have a typo : it should be \"Rows 3-5\" I think. \n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "----------------------------------\n**Summary**\n\nThis paper proposes a new way for incremental class learning. Each block of the network will be replaced by a plastic block and a stable block. The plastic block will have more learnable parameters whereas the stable block will have less. The two blocks then sum up their activations together by multiplying with a pair of mixing coefficients per each layer. The mixing coefficients are meta-learned from a held-out set. Experimental results show that by using the meta-aggregation design, the paper can stably improve prior methods such as iCaRL, LUCIR, Mnemonics, and PODNet. However, as detailed in my comments below, I am not convinced that the gains brought by meta-learning aggregation and balanced set are significant enough.\n\n----------------------------------\n**Strengths**\n\n1. The model design has the merits of simplicity and elegance. Meta-learning the mixing coefficient is also a neat design that circumvents the difficulty of learning the learning rates.\n2. The authors also made an attempt to perform a variety of ablation studies, which I appreciate.\n3. Based on Table 1 & Figure 4, the proposed method improves upon SOTA methods.\n4. Figure 5 is an interesting analysis on the plasticity vs. stability tradeoff. \n\n----------------------------------\n**Weaknesses**\n\n1. The proposed mechanism relies on some other CIL algorithms. Why is this the case? Why wouldn’t the proposed algorithm just work as a whole package on its own? What would be the performance of the model, if we just train using Algorithm 1?\n\n2. Clarity of writing.\n\n    a) Definitions of plastic and stable blocks are not clear in the model section. This is partially due to the fact that the authors try many combinations in the experiments, but I was not able to understand for example the sentence: “For example, when using the neurons of size 3x3…”\n\n    b) Page 8 Sec “The values of $\\alpha_\\eta$, $\\alpha_\\phi$” & Fig 5: What does it mean by “levels”? Can we refer them by the name of the layer? There are well-defined layer names for ResNet, VGG, and AlexNet.\n\n    c) It wasn’t immediately clear what does the \"plus’’ mean in the ablation table, (e.g. \"all\" + \"all\"), as it is not explained in the text.\n\n3. A lack of clear analysis in ablation studies. And with the current set of results, I am concerned whether the gain is significant enough.\n\n    a) There are many variants of \"all\", \"scaling\" and \"frozen\", and the authors basically pick the best performer of all in Table 1. In ablation studies, it seems sensitive to the choice of the combination.\n\n    b) Let’s look at Row 7 & 8, which I assume that they are without the highway connection, since Row 6 is the only one that marks “with highway”. Then if we compare Row 7 & 8 with \"all\" + \"scaling\" (Row 3), the difference is genuinely small, which makes me suspect of whether we need the aggregation coefficients meta-learned at all.\n\n4. There are several ablation comparisons that I would like to see (but I am unable to find them in the current version):\n\n    a) How well does a network do if it has a layerwise learning rate? One of the conclusions is that the lower layers need more stable blocks whereas the higher layers need more plastic blocks. What if we have a smaller learning rate in the lower layers, and have a higher learning rate in the higher layers? Instead of using a mix of activation between two fixed learning rates?\n\n    b) With the layerwise learning rate, now we add in the aggregation mechanism but instead fix the value of aggregation coefficient, how well does it perform?\n\n    c) How well does a network do if we subsample a balanced mini-batch every time? So it can still have the rest of the component but we don’t need to meta-learn the aggregation coefficient with a balanced set?\n\n5. The visualization in Figure 3 is interesting. However, I was not able to clearly grasp the conclusion.\n\n    a) First it is not clear how the plots are generated. I don’t understand how the authors separately visualize the plastic block and the stable block, since they are fused for every layer. Then simply disabling the branches might not make sense since the upper layers could be sensitive to any changes made to the lower branches. It is also not clear which layer did the authors backprop the gradients into. Are we comparing the gradients backproped into different layers? More clarification on this will be appreciated.\n\n    b) Both cases of Figure 3 show correct/wrong placement of attention to the surrounding background vs. the main object. This suggests that a different mechanism, allowing the mixing coefficients to be spatial-aware and input-aware, might be more useful than the one suggested by the paper? Why would the network differentiate input from Phase 0 or Phase 5, and apply different aggregation based on different inputs? What is the mechanism here?\n\n6. Less significant results on the full ImageNet benchmark. The benefits brought by adding MANet on top of several SOTA methods (LUCIR, Mnemonics, PODNet) is much less significant on the full ImageNet, despite the fact there are numerous variants that can be added, e.g. highway connections, combinations of all vs. scaling vs. frozen, etc. This can be concerning that the proposed changes only bring major gains on CIFAR which has much smaller image resolution and dataset size. Another possible explanation is that when the number of classes go up, the effect of forgetting goes away (e.g. see Davidson & Mozer, 2020).\n\n----------------------------------\n**Minor comments**\n\n1. The title would sound less awkward with “meta-aggregation” instead of “meta-aggregating.”\n\n----------------------------------\n**Conclusion**\n\nI appreciate the idea and the technical contribution of the paper. However, there seem to be some clarity issues and I am not convinced by the gain brought by the proposed design, especially the meta-learning and the balanced set part. Therefore, my rating is 5 but I am willing to change it if most of my concerns can be addressed.\n\n----------------------------------\n**References**\n\n- Davidson, Guy and Mozer, Michael C. Sequential mastery of multiple visual tasks:Networks naturally learn to learn and forget to forget. In CVPR 2020.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper but further justification",
            "review": "In this paper, the authors develop an algorithm for continual learning in class-incremental learning. The idea is based on designing a new feature extraction block, called MANet, to encode properties of both the older and the classes. The parameters of this block are decomposed into stable parameters to remember past learned classes and plastic parameters to learn new classes. The features resulted from these two sub-blocks are aggregated and fed to higher-order layers. Experiments on three standard benchmark datasets are provided to demonstrate the effectiveness of the proposed algorithms.\n\n\n\nPros:\n1. The paper is well-written and easy to follow.\n\n2. A simple, yet effective idea?\n\nCons:\n\n1. I think the major strength of the paper is being effective in the experiments. However, due to the absence of some details which I will list, the results are not fully convincing.\n\n2. It is important to know what is the number of learnable parameters for MANet blocks, compared to the rest of the networks that are used in the experiments. If there are considerably a more number of learnable parameters, the comparison with other methods is not fair. Because when the learnable parameters increase, it is very natural that the forgetting effect can be addressed with regularization based continual learning methods.   \n\n3. A lacking aspect in experiments is an ablation study on the effect of initial training on half of the classes. A large number of CIL works do not have this step and it is informative to see how important this step is on the performance of the algorithm. \n\n4. What is the number of exemplars in the experiments? A fair comparison is possible if the same number used for all the memory buffer-based methods. Is this number the same for all the methods listed in Table 1?\n\n5. What is the strategy to select the exemplars after learning a class and also in the future when some exemplars must be discarded to satisfy the fixed memory size constraint? \n\n6. Adding std in tables and curves is important. It is important to check how stable the results are. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}