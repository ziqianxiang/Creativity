{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting paper, however, it is not clear the quality of the proposed dataset is good enough.",
            "review": "This paper proposes to train a model to pass the American Board of Pathology Test. To complete this goal, the authors proposed to solve 3 challenges: 1, collect the pathology images question answering dataset which requires expert knowledge. 2, propose data bootstrapping to handle noisy samples. 3, self-supervised learning to learn better representations. \n\nPathology image question answering is a very interesting task. Since the paper mentioned a lot of ABP tests, I wonder are there any results on the proposed model for ABP test? It will be great if the authors can show some results on the actual test :) \n\nAs mentioned by the authors, the dataset is noisy and hard to collect. Given the difficulties of the task, is it possible to train a useful model with 3000 images (20000 QA pairs)? Is the model just overfitting or can actually generalize to ABP questions? It will be great if the authors can show the results on out-of-domain test set. \n\nThe dataset may also biased, for example, Table 6 shows that the model can answer 85.1 even without any image information. Is this mean that there is a strong bias in the questions? what is the performance of fine-tuning questions on bert? I guess the author can achieve even higher performance. Thus this baseline is necessary to show a strong baseline for the task. \n\nThe paper is poorly written and hard to follow, for example, \n1: What is N^{tr}? the number of training examples? Is W denote all the weight of VQA model or the last classification layer? \n2: It will be good to replace Method 1 and Method 2 with the actual method name.  \n\nThis paper proposed a very interesting task. However, the paper's method lacks novelty and there are some doubts about the dataset size, bias, and baseline methods. I will increase my eating if the authors can prove the trained model can actually generalize to out of domain question in ABP test instead of just overfitting the dataset. ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A pathology VQA dataset has been proposed. Cross modal self-supervised learning and learning to ignore has been proposed to tackle the data insufficient problem of pathology VQA.",
            "review": "Contribution Summary:\n1. A first pathology VQA dataset has been proposed. The dataset collection pipeline is interesting which collect language and image pair from the textbook and rectified by a doctor. \n2. To tackle the lack of data problem, Cross-modal self-supervised learning and Learning to Ignore approach has been proposed.\n3. Extensive ablation study to testify the proposed method and provide many strong baseline methods for pathology dataset.\n\nMy concern:\n1. I am concerned with the contribution of a pathology VQA dataset is enough to be accepted by ICLR.\n2. Cross modal self-supervised learning has been widely explored and testified in NLP, CV, and multimodality community. In this paper, the author applies the self-supervised learning approach directly. It would be nice if the author could correct me on the novelty of proposed cross-modal self-supervised learning compared with other methods.\n3. The Learning to Ignore algorithm is a simple modification of meta-learning algorithm.\n4. The language and image pair of the dataset is too small.\n\nBased on my concern, I recommend a weak reject of this paper. However, considering the extensive ablation study and much engineering work in this paper. I will change my score by considering the recommendation from other reviewers. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid paper with limited methodology innovation",
            "review": "Summary:\n\n* This paper collects and annotates a Visual Question Answering dataset for Pathological images, which contains 33k questions on 5k pathology images. \n* This dataset is large scale but have labeling noises, as hiring many highly experienced pathologist for annotation is very difficult. \n* To address the labeling noise issue, this paper proposes a simple approach to reweight data points, which learns a variable for each data point to indicate the level of labeling noise. \n* In addition, this paper also uses a cross-modal contrastive learning approach to perform representation learning for both pathology images and question/answer pairs.\n* The dataset will be made publicly available\n\nPros:\n\n* The problem this paper investigated definitely has real-world value, in creating a better AI-assisted health care system.\n* The dataset this paper collected can contribute to the problem it studies and benefits research, especially in the digital medical systems.\n* The experiments is thorough, which includes two baseline VQA methods. Meanwhile,  a detailed ablation study is provided to study each technique this paper presented.\n* Section 1-4 of the paper is overall clearly presented. \n\n\nCons:\n\n* The methodology this paper presented has relatively low technical originality.\n(1) Learning by re-weighting data is not a very novel algorithm as many prior works (reviewed in related work) have studied it. In this paper, it is very hard to differentiate the presented approach \"learning-by-ignoring\" from those prior works. For example, why is this a better approach than Ren et al, which uses meta-learning to learn the weights of data? It has not been explained or empirically verified. \n(2) Self-supervised learning on image and text data has been previously studied in cross-modal contrastive learning for image/text matching such as [1,2]. In my opinion, this paper applies the technique to perform contrastive learning for image and question, as well as image and answer. This is quite incremental.\n\n* In Table 4/5/6, does joint pre-training stands for the multi-task learning mentioned in Section 4.2? Since there are three tasks in total, are they pair-wise beneficial to each other?\n\n* Section 4.3 contains a very brief explanation about the backbone VQA model used in this paper, it would be great if authors could expand this section in the supplementary material with more details.\n\n\n\n[1] Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\n\n[2] VSE++: Improving Visual-Semantic Embeddings with Hard Negatives\n\n\n\nMinor:\n* Results in Table 5 seems weird. Following the caption, it seems the accuracy of best model on Why question is 0.59%? \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "\"Pathological\" Visual Question Answering with Active Self-supervised Learning",
            "review": "\"Pathological\" Visual Question Answering with Active Self-supervised Learning\n\n**Quality:**\n\n_Pros._ The authors proposed the VQA dataset for developing an AI Pathologist. An extensive introduction and related works help to understand the motivation of this work and the previous works that the paper is based on. \n\n_Cons._ The abstract is too long (17 sentences). It would be better to summarize it within 5-8 sentences. For instance, the three points described in the abstract could be briefly summarized to give an idea, then elaborated in the introduction.\n\n**Clarity:**\n\n_Pros._ The writing is generally well-written to convey the main idea of work, and appropriate figures show the collected image in the dataset and how the proposed methods work.\n\n_Cons._ In the abstract, why do the pathologists have no time to help? Please explain the reason for readers (in the introduction, not in the abstract). The expression of \"very\" is too frequently appeared (>7 times) in the introduction, which may limit the persuasiveness of the paper.\n\n**Originality:**\n\n_Pros._ The paper firstly propose the VQA dataset for pathology and two learning techniques, \"learning to ignore\" and \"self-supervised learning\" on PathVQA, to handle the issues in the collected dataset.\n\n_Cons._ How do you handle the copyright issue from using the textbooks and online digital libraries to create your dataset? Please specify every data source used in the paper has no copyright issue in the manuscript (Muir et al., 1941; Robbins et al., 1981; PEIR digital library).\n\nAlthough the authors argue that the proposed dataset is about pathology instead of radiology and providing more open-ended questions (33K to 15K of VQA-Med) comparing with VQA-Med and VQA-RAD, this work seems to have a limited novelty since the authors fail to show the significant difference from the VQA datasets in radiology. \n\nTwo learning techniques, \"learning to ignore\" and \"self-supervised learning\", could be applied to VQA-Med and VQA-RAD for the same reason of data scarcity since the two datasets have a smaller number of question-answer pairs. \n\n**What expected in rebuttal:**\n\n(1) Leaning toward rejecting this paper is for novelty. What is the interesting point in the visual question answering tasks for pathology, compared to the natural image-based VQA or the VQA for radiology? Do the open-ended 33K QA pairs reveal an interesting aspect compared to 15K QA pairs in VQA-Med? The analyses of the collected dataset are shallow to discover a novel aspect. \n\n(2) The contribution of two techniques, \"learning to ignore\" and \"self-supervised learning\", are meaningful, but does not support the novelty of the collected dataset.\n\n(3) For \"learning to ignore,\" how many examples are ignored in the experiments? Is there any tendency in the ignored examples? Is there any significance in the question/answer/images?\n\n(4) Please specify that there is no copyright issue (possible modification and redistribution) from using the textbooks and online digital libraries to create your dataset in the manuscript.\n\n**Minor comments:**\n\nIn Section 5, shouldn't the citation for \"dropout\" be Srivastava et al. (2014), instead of Krizhevsky et al. (2012)?\n\nIsn't the metadata for the pathology images available, e.g., zoom level, dyeing method, etc.?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}