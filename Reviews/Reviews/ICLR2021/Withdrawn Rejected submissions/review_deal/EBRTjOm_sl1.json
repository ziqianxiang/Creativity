{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The authors propose to linearly combine the utility functions of (batch) active learning algorithms. The linear combination coefficients are \"learned\" with Monte Carlo estimators to adapt the coefficients to different kinds of tasks automatically.\n\nThe reviewers find the presentation within the papers generally clear. The simplicity of the approach, which is highlighted in the authors' rebuttal, should be appreciated. The authors also addressed the issue of robustness with respect to the batch size. But the paper left quite a few unanswered issues even after the authors rebuttal. The novelty with respect to several earlier papers require clarification and concrete comparisons, such as the ones in reinforcement learning and bandit learning as pointed out by reviewers. The lack of comparisons to those existing works, both illustratively and empirically, is a key weakness of the current paper. A more careful study of RL setting (such as reward shaping) is also important to understand the value of the work. Finally, the gap between the ensemble approach and the single approach also deserves more investigation to justify the significance of the contribution.\n"
    },
    "Reviews": [
        {
            "title": "Review of AnonReviewer2",
            "review": "Summary:\nThe paper proposes learning a batch mode active learning (AL) policy as a weighted ensemble of existing AL techniques (or agents). In the proposed method, the ensemble weight vector (\\beta) is learnt from data. AL is simulated on a set of training tasks where performance for various choices of \\beta are estimated using a Monte Carlo approach. Subsequently, the optimal choice of \\beta is found using tree Parzen estimators, a black box hyperparameter optimization technique. Experimental results are shown on Checkerboards, Fashion MNIST, bAbI and 10 UCI datasets. \n\n+ves:\n+ The Related Work section is extensive and clearly explains the existing methods in literature, building up to the reasons behind developing the proposed approach\n+ The batch mode active learning framework is clearly defined and well-explained, thus setting the stage for the proposed ensemble based method. \n\nConcerns:\n\n- Sec 2.3, para 2, “In domains where it is not possible to learn the best parameters because there is no similar training task, an ensemble of active learning agents using such an engineered combination of them might be suitable.”\nThe sentence suggests that an ensemble of AL agents is suitable for domains where similar tasks are not available. However, the second contribution in Section 1 states the following. “We show with our experiments that it is very important to train the ensemble on a similar task it is evaluated on.” \nThis contribution seems to be in direct contradiction with the first statement. \n\n- In Sec 3.1, the reward is chosen as the improvement in current model’s accuracy after a step i.e., choosing a sample or a batch of samples. However, Sec 3.3 and Alg 2 suggest that the objective of the \\beta optimization problem is calculated as the final AL performance i.e., the performance of the supervised model at the end of the AL process. If this is the case, then reward shaping technique mentioned in Sec 3.1 might not be necessary, because the intermediate rewards are not used to learn the ensemble weights. This was not clear, kindly clarify if I missed something here.\n \n- Sec 4.6, para 2, “..ensemble trained on the checkerboard dataset has a weight of uncertainty sampling being zero..”. \nThe sentence says that the weight for uncertainty sampling is zero for checkerboard, however Table 1 shows that uncertainty has weight 0.93 for the checkerboard dataset.\n\n- The results would’ve been much stronger and more conclusive if the proposed method of learning ensemble weights had been compared against an ensemble of agents with equal weights (or random weights). Then, the superior performance of the proposed method would’ve strongly validated the need to learn ensemble weights, as opposed to giving them equal weights.\n\n- While it is suggested in the introduction that learning a few weight parameters is much easier than reinforcement learning, the results would’ve been more complete if details were included in the Experiments section, on the computational complexity of the proposed method, as compared to RL methods in terms of time taken. \n\n- The paper could have benefited with a more thorough discussion section. Since ensembling is a fairly well known idea, an in-depth discussion would’ve helped to understand how well ensembling works in case of active learning. For instance: \n(1) Why is there a major difference of three orders of magnitude between the weights for representative sampling for MNIST and bAbI? (Table 1)\n(2) In Table 1, for bAbI dataset, uncertainty and diversity have zero weight. Then, why does uncertainty-diversity sampling has a positive and high weight of 21.5?\n(3) It would’ve been interesting to see an ablation study on how strongly the prior of \\beta parameters would affect the final learned \\beta vector.\n(4) It is mentioned that if the \\beta parameters are learnt properly, then the proposed method will perform at least as good as the best heuristic, in the worst case. In this vein, a discussion on how likely the tree parzen method will attain those optimal parameters, could’ve been insightful. Also, it would help the community to see the limitations of the proposed method. For instance, how do the following factors affect the optimality of \\beta parameters? (a) complexity and size of the dataset (b) choice of the hyper parameter optimization technique. \n\n- The details of what classifiers were used for the experiments on UCI datasets is not provided. A short section could have been introduced as Supplementary material to provide these details. These may be important, especially considering there seem to be different choices of classifiers for different datasets. Why was this justified, and why would this observation generalize? Some ablation studies and analysis on one dataset as to how the performance would change if another classifier was used would help showcase the effectiveness of the proposed method.\n\nIn summary, the paper aims to propose ensembling as a simple alternative to computationally costly RL techniques for active learning. While the method is well-motivated, it is missing some key experiments (especially on the significance of the contribution) and analysis, and has a weak discussion/analysis section.\n\nMinor comments:\n(which did not affect the decision):\n\nQuite a few editorial mistakes were across the paper. Here are a few examples and their corrections. (Note that this is not an exhaustive list, and only examples of similar mistakes across the paper).\n[Abstract, line 2] chose -> choose\n[Abstract, line 5] fix -> fixed\n[Abstract, line 8] applying -> apply\n[Abstract, para 1 last sentence] “to adapt to” -> “adapt to”\n[Introduction, para 3 line 1] shortcoming -> shortcomings\n[Sec 2.1, para 2, line 9] which a -> which are\n[Sec 2.2, para 1, line 1] dataset -> datasets\n[Sec 2.3, sentence 1] “combined to an” -> combined to form an\n[Sec 3, line 3] ensemble of different active learning -> ensemble of different active learning methods\n\nPOST-REBUTTAL:\n\nI thank the authors for their response. While some of my concerns have been addressed, a few key questions haven't been answered.\n\n* The contribution is limited in novelty.\n\n* The general author response mentions that reward shaping is not used in the proposed method. In that case, Sec 3.1 seems a bit pedagogical and misleading, since MDP for AL is described in detail but is not even empirically compared with the proposed method.\n\n* Regarding the ablation studies on a different classifier, while I agree that SOTA networks need to perform well, the ablation study on a different classifier was suggested to rule out the effect of the choice of SOTA classifier in the effectiveness of the proposed method. Also, the authors respond to R1 that the gap between the ensembles' performance and each single classifier's performance is small since they chose SOTA models for the individual classifiers. This is perhaps even more reason to show how ensembling works when the base models are not SOTA.\n\n* Beyond just a statement in the response, it would have been good to see some empirical comparisons between the proposed method and, say, Q-learning-based RL methods - especially in terms of actual running time complexity.\n\n* I also agree with R3 that similar ideas have been explored before (papers cited in R3's review), and it is important to compare with those methods as baselines in the experiments.\n\n* Also, the choice of the datasets used is not justified appropriately.\n\nI stay with my original decision.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Not fully convincing ",
            "review": "The paper \"Learning Active Learning in the Batch-Mode Setup with Ensembles of Active Learning Agents\" proposes to deal with the problem of active learning via a weighted ensemble of agents. Each agent sequentially selects data to include in the batch to be labelled according to specific heuristics. Finally, the various agents are weighted according to parameters found by a gradient-less approach. \n\nWhile meta-learning of active learning is a very interesting and useful problem, I find the contribution of this paper too weak for a conference as ICLR. The approach is rather straightforward (only a linear combination of different heuristic agents), the experimental results are not fully convincing (there is no real gap between the ensemble and the best individual approach, so using the best agent at training time is maybe a strong alternative) and the paper lacks clarity and details. From my point of view the related work section should shortened to focus on mainly important aspects related to the presented work, a better detailed view (more formalized, less algorithmic) of the approach should be given, and some theoretical insights should be given before it could be considered for publication in a top machine learning conference. \n\n   ",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors introduce a novel, batch-mode ensemble approach to Learning to Active Learn that combines the best of both worlds: heuristic- and learning- based active learning. The main idea is to create an ensemble of parametrized active learning agents that will perform better than any individual agent.",
            "review": "The authors introduce a novel, batch-mode ensemble approach to Learning to Active Learn that combines the best of both worlds: heuristic- and learning- based active learning. The main idea is to create an ensemble of parametrized active learning agents that will outperform any of the individual agents.\n\nThis is a well-written, easy to follow paper on an important topic. The work appears to be original, and the findings are significant. \n\nThe basic idea of the paper is to combine the-best-of-both-worlds of heuristic- and learning- based active learning. The authors introduce a simple & efficient way to do this. The empirical validation shows that the proposed approach is robust across a variety ML problems. \n\nThe paper would benefit by further strengthening up the empirical validation by answering the following questions:\n- is the proposed approach robust wrt sample size? In other words, what happens when, for all four evaluation tasks, we consider batch sizes of 1/2/48/16/32/64. Most of these graphs could be part of an appendix, with the main paper summarizing the findings\n- how many more labeled examples would take the \"loser\" approaches to reach the accuracy of the \"winning\" one(s)\n- how many more iterations (and at what cost) would the proposed approach reach state-of-the-art accuracy on each evaluation task?     \n\nOTHERS:\n- please run a spell-checker to avoid errors such as \"acitve learning\" (page 2) or \"lineracombination\" (page 8)",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Missing comparison and connection with prior work",
            "review": "Summary of the paper:\nThe paper proposes an algorithm for batch-mode active learning using an ensemble of 4 active learning heuristics. The basic idea is to use an ensemble of heuristics/agents as the utility function to select a batch of samples. The paper proposes to use black-box optimization for optimizing the ratio of combining the agents. The authors perform experiments on various datasets. The results show that the proposed method outperforms the baseline heuristics in most settings, and sometimes performs significantly better.\n\nReview:\n\nI would vote for rejection of this paper. \n\n1. The authors use the whole section 3.1 to describe the MDP formulation of the batch-mode active learning problem. However, the final method is not any RL algorithm but using BO. This is disappointing and also misleading. Why is policy-gradient/Q-learning not used? What will the performance be if we use RL algorithms? \n\n2. The experiments are only comparing to the baseline heuristics, and are missing comparison with other previously proposed learning AL methods as in Section 2.2. This is not the first paper on learning to active learn in batch-mode, see e.g.:\n\t Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford and Alekh Agarwal. Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. ICLR 2020.\n\n3. The idea of ensembling different heuristics is not new either. For example, this paper uses a multi-armed bandit view to combine heuristics:\n\tGao L, Yang H, Zhou C, Wu J, Pan S, Hu Y. Active discriminative network representation learning. InIJCAI International Joint Conference on Artificial Intelligence 2018 Jan 1.\n\nMinor Comments:\n\n1. Last paragraph on page 4 - it should be $\\binom{n}{b}$ options to choose the batch (the orders do not matter for a batch).\n\n2. Algorithm 2: It is better to replace \"noEpisodes\" with \"numberEpisodes\".",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}