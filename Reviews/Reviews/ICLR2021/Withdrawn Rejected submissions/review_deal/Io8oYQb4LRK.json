{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper investigates methods for gradient-based tuning of optimization hyperparameters.  This is an interesting area, and the paper isn't bad.  The examination of hypervariance seems relatively novel and useful.  I also appreciate the point about Bayesopt sometimes working well simply due to small ranges.\n\nHowever, I agree with the criticisms of the reviewers.  Overall this paper isn't quite clear, thorough and impactful enough to make it in this round, but I think with more attention to baselines and scope this paper could be acceptable.\n\nSome minor comments:\n\n1) The signed-based optimizer, while simple and sensible (which is good), seem kind of ad hoc.\n2) The authors don't seem to have properly scoped the problem and method, since greediness is only a major concern for inner optimization hyperparameters specifically.  It's not clear that for regularization parameters that this problem exists or that your method would apply.\n\nA small nit:  Is hypervariance the right thing to look at, since the problem can exist even in deterministic settings?  Perhaps some sort of sensitivity analysis would be more appropriate.  Also you should reference Barack Pearlmutter's thesis which first explores these issues.   I would also mention that the hypervariance is generally tiny for smaller-than-optimal learning rates, and massive for larger-than-optimal learning rates, (the chaotic regime)."
    },
    "Reviews": [
        {
            "title": "Reviewer 2",
            "review": "The paper proposes an algorithm for tuning hyperparameters over long optimization problems to mitigate sub-optimal greedy solutions from short-horizon bias.  This is done by:\n \n(1) sharing hyperparameters across optimization steps which reduces the variance of hypergradients and computational cost\n \n(2) A method for assessing when our hyperparameters have converged\n \n(3) A forward-mode gradient calculation for SGD with momentum\n \nThese components are combined into an algorithm and empirical results are presented.\n \n \n \nStrengths:\n \nThe paper is well-written\n\nThe contributions are well-placed with the related work\n\nSharing hyperparameters across multiple steps is well-motivated.\n\nThe experiments on hypergradient variance are a useful diagnostic for these kinds of algorithms.\n\nTuning hyperparameters like this could be impactful if we scale it to state-of-the-art models and optimization procedures/horizons.\n \n \nWeaknesses:\n \nThe forward-mode algorithm is limited to SGD with momentum and weight-decay.  More complicated optimization algorithms are used in many domains.\n\nIt’s not clear to me the sign of the hypergradient is a good test for convergence.\n\nIf we can’t use modern optimizers like Adam with the hypergradient, I am worried that the algorithm is very brittle.\n\nThe improvements aren’t very large relative to the hand-tuned baseline.  See CIFAR-10 where the baseline is 89.2 +- .2 and theirs is 89.3 +- .1.  Perhaps your algorithm may find this setting faster since there are about 100 runs for the hand-tuning.  I would want to see if there is some way to offer an improvement over the grid-search.  What happens if you start your training with the hyperparameters initialized at the best-value from the grid-search?\n\nOnly a handful of hyperparameters are tuned -- perhaps this is due to how the gradient calculation scales with the number of hyperparameters.  In this hyperparameter regime, a good baseline to test against might be bayesian optimization as opposed to grid-search.\n \n\nMy recommendation for the paper is a 6 -- i.e., marginally above the acceptance threshold. This is because the paper makes useful contributions toward optimizing hyperparameters over long-horizons, by motivating sharing hyperparameters via gradient variance and providing a forward-gradient calculation skeleton.  However, the current improvements relative to the baseline are mediocre, and it’s not clear how to scale the method to optimizers besides SGD with momentum.\n \n \nThe following points may help improve the paper, but did not affect my score:\n \nFigure 1 says “10, 50 and 50”.  Is there a typo here?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper propose a new technique to compute hypergradients, relying on hyperparameter sharing, in order to prevent gradient degradation. In addition authors proposed a different stopping criterion for the bilevel optimization problem, which is paramount in practice. The paper seems very very interesting from a practical point of view.",
            "review": "Overall I vote for (weak) rejecting. IMO the main weakness of the paper is the clarity, for instance I had to read the article multiple times before understanding the main contribution. Some crucial terms should may have to be mathematically defined in the paper, like 'greedyness' for hyperparameter optimization, a term a was not aware of.\n\n\nAdvantages of the paper:\n- the proposed algorithm seems to lead to significance gain in performance in practice\n- authors proposed a new stopping criterion which seems to be more efficient in practice\n\n\nConcerns:\n\n1- The notion of greedyness is paramount for the paper, however it is not defined. Moreover the references provided are not very helpful to understand the concept. The word 'greedy' appears once in [1], twice in [4] and is not defined properly. The word 'greedy' does not appear in [2, 3]. Would it be possible for the authors to define the concept properly in the paper, or the provide a selfcontained reference?\n\n2- I founded the main contribution of the paper hard to find. If I understood well, authors propose a new to estimate the hypergradient by averaging out, and then updating the hyperparameter. This is very subjective, I would recommend to highlight this contribution, and maybe to remove 'we combined the above [...] with momentum decay' in the introduction.\n\n3- The experiments do not seem clear to me, I do not know if it comes from my lack of experience in the field of from the lack of clarity of the paper, but experiments were hard to follow. In particular the number of steps in the inner problem is provided, but what is the size of the hyperparameter searching space? In particular all the experiments are provided with convergence as a function of the number of steps. Since authors rely on forward differentiation, one step of the proposed algorithm can be more costly than one step of the algorithms in the baseline. Am I missing something? Is forward differentiation paramount for the proposed algorithm?\n\n4- Authors claim that hyperparameter sharing is equivalent to averaging (page 5). Is it mathematically grounded? Is it trivial? Or could authors provide a reference?\n\nThere are a lot of things I did not understand, thus I voted for weak reject. However if authors answer my questions I am of course willing to change my score.\n\n\nMinor:\n- in section 1 there should have a space after the point in l3\n\n\n[1] Luketina, J., Berglund, M., Greff, K., & Raiko, T. (2016). Scalable gradient-based tuning of continuous regularization hyperparameters. ICML 2016\n\n[2] Franceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017). Forward and reverse gradient-based hyperparameter optimization. ICML 2017\n\n[3] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. JMLR\n\n[4] Donini, M., Franceschi, L., Pontil, M., Majumder, O., & Frasconi, P. (2019). Scheduling the Learning Rate via Hypergradients: New Insights and a New Algorithm. arXiv preprint\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons",
            "review": "Review Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons\nThe paper presents an approach to hyper parameter optimisation over long horizons avoiding greedyness. This is achieved by calculating gradients wrt to the hyper parameters and using the sign of the gradients to indicate convergence. The paper is well written and structured and the suggested approach shows promising results. I only have two minor comments/ suggestions. \n\nIntroduction:\nAfter first sentence: there is an empty space missing: \").This\"\n\nExperiments:\nCould you compare your results based on some Bayesian Optimization based algorithm and compare to the computational cost? If not, why?\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "This paper tackles the important problem of HPO, and focus on the optimization of real-valued hyperparameters for DNNs using a gradient-based method",
            "review": "The authors tackle the problem of HPO, focusing on the optimization of real-valued hyperparameters for DNNs using a gradient-based method. This novel method enables non-greediness through the mitigation of gradient degradation and allows for long horizons.\n\nHow does this work compare against tools like BOHB (not cited in this paper) [1]? The method introduced here is based on hypergradients while BOHB is based on a combination of Hyperband and Bayesian optimization, so these are two different algorithms. Nonetheless, since BOHB can perhaps be considered state-of-the-art for DNN HPO including continuous and discrete hyperparameters, it is still worth to provide a  comparison in the experimental section. This is reinforced by the fact that the authors use random sampling as a baseline which has been shown to be inferior to BOHB on similar or identical benchmarks.  \n\nIn the methodology section, 3.1 and 3.2 are background material. It'd perhaps be clearer if the authors move these parts in a background section so as to differentiate the contributions from the background. The contribution section of this paper is section 3.3 where the key idea is the reduction of the hypervariance induced by the gradient degradation phenomenon. In the experiment section, the authors define the hypervariance, which should be defined in a previous section instead of the empirical part. Or else, since the hypervariance is somewhat defined twice, these two parts could be merged in section 3.3.  \n\nTo reduce the hypervariance the authors propose hyperparameter sharing and show empirically how it helps gradient degradation. In addition, the authors propose to use gradient signs as an indicator of convergence and show the impact empirically. No formal justification is provided for these two contributions which is a weakness of the paper. \n\nMinor remarks: \n- The abstract should explicitly mention that this is a Deep learning paper. The authors shouldn't expect that since the paper is submitted at ICLR then a reader will know that the paper is on DL. \n- It is usually a good idea to have the figures either at the beginning or at the end of a page rather than in the middle of the page. See for example Figure 2. \n- It is odd to have figure 1 on the second page if the figure is then used on page 8. On page 2, the readers are not able to understand the figure because the outer step in the figure is introduced later in the text. \n\n[1] Falkner, Stefan, Aaron Klein, and Frank Hutter. \"BOHB: Robust and Efficient Hyperparameter Optimization at Scale.\" ICML. 2018.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}