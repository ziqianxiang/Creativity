{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting direction, but limited novelty and requires improved experimental comparison",
            "review": "The authors claim to identify that weight decay controls the cross-boundary risk, in addition to the already known impact on the effective learning rate. They then propose a new regularisation method that directly influences the cross-boundary risk and effective learning rate. In addition, they propose a method for tuning the hyperparameters of this method. Finally, they contribute well-tuned hyperparameters for commonly used networks.\n\n## Strengths\n1. The writing is sufficiently clear and the arguments are easy to follow.\n2. The authors identify an important aspect currently ignored in previous literature investigating the interaction between batch normalisation and weight decay---namely, that the effective learning rate argument does not apply to the final layer of the network.\n3. They develop an effective hyperparameter tuning method for their method that tunes the parameters much faster than Bayesian optimisation approaches.\n\n## Weaknesses\n1. One aspect of the proposed regularisation strategy is to remove weight decay from convolutional layers and instead employ a form of weight normalisation. This seems to be of limited novelty, as other approaches mentioned in the related work section already do this. The authors should empirically compare the difference between their proposed global normalisation scheme and that of, e.g., Hoffer et al (2018), which does the normalisation on a per-layer basis.\n2. The analysis in Section 2.3 is on the right track but not entirely convincing. For example, I do not see why this statement is true:\n\t>In the meantime, $p_j$ will shrink faster because of the nature of softmax transform, thus weakening the overall gradient. This will leave $x$ being closer to the class boundary between $W_j$ and\n$W_k$ (larger cos $\\beta$).\n\n\tFirstly, I am not completely sure what is meant by \"weaken the gradient\", but I assumed it meant the magnitude of the gradient is reduced. This will only happen if the model is generally classifying instances correctly with high confidence, which could happen both because the norm of the weights is large or because the direction of the weights is particularly discriminative. It seems that you are assuming this can only be due to the first reason.\n\n\tFurthermore, the exploratory experiment demonstrates that the proposed method exhibits similar behaviour (i.e., weight norms and MCBR) to standard weight decay, so why should we expect FixNorm to perform *better* than weight decay?\n3. A large volume of work has been conducted that investigates the effect that weight decay on generalisation in both deep networks and linear models, which limits the novelty cross-boundary risk analysis and contradicts claims that weight decay is poorly understood from a generalisation point of view. In particular, it is known that constraining the norm of the weights is often equivalent to weight decay [1], and that bounding weight norms in models (both linear and deep networks) is a theoretically well-founded (e.g., [2,3,4]) strategy for improving generalisation performance. The prior theoretical analysis leads directly to the conclusion that when performing empirical risk minimisation with norm-bounded models and loss functions that take into account both the class assignment and a measure of confidence (e.g., hinge loss, cross entropy), the learning algorithm cannot just maximise the norm of the weights in order to increase confidence and therefore minimise the loss---it must move the decision boundary to better align the features with the weights of the linear classification layer.\n4. The proposed regularisation strategy is to control the magnitude of the convolutional and linear layers. Several existing approaches exist that also aim to do this [5,6,7], for various measures of magnitude, but this paper claims state of the art performance while only comparing with weight decay.\n\n## References\n[1] Oneto, L., Ridella, S., & Anguita, D. (2016). Tikhonov, Ivanov and Morozov regularization for support vector machine learning. Machine Learning, 103(1), 103-136.\n[2] Neyshabur, B., Tomioka, R., & Srebro, N. (2015, June). Norm-based capacity control in neural networks. In Conference on Learning Theory (pp. 1376-1401).\n[3] Bartlett, P. L., Foster, D. J., & Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems (pp. 6240-6249).\n[4] Long, P. M., & Sedghi, H. (2020). Generalization bounds for deep convolutional neural networks. In International Conference on Learning Representations.\n[5] Yoshida, Y., & Miyato, T. (2017). Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941.\n[6] Gouk, H., Frank, E., Pfahringer, B., & Cree, M. (2018). Regularisation of neural networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368.\n[7] Sanyal, A., Torr, P. H., & Dokania, P. K. (2020). Stable Rank Normalization for Improved Generalization in Neural Networks and GANs. In International Conference on Learning Representations.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting approach with unclear relevance. ",
            "review": "Summary: The authors propose a training method for deep neural \nnetworks that is based on analysis of the interplay between\nbatchnorm and weight decay. This method can also be understood\nas a reparametrization of the hyperparameters learning rate \nand strength of weight decay. The authors perform experiments\nwith ResNets and MobileNet on ImageNet.\n\nPros:\na) The paper is clearly written and the line of thought can \neasily be followed. \nb) The notation is clean and clear. \nc) The mathematical formulations are sound.\nd) To the reviewers knowledge, the suggested methods are new. The relation\nbetween batch norm and weight decay is known, which is also clearly \nstated in the paper. \n\nCons:\na) The work has limited relevance due to the \nfact that several claims are insufficiently backed\nby theory or data. Additionally, the reported \nimprovements are achieved for architectures \nthat are currently relatively far from state-of-the-art.\nAdditionally, repetitions of runs, error bars and \nsignificance tests are missing for some experiments.\ni) For example the difference between methods in Figure 2 (left)\nis unclear. It cannot be judged whether any of those \nmethods is better than another due to missing repetitions. \nHowever, this is one of the main points in the paper. \nThe authors should perform repetitions and also show this\neffect on multiple datasets.\nii) On page 4 bottom, the authors make a point that\nlarge \"g\" will yield small p_j which will in turn move\nx closer to the classification boundary, which will in \nturn lead to worse generalization. These conclusions\nare tenuuous and are hardly backed with data. If this\nis the fact, then the authors should approach domain \nshift data sets, and demonstrate that these statements\ncan be backed empirically. \niii) The role of different optimizer and initialization techniques \nhas been neglected. However, Adam and similar optimizer would \nimplicitly be able to consider the effect of the norm of the weights.\nAdditionally, initialization techniques mainly concern the norm \nof the weights.\nThe authors should elaborate on the effect of optimizers and \ninitialization techniques. \niv) The authors claim new SOTA on ImageNet Top-1 accuracy (Section 3.2).\nHowever, they improve architectures in with accuracies around 78%, which\nwas already reached in 2016 [1]. The authors should consider to \nimprove SOTA architectures with FixNorm, e.g. [2]. \nv) Embedding into related work should be improved. The reviewer suggests\nto include current Section 4 into the Introduction because it is \nthe basis for this work. Other related works, such as normalization \ntechniques, hyperparameter optimization, and reparametrization \nshould go into a related works section. \nvi) The FixNorm operation appears relatively general and not \nconstrained to convolutional networks and image classification. \nThe authors should also test this on other classification tasks to\ndemonstrate broader applicability. \n\n\nQuestions: \nQuestions are implicitly contained in the suggestions above.\na) What are potential interactions between optimizers and\ninitialization on FixNorm and weigh normalization?\nb) Figure 1, Figure 2, Figure 4: Does this hold across\nrepetitions and across different datasets\n\n\n\nMinor: \na) The paper contains many typos. Editing required.\nb) The citations should have a space after the last word, e.g. \"[...] performance(Krizhevsyk et al. [...]\" --> \"[...] performance (Krizhevsyk et al. [...]\" .\nc) When abbreviations are introduced, a space should be left\nbefore the bracket. e.g. \"[...] effective learning rate(ELR)\" --> \"[...] effective learning rate (ELR)\" \nd) \"Does it covers [..]\" --> \"Does it cover [...]\"\n\n\nReferences:\n[1] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n[2] Xie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical comparison to norm-bounded WN is required",
            "review": "##########################################################################\n\nSummary:\n\nThis paper first proposes FixNorm that replaces the weight decay (WD) for convolutional layers followed by batch normalizations (BNs). FixNorm normalizes weights of all convolutional layers, which is expressed as a single vector, in each iteration of optimization. FixNorm for conv layers is empirically examined to have a comparable effect to the ordinary WD in terms of the regularization. This paper then proposes FixNorm-FC for replacing WD in the final fully connected layers. FixNorm-FC is a variant of weight normalization (WN) in which parameter $g$ in WN is bounded by a new hyper parameter $\\alpha$. FixNorm-FC is introduced from the analysis of cross-bounbary risk, implying that the larger the weight norm of the final FC layer is, the more the classification is done near the class boundary. The authors also propose a greedy heuristic to tune hyper parameters of FixNorm/FixNorm-FC (learning rate and $\\alpha$).\n\n##########################################################################\n\nReasons for score: \n\nOverall, I vote for weak rejecting. The analysis of cross-boundary risk is interesting and it is practically nice that tuning lr and $\\alpha$ is easier than tuning lr and $\\lambda$ in WD. However, the idea behind FixNorm is almost the same as the norm-bounded WN in Hoffer et al. (2018) and FixNorm-FC is also a slight modification of norm-bounded WN. Although the authers discuss the difference between the proposed method and norm-bounded WN in Section 4, it would be necessary to compare them in their experiments.\n \n##########################################################################\n\nPros: \n\n1. The impact of weight norm in the final fully connected layer is first analyzed from the viewpoint of cross-boundary risk.\n\n2. Tuning the hyper parameters of the proposed method is easier than tuning that of the ordinary weight decay.\n \n##########################################################################\n\nCons: \n\n1. The proposed method has a limited novelty, regarding the norm-bounded WN in Hoffer et al. (2018). The difference is not examined in the experiments.\n \n##########################################################################\n\nMinor concerns:\n\n- The discussion of ELR in Sec. 2.1 considers a normalized weight vector ($\\hat{w}). Then why not normalize each weight vector in line 6 of Alg.1, instead of normalizing all conv weights in a single vector.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Experiment results are good, but the motivation and analyses can be improved.",
            "review": "Summary: The paper proposes to replace weight decay in DNN training by the proposed FixNorm scheme, i.e. rescaling conv parameters to their initializations and using weightnorm and weight decay for the final fully-connected layer. The main advantage of FixNorm is that it decouples the effect of weight decay on effective learning rate (ELR) and cross-boundary risk so that the hyperparameters in FixNorm can be more effectively searched than those in weight decay. Through extensive hyperparameter searching, FixNorm achieves a better performance than the baseline training scheme in image classification, detection and segmentation. \n\n\nPros: \n1) The FixNorm decouples the effect of ELR and cross-boundary risk of weight decay into learning rate searching and weight decay for the final fully-connected layer so that the two hyperparameters, learning rate and alpha in FixNorm, do not interfere with each other when searching for the optimal setting. Empirical studies validate that the hyperparameter searching in FixNorm achieves a better performance than searching hyperparameters in weight decay in the image classification task. The proposed FixNorm offers another possibility of hyperparameter searching and may benefit some researchers in this field.\n\n2) The writing and structure of this paper are clear, which makes the paper easy to follow.\n\n\nCons:\n\n1)\tThe motivation for using FixNorm is not convincing. First, the cross-boundary explanation may have its own problem. Second, the reason for rescaling the conv parameters is not explicitly given. See detailed comments below.\n2)\tThe practical significance of this method is not fully demonstrated in this paper. First, there are many ways to improve the performance of DNNs on ImageNet by sacrificing computational time, like data augmentation [1, 2] and adversarial training [3]. It is unknown why people should use FixNorm to improve accuracy on ImageNet instead of data augmentation. If the paper can show that using FixNorm improve the performance of some practical tasks without heavy hyperparameter-tuning, the practical usage of FixNorm will be improved a lot.\n\n\n\nComments and Questions:\n1)\tThere are three questions on the argument of cross-boundary risk. First, In the 4th paragraph of Section 2.3, the paper says p_j will shrink faster than the growth of g. There is no empirical or theoretical evidence on this claim. Second, the argument that the overall gradient is weakening does not consider the change of W_k and W_j. The gradient with respect to weights may encourage the difference between W_k and W_j. Finally, the gradient of loss w.r.t. features is shrinking as the training continues is a natural consequence of SGD optimization. If the gradient is always large, then it means the SGD optimization does not converge. And no matter how small the gradient is, it always aims to push x away from the decision boundary. In conclusion, the shrinking gradients of Equation (8) cannot be taken as the reason for x being misclassified. \n\n2)\tThe reason why rescaling convolution kernels during training is not fully explained. If the paper accepts the hypothesis that the effect of weight decay is due to ELR, then rescaling the parameters to the initializations may contradict the hypothesis since the rescaling keeps conv parameters at the same norm levels. If the paper suggests that searching for the learning rate is going to increase the effective learning rate, then it should demonstrate that the searched learning rate matches the effective learning rate in weight decay. \n\n3)\tThe comparisons between FixNorm-tune and reference in detection, segmentation and group norm are probably not fair since FixNorm-tune uses 11 times more time than the baseline to search for a hyperparameter setting. It is unknown whether the performance of the reference will be better than FixNorm-tune, if the baseline also uses 11T_max time to search for hyperparameters or augment data in some way.\n\n4)\tFigure 4 uses the accuracy of training 50 epochs to demonstrate that the optimal value of learning rate and alpha are somehow independent. What if we train the network using full epochs, like 120 epochs in Resnet50? Does the independence also hold?\n\n5)\tAlthough the empirical studies on ImageNet are important, it is also interesting to see how FixNorm performs in small datasets like CIFAR10 or SVHN so that other researchers can have a quick test on the proposed method, like many did in their paper, e.g. resnet, densenet etc. \n\n\nMinor:\n1)\tIn the second paragraph of Introduction, the sentence should be “Does it cover the full mechanism(s)…” instead of “Does it covers the full mechanism(s)…”\n\n2)\tIn equation (7), (8), (9) and related equations in the appendix, it is better to distinguish the input x and the features after the final convolution layer or global average pooling layer. \n\n\n[1] Zhang, Hongyi, et al. \"mixup: Beyond Empirical Risk Minimization.\" International Conference on Learning Representations. 2018.\n[2] Yun, Sangdoo, et al. \"Cutmix: Regularization strategy to train strong classifiers with localizable features.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[3] Xie, Cihang, et al. \"Adversarial examples improve image recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}