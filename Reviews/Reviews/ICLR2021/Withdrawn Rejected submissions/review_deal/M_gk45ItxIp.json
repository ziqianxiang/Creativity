{
    "Decision": "",
    "Reviews": [
        {
            "title": "An interesting hybrid approach of HRL and neuro-symbolic reasoning but not adequately presented and relevant baselines missing.",
            "review": "The paper proposes to address the scalability issues of neuro-symbolic RL approaches by integrating neuro-symbolic learning into a hierarchial RL framework and using ideas from multi-hop reasoning on knowledge graphs to induce interepretable rules rather than the widely-used differentiable ILP approach. The proposed method uses a pre-defined set of entities and relations to define a higher-level symbolic state as a tensor made up of N MxM matrices (N for total number of relations/predicates and M for total number of entities), and learns soft attentions over all relations at each time step of a potential path, and over multiple paths of different lengths. \n\nWhile this approach was evaluated on a more complex domain successfully, I have a few concerns as follows:\n\n1. While neuro-symbolic approaches may not have been shown to scale to more complex domains, they still serve as good baselines in order to measure the progress in neuro-symbolic RL. Thus, I recommend the authors to evaluate their proposed method on toy benchmarks such as BlocksWorld and GridWorld and compare their performance against baselines such as (Jiang & Luo, 2019; Dong et al., 2019; Payani & Fekri, 2020). Otherwise we'll end up with many neuro-symbolic RL approaches that are probably not any better than those already existing.\n\n2. Figure 1 shows a pipeline of extracting objects from the scene. This is misleading as this is not part of the proposed method and is external to it. Furthermore, the paper does not explain what's used to extract the objects and, more importantly, how the relations are extracted amongst the objects; and, also how the accuracy of this process has on the overall performance of the system.\n\n3. It's not clear how the networks in (8-11) are trained. How do you go from rewards to the Q values in (12) and back to these networks? This significantly hinders reproducability of the work.\n\n4. v_prime(x) in (12) is a one-hot encoding of object x. Which means the pipeline reasons about paths between 2 entities, x and x_prime. How does that lead to a mapping from a state (which is an Nx2|X|) matrix of multiple objects and relations, to an action which, presumably, is a relation?\n\n5. In Algorithm 1, what are Dm, Ds1,....,Dsj and Qm?\n\n6. Figure legends in Figure 3 are hard to read.\n\n7. Why does SDRL fail when it was shown to solve the task after 1.5M steps in (Lyu et al., 2019)? It was shown to even be better than hDQN on this task. This renders the experiments questionable.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A not very clear approach of how to use of symbolic logic in deep reinforcement learning",
            "review": "The authors introduce symbolic logic into deep reinforcement\nlearning. States and actions are represented as predicates used in a\nhierarchical reinforcement learning framework.\nThe proposed approach, called NSRL, first extracts objects and\nrelations to create relational symbolic states to the controller which\ngenerates tasks and provides intrinsic rewards to the controller.\nThe controller interacts with the environment until achieving the\ntask or reaching a maximum number of steps\n\nThe meta-controller has a reasoning module, an attention module, and a\npolicy module. The reasoning module uses the framework described in\nYang et al. 2017,  where queries between two entities are answered by\nsearching a relational path between them. A multihop reasoning\napproach is used to search for chain-like logical rules or relational\npaths.  \n\nTwo attention modules are used, using Transformers (Vaswani et al. 2017),\nto weight the predicates and the paths created by the reasoning module.\n\nFor the policy module, the predicate of the last hop is constrained to\nbe an action and a multi-layer perceptron is used.\n\nThe approach is tested in the Montezuma's Revenge game Atari game, and\ncompared against HDQN and SDRL.\n\nThe description of the approach is not clear enough.\n- How is the policy module trained?\n- It seems that the symbolic state and relations are extracted from\nuser defined functions, please clarify. How easy can these functions\nbe defined? How sensitive is the performance of the proposed approach\nto these definitions?\n- It is not clear what is the role of the controllers? how to choose\nthe number of controllers to use? how are the controllers learned? how\nlong it takes them to learn?\n\nThe authors talk about FOL, however, they should clarify that they are\nnot considering function symbols or recursive definitions.\n\nAlso their hierarchical reinforcement learning framework seems to be\nrestricted to only two levels. \n\nPros:\n- An interesting combination of FOL with transformers to produce\ninterpretable results.\nNegs:\n- The description of the proposed approach is not very clear.\n- The experimental section is weak, as the proposed approach is\ntested on a single domain, and compared against a baseline an another\nrelated algorithm\n- A set of tailored predicates to the domain, needs to be defined.\n\nThere are some English errors, in particular in the description of the\nexperiments, that need to be attended.\n\nTypos:\nLogic(FOL) => Logic (FOL)\nfinding find\nperfrom (in Algo. 1)\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper but not clear",
            "review": "The authors propose a deep reinforcement learning system for sequential decision making whose decisions are logic rules and hence interpretable.\nThe approach seems interesting and the tackled problem could interest a wide audience. However, the main issue of the paper is its presentation. In my opinion, the paper lacks clarity. There are several concepts that are not introduced and entire sections that are not explained.\n\nSome of the unsolved questions:\n - Could the authors explain the concepts of intrinsic and extrinsic rewards?\n - Why there are several controllers?\n - Subsection 4.3: \n   - the input of MHDPA, i.e., query, key and value representations, is not explained, what are they?\n - Algorithm 1 is not explained and it seems completely unrelated to subsection 4.4. \n   - What does EPSGreedy function do?\nIn summary, it is not clear to me how logic rules are obtained.\nFor all these reasons I think the paper is not ready for publication.\n\n[Minor comments]\n\nP. 3\n“A typical First-Order Logic(FOL)” -> “A typical First-Order Logic (FOL)”\n“components :” -> “components:”\n\nP. 5\n“can be seen as an embedding of predicate varying with time”. The word “time” is rather confusing, do you mean reasoning steps?\n\nSection 7 does not have any content.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Recommendation to Reject",
            "review": "#### Summary\nThis paper proposes a neuro-symbolic reinforcement learning approach that extends the SDRL (Lyu et al. 2019) by changing the symbolic planning based meta-controller to a differentiable ILP module to enable differentiable symbolic learning in the DRL context. The proposed method first utilises a pre-trained object detector to interpret image inputs into symbolic states defined by the predicates in a prior background knowledge base; then the differentiable-ILP module is used for learning policies that can be represented by first-order definitive clauses; the learning is enabled by a set of controllers that can translate logic policies into agent actions and interact with the environment. The policies are represented by a set of first-order chain-rule, and the differentiable logical reasoning is realised by tensor multiplications like Logic Tensor Networks (Donadello et al., 2017). Experimental results in the first room of Montezuma’s Revenge showed that the proposed approach could learn human-interpretable policies while achieving comparable performance to hierarchical DQN.\n\n#### Pros\n+ The proposed approach unifies differentiable ILP with deep reinforcement learning such that the policies are learned explicitly in symbolic form while using gradient descent in the deep reinforcement learning (DRL) context;\n+ Using attention models to represent logic rules as relational paths is straightforward and effective;\n\n#### Cons\n- The proposed approach is incremental; it is a variation of SDRL, which extends HDQN (Kulkarni et al., 2016) with a symbolic controller. The only difference between SDRL and this paper is that this paper uses a neural symbolic model instead of a pure symbolic planner. However, in the experiments, the SDRL cannot reproduce the performance in its original paper, and there is few discussion about why does it happen.\n- The input data are converted with pre-trained object detector, and the domain-specific language (DSL) in the experiment is too specific, which can only describe one room in the game;\n- The experiment is not strong enough to verify the effectiveness of the presented approach; I suggest the authors include more ablation studies to support their claims:\n  1. Comparing SDRL and NSRL with the same language bias, and add more discussion about why using differentiable ILP is better than a pure symbolic planner;\n  2. Including more experimental domains;\n  3. Comparing to other SOTA neural-symbolic reinforcement learning approaches, e.g., NLRL (Jiang and Luo 2019). Because the data is already converted to symbolic form, it should be fairly easy to apply these approaches on this task.\n- Chain-rule with dyadic predicates has limited expressive power; one needs to carefully design the DSL before learning to represent the states and hypotheses, which makes the approach difficult to be applied to general problems.\n\n#### Recommendation\nOverall, I think this paper is not ready for publication in its current state. The idea of combining differentiable ILP and deep reinforcement learning is promising, and I like the idea of using attention to learn logic chain-rules as relational paths. However, the experiments are too weak to verify the effectiveness of the presented approach, and the difference between this work with previous works needs to be clarified.\n\n#### Additional questions and comments\n- Is the compared SDRL in this paper using the same set of primitive predicates as NSRL? In the original paper of SDRL, the DSL of the same symbolic domain has 6 predicates; while in this paper, your experiment defines the symbolic states with 8 predicates. Is the difference of language bias that results in the bad performance of SDRL?\n- What is the EPSGreedy in Algorithm 1? Is it the same procedure in Kulkarni et al., (2016)?\n- Since the reasoning module is already differentiable, is it possible to learn the policies and object detector simultaneously by end-to-end learning?\n- The performance of  SDRL is significantly worse than that in its original paper, which shows that SDRL is sensitive to language biases. However, considering the similarity between NSRL and SDRL, I think the authors should consider the language bias of DSL in (Lyu et al., 2019) and apply it to both approaches to make a fair comparison.\n- An advantage of learning explicit symbolic rules is that the learned model can be easily re-used to extrapolate on unseen data. I think the authors should use a general-enough DSL to represent one DRL problem and show that the learned rules can be re-used on different stages in one game.\n- Typo below equation (1): ... query corresponds to finding find a relational path ...\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}