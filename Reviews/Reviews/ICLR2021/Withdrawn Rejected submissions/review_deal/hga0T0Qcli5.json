{
    "Decision": "",
    "Reviews": [
        {
            "title": "Review of the paper",
            "review": "This paper proposes a new embedding method for entities and relations in a knowledge graph based on quaternions. It builds on the recent work by Zhang et al. 2019 which introduced QuatE. Based on what authors have mentioned, the problem with QuatE is that it does not consider the correlation between the attributes of the head and tail entity. Authors have mentioned that this could result in the degradation of performance. The solution that authors have proposed is to use two relation aware quaternion vectors for each relation. Authors have mentioned that this increases the correlation between head and tail entities.\n\nThe major problem with this paper is its contribution and novelty which is limited. The major idea of using quaternions was first introduced by Zhang et al. in 2019 and here authors have replaced the relation vector in their work with two relation aware quaternion vectors in their work. On the other hand, this simple change has resulted in outperforming QuatE which is indeed encouraging and intriguing.\n\nI have several questions about the paper:\n- Why just using two quaternion vectors and why not more? What happens if we use more? are we going to capture correlations in a better way?\n- What happens when you use specialized initialization scheme in Zhang et al.? In the results presented in this paper, have you also initialized QuatE with the standard Glorot initialization? \n- Other than the visualization in Figure 2, how else can we show that having more than one vector to represent the relation is helpful? and why is it helpful exactly?\n\nMinor comments:\n- typo on page 3 before equation 2, \"a n-dimensional\"\n- the operations for the quaternion algebra could be moved to the supplementary material.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A variant of quaternion knowledge graph embedding",
            "review": "The paper proposes QuatRE, a new variant of quaternion knowledge graph embedding. It represents entities and relations in the Quaternion space. The scoring function of a relational triple is designed by measuring the quaternion-inner product between the rotated head entity embedding and the rotated tail entity embedding. Experiments on the standard benchmark datasets for link prediction demonstrate the effectiveness of the proposed QuatRE model.\n\nThe biggest weakness of this paper lies in that the proposed technique QuatRE is very similar to a NeurIPS-2019 paper QuatE. QuatE uses the relation vector to rotate head entity embeddings. QuatRE extends QuatE by associating each relation with two quaternion vectors used to rotate the head entity embeddings and tail embeddings, respectively. Although the authors claim that such rotations can strengthen the correlations between head and tail entities, although the extensive experiments demonstrate the advantage of QuatRE, in my opinion, the technical contributions are still limited and insufficient. Maybe the authors can further exploit the usage of quaternion knowledge graph embeddings for more tasks to enhance the novelty. \n\nOverall, I think the novelty and technical contributions of this paper are insufficient.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Incremental work of QuatE that lacks of insights and details of the analysis ",
            "review": "The authors propose QuatRE, a knowledge graph embedding method that mainly builds upon QuatE [1] but using two relation-aware quaternion vectors to rotate the head and tail entities before calculating the same scoring function as QuatE. \n\nStrengths of the paper:\n\n1. This paper is well-writing and easy to follow.\n\n2. This model appears to achieve the state-of-the-art performances on benchmark datasets in terms of the link prediction task.\n\nWeaknesses of the paper:\n\n1. The main idea and contribution of this paper is merely the augmentation of relation-aware quaternion vector, which is similar to many existing works, such as TransR [2]. So this contribution is somewhat incremental.\n\n2. The performance improvements of the proposed model over QuatE are minor in both the WN18 and FB15k datasets (e.g. in some metrics, it even performs worse than QuatE). No significant test result is provided, and don't know how many runs were conducted for obtaining these numbers. So it is hard to know whether some of the improvement is consistent. Some analysis should be given to explain the reason why it performs worse in dataset WN18 and FB15K.\n\n3. No insights and details of the analysis are provided for validating the component of the relation-aware quaternion. So it is hard to judge where is the improvement comes from.\n\n4. No codes are provided, which lower the reproducibility of this work and make the reviewers hard to judge if this is a fair comparison.\n\n5. The analysis and experiment of the model capability to patterns of the relation, i.e., symmetry/antisymmetry, inversion, and composition, should be added as previous researches.\n\n6. The conclusion of Figure 2 is not convincing for me. The two visualization figures have different scales (e.g. on x axis, [-100, 100] on left v.s. [-150, 150] on right), so it is hard to judge which one is denser than the other one.\n\n7. Mathematical rigor and insights of the proposed relation-aware rotation are not sufficient. \n\n\nOverall, this is an incremental work of QuatE that explores the integration of relation-aware quaternion vector. There is a number of weaknesses as indicated above, with the major one being the lack of novelty of the proposed method, hence I vote for a reject.\n\n[1] Zhang, Shuai, et al. \"Quaternion knowledge graph embeddings.\" Advances in Neural Information Processing Systems. 2019.\n[2] Lin, Hailun, et al. \"Learning entity and relation embeddings for knowledge resolution.\" Procedia Computer Science 108 (2017): 345-354.",
            "rating": "2: Strong rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "KG completion model based on quaternions that requires improved evaluation",
            "review": "This work proposes a new model for KG completion, based on embedding entities and relations into quaternion space, achieving promising performance on standard KG completion datasets.\n\nAlthough the proposed model achieves promising results, the comparison to most existing KG completion models is unfair given the different number of parameters and an improve negative sampling strategy used. Separately, further ablation studies are needed to determine which relation-specific component of the model gives the most improvement compared to QuatE. The benefit of the negative sampling strategy on the model performance should also be evaluated. Therefore, I believe the paper is not ready for publication.\n\nDetailed comments and questions for the authors:\n\nSec. 1\n- I've found the introduction to be too detailed, containing equations that belong to the background and main body of the paper.\n- The two main contributions are in fact one: a new KG completion model that achieves relatively strong performance.\n\nSec. 3.1\\\nThe standard textboox definitions on quaternions are taking up too much space and should be moved to the appendix and replaced with the below proposed ablation studies.\n\nSec. 3.2\n- I can understand the motivation behind v_r1 and v_r2 to make different quaternion components of v_h and v_t interact with each other, but what is the role of v_r, i.e. why do we need to rotate v_h twice?\n- It would be interesting to see an ablation study of how much each relation vector (v_r1, v_r2 and v_r) contributes to the overall performance.\n- The negative sampling method proposed may make QuatRE comparable to QuatE, but it also makes it incomparable to other approaches which do not use it. An ablation study of the influence of negative sampling is needed.\n\nSec. 5\\\nThe comparison of QuatRE to existing models other than QuatE is not fair, as QuatRE contains 4x more parameters than any of the other approaches, due to 4 components of the quaternion. The authors should compare QuatRE to existing models with a comparable number of parameters. \n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}