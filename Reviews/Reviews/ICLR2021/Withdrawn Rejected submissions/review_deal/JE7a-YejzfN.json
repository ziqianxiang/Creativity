{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper presents the method of using Fisher information matrix values to identify examples near the decision boundary for a model, and proposes to preferentially use these examples in evaluation. \n\nPros:\n- Reviewers found this use of FIM values to be novel and interesting.\n- The paper presents fairly extensive results demonstrating the properties of examples selected and perturbed under the proposed methods. \n\nCons: \n- One of the main aims of this paper is to promote the use of examples selected in this way as evaluation sets. The method relies on values estimated from a specific model to select difficult examples, so this raises a fairly serious objection: If our goal in evaluation is to produce a fair comparison of two models, how do we choose which model we should use to select the test examples?\n\nReviewers were fairly unanimous in the objection that they raised, and while there was substantial discussion both with the authors and among reviewers, no reviewer was satisfied that the authors took this concern seriously or offered a clear resolution."
    },
    "Reviews": [
        {
            "title": "Potential of 1st Eigenvalue of Fisher Information Metric to identify difficult examples, More experimental results needed",
            "review": "Summary:\nThis paper defines a new metric to quantify the difficulty of an example in language classifiers: the largest eigenvalue of the Fisher Information Metric (FIM).\nThey show a geometric interpretation of the FIM, the greater the largest eigenvalues of the FIM, the closer the example is to the decision boundary. The authors illustrate this with a toy example. They then show how perturbations of examples with high eigenvalues for FIM, e.g., substitutions of one or more words, can flip the output of a classifier. Whereas, examples with lower eigenvalues are not as vulnerable to substitutions. The paper also argues that previous work on adversarial examples using contrast sets and counterfactual examples, are in fact not as vulnerable to substitutions, measured by the FIM. \n\nStrengths:\n- Development of a quantifiable metric to identify difficult examples. It would indeed be great to have such a metric, and help model developers to identify examples that are especially vulnerable to small perturbations. This in turn can help in developing more robust models. Therefore, I think there is potential for broad impact of this work.\n- As far as I know, the idea of using eigendecomposition of FIM to differentiate between easy and difficult examples for NLP applications is novel. It has been used to generate adversarial examples in computer vision. Though I think the application to NLP is a strong independent contribution. Assuming the claim can be established more definitively, the community can report results on thus identified difficult examples thus allowing us to inspect the vulnerabilities of any model. \n- The potential geometric interpretation of FIM is appealing. If this can be shown to hold given the nonlinearities in deep learning models this would be a very impactful result.\nWeaknesses:\n1. More empirical results are needed. Alternately, theoretical proofs may be used to substantiate the claims.\na. The idea that examples associated with large eigenvalues for FIM are more sensitive to perturbation is illustrated on a few select examples. However, it would be great to see this hold across the dataset. One way to show this might be to perturb all or some subset of examples in the test set, and plot the number of perturbations needed to flip the classifier output against largest eigenvalue of FIM.\nb. The reasoning that high eigenvalues correspond to points near the decision boundary is made based on an illustration on a linearly separable toy example. It is not clear this will extend to deep learning models with more complex decision boundaries. \nc. The only case against contrastive sets and counterfactual examples is that they dont have a high eigenvalue for the FIM. However, as this paper does not definitively establish yet that high FIM values is always correlated with difficulty in classification/sensitivity to perturbation, the argument against competing methods is insufficient.\nd. Since the use of eigenvalues as FIM as a measure of example difficult is generally applicable across several NLP tasks, the authors can make a stronger case by adding more datasets and tasks to the experimental results.\n2. Related work should be explained in more detail. A case is made comparing the proposed metric against contrast set and counterfactual examples (recent works). These should be described in more detail so paper is self-contained.\nSimilarly, prior work around eigendecomposition of FIM should be given due credit. Zhao (2019) is mentioned in the introduction. However, the derivation in the methods section should be duly attributed as well.\n\nRecommendation:\nMy recommendation is to reject the paper at this time. While I think there is great potential in the idea of using eigenvalues of the FIM to identify difficult examples, I think the experimental results are not sufficient to support the case as described above. \n\nI encourage authors to make a stronger case in a future revision, by addressing the following points:\n- On any existing task/model, can it be shown that examples that are erroneously classified typically have high eigenvalues for FIM.\n- Can the result be demonstrated on more than one dataset/task? \n- Can the result be demonstrated on more than a handful of examples for a task. e.g., by reporting average eigenvalues vs number of perturbations described above.\n- Can the geometric interpretation be illustrated on more complex decision boundaries?",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "interesting observation, but the method is used and evaluated for the wrong purpose",
            "review": "## Summary\nThe authors argue that we should evaluate the robustness of NLP models near their decision boundaries, and argue that contrast sets and counterfactual examples cannot fullfill this purpose. The authors propose to find examples near the decision boundary using the largest eigenvalue of the Fisher information matrix, arguing that this value gives us a sense of how stable the model prediction is near the input. To verify that FIM can identify examples where the prediction is unstable, and perturbation leads to larger prediction change, the authors use some heuristic adversarial attacks: first identify tokens to replace using integrated gradients, then replace the tokens with synonyms (to confirm the prediction is sensitive) or antonyms (to confirm that the prediction is insensitive).\n\n## Detailed comments\nIG-based perturbation: I was initially confused by how the authors plan to use IG for perturbation, since IG gives a scalar score for each word and does not tell us a direction for the word embedding to move along. Then I realized that IG is only used to select which token to perturb, and the actual perturbation is done by replacing words with their synonyms/antonyms. I think it’d help to clarify this point.\n\nFraming: the largest eigenvalue of FIM tells us how easy it is to change the output probability distribution of the model by perturbing the input, but we don’t know if the groundtruth label of the example will change after perturbation. In other words, in the general case, whether or not the perturbation leads to adversarial examples still requires human verification. In this paper the authors get around doing human verification by replacing words with only synonyms or antonyms. But I think it would help reinforce the message to make this point clearer. For example, instead of saying “ and these examples are most susceptible to perturbations” (from second paragraph of section 4.1), it might be better to say “the model prediction on these examples are most susceptible to perturbations to the input”. I’d argue against calling examples selected by FIM “difficult” and “easy”, maybe “unstable” and “stable” is better. Keep in mind that FIM is computed specifically for a model, and the largest eigenvalue of FIM describes a property of the model’s induced distribution near that example, not the example itself.\n\nWhy do we need FIM? This paper did not introduce a new perturbation, it introduced a way to identify examples that have high potential to be perturbed into adversarial examples. Under this framing, I’m not sure that the proposed method is evaluated against proper baselines. Importantly, the authors motivate this work by first observing that contrast sets and counterfactual examples do not really evaluate the models near their decision boundaries. That is probably a fair assessment, but those work have a different goal in mind. Both contrast sets and counterfactual examples ask human annotators for small changes that significantly change the _groundtruth label_ of the example. Maybe these edited examples are not near the decision boundary, that’s because they are not designed to be; being close to the decision boundary is purely incidental. I think this is an important distinction between contrast set/counterfactual examples and gradient-based adversarial attacks such as [HotFlip](https://arxiv.org/abs/1712.06751) and the human-in-the-loop counterpart such as [TrickMe](https://arxiv.org/abs/1712.06751).\n\nThe authors verify that the examples selected by FIM are indeed “difficult” (again, this is not the best term) using several heuristic-based attack (replacement with synonym, antonym, and replacement with actress names). I don’t think this is a convincing evaluation of FIM as a difficulty measure: one can simply try these attacks and find the same set of “difficult” examples. A more valid baseline is perhaps selecting examples by prediction confidence.\n\nI think whether “models should be evaluated near decision boundary” is debatable. Yes, it’s likely easier to create adversarial examples near the decision boundary. But considering that we cannot yet claim that robustness in NLP is solved for the “easier” examples, I’m not sure focusing on examples near the decision boundary is the right objective right now.\n\nFigure 1(a): no details were provided as to how the figure is generated for IMDb examples. How did you reduce the dimensionality of the decision boundary to 2D? How did you pick the examples? What does the green band signify? Are these real examples & real models or is this figure illustrative?\n\nFigure 1(c): are colors (blue vs red) indicative of the sign of the eigenvalue? It looks like the direction of the corresponding eigenvectors are only visualized for red points, how are they selected?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting idea but lacking concrete empirical/theoretical results",
            "review": "**Update after author response:**\nI went over the author response and have had a chance to carefully evaluate the updated draft. I appreciate that the authors have taken the time to address two of my comments but I believe major concerns still remain unaddressed, so my evaluation remains unchanged.\n\n1. I appreciate the time taken to conduct the fastText experiments, would be helpful to show this on BERT (which is arguably your strongest model) as well.\n\n2. Per my reading it appears that you've shown how models behave if the replacements are sampled at random. I appreciate the time taken to show that. I believe this could be made better with a concrete discussion of the same.\n\nCons 3, 4, 5, and Additional comments 1,2: I do not think these have been convincingly addressed. You point to Dynabench while suggesting that this would eventually lead to better evaluation sets but as it is shown in [1] and [2], \"difficult\" evaluation sets that are tied to one model are not necessarily difficult for other models. If the idea is to identify model's vulnerabilities and not use these as common evaluation sets, then this is more or less the same as generating adversarial examples (but called \"difficult\" examples, in which case this would be better presented as an application/extension of Zhao et al to identify adversarial examples in NLP). If the idea is to only use these examples for evaluation purposes, then the comparison to counterfactually augmented data (which you show does not include \"difficult\" examples per your definition) does not make sense since that is meant solely for augmenting training sets. Discussion in Section 4 and onwards is shallow and often unclear. That is primarily where I believe the exposition can be improved significantly. For instance, Section 4 says FIM values capture resilience to linguistic perturbations but that has not been discussed in any of the paragraphs that follow. Section 5.1 ends in a sentence that says, \"By repeating this process multiple times, more robust classifiers can be created\", and that is not discussed any further as to why you think that would be the case. It is also not supported by any theory or empirical results presented in your prior or updated draft.\n\n[1] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. \"Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering.\" In TACL.\n\n[2] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. \"Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.\" In TACL.\n\n---------------------------------------------------------\n\nSummary:\n\nThe paper proposes using the largest eigenvalue of the Fisher information metric (previously studied to identify adversarial perturbations in computer vision) as a measure to construct \"difficult examples\" for the IMDb sentiment analysis task. The authors suggest that one may perform such perturbations by replacing \"important\" words in the reviews. They further analyze those examples comparing them to counterfactually revised data (CRD; Kaushik et. al, 2020) and contrast sets (Gardner et al., 2020) and show that based on the FIM criteria, CRD and contrast sets are not too different from original reviews in the IMDb dataset.\n\nPros:\n- This paper describes a simple method for understanding difficulty in terms of ease-of-perturbation.\n- The authors find that, interestingly, sentiment analysis datasets generated by counterfactually revising data are not \"difficult\" for classifiers.\n\nCons:\n- While the idea of using FIM to identify examples closer to a classifier’s decision boundary is interesting, it is a new application to NLP even though the idea has been discussed extensively in Zhao et al. (2019). However, the paper doesn't show why these examples may be useful. The paper reports no empirical results or analysis thereof to understand the practical efficacy of these \"difficult examples\" while discussing only the changes in eigenvalues.\n- It is not clear as to how generalizable this approach is. For instance, the paper uses a replacement strategy based on substituting synonyms, antonyms, and certain kinds of noun phrases (actress names etc.)  which may not directly translate to other tasks such as question answering, news classification, VQA, or even NLI. \n- The paper makes many philosophical claims without theoretical or empirical justification. At one point it suggests that FIM captures resilience to linguistic perturbations but provides no backing for the claim. The paper further suggests that evaluation sets should increase FIM but provides no theoretical/empirical justification for the same. At another instance it says, “[t]ransfer learned models like BERT capture rich semantic structure … and tend to rely on semantically relevant words for classifying movie reviews” without citing any prior work that supports this claim.\n- The approach is also very model specific. For one model some data points may turn out to be not \"difficult\" but for a different model they may, which casts further doubt on the general applicability of this method in this setting. Note Zhao et al use it to identify adversarial examples, which by definition are specific to a model. Furthermore, if we go by the authors' suggestion that evaluation sets must increase FIM, then by using such \"difficult\" examples for evaluation we end up discriminating (by design) against the model that was used to identify them, and unintentionally favor other models, leading to a potentially flawed comparison. I don't think that it is a desirable characteristic of an evaluation set.\n- Particularly given the lack of empirical evidence showing the benefits of \"difficult examples\" identified by this approach, the core claim of the paper is that examples generated by prior work are not \"difficult\" in terms of the discussed metric and perhaps they need to be so. However, the motivation expressed in Kaushik et al. is not to create \"difficult\" examples for training or evaluation, but to learn better models by intervening on causal variables of interest. A good counterfactual example generated by their approach would be one that modifies only sentiment-related variables and leave others intact (intervene on causal variables to d-separate labels from the spuriously correlated variables, see [1] for a detailed explanation), and it has no relation to any particular classifier. I agree that Gardner et al. do discuss their motivation as to construct examples closer to the decision boundary.\n- The writing can be significantly improved.\n\nAdditional comments:\n- It is also not clear to me how replacements are sampled, it would be good to provide details on that.\n- It would be nice to see how well models perform in-sample and out of domain when trained on \"difficult examples\" and how models trained on CAD perform on previously identified \"difficult examples\" in test sets. Furthermore, are there overlaps between such examples if FIM is calculated with respect to a model trained on original data alone vs. one trained on CAD?\n\nMissing references:\n- Please cite the IMDb dataset: \"Maas, Andrew, et al. Learning Word Vectors for Sentiment Analysis. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011.\" \n- Prior work building on Gardner et al. and Kaushik et al. demonstrating the efficacy of their work: \nTeney, D., Abbasnedjad, E., & Hengel, A. V. D. (2020). Learning what makes a difference from counterfactual examples and gradient supervision. arXiv preprint arXiv:2004.09034.\n- There are also a lot of arxiv citations for papers that have been peer reviewed.\n\nErrata:\n- Page 2, line 3: “BERTby” -> BERT by\n- Page 2, second paragraph, line 1: Fisher information metric has already been defined in the previous paragraph so you may refer to it just as FIM over here.\n- Page 3: Glockner et al. should be \\citep and not \\citet. Same for Rajpurkar et al., Recht et al., and Beery et al.\n- Page 3: “learning byMitchell et al. (2018)” -> learning by Mitchell et al. (2018)\n\n\n[1] Kaushik, D., Setlur, A., Hovy, E., & Lipton, Z. C. (2020). Explaining The Efficacy of Counterfactually-Augmented Data. arXiv preprint arXiv:2010.02114.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper proposes an analysis technique for studying the 'difficulty' of a pair of test dataset examples in NLP. The setup proposed by past work on Contrast sets and Counterfactual Examples (Gardner et al, 2020 and Kaushik et al 2020 respectively) is to manually construct two dataset examples (x,y) with different labels y, while the inputs x differ only minimally. This paper argues to compute the measure of a contrast / counterfactual example pair by extracting the largest Eigenvalue of a matrix (defined in part using the Fisher Information Matrix).\n\nStrengths: \n* the idea of using the Fisher Information Matrix to categorize the difficulty of (contrast) examples in NLP seems new to this reviewer and well motivated.\n* The paper argues that many of the existed (hand-crafted) perturbations are not difficult enough for models because they lie far away from the decision boundary, which could possibly lead to insights about how to build better contrast sets later on.\n\nWeaknesses and questions:\n* To this reviewer, it is unclear why this paper leads to a different conclusion than that of Gardner et al, 2020 or Kaushik et al 2020. It seems like those authors were able to mine 'difficult' examples where 'difficult' could be defined as resulting in a flipped prediction -- at least in their experimental setup. It is unclear why if that is the case from the original papers, why the FIM eigenvalue doesn't increase -- but perhaps I'm missing something :) It would make this paper much stronger if the FIM metric was also compared with metrics proposed by those other papers, such as difference in accuracy.\n* How do you define $\\eta$ for BERT-style models? Particularly since the representations that these models learn about what a sentence is change quite a bit during finetuning.\n* Perhaps this is more for future work, but to this reviewer, it is not clear what we learn about the insights of fragility of NLP models from this direction. The methodology would be a lot stronger if we could use this technique to sample examples that are quantifiably difficult for existing models, e.g.\n\n* Nie et al 2019 (Adversarial NLI)\n* Zellers et al 2019, HellaSwag: Can a Machine Really Finish Your Sentence?\n\nOverall: at least to this reviewer, this work seems promising but there are a few open questions about how it works and how insightful it is. I would be happy to raise my score if those questions were answered appropriately.\n\n----\n\nUpdate: thanks for the response! I read over the updated draft also but I'm still not sure what insights we learn about the fragility of NLP models under this evaluation paradigm. For that reason I'm still confused as to whether this paradigm is better or worse than the original approaches of Gardner et al / Kaushik et al 2020, and so I'd like to stick to my score.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}