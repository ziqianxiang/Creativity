{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "The paper proposes a layer-wise or block-wise distillation scheme, Neighbourhood Distillation, that aims to reduce the training time and to improve parallelism when distilling large teacher networks. By breaking down the end-to-end distillation objective into blocks, the proposed method enables faster distillation when applied to model compression and block-wise architecture search. Several concerns of reviewers were addressed during the rebuttal period by the authors.\n\nHowever, there are still some concerns among the reviewers after the discussion:\n\n1) Computational cost-benefit of the proposed KD method seems marginal in comparison to the baseline, given that we still have to train all the neighborhoods in parallel and potentially to fine-tune in the end. \n\n2) It was brought up by a couple of the reviewers that the experiments lack diversity. It would be great to a clearly defined metric applied to a wide variety of the model architectures and datasets. It will also strengthen the paper by providing more details on the experiments.\n\nThe basic idea is interesting, but the paper needs further development and modification for publishing. \n\n"
    },
    "Reviews": [
        {
            "title": "An interesting work, but lacking details ",
            "review": "This paper studies knowledge distillation in the context of parallelly training sub-networks (called neighbourhoods) instead of commonly used end-to-end training paradigm. The authors explore the applications of the proposed neighbourhoods distillation in improving sparse networks,  searching a good student structure given the teacher and knowledge distillation merely using synthetic data. Both CIFAR and ImageNet datasets are considered in the experiments.\n\nThe proposed method is interesting. My main concerns are as follows.\n\n--- Thresholding effect.\n\nThe experimental findings regarding thresholding effect is interesting. In common understanding of CNNs, non-linear activations play a critical role in training. When perturbing a CNN or its  neighbourhoods by artificially introducing some gaussian noise of amplitude epsilon to each activation output, it will break non-negative property. What is the underlying reason why it does not affect final predication once the gaussian noise of amplitude epsilon is small enough?  Why the behavior for injecting noise to network weights is different from noise injection to activations? They are not clear enough. Besides, more details on how to inject noises to a network or its neighbourhoods would be useful.\n\n--- Neighbourhood distillation (ND).\n\nIn the formulation, the authors assume the teacher is pre-trained and available. And the authors claim that ND can be performed in a much faster manner, compared to conventional KD. However, to my understanding, it is not clear enough.\n \na) From Figure 2, I have not seen obvious differences against conventional KD or intermediate feature/attention maps guided KD, if we consider a subnetwork as the student.\n\nb) The authors claim \"These activation maps can be pre-computed and stored before training. This makes each student neighbourhood extremely fast to train, as one does not need to compute activations through a big model in each training step.\" This can also be used to conventional KD or intermediate feature/attention maps guided KD, if we consider a sub-network as the student.\n\nc) The authors claim \"Different student neighbourhoods can be distilled independently from each other and composed into\na final student model\" and then the final model is fine-tuned. How about computational resource cost when training them parallelly? Does the speed-up is also benefited from parallel training?\n\nd) In the equations such (1) and (3), many notations or terms are not clarified.\n\nI am confused by those points, which makes me think the presentation and applications of ND are not convincing.\n\n--- Experiments\n\nIn the experiments, the authors assume the student and teacher networks have strong relations, multiplier k. However, convention KD and its variants do not have any structural constraints between the student and teacher networks. That is, they usually have different structures, such as DenseNets vs. ResNets.\n\nKD methods are not limited to two-stage training, as many KD variants enable on the fly training of teacher/peer together with the student. Is it possible to apply ND to such kind of training scenarios?\n\nRegarding speed-up results over KD, it lacks details how they are implemented/measured. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "I vote for rejection at this moment. This paper proposes a new knowledge distillation method called Neighbourhood Distillation to speed up knowledge distillation process. Compare to conventional KD approaches, this paper can achieve 3-4 times speed and obtain similar performance. Experiment on several benchmarks shows the proposed method can be beneficial in several application.\n Overall, I think the idea to speed up the process of knowledge distillation is a direction that could be discussed in the future,  while the training cost of current KD in experiments in this paper is almost affordable and the speedup is not very impressive. Besides, several concerns need to be addressed.\n\n####Pros:\n1) The observation of  Thresholding effect is somewhat novel to me.\n2) The methodology part is clear and easy to understand.\n3) This paper has investigated the ND in several tasks, including NAS, data-free KD,  and this is worth to mention. \n\n#### Cons:\n1)  For me, the connection in Thresholding Effect and NEIGHBOURHOOD DISTILLATION is a little weak. I noticed the author claim this motivated to use NEIGHBOURHOOD DISTILLATION: each student neighbourhood is trained to match its output of teacher. Such an idea is already used similarly way in Hint-Training and the following work.\n2)  The goal of this paper is to speed up the KD process, however existing cost of KD experiments in the paper is affordable. And the speed up is not very impressive.\n3)   The paper should add discussion some recently KD methods, especially matching intermediate features between student and teachers.\n4)  The results of Table 1 should be run several times and report the average mean.\n5)  The GPU time (H) is not a very objective index, could you provide the flops of each method?\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new training schema for KD with various applications, but inadequate experiments",
            "review": "This paper introduces Neighbourhood Distillation (ND), a new training pipeline for knowledge distillation (KD), which splits the student network into smaller neighbourhoods and trains them independently. The authors breaks away from the end-to-end paradigm in previous KD methods and provides empirical evidence to reveal feasibility and effectiveness of ND. Specially, ND can: 1) speed up convergence, 2) reuse in neural architecture search and 3) adapt to the synthetic data.\n\nStrengths\n1) The paper is well written and easy to follow. An empirical evidence of the thresholding effect is provided to explain the motivation.\n2) The idea is simple and intuitive. The ND seems more like an initialization method for DNN’s local components and a finetuning procedure is sometimes needed for recovering the accuracies. Benefit from parallelism and small training components, such training schema can speed up the convergence of standard KD.\n3) Several different applications are conducted to demonstrate the flexibility of ND.\n\nWeaknesses\n1) Missing a relevant paper. [1] proposes a similar blockwise knowledge distillation method. The authors should cite and explain the differences between ND and [1].\n2) What is sparsification in Sec. 4? Is it the sparseness of the convolutional kernel or the channel? \n3) The authors mention that the work seeks to overcome the limitations of training very deep networks. However, the ResNet50 (the deepest model in experiments) is not deep enough. Usually, it is easy to converge.  \n4) In Sec. 5, only the width search experiments are conducted, which is more like layer-wise or block-wise pruning. However, architecture search is a general method that can not only search the widths but also the operations. Why only mentions “This set could contain variants of the same architecture...”? Is there any limitation when the searched candidates contain different architectures/operations? \n5) All the experiments are done on ResNet series. Different teacher and/or student architectures, such as VGG, ShuffleNet etc., should be considered.\n6) Does the observation of thresholding effect benefit from the shortcut in Resblok？Is it suitable for plain CNN, such as VGG? Which blocks are chosen in Fig. 1(a)? Does the shallow and deep blocks have the same phenomenon when perturb small number of blocks?\n7) How to record the GPU time of ND? Is it the time of paralleling on multi-GPUS or on single GPU?\n\nI am currently leaning towards a slightly negative score but would like to see the authors' responses and other reviewer's comments.\n\n\n[1] Hui Wang, Hanbin Zhao, Xi Li, Xu Tan. Progressive Blockwise Knowledge Distillation for Neural Network Acceleration. IJCAI, 2018.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}