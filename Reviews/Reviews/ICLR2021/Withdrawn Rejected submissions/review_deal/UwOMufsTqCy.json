{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This paper falls in the borderline area and there are still some concerns (for instance by AnonReviewer5 and AnonReviewer2) that deserve further treatment. Given that most ideas can only be validated in experiments (as the results are not theoretical), some points that remain are the comparison with other approaches (there are reasonable comparisons, but there are very famous contenders missing such as xgboost, ok that LightGBM is, but why not the other?), details about the tuning, the significance of results (practical and statistical is not complete/detailed enough), and the reasoning about situations with many rules and interpretability seems to be worth exploring/discussing further."
    },
    "Reviews": [
        {
            "title": "Concerns with interpretability of the resulting model.",
            "review": "## Summary\nThe authors propose a new approach for training interpretable discrete models via gradient descent. They claim three contributions: 1) incorporation of layers implementing logical conjunction and disjunction operations; 2) the gradient grafting technique for performing gradient descent on discrete structures; and 3) a logical activation function that enables models to scale to larger training datasets. Additionally, they also claim competitive performance with comparable learning methods.\n\n## Strengths\n* The paper is well organised and easy to follow.\n* The proposed gradient grafting technique, while heuristic in nature, is novel and seems to work quite well in practice.\n* The proposed method achieves good accuracy compared to the other approaches considered in the experiments.\n\n## Weaknesses\n* There is limited novelty in the proposed model. The continuous/discrete conjunction and disjunction operations used in the logical layers are the same as those used in Wang et al. (2020), however they are arranged differently in this method.\n* The connection betweent the continuous version of the model and the discrete version is weak. One of the claimed contributions is that the discrete model is optimised directly, but this does not seem to be the case in practice---the continuous version of the model is optimised. It is also unclear how the continuous-valued weights are binarised in order to work with the discrete version of the model.\n* The results in Figure 2 indicate that typically a large number of rules are learned, yielding models that are unlikely to be interpreted reliably. The log scale on the x axis of the plots is somewhat deceptive---I find it hard to believe that models with between 64--256 terms are interpretable. I think further investigating what tradeoff exists between accuracy and interpretability is necessary before the paper can be accepted.\n\n## Other comments/questions\n* Comparing average accuracy is generally not a good idea. It is more informative to compare the average rank of each method across datasets [1].\n* Can the authors elaborate on how hyperparameters for each of the competing methods are chosen?\n* The computational graph for gradient grafting seems related (yet distinct) to the Gumbel softmax estimator---perhaps the authors could discuss this relationship further?\n* The authors could do more to discuss related work on training decision trees and rules using gradient-based optimisation. E.g., XGBoost [2], DNDT [3], and SGT [4].\n* Given that the model is trained with gradient descent, and arbitrary loss functions, could the model be evaluated on tasks other than classification?\n\n## Update\nI thank the authors for some of the updates and response that address my concerns with clarity, but my main concern related to the interpretability aspect has not been resolved. The authors suggest that one can discard some of the rules learnt by the algorithm if the model becomes too large, using the weights in the linear layer to determine which rules should be kept. This seems like a good direction to explore, but it is unclear if a few rules typically dominate the predictions, or whether one will sacrifice significant accuracy in order to gain interpretability. As stated in my original review, I would have liked to see analysis of the *tradeoff* between interpretability and accuracy of this method.\n\nA few other things:\n* I still do not understand exactly how the hyperparameters were tuned. The authors have provided a list of those that were tuned, but I still don't understand the process used for tuning them.\n* Some of the responses to other reviews have reinforced some of my concerns. In particular, it seems the distribution of weights attached to rules is not optimal for the proposed (but unevaluated) rule pruning method. Also, the authors conflate statistical significance with practical significance when replying to Reviewer 4.\n\n[1] Dem≈°ar, Janez. \"Statistical comparisons of classifiers over multiple data sets.\" Journal of Machine learning research (2006).\n[2] Chen, Tianqi, and Carlos Guestrin. \"Xgboost: A scalable tree boosting system.\" In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016).\n[3] Yang, Yongxin, Irene Garcia Morillo, and Timothy M. Hospedales. \"Deep neural decision trees.\" arXiv preprint arXiv:1806.06988 (2018).\n[4] Gouk, Henry, Bernhard Pfahringer, and Eibe Frank. \"Stochastic Gradient Trees.\" In Asian Conference on Machine Learning (2019).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, accept",
            "review": "Summary:\n\nThis paper presents a new rule based  classifier called Rule based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation.  In order to train this model efficiently  the paper presents a learning algorithm that projects the discrete RRL model to a continuous space and hence optimise the model using gradient descent. Through experiments on 9 small and 4 large datasets shows that RRL improves over other methods, has low complexity and interpretable. \n\nReasons for score:\n\nI think this is a Good paper and vote for accepting.  This paper presents a solid contribution in learning rule based classifiers and hence to interpretable machine learning.  The paper is clearly written and superiority of the presented models are backed by strong experimental results.\n\nPros:\n\n1. The paper is clearly written and easy to follow.\n2.  The paper addressees an important problem of interpretable machine learning and presents a novel model which is scalable.\nI find the technique of gradient grafting particularly interesting.\n3.  Very strong experimental results and ablation studies.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good idea, but not enough impact.",
            "review": "Authors propose a new scalable classifier, named Rule-based Representation Learner (RRL), that can automatically learn interpretable rules for data representation and classification. For the particularity of RRL, the authors propose a new gradient-based discrete model training method, i.e., Gradient Grafting, that directly optimizes the discrete model. Authors also propose an improved design of logical activation functions to increase the scalability of RRL and make RRL capable of discretizing the continuous features end-to-end. The experimental results show good performance of RRL both high classification performance and low model complexity on data sets with different scales.\n\nHowever, it is questionable whether the difference in accuracy between decision trees such as C4.5 and RRL in the experimental results is crucial. If the difference in accuracy is as small as this, the ease of interpretation of the decision tree may be superior to that of the RRL. Alternatively, the choice of experimental data may not be appropriate to demonstrate the superior performance of RRL.\n\nI also didn't understand why the fuzzy/soft rule sacrifices the model interpretability. Could it be that the fuzzy representation is intended to be closer to human subjectivity, and thus aid in intuitive understanding?\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Multi-layer rule learner with missing discussions on scalability, interpretability and implementation details",
            "review": "The authors propose a classifier consisting of multiple layers. The inner layers construct rules in conjunctive normal form. The last layer is used to assign weights to the constructed rules. To train the overall model and obtain discrete solutions, the authors use a simple rounding mechanism leading to a method that they call gradient grafting. The paper concludes with a computational study, where the authors report the classification performance as well as the model complexity on a set of problems. \n\nThis is an interesting paper proposing a new method to tackle the trade-off between interpretability and accuracy. Indeed, rule-based learners are considered to be more interpretable. The authors also claim that the proposed rule-learner is also scalable. Overall, introduction of a scalable, interpretable and accurate method could have been considered as a big achievement. However, I have several questions and comments about this achievement as I list below:\n\n- Are the results given for test set? If so, what is the train-test percentage?\n\n- What are the computation times? Without these figures, it is hard to assess the scalability of the proposed method.\n\n- The numbers of edges that you report range from 50 to 1000. These values seem quite large. How does this affect the interpretability?\n\n- In Figure 4, only five clear rules are reported. What is the distribution of the weights of the remaining rules?\n\n-  How do you decide various design parameters; such as, number of layers (n_l), number of bins for feature discretization (k), number of layers (L)?\n\n- As you need a binarization layer to divide the continuous features into bins, can't we just say that the method works only with discrete features? This is how it would be presented by other rule-learning methods.\n\n- I guess Section 3.2 is superficial as it is straightforward to split a continuous feature into bins. Am I missing something here?\n\n- How do your results compare against the results obtained with other rule/tree learning methods based on (integer) linear optimization? Some names from that field are Bertsimas,  Rudin, Gunluk.\n\n- Can you guarantee that the resulting set of rules covers the entire feature space? In other words, is it possible that a test sample is not classified with the output set of rules?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}