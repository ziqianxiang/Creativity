{
    "Decision": "",
    "Reviews": [
        {
            "title": "Learn to match graphs by developing a new deep learning technique to capture the so-call node-graph interaction in addition to node-node interaction between two graphs.",
            "review": "Summary:\nThis work proposes a multi-level graph matching network (MGMN) for graph-graph classification/regression problems. MGMN tries to capture both node-graph interactions and node-node interaction. The authors created and collected a set of benchmark datasets for both graph-graph classification and regression tasks with different sizes that can be used to evaluate the robustness of models. Experiment results are provided to compare MGMN and state-of-the-art baselines on the benchmark datasets.\n\n\nPros:\nThe idea of learning node-graph interactions is interesting. \n\nCons: Need to improve writings. No 100% sure if the proposed approach performed better in experiments, and for what reasons.\n(a)\tFigure 1 is not self-explained and is hard to understand from its immediate context. It is better to move it to section 3 where symbols are explained. It can help readers if the authors can marked NGMN and SGMN components.\n(b)\tAggregation layer: In what order are the node embeddings fed into BiLSTM? Is order critical?\n(c)\tWhat results (training, valiation, or test) are reported in the tables 2-6. Make tables more self-explained in their captions.\n(d)\tWhat results (training, valiation, or test) are reported in Appendixes 3 &4.\n(e)\tThe proposed multi-perspective is a simplified version of multi-head attention. Why multi-perspective worked better than multi-head in experiments? Some discussions will help readers.\n\nMinor:\n* Is the operator between x_1 and w_k in eq. 4 an inner product operator?\n* “Node-graph matching layer” may need a better name as it is not clear what are being matched.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper can deliver good performance but with limited novelty and insight.",
            "review": "This paper is focused on the topic of learning a similarity for graph comparison. The paper is in general well written and easy to follow. The performance of the proposed framework seems convincing and moderately impressive. Moreover, the authors collected a new dataset that brings help to the structural metric learning community.\n\nHowever, I still have several main concerns about this paper as follows:\n\n1) In general, the novelty and the contribution of this paper is limited. Updating the node-level feature taking into account the features from another graph in an attentive fashion is not a novel idea and has been applied in several graph matching methods (e.g. [1-3]). While the authors claimed that the cross-graph feature update is the main contribution of the paper, this fact greatly limits its novelty. \n\n2) Continuing from concern 1), I don't think the proposed mechanism really considers the relation between a set of one graph and another whole graph by simply focusing on node-wise attention. This claim is too strong.\n\n3) In the ablation study, the authors compared their Hadamard product with matrix multiplication and showed the  performance difference. Though the authors suspect this fact may be caused by overfitting, they didn't verify. It seems verification of this hypothesis is pretty easy. Without this part, I cannot infer why Hadamard product is more suitable for graph similarity task. The authors should more insights rather than giving a suspect which is easy to verify.\n\n4) The algorithmic design of BiLSTM is somewhat ad-hoc. If the graph is big (with many nodes), is BiLSTM in this case sensitive to the order of the nodes? Or in the training phase, the previous modules feeds BiLSTM with diverse ordering? This is still unclear and can be essential to the final performance.\n\nIn general, I don't think the paper has enough contribution and sufficient investigation to the inside of the mechanism. Therefore I vote a reject.\n\n[1] Aberman, Kfir, et al. \"Neural best-buddies: Sparse cross-domain correspondence.\" ACM Transactions on Graphics (TOG) 37.4 (2018): 1-14.\n[2] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. \"Learning combinatorial embedding networks for deep graph matching.\" ICCV. 2019.\n[3] Yu, Tianshu, et al. \"Learning deep graph matching with channel-independent embedding and Hungarian attention.\" ICLR. 2020.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Graph matching networks for graph similarity learning",
            "review": "The paper proposes a graph neural networks structure for graph similarity learning. Basically, the authors claim it as a multi-level model which consists of NGMN (i.e., node-graph interactions) and SGMN (graph-level). The key component NGMN is done by using attention mechanism to adaptively aggregate the node embeddings from the other graph. The experiments can show the whole model (i.e., MGMN) outperforms the baseline methods.\n\nPros:\n1. The idea of the proposed model is simple yet seems to be effective. The paper in general is introduced clearly.\n2. The ablation studies are solid to understand the model.\n\nCons:\n1. The major problem of the paper is the claims in NGMN. First, for Eq. 3, the authors claim Eq. 3 computes the attentive graph-level embeddings of one graph from the view of a node in the other graph. I totally disagree with this claim. What the authors describe is exactly same as \"computes the attentive node embeddings in the other graph\", which hence means that the layer of NGMN is not node-graph matching layer, but rather a cross-graph node-node alignment layer. From this perspective, the whole proposed model is not a multi-level model.\n2. The paper also mentions in Introduction that the size of input graphs matters, but the proposed model looks not handling that challenge. The authors only split the dataset by sizes of graphs in the experiments. \n3. The proposed model naturally encounters high complexity since it computes the attention scores for all pairs of nodes.\n4. The paper keeps cosine similarity measure in the model, but what about other choices? The authors need more ablation studies to see how it performs when using other measure (e.g., inner product).\n5. Table 2 needs to report the performance of SGNN with BiLSTM since Table 8 demonstrates that SGNN w/ BiLSTM performs much better than using Max aggregator (used in Table 2). In addition, the combination of SGNN and NGMN seems to only achieve very marginal improvement.\n6. One minor issue is that the paper claims it as a multi-level, but it at most considers only two levels (i.e., node and graph). So I barely agree with the statement of multi-level as multi-level usually considers cluster level or subgraph levels.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The proposed framework largely follows existing Siamese Graph Matching Network Structures",
            "review": "This paper proposed a siamese network architecture to evaluate the similarities between graph pairs. The main novelty claimed is the so-called cross-level similarity that is defined between nodes (in one graph) and the  graph (the second). Besides,  authors have have created and a set of datasets for both graph-graph classification and regression tasks, with an emphasis of the size of the graph in impacting the matching performance. \n\nA major concern is that the proposed method shares some important similarity with the ICML 2019 paper “Graph Matching Networks for Learning the Similarity of Graph Structured Objects” by Li et al.. In that paper, Siamese network structure was proposed and each node in one graph will be compared against all the nodes in the other graph through an attention-based module. The current work follows basically the same idea with slight modifications (detailed form of attention, aggregator through Bi-LSTM), which should not be considered sufficiently novel contributions anymore. Another modification is to compute the similarity vector between one node in the first graph with the aggregated node embedding in the second graph. However, the definition of the similarity function f_m() is unclear.\n\nAnother concern is the claim of multi-level graph matching. Typically multi-level refers at least 3 distinct levels or scales. However, I do not see a third or median level besides the finest, node level operation and the coarsest, graph level operations (and the latter is simply a weighted summation of the former).  The proposed method is very far from a standard hierarchical architecture as one might typically expect for a multi-level approach, and so I question the use of multi-level in the title and throughout the paper.   \n\nSome detailed comment\n\n(1)\tIn figure(4), the f_m function is used to compute the similarity between two vectors. No explicit form of the function is given. It’s unclear how the output becomes a similarity vector (not a score) given two input vectors. In addition, what is meant by a similarity vector?\n\n(2)\tIn equation (6), the similarity vectors are aggregated by LSTM. This seems difficult to interpret. What does it mean to aggregate the similarities of each node in the graph (against the 2nd graph)? Using the similarity vector as features appears somewhat problematic to me, especially when the vector is not clearly defined.\n\n(3)\tIf one exchange the order of the two graphs in the graph pair, will the estimated similarity score be invariant? This may question the choice of the similarity vector defined in (6) as well if it is not symmetric. \n\n(4)\tIn case of graph-graph classification, can the labels of the (G1, G2) pair be obtained by 1(label(G1) == label(G2)) where label(G) is the graph labels in a standard graph classification task?  \n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novelty is limited.",
            "review": "Summary:\n\nThe author proposed a new framework to compare the simiarity between different graphs. The proposed framework consists of different components and the components are mainly from existing technolies. The performance looks good but there are some issuses to be solved.\n\nPros:\n\nThe authors proposed a new framework for graph similarity computing. The proposed method contains serval key steps. First graph convolution layers are used to learn the embedding of each node. Then the node embeddings in one graph are used to match another graph. And the matching output will be fed into an aggregation layer and predict the similarity of two graphs. The whole pipeline is presented in  detailed and easy to undertstand.\n\nCons:\n\n1. Although the proposed method is clear and employed many advanced techonology, the are from existing work. There is no strong motivation about why we need to organize different components in this way. It would be better to provide more motivation about the proposed method.\n\n1. The authors proposed to employ BiLSTM layers to aggregate features. When using BiLSTM, different orders can affect final results. Which order did authors use in the papers? How did different orders affect the final results? It can be seen from the results that in most case BiLSTM cannot outperform FCMax. Can you provide more analysis about this?\n\n2. Although the authors provide computation complexity and claim that it is comparable with previous work. It still time-consuming. The graph that used in the experiments have limited nodes. It is worth to discuss the performance and computation time on large graphs.\n\nOverall: Although a new framework is proposed to compute graph similarity and it can outperform STOA. I vote to reject considering the limited novelty.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}