{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "This work proposes a method for identifying appropriate graphical models through enumeration, pruning of redundant dependencies, and neural network conditionals. While structure learning is an interesting application and there are some promising results, there were a number of concerns around experimental evaluation, computational complexity of the method, clarity of the presentation, and connections to prior work. In particular, R1's concerns around the large field of structure learning in Bayesian Networks, and unwillingness to use the established terminology (and comparing to methods there) was not sufficiently addressed in the rebuttal."
    },
    "Reviews": [
        {
            "title": "Enumerating all possible graphs can achieve better performance on EEG datasets, but with potential issues on scalability.",
            "review": "**Update**\nI have read the author's rebuttal, and happy to see that a discussion regarding parameters is added (Figure 9). Other than that, my personal concern is similar to Anon Review 3's -- it seems that the core idea of the paper is drowned in too many technical details (granted, many of these are needed in order to implement this correctly). I wonder if a clearer discussion can be made like this -- you have a variational inference problem with certain independence assumptions, so write this out in the most abstract manner possible. To come up with a concrete objective, the question then becomes \"given the factor graph, how do we add the networks (this basically lines 11 - 16 in Algorithm but not in the flow of the main text)\". I think a better presentation and clarity in the main paper would greatly help acceptance.\n\n**Overview**\nThe paper proposes AutoBayes, which enumerates all the plausible graphical models between data, label, and nuisance variables, remove redundant edges with d-separation rules, and learns neural networks to represent the parent-to-child information.\n\n**Strengths**\nEmpirical results are very strong for certain EEG datasets such as ErrP and RSVP. It appears that on the EEG datasets, different graph structures would have very different performances so there might not be a graph structure that works equally well on all of them, which motivates the need for searching for the optimal graph structure.\n\n**Weaknesses**\n\nScalability: unlike existing methods in AutoML, AutoBayes does not seem to attempt to optimize the process from which graphs are selected (i.e. pruning of graphs that are unlikely to work well), resulting in the need to enumerate all possible graphs (where the complexity is doubly-exponential with respect to the number of variables). This means that the method will have difficulty even scaling to a small amount of variables (e.g. 10).\n\nInterpretation of the learned structures: it seems that on some datasets, CVAE already provides comparable performance to the best AutoBayes architecture, and on others best AutoBayes architecture perform much better. Can we extract any insight from the AutoBayes procedure?\n\nFairness of experiments: for each component of the graph, the network structure is the same; therefore, compared to a structure X->Z->Y, we have fewer parameters if we use X->Y. It is unclear as to whether the empirical improvement can be gained simply by representing edges with larger networks.\n\nEmpirical comparison: it appears that model ensembles have a significant effect over the performance. However, since one can also ensemble models from different initializations but the same architecture, it is unclear what the performance gap would become if we also do ensemble on one model but with different parameters.\n\n**Minor suggestion**\nPerhaps spend some text describing what is special about EEG datasets, and why do we expect having to predict nuisance variables to improve performance.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting idea but further motivations and experimental discussions are needed.",
            "review": "Summary: \nThe paper presents AutoBayes: a new approach for nuisance-robust deep learning which explores different Bayesian graph models to search for the best inference strategy. It automatically builds connections between classifier, encoder, decoder, nuisance estimator and adversary DNN blocks. The approach also enables disentangling the learned representations in terms of nuisance variation and task labels. Different benchmark datasets have been used for evaluation.\n \n##################################################################\n\nStrenghts:\n- The idea of automatically exploring various graphical models to select the best performing one is interesting.  \n- Overall, the paper is well written and easy to read.\n- Ensemble learning is performed by stacking the explored graphical models allowing to improve performance.\n\n##################################################################\n\nWeaknesses:\n- My main concern about the paper is that even though quantitative evaluation has been performed on several datasets with different modalities to show the benefit of using AutoBayes, the experimental evaluation section is not convincing enough as it lacks interpretation and analysis of the obtained results. For instance, one major problem addressed in this paper is models robustness to nuisance factors, however this was not discussed in this section. Hence, it would be good to include an experimental evaluation on this point. \n- In Table 4 in Appendix, the simple Model B which assumes independence between Z and S performs remarkably well on task classification of the 5 first datasets since it outperforms state-of-the-art methods on all of them except QMNIST and achieves classification accuracies that are very close to the best ones (with either variational or non-variational inference), outperforming most of the presented graphical models. On the remaining datasets, Model A which is independent of S and Z, performs well compared to other graphical models especially on Faces Basics and Faces Noisy datasets. Considering these observations and perhaps the time consumption of AutoBayes, what would be the motivation of exploring all the presented graphical models (besides stacking them for ensemble learning) rather than using the simplest models A or B with good hyperparameter search? Could these results be explained by the fact that potentially some of these datasets do not present high nuisance variabilities?\n- One of the main contributions as presented in Section 2 is the extensibility of the proposed framework to multiple latent representations and nuisance factors. Although this was demonstrated theoretically, it would be interesting to demonstrate this experimentally.\n- One of the properties of AutoBayes is its ability to learn disentangled representations in terms of nuisance factors. In practice, how can this be evaluated?\n\nMinor comments: \n- Typo in Equation 6: xˆ  = p_μ(z1, z2) instead of xˆ  = p_μ(z1)?\n- Steps to derive Equation 7 are not straightforward and can be more clarified.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Very interesting idea and results, but relevant previous work regarding Bayesian networks and structure learning is not discussed at all.",
            "review": "The authors propose a framework, called AutoBayes, to automatically\ndetect the conditional relationship between data features (X), task\nlabels (Y), nuisance variation labels (S), and potential latent\nvariables (Z) in DNN architectures.  Assuming a Bayesian network (BN)\nwhich represents the (possibly) conditional independencies between the\naforementioned variables, the authors propose a learning algorithm\nwhich consists of applying Bayes-ball to detect and prune unnecessary\nedges in the graph (effectively finding a subgraph, independence map\nof the BN), train the resulting DNN architecture, and choose the network\nwhich achieves the highest validation performance.  This idea is\ninteresting, especially compared to hyperparameter optimization\napproaches for model tuning, and the results seem convincing.\n\nHowever, relevant previous work is not cited and discussed in the paper.\nSpecifically, BN structure learning and inference in BNs (both of\nwhich are well studied and have extensive literature) are fully\nrelevant, but are not discussed or mentioned at all.  For instance,\nthe paper uses undefined terms such as \"Bayesian graph model,\" \"Bayesian\ngraphs,\" and \"graph model,\" in place of Bayesian network (which is\nrigorously defined).  It is important that such related previous work\nbe discussed to delineate what is novel in the presented approach and\nplace its contributions within the greater context of this previous\nwork.  This inclusion would also help the presentation of concepts in\nthe paper.  For instance, the discussion surrounding equations 1\nand 2, i.e.:\n\"The chain rule can yield the following factorization for a generative\nmodel from Y to X (note that at most 4! factorization orders exist\nincluding useless ones such as reverse direction from X to Y )...,\" is\nthe concept of an elimination ordering in the elimination\nalgorithm for BNs (and graphical models in general).  Showcasing the\npresented work in this light, (i.e., as a natural\ncombination of BN structure learning with macro-level neural-architecture\noptimization) would be particularly novel and compelling.\n\nFinally, it is important to discuss the complexity of the presented\nalgorithm.  Given the Bayesian networks (BNs) in Algorithm 1,  each\nindependence map (and the underlying DNN\narchitecture) must be trained then validated.  This algorithm scales\nfactorially in the number of nodes in the BN.  It is great that the\nselected subgraph performs so well (Figure 2), but super-exponential\ncomplexity multiplied by DNN cross-validation training is going to be\nvery hard to do as m and n grow in Algorithm 1.\n\nOther comments:\n\n-The definition of nuisance and nuisance-variables are implicitly\nassumed throughout the paper.  An exact definition of what is meant by\nnuisance, in the context, of the paper would be very helpful.\n\n-Algorithm 1 is mostly one large block of text, and is very hard to\nparse on the first read.\n\n-In the main contributions enumerated from pages 2-3, contribution 3\nlooks redundant given contribution 1.\n\n-\"Besides fully-supervised training, AutoBayes can automatically build some relevant graphi-\ncal models suited for semi-supervised learning.\" <- please include a\nlink to where this is discussed.  The enumerated list of contributions\nwould be a perfect roadmap for the paper (just include references to\nsections after every contribution)\n\n-\"We note that this paper relates to some existing literature... as addressed in Appendix A.1. Nonetheless, AutoBayes\nis a novel framework that diverges from AutoML, which is mostly employed to architecture tuning\nat a micro level.  Our work focuses on exploring neural architectures at a macro level, which is not\nan arbitrary diversion, but a necessary interlude.\" <- Appendix A.1\nshould really be included in the\npaper, related work is a not an optional section.  For instance, the\nreader may not know what the authors mean in terms of micro versus\nmacro level.  Reading this without further explanation until later\nin the paper, it would\nseem that micro-level is more nuanced than macro-level and the former\nperhaps subsumes the latter; the authors should detail what they mean\nand distinguish their work from previous works in the main paper.\n\n-The terminology is very clumsy: \"Bayesian graph models,\" \"Bayesian\ngraph,\" and \"graph model\" are not established terms and, as such,\nshould be defined so the reader knows what type of ML method is being discussed.  The authors should specify that they have a Bayesian\nnetwork whose factorization describes the conditional relationship\nbetween (random) variables.\n\n-\"VAE Evidence Lower Bound (ELBO) concept\" <- please include citation\nfor the ELBO\n\n-\"How to handle the exponentially growing search space of possible\nBayesian graphs along with the number of random variables remains a\nchallenging future work.\" <- this is exactly structure learning in\nBayesian networks (see the Bayesian information criterion, i.e., BIC score).",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, encouraging results,  but poorly organized paper",
            "review": "The authors present a novel method dubbed AutoBayes that tries to find optimal Bayesian graph models for \"nuisance-robust\" deep learning. They employ the Bayes-Ball algorithm to construct reasonable inference graphs from a generative model given by iterative search. The corresponding DNN modules are then built/linked and trained using a form of variational inference with adversarial regularization where applicable. The authors also propose the use of an ensembling approach to further improve robustness of the \"best\" model.\n\nPros:\n- Interesting and novel approach\n- Experimental results are encouraging and seem to demonstrate a clear advantage of AutoBayes over other methods\n\nCons:\n- The organization of the paper is erratic; overall structure doesn't flow well.\n- Multiple figures have details (e.g. color or style of arrows, shading, etc.) which are never explained.\n- Little to no context is given for what nuisance-robust learning (and correspondingly, nuisance \"variation\" variables) actually is. Similarly for the datasets and baselines.\n- No discussion of time/space complexity or computational demands, which is surprising given the apparent combinatorial complexity of the nested for-loops in Algorithm 1.\n- Placement with respect to existing work is somewhat vague (the authors refer to \"similarities\" and \"relationships\" but do not concretely describe them).\n- The paper needs to be proof read several more times. There are numerous grammatical errors and poorly phrased sentences. I will give a few examples below, but this is largely up to the authors to sort out.\n\nOverall, the paper reads more like a rough draft of a technical report than a standalone research article. It's confusing, difficult to read (I found that I needed to jump around and re-read a lot in order to understand what the authors were trying to say), and fails to give necessary context to readers who aren't familiar with a very specific subset of the deep learning literature. While it's of course always fine to defer to references for details, it's important that the reader can broadly understand your method and the problem it's trying to solve without hunting down a dozen references. This paper seems to assume that the reader has read every relevant paper and is arriving at this work in sequence.\n\nIf the authors are willing to do some reorganizing and more diligent proof-reading, I'd be happy to reconsider my rating after seeing the revisions.\n\nA couple of examples of poorly phrased or grammatically incorrect sentences:\n\n\"It may be because the probabilistic ~relation~ *relationships* *in the* underlying data ~varies~ vary ~over~ across datasets\" (pg 2)\n\n\"The ~whole~ DNN blocks are trained with ~adversary~ *adversarial* learning ~in a~ *using* variational Bayesian inference.\" (pg.3)",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}