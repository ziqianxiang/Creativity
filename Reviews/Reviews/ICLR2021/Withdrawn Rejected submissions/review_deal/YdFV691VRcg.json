{
    "Decision": "",
    "Reviews": [
        {
            "title": "Mixing well-established ideas with little reasoning on the choice of setup",
            "review": "The authors propose to solve classification problems by\n* using nearest neighbor distances as features to the softmax.\n* augment the loss function with an additional similarity loss on example pairs induced from class-labels.\n\nUsing the above the authors show a) robustness to adverserial/domain-shifted examples, b) retrieval and verification capabilities.\n\nClarity\n* I thank the authors for using understandable simple notations and making the key points of the work very clear.\n\nOriginality\n* The various ingredients proposed by the authors have been studied before, e.g. see [1] (and their references). Augmenting the feature space by using neighbors is the key idea in any instance-based learning approaches (e.g. Local linear regression) and I wasn't convinced that can count as a key contribution of this work.\n\n* Jointly training classification and similarity function has also been proposed before (e.g. see [2])\n\n* I agree that I couldn't find any work that explicitly combined both, though I'm viewing this as limited novelty.\n\nQuality\n* While I liked the clear presentation, the authors could focus more on why some of the choices were relevant and how it affected the experiments. For example,\n -- Why is a new class prototype represented only the average of the neighbors? It's not the case that existing class prototypes have that property, i.e. it's not the optimal solution of the algorithm minimizing the loss function.\n -- Have the authors tried just using the encoder learnt for classification directly for retrieval? \n -- Does the choice of similarity function matter?\n \n* It would've been nice to understand what aspects contributed to better metrics (be it retrieval/classification/verification etc).\n\nSignificance\n* Given the above comments, I'm not convinced that is of sufficient significance for acceptance.\n\n\nI'm reasonably confident in my review. I am, however, not familiar whether the datasets used are widely accepted benchmark datasets for these tasks.\n\n[1] Combining instance-based learning and logistic regression for multilabel classification\n[2] Distance Metric Learning with Joint Representation Diversification",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This paper proposes a general model to simultaneously deal with multi-class classification (with robustness against adversarial attacks and domain shifts), verification, image retrieval and few-shot classification. The model consists of an encoder to convert an image to embedding, a similarity module to compute similarity scores between embeddings, and a set of class prototypes. With a common training procedure with class labels, the model can be adopted in different tasks mentioned above. ",
            "review": "Pros:\n+ It is an interesting idea to consider a general model that is capable of handling multiple types of tasks. \n+ Experiments show its performance on many types of tasks against non-general purpose models.\n\nConcerns:\n- However, the tasks chosen by this paper are highly correlated, and it seems straightforward to combine them. Furthermore, the paper does not point out the underlying challenges, which prevent us from using naive solutions, and the relations between the challenges and the proposed solution. For example, verification and image retrieval both usually contain a procedure to compare image pairs. Once we have a deep metric learning model to encode and compare two instances, like images or audio clips, we can perform both verification and retrieval using a single model. Also, class prototypes can also be constructed by averaging encoded image embeddings. Once we have class prototypes, we can perform both multi-class and few-shot classification. Therefore, instead of simply claiming that most deep metric learning methods are only for image retrieval and presuming that they will not work on other tasks, I think they should be treated as baselines for comparisons with the proposed method. Also, Gulrajani & Lopez-Paz (2020) is regarded as a general purpose method in Table 2. Thus, it should also be compared in other tasks to show the effectiveness of the proposed solution as a general model. \n- For the model design, the proposed method is related to (with partially idea-overlapping) to proxy-based metric learning methods, like SoftTriple [1] and Proxy-NCA [2], that utilize class embeddings to attract similar images and push away dissimilar images. However, this paper does not cite or compare these related papers. \n- For the experimental results, the proposed method achieves the most outstanding performance on multi-classification with domain-shifts. However, the improvements are not significant in other tasks, especially in image retrieval and multi-class classification with adversarial attacks. \n[1] 2019 ICCV, Qian et al., “SoftTriple Loss: Deep Metric Learning Without Triplet Sampling.”\n[2] 2017 ICCV, Movshovitz-Attias et al., “No Fuss Distance Metric Learning using Proxies.” ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The proposed method combines different techniques together trying to provide a general-purpose solution to multiple problems but fails to provide convincing quantitative results",
            "review": "This work tries to tackle the following question: if there is a multi-class classification approach which is more robust again adversaries and distribution shifts while supporting adaptation to novel classes. The solution proposed by the authors extends the popular prototypical network for few-shot learning with three modifications: 1) the proposed method uses a learned similarity function instead of a fixed distance function (squared Euclidean distance), 2) the incorporation of the triplet ranking loss, and 3) the vanilla fully-connected layer on top of the learned representations. The authors advocate the proposed method as a general solution to not only different problems including classification, verification and retrieval but also offering improved robustness against adversaries and distribution shifts. Although few-shot learning and inclusion of new classes was also listed as one of the contributions, but this can already be fulfilled by the original prototypical network.\n\nIn terms of originality and significance, this work is incremental. The extension of the prototypical network to multi-class classification problems has been considered in the literature (for example [2]) and a different way of learning the ``\"prototypes\" is also considered ([4,5,6] for example) where they are learned as parameters instead of class centroids. Importantly, they have shown that this method can be learned individually without the help from the auxiliary loss introduced in Section 3.1. On the other hand, the incorporation of the pair loss seems mainly for making the method more suitable for verification or retrieval without much relationship with the prototypical loss. However, [3] managed to marry this two together and showed improved results.\n\nThe quantitative results are not very convincing. First, there is no comparison between the direction application of prototypical network (as in [2]) with the proposed method to justify the incorporation of learned similarity, pair and auxiliary losses. The only comparison is in the Table 6 in the supplementary material. But it is hard to understand what \"Ours - Cosine\" and \"Ours - Cosine + SIM\" mean. More importantly, the authors should show the necessity of using them in order to be general-purpose. Second, the proposed method does not show promising robustness against adversarial attacks in Table 1. Indeed, it shows better defense comparing to the undefended model, but the performance still lags behind simple techniques like nearest neighbor and data augmentation. Third, there is no comparison with many existing metric learning methods which leverage the prototypical network ([3] for example).\n\nIn summary, the proposed method combines different techniques together trying to provide a general-purpose solution to multiple problems but fails to provide convincing quantitative results to support the model choices and the claimed contributions.\n\nMissing references:\n- nearest neighbor could improve the robustness of pretrained image classifier, including [1] E. Orhan. A simple cache model for image recognition. NeurIPS 2018.\n- metric learning with prototypical loss, including [2] J. Wang, K.-C. Wang, M.T. Law, F. Rudzicz, and M. Brudno. Centroid-based deep metric learning for speaker recognition. ICASSP 2019, and [3] G. Doras, and G. Peeters. A prototypical triplet loss for cover detection. ICASSP 2020.\n- cosine classifiers, including [4] S. Gidaris, and N. Komodakis. Dynamic few-shot visual learning without forgetting. CVPR 2018, and [5] H. Qi, M. Brown, and D.G. Lowe. Low-shot learning with imprinted weights. CVPR 2018, and [6] S. Gidaris, A. Bursuc, N. Komodakis, P. Perez, and M. Cord. Boosting few-shot visual learning with self-supervision. ICCV 2019.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting approach that has not been elaborated and studied carefully enough.",
            "review": "In order to organize my review, I use the NeurIPS2020 template with slight adaptions to fit the ICLR requirements. \n\n### Summary and contributions: Briefly summarize the paper and its contributions.\n\nThe paper proposes a method to learn an encoder, a similarity measure, and a set of prototypes in an end-to-end setting. Thereby, the encoder transforms the input data into an embedding space where the similarity between two embedded samples is measured in terms of the similarity function. Moreover, a set of prototypes is learned in the embedding space in order to learn class representations. The authors nicely present the advantages of such an architecture by showing how easily it can be adapted to different use cases. In particular, by showing that the proposed network architecture can be applied to multi-class classification, few-shot classification, verification, and retrieval and achieves results comparable to state-of-the-art methods.\n\n\n### Strengths: Describe the strengths of the work.\n\nThe proposed architecture is fairly simple and straightforward. The idea to incorporate prototype-based learning principles in the architecture proposed by Monteiro et al. (2020) is neat. Additionally, how the authors derive the applicability of the architecture to different use-cases is sound.\n\n\n### Weaknesses: Explain the limitations of this work along the same axes as above.\n\nBased on previous work, the theoretical contribution of the paper seems to be small and not fully investigated (see my comments below). An important contribution could be the evaluation of the proposed architecture across all the different tasks. Unfortunately, the authors fail to provide a reproducible and complete evaluation since they try to fit all the different experiments into the main part of the paper. For this reason, the evaluation is not convincing (see additional feedback for improvement below). Furthermore, the learning paradigm of the prototypes does not reflect the state of the art and is not motivated by known methods.\n\n\n### Correctness: Are the claims and method correct? Is the empirical methodology correct?\n\nOverall, the proposed method seems to be correct. However, I consider the general statement that the proposed architecture is more robust against adversarial attacks as incorrect since the presented evaluation is not sufficient for such a general statement (see comments to the authors below).\n\n\n### Clarity: Is the paper well written?\n\nIn general, the paper is well written but has some points for improvement. Please consider a correct citation style (\\citet{} vs. \\citep{}). Moreover, mathematical notations are partly hard to read. For example, in Equation (3) the authors use $\\mathcal{S}(\\cdot)$ but $\\mathcal{S}$ is previously defined as a function that takes two arguments. I think that I understand how the authors read this equation. However, such a mathematical style makes the paper unnecessarily difficult to understand.\nPlease have your submission proofread for English style and grammar issues.\n\n\n### Relation to prior work: Is it clearly discussed how this work differs from previous contributions?\n\nThe discussion of prior work is at a high level and *highly* similar to Monteiro et al. (2020). However, some material is missed that I think should be discussed. For example,\n\n* Distance metric learning was already investigated before Xing et al. (2003):\n   * See Hammer, Barbara, and Thomas Villmann. \"Generalized relevance learning vector quantization.\" Neural Networks 15.8-9 (2002): 1059-1068, where they learned a relevance matrix (in the authors’ notation $W$ with the constrained to be diagonal).\n   * See Hastie, Trevor and Simard, Patrice and Säckinger, Eduard. “Learning Prototype Models for Tangent Distance” NIPS (1995), where they learn a tangent distance fully data driven. Note that there are a couple of more relevant publications related to tangent distances. \n* Prior work about pure prototype-based learning:\n   * Kohonen’s Learning Vector Quantization (LVQ) algorithms, including the original heuristic versions and the end-to-end trainable versions like Generalized LVQ (by Sato and Yamada), Generalized Matrix LVQ, etc.\n   * Or to get an overview over the missed field, see Bien, Jacob and Tibshirani, Robert. “Prototype selection for interpretable classification.” The Annals of Applied Statistics (2011)\n* Recent work about the fusion of encoder architectures and prototype-based classifiers:\n   * Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Su, Jonathan and Rudin, Cynthia. “This Looks Like That: Deep Learning for Interpretable Image Recognition.” NeurIPS (2019). The proposed method in this paper also fuses an encoder architecture with prototype-based learning.\n   * Hong-Ming Yang and Xu-Yao Zhang and Fei Yin and Cheng-Lin Liu. “Robust Classification with Convolutional Prototype Learning.” CVPR (2018)\n   * To name just a few.\n\n### Reproducibility: Are there enough details to reproduce the major results of this work?\n\nThis is one of my main concerns about the paper. The paper is not self-contained regarding this aspect. Throughout the paper, the authors do not provide enough details to reproduce the results – also not in the supplementary material. Moreover, the limited side-information about the architecture setup, training pipeline, and evaluation framework limit the assessability of the numerical results with respect to their quality.\nI have noticed that the authors provide the source code of their implementations. However, the source code should accompany a paper to assess the evaluations and should not be the main material to understand an experimental setup – a scientific contribution should be self-contained.\n\n\n### Additional feedback, comments, suggestions for improvement, questions for the authors, and a **recommendation (accept or reject)**.\n\nCould the authors please elaborate on why they call it semantic similarities? This wording is used in the title but not discussed in the paper.\n \nIs the adaption scheme for the prototypes inspired by the Hebbian learning principle? Is there a reason why the authors did not use other adaption schemes (see the early versions of Kohonen’s LVQ versions) or why the prototypes are not learned by gradient descent? Have the authors observed any cross-effects between the auxiliary loss and the prototype adaption scheme?\n\nIntroduction: In the first bullet point, the authors mention that adversarial examples pose a danger to the use of current classifiers. I do not agree with this. It is a threat in safety-critical applications, but not in general.\n\nOn page three: What is a “semantically meaningful distance measure”? According to the given definition, the authors distinguish between \"deep metric learning\" and \"distance metric learning\" by the former “focuses on projecting the data onto a space where standard distances are meaningful”. What are *standard distances*? According to the previous definition, learning of a Mahalanobis distance is a deep metric learning method as well since $W$ can be decomposed (because it is positive semidefinite) into $A^T A$ so that after transforming the data points by $A$ the Mahalanobis distance becomes a Euclidean distance. Moreover, also basic distance learning schemes used preprocessing methods (e.g., PCA) before the distance measuring, which is again a reason against the proposed differentiation.\n\nWhat is the reason for defining the similarity model as a mapping to $\\mathbb{R}$? How is the similarity model defined in the experiments?\n \nWhy have the authors not used better initialization schemes (known from prototype-based learning) for the prototypes? For example, initialize the prototypes of one class by k-means after the data samples of this class have been transformed by the initialized encoder.\n\nRegarding the adversarial robustness evaluation: Please note that ALP could suffer from gradient obfuscation (Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich “Logit Pairing Methods Can Fool Gradient-Based Attacks”  arXiv:1810.12042) and should be used with caution. Generally, evaluating model robustness only by white-box attacks could suffer under gradient obfuscation, so that robustness results are overestimated, especially if the model architecture becomes more complex. Therefore, I recommend that the authors evaluate the robustness in terms of black-box and white-box attacks. Please note that one of the results in Table 1 is erroneously printed in bold: Clean accuracy of Ours-SIM vs. TLA.\n\nRegarding the Robustness under domain shift evaluation: Have the authors observed any correlations between the features of different domains? What happens if the model learns several prototypes per class? Could the authors explain why the model is so good at this task?\n\nAre the results of benchmark methods presented in the comparisons cited or obtained by an own evaluation?\n \nRegarding the verification evaluation: Could you add the results Cosine+SIM of Monteiro et al. (2020) in order to make the comparison fair. \n \nAblation study: Could the authors explain why the auxiliary loss has such an impact? Have the authors tried to visualize the embedding space via t-SNE for example to gain an understanding?\n\nRetrieval results Tables 7 and 8: Note that the proposed method is also outperformed by Roth et al. (2019) and not only by the ensemble. \n\nOverall, it is difficult to judge whether the empirical results are meaningful considering the little information available on the evaluation setting. A question that arises immediately is whether the improved performance is achieved due to more tunable parameters. \n \nConsidering all the above points, I vote for rejection, since the paper does not have the right maturity level to be published at ICLR.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}