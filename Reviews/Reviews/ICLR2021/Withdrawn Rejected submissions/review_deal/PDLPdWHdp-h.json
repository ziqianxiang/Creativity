{
    "Decision": "",
    "Reviews": [
        {
            "title": "Official Blind Review #3",
            "review": "This work explores the phenomenon discovered in \"Adversarial Examples Are Not Bugs, They Are Features\" where a relabeled training set that has been adversarially perturbed towards the new labels yields good performance on the clean test set. This work examines whether the effect extends to autoencoders and, finding that it does, explains the phenomenon with theoretical analysis. This analysis contains valuable insights, such as the observation that adversarial perturbations in the case of a linear encoder and 2-layer encoder decompose into a term attenuating the source input's signal and a term magnifying the target intput's signal.\n\nPros:\n- Improved understanding of an interesting phenomenon\n- Insightful theoretical analysis\n- Empirical results with DNNs that match the toy empirical results from the theoretical analysis\n\nCons:\n- No empirical results for the 2-layer encoder\n- Unclear if this would be interesting to more than a narrow audience\n\nClarity is not superb at present. One has to dig around to understand what is meant by \"predictive\" and \"knowledge distillation\", and there are several typos affecting clarity. Other than that, the writing is good.\n\nTo my knowledge the work is original, and it could foster useful discussion in the adversarial examples community.\n\n______________________________________________________________________________________\nTypos:\n“Additionally, the authors show that classifiers are h”\n\n“MNIST and CelebA as shown in figure ??.” (fix Figure references throughout paper)\n\nIn equation 1.1, there should be a delta under the argmin\n\nIn equation 3.1, there is a floating subscript\n\nAre \\delta_n and \\delta_x the same thing? The definition of x_adv in section 4 suggests that they might be. This is confusing.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Poor writing and too many typos even in equations.",
            "review": "This work tries to study the predictive power of adversarial examples on autoencoders through the lens of comprehensive sensing. However, this current version is very hard to understand due to many typos, even in the math equations. Therefore, I need to guess what these equations/notations represent and could hardly be convinced about the conclusion in the paper.  I would like to list some of the major mistakes in the equations/notations:\n1) In the second paragraph of Section Background, \"to form a lower dimensional representation y ∈ R^(M x N)\". In this sentence, y seems to be a matrix . However, the encoder should map the input to be a vector y and in the following sections, the authors also refer y as a vector. Therefore, it is pretty confusing to know if y is a matrix or a vector.\n2) In page 3, the second line of text,  there is a missing reference as \"The P0ε problem shown in ??\".\n3) There is a significant mistake in Eqn 3.1. \"min∥Φ(s + δ) − Φx_t∥2\" in Eqn 3.1 supposed to be \"min∥Φ(x_s + δ) − Φx_t∥2\".\n4) The derivations of Eqn 3.3 and 3.4 are important for all the following study. However, L(δ_n,λ) is not explained at all and I feel pretty confusing what motivates all these derivations.\n\nThere are also many typos in the writing that I do not want to list all of them. All of these mistakes stop me from trusting the conclusions in the paper.  In all, the quality of this work is not good enough for a ICLR paper.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting topic, hard to follow",
            "review": "The paper studies the predictive characteristics of adversarial attack on autoencoders. It is shown that the perturbation is related to the sparse representation of input. While to conclude this proposal more future works are required, it opens up an interesting topic in this field. However, I have some comments:\n-\tThe paper could be written better and more organized. I found it difficult to follow it easily, as there are number of references missing, some notations are not explained, and there are some typos.\n-\tIn the background section y (low dimensional representation) should be from a M dimensional space.\n-\tA parameter lambda is introduced in Section 3 for computing delta_s and delta_t, but it is not introduced and it is not clear what value it takes in the experiments.\n-\tIn the experiments on MNIST and CelebA, different steps are taken in which, delta_s and delta_t are calculated separately, while according to formula 3.3 and 3.4 they could be found at the same time. How do you explain this?\n-\tIn multilayer scenario, do we really need to perturb on each layer or only input perturbation is enough.\n-\tIs the sparse representation assumption essential for this study? \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "The paper proposed a form of adversarial attacks on autoencoders and showed that adversarial perturbations are predictive with experiments and explanations using compressive sensing. However, the writing of the paper is unsatisfactory, with a lack of clarity and lots of typos/errors. ",
            "review": "The paper proposed a form of adversarial attacks on autoencoders and showed that adversarial perturbations are predictive with experiments and explanations using compressive sensing. The proposed form given by eq (3.1) and (3.2) is both interesting and intuitive. Also, the paper contains experiments with details given. What's more, the studied problem (i.e. adversarial robustness for autoencoder) has its own importance to the ML community. \nHowever, the writing of the paper and the clarity of this work is unsatisfactory, which greatly affects the reading and the rating of the paper. Please see the detailed comments below.\n\nDetailed Comments:\n1. In section 2, notation $\\Phi$ is used to refer to both a measurement matrix in the compressive sensing setting and a linear encoder, without explanation about why this can be used interchangeably. Of course, if we consider an encoder with only 1 linear layer, then it is equivalent to a matrix multiplication. But the authors should mention/emphasize that instead of letting the readers guess.\n2. Also in section 2, at the bottom of page two, the authors mentioned the redundancy of $\\Phi$ without defining it. Again, for a matrix, I can guess the redundancy probably means the linear dependency between the column vectors.\n3. In the first paragraph of the paper, there is one sentence \" Additionally, the authors show that classifiers are h\", which is not complete.\n\nSome of the typos:\n1. Page 2 line 3, the objective is formulated as in eq 1.1 instead of 1.2;\n2. Section 2 paragraph 2, the $y$ and $b$ seem mixed up.\n3. In the paragraph following eq 2.3, there is a missing citation.\n3. eq 3.1: missing $x$\n4. misuse \\citep and \\citet in, for eg, paragraph 2 in sec 2, paragraph 1 in sec 3. \n\nAlthough the problem considered in the paper is interesting and the authors did some experiments to address the issue, the understanding of the work is largely affected by the lack of clarity and existence of many typos. Therefore my temporary rating is 4.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}