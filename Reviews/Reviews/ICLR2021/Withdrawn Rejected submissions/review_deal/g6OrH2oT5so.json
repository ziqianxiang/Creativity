{
    "Decision": {
        "title": "Final Decision",
        "decision": "Reject",
        "comment": "Reviewers acknowledged that the problem addressed in this paper is interesting and is not solved by the existing literature. They appreciated that the setup was well defined and the paper was clearly written. Yet they kept several concerns after the rebuttal. Especially, they expected the comparison to be done with algorithms using both demonstrations and rewards and the current empirical evaluation was not judged as fair. Also, the simple baseline consisting of adding an LSTM to BC to integrate past observations has not been considered either. This baseline is still missing to assess the quality of the proposed method. "
    },
    "Reviews": [
        {
            "title": "Bridging the Imitation Gap by Adaptive Insubordination",
            "review": "Pros:\n1. this paper studies an interesting problem, \"imitation gap\" in imitation learning. \n2. the paper is easy to follow. Especially, the example part is easy to understand.\n3. the intuition for this idea is well-explained.\n\n\nCons:\n1. from my perspective, the basic idea is dynamically using imitation learning and reinforcement learning for agent learning by a weighting function. It is a straightforward idea but lacks some novelty. \n\n2. the main contribution of this paper is proposing an advisor and integrating it with the imitation and reinforcement learning process. However, only averaging loss to leverage imitation and exploration is a comparatively little contribution.\n\n3. in experiment PD, adding advisor from the beginning seems not fair enough, since at the beginning the reward is much higher than baselines. Maybe the experiments need a comparison before the advisor is added and after the advisor added.\n\nIn summary, in this paper, the motivation is clear and easy to understand, and the problem is worthy to study. But the contribution of this paper is a little limited. A better solution is needed.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Introducing a critical issue in imitation learning with a simple solution",
            "review": "### Summary\nThis paper identifies a problem in imitation learning when an expert has access to privileged information that is not available to the learner. When a decision has to be made based on the privileged information, the learner tends to choose average or uniformly random actions of the expert due to the lack of important information, which is called the \"imitation gap\". Therefore, in such cases, learning from the expert can actually harm the training of the learner.\nThis paper proposes to tackle this imitation gap by following the expert only when the learner can reproduce the expert's behaviors only with partial observations. Otherwise, the learner ignores the expert's guidance and relies on reward-based RL. The proposed method effectively balances between learning from the expert and learning from reward with the divergence between action distributions between expert and an auxiliary policy, which is trained sorely from the expert. The exhaustive experiments prove that the proposed method outperforms baselines in environments with a large imitation gap.\n\n### Strengths\n- This paper identifies the \"imitation gap\" that could be critical for many imitation learning applications with intuitive examples.\n- The proposed method is simple but effective in resolving the imitation gap by learning from the expert only when the learner does not require access to privileged information.\n- The comparative experiments are thoroughly conducted and well presented.\n- The proposed method has many potential applications, such as sim2real transfer, where a policy trained in simulation has access to state information, and thus the imitation gap can be problematic.\n\n### Weaknesses\n- As the proposed method involves multiple learnable components, providing the algorithm could clarify how the method works. Currently, it is not clear to understand how the auxiliary policy is trained together with the imitation learning policy.\n- The tasks tested in the paper are limited to simple 2D examples, which are tailored to the proposed method. Including more general and complex tasks could improve the work. It would be great to see how it can be generally applicable to continuous action space, e.g., ant navigation or manipulation tasks with partial observations or cluttered environments.\n- ADV fails to achieve good performance (stuck around 0.2) on the LC task. In theory, ADV should learn as much as BC-demo+PPO and ADV-demo+PPO can. More explanation would help to understand the low performance of ADV compared to other methods.\n- The observation and action spaces can be briefly described in the main paper. For example, the action space for PoisonedDoor is not presented in the paper.\n\n### Questions and additional feedback\n- Cite a relevant paper \"Zhu et al. Reinforcement and Imitation Learning for Diverse Visuomotor Control, RSS 2018\", which combines GAIL and RL, and demonstrates its effectiveness on diverse manipulation tasks.\n- What is the reference for \"teacher-forcing\"?\n- Attach the appendix at the end of the main paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for paper \"Bridging the Imitation Gap by Adaptive Insubordination\" ",
            "review": "[Summary]\n\nPaper aims at attacking the \"imitation gap\" in the canonical LfD problem, where the expert and learner have different observation spaces, leading to poor performance if merely adopting imitation learning without self-exploration. A novel learning-based \nweight proposing function is introduced to mitigate the issue by dynamically weighing the objective of imitation learning (specifically, behavior cloning) and reinforcement learning. The weight is computed based on whether the expert policy disagrees with the best imitation policy on a certain state. Experiments on several artificial tasks demonstrate the effectiveness of the proposed method over pure RL, pure IL, and some canonical RL+IL methodologies.\n\n[Strength]\n\n+) Personally, I like the research problem presented here. Compared to its counterparts on imitation learning with imperfect demonstrations, the authors did an excellent job of making their \"imperfectness\" concrete and also meaningful to potential real-world applications. I won't say it is universally applied but with the intuitive examples, the incentives and the resulting solutions are way more comprehensive than those focusing on injecting noise or perturbation into the demonstrations. \n\n+) The paper is overall clear and well-written. I can't find any major technical issue, while some of the notation could be confusing; Nonetheless, I feel I can get the main idea after reading the main text plus the appendix.\n\n+) The experiments sufficient enough to validate the proposed method. A combination of an artificial but contrived task (PD) and a challenging mini-grid task seems sufficient enough for me.\n\n[Weakness]\n\nThe main concern I have with this submission lies in some missing factors and their potentials to the rationale of the main approach presented here.\n\n-) First thing first, it seems that eq.2 (supposed to be the key loss of the proposed method) implicitly assumes that pi_f is ideal in terms of an imitated policy, so as to the only factor that accounts for the gap between pi_f and pi^exp is the misalignment of observation space. However, I find this assumption could be unrealistic as there are several factors that could affect the imitation performances as well, to name a few: the number of demonstrations (major), different imitation learning algorithms (major), and even the training strategy (epochs, etc, but minor though). The authors did not investigate these factors in their experiments and therefore, it is still unclear whether the loss in eq.2 makes sense as few demonstrations and underfitting will also possibly lead to the disagreement between these two policies. The authors are encouraged to conduct some ablation study with diff. number of demonstrations and also imitation learning methods other than BC to verify how these factors could affect their approach.\n\n-) In the appendix, the authors emphasize that they \"spending too much effort in trying to create a high-quality estimate of pi^aux to pi_f\", while if my understanding is correct, they achieve this by co-train pi_aux with an imitation-only object along with the training of other models. I'm wondering why not just train pi_aux first then query it in the training of the rest? Is there any specific reason to sacrifice the estimation accuracy with co-training? Will the proposed approach enjoy better performance with this pre-train strategy?\n\n-) According to eq.2, the computation of the weight requires a query on state s to both pi_f and pi^exp, while the authors introduce a demonstration-only variant in their experiments. As only the action is marginalized out, it could be tricky to do such computation merely with samples of (s, a). The authors are expected to provide more details on their implementation.\n\n[Suggestions&Questions]\n\n(1) Add an ablation study on how the number of demonstrations and imitation learning algorithm could affect the performances.\n\n(2) Add a baseline that only pre-train pi^aux rather than co-train with other models.\n\n(3) How can eq.2 work with a demonstration-only expert? Please clarify.\n\n(4) Some citations need to be added:\n\nReinforcement learning from demonstration through shaping, in IJCAI, 2015\n\nDirect policy iteration with demonstrations, in IJCAI, 2015\n\nPolicy shaping with human teachers, in IJCAI, 2015\n\nTruncated horizon policy search: Combining reinforcement learning and imitation learning, in ICLR, 2018\n\nReinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance, in AAAI, 2020\n\n[Post-rebuttal]\n\nI have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. I  do believe some of my concerns have been addressed while the main issue on the lack of some necessary evaluations (e.g. num. of demonstrations) remains. Thus I may not be able to escalate my justification to even higher.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of: \"Bridging the Imitation Gap by Adaptive Insubordination\"",
            "review": "Short summary:\n\nThe paper presents an algorithm that allows an agent to leverage demonstrations from an expert with potential privileged  information compared to the agent. The main idea consists in measuring the distance or divergence between the imitation policy learned on the expert demonstrations and the expert policy and use this information to weight (convex combination) the importance of the imitation term and the reinforcement learning term in the combined loss function. \n\nIf there is indeed a discrepancy between the learned imitation policy and the expert policy, this could be linked to the fact that the expert and the agent do not share the same belief state.\n\n General comments:\n\nThe paper presents an important problem that could arise when naively applying imitation learning methods in partially observable MDPs where expert and apprentice do not share the same amount of information about the underlying state.  Basically, if the apprentice cannot distinguish states that are actually different for the expert, the imitation policy will be an average distribution of the expert policy over those states. This can be quite bad because the apprentice takes an average action instead of a precise one as it does not understand the underlying state as well as the expert. This is the imitation gap phenomenon.\nHowever, it is also important to remind the reader that most imitation learning algorithms have been designed under the MDP hypothesis and therefore are not made to answer the imitation gap problems. It is only careless application of those algorithms in inappropriate settings such as POMDPs with asymmetric privileged information between expert and apprentice where this problem can arise. Therefore, I think the authors should mention some previous works that consisted in mitigating the problem of imitation gap such as in the field of conditional imitation learning:\n- End-to-end Driving via Conditional Imitation Learning (https://arxiv.org/abs/1710.02410)\n- Urban Driving with Conditional Imitation Learning (https://arxiv.org/abs/1912.00177)\nwhere the goal is to add privileged information to the apprentice (in the form of navigational instructions) in order to have a better understanding of the underlying state. In addition the authors may also refer to works in representation learning for Imitation learning such as:\n-  Learning Belief Representations for Imitation Learning in POMDPs (https://arxiv.org/abs/1906.09510)\nthat  try to provide better underlying state for the apprentice. Even if those works are not directly related to deal with imitation gap, they are partial solutions to it. Unlike the method proposed by the authors that consists in reducing the importance of demonstrations where there is an imitation gap, they directly try to improve the amount of information of the state provided to the apprentice.\n\nFinally, concerning the method proposed by the authors, I do think that it is an interesting algorithm that weights demonstrations depending on the imitation gap. However this should be coupled with a method that try to enrich the state of the apprentice in order to not lose too much demonstrations. Using an LSTM is a first step towards that direction but more could be done (see papers provided in the previous section).\n\nDetailed remarks:\n\nI have several comments concerning specific sentences in the paper:\n- \"To overcome this imitation gap, prior work often uses stage-wise training\": I do not agree with that at all. I think that previous work used stage-wise training mainly to improve upon the expert demonstrations. Indeed in some of the previous works,  expert and apprentice had directly access to the full state and therefore no imitation gap was possible. Still uncertainty on the expert policy could remain because demonstrations could come from different experts for instance.\n\n-\"As we show empirically, ADVISOR combines the benefits of IL and RL while avoiding the pitfalls of either method alone.\" I would also add that most of the IL methods were not designed to deal with the imitation gap as they make the MDP hypothesis.\n\n-\"These advances have been further improved upon through policy gradient methods\", this has not really been shown or only on a very small set of control tasks. I think this should be reformulated because it sounds like policy gradient methods are better than deep-Q learning methods.\n\n-\"To the best of our knowledge, this hasnâ€™t been studied before.\" weighting demonstrations depending on the imitation gap may not have been studied but methods that try to fill the gap have. It will be good that add this nuance. \n\n- Concerning the imitation gap section, I would directly introduce the POMDP setting and explain simply the imitation gap there. Then I would go on formalising it. The example 2 description is too long and could be more concise and have more impact. I feel also the same with the algorithmic section that could be better organise.\n\nExperiments:\n\nThe experiments are conducted in grid-worlds. They are interesting to show the intuition behind the algorithm. However, it will be easier to grant acceptance if the experiments were on visually rich 3D environnements or in robotics.\n\nRating: I think I could increase my rating if the authors take into account my remarks. I also believe that it is important to not only discard demonstrations where the information gap is important but to try to close the gap by better representation learning or memories. In addition a careful use and collection of expert demonstrations could also mitigate this problem.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}