{
    "Decision": "",
    "Reviews": [
        {
            "title": "Interesting Direction",
            "review": "This paper looks at predicting implicit occupancy functions to represent a 3D shape and brings in insights from meta/few-shot learning to do so. Prior methods typically use an instance-specific latent-variable conditioned implicit representation $f(x, z)$ (with $z$ predicted from some input e.g. an image) to predict occupancy for a point $x$. This work instead, drawing upon progress in differentiable optimization, proposes to generate training data from $z$ i.e. $D(z) = \\\\{x_i, y_i\\\\}^N$ s.t. an optimal linear predictor $f_D$ trained over $D$ does well on held out points. Differently from prior in few-shot learning work though, the linear classifier takes as input 'embedded joints' which also have access to the image embedding. \n\n**Strengths**\n\n- The key insight here is to obtain the implicit occupancy function $f_z(\\cdot)$ via a differentiable optimization over a generated dataset $D(z) = \\\\{x_i, y_i\\\\}^N$ instead of the 'regular' way of using a latent variable conditioned $f(\\cdot, z)$. This is a neat and novel idea, and offers a different perspective to view prediction of implicit 3D representations.\n\n- The empirical results for single-view 3D prediction are quite strong and the paper compares to the relevant SOTA approaches across different metrics and reports superior performance.\n\n- The qualitative results and ablations are also insightful, in particular the ones showing the predicted training points and the meshes in embedding space.\n\n- The paper is in general well written with key ideas and connections presented clearly. In particular, I enjoyed reading the central connection between few-shot learning and 3D implicit representations.\n\n**Concerns and Comments**\n\n***Disentangling Importance of Generated Training Data***\nWhile I like the overall idea and the connection with few-shot learning, I am not convinced the training data generation process as proposed here is critical.\n\n-  More specifically, the image encoding $\\lambda$ is used in two places: i) to generate $D^{train}$ and consequently the decision boundary via $\\alpha$, and ii) to embed points $x$ to $\\tilde{x}$. I am not convinced if the former is needed at all, and the text does not report ablations regarding this. In particular, if one 'fixed' the decision boundary in the embedding space (say $\\|\\tilde{x}\\| < 1$) and trained the proposed approach, what would the resulting performance be? Essentially, compared to approaches like occupancy networks, this method differs in two ways: i) generated training data to compute decision boundary, ii) inductive bias of a 'simple' boundary in embedding space - I am not sure if only ii) is sufficient.\n\n- Slightly related to the above, the generated points aren't very interpretable and don't capture object details (e,g, no points near chair legs), and yet the predicted shape is accurate - this again implies that perhaps most 'heavy-lifting' is done via image embedding and not the generated training data points.\n\nIn Sec 4 'Data', the text states a different way of sampling training points, combining the tricks in 'SIF' and 'OccNet'. Is this improved/different sampling also available to the baseline  methods? If not, the empirical improvement could also be due to more balanced sampling!\n\n-----\n\nOverall, I really like the key ideas proposed and would be inclined to recommend acceptance. However, I would hope the requested ablation and the clarifications regarding fair comparisons to the baselines would be addressed in the rebuttal.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New approach to implicit deep shape reconstruction but requires additional experimentation and results to motivate",
            "review": "### Summary\nThe authors propose a novel implicit shape representation for deep learning inspired by recent few-shot learning literature. Instead of training a network that maps a point and a latent code to a distance or indicator function value, they instead map a latent code to a labeled point set, which is projected using a learned function and then used to train an SVM, which predicts whether a point is inside or outside the shape. Then, at test time, a new point is embedded, and the SVM is queried. The authors are able to outperform other implicit reconstruction methods based on F1-score.\n\n### Explanation of rating\nWhile the proposed method is interesting and seems to produce competitive results, some additional motivation and experimentation is necessary. In particular, I believe on of the biggest disadvantages of existing deep implicit reconstruction approaches is the usability of the representation---even querying the reconstructed shape is expensive, and geometrically manipulating the shapes is a non-obvious task. This paper does not seem to address these issues. At the same time, qualitatively it does not seem to make a substantial improvement in terms of reconstruction quality. Moreover, not many qualitative results are shown, and some comparisons to state-of-the-art work are missing.\n\n### Pros\n\n- The authors make an interesting attempt to apply some of the recent methodology from meta-learning and few-shot learning to 3D shape reconstruction. The paper is clearly written, and the methodology is well described.\n- I think this paper makes progress towards interpretability in implicit shape reconstruction, i.e., by embedding the point sets before using a simpler machine learning model (SVM). This may be a step towards the goal of making such methods more intuitive for manipulating in practice.\n\n### Cons\n- I am not sure what the advantage of this method is over other deep implicit shape reconstruction methods. While the reconstructions appear to be quantitatively better w.r.t. to F-score, very limited qualitative results are provided. There are no qualitative comparisons shown to any of the competing methods. Also, while the main motivating task is single image reconstruction, practically no evaluation is provided specific to that task.. Not a single input image-output reconstruction pair is shown. The reconstructed meshes that are shown are pretty good quality but are unable to capture features such as sharp corners. Are there any specific applications that are possible with this representation over, e.g., DeepSDF or Occupancy Networks?\n- How does training and test time compare to other implicit methods? It seems like this network would be trickier to train given the SVM head. Is this the case?\n- Figure 5 demonstrates that the point embedding network transforms all the point clouds into ellipsoids prior to training the SVM. How does this work for shapes with topology that is not a sphere? Figure 3 has one example of a table with such more complex topology, so I'm curious about what is going there.\n- The authors do not compare to some of the more recent state-of-the-art deep implicit learning methods, e.g., SAL by Atzmon and Lipman.\n- Basic hyperparameters (learning rate, optimizer, batch size, etc.) are not provided.\n\nMinor typos:\n- page 1: \"training set which will be feed\" -> \"training set which will be fed\"\n- page 1: \"will be used as training set\" -> \"will be used as the training set\"\n- page 2: \"mainly composed by 3 sub-networks\" -> \"mainly composed of 3 sub-networks\"\n- page 3: \"projects the point set into embedding space along with $\\lambda$\" unclear, makes it sound like $\\lambda$ is being projected\n- page 4: $\\mathcal{D}^\\mathrm{train}$ index variable should be $i$ not $N$\n- page 5: double sum has same index of summation in eq. 4\n- page: 6: $x_j$ not used eq. 7\n- page 6: \"For the Feature Networks, We\" -> \"For the Feature Networks, we\"",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel and interesting approach, but overall lacks clear presentation.",
            "review": "This paper proposes a model for 3D shape reconstruction from a single image. It choses the implicit function representation for 3D shape outputs and is trained to classify query points as lying inside or outside of the object. Interestingly instead of directly classifying query points concatenated with the global embedding vector as is common in the related work, this paper adopt the few-shot classification approach and provide an SVM classifier for each shape with unique parameters inferred from the input image. To achieve this they propose to use Point Generation Network which provide labeled points in 3D space given an image. These generated points are put through an Embedding Network and used to optimize the parameters of the kernel SVM, while at the same time the query points are put directly through the Embedding Network and are classified by the obtained differentiable kernel SVM.\n\nPros:\n1) The idea is novel and interesting, linking few-shot classification to implicit function prediction for 3D shape reconstruction is insightful.\n\nCons:\n1) A lot of critical details explaining separate components of the model and training/testing procedures are missing which affects overall perception and potential reproducibility.\n2) Presentation of the experimental results can also be improved.\n3) The paper is not written clearly, has a lot of minor mistakes, and overall lacks polish.\n\nI appreciate the novelty of the proposed approach, but, given the problems stated in cons, unfortunately, I can not recommend to accept this paper in its current state. I can only encourage the authors to continue working on the paper, since, in my opinion, it is definitely a publishable work.\n\n######################################\n\nAdditional comments and questions:\n\nAbstract:\n«…will be feed…» -> will be fed\n\nIntroduction, 1st paragraph and Related Work, 1st paragraph:\nGroueix et al. (2018) consider point clouds in their work rather than meshes.\nI don’t think it makes sense to repeat the lists of explicit and implicit representations 2 times in introduction and related work.\n\nIntroduction, last paragraph:\n«…from single image…» -> from a single image\n\nRelated Work, Single image 3d reconstruction:\nSection misses some recent citations for:\n1) Voxel grid representation: [1, 2],\n2) Point cloud representation: [3, 4].\n\nRelated Work, Few-shot learning:\n«To further developing…» -> To further develop…\n\nSection 3.1 and later on:\nNotation for meta-train and meta-test sets is misleading. In your text both sets contain $\\mathcal{D}^{train}$ and $\\mathcal{D}^{test}$ sets indexed by different indices, while in practice meta-test set contains tasks, which differ from the meta-train tasks. So it should be something like, for example, $\\hat{\\mathcal{D}}^{train}$ and $\\hat{\\mathcal{D}}^{test}$ in one of the sets.\n\nEquation 3:\n$\\theta$ was not properly introduced.\n\nThe sentence after Equation 5 «Using recent advances…» should be reformulated.\n\nSection 3.5:\n«…predicator…» -> predictor\n\nEquation 6:\nAre there any arguments to use L2-distance rather than cross-entropy? L2-distance is generally a less common objective for classification problems.\n\nSection 3:\nTraining procedure is undefined in the paper. What is the final training objective, just equation 6? How do you incorporate the optimization for the SVM in the training procedure?\nIt is also not clearly stated how the computations change during test time.\n\nSection 4, Networks:\nWhich design do you choose for an MLP in the point generation network? Does it take $\\lambda$ in and returns an output of the size $3 \\times (N+1)$ for $\\sigma$ and point coordinates? Where do you get the corresponding labels of every generated point and how do guarantee that you generate clouds with equally sized positive and negative subsets of points, given that this is the output of the network? Also, how can you guarantee that they will stay in the bounding box of a shape?\n\nIs an MLP in the embedding networks designed in the same manner (takes and outputs flattened vectors of size $3 \\times N$) or does the MLP work here similarly to PointNet layers (each point separately passes through a shared MLP)?\n\nSection 5: \n«Volumetric IoU is obtained on 100k uniformly sampled 100k points.» - remove the last 100k.\nWhat is the resolution of the grid for IoU calculation? $\\sqrt[3]{100000}$ is not an integer.\nBoth CD and F1 metrics are sensitive to the scale of the input point clouds / meshes. How do you scale the input data?\n\nTables 2, 3:\nprobably miss the coefficient for CD metric values.\n\nFigure 3:\nSince the task is the reconstruction from a single image the input images should be added.\n\nSince prior works with implicit function representation usually train a single binary classifier for all the shapes in the dataset, it might be worthwhile to include results with a corresponding baseline especially if you deviate from the architectures in that works. Namely, to train your model without the upper branch in Figure 1. Such a comparison will clearly show the improvements allowed by the point generation network and kernel SVM classifier.\n\nIt will be interesting to see CD and F1 comparisons to more recent point cloud-based approaches, like [3, 4], possibly using their evaluation protocol.\n\nA set of randomly selected reconstructions and less trivial interpolations (between less similar objects) will also be interesting to look at.\n\n[1] Richter, S. R., Roth, S.: Matryoshka networks: Predicting 3d geometry via nested shape layers. In CVPR’18.\n\n[2] Klokov, R., Verbeek, J., Boyer, E.: Probabilistic Reconstruction Networks for 3D Shape Inference from a Single Image. In BMVC’19.\n\n[3] Wang, K., Chen, K., Jia, K.: Deep cascade generation on point sets. In IJCAI'19.\n\n[4] Klokov, R., Boyer, E., Verbeek, J.: Discrete Point Flow Networks for Efficient Point Cloud Generation. In ECCV’20.",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea, but need more verifications for the advantages. Also, the linking to the few-shot learning does not look appropriate.",
            "review": "This paper proposes a novel idea about the neural implicit shape representations. While most of the previous work about the neural implicit shape representations learn the implicit function as a neural network, this paper proposes to parameterize the implicit functions with a fixed amount of 3D points and binary labels (indicating inside/outside). The predicted labeled point cloud is then further mapped to an embedding space and derives a binary classifier in the embedding space. The new representation of the implicit functions provides better reconstruction results compared with previous work in the experiments. However, there is one missing previous work in the comparison, which looks more relevant to the proposed method than the others. Also, I cannot agree that this proposed method is a few-shot classification setup as described in the title/writing. More details are described in 'Comments and Questions'.\n\n\n*** Strengths ***\nA novel idea about the neural implicit shape representation is presented, and the experimental results demonstrate its outperformance compared with previous work.\n\n\n*** Weaknesses ***\nThe title/writing linking the proposed method to few-shot learning looks misleading the readers.\nMore experimental results and intuitive expositions would be needed to explain why the proposed representation works better than the other neural implicit shape representations.\nMore details are described in ‘Comments and Questions’.\n\n\n*** Comments and Questions ***\nWhile the loss function might be relevant to few-shot learning, in my understanding, this is a totally different setup with the few-shot classification. The point cloud D_train, which is used to compute the classifier, is not given but ‘predicted’. It’s not learning from few-shot examples, but the neural network predicts a labelled point cloud that can derive the final classifier. I think the authors' exposition linking the proposed method with the few-shot classification misleads the readers. I would say that the proposed method just has an inner optimization obtaining the binary classifier from the predicted labelled point cloud. Figure 2 is also not 2-shot-3-way. It has two classes (inside/outside) and a lot more examples (points) than two. \n\nAlso, I could not see exactly 'why' the proposed representation is better than the other neural implicit representations. OccNet (Mescheder et al, 2019) is compared in the experiments, and the difference between the proposed method and OccNet is that the proposed method has an input-dependent classifier while OccNet has a centralized classification network taking a latent code of the input. In this respect, actually the proposed method is more analogous to the following work:\n\nLittwin and Wolf, Deep Meta Functionals for Shape Representation, ICCV 2019.\n\nThis work also learns an implicit function for 3D shapes, and similar to the proposed method, it has a 'hypernetwork' predicting the parameters of the subsequent classifier. But the classifier is defined as a network. My guess is that a neural network would perform better than kernel SVM in the classification. Hence, I hope the authors provide a comparison with the approach of Littwin and Wolf. If the benefit of the proposed method comes from having a smaller number of parameters and thus having a smoother implicit function (although, it’s not clear to me how a smoother implicit function can give higher reconstruction accuracy), one can also consider having a smaller classification network in the Littwin and Wolf’s setup with fewer layers.\n\nLastly, the points do not need to be embedded in a 'three'-dimensional space. It would be good if an additional experiment shows the impact of varying embedding dimensions in the performance.\n\n\n*** Justification ***\nThe proposed method looks interesting. But my guess is that it would work better when replacing the combination of the labelled point cloud prediction and the kernel SVM to a neural network, as done by Littwin and Wolf’s work mentioned above --- a hypernetwork predicts weights of the classifier network. Also, I think the text requires a substantial revision regarding the linking to the few-shot learning. Hence, I’ll lean toward rejection unless the authors successfully address all my concerns in the rebuttal.\n\n\n*** Minor Comments ***\nSection 2: I suggest changing the paragraph title ‘Single image 3D reconstruction’ to ‘Neural implicit shape representation’.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}