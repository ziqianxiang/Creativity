{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Knowledgeable R3 found the paper very good (8). He/she found the authors' responses very informative and that edits made the paper much stronger. R2 expressed reservations about rank collapse being the cause of degradation of performance, but also indicated his/her willingness to increase the score if the authors can convincingly respond to his/her concerns. This concerned was shared by other reviewers, and there was an extensive discussion during the discussion period. R3 and R1 found the authors' responses very convincing. Fairly confident R1 found the paper good, appreciated the discussion, and recommends the paper to be accepted. R4 found the paper marginally above the acceptance threshold, however expressing a lower confidence in his/her assessment. In summary, the article contains extensive experiments, theory, and a well motivated idea, elucidating an intriguing phenomenon and useful for designing better bootstrapping-based deep RL methods. Although the reviewers expressed some reservations in their initial reviews, there was a lively discussion with quite positive final feedback. Weighing the ratings by confidence and participation in the discussion, I am recommending the paper for acceptance. I would like to encourage the authors to make efforts in making the presentation as clear as possible, having in mind the discussion and comments from the reviewers. "
    },
    "Reviews": [
        {
            "title": "Review of \"Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning\"",
            "review": "This paper discusses a phenomenon wherein the feature vectors of the learned value function in reinforcement learning (RL) lose their diversity as training progresses. The paper analyzes the rank of the final hidden layer in the model parameterizing the value function and shows experimentally that for offline-RL and online-RL setups on Atari and Gym benchmarks, this rank collapse occurs with a drop in the average return. The paper further develops two models for understanding this phenomenon, (i) where the value function is modeled using the neural tangent kernel, and (ii) where the value function is modeled using a deep linear network. The paper argues that bootstrapping results in reduction of the rank of the feature matrix as training progresses for these models. A regularization term that equalizes the singular values of the feature matrix is used to mitigate this rank collapse and experimental results on Atari benchmarks are shown with this regularizer.\n\nThe main claim of this paper is to identify the phenomenon of rank collapse of the feature matrix. I have concerns about the experimental findings of this paper and correctness of its theoretical claims, which are discussed below. I am willing to increase my score if the authors can convincingly argue otherwise. Broadly, I agree this is an interesting direction but current manuscript does not convince the reader that rank collapse is indeed the cause of degradation of performance.\n\nComments.\n\n1. Figure 1 does not completely validate the claims on page 3. In Asterix, increasing the amount of data does not lead to rank collapse but the returns degrade significantly during training, why? In Seaquest, the returns (blue) have degraded essentially to zero even when the rank (blue) is at its maximum. This suggests that there are other factors which are causing the drop in performance instead of/in addition to the rank. The trends in Appendix A1 are similarly inconsistent, as is Figure 2 (Ant-v2). The implication “if low rank, then low returns” is reasonable to expect due to reduced capacity of the value function approximation. But how do the authors deduce from these experiments that “rank collapses in data-efficient RL” (first sentence of Section 3.1).\n2. I have a similar concern about Fig. 3b (Seaquest). The rank for n=4 gradient steps/transition clearly collapses, yet the TD error remains small, and yet the returns are quite bad. If rank collapse entails that the TD error is not minimized well-enough, and that is the cause of the drop in returns, then how can one explain this figure? I suspect the discrepancy is because the TD error is used in Fig. 3b. Can you perhaps compute a pseudo-optimal policy using a good RL method (say Rainbow) for Seaquest and use its value function as the surrogate for Q*?\n3. The narrative will benefit from being more precise. There is an egregiously large number of sentences where the word “implicit” (the paper uses this word 37 times in the first 8 pages) is used in a vague manner (see for instance Definition 1). Further, “implicit under-parametrization” a bad monicker, should the lottery ticket hypothesis be also called implicit under-parametrization?\n4. Why is Theorem 4.1 here not a direct application of Theorem 5 of Mobahi et al., 2020? Further, the big intellectual gap in the argument is that while we are trying to find the fixed point of the Bellman equation in RL, there is no such fixed point in kernel regression. So while the argument that self-distillation during iterative TD^2-minimization may cause a loss of diversity of the feature space, it does not seem to the only reason, after all some examples in Fig 3 do not show rank collapse.\n5. Perhaps the underlying problem is really that minimizing TD^2 is not an appropriate way to find the fixed point of the Bellman iteration when using function approximation. Indeed, if the TD error is small (Fig. 3b, n=4), there is nothing the network can do to improve the returns. TD error is small in this case in spite of the feature matrix having low rank; it indeed depends on the complexity of the value function.\n6. The development in Sec 4.2 using the work of Arora et a., 2019 around eq. (5) argues that when Q_k(s,a) = Q_{k+1}(s,a) for all pairs (s,a) you get rank collapse; this is a very special situation where the value function at each (s,a) is essentially proportional to the rewards at that state-action pair. I tried to follow the proof of the argument for the botostrapped updates in Theorem 4.2 but to my understanding it hides this same issue, e.g., in eq. (D.15) it is assumed that zeta is small enough which is not true. By this argument simply rescaling all the rewards to have small magnitude should result in rank collapse.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "The main contributions of the paper are the following ones:\n 1. Identifying feature rank collapse problem in RL algorithms using bootstrapping and gradient descent optimization for value function estimation and pinning down this problem to these two factors.\n 2. Theoretical analysis of rank collapse based on Neural Tangent Kernel framework and ideas from analysis of continuous-time differential equations. In particular, the authors showed that rank collapses near optimal point when fitting resembles self-distillation.\n 3. The regularization term heuristic to prevent rank collapse.\n \nOverall, the paper contains a very extensive experimental part, theoretical part and very-well motivated idea. However, the authors tried to put too much information into one paper, therefore sometimes it is difficult to follow. For example, Preposition 4.1 is difficult to follow, since a lot of interesting and important details are hidden in Appendix.\n \nSome questions and issues.\n 1. There is an assumption that S is a normal matrix in theorem 4.1. How restrictive is this assumption?\n 2. Is it possible to extend theoretical analysis from policy evaluation settings to policy training settings. i.e. when Bellman optimality operator is used instead of Bellman operator?\n 3. I would recommend adding a title to Figure 3 (a).\n 4. Figure 3 (d) contains one red trajectory, for which srank does not collapse. Could you please comment what special properties this trajectory has that srank stays almost the same?\n 5.  \"Similar to Arora et al. (2018; 2019), we assume that all except the last-layer weights share singular values (a.k.a. “balancedness”).\" According to Appendix the stronger assumption W_j*W_j^T = W_{j+1}^T*W_{j+1} is required.\n 6. I assume lambda is missing in equation (C.12).\n 7. The question regarding the equation between D.4 and D.5. I understand how the derivative was computed, but I am not sure that I understand what 0 means in dL_0(W_{N:1})/dW_{N:1}.\n 8. I am a bit puzzled by the fact that Rainbow performance increased, while DQN performance decreased in online settings. What is the key underlying component that leads to different results? \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Post discussion review\n-----\n-----\n\n# Summary\n\nThe authors present evidence that the approximate rank of the features is correlated with the learned policy's performance and that this rank shrinks when using bootstrapping. They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks. Finally, they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases.\n\n# Reason for score\n\nThe authors isolate an interesting phenomenon and present some compelling empirical evidence. This is interesting work and I have have no doubt that it is of sufficient quality for publications.\n\n# Pros\n\n* The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent, a critical aspect of many RL methods. Using neural nets to learn (Q-)value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is. As a result, this subject of this work is quite important and likely of great interest to the field.\n* The experiments are well designed and relevant to the main thesis. The empirical results are well presented and easy to understand.\n\n# Cons\n\n* After a very productive and enlightening discussion with the authors, the only noteworthy issue is that this paper contains too many contributions for the format making some of them hard to appreciate. A more focused in depth dive into a subset of the theoretical contributions might have been preferable and possibly provide more insight.\n\n# Conclusion\n\nI strongly support the acceptance of this submission. After discussion with the authors and resulting updates to the paper, I don't see any reason for rejecting this paper. All of the major concerns from my initial review have been addressed.\n\n\nInitial review\n-----\n-----\n# Summary\n\nThe authors present evidence that the approximate rank of the features is correlated with the learned policy's performance and that this rank shrinks when using bootstrapping. They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks. Finally, they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases.\n\n# Reason for score\n\nAlthough the authors isolate an interesting phenomenon and present some compelling empirical evidence, I have a few concerns about the theoretical contributions which, hopefully, the authors can address or clarify any misunderstanding. This is interesting work and I am more than willing to adjust my review if the authors can assuage my concerns.\n\n# Pros\n\n* The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent, a critical aspect of many RL methods. Using neural nets to learn (Q-)value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is. As a result, this subject of this work is quite important and likely of great interest to the field.\n* The experiments are well designed and relevant to the main thesis. The empirical results are well presented and easy to understand.\n\n# Cons\n\n* This did not feel like an 8 page paper. This paper took a long time to review. With 18 pages of appendix, 9 of which are clarifications and proofs, what is left of the theoretical contributions in the main body of the paper doesn't provide much insight into the role/importance of the assumptions or into what makes each claim true.\n* The proof for theorem 4.2 appears to make use of the assumption that $\\varepsilon(s, a) = W_N \\cdot \\zeta [s; a]$ and $y_k = Q_{k-1} + \\varepsilon$. This is not conveyed in the main body of the paper but seems to be a fairly strong assumption on the form of the bootstrapped targets.\n* Similarly, I would argue that the premise that the bootstrapped targets will eventually be close to the previous, i.e., $y_k \\approx Q_{k-1}$, is flawed. There is no guarantee that applying the Bellman operator will return a function that is inside your function class, even in the linear case. Furthermore, we know this phenomenon to be significant and motivated work on the projected Bellman error, a concept heavily used by the various variants of gradient temporal difference learning.\n* In theorem 4.1, the assumption that $S$ is a normal matrix seems impractical and likely makes this result only applicable to very rare cases.\n* In proposition 4.1. it isn't immediately apparent where in the proof the assumption that the loss $L$ is the TD loss is leveraged. If it isn't used, this would suggest that this is a general property of deep linear networks and wouldn't support the authors observations that the rank issues are specific to bootstrapping.\n\n# Questions for the authors\n\n1. Was anything done to \"normalize\" the results in figure 2 to account for the differing number of total updates as a result of different n? Can these observations be explained by the fact that more updates results in the parameters traveling further from their initial values? What happens when plotting the srank vs. # of updates in the setting? (These likely don't need 3 distinct answers)\n2. Could the authors elaborate on why the normal matrix assumption might be reasonable, or, otherwise, explain why this doesn't make it a vacuous result?\n3. What is the purpose of the \"explaining implicit under-parameterization across fitting iterations\" section? I think I am missing the insight this is trying to provide. Why would the parameters change at all if I reuse the results of the previous minimization as targets? What does the Bellman error refer to here and what does it mean to attain zero (or any value) TD error when the targets are just the Q-values?\n4. The balanced assumption used with the deep linear networks seems critical for the proof. Is my assessment correct or could these results possibly hold without it? How does this assumption limit the applicability of the insight gained here to more practical neural networks?\n\n# Misc comments and typos\n* page 2, Yang et al. don't seem to use the term \"effective rank\", but do use the term \"approximate rank\".\n* page 4, \"we first remove the confounding **with** issues [...]\"\n* page 30, proof, it would help to explicit state the dimensions of $\\zeta$. Is the $\\top$ on $\\zeta^\\top$ a typo? Otherwise, why is it not used further down?  (no need to answer either way, just reporting on something that tripped me up)",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A very intriguing  observation for the conflicts between the model regularization and  bootstrapping method in deep RL ",
            "review": "###Summary:\n\nThis paper identifies a type of implicit under-parameterization phenomenon in deep RL methods that use bootstrapping.  It is found that after an initial learning period, the effective rank of the feature matrix keeps decreasing. This implies that the representational power of the network is not fully utilized. The authors call it a type of implicit under-parameterization.  Moreover, the emergence of this under-parameterization strongly correlates with the poor performance.  Some preliminary theoretical analyses are provided to explain this phenomenon. \n\n###Pros:\n\nThe paper is well written, and I can follow the idea very smoothly. The implicit under-parameterization phenomenon seems very intriguing and useful for designing better bootstrapping-based deep RL methods.  The theoretical analyses are very illustrative though still preliminary. \n\n###Cons:\n\n1. The analysis in Section 4.1 seems not correct to me. For kernel regression,  the implicit bias of gradient descent with an infinite number of iterations is to pick up the solutions with the smallest RKHS norm. This implies  $c=0$ in Eq. (1), which would make the subsequent analysis problematic.  However, if early stopping is applied, the GD solutions will be equal to the one given in Eq. (1) with $c>0$. The value of $c$ depends on how the early stopping is applied. Please refer to [1] for more details. \n\n\n###Other comments:\n\n1. The analysis in Section 4.1  is very illustrative due to the use of the kernel model.  But all the experiments are done for neural networks. It would be much helpful if some extra experiments with kernel models can be added, for which we can directly compare the experimental results and the theoretical analysis. \n\n\n\n\n[1] Suggala, Arun, Adarsh Prasad, and Pradeep K. Ravikumar. \"Connecting optimization and regularization paths.\" Advances in Neural Information Processing Systems. 2018.\n\n\n### Post-rebuttal Comments\nI thank the authors for the response and the efforts to update the drafts. I  think this submission made some original contributions. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}