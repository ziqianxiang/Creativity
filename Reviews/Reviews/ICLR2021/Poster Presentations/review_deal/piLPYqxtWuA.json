{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a number of techniques to improve the existing non-autoregressive end-to-end TTS model -- FastSpeech. These techniques include replacing the teacher forcing with ground truth targets and using a variation adaptor to introduce auxiliary information such as duration, energy and pitch.  The experiments show that the proposed Fastspeech 2 model is faster in training  compared to the existing FastSpeech model and meanwhile can still achieve high quality synthesized speech.  The work reported in the paper is essentially about system improvement over FastSpeech but has it value in the speech community given the current interest in non-autoregressive rapid TTS.  On the other hand, concerns are also raised regarding the complexity of the pipeline and the significance of the novelty. The authors' rebuttal is good and has addressed most of the concerns.  Overall, this is an interesting paper and can be accepted.  "
    },
    "Reviews": [
        {
            "title": "Official Review",
            "review": "This work presents several improvements over the original teacher-student framework in FastSpeech: 1) training the model with ground-truth target instead of the output from teacher, 2) extract phoneme duration, pitch and energy from speech and directly take them as conditional inputs in training and use predicted values in inference.  Importantly, it uses pretrained forced aligner (MFA) to extract the phoneme durations for training.  \n\nComments:\n\n1, Typo: In introduction, \"alleviate the one-to mapping\"\n\n2, The \"one-to-many mapping\" is not an issue in general. This setting widely exists in generative modeling, e.g., label conditioned image synthesis, mel spectrogram conditioned waveform synthesis. The problem of FastSpeech (and other non-autoregressive TTS models) is really due to the over-simplified output distributions, which assume conditional independence between frames and frequency bins for mel spectrogram. As a result, these models doesn't account for the variations in real data.\n\n3, The introduction, section 2.1, and section 2.2 contain too much duplicated information. One may shorten the text properly.\n\n4, The motivation & architecture of FastSpeech 2s (Figure 1 (a)) is similar to the previous text-to-waveform model clarinet. Both of them use mel prediction task to guide the training. The authors overclaim that \"FastSpeech 2s is the first attempt to directly generate waveform from phoneme sequence\".\n\n5, The model is similar to traditional TTS pipeline with separate duration model, pitch/F0 model etc. The CWT/iCWT-based pitch predictor is interesting. \n\n6, The MOS of autoregressive Tacotron 2 is relatively low. Which implementation did you use?\n\n7, One may also report the standard deviation, skewness, and kurtosis of pitch in synthesized audio from autoregressive model (Tacotron 2 and Transformer TTS). I assume their results would be closer to GT.\n\n8, In Table 5, the durations from teacher model are extracted by teacher forcing ground-truth mel spectrogram? One may mention that MFA is pretrained on a much larger dataset, thus may have better generalization than the teacher model trained on small LJSpeech dataset.\n\nPros:\n- Good sample quality.\n- Sufficient ablation study.\n\nCons:\n- The proposed pipeline is far more complicated than existing end-to-end TTS model.\n- Inaccurate & confusing claims (see my comments).\n- The novelty is rather limited. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "New improvements to the FastSpeech model that lead to improved quality and  performance",
            "review": "This paper is motivated by, and intends to address shortcomings of, the previously published FastSpeech paper, specifically problems with the internal duration prediction of that model, and its requirement for a teacher-student configuration. The submission makes clear what the problems with that original approach are, and the evaluation later confirms this. What is missing from this work, however, is a more candid statement about the trade-offs between the FastSpeech and FastSpeech2 architectures since the current implementation requires externally aligned datasets, or the resources to develop them.  \n\n\nIn other words, although the authors present this paper as a natural and organic extension of the original FastSpeech work, one could argue that in some ways it represents a step backwards with respect to the spirit of sequence-to-sequence models that solve the alignment problem without the need for external supervision. One may very well have the resources to afford such supervision, so there's definitely a place for this type of model, but the paper should be more forthcoming in the pros and cons of this new approach.\n\nHaving a duration-prediction model explicitly trained with externally-determined duration targets, in fact, brings this work closer to the more \"classical\" earlier-generation pipeline neural architectures (pre seq-to-seq), with the notable difference that everything here is trained jointly. I think the paper would be well served by a broader review of the literature that tries to situate it beyond its similarity with FastSpeech only. Some relevant parts of the literature seem uncited (I've included examples below). If space is an issue, some of the material in the Appendix could be shortened, or just referenced.\n\n\nThe paper is generally well written and easy to follow. Some comments and needed clarifications follow:\n\nThe paper says that the variance adaptor \"adds\" different variance information. Do you literally mean that you sum up the different embeddings (the use of a + in Fig. 1b would suggest as much), or do you concatenate/stack them? If it's the former, try to motivate this choice since summing up linguistic embeddings with acoustic embeddings doesn't necessarily seem an obvious choice.\n\nSec. 2.3 --  *Previous neural network based TTS systems with pitch prediction (Arik et al.,2017; Gibiansky et al., 2017) often predict pitch contour directly.* -- There is, in fact, an extensive (and relevant, but uncited) literature on direct neural prediction of f0 with external f0 targets as the authors do here. See, e.g,:\n- Statistical Parametric Speech Synthesis Using Deep Neural Networks (Zen et al., 2013)\n- Prosody Contour Prediction with Long Short-Term Memory, Bi-Directional, Deep Recurrent Neural Networks (Fernandez et al., 2014)\n- TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks (Fan et al., 2014)\n\nThe idea of conditioning seq-2-seq models on some acoustic properties of the output (in the spirit of the variance adaptor of FastSpeech2) is also not new, and has been proposed to either improve control or stability of such models. See. e.g.,\n\n- Sequence to Sequence Neural Speech Synthesis with Prosody Modification Capabilities (Shechtman et al., 2019)\nand more recently:\n-  Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features (Raitio et al., 2020)\n\n\n Sec. 2.5: *Previous non-autoregressive vocoders (Oord et al., 2017; Prenger et al., 2019; Yamamoto et al.,2020; Kumar et al., 2019) are not complete text-to-speech systems, since they convert time aligned linguistic features to waveforms, and require a separate linguistic model to convert input text to linguistic features or an acoustic model to convert input text to acoustic features (e.g., mel-spectrograms). FastSpeech 2s is the first attempt to directly generate waveform from phoneme sequence, instead of linguistic features or mel-spectrograms.* -- A phoneme sequence is a type of linguistic information that needs to be extracted from the input text. It's hard to argue that something that requires a G2P is more of a full TTS system. I assumed that the input text was also normalized prior to the G2P, so by starting from a phoneme sequence, you're assuming access to both a normalizer and a G2P.\n\n\nEvaluation; What type of sentences were used in the evaluation? The paper mentions that the training of FastSpeech2s required using shorter clips to deal with memory issues. Do you restrict your listening test to also contain only simpler, shorter sentences? Systems that perform close to each other when synthesizing shorter prompts can behave very differently when attempting more complex inputs (questions; lists; sentences with syntactic embedding or relative clauses; etc.)\n \nThough the numbers cluster nicely, consider adding parwise significance tests to the results in Table 1.\n\nThe analysis/resynthesis condition (GT(Mel+PWG)) shows a drop of 0.38 MOS points with respect to the ground truth (4.3 vs 3.92). Can you comment on this? It seems large.\n\nSec. 2.2.3. Ablation studies -- Any reason why you didn't include similar tables to Table 6 for the other studies (2nd and 3rd paragraph in this section)?\nMinor edits:\n\n- since waveform contains --> since the waveform contains\n- discuss the challenges lie --> discuss the challenges that lie\n- pitch predictor consists --> the pitch predictor consists\n- A few acronyms incorrectly capitalized in the bibliography (tts --> TTS / flowavenet --> FloWaveNet / etc.)\n- It's diffcult to make sense out of the Ryan, 1994 reference.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Clear improvement over FastSpeech",
            "review": "This paper describes a number of improvements over the non-autoregressive FastSpeech TTS model.\n\nOne of the trends that TTS has seen since Tacotron was announced is a retreat from fully end-to-end modeling for TTS.  FastSpeech2 is another entry in that.  The text input is first converted to phones, and prosodic information can be inferred from text (in a unified system) or can be conditioned by a user.  All of these are valuable improvements for quality and customizability.  \n\nIn particular, FastSpeech takes an interesting modular approach to conditioning on pitch, duration and energy for its prosody component.  It relies on ground truth predictors during training, but learns to infer these via multitask training so they are not required during inference.   The use of a dense and continuous pitch representation as a pitch spectrogram is interesting as well.  This could be a limitation for customizability as a user would be more comfortable to provide a pitch target rather than specifying a pitch spectrogram.\n\nThis paper is clearly written and includes sufficient detail for reproducibility.  The evaluation is convincing especially as it relates to training and inference time.\n\nMinor comment: In Table 2 the FastSpeech2s training time number is omitted.  This is because it is not directly comparable to FastSpeech2 or the other training numbers -- it includes training a neural vocoder while the others do not.  However, since the discussion of training time as a valuable measure is already begun, I would suggest that the authors include this number, with the explanation that it is not comparable. (or include vocoder training time along with the other methods.)\n\nComment: The one-to-many TTS problem --- where one lexical utterance can be produced in many different correct ways -- is only partly addressed here for two reasons. 1) there is variation other than duration, energy and pitch that can vary between realizations of the same utterance, voice quality, background and channel noise, and speaker effects.  2) more significantly, the one-to-many problem is only approached during model training.  During inference, the process is still deterministic given the state of the model.    The inference side of the problem is important for objective model evaluation.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Non-AR end-to-end TTS model with controllability",
            "review": "This paper further extends the non-autoregressive end-to-end TTS model \"FastSpeech\".  The new work includes (1) removing the knowledge distillation from an AR model to the non-AR model which was required in FastSpeech, (2) introduced \"variance\" representations such as energy and F0 in addition to duration.  (1) made the training procedure simpler.  (2) improved the naturalness as well as controllability of synthesized speech.\n\nTechnical details and model architecture are available in the body of the paper, whereas additional details are also provided in the appendix.  The model was evaluated in the LJSpeech dataset, which is a publicly available dataset and often used for TTS model evaluations.  Audio samples are also provided in the demo page.\n\nThere are a few parts need to be addressed.  (1) made the training procedure simpler but it also introduces the necessity to rely on external aligner, which actually complicates the whole voice building process.  It is not well discussed in the paper.  (2) also introduces the dependency to external modules, such as F0 extractor and makes the model less \"end-to-end\".  The dependency to an external F0 extractor can introduce errors, as F0 extractors sometimes produce errors such as pitch halving/doubling.  The authors should also discuss it from this point.\n\nComments:\n- For pitch and energy 256-bin quantization was used.  Did the author observe any issue if continuous values were directly used?\n- It is unclear whether there is any difference between FastSpeech 2s waveform decoder and Parallel WaveGAN decoder.\n- Comparison with FastPitch: the authors claimed \"results in better prosody\" but I cannot see any evidence or experimental validation for this claim.  If you have any results, please include it.  Otherwise, this is too strong claim.\n- To demonstrate FastSpeech 2 comparable to AR models, having a side-by-side comparison between them is helpful.\n- Table 3 includes moments of pitch of natural and synthetic speech.  Can you include these numbers of FastSpeech 2 with F0 fit via MSE which you discussed in the ablation study? \n- If I understand correctly pitch mean/var are constant within an utterance, but pitch spectrogram is frame-by-frame.  In Figure 2 both of them are predicted from pitch predictor but the nature of these values are different (global / frame-by-frame).  Please include more explanation how they are predicted in the pitch predictor.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Fastspeech 2: fast TTS training and inference with decent speech quality",
            "review": "Summary: \nIn FastSpeech 2 and 2s, the authors propose several changes to the non-autoregressive FastSpeech model. An autoregressive teacher model was used in FastSpeech to alleviate the one-to-many problem of TTS by providing phoneme durations estimated by attention mechanism, and predicted mel-spectrograms as soft targets to the final non-autoregressive model. In FastSpeech 2, the authors do away with the teacher-student knowledge distillation pipeline and instead utilize the following to alleviate the one-to-many problem while retaining a non-autoregressive architecture:\n- ground-truth mel spectrogram is used as the training targets\n- a force alignment tool is used to extract more accurate phoneme durations\n- pitch and energy information is also used as conditioning for the model as a means to introduce variance information\n\nThese changes boost the speech quality scores of the FastSpeech 2 architecture over autoregressive TTS architectures while reducing both training and inference times considerably. The authors also introduce an end-to-end text-to-waveform variant of the model: FastSpeech 2s. This model leverages the mel-spectrogram decoder, adversarial training, and a wavenet-like architecture to enable waveform prediction at a shorter time-scale. The Fastspeech 2s architecture performs at par with autoregressive TTS models. The authors further perform ablation studies that demonstrate the importance of utilizing accurate duration, pitch and energy information.\n\nPositives:\n1. The authors explain the model architecture clearly and perform thorough evaluation and ablation studies.\n2. The changes in FastSpeech lead to improvements in performance both in terms of speech quality and time (training and inference). The quality of the samples provided is pretty good and since the models having only been trained with a publicly available dataset which is not unreasonably large shows great potential.\n\nNegatives:\n1. The authors make several claims that need substantiating. Below are a few:\n    - 2.3: The authors claim that pitch is the key feature to convey emotion and affect speech prosody while energy affects speech volume. This is not entirely correct. Emotion can be conveyed by variation in pitch as well as energy and the contribution of each of these features towards speech prosody cannot be so easily disentangled. I would recommend adding that energy is also an important factor in speech prosody. \n    - footnote 3: The authors mention that energy is not as highly variable as speech. This may be an artifact of the dataset being utilized which consists only of neutral speech. This could be included in the text as well.\n    - 3.2.2: The authors claim that since the distribution of pitch extracted from FastSpeech 2 synthesized speech is closer to groung truth distribution, the pitch contours are more natural. This claim is easier made if a comparison of time-series pitch contours is made. Including a simple DTW distance would strengthen the argument. Additionally, there are metrics proposed in prosody transfer literature[1] that can be used here.\n2. FastSpeech 2 method section is not very clearly explained. While it is understandable that there are space constraints, each component requires at least one sentence explaining its purpose and how it works. The paragraph lacks any detail of the adversarial training method and how phase retrieval happens.\n\nMinor edits:\n\"challenges lie in\" -> challenges in\n\nReferences:\n[1] Skerry-Ryan, R. J., et al. \"Towards end-to-end prosody transfer for expressive speech synthesis with tacotron.\" arXiv preprint arXiv:1803.09047 (2018).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}