{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper's main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable.  This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance.  The initial version of the paper was not well written, but the authors improved the writing significantly.  The paper includes a thorough empirical evaluation to support the main message.  I have read the paper and I believe that this work will be of interest to a diverse audience."
    },
    "Reviews": [
        {
            "title": "Review for Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation",
            "review": "The paper provides a number of adversarial attacks on hybrid neural-symbolic systems. The systems are recommender and QA systems which use an underlying knowledge-graph (KG) such as ConceptNet. Previous work has suggested that the KGs are important for good performance, and moreover that the use of KGs lends the system a degree of interpretability. The attacks are successful - maintaining performance whilst seriously degrading the KG - throwing doubt on these claims.\n\nTwo main approaches to attacking the systems are followed: a simple heuristic method in which labels in the KG are modified randomly, and a more sophisticated method in which deep reinforcement learning is used to learn an optimal policy to change the KG whilst maintaining good performance on the task.\n\nOverall I have a lot of sympathy for the motivation of this paper. There is increasing evidence that the attempted use of symbolic methods in hybrid systems, and in particular the use of symbolic methods to provide explanations, is just picking up on incidental properties of the data and exploiting the power of the deep network in ways unrelated to the symbolic representations. This paper provides further compelling evidence.\n\nHowever, the paper is currently an extremely frustrating read, and it took me a number of attempts to get through the paper to write this review. Part of the problem is that the authors have tried to cram in too much material. I would have preferred to have seen fewer experiments described, but in more detail, with the remainder briefly mentioned in a paragraph or two, or perhaps in an appendix (if that's allowed for ICLR).\n\nThe other problem is that the presentation in the paper is poor. Part of that is down to the non-native English in parts (which is not the fault of the authors), but part of it is also just down to sloppiness in the presentation.\n\nMore detailed comments\n--\n\nA KG is denoted as G = (E, R, T ), where E and R are the entity set\nand relation set respectively - this is a little confusing since T is\nalso the set of relations (tuples over E). R is the set of relation\n*labels*?\n\nFinally, both c and k are concatenated for calculating the\n plausibility score - [presuambly the concatenation is then put\n through an MLP?]\n\nIn both graph encoders, the subgraph G(q,a) together with the\naggregation weights - [neither of the descriptions mention aggregation\nweights]\n\nwhile the task is to predict the unobserved interaction (yuv = 0). -\n[does the zero here mean that the interaction is unobserved, or that\nthe user has not engaged with the item? (or both?)]\n\nwhere the aggregation weights are personalized for u - [you need to\ndefine how the aggregation works]\n\nWe randomly choose two triples (edges) in the KG and swap their\nrelations. - [it would be good to see some actual examples here from\none of the applications/KGs.]\n\nwhere sG(·) is a KG scoring function trained on G.  - [is this defined\nanywhere?]\n\nThe font in table 1 is really small. If you get more room in a final\nversion please turn this into two tables.\n\nin-house test accuracy - what's an \"in-house\" accuracy?\n\nWe then compute the relation\nspecific clustering coefficient vector c^r - do you define this anywhere?\n\nOpenBookQA (OBQA) - do you give a reference?\n\nWe randomly select 10 questions - this seems like a small sample on\nwhich to peform the human evaluation. This evaluation would be more\npersuasive with a larger sample (even 20 or 30).\n\nTypos etc. (not exhaustive)\n--\n\ndespite a deceptive symbolic structure - [I'm not sure that\n\"deceptive\" is the right word here (and lots of places elsewhere),\njust something like \"incorrect\" may be better.]\n\n that KG -> the KG\n\nThe preliminary results indicate that KG can be easily manipulated and\nlost its benefit -> lose\n\nIt brings more worrisome scenario - [rephrase]\n\nwithout noticeable performance drop -> without a noticeable\nperformance drop\n\nIn specific -> More specifically\n\ne.g., commonsense question answering (QA) and recommender system, ->\ni.e.\n\naggregating its neighbors’ embedding -> aggregating its neighbors’ embeddings\n\nThis heuristic changes the semantic -> semantics\n\nThis heuristic also does not perturb KG’s structure but its semantic\n-> This heuristic also does not perturb KG’s structure but its\nsemantics\n\non both of commonsense QA and recommendation system tasks -> on both\n commonsense QA and recommendation system tasks\n\nsince one\nof our goal -> goals\n\nhave captured the information of KG. -> the KG\n\nsequentially to LSTM -> sequentially to the LSTM\n\nEvaluating the KG on downstream task -> Evaluating the KG on a\ndownstream task\n\nRelation Swapping(RS) -> Relation Swapping (RS)\n\nbut keep the relation distribution -> keeps\n\nWe also leverage the validity scores given by human -> humans\n\nranging from gradient based(Chen - [space]\n\nAutoEncoder based(Chen - [space]\n\nwhich can lead to corrupt explanations -> which can lead to incorrect\nexplanations\n\nour RL-RR method always yield -> yields\n\nbetween entities instead of the semantic -> semantics\n\nthat investigate into the problem -> that investigate the problem\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great work on an interesting problem",
            "review": "This paper shows that knowledge graph augmented question answering and recommendation system models are so graph structure/content change invariant that by using a simple heuristic or more sophisticated reinforcement learning-based approach, we can change the knowledge graphs without significant change in the model performance. This phenomenon leads to corrupt non-sense explanations for such models' decisions/outputs.\n\nStrengths:\n- The paper is very well-written and crystal clear.\n- The idea is very interesting and novel.\n- The evaluations are relatively strong.\n\nWeak points:\n- I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples.\n- I am wondering what is the source of a more significant gap in the VALIDITY of explanations for original kg vs RL-PR in OBQA compared to CSQA. I am suggesting the authors provide an explanation for this difference.\n\nOverall, a study on the behavior of knowledge graph augmented models when they encounter the perturbed graphs is an interesting idea. The paper is also very well-written. I'd like to see the revised manuscript being accepted by the conference. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper is not properly motivated or need better justification.",
            "review": "This paper proposed to learn a RL model to modified the KG. They showed that their model can successfully deceive the KG-augmented models with most relations replaced, compared to heuristic-based strategies. \n\nI am not convinced by the motivation of this paper. The authors claimed that their model can learn to modified a KB so that a KG-augmented model can yield similar performance as before. This is under the assumption that similar performance in predicting the correct answer (e.g. accuracy) will lead to similar quality of explanation. This assumption does not always hold. If the authors really care about explanation, you should experiment with the explanation model that used KGs. \n\nThere's also a discrepancy between the authors' motivation and their evaluation. If the authors assume KG is really important in generating explanations and can be easily fooled by their framework, they should also evaluated on KG only tasks, e.g. KBQA, KBC etc, besides these KG-augmented models.\n\nSome detailed questions:\n1. Many of the downstream models that the authors evaluated on are based on GCN approaches. GCN is good at filtering irrelevant information from their neighbors. How did you make sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps?\n2. I would like to argue the LSTM is not appropriate here. LSTM is designed for \"ordered\" input. Making tricks to adapt it to un-ordered inputs is not a very good option. In each update step, the LSTM basically learns a gate variable \\sigma on the input. Can you do something like: G += \\sum \\alpha \\dot g, where g is the embedding of your single updates?\n3. In some cases, with replaced relations, the model can have better performance as the original model w/ KG (e.g. OBQA RN Acc in Table 1). Do you have an intuition why that could happen?",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting paper with interesting findings",
            "review": "The paper presents an interesting finding that some of the existing KG-augmented models, such as those for QA and item recommendation, may not actually capture or leverage the semantics in KGs, and their performance improvement cannot be attributed to the usage of additional knowledge. I think this finding is of some significance.\n\nPros: \n\n1) The paper presents four heuristic methods and an RL-based method for KG perturbation. It obtains some interesting and noteworthy findings based on the experimental results in terms of QA and recommender systems.\n\nCons:\n\n1) In my opinion, perturbing KGs by randomly or heuristically changing existing edges is not well-motivated. Although it supports the experimental study in this paper and reveals several interesting findings, it has very little practical significance, because the perturbed KG would inevitably contain many invalid and incorrect facts without positive effects for downstream applications. Hence, the technical contribution of the proposed heuristic strategies and RL model for KG perturbation is not significant.\n\n2) The analysis for experimental results is somewhat superficial. It fails to provide deep insights into why some models can still work with the perturbed KG. The authors should provide more analysis to explain the experimental results that are against common sense. For example, an experiment conclusion says that \"it is the (false) connection between entities instead of the semantic stored in the KG that leads to the improvement over non-KG baseline.\" Why? I think this is an interesting and noteworthy finding, but no in-depth analysis is given. Besides, I also have a minor question, i.e., how does the edge deletion method perturb all the triples (100%) in a KG? How many triples are left after edge deletion?\n\nSome typos: \n\n1) these false connection ->  these false connections\n2) Relation Swapping(RS) -> Relation Swapping (RS)\n\nAnyway, the paper presents a noteworthy finding, which calls for further investigation into what information from the KGs is actually captured to improve the neural-symbolic models. So, I would like to recommend a weak acceptance of this paper. \n\n--- after rebuttal ---\nThank the authors for their response which addressed my concerns. Based on the response, the revision, and other reviews, I would like to keep my score unchanged at 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}