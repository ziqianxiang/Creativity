{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduced a new ODE integration scheme that allows constant-memory gradient computation.  I was concerned that the low order of convergence of this method would make it impractical, but the authors performed extensive experiments and got impressive results.  Overall the paper addresses one of the main practical difficulties with large neural ODE models.  The authors satisfactorily addressed the reviewers' concerns in the discussion."
    },
    "Reviews": [
        {
            "title": "Neural ODEs + Invertible networks",
            "review": "Summary: \nThis paper proposes a new algorithm for solving neural ODEs. Each numerical solver step of the neural ODE is implemented as an invertible neural network via a variant of the asynchronous leafprog integrator. While still computing an accurate gradient, this allows memory savings by discarding intermediate data from the numerical integration steps since it can be reconstructed using the inverse. A theoretical stability analysis is provided. The experimental results show that the algorithm achieves similar performance to previous methods (e.g. ACA) while using less memory. \n\n\nStrengths:\n+ Identifies a nice connection between invertibility and memory efficiency. Beyond neural ODEs, this could enable use of larger models where invertible networks are useful (e.g. normalizing flows)\n+ The theoretical analysis of stability is useful to build intuition\n\n\nConcerns / weaknesses:\n- Most experiments in the paper use damping_factor $= \\eta = 1$. Since theoretically, this is not stable, it would be nice to see if the empirical improvements hold up for $\\eta < 1$, where stable regions do exist\n- The naive method seems too naive. Why are all results from all $m$ evaluations being saved? It is obvious that $m-1$ of these are unnecessary for gradient computation since they don't affect $z(T)$. The related claim about the computation graph being deeper for the naive method also seems incorrect.\n\n\nOther comments:\n- In Algorithm 1, shouldn’t $error_est = \\inf$ be inside the while loop?\n- In Algorithm 4, shouldn’t $a(T)$ be the partial derivative of $L$ wrt $z(T)$ instead of total derivative?\n- In Theorem 3.2, what is $\\sigma$? Should it be $\\sigma_i$?\n- In various locations, notation like $O(N+1)$ is used. Should this just be $O(N)$ since I assume $N$ is at least $\\Omega(1)$?\n- It seems a bit strange that we have to do a local forward and local backward pass in Algorithm 4. Could this be solved by making each layer of f invertible? In the same vein, it seems that the adjoint method needs to do a separate solve of the reverse ODE because of loss of information. If we were to assume invertibility of the forward map, is there a way to modify the adjoint method to exactly retrace the path backwards?",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "A good submission & a few questions",
            "review": "Summary: This paper presents a memory-efficient asynchronous leapfrog integrator for numerically solving neural ODEs, referred to as MALI. The method comes with a constant memory guarantee (like the adjoint method) and also guarantees reverse-time accuracy (like the adaptive checkpoint adjoint (ACA) method). The authors also give a rigorous theoretical analysis of MALI, and also discuss a \"damped\" version with an increased stability region. The method is evaluated on a variety of tasks which includes classification, dynamical modelling and generative modelling.\n\nPros:\n- The theoretical analysis looks correct, noting that I haven't worked out all the details.\n- Experimental evaluation is very exhaustive, and MALI achieves near-the-best performance in all tasks.\n- The method is proven as accurate as the standard numerical ODE solvers. Thanks to its reduced memory cost (compared to ACA), MALI can then be treated as an off-the-shelf replacement.\n\nCons and Questions:\n- Looking at the results, I'm having difficulty seeing any significant improvement upon ACA. Then the main contribution (in addition to the theoretical analysis) is the reduced memory consumption, which makes me rethink whether ICLR is a suitable venue.\n- Although the memory costs of the adjoint method and MALI are $O(N_f)$ and $O(N_f+1)$, this doesn't really reflect in Figure 4c, where the blue bar doubles the red one. I'd be happy if the authors can briefly explain why\n- Looking at Table-2, why does the test performance of a NODE trained with MALI increase when we switch from MALI to RK4? It would be much nicer to see some error variance estimate.\n- I would be happy to see an experimental evaluation of the \"A-stability\". As mentioned by the authors, the stability analysis is asymptotic and T could be arbitrarily small in, e.g., continuous-time flows. However, that's not the case in time-series modelling. So I wonder if the stability claim can be verified on a scenario in which, e.g., 100 observations arrive uniformly in time with 10 secs gaps.\n- To generate Table-2, did you train a ResNet without any differentials/integrals involved and try to evaluate the test performance using an ODE solver (simply using the trained ResNet as the drift function)? If so, I don't think this makes any sense except Euler-1 solver, and the entire ResNet row in Table-2 could go away.\n\nAdditional comments:\n- Figure-4 caption could include some more detail (at least mentioning the experiment name) \n- Why is there a \"local forward\" step within the for loop in the backward routine in Alg.4?\n- It would be nice to see a brief description of the mujoco dataset.\n- Typo in the title of section B.3.2.\n\nNote: After rebuttal, I increase my overall score from 6 to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Gradient estimation of neural ODEs with constant memory footprint analysed and compared",
            "review": "1) Summary\nThe manuscript proposes a reversible integration scheme for approximately estimating the gradient of neural ordinary differential equations. These ODEs represent DNNs with continuous rather than discrete values for the number of layers. The solver is theoretically analysed and empirically compared to other solvers.\n\n2) Strengths\n+ The paper is mostly well written.\n+ The reversibility property of the solver leads to a memory footprint that does not depend on integration time.\n+ The model is applied to standard datasets.\n\n3) Concerns\n- The concept of Neural ODE models, their scope and their expected usefulness should be better motivated. It is not obvious which role these models play and what they offer as potential strengths.\n- The integrations scheme seems already known and well established.\n- It does not seem that the paper makes code and data available to the public.\n\n4) Remarks/Questions\n  a) Algorithm 1: It seems that the stepsize h should be initialized upon every step. Otherwise the steps can only get smaller. \n  b) References: capitalization not correct e.g. \"neural information processing systems\", \"ode-net\", \"lennard-jones\"\n  c) What benefits does the neural ODE model have in the context of image classification? What is the intuition behind the \"continuous depth\" idea in this scenario?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Simple method with convincing experimental results, although there are some clarity issues",
            "review": "**Paper summary**\nThere are typically two methods for estimating the gradients with respect to the loss for neural ODEs. The naive method directly backpropagates through the steps of the ODE solver leading to accurate gradients but very large memory cost. The adjoint method in contrast does not store the entire trajectory in memory, but has reverse trajectory errors (i.e. the numerical solution in the reverse direction will not be the inverse of the numerical solution in the forward direction). In this paper, the authors propose a method that is both reverse accurate and has low memory cost.\n\nTo achieve this the authors take advantage of the asynchronous leapfrog solver. This numerical method is reversible: solving an ODE numerically in the reverse direction is the inverse of solving the ODE numerically in the forward direction. This is not generally true for the ODE solvers typically used (RK4 and so on) in the neural ODE literature. As the numerical solver is explicitly invertible, the authors can (from only the final state and not the entire trajectory) locally reconstruct each ODE solver step to get the local gradient of the parameters with respect to the loss. They can then calculate these gradients along the entire reverse trajectory to obtain an accurate estimate of the gradient with respect to the parameters. As each step of the numerical solver is reversible, they do not need to store the entire trajectory. The authors analyse the stability and numerical error of their proposed method and provide a toy example to show how well their method estimates gradients compared the naive, adjoint and adaptive checkpoint methods.\n\nThe authors then perform experiments on a variety of tasks to test their model. They test their model on image classification experiments, both on CIFAR10 and Imagenet and achieve good results compared to the baselines. In addition, they perform adversarial robustness experiments on ImageNet and also show good performance. Finally, the authors test their method both for time series modeling and continuous normalizing flows, again showing good performance compared with naive integration methods.\n\n**Positives**\n- The motivation and core idea of the paper is clear. Numerical solvers are in general not reversible and this can lead to inaccurate gradient estimates when using the adjoint method. The authors explain this clearly and then propose a method that effectively solves this.\n- The experimental results are quite impressive. The model performs on par with the adaptive checkpoint method in terms of accuracy but is much more memory efficient (and notably memory is independent of the number of integration steps). This allows the authors to run their model on large scale datasets like ImageNet which was not previously possible with most neural ODE methods. Further, the authors achieve good performance on quite a wide variety of tasks (image classification, adversarial attacks, time series modeling, generative modeling) which is nice.\n- The authors perform a thorough analysis of the runtime of their integration method compared to others which is very helpful.\n\n**Negatives**\n- The presentation of the method and results is not always very clear. For example, the section about damping for the ALF integrator is not clear. The authors mention that ALF is not stable for eta=1, but (as far as I can tell) never mention what value of eta they use in practice and whether choosing this value is difficult. Further, it is not clear if ALF is still reversible with this eta parameter. Presumably you would have to use 1/eta in the reverse step for it to remain invertible, in which case the reverse is not stable? The authors should be more clear about this.\n- The toy example is confusing. How come the integration time starts from t=20? Is this because the error only grows after t=20? As you use T=1 for all experiments (and the rtol and atol are also roughly the same for all experiments), it would be nice to see if this actually makes a difference also for t<1. In Figure 4, the authors also mention the derivative dL/dy_0 but this derivative is never mentioned in the text. Do you mean dL/dz_0? The plots of memory consumption are nice and clear though.\n- The ALF solver already exists, so the main contribution of the paper is simply to apply the ALF solver to neural ODEs. This means that the novelty of the method is somewhat limited, but I do not think that this is a major issue as the method works well and is clearly motivated.\n- The section about using different numerical solvers for ResNets does not make much sense. ResNets are not designed as flows and do not behave as flows in practice, so we should not expect them to work at all with other numerical solvers than Euler with timestep=1. I don’t really think these experiments show anything interesting and should be removed for clarity.\n\n**Recommendation**\nOverall the paper has a clear motivation, provides a nice and simple solution to an interesting problem and has good experimental results. However, there are some clarity issues which make some aspects of the model and method confusing. I therefore recommend a weak accept but would increase my score if the clarity issues are solved.\n\n**Questions**\nThe model achieves extremely good bits/dim on MNIST (0.87). However, it seems from the appendix that the samples are fairly poor (compared to vanilla FFJORD for example). Log likelihood and sample quality are not always correlated, but the difference seems particularly jarring here. Do you know why this is?\n\n**Typos and small comments**\n- In many places the authors use latex math to write words that should either just be written in italics (w.r.t) or using \\text{} in math mode (e.g. atol, rtol).\n- There are several typos in the script, so I think it would be a good idea for the authors to read through the script again to fix those.\n- In several places, the authors write O(N_f + 1) which instead should be O(N_f)\n- The authors often write “constant memory cost with respect to integration time”. I think it would be more helpful to say “number of solver steps” or something along those lines as integration time typically refers to the upper limit of the integral when solving an ODE.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}