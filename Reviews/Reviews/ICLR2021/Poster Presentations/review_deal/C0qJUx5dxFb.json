{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes to learn an ensemble of weights given a set of base weights from some point late in normal training. The authors apply this approach to a number of configurations and find modest performance improvements for normal test settings and larger improvements for out of distribution settings. While reviewers had some concerns about the size of the improvement relative to baselines, all reviewers agreed that the proposed method is interesting and will likely impact future work, especially given the new experiments provided by the authors. I recommend that the paper be accepted. "
    },
    "Reviews": [
        {
            "title": "Official review",
            "review": "\n### Summary\nThe authors propose late-phase weights, a method of updating the weights near the end of training via a splitting and ensembling mechanism. They analyze the benefits in the noisy quadratic setting. The method improves validation performance on a range of image recognition tasks and on enwiki8.\n\n### Comments\n* The weight interaction functions $h$ should be more explicitly defined rather than just described in text.\n* The paper is overall well written and flows smoothly.\n* I think there should be more discussion on the choice of $T_0$. For example, in table 1, why does SGD perform worse when $T_0=0$? It would be good to get a sense of robustness to this hyperparameter.\n* Good results on CIFAR. Late-phase weights are shown to boost performance over SGD and to be complementary with SWA. There are some benefits in the OOD setting as well.\n\n### Recommendation / Justification\nI vote to accept the paper. The idea is interesting, well-motivated, and seems straightforward to incorporate into existing pipelines. However, the improvements seems modest in some settings (e.g. ImageNet) and for the best performance, it seems like we should still stick to Deep Ensembles.\n\n### Questions\n* On the ImageNet experiments, what is the validation accuracy of the pre-trained model?\n* Can you comment on the computaional and memory complexity of your algorithm versus vanilla SGD?\n* In the comparisons between late phase weights and SGD, do both algorithms consume the same amount of data? If so, this would be good to mention.\n* Could the entire network be treated as \"late-phase weights\"? Would this help performance?\n\n### Minor comments\n* I would consider alluding to possible choices of the weight interaction functions $h$ when it is first introduced at the start of 2.1.\n* In Algorithm 1: How does the loss function consume three inputs? This is different from when it is initially described.  \n* It's a bit unclear what is being compared in Figure 2.\n\n\n(increased score from 6 to 7)\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "Summary:\n\nThe paper proposes a method to improve solutions found by SGD by ensembling subsets of weights in late-phase. A family of low-dimensional late-phase methods are analyzed and shown to improve generalization in CIFAR-10/100, ImageNet and enwik8. Authors also analyze the method in more tractable noisy quadratic settings. \n\nContribution of the authors is that rather obtaining ensemble they utilize efficient ensemble to guide SGD training and ultimately obtain a single model. \n\nReason for score:\n\nWhile the paper discusses efficient ways of utilizing late-phase weight ensemble and improving SGD training, the demonstrated benefit is not significant enough for practitioners to pursue the method. Without strong practical application potential, merit of the proposed method is weak since it does not obviously elucidate some aspects of neural network training. \n\nPros:\n\nThe paper is clearly written and easy to understand the proposed method is. It is well structured that helps to improve the clarity. \n\nProposed method tackles a significant problem in the standard ensemble method in which both training/inference computation can be quite costly. The paper’s method only ensembles subset of weights therefore added training cost is minimal and since inference is done on averaged weight, it becomes essentially a single model.\n\nAmong various late-phase schemes, BatchNorm late-phase seems to work well which is widely used among vision models so easily applicable. Also since late-phase can be applied post-pretraining, it can be used to improve pre-trained models.  \n\nAs far as I can tell various experimental conditions are very well controlled and thoughtfully designed.\n\nCons:\nThe idea of weight averaging is not so novel as duly noted by the authors.\n\nMain question arises for the paper is whether the proposed method is worth the effort. While all experiments show that the proposed method improves the baseline somewhat, deep ensemble baselines remain strong. Also quoted difference between methods does not mean statistically significant effect (see Vincent Vanhoucke’s article on reporting significant figures https://towardsdatascience.com/digit-significance-in-machine-learning-dea05dd6b85b). According to this article, results reported in Table 1, CIFAR-10 in WRN, a significant figure with a 10k test set should be around 0.2% and differences between different methods are at best marginal. This can be applied to most tables and except for Deep Ensemble’s improvement other differences are not very significant.\n\t\nI wonder as discussed by the authors, this is due to mostly the benefit of ensembles is\nthrough incorporating different modes as argued in [Fort et al., 2020] rather than a single mode. I imagine a single mode ensemble could be beneficial when variance within the mode is large, however for models considered by the authors seem to have small model variance which minimizes effect of technique utilizing single mode. \n\nWhile \\sigma_0 and T_0 are hyperparameters of the algorithm, no good way to determine it is explained. \n\nThe role of section 3.1 is not clear. For one thing, the legend in Figure 1 is confusing where the role of non-integer K is mysterious to me. I would suggest clarifying what the message of the section would be in context of understanding late-phase weight models. \n\n\nNits and additional feedback:\n\nAnonymized link is neither there in the main paper or included as supplementary material. If the authors intended to include the code, this is a note that code can not be found to the reviewers.\n\nFor models that do not use BatchNorm, I believe most interest to practitioners would be using Transformer based models. I wonder if rank-1 late-phase or LayerNorm late-phase would show improvements in this case. \n\nWas “Late-phase classification layers” ever evaluated or discussed in the main paper? I find some discussion on the appendix but seem to be missing in the main text. \n\n---\nI thank the authors for their hard work addressing issues raised by the reviewers.\n\nAuthors have answered many issues pointed out (by improved performance and showing robustness to hyperparameters) and I've increased my score from 5 to 6, and support accepting the paper. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "review",
            "review": "This work suggests a variant of ensembling that is more compute-efficient. Specifically, it involves forking an ensemble only in the late stage of training, and forming this ensemble via a \"low-dimentional\" family. That is, instead of maintaining independent networks, maintain only \"low-rank\"-style perturbations of the base network (for various instanciations of \"low-rank\").\nThe experimental results are somewhat limited, but appear to be competitive with current efficient-ensembling approaches like SWA/SWAG. The absolute improvement of this method is not very large (<0.3% on CIFAR, <0.2% on imagenet), and there is a large gap to Deep Ensembles. I weakly recommend acceptance, because the method appears promising for future work, and the experiments seem correct. \n\nThere is also a theory section included, though I am generally unconvinced by results in such simple toy examples.\n(such settings can usually be contrived to exhibit any desired behavior)\n\nWeaknesses:\n- The experimental section would be greatly strengthened by additional experiments for different models and settings. There are only 2 architectures tested on CIFAR-10, for example. It would also be informative to see the performance of these methods in \"harder\" settings -- for example, CIFAR-10 with fewer train samples.\n- The OOD uncertainty results could be expanded. Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods, so it is especially important to evaluate ensembles in this context. Currently aggregate results are shown in Table 4, but it would be good to explicitly see, for example: how the performance of this method degrades with increasing CIFAR-10C corruption severity, as opposed to Deep Ensembles. Also, reporting the Mean Corruption Error (mCE) for each dataset individually will allow standard comparison to prior methods.\n\nComments which do not affect the score:\nIt seems that starting the ensembling at a \"late phase\" in training is the main contribution of this work. This could be applied to any ensemble method, and you propose several explicit instantiations. It could help to focus the writing in terms of this contribution, and also to further investigate the role of T0 (the time at which ensembling starts).\n\n---\nEdit after rebuttal: Increased score from 6 to 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "I think both the methodology and the writing need to be improved. ",
            "review": "To improve the generalization performance of SGD methods, \nthis paper proposes to use an efficient ensemble-like approach \nwhich computes an average of an ensemble of SGD weights\nwhen retrained from some late-phase of SGD dynamics. \nThis idea is different to most recent ensemble-based approaches which \naim to average the predictions of the models. \n\nThe paper focuses on some specific layers of neural networks \nin order to apply the late-phase training. \nThe batch normalization layers are shown to be \nsimple and effective. Some other layers are also analyzed, \nincluding a recently introduced rank-1 multiplicative matrix \nweights idea for full-connected layers. \nSection 3 presents the numerical results and show that the generalization of SGD \nis more-or-less improved on various benchmarks.\nExplanation of why the generalization is improved in relation with the flatness\nof energy landscape is also discussed.  \n\nI find that this approach is quite sensitive the choice of the hyper-parameters, \nsuch as the beginning of the late-phase T0, and the noise perturbation sigma0. \nIt is written in Section 2.1 that in practice … sigma0>0 yields a set of models …\nthis results in improved final generalization. However, in the result of ImageNet in Section 3.3, \nthe sigma0 equals to 0. Thus, it is not conclusive that sigma0>0 is better. \nAs the improvement in Section 3.3 seems marginal compared to the baseline and the \nstandard deviation, it thus does not fully support the effectiveness of the batch normalization layers. \nI would recommend using some other dataset or models, \nbut with a more consistent set of hyper-parameters. \n\nIn terms of writing, I would recommend to write out\nthe full algorithm of Alg. 1 or at least in the Appendix, \nincluding the variant of the SGD momentum and Adam. \nThe SWA is also worth writing out clearly, which is not clear to the reader. \nIs the DeepEnsemble result in Table 1 from SGD or SWA? \nThis is not clear from the text.\n\nOverall, I think both the methodology and the writing need to be improved. \n\n##\nThe revisions made by the authors have addressed all my concerns. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}