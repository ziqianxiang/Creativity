{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper has merits on providing a particular way of understanding a prediction model based on auxiliary data (concepts). I have a generally more positive view of it, aligned with the higher-scoring reviews. However, I feel a bit uncomfortable of framing it as \"causal\" in the sense it does not aim to provide any causal predictions, but it is more of a smoothing method for capturing signal contaminated with \"uninteresting\" latent sources - this is more akin to regression with measurement error (see e.g. Carroll, Ruppert and Stefanski's \"Nonlinear regression with measurement error\") where, like in this paper, different definitions of \"instrumental variables\" also exist and are different from the causal inference definition. I can see though why we may want to provide a causal interpretation in order to justify particular assumptions, not unlike interesting lines of work from Scholkopf's take on causality. The paper can be strengthened by some further discussion on the assumptions made about additivity on equations (2) and (3), which feel strong and not particularly welcome in many applications.\n\nThe proposed title is still a bit clunky, I feel that the two-stage approach is less important than the structural assumptions made, perhaps a title emphasizing the latter rather than the former would be more promising."
    },
    "Reviews": [
        {
            "title": "Interesting direction but not convincing enough",
            "review": "Summary: The paper studies the causal nature of concept explanations. Specifically, the authors treat labels as instrumental variables to then debias explanations and improve predictive performance as well.\n\nStrengths\n- Figure 2 was helpful for understanding the contributions of this work. However, it is not clear if \\hat_d captures the same concepts that c alone would. If you could motivate using \\hat_d with a pictorial example of where c, the concepts alone, fail that would be helpful.\n- The use of ROAR for concepts is clever and does show the utility of the method, but additional experimentation to show the concepts captured align with human intuition would have been nice. At the minimum, showing how the concepts recovered by the proposed method and CBM differ would be helpful.\n\nWeaknesses\n- While both experiments (synthetic and BIRDs) show the method's utility, it would have been nice to see experiments on other datasets. OAI perhaps like in Koh et al.\n- The connection to Yeh et al. is not clear to me. What is the notion of completeness in the proposed method? \n\nQuestion\n- Can you extract uncertainty estimates for concepts from Equation 5?\n- Can you please explicitly the utility of your method in the linear Gaussian case? It seems as though using \\hat_d for the concepts simply recovers the independent concept case from Koh et al.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Concerns about the model",
            "review": "The focus of the work is on model interpretability using concept-based explanation. The authors consider the issue of concepts being correlated with confounding information in the features. They propose a causal graph for representing the system and use instrumental variable methods to remove the impact of unobserved confounders. The proposed method is evaluated on synthetic and real data.\n\nI have the following comments:\n\n(1) The authors provide a graphical modelings for the setup of the problem. They assume that: 1. There is an \"unconfounded concept\" generated only from the label, 2. There is no confounding effect on the label.\n\nUnfortunately, no arguments is provided for justifying the conditional independence assumptions in the model. Any edge that is removed from a graphical model implies conditional independency assumptions and they should be carefully justified, specially since the topic of the work is interpretability. This includes the conditional independencies above as well as the way it is assumed that variable c is generated.\n\n(2) The model in this work is different from an IV model: One of the main requirements of the IV framework is exclusion restriction which requires that the effect of the instrument variable on the outcome should be only through the treatment variable. In the proposed model, variable y is also directly connected to variable x. Also, d is not confounded or observed, which again make the model different from IV model. Therefore, the model in this work does not represent the IV model, and y is not a valid IV, although it seems that that was actually not used in the approach. Only an independence assumption is actually used in the approach, which as mentioned above, is not justified.\n\n(3) It is not quite clear what exactly the variable d represents compared to variable c, and what is its interpretation.\n\n(4) In the synthetic simulations, the value of the variance chosen, specially for noises is very small. It is important to see the performance for larger values for the variance of the noises.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, with a clever use of instrumental variables. I liked the results but I would have liked a clear description of the scope in which the method can be applied.",
            "review": "Abstract: The paper proposed a way to learn unbiased (debiased) concept-based explainable models in the presence of unobserved confounders by the use of labels as instrumental variables. The proposed algorithm has 3 main steps: (1) regresses concept labels from the final labels (2) replace the original concepts with the learnt concepts and learn a model of debiased concepts as a function of features (3) predict label as a function of debiased concepts. The authors show in the experimental section that their training method captures the most salient concepts using the ROAR (Remove and Retrain) evaluation framework much better than the vanilla (non de-biased) approach. \n\nPros: \n- Clarity: The paper is quite well-written and the explanations are clear. \n- Significance/Impact: The ability to explain modeling prediction is quite important and furthermore it can lead to better generalization and robustness\n- Experimental design: I really liked the experimental section, very clear and well-done\n\nCons:\n- The causal graph assumption \n- Motivation: I would have spent a bit more time on explaining the motivation of the work and the scope that it adresses. For example, the causal graph assumption that makes labels to be the causal root of the graph, works well in image classification, but not in learning-to-control applications, such as recommender systems where the label is the reward and is the final consequence of the causal chain.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper whose exposition needs improvement",
            "review": "Overview of the Paper\nThis paper proposes a procedure to 'debias' and account for confounding while using 'concepts' as interpretations for black-box models. The approach proposes a causal graph, and then notes based on the proposed graph that sample labels satisfy conditions required of an instrumental variable. Given an instrumental variable, then they propose a multistage approach to debias the interpretations. In general, this paper provides a useful approach for helping to generate concept explanations that are not confounded with the label or other features. \n\nGeneral Feedback / Quality\nThis paper contains several interesting ideas. I believe this is the first paper to bring the idea of using instrumental variables as debiasing approaches for explanations. Having this said, I cannot recommend the paper in its current form because the exposition makes the paper a tough read. For example, this paper builds on the concept bottleneck approach of Koh et. al. from ICML 2020. However, no contained overview of that approach is given. The only reason I am able to follow the discussion is because I previously read that the Koh et. al. paper and I am generally familiar with instrumental variables. A more damning issue is that the paper does not explicitly set out the question/problem that it seeks to address. As it stands, figure 1 provides a vague motivation, but it is not clearly contextualized. The dataset and the discussion of concepts is not explicitly presented, so one is left to guess how the dataset was processed etc. I have worked with the CUB birds dataset before, so I am familiar with the concepts and data. My takeaway is that the paper requires a substantial rewrite that hand-holds a reader through the work. I document some of the other questions I had in the latter parts of this review. \n\nOriginality\nThe use of instrumental variable methods for explanation debiasing is new and original. The intersection of causal inference and post-hoc interpretability is a new and emerging area, so this work is one of the first to set out this path. There is other work in this area, but these do not address concept bottleneck models. \n\nSignificance\nAs noted above, the main idea and goal of this work is important. Overall, it is important to obtain explanations from models that are not confounded with other variables. It is also likely important for diagnosing bugs. \n\nClarity and Writing\nThe paper is mostly free of typos, but as it stands it is quite difficult to read. Several important concepts are not properly introduced, and a substantial rewrite is likely needed. See the overview for a discussion of some of these issues. \n\nSome other questions/concerns\n\n- Conceptual Confusion: Throughout this paper, it is unclear to how the methods here should be operationalized. Do the authors envision that instead of computing standard TCAVs or concepts, these concepts would first be debiased via the an instrumental variable following the protocol described here? \n\n- The causal graph: I am not an expert on causal inference but I wanted to get the authors' opinion on whether the proposed causal graphs are standard in that literature. Take fig. 2a., the causal graph says that the labels cause the concepts, which then cause the features. However, could this also be the other way? Here is an example: is it the case that calling an object a chair causes the object to have four legs or does an object with four legs cause or induce a label chair? I think the second graph would not fit under the scheme presented, but I am curious if this is the case. Primarily, I don't think the model labels can be used as instruments in such setting, but perhaps the framework is still appropriate? \n\n- The evaluation. I think the paper missed an opportunity the show a wow factor here. From the CUB dataset, it seems like the take away is that the debiased concept interpretations can identify a spurious correlation that a standard concept interpretation cannot? If this is the case, then that is a substantial positive for the approach. Instead of the ROAR evaluation, the authors should have created models with artificial spurious correlations and show that standard concept methods are unhelpful. \n\n- The ROAR approach used in this work is not an appropriate form of evaluation. I agree with the authors that explanation evaluation is difficult but the ROAR metric does not do this. The method retrains a model after masking out some highly important features, and then interprets the retrain model in comparison to the original one. This is not a comparison, especially when the models that are being compared a not convex. There is no guarantee that a retrained model relies on the same set of rules as the original model.  I would suggest the authors create toy settings (with real or toy datasets) where the DGP can be easily manipulated and show the promise of their approach that way. \n\n------\nPost Reponse Update\n-----\nThanks to the authors for their updates. I have updated my score by 1 point here. I believe the exposition in the paper could still be improved at this point. In general, this work provides an interesting use of IV techniques for interpreting black box models.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}