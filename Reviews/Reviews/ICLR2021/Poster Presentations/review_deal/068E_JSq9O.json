{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents new contrastive based self-supervised objective based on Chi squared divergence that helps with mini batch sensitivity, training stability and improved downstream performance.\nAn accept."
    },
    "Reviews": [
        {
            "title": "Incremental contribution to contrastive learning objective",
            "review": "This paper proposes a new objective for self-supervised contrastive learning. In the general framework proposed by Tsai et al. (2020b), the proposed method boils down to using a divergence related to $\\chi^2$-divergence. Compared to other objectives for contrastive learning, the authors illustrate the advantages of the proposed one in training stability (or easiness to train), sensitivity to batch size, and downstream task performance. However, introducing three new hyperparameters is a cause of concern since they make it more difficult to select optimal hyperparameters. Also, some important details of the experiments are missing. For example, how many runs to obtain the results shown in Tables 2 & 3? What's the confidence interval on the results? Any test to establish the statistical significance? What are the settings for supervised training? When the authors compare the results among different methods, did they select the optimal hyperparameters (e.g., learning rate) separately for each method?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper presents a new contrastive representation objective that has good training stability, minibatch size sensitivity, and downstream task performance. ",
            "review": "This paper presents a new contrastive representation objective that has good training stability, minibatch size sensitivity, and downstream task performance. This objective is a generalization of Chi-square divergence, the optimal solution is the density ratio of joint distribution and product of marginal distributions, the estimation is consistent and its variance goes to 0 as sample size goes to infinite, so the paper is theoretical sound. The authors conduct comprehensive experiments to show that the training based on this objective is stable, not sensitive to batch size and leads to good downstream task performance  in vision and phoneme and speaker classification. \n\nHowever the theoretical results don't provide any clue to explain why this estimation leads to better downstream task performance, where in mutual information (MI) estimator it is easy to understand since MI is a good measure of dependency of two random variables. Section 3.5 states the relation to MI estimation, but it is just a plug-in method that won't be able to say anything on the goodness. \n\nFurthermore the paper misses an important reference, David McAllester and Karl Stratos, Formal Limitations on the Measurement of Mutual Information, AISTATS 2020, where David and Karl propose a method called DoE that has a good estimation of mutual information even when the mutual information is large. So I think the authors should compare RPC with DoE in the synthetic data experiment in Section 3.5 when the mutual information is large, say 100, instead of 10. In the case of large MI, all other methods fail to provide a good estimation.\n\nFrom my experience, when CPC is applied to ASR, the batch size is not a sensitive factor to WER results. \n\nIn the notation and In section 2.1, second line from bottom, I don't think it is appropriate to use joint distribution for a related or positive pair, product of marginal distributions for an un-related or negative pair.  It is the density ratio matters. For positive pair (X,Y), its MI is large, and for negative pair (X,Y), its MI close to 0. \n\nIn the proof of Lemma 5, there is a typo in the second line from bottom, the second term should be expection of P(x)P(Y), not P(X,Y).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid contribution to contrastive representation learning",
            "review": "The authors provide a clear review of different divergences used in contrastive learning and their relative strengths and weaknesses in terms of training stability, minibatch size dependence, and usefulness on downstream tasks. This motivates the need for a new divergence which they introduce based upon chi-squared divergence.\n\nThey provide strong empirical and theoretical support for the new divergence, with extensive experiments on large-scale image and speech classification tasks. They also perform comparison studies across batch size and training stability that support their earlier arguments, and a hyperparameter sweep across term weights to make it clearer how to tune them in later work. Further, they demonstrate the decreased bias and variance in MI estimation experiments.\n\nThe paper is well-written, and provides helpful context to not just motivate the value of the new technique, but quantitatively and qualitatively contrast with existing techniques that helps inform the reader about the broader field. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direciton towards stable self-supervised training objectives",
            "review": "Authors propose RPC (Relative Predictive Coding), which is supposed to improve training stability (with chi-squared distance based regularization), minibatch size sensitivity (avoid sampling large batches), and downstream task performance (show generalization).\nAuthors discuss estimation of MI and/or probability ratio (of related divided by unrelated egs). Proposed solution stably estimates this. Experiments are convincing. It is a good direction in self-supervised training with convenient training schemes.\nSome weaknesses:\n1. Fix alpha=0 and find ratio or values of beta and gamma which gives maximum performance. It would be interesting since low value of alpha is also giving good performance.\n2. While discussing sensitivity to batch size, larger batch sizes should be tried since it is discussed in the initial part of paper that SimCLRv2 requires huge batch size.\n3. Since the proposal is generic, can authors give a word on using this on something other than SimCLRv2?\n4. Rather than reporting specific values of alpha, beta, gamma (the proposed \"relative parameters\"), if results can be reported in graph format, it would be vastly more helpful. For e.g. fix alpha=0.001 and x and y axis of plot could be other two relative parameters. (this is related to my point 1)",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}