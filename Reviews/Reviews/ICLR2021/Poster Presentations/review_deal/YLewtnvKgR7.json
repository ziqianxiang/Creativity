{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The initial reviews were mixed (2 positive, 2 negative).  The main concerns were about presentation issues: unclear contribution or main point; unclear analysis of figures; missing some motivation of selecting object detectors; etc.).  On the other hand, reviewers appreciated the well-formulated paper, analysis and recommendations from the experiments; \n\nThe author response addressed the presentation issues and added additional motivations and clarifications. All reviewers in the end recommended accept. "
    },
    "Reviews": [
        {
            "title": "reviews ",
            "review": "This paper explores the predictive uncertainty estimation problem in object detection. They observe that the commonly used NLL loss leads to high entropy predictive distributions but regardless of the correctness of the output mean. Instead, they use energy score as a non-local proper scoring rule. They also propose an alternative evaluation method. \n\nStrengthens:\n1. The paper is well formulated. \n2. The authors give a thorough analysis of the experimental results. \n\nWeaknesses and Questions:\nI am confused by the main contribution of this paper. Although the authors summary their observations at the end of this paper. I didn't clearly get the differences in the analysis part. \n1. No single proper scoring rule can capture all the desirable properties of object detection seems common sense for me. As object detection is a complicated problem that includes both region classification and localization. \n2. In Table-1, what does it mean the results of DMM and ES for DETR are much better than NLL , but incremental for RetinaNet and Faster R-CNN?\n3. I cannot figure out enough differences between NLL, DMM, and ES from the analysis figures in the experimental part.\n\nI hope the authors can highlight their technical contributions during the rebuttal. I am glad to hear more high-level and straightforward statements for their contributions.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "strong paper with a very good evaluation and takeaways for future research",
            "review": "#### Summary\nThis paper examines the question of how to best estimate the bounding box distribution in probabilistic object detectors, specially regarding the use of non proper scoring rules and issues with the standard negative log-likelihood (NLL, where there is no ground truth for predictive variance).\n\nI believe this paper has many interesting findings that will be useful for the design and training of future probabilistic object detectors. Two main takeaways are that the NLL is not appropriate for training bounding box regressors since it produces high entropy distributions, penalizing overconfident predictions too much. The second message is that the way object detectors select bounding box regression targets with an IoU > 0.5 threshold affects uncertainty in the bounding boxes, since this prevents variations across all IoU ranges to be presented and learned by the detector.\n\n#### Reasons for Score\nThis paper provides a much needed evaluation of probabilistic object detection from a statistical point of view. Many Computer Vision researchers focus into chasing state of the art metrics (mAP, PDQ, etc) and do not put enough attention into statistical issues such as using non-proper scoring rules, which can hide details and hinder progress.\nI believe that this paper will bring a breath of fresh air to the probabilistic object detection field, from its takeaways there are clear directions to improve this kind of detectors and advance the state of the art into other than just chasing metrics. The quality of predictive uncertainty is very important for safe applications of object detection, for example in autonomous driving or robot perception.\n\n#### Pros\n- Very good evaluation, with multiple datasets (COCO, OpenImages, and corrupted variations of COCO), producing a robust results and conclusions.\n- Good selection of object detectors (DETR, Faster R-CNN, RetinaNet), including one stage and two stage detectors and the recent DETR that proposes a set-based approach to object detection.\n- Clear recommendations for future research: use the energy score for training, evaluate with multiple scoring rules, and put attention in the way how bounding box regression targets are selected, with a varied selection of targets being the best, instead of being limited to higher IoU values. We should also rethink the way probabilistic object detectors are evaluated, using proper scoring rules and multiple metrics, since the standard mAP and PDQ can hide details and differences between detectors.\n- Using the energy score even for training has a ~2% improvement in mAP over NLL which should also be noted.\n- This paper provides a more statistical view on object detection, focused on the bounding box regression problem of a probabilistic object detector, which is not that common in the literature, specially in Computer Vision.\n- The paper is well written and a pleasure to read.\n\n#### Cons\n- I believe that the energy score might be a bit problematic for object detection, since it requires sampling from a Gaussian, the loss then becomes stochastic (adding noise to the process) and sampling bounding boxes might become a bottleneck during training. This is a minor issue.\n- The paper focuses into predictive uncertainty, instead of separating epistemic and aleatoric uncertainty. This is explicitly mentioned in the paper and only a minor issue, since separating both kinds of uncertainty is difficult.\n\n#### Questions for Rebuttal Period\nCan you motivate the selection of object detectors? There are many probabilistic object detectors, including Gaussian YOLO for example, so a motivation for these specific detectors would be an improvement.\nCan you also motivate the selection of the scoring rules? Why is were the energy score and DMM selected, were other alternatives considered?\n\n#### Minor Issues\n- In Section 3.3 and Figure 1, please specify the values of p for DMM and M for the energy score (I assume it is 1000 as mentioned in the Appendix) that are used for evaluation.\n- In Figure 3, please add that these results are produced using DETR to the caption, it will be easier to read and interpret.\n- This paper has a very statistical view on the object detection problem, maybe it is worth to also take a look at per-class metrics and visualize the produced bounding boxes, some of this analysis could be added to the appendix. Aggregated metrics could also hide details in some classes.\n- I think the title could be more informative, since this work focuses into bounding regression uncertainty/variance, this could be part of the title.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper explores a new perspective and has comprehensive experiments but some claims are not convincing enough.",
            "review": "Summary:\n\nThis paper gives an empirical analysis of the predictive uncertainty for object detection and proposes a tool to establish and evaluate the uncertainty. The main claim of this paper is that, for the bounding box regression, non-local based algorithms have superiority in the consistency with the predictive distributions than local-based methods. The experiments are conducted on some state-of-the-art detectors and fairly demonstrate them.\n\nStrength:\n\nThe ablation studies are sufficient and insightful, especially three kinds of state-of-the-art detectors and two important datasets are explored.\nThe analysis is technical sound and considerably well written.\nThe performance gain on the transformer based detector (DETR) looks satisfactory.  \n\nWeakness:\nSection3.3 presents a toy example to show the properties of NLL and the authors claim the weakness of local-based algorithms comes from penalizing lower entropy distributions more severely than higher entropy distributions. But I think this claim is not convincing enough.\nThe effectiveness of conventional detectors (Faster RCNN and RetinanNet) is relatively limited. Specifically ES has only 0.16 mAP absolute gains over NLL.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Exploring non-local proper scoring rule and assessing predictive distributions for detectors. ",
            "review": "This paper introduces several ways to assess probabilistic object detectors that output predictive distributions. Moreover, it presents a proper and non-local scoring rule for training such probabilistic object detectors.  The model backbone consists of standard detectors (DETR, RetinaNet, FasterRCNN) with a variance regression head and a mean bounding box regression head.  The loss functions explored in the paper are the negative log likelihood (NLL, which is local and proper), energy score (ES, which is non-local and proper), and direct moment matching (DMM, which is non-local and non-proper), so this means that nine different combinations of detectors and losses can be formed.   The assessment uses NLL, MSE, ES and Brier Score on four partitions of the detection results (based on their IOU with ground truth): true positives, duplicates, localization errors, and false positives.  The training set is COCO, where there are three testing sets: the in-distribution test set is the COCO validation set, the artificially shifted data is the COCO validation dataset modified with 18 different image corruptions (Hendrycks & Dietterich, 2019), and the naturally shifted data is the OpenImages dataset (Kuznetsova et al., 2020).\n\nThe main results found by the paper are: 1) none of the proper score functions alone enables a complete analysis of the probabilistic object detectors -- for a thorough analysis, one should use both local and non-local proper scoring functions on multiple testing partitions; 2) proper scoring functions not necessarily provide good predictive uncertainty estimates for probabilistic object detectors, and non-local scoring functions allow better calibrated and lower entropy predictive distributions than local functions; 3) training based on IOU restricts results to high IOU candidates, which can lead to unreliable bounding box predictive uncertainty estimates; and 4) variance regression outputs large predictive differential entropy when presented with out-of-distribution data.\n\nThis paper has potential, but I found it not particularly well written.  In fact I found it hard to get its main point since it touches in many aspects of probabilistic object detectors, but none of the points are really clear. There are some other issues that need to be clarified -- please see below.\n1- It is not clear from the formulation how the paper handles multiple detections\n2- In Section 3.1, there are inconsistencies in the nomenclature -- x is written as boldface and then in italics, but they mean the same thing.\n3- The experimental setup is not clear -- can you defined better \"with any corresponding ground truth bounding box\" (page 4)?\n4- From the definitions of ES and DMM, it is not clear what is done with multiple instances of objects to be detected in an image.\n5- Given the ES formulation, can we say that ES is prone to be overconfident?\n6- The proofs for the scoring functions being proper or not should be more formal.\n7- In Fig. 2, in each column the graphs should have the same range.\n8- Letters in Table 1 are too small.\n9- The definition of true positives, duplicates, localization errors, and false positives should be present.\n10- On page 6, the \"Advantages of our evaluation\" sub-section only shows an anecdotal example of the advantages of the proposed measure.  It is expected a more direct comparison with competing approaches, such as PDQ.\n\nAfter rebuttal, I raised my score from 4 to 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}