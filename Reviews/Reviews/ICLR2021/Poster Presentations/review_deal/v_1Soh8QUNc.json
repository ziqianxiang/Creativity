{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents an interesting model to sample from the Gibbs distribution using diffusion based method. The theory is interesting and it is related well to the current research in the field. All reviewers agree that this is a noteworthy contribution to ICLR. "
    },
    "Reviews": [
        {
            "title": "Interesting approach for training EBMs as a sequence of conditional EBMs",
            "review": "This paper describes training a sequence of conditional EBMs (inspired by  Ho et al. (2020)) instead of training unconditional EBMs.  Each conditional energy describes the probability of recovering x, given its noisy version \\hat{x}. The noisy version of x can be described as a normal distribution centered at x, so the condition EBM has an additional term ||x - \\hat{x}||^2, which constrains the Langevin dynamics to remain in the vicinity of \\hat{x}, so it converges faster!\nThe inference is that starting at white noise x_0, it defines a conditional EBMs given x_0, runs Langaving dynamics to sample from P(x|x_0) defined by the conditional EBMs, and then use the sample as the evidence of another conditional EBMs.\n\nI assume the main advantage of this training over Ho et al. (2020) is having an energy-based model that can be used in other applications, so that would be nice to see the performance of trained EBMs on some applications other than image generation such as image inpainting and robust classification as discussed in Du and Mordatch (2019).\n\nIn general, I believe that this is an exciting paper and an important step towards training better EBMs.\n\nIn connection to score matching, Saremi et al. (2018) and Saremi and Hyvarinen (2019) should be cited as well.\n\nSaremi et al. (2018), Deep Energy Estimator Networks.\nSaremi and Hyvarinen (2019), Neural Empirical Bayes.\n\ntypo: \"score-based based methods\" in Section 4.1\n\nAlg2: for t \\gets T - 1 to 0 do",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Very nice method!",
            "review": "The paper proposed a novel method to train EBMs based on diffusion recovery likelihood. It constructs a sequence of noisy version of data and learn a conditional between consecutive noisy pairs. Compare to working with the likelihood directly, doing so makes the training much easier. Besides, even using a potentially non-convergent MCMC for gradient estimation, it still leads to a well-behaved energy potential, unlike EBMs trained via maximising the likelihood directly.\n\n## Pros\n\n1. The paper is well-written and easy to understand. \n2. The method is novel and solves the training difficulty issue for EBMs neatly.\n3. The analysis of the normal approximation and how it leads to choose step sizes in the Langevin dynamics is neat.\n4. The paper discusses related work well while motivating the method and also makes interesting connections to VI and score matching in Section 3.4.\n\n## Cons\n\n1. Only applications on image generations are conducted.\n\n## Questions\n\n1. Does the method work only with the specific architecture here, especially the Transformer sinusoidal position embedding?\n2. It says T = 6 and K = 10 leads to unstable training in Section 4.1. Does the optimization diverge or something? It would be good to see some trace plots for the training procedure.\n3. Why do you use the specific HMC setup for Section 4.2? Specifically, using a leapfrog step of 2 can lead to issues in which the MCMC only explore local modes. I would like to see how a long-run NUTS works here. A well-behaved potential should work in both settings.\n\n## Discussions\n\n1. I'm quite glad to see Section 4.2 that shows the long-run MCMC works on the trained EBMs, even with (potentially) biased gradients for training. Is it because we only need relative short-run MCMC for convergence when using recovery likelihood so that the bias is actually small?\n2. How does the method relate to amortized MCMC?\n\n## Related work\n\nThere are a few more related works that the authors may consider including.\n\n[1] proposed a similar idea of making the modelling task easier using a sequence of targets (for EBMs), but achieved by density ratio estimation.\n[2] uses coupled MCMC to resolve the biased gradient issue when using CD.\n\n[1] Rhodes, Benjamin, Kai Xu, and Michael U. Gutmann. \"Telescoping Density-Ratio Estimation.\" Advances in Neural Information Processing Systems. 2020.\n[2] Qiu, Yixuan, Lingsong Zhang, and Xiao Wang. \"Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models.\" International Conference on Learning Representations. 2019.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Strong theory, experimental part is weaker",
            "review": "#### Summary of the paper :\nThe authors propose to learn the recovery likelihood of a sequence of Energy-Based Models (EBM) trained on increasingly noisy version of the datasets. The authors demonstrate that optimizing the recovery likelihood is more convenient than learning the actual marginal likelihood, as it leads to more stable MCMC sampling and provide high-fidelity samples. The authors show competitive generation performances on CIFAR-10, Celeb1 and LSUN datasets.\n\n#### Pros : \n* The problem of non-convergent MCMC sampling in EBM trained with marginal likelihood is well motivated and well illustrated.\n* The authors proposition is well grounded in theory, and comes with rigorous and strong mathematical derivation.\n* The paper is clear, well written, and well structured\n\n#### Cons:\n* The quantitative comparison (IS and FID) is performed only on the CIFAR-10 datasets (see detailed comments).\n* The experimental evidences are not convincing.\n\n#### Recommandation:\nGiven the strength of the theoretical derivation, I would tend to accept the paper (marginally above threshold). However, stronger experimental evidences / discussion concerning the comparison with [1] & [2] would be beneficial for a better recommandation.\n\n#### Detailed comments:\n* While the authors show qualitative sampling results on LSUN, CIFAR-10, and CelebA they are quantitatively comparing the proposed framework on CIFAR-10 only. Even if CIFAR is challenging for generative models, I think the quantitative comparison on other databases is missing. In particular the comparison with [1] and [2] is necessary as the main claim of the paper is to show improvement using recovery likelihood.\n\n* The strength of the authors proposition is to provide a framework that could leverage less diffusion time steps than [2] while keeping good generation performance. Even if the authors clearly state such a theoretical advantage in paragraph 3.4, the experimental evidence is not convincing. Less diffusion time step (e.g. the case T=6, K=50) results in degraded FID and IS compared to [2]. In addition, the comparison between  T=1000, K=0 (DSM) versus [2] show a significant advantage in favor of [2] (they also use 1000 time step in [2]). What is the reason ? It suggests that optimizing the recovery likelihood is less efficient than optimizing the marginal one in diffusion problem...\n\n* Comparable setup (i.e T=1000, K=0 (DSM) versus T=1000, K=0) lead also to degraded performance : Why score-matching approach leads to better performance than diffusion recovery ? The quality of the paper would be greatly improved with such a discussion.\n\n#### Typos and suggestions to improve the paper\n* The initials 'MLE' are not explicitly defined\n* The Eq 3 should come with a demonstration (in Annex) or at least with a reference .\n* Eq 9 should be an approximate sign as it is a Taylor expansion.\n* The transition from Eq.9 to Eq.10 should be demonstrated (in Annex)\n* Eq 13 : N â€”> \\matcal{N}\n* Algorithm 2, line 2: It seems that there is a typo : t <- T-1 to T? Is it : t <- T-1 to 0\n\n[1] : Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. On learning non-convergent short- run mcmc toward energy-based model. arXiv preprint arXiv:1904.09770, 2019b. \n\n[2]: Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}