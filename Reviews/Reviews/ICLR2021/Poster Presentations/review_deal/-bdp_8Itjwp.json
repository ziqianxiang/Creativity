{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper received borderline reviews, but all lean toward acceptance.\n\nThe reviews highlighted strengths in the paper, citing that they liked the main idea and its mathematical treatment:\n* R3: \"I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards\"\n* R3 post-discussion: \"I recommend accept because authors have a solid theory which would be useful for the self-supervised learning community.\"\n* R4: \"This work presents a very detailed theoretical analysis for self-supervised learning objectives. The idea of inverse predictive learning for filtering task irrelevant information is interesting.\"\n* R2: \"I like the idea of discarding the redundant task-irrelevant information to improve the self-supervised learning\"\n\nHowever, there was a consensus among reviewers that the experimental validation was weak, both in terms of not showing enough improvement on enough examples and in terms of studying the effect of certain hyperparameters:\n* R2: \"lack of persuasive experiment results to prove the effectiveness of the proposed method. In fig.3, the improvements on two dataset are marginally, which can not convince me. The \\lambda (λ_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method.\"\n* R3: \"Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected\"\n* R3 post-discussion: \"All reviewers have concerns about lack of solid experimental evidence [...]  I can not improve my score further because of weak experimental evidence.\"\n* R1: \"The experiments are conducted in a controlled way [...] Traditional uncontrolled experiments [...] are suggested.\"\n* R4: \"The variation in the performance shown in Figure 3 is very marginal. [...] Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal. [...]\"\n* R4: \"weights required for inverse predictive learning in the loss formulation is not trivial. [...] Is there a simple way to determine this weights without exhaustive search on target dataset?\"\n* R4: \"However, it is not clear from the experimental results if this is really effective.\"\n\nThe authors' revisions aim to improve the discussion of the $\\lambda_\\text{IP}$ parameter.\n\nGiven these experimental limitations, my recommendation is for acceptance but with a low confidence score."
    },
    "Reviews": [
        {
            "title": "This paper presented a novel theoretical framework to explore the self-supervised learning by information theory. The authors analyzed the self-supervised learning theoretically and proposed a composite objective including contrastive learning, forward predictive learning and inverse predictive learning. The paper is well written and easy to understand. ",
            "review": "Strengths:\n\tAnalyze the self-supervised learning methods from extracting task-relevant information and discarding task-irrelevant information from the input theoretically and clearly.\n\tPropose a new objective to discard task-irrelevant information explicitly. And it seems easy to combine with other SOTA self-supervised methods.\n\tConduct experiments on visual setting and multi-modal setting.\n\nWeakness:\n\n\tI like the idea of discarding the redundant task-irrelevant information to improve the self-supervised learning. However, the composite objective proposed in this paper seems just like a simple combination of three tasks, which is not strikingly novel. \n\tanother concern of this paper is the lack of persuasive experiment results to prove the effectiveness of the proposed method. In fig.3, the improvements on two dataset are marginally, which can not convince me. \n\tThe \\lambda (λ_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method. I hope the authors provide more explanation about this.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great theory but emprical evidence may need more clarification and improvement. ",
            "review": "Summary: \nAuthors give an information-theoretical abstraction to represent various self-supervised learning methods and understand them better. Specifically, authors claim that self-supervised learning methods can extract task-relevant information and discard task-irrelevant information. Authors also do some controlled experiments and try to support their theoretical analysis. \n\nRecommendation: I recommend to accept the paper (rating 7). I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards. Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected.  \n\n1) In assumption 1: I(X;T|S) <= \\epslion. If task-relevant information lies mostly in the shared information between the input and the self-supervised signals then I(S;T|X) <= \\epslion, should also be true, right?\n2) Can authors give an intuitive proof sketch for Theorem 1?\n3) In Proposition 1 and Theorem 3, it looks like d can be arbitrarily large and even scale with n. Is there any upper bound in d? Would this invalidate all claims?\n4) In Figure 3 a and 3b, waht happens when you don’t use L_{CL} but use L_{FP} instead?\n5) Figure 3 c is not clear. Why L_{CL} overfits and why does adding L_{FP} avoid that over-fitting? Why did you use \\lambda_{FP} = 0.005?\n6) From figure 3, It looks like L_{CL} is good enough if early stopping is used properly. How does one justify the use of FP and IP? One can also use some regularization along with CL (would the regularization act as “discarding  task-irrelevant  information”)?\n7) How were hyper-parameters (learning rate, batch size, etc) chosen for all the experiments? Was proper hyper-parameter tuning done for all the methods? Can one achieve the same best results as in Figure 3a and 3b with proper tuning of method when \\lambda_{IP} is zero?\n8) It is not very convincing that controlled experiments in section 3 actually support Theorem 1 and 2. “Theorem 1 indicates that the self-supervised learned representations can extract almost as much task-relevant information as the supervised on” and “Theorem 2 indicates that a compression gap (i.e.,I(X;S|T)) exists when we discard the task-irrelevant information from the input”. Can authors clearly point out how conclusions from Figure 3 and Figure 4 are exactly supporting the respective statements of theorem 1 and 2 respectively?\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An information-theoretical point to better understand SSL from a multi-view perspective, but the experimental part needs to be enhanced",
            "review": "This paper explores both theoretical and empirical perspectives on self-supervised learning. The authors attempt to prove that self-supervised learning could extract task-relevant information while discard task-irrelevant information. They also provide a composite objective that includes contrastive and predictive loss functions along with an additional regularization. Controlled experiments are conducted to support the presented theorems. \n\nThere are three major concerns for me.\n\n1)\tThe experiments are conducted in a controlled way, including visual representation learning and visual-textual representation learning. Traditional uncontrolled experiments, such as unsupervised learning on Cifar10 or ImageNet are suggested. \n\n2)\tThe results in Fig. 3 and Fig. 5 demonstrate that the performance is sensitive to the hyper-parameter of $L_{IP}$. How to set the hyper-parameter $\\lambda_{IP}$  in practice? Besides, the best performance achieved by $L_{IP}$ is only marginally better to those without it.\n\n3)\tSome previous related works, such as InfoMin [Tian et al., 2020], are suggested to discussed and compared with more details. As far as I know, InfoMin analyzed influence of different view choices using mutual information theory. Therefore, it would be helpful to compare the similarities and differences between this work with it. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This work presents a very nice theoretical analysis of self-supervised loss formulations but lacks solid experimental results",
            "review": "In this work the authors focus on self-supervised representation learning. This work proposes a composite loss function which includes contrastive loss, predictive loss, and a novel inverse predictive loss. The authors have provided theoretical analysis as well as experimental results on multiple datasets under control environment.\n\nPros:\n\nThis work presents a very detailed theoretical analysis for self-supervised learning objectives. \n\nThe idea of inverse predictive learning for filtering task irrelevant information is interesting.\n\nCons:\n\nThe inverse predictive learning appears to be similar to contrastive learning if we ignore the negative samples in the contrastive learning formulation and use the projection head from contrastive learning for inverse predictive learning. How will the authors differentiate between these two formulations?\n\nThe variation in the performance shown in Figure 3 is very marginal. Also, only selective results are shown in Figure 3. No results are shown on CIFAR-10 which uses all the three losses at the same time.  Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal. Also, how was the loss weights determined? Experiment on a large scale dataset, maybe ImageNet will be very useful to demonstrate the effectiveness of the proposed loss.\n\nThe weights required for inverse predictive learning in the loss formulation is not trivial. As shown in Figure 3, it varies with different datasets and varies a lot (1 for Omniglot and 0.1 for CIFAR10). Is there a simple way to determine this weights without exhaustive search on target dataset?\n\nThis work mainly focuses on the theoretical aspect of semi-supervised learning and have also presented an interesting inverse predictive learning task for self-supervision which can dis-regard task irrelevant information. However, it is not clear from the experimental results if this is really effective. The results shown in Figure 3 and Figure 4 are not sufficient to demonstrate its effectiveness. Also, quantitative comparison with existing approaches will also strengthen this work, which is only partially addressed. A more in-depth experimental analysis (apart from the theoretical analysis) is required to show its usefulness.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}