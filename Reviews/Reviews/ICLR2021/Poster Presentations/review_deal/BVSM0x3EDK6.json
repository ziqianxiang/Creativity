{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviewers concurred that this is an interesting paper with contributions worthy of publication. The authors also provided many details in the rebuttal which makes the paper even more strong."
    },
    "Reviews": [
        {
            "title": "Interesting Method, Several Issues in Experiments",
            "review": "Summary: This paper uses random convolution augmentation to learn representation that is robust for transferring across domains.\n\nQuality: The paper is well-written, easy to following. The method is presented clearly, and the enough experimental evaluation was conducted. Overall, the quality is good.\n\nClarity: The method is straightforward, and the authors describe it clearly. I think the idea is reasonable, but there is something to clarify in experiments:\n1. Compared with [1], all the methods have a drop in Table 2. Could you explain why? In fact, Table 2 is really confusing, the bolded numbers are not the best numbers.\n2. I notice that in [1], the performance of PAR can be further improved with strong data augmentation. In Table 2, the method is compared with Grey Scale/ Color Jitter. I wonder if this is fair comparison, since RandConv composes various random convolution schemes and mix. I wonder how the performance will change when the baseline uses stronger augmentation, for example, combining multiple data augmentation like self-supervised learning [2].\n3. Just wondering: will your method result in a network with higher robustness against adversarial attacks?\n\nOriginality: As far as I know, the method is new.\n\nSignificance: I think this paper proposes a simple yet effective method. The use of random convolutions is interesting. I assume this work can be a good contribution to the community if they can refine their experimental evaluation to better justify the effectiveness of their method. \n\nReference:\n[1] Learning Robust Global Representations by Penalizing Local Predictive Power https://arxiv.org/pdf/1905.13549.pdf\n[2] A Simple Framework for Contrastive Learning of Visual Representations https://arxiv.org/abs/2002.05709",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A novel and effective data augmentation method ",
            "review": "Summary: This paper describes a method 'RandConv' which uses random convolutions to generate images with random textures but maintain global shape to improve training and generalization for classification tasks. The authors compare both random convolutions as images and mixing them with original training images. \n\nStrengths:\nThe authors demonstrate success with their method across various datasets as well as for pre-training experiments and evaluating on fine-tuning datasets. \n\nThe authors describe why their method should work as well as why their random convolutions preserve global shape. Far too often in ML papers we stop at 'this worked' and do not take the time to understand the why or how. \n\nThe authors compare their methods to other data augmentation methods and demonstrate improvements. \n\nThe t-SNE visualization of the difference in training is great evidence for this work and very clear from a reader standpoint. \n\nWeaknesses:\nThe authors neglect to compare across a significant amount of data augmentation methods, making it less clear how their method would compare. \n\nTheir method has worse performance in their PACS experiments for the Photo category and results vary significantly based on some algorithm choices in using their RandConv methods. For this to be more useful, a clearer understanding of the use cases is needed. \n\nMinor comment: typo on page 5 'w train a simple CNN' under 4.1 paragraph 1\n\nThe authors present a novel and interesting approach for data augmentation that can be directly coded into CNNs and demonstrate improvement for some tasks. I recommend accepting this work, although more work needs to be done to understand when and how to use their methods. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting use of Random convolutions for Data-Augmentation ",
            "review": "This paper proposes a simple way to increase the robustness of the learned representations in a network perform a series of object recognition tasks by adding a random convolution layer as a pre-processing stage, thus “filtering the image” and preserving the global shape but altering the local `texture’ of the newly transformed image. Here, the hope is that  -- analogous to Geirhos et al. 2019 that induces a shape bias by transforming the image distribution into a new one with altered *global* textures that induce a shape bias and increases general robustness to o.o.d distortions --  the authors here go about doing something similar at the local level given the small size of the receptive field of the filter, thus preserving the shape and slightly altering “the texture”.\n\nPros:\n* While the innovation is simple and efficient, this data-augmentation scheme works, and I can see how other future works may use this as well as a data-augmentation technique for object recognition. I am not sure however if no one else has explored the effects of random convolutions for robustness. It sounds too good to be true, but then again -- there is always beauty in simplicity and it is possible that the authors have hit the nail on the head on finding a somewhat ‘contrived’ filtering process as a bonus rather than a limitation. Simple, yet counter-intuitive findings like these are relevant for ICLR.\n* Authors provide lots of experiments that to some degree prove the success of their augmentation strategy (although see Cons).\n\nCons:\n* Biological Inspiration: What is the biological mechanism linked to the success of using random convolutions. One could argue that this point is ‘irrelevant’ to the authors and the readers, but as there is a plethora of different data-augmentation techniques to choose from, why should computer vision and machine learning practitioners choose this one? (See Missing Reference for a suggestion)\n* Insufficient/Incomplete Baseline: The model is inspired loosely by Geirhos et al. 2019; but how does the model compete with Geirhos’ et al.’s Stylized ImageNet? I would have wanted to see a baseline between the authors proposed model and other texture-based augmentation strategies. This would elucidate the Global vs Local advantages of “texture”/style transfer on learned representations. I think this is where authors could capitalize more on.\n* The word `texture’ in the paper is a mis-nomer. Here what is really done is 1st order filtering via a convolution operation with a filter that does not happen to have a Gabor-like shape. “Texture” in other contexts going back to vision science and even computer vision and image processing (style transfer included), is usually computed by a set of cross-correlations between *outputs* of a filtered image (analogous to the Gramian Matrix of Gatys et al. 2015), or the principled Portilla-Simoncelli texture model from 1999.\n\nMissing references:\n* Excessive Invariance increases adversarial vulnerability by Jacobsen et al. ICLR 2019. The augmentation procedure proposed by the authors shows robustness to common distortions, but how about adversarial robustness? Is this relevant? Was this tried? I’d love to hear more about the authors thoughts on this to potentially raise my score.\n* Emergent Properties of Foveated Perceptual Systems (link: https://openreview.net/forum?id=2_Z6MECjPEa): An interesting concurrent submission to this year's ICLR has shown that the biological mechanism of visual crowding (that resembles texture computation for humans in the visual periphery) is linked to some of the operations introduced in the paper by the authors. It would be great if the authors potentially cite similar (and/or the before-mentioned) works to provide a link to a biological mechanism that may support why their data-augmentation procedure works and/or should be used; otherwise it seems contrived and could be seen as “yet another data-augmentation procedure that increases robustness but we don’t know why”.\n* Implementing a Primary Visual Cortex in the retina increases adversarial robustness by Dapello, Marques et al. 2020 (NeurIPS). This recently published paper in a way shows almost the opposite of what the authors are proposing here. Rather than using random convolutions, they actually mimic the gamut of spatial frequency tuning properties of Gabor filters in the first stages of convolution as done in human/monkey V1. The authors should discuss how their results fit with Dapello, Marques et al. 2020 and how they can reconcile their somewhat opposing views.\n\nFinal Assessment:\nI am on the fence of having this paper accepted at ICLR given the limitations expressed above, but I do like it’s simplicity that should not take away it’s merit -- thus my slight lean towards acceptance. I am willing to raise my score however if authors address some of the cons/limitations, and am also curious to see the opinion from other reviewers, it is possible that I may have missed a key reference regarding data-augmentation that may weaken my assessment.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Why this particular approach and what about colour?",
            "review": "Update\n-------\n\nI've updated by score in light of the discussion; as I said in the comments, from a purely experimental point of view there are good results, however the presentation of the paper confounds too many aspects. If the authors can address the terminology issues then it would make the work stronger.\n\n\nSummary\n-------\n\nThis paper proposes the use of a random convolution layer at the beginning of a network during training. The weights of this layer are not learned, and are changed randomly each time an input is passed through. In the experiments presented the layer has 3 input and output channels, so that the effect of the convolution is to jumble-up colour and modify local texture (at a scale determined by the size of the convolution kernel, which is itself uniformly sampled from a set of possible scales). During training the objective is modified with an additional loss term that intends to ensure that the distribution of network outputs from three independent forward passes of the network with the random convolution are consistent with each other. The normal task loss is just computed on the basis of the output from one of the forward passes.  The authors posit that this approach will lead to increased robustness and generalisation of the network as if forces a bias towards shape and away from texture (and colour), and demonstrate this with a wide range of experimental results.\n\n\nPositives\n---------\n\n- A wide range of metrics and datasets has been used to validate the performance of the proposed approach, and the evaluation appears thorough with respect to comparing against state-of-the-art approaches.\n- Aside from a slight lack of detail on the testing regime (see questions below), which is easily corrected, the paper itself reads very well.\n- The results do show that the proposed approach can have significant performance gains in a number of tasks.\n\n\nConcerns\n--------\n\n- The proposed method confounds texture and colour; these are different things and should be treated as such. One of the likely reasons for the reduction in performance on photos is for classes where colour is important to choosing the correct class. This probably also explains why mixing might be important/useful in some cases. I'm confident that if the authors had addressed this they would see further improvements in their results.\n- Whilst I agree that the random convolutions would obviously bias a network to learn a shape bias due to the destruction of local texture and colour information, using random convolutions to do this seems incredibly arbitrary given the myriad of possible ways of achieving this. For example, a trivial way to achieve the same thing would be to use a bandpass filter such as a Laplacian of Gaussian (coupled with random mixing of the colour channels if you really wanted to remove the colour information too). Such an approach could also trivially be made multi-scale if desired, and would be justifiable (e.g. Marr from a neuroscience perspective; Lindeberg from scale-space theory). \n- No adequate baseline has been provided; following on from the above point, I would have at least expected experimental comparison against a _simple_ baseline to demonstrate the advantages of the proposed technique (e.g. using a fixed bandpass filter instead of a random convolution perhaps).\n\n\nRationale for score\n-------------------\n\nAs it stands, although this paper clearly provides a method that gives good results, I find it difficult to give a high score because of the arbitrariness of the proposed method and the lack of justification for the approach (other than \"it seems to work\"). I am also strongly concerned by the way the paper confuses texture and colour.\n\nQuestions during rebuttal period \n--------------------------------\n\n- Just to confirm: what is the testing set-up used? Do you input the raw image data into the network without passing through the RandConv? \n- Have you measured the shape bias (Geirhos'19) of networks trained using RandConv? Do the networks show bias to shape over texture using this metric?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}