{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces a game-theoretic framework to improve our understanding of dropout. All reviewers appreciated the contribution of the paper. While they had a number of questions/suggestions, almost all of them were adequately addressed. Three reviewers are satisfied and recommend acceptance, while a lone reviewer is on the fence, he/she admits he/she is less knowledgeable about game theory. Overall, I think this paper makes a solid contribution to ICLR."
    },
    "Reviews": [
        {
            "title": "An interesting explanation of dropout from the lens of game theoretic interactions, experiments require some important improvements.",
            "review": "Summary.\n\nThis paper aims to explain dropout from the lens of game theoretic interactions. Let x denote the input of a deep neural net (DNN), intuitively, the interaction between two variables x_i and x_j quantifies how much the presence/absence of the j-th variable affects the contribution of the i-th variable to the output of the DNN. With the above definition in place, the authors show theoretically and empirically that dropout reduces the interactions between input variables of DNNs. As this type of interactions turn out to be strongly correlated with overfitting, the authors suggest that dropout alleviates overfitting by reducing interactions between input variables (or activation units) of DNNs. Based on this understanding of dropout, an alternative regularization technique is proposed, which explicitly penalizes pairwise interactions between variables. \n\nStrengths.\n\n1. The paper is well written and clearly presented.\n\n2. Although it is already well known (or at least widely accepted) in the community that dropout reduces dependencies among activation units in DNNs, the explanation of dropout from the perspective of game theoretic interactions is interesting, and it is supported both theoretically as well as empirically.\n\nWeakness.\n\n1. Hyperparameter settings (e.g., optimization-related ones) to reproduce the results are not provided. It is not clear what dropout rate was used in the experiments. Is it 0.5? In all cases, different dropout rates should be investigated before claiming the superiority of the proposed interaction loss (regularization) over dropout.\n\n2. Experiments are curried out on only one task (classification), one type of data (images), and one family of DNNs (convolutional neural nets). However, the paper draws quite general conclusions regarding the understanding of dropout from the perspective of game theoretic interactions. Therefore, considering at least one more task involving a different type of data and another family of DNNs would reinforce the findings of this paper.\n\n3. Computational time analysis of the proposed interaction loss and training time comparisons with dropout are lacking.\n\nAdditional comments\n\n1. Dropout is used both at convolutional and at fully connected layers. However, one can argue that applying dropout to convolutional layers does not make sense owing to the sparsity of connections in this type of layers.\n \n2. I would recommend revising the title of the paper. What is proposed is more of an alternative regularization form to dropout than an improvement for the latter. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "AnonReviewer3",
            "review": "*Paper Summary*\nThe authors provide a novel interpretation of dropout regularization using Banzhaf interactions, a tool from game theory. \n\n*Pros*\n* The authors are able to mathematically prove that dropout is capable of suppressing neural co-adaptations, the latter being one of the reasons for overfitting. Visualizations are also provided in this respect on a dataset for face analysis.\n* Through their mathematical analysis, authors are able to improve upon the classical dropout training, by making it more compatible with batch normalization, so that these two classical regularization strategies show a better complementarity.\n\n*Cons*\n* Some of the results does not read well, like Table 3 or Figure 4, but this is really minor and fixable\n\n*Preliminary Evaluation*\nI believe that the overall analysis provided by authors is complete and interesting, so I am akin to call for a full acceptance of the paper which I deem suitable for such a venue like ICLR. In order to improve their paper, I would encourage authors to better investigate over the following aspect: since many times authors established a principled connections between dropout and neural activations, it would be very interesting to discuss the relationship with the present work and another paper [Gomez et al, Targeted Dropout, NeurIPS Workshops 2018] in which a computational variant of dropout is proposed, such that the dropout rate depends upon neural activations. \n\n*Post-Rebuttal Evaluation*\nI have carefully read the response provided by authors and checked the revised manuscript. I confirm my preliminary acceptance rate.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "New way of measuring dropout's effect on interaction strength, but weak experimental validation and lacking in clarity",
            "review": "Summary:\nThis paper analyzes the effect of dropout on interaction between units in a neural network. The strength of the interaction is measured using a metric that is used in game theory to quantify interaction between players in a co-operative game. The paper shows that dropout reduces high-order interaction (as measured by this metric), and that reduction in interaction is correlated with better generalization. The paper introduces a new regularizer that explicitly minimizes the metric and claims that using this regularizer instead of dropout has some advantages.\n\nPros:\n- The idea that dropout reduces overfitting by breaking up complex co-adaptations and regularizing interactions is widely believed to be true. However, this paper tries to explicitly quantify the amount of interaction and presents theoretical and experimental evidence that interaction reduces as a result of having dropout.\n\nCons:\n- The proposed metric is hard to compute exactly since it requires summing over exponentially many terms, each term requiring a forward prop through the network.\n- The assumptions made in computing this metric approximately seem unclear to me (Appendix H). I could not understand what probability distributions are being expressed and why. In particular, how is the term in Eq 38 approximated by the one in the first line of Eq 41. The paragraph after Eq 40 was also unclear.\n- It is not discussed how this metric for evaluating interaction strength compares to something conceptually simpler like the Hessian \\\\(\\nabla^2_{i,j} L\\\\) which directly measures the dependence of the network's loss on pairs of input variables, and its magnitude is proportional to the interaction strength.\n- The paper mentions that an advantage of the proposed loss is that the weight \\\\(\\lambda\\\\) applied to the interaction loss can be explicit controlled, whereas the strength of dropout cannot be controlled (Section 4 \"advantages\", \"Unlike the interaction loss, people cannot explicitly control the strength of dropout ..\"). This does not seem correct. The dropout probability provides such as control mechanism for dropout.\n- For the experimental results in Table 3, it is not mentioned what value of the dropout probability was used, whether this value was tuned for each architecture, and which network layers was dropout applied in. These factors can have a significant impact on overall performance. On the other hand, the \\\\(\\lambda\\\\) parameter for the proposed interaction loss is tuned. So the resulting comparison is not fair.\n- It is not clear what additional insight this metric provides about dropout, beyond confirming what is intuitively apparent : that having randomly dropped neurons will make it harder for the network to learn high-order interactions.\n\nOther comments and suggestions:\n- The introduction includes a discussion around Banzhaf value, without describing what it means. The concept of Banzhaf value might be new to many readers in the ML community. I would suggest including a short explanation to give some intuition about what it means, before discussing it in more detail.\n- \" the output of the DNN corresponds to the score f\" : would it make sense to say that (negative) loss corresponds to the score f, rather than the output of the network ?\n- \"award\" -> \"reward\" or \"utility\" ? (I'm not familiar with game theory literature, so I'm not sure if \"award\" is a commonly used term there).\n- The title of the paper is a bit misleading as it seems to suggest that the paper is about using dropout in Game theory (i.e. solving problems in game theory using dropout).\n\nPost rebuttal\nThe authors addressed the concerns around the clarity of the paper and added useful additional experiments. I will increase my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A new perspective of game theory to understand dropout ",
            "review": "Summaryï¼š\nThe paper proves that dropout can suppress the strength of interactions between input variables from the perspective of game theory. It further improves the utility of dropout by introducing an explicit interaction loss. Experimental results verify the theoretic proof and the effectiveness of the proposed loss.\n\nStrengths:\n1. The paper introduces a new perspective of game theory to understand dropout.\n2. Experiments are conducted on various datasets to support the theoretic proof and the proposed interaction loss\n\nConcerns:\n1. Although I have no background in game theory, I try my best to understand the terminology and the analysis. However, I do not have the ability to verify the correctness of its proof. Thus, I cannot evaluate the main contribution of this paper. For experimental results, the conclusion that dropout suppressing the input interactions is not a new story.\n2. It would be more interesting if the author can further explain the disharmony between dropout and bn from the perspective of game theory. ",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        }
    ]
}