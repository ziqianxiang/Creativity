{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work provides interesting insights on the transferability of adversarial perturbations and proposes ways of making it more effective. While several reviewers have found parts of the paper unsatisfactory, there are interesting results to merit acceptance."
    },
    "Reviews": [
        {
            "title": "This work defines interaction of coordinates in the input data based on Shapley values, and shows a negative correlation between the adversarial transferability and the interaction.",
            "review": "The paper presents a negative correlation between the adversarial transferability and the interaction between coordinates.  The interaction is defined by the Shapley values used in the game theory to measure the contribution of players.  The author(s) defined the interaction between coordinates (players) as the difference between the joint contribution and the sum of conditional contributions.  They show a negative correlation between the adversarial transferability and the interaction with various known adversarial attacks.\n\nThis work also shows an improvement in the transferability of adversarial perturbations by incorporating the interactive loss with the classification loss. \n\nWhat is missing in the paper is the reason and motivation for using the Shapley values for defining the interaction.  No comparison and discussion is given the difference between their method and other methods that define the interaction between variables.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting work and solid analysis",
            "review": "This work proposes that the transferability of adversarial attacks has a negative correlation with the interaction within an input perturbation. By defining the interaction of perturbations with the Sharpley value, it can quantify the interactions and demonstrate the negative correlation with the transferability. Furthermore, this work shows that prior work on adversarial attacks (e.g., VR attack and MI attack) can be explained by the (expected) interaction scores. This work further demonstrates that the way of enhancing transferability by minimizing the interaction within input perturbations, with the experiments on the image classification task. \n\nOverall, I think this work provides a new perspective of understanding transferability and presents solid analysis/experiments to verify the hypothesis. \n\nOnly one comment about the definition of interaction scores. In some literature [Lundberg et al., 2019], it is called the Shapley interaction index, which uses the definition in equation (13) of Appendix D. Shapley interaction index has mainly been used in the machine learning literature recently for explaining feature interactions within models. E.g., \n\n1. Lundberg et al. Consistent Individualized Feature Attribution for Tree Ensembles. 2019\n2. Chen and Ji. Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers. 2020\n",
            "rating": "10: Top 5% of accepted papers, seminal paper",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper mainly deals with the negative correlation between adversarial transferability and the interaction inside adversarial perturbations. The authors claimed that utilizing the correlation can be regarded as a unified perspective to understand previously proposed methods. To this end, they presented an adversarial attacking loss, which can directly reduce the interaction, defined as the individual interaction between two perturbation units to the total reward function.\n\nThis paper provides an intriguing perspective that can explain adversarial attacking mechanisms. Although I am not sure about the exact settings that draw the numbers, the empirical results seem good; thus, reducing the interaction inside perturbations could be useful in practice. However, I cast doubt on the theoretical significance of the central hypothesis. This concern derives from lacking analyses on 1) a valid (theoretical) reason for the negative correlation, 2) qualitative benefits of directly reducing the interaction based on the game theory. \n\n[Quality]\n\nThe paper follows one solid logical structure aforementioned in Abstract and Introduction. However, I think Section 3 & 4 are confusing, which indeed are the core parts. For example, the propositions are overly verbose and hard to follow. Also, the theoretical statements and interpretations are not well aligned. I defer the detailed comments below.\n\n[Originality]\n\nThe originality of the paper is not outstanding, but sufficient for acceptance. \n\n[Significance]\n\nThe paper reveals one aspect of adversarial transferability for sure. However, bringing the game theory-based approach in this domain is quite unconventional. Hence, I strongly feel there should've been more theoretical arguments that strengthen the significance of this approach. I guess the authors' intention was to prove their claims empirically, and the paper provides some good results. Unfortunately, I feel this work is reluctant to compare the IR method with the State-of-the-Art methods (such as \"MI vs. vanilla IR,\" \"VR vs. vanilla IR,\" \"SGM vs. vanilla IR\").\n\n\n[Comments & Questions (sorted by priority)]\n1. As far as I understand from Proposition 1, it says that \"multi-step attacks generate more interaction than single-step attacks.\" Please elaborate that this statement can be generalized to the following: \"the adversarial transferability and the interactions inside adversarial perturbations are negatively correlated.\" \n2. In Table 3, why is the result of (vanilla) IR Attack not reported? The table shows that HyridIR, a combination of all techniques, achieves the best performance. Hence, it implies that reducing the interaction in multiple ways can be stacked for achieving good performance. However, the table does not show the success rate of the pure IR attacking method. My concern is whether solely applying the IR attacking method has a clear contribution to be accepted in this conference. Please describe how IR attack (using only Eqn. 5) will perform compared to the SGM attacking method, which seems to be one of the State-of-the-Art. \n3. According to the definition (Eqn. 3), the actual computation of the interaction is not very scalable when the set of players (\\Omega) is large. I think Eqn. 4 also lacks scalability because it is natural to think that the set of the adversarial perturbations is a continuous space.  \n4. Is high $\\lambda$ always effective? In Fig. 3, how the success rate changes when $\\lambda = 10^2, 10^3, \\dots$? In other words, I want to know when the tendency of Fig. 3 (a) converges to Fig. 3 (b) as $\\lambda$ goes higher.\n5. (Similar to #2) Why are the results of MI, VR, SGM methods not presented in Table 1?\n6. Some of the arguments, especially propositions, are unnecessarily lengthy and hard to follow. To improve readability, I suggest clarifying the main points in the theoretical arguments. For example, please remove the redundant part \"Given an input image $x \\in\\mathbb{R}^n$ ... $\\delta^m_{multi}), y)$.\" in Props. 1, 2 & 3 and make this statement as a proper Definition.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #4",
            "review": "Summary\nThe authors analyze the transferability of adversarial examples from the perspective of interactions based on game theory. They have discovered and shown  the negative correlation between the transferability and interactions inside adversarial perturbations. This discovery leads to an explanation of the adversarial transferability by the interaction inside adversarial perturbations. Thus, they proposed a new loss called interaction loss to penalize the interactions between perturbation units during attacking and experiments show the improvement of the adversarial transferability.\n\nStrengths\n1 An interesting understanding of adversarial transferability is provided.\n2 Mathematical proofs are admired.\n3 Results are very nicely presented, and writing is clear throughout the paper.\n\nWeaknesses\n1 Results are reported only on one dataset (ImageNet).\n2 The appendix needs to be revised for better presentation.\n3 The variances of the results are not reported.\n\nThis paper is well motivated because the authors observe the negative correlation between the transferability and interactions inside adversarial perturbations, and they provide a possible explanation of why the related research tasks  can improve the adversarial transferability. Further mathematical proofs and experiment results verify the observation. \n\nHowever, I have some questions about this paper.\n\nQuestions\n1.\tThere are many outliers off the blue shade in the subgraph of Figure 1. Could the authors give some interpretation of why there are so many outliers?\n\n2.\tFor Equation 4, the value of the expected interactions is equivalent to the expectation of the contribution for each pixel. The authors aim to minimize the value of the expected interactions that is the same to average the contribution to all pixels. That may contradict the idea of the one-pixel attack. More interpretation should be given to understand the concept of interactions.\n\n3.\tThe authors claim to provide a unified view to understand the enhancement of transferability; however the authors only explain three baseline models. Other types of adversarial example generation methods are not considered, such as Translation-Invariant Attack that the authors already mentioned in the related work. What is more, the Translation-Invariant Attack aims to compose the gradients of neighboring pixels together, which is contradictory to the authors’ idea of reducing the interaction of perturbations.\n\t\n\n4.\tFrom the experiment results in Table 1, why the performance of the IR attack is worse than the baseline when attacking RN-152 and DN-201 in the ensemble setting?\n\n5.\tIn Section 5, Experiments, baselines: “the transferability of each baseline was computed based on the best adversarial perturbation during the 100 steps…” and you also mention that “Previous studies usually set the number of steps to 10 or 20”. I am confused about why you set the step to be 100? For a fair comparison, you should follow the setting in previous studies.\n\n6.\tIn Section 5, Experiments, baselines: “we set $\\lambda = 1$ for the IR Attack, when the source DNN was ResNet, and set $\\lambda = 2$, for other source DNNs.”, but in Figure 3, part a, the range of $\\lambda$ is from 0 to 1.2 for DN121, which does not include the \\lamda value the authors suggested. \n\n7.\tThe computational cost is not discussed. As the authors said in Section 5 “the computational cost of the interaction loss is intolerable”. No discussion about the running time is provided. Moreover, the authors choose the step to be 100, which further increases the running time.\n\n8.\tThe third line on page 5, the term $|\\delta|_{p}^{p}$ is wrong. \n\n        It should be $|\\delta |_{p}$. \n\n9.\tAppendix: The is some problem with the reference format about Figure 4 and Figure 7. I also suggest reorganizing the appendix.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}