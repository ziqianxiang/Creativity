{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper."
    },
    "Reviews": [
        {
            "title": "This paper transferred the reference-based method in Super-Resolution to help remove the global spatial redundancy in the entropy model for image compression and achieved performance improvement. Generally, the motivation is clear and reasonable. However, some details about the model, experiments, and statements seem to be missing, which makes the whole paper somewhat unconvincing.",
            "review": "1\tThe explanation of the proposed “confidence U” is significantly deficient. Further statements to the calculation process, theory and function are expected. Please provide more details.\n2\tWhat the point of the operation that the output of global reference is multiplied by similarity S and confidence U directly as shown in Figure 5?  Will the direct multiplication by the confidence U impair the function of the attention S? A more reasonable and efficient method is expected for applying the confidence map.\n3\tDifferent from conventional entropy models, this paper proposed to progressively use the prior from context model, reference model and hyperprior model instead of obtaining priors respectively and then combining them to obtain estimation results. But the paper lacks the explanation and experiments to the rationality of the design of the progressive process. The comparison results of whether progressive or not are missing.\n4\tGenerally, the performance gain is very limited as shown in the RD curves in Figure 6. In particular, why the performance gain appears to be so minor in the low bitrate range? \n5\tIn Figure 10, why the comparison results with any learned compression methods are missing? The comparisons with various methods only on Kodak dataset in Fig. 6 are not convincing. \n6\tThe visual comparison results on the CLIC dataset of high resolution are missing. And some key references for deep-network based image compression are also missing.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting paper taking into account the global context to efficiently decrease the compression rate for image compression",
            "review": "**Summary of paper**\nThe paper presents a learning-based approach for image compression. To reduce the compression rate, it describes two novel extensions, one to take the global context into account and an improved version of the commonly used GDN layer. Their advantage has been shown in a thorough ablation study. Overall, the method achieves superior performance compared to standard codecs as well as other state-of-the art learning-based method on the evaluated dataset (from Kodak).\n\n**Strengths**\n\n(S1) The approach is clearly described, and the figures help to follow the paper.\n\n(S2) The proposed approach of using a reference model to consider the global context is novel for the application of image \ncompression. \n\n(S3) Suggestion to improve GDN layer to address the mean shift problem of the existing formulation.\n\n(S4) The ablation study shows the effect of the different modules.\n\n(S5) The method achieves superior results on the tested dataset (Kodak), standard in image compression, and both PSNR/SSIM have been reported.\n\n**Weaknesses**\n\n(W1) Runtime for encoding and decoding not listed. In case, that the global reference model leads to some overhead in runtime, especially for decoding time, that would be worth mentioning. Also are there any limits in terms of image size the method can handle?\n\n**Justification**\nInteresting approach with superior results.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper propose two methods for improve deep image compression performance: (i) Global Reference Module and (ii) Mean-shifting GDN Module (GSDN). (i) Global Reference Module searches over the decoded latents to find the relevant latents to the target latent for improve accuracy of entropy estimate. Authors extended Yang et al. 2020 method to using masked patch. (ii) GSDN extends GDN to use subtractive operation.\n\nPros: \n- Proposal seems better Rate-Distortion results than Lee 2019 and Minnen 2018 (Figure 6 and 7).\n\nCons: \n- The method for generating U (2D feature maps) is not clearly described; it is unclear how the output channel of the parameter network (768) is calculated in the form of 2D feature maps.\n- In Table 1, the output channel of the Encoder is 384, while the corresponding input channel of the decoder is 192. I couldn't understand why the number of channels are not the same.\n- In Figure 5, the meaning of σ's log is unclear and seems not appear to have been mentioned in the text.\n- The proposed method uses the mix quantization approach (Minnen & Singh 2020), but the evaluation of figure 7 is compared to Minnen 2018 as Context + Hyperprior so it is not fair comparison. For example, to put the results of the Minnen 2018 approach + mix quantization on it and compare it will make the claim of the proposal effect credible.\n- According to Figure 7, GSDN appears to be effective. It is an interesting, but I thought the effects were less explained. It would be more convincing if direct data on the mean-shifting problem were presented.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "This is a good work, but I have concerns on the diversity of test examples, and computational complexity. ",
            "review": "The authors introduce global reference into the entropy model for deep image compression. \nThey also develop a reference algorithm to ensemble local context, global reference and hyperprior. \nThis causes the algorithm to be robust to background noise.\nAlso, the authors develop GSDN module to handle mean-shifting issue.\nThe proposed method demonstrates good quality and memory usage gain. \n\nThis paper propose to take into account the global information as well as the local information to perform better image compression. \nThe authors also demonstrate comparison to popular image compression standards and recent deep learning approaches. \nI think this work is a nice work, however I have two main concerns. \nThe dataset used for evaluation is rather outdated. Have the authors tried evaluating on recent image compression datasets, or custom data and compare with the state of the art?\nHave the authors compared computational complexity? The main reasons why industry standards are not enthusiastic about deep learning approaches to compression is due to the computational complexity, not so much memory. Have the authors compared FLOPS? Moreover, since this work is dealing with global image information, it seems the complexity would increase rapidly with image size, while standard jpeg will relatively be not as severe. Have the authors experimented computational time with UHD, QHD, or 4k?\n\nI am leaning towards accept but not by a lot.\nI would like the authors to discuss upon \n- Empirical results on more recent datasets\n- Computational complexity and in terms of image size\n- FLOPS\n- Computational complexity and time with high resolution like UHD to 4k\n\nAfter these comments, I would like to adjust the rating",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}