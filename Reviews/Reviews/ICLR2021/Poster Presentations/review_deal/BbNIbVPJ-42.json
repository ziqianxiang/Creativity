{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors have made significant efforts to thoroughly address all the concerns. Due to the amount of discussions, I had to go through the paper myself and agree with the authors on many of the points. In my opinion, this is a solid theoretical work on the pitfalls of IRM. "
    },
    "Reviews": [
        {
            "title": "The work provides notable theoretical understandings on the invariant risk minimization scheme.",
            "review": "Pros:\n* The work gives extended theoretical analysis on the effect of invariant risk minimization scheme, which is an increasingly popular framework for robust prediction. The work considerably extends the results in the original IRM paper. The results seem reasonable, and clarify implausible beliefs on the framework. The intuitions and proving techniques could also inspire new methods to improve the current framework.\n\nCons (minor issues):\n* I know the main contribution of the work is on the theoretical side, but a simple experiment or simulation for the nonlinear case to demonstrate the empirical consequence would be appreciated.\n* In the considered data generating model, it is assumed that $z_c \\perp z_e | y$ (conditional independence). Is it required in the proofs? How natural/general can it be? For one example, it is mentioned that the causation can be viewed in the other direction, i.e. $z \\to y$. In that case, observing $y$ would render components of $z$ correlated. Also, a graphical illustration of the data generating model would be helpful.\n* OOD generalization to an arbitrary environment in the nonlinear case (even with the invariant optimal predictor on top of representations) may not be possible anyway, and some recent works on OOD generalization does not expect the learned model to work well in environments out of the convex hull/combination of training environments. So I am wondering how arbitrary can the test environment be in Theorem 6.1, e.g., can it be out of the convex hull/combination of training environment? The result would seem more interesting to me if the test environment can be.\n\n=== EDIT: post-rebuttal ===\n\nThanks for the additional explanations.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper takes a critical view of IRM. It shows under a particular DGP, in the linear case, a large number of environments are necessary for the recovery of the invariant predictor.  Furthermore, it shows that in the nonlinear case, IRM does not generalize to distributions that are different from the training environments.",
            "review": "- pros:\n    - The paper examined the claims of IRM thoughtfully and critically.\n    - the exposition is clear --- I particularly enjoyed the informal results in section 3.1. I applaud the authors' effort to make the results intuitive!\n    - The appendix included illustrative and convincing empirical studies.\n    - The paper brought up a failure case of IRM, which shows that there exists a nearly optimal classifier, similar to the invariant predictor in the training environment, but performs equal to the ERM solution in the testing set. \n- cons\n    - There is only one DGP used in throughout the paper.  In particular, the DGP  has the same complexity in the y->z_c and z_c -> y relationship. This seems counter-intuitive to me. I would be interested in learning if this phenomenon generalizes to other DGPs\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Clear theoretical analysis.",
            "review": "Main comments:\n\nThis paper studies a theoretical aspect of IRM and how will it fail. Main contribution is pointing out that IRM is ineffective when the number of environments $E$ is smaller than the dimension of environmental feature $d_e$. A simple but universal model assumption is built, where environmental feature $z_e$ and causal feature $z_c$ is sampled from Gaussian conditional on label $y$. The analysis is two-fold: linear regime and non-linear regime. In the former part, given the feature extractor $\\Phi$ is linear, a constructed solution to IRM is built to demonstrate the result. For the latter part, the other show the failure of IRM via several results in Thm 6.1 / D.3. The whole analysis is clear and easy to follow, thus the reviewer believe this submission deserves to be accepted. \n\nMain comments:\n\nThe simulated experiments in Appendix C demonstrate the theoretical results. However, it's a toy run and  thus one drawback to the reviewer is that, for Colored MNIST experiments in IRM, it's obvious that $d_e > E$ while IRM still works. It would be great if the authors could give some explanation on this point.\n\nAnalysis in the paper is more a constructed one, whereas lacks of how optimization algorithm can lead to such a solution, which is another drawback of this paper.\n\nAnother minor comment:\n\nOne seminal reference seems missing: Causal inference using invariant prediction: identification and confidence intervals. Jonas Peters, Peter Bühlmann, Nicolai Meinshausen.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting results, but I believe slightly misleading",
            "review": "The paper, entitled \"The Risks of Invariant Risk Minimization\", provides a theoretical analysis of the IRM learning objective under particular conditions. It claims to provide two theoretical results, namely:\n1) in the linear setting (i.e., the optimal model is linear), they provide a necessary and sufficient condition for IRM to recover the optimal invariant predictor.\n2) in the non-linear setting, they demonstrate that IRM does not perform better than ERM (empirical risk minimization).\n\nStrong points of the paper:\n1) the paper addresses an important question: what are the theoretical guarantees of the IRM objective ?\n2) the paper provides a solid argument against the IRM objective, which is shown to be no better than ERM, in a limited but non-trivial setting\n3) the paper tries to relate IRM to several prior works from the literature\n\nWeak points of the paper:\n1) I believe the paper contains a major flaw: the so-called \"invariant classifier\", which uses only invariant features, is not the optimal invariant predictor sought by IRM. As such the first claim of the authors, necessary and sufficient conditions under which IRM recovers the optimal invariant predictor in the linear setting, is wrong.\n2) The paper suffers from a poor structure, a lack of clarity, and a general lack of rigor in the definition of the key concepts.\n\nDespite commendable efforts from the authors to study IRM theoretically, even in a limited setting, I recommend rejection for the paper.\n\nFirst, while it appears rather theoretical, the paper lacks rigor and clarity in the definition of the problem, the key concepts, and the assumptions. The description of the OOD generalization problem, of IRM and its assumptions, of the assumptions made by the authors in their simplified model, and of the introduced concepts (notably the \"invariant classifier\"), are lacking a formal mathematical description. The main results of the paper are given in Section 3, before the studied object, IRM, is even formally  introduced in Section 4. The authors interchangeably use the adjectives \"causal\" or \"invariant\", without having properly defined the meaning of those concepts in the first place, and with no particular utility. Maybe IRM can be given a causal interpretation, under some assumptions, but clearly this is out of the scope of the paper. The theoretical analysis of IRM conducted in the paper does not require the notion of causality, which only brings confusion to the paper in my opinion.\n\nSecond, and most importantly, I believe that one of the author's claims (which occupies about half the paper) is simply wrong (see my detailed comments below with a counter-example). There seems to be a misunderstanding as to what IRM actually seeks for, which is not the so-called \"invariant classifier\" that uses only invariant features. IRM does not seek to use invariant features only, but to learn those invariant features, possibly by using non-invariant ones. As such, results such as Theorem 5.1, Corollary 5.2, and Theorem 5.3, have very little value I think. The second claim of the authors remains interesting, however I believe the paper requires a substantial revision and in its current state can not be accepted at ICLR.\n\nI would appreciate if the authors could formally describe what they mean by \"invariant classifier\", and point me to the evidence where the IRM authors argue that IRM will recover such a classifier.\n\n**Post-discussions**: the authors have clarified the meaning of \"invariant classifier\", which is now tied to their specific toy problem, and I now believe such a classifier is indeed the minimax OOD classifier supposedly seeked by IRM. While I still think the paper does not shine on the side of clarity, my main concern about the incorrectness of the presented theorems has been answered and I believe the paper will be of interest to the community. I therefore raise my recommendation towards acceptance.\n\nBelow are my detailed comments:\n\np.1 §3: each environment corresponds to interventions on the SEM (Pearl, 2009) on just the non-causal mechanisms -> What does non-causal mechanism mean here ? If the SEM is given a causal interpretation, then all mechanisms are causal. Do you mean the mechanisms which are not causaly impacting the variable of interest Y ?\n\np.1 §3: invariant features in the SEM -> Do you mean the causal features ? Or the invariant mechanisms ? Invariant features does not make sense. If interpreted as constant features, those are uninformative. If interpreted as having the same distribution across environments, this does not make sense either, since it does not guarantee such a feature will be causal.\n\np.2 §5:  invariance of the feature-conditioned label distribution -> I like the effort the authors put in grouping the existing works in a unified framework and notation. There is however a bit of inconsistency and ambiguity in the terminologies employed throughout the paper. Invariant distribution? Domain-invariant representation ? Invariant features ? A proper definition would help, and enforcing consistency throughout the paper, by using the same names when those refer to the same concepts, would benefit readability.\n\np.2 §5: The main difference [...] train to test distributions. -> I like that the authors make this comparison, but I also would have liked this discussion to be further extended. There is a clear relationship indeed. If IRM looks for a representation $\\phi(x)$ such that $p(\\phi(x))$ can change between domains, but $p(y|\\phi(x))$ remains the same, then this is exactly the assumption of covariate shift: $p(x)$ can change, but $p(y|x)$ remains the same. However, finding such a representation $\\phi$ does not entirely solve the problem. First, the representation has to be predictive enough for $y$, ideally so that $y \\perp x | \\phi(x)$. Otherwise, even a constant $\\phi$ might result in $p(y|\\phi(x))$ being invariant if $p(y)$ is. But then marginal predictions from $p(y)$ would probably be sub-optimal. Second, while in theory (infinite model capacity, infinite data) covariate shift is not an issue (Bayes-optimal predictors do not need $p(x)$, only $p(y|x)$ over the support of $x$), in practice it is another story, and additional methods such as sample reweighting can be beneficial. See paragraphs 3,4 from the introduction in V. Tran, and A. Aussem. A Practical Approach to Reduce the Learning Bias Under Covariate Shift. ECML/PKDD 2015.\n\np.2 §6: whose joint distribution with the label is fixed for all environments -> this is a strong assumption which IRM, as far as I understand, does not make\n\np.3 §3: invariant (causal) relationship -> I am a bit uncomfortable with the authors mixing the invariant and causal adjectives in this context. In your SCM, if given a causal interpretation, $y$ is a cause of $z_c$, and $p(y|z_c) \\neq p(y|do(z_c))$. I would suggest for the authors to stick to the concept of invariance (which they still have not defined properly...), and drop the causal interpretation, which brings nothing but confusion to the paper. Causality really is a different business, which is not required to study the capabilities of the IRM objective.\n\np.3 §4: the causation can just as easily be viewed in the other direction -> Exactly. So why even talking about causality ? Do you want a model of $p(y|x)$, which generalize across different environments with different distributions for p(x) ? Or do you want a causal model of $p(y|do(x))$ ? Those are different objectives, and using them interchangeable really is confusing, as they do not require the same assumptions. Here, as you say, we do not care whether $x$ causes $y$ or $y$ causes $x$.\n\np.3 §5: a constant marginal -> Marginal of what ? Please be precise.\n\np.3 §5: is necessary: Necessary to what ? Which assumptions of IRM would be violated as a result of not having $y \\perp e$ your model ? The IRM assumptions were never clearly stated. Would that make finding $\\psi$ such that $p(y|\\psi(x))$ is invariant impossible ?\n\np.3 §8: whose joint or conditional distribution -> The conditional is implied by the joint. You can remove \"or conditional\" here.\n\np.3 §8: Definition 1 -> The paper would gain a lot in clarity if this \"invariant classifier\" was given a formal, mathematical definition. For example, what do you mean by \"invariant features\" ? The same holds for you assumptions, which could be defined more clearly: $z_c,y \\perp e$, $e$ uniform. According to your definition, I can think of the following:\n\n$$\nh^\\star = \\arg \\min_h \\mathbb{E}_{(x,y,z_c,z_e,e)}[l(h(x), y)], \\text{subject to } h(f(z_c,z_e)) = h(f(z_c,z'_e)), \\forall z_c,z_e,z'_e \\text{,}\n$$\n\nThe \"invariant classifier\" from this definition is not the one that IRM seeks. It suffices to consider a scenario where $f=I$ $Z_c=\\emptyset$, and two environments such that $\\mu_{e_1}, \\sigma_{e_1}=(100, 1)$ and $\\mu_{e_2}, \\sigma_{e_2}=(1000, 1)$. Then $z_e$ is not an invariant feature, but clearly the representation $f(z_e)=sign(z_e)$ is invariant, and is very informative of $y$. I think this is a major confusion in the article, which becomes clear only when ones writes down this definition formally.\n\np.4 §1: Observe that [...] the invariant classifier does not. -> This is I believe the major flaw of the article. The objective in OOD generalization is to find the Bayes classifier, not the \"invariant classifier\". Likewise, the objective of IRM is not to learn the \"invariant classifier\". As such, showing that IRM does not recover this \"invariant classifier\" has little value.\n\np.4 §3: Since those are informal theorems, I would rather name them differently, e.g.,  Result 1, Result 2, Result 3.\n\np.4 Section 4: The definition of IRM should be presented before the Section 3: results.\n\np.4 Section 4 §2: The authors argue that such a function will use only invariant features -> I do not find this argument in the original IRM paper.\n\np.5 Theorem 5: E training environments -> and how many environments in the test distribution ?\n\np.5 Appendic C.1: In Appendix Figure C.1, I believe the y axis should read $\\eta$, not Accuracy ? Also, I fail to understand why ERM is a flat line in this Figure. If more environments are provided, even with unlimited data, then the ERM classifier should change, and the test performance as well. Maybe I am missing something here, but the experimental setup (such as how model parameters are chosen for each new environment) is not clearly described.\n\np.5 Corollary 5.2: I am not sure what this implies. Maybe IRM does not recover the \"invariant classifier\" in that case, but what we really want for OOD generalization is the Bayes-optimal classifier. Again, it is not clearly stated what IRM wants to achieve...\n\np.6 Theorem 5.3: The initial assumptions these results rely on should be stated formally. Do these results hold for any $p(x,y,e)=p(e)p(y)p(x|e,y)$ distribution ? For any $p(x,y,e)$ distribution satisfying the IRM assumptions ? Only for the specific model the authors consider ?\n\np.6 Theorem 5.3: feasible -> Feasible for IRM ?\n\np.6 Figure C.2: Some labels are missing from Figure C.2. What do the doted horizontal lines represent ?\n\np.8 Conclusion: the number of training environments needed to ensure good generalization under arbitrary distribution shift is linear in the number of non-invariant features -> I believe this is wrong. The Bayes classifier is the one ensuring good OOD generalization. The \"invariant classifier\", which you prove requires a number of environments equal to the number of non-invariant features, is not the Bayes classifier.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}