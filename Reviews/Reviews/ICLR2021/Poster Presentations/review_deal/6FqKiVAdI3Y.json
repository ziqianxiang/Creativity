{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a decomposition of the value function in the context of CCDA.\n\nMost reviewers find this paper clear and well written, although one reviewer suggests to change the paper structure.\n\nThe method presented in this paper is simple and well justified by a theoretical section. Experiments on several domains, including Starcraft 2 micro-management tasks, are supporting the claims of that section. After some reviewers pointed out that the tabular setup is not useful in practice, the authors have extended the empirical and theoretical results to a more general setup.\n\nSome reviewers point out that some theoretical results may not be directly related to the experimental findings. In particular, reviewer 3 does not support a central claim of the paper, and find that CDM is misleading and not provably representing the core problem.\nIn general, reviewer 3 does not support acceptance of this paper, but I still believe this paper should be accepted based on the other reviews (clearly in favour of acceptance). I hope that the authors and reviewer 3 will be able to further discuss and reach understanding, which hopefully should lead to fruitful results."
    },
    "Reviews": [
        {
            "title": "Review of DOP",
            "review": "Summary:\n\nThe paper proposed a simple but powerful idea of assuming a linear decomposition structure of the centralized critic into individual critics. It demonstrated 2 existing problems in the baseline MAPG agents (COMA, MADDPG), namely \"centralized-decentralized mismatch\" problem and credit assignment issues, and offers a theoretically motivated solution with carefully conducted experiment results including ablation studies as well as comparison with sota gradient-based and value-based algorithms.\n\n##########################################################################\n\nReasons for score: \n \nThe paper is very well written. Its proposal is simple but theoretically motivated to solve existing problems in MAPG algorithms. The experiment results support its claims and provides additional insightful analysis. Therefore I recommend \"accept\". However, its theoretical justification is weakened by assuming a tabular case of pi and Q functions which is rarely the case in practice. Furthermore, in Proposition 1, the conditioning just above Eq. 11 looks quite particular and I'm not sure how general it is satisfied in practice. Last but not least, it would be nice to see comparisons with more recent value decomposition algorithms such as NDQ and QTRAN/QPLEX. Overall I gave it a score 7.\n \n##########################################################################\n\nPros: \n\n1. The paper is organized in very clear fashion. It is well motivated by existing problems and provides a simple and effective solution. \n2. It conducted clever experimental analysis that made the advantages of DOP very obvious and understandable.\n3. Its method of linear decomposition of the central critic into individual critics is simple yet powerful. The paper gave adequate theoretical support to this method. \n \n##########################################################################\n\nCons: \n\n1. Both the tabular assumption on functions pi and Q, and especially the conditioning of Proposition 1 weaken the theoretical guarantee that the algorithm will converge to local optima.\n2. The paper did not compare with more recent value decomposition algorithms such as NDQ and QTRAN/QPLEX. It's unclear that the baselines are the SOTA results on the chosen domains.\n \n##########################################################################\n\nQuestions during rebuttal period: \n\n Please address the two points in \"Cons\" above.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Superlative work",
            "review": "This works motivates the use of a factorized critic for multi-agent policy gradient.   The technique is well-motivated, and the exposition anticipates and answers readers' likely concerns.   The experiment section is well-organized, supports the paper's major claims, and is empirically compelling.\n\nThe policy improvement claims in section 4.1.2 are initially unintuitive, but ultimately are intelligible as an agent-block-coordinate local optimality statement.  However this reviewer is not clear on the quality of these local optima (i.e., when do we get \"trapped\"?).  For example, is it possible to design a task where the local optima are all very poor?   Of course, the experiment section indicates many benchmark tasks are amenable to this decomposition; but perhaps reasoning about this would help in (re)defining multi-agent problems to encourage success, e.g., it would be interesting if adding actions that communicate information directly between agents mitigates the local optima problem.\n\n\n\n ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "More clarity needed as problem discussed is not well motivated",
            "review": "This paper focuses on the problem of multi-agent reinforcement learning (MARL) for CTDE scenario which is well studied in recent literature.  The work discusses shortcomings of actor-critic methods for MARL and proposes a solution using linearly factored critic. The paper is somewhat difficult to read and can be made better by deferring the details about previous methods to appendix. However my main concern is with the problem of centralized-decentralized mismatch (CDM) motivated in the paper and its proposed solution itself. \n\n1. How exactly is a regular critic bad? As such a critic is supposed to be \"true\" to the policy, the requirement of decentralization has little bearing on the variance of policy gradients. Gradient noise increases with number of agents irrespective of whether there is centralized or decentralized execution. \n\n2. The so called problem of CDM seems rather redundant (see 1 above), for example the authors say in page 3, 3rd para from bottom in line 3 that if the critic expectation under policy is negative, then individual policy performance is hurt. Such problem can easily be fixed using baselines, see Sutton and Barto, 2018 for example.\n\n3. How is a linear factored critic compatible with an arbitrary joint policy? In general this not true and requires many strong assumptions, see for ex. Bhatnagar, 2009. While the authors acknowledge this, bypassing the actual complexity for modelling a joint critic with a linear one will in general render it insufficient to model inter-agent interactions. This puts into serious question, whether coordination is required in the experiment domains in the first place and if the performance improvement is just coming due to a biased but albeit easier to learn critic. \n\n4. There are some unsupported claims which need better explanation like \"This becomes problematic because a negative feedback loop is created, in which the joint critic is affected by the suboptimality of agent i, which disturbs policy updates of other agents\" How is that so? The updates in principle can affect the policies of already suboptimal agents, which might fix them?\n\n5. \"Learning the decomposed critic implicitly realizes multi-agent credit assignment, because the individual critic provides credit\ninformation for each agent to improve its policy in the direction of increasing the global expected return\" again how so? claims like this need to be well supported.\n\n6. Expectations are usually sampled so in principle even the $O(|A|^n)$ can be estimated with fewer samples incurring some variance, it might not be necessary to bias the critic drastically for this.\n\n7. The authors need to shed more light on when the precondition $Q_i(\\tau, a_i) > Q_i(\\tau, a_i') \\iff \\beta_{a_i, \\tau}\\geq \\beta_{a_i', \\tau}$ in Prop. 1 holds beyond tabular settings. It seems a rather strong assumption to hold for all trajectory and \n$O(|A|^n)$ inputs. Right now it seems rather grab bag to show policy improvement.\n\n8. Why isn't comparison on SC2 done against more recent baselines like QTRAN, MAVEN, ROMA etc.?",
            "rating": "3: Clear rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Value decomposition and off-policy critic training for multi-agent RL with good experimental performance",
            "review": "In the context centralized training distributed execution in cooperative multi-agent reinforcement learning (MARL), the paper proposes an architecture to learn a decomposed action value function expressed as a weighted sum of the agent's individual functions (plus an additional weight). Those weights are themselves learned and depend on the observed history. Thanks to this decomposition, gradients can be decomposed over each agent. The authors propose to use a combination of off-policy (using tree backup) and on-policy (using TD(\\lambda) methods for estimating the decomposed critic. They formulate both a deterministic and stochastic decomposed policy gradients, which are analyzed theoretically to some extent and evaluated experimentally.\n\nPROS\n\nThe paper contributes to the active effort of designing more efficient MARL algorithms. The authors introduce the idea of value decomposition, which was investigated first in the value-based methods, to the actor-critic scheme. The experimental results suggest that the proposed combination of value decomposition and off-policy critic training has a good performance.\n\nThe writing of the paper is clear. The identification of the issue of centralized-decentralized mismatch and how it is tackled by the proposed methods are useful and interesting.\n\n\nCONS\n\nThe formulation of Prop. 1 is hard to understand. What is Q_i^{\\phi_i}? Is it the value function wrt \\pi or \\pi^0? Besides, how do we ensure that the strict inequality about Q_i^{\\phi_i} holds?\n\nSome theoretical analyses (e.g., A3 or C) are only loosely related to the actual proposed method, although I agree that a direct analysis would be difficult to conduct.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}