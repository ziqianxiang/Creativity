{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents an interesting mix of new theoretical and empirical results showing how learning temporally extended primitive behaviors can help improve offline (batch) RL.\n\nAlthough 2/3 reviewers initially raised concerns regarding the motivation of the approach and some of the choices that were made, the authors did an excellent job at addressing these concerns in detail, and there is now a consensus towards acceptance.\n\nI consider that this work is a meaningful contribution towards better offline RL, which is definitely a very important use case in practice. The authors have given convincing explanations to motivate their approach, and made several improvements to the paper. As a result, I am recommending it for acceptance, as a poster."
    },
    "Reviews": [
        {
            "title": "While interesting,  baselines are weak and OPAL does not directly address problems in offline RL",
            "review": "Summary\n-------\n\nTo best leverage diverse datasets of logged experience, the authors\npropose to extract a space of primitive behaviors in a continuous space,\nand to use these for downstream learning. The primitive behaviors are\nlearned through a VAE loss, and CQL is applied to learn a policy over\nthe primitives. The authors claim that this approach to offline RL\navoids known distribution shift and allows for temporal abstraction. The\nmethod is also applied to few-shot imitation learning, exploration and\ntransfer to online RL.\n\nDecision\n--------\n\nDespite this paper being an interesting read, I feel that my concerns\nabout the experiments lead me to not be confident in the proposed\napproach. As such, my preliminary rating for the paper is \"Okay but not\ngood enough\". The baselines chosen for the experiments do not seem\nrepresentative of the problem being addressed. This also leads into the\nmotivation, where OPAL is motivated from the offline RL perspective but\ndoes not explicitly mitigate the issues in offline RL. Only the\ntheoretical results investigate the effect of a fixed replay-buffer, but\neven these claims are framed in terms of what $\\mathcal{D}$ should be to\nensure downstream RL.\n\nOriginality\n-----------\n\nThe proposed approach seems limited in its novelty, combining approaches\nfrom skill-discovery, hierarchical RL and somewhat from offline RL. The\ntheory section is interesting, however the technical arguments seem\nsimilar to that of Nachum et al. (2018).\n\nQuality and Clarity\n-------------------\n\nThe overall quality of the paper is very high. The writing is clear, and\nthe theoretical arguments are put into an easily understandable context.\n\nStrengths\n---------\n\n-   OPAL leverages techniques from many areas of machine learning:\n    unsupervised learning, hierarchical RL and offline RL. This is an\n    interesting combination, and deserves to be investigated. It seems\n    like the workhorse of OPAL is the unsupervised learning component\n    however, and perhaps this should be emphasized and investigated\n    independently.\n-   The theoretical section is very clear and well contextualized. The\n    claims in the paper seem correct, and they do provide much needed\n    validation for hierarchical RL. The assumption of an optimal\n    high-level controller seems strong, but the analysis is nonetheless\n    interesting.\n\nWeaknesses\n----------\n\n-   Despite Figure 2, OPAL is difficult to disentangle with many moving\n    parts. It doesn't help that there is no discussion of how\n    hyperparameters were chosen for each component, or an ablation study\n    to investigate different hierarchical RL approaches, fine-tuning,\n    offline RL algorithms, or VAE losses.\n-   The proposed method is not specifically designed to leverage\n    anything particular in offline RL. As you show, it can be applied to\n    online RL. However, why is this motivated from the offline\n    perspective? OPAL itself does not seem to mitigate distributional\n    shift.\n-   It would be helpful to directly compare to other skill-discovery\n    methods that have been applied to online RL but can be adapted to\n    offline RL. For example, what prevents the work by Campos et\n    al. (2020) and the baselines therein to be applied in conjunction\n    with an offline RL algorithm? The baselines for the experiments do\n    not seem representative of the problem you are addressing. You use\n    standard offline RL algorithms, yet claim that the temporal\n    abstraction of skill discovery is crucial to the results. As such,\n    you should compare OPAL to other skill-discovery algorithms that can\n    be combined with offline RL.\n\nDetailed Comments\n-----------------\n\n-   Section 4.1: \" Prior: ρω(z|s0) tries to predict the encoded\n    distribution of the sub-trajectory..\" Prior doesn't seem like the\n    right word here, since it is being learned.\n-   Section 4.2: In what way is the task-specific policy $\\pi_\\psi$\n    different from the learned prior in Section 4.1? Both are\n    distributions over latent $z$ conditioned on a state. While the\n    prior is designed to only be conditioned on the initial state, this\n    is still quite similar to the high-level policy $\\pi_\\psi$.\n-   Section 4.2: \"\\[to\\] ensure that the c-step transitions remain\n    consistent … with the labelled latent action $z_i$\" Why is this\n    necessary, should CQL ensure consistency at the action-level, while\n    the latent actions are consistent by design?\n-   Corollary 4.1.1: How would $\\epsilon$ actually approach\n    $\\mathcal{H}_c$ in the algorithm? The condition is never explicitly\n    enforced because you formulate it as a constrained optimization\n    problem and you are never able to change $\\mathcal{H}_c$ since it is\n    a constant.\n-   Section 5.1, baselines: shouldn't baselines be compared to different\n    unsupervised skill discovery algorithms, paired with offline RL\n    algorithms? BEAR and EMAQ are offline RL algorithms without any\n    temporal abstraction. And as you say in the results section with an\n    ablation study, this temporal abstraction is crucial.\n-   Section 5.1, results: An ablation study is discussed but I cannot\n    find the corresponding results table/figure in the paper.\n-   Section 5.2, Table 3: With so many 0.0's, this leads me to believe\n    that the baselines are quite weak, or the problem is too hard for\n    the baselines. How were the baselines chosen. For example, why was\n    DDCO paired with DDQN instead of SAC?\n\nMinor Comments\n--------------\n\n-   Section 5.2 mis-capitalized We \"in this setting We use the Antmaze\n    environ-\"\n-   Section 5.3, Table 4: why are there no standard errors for SAC?\n\nPost Rebuttal\n--------------\nAfter discussion with the authors, I have decided to increase my score to a 6. The authors have addressed many of my concerns with respect to motivation, theoretical analysis and empirical evidence. As it stands, I still think OPAL is hampered by the many \"moving parts\" involved. The theoretical analysis and empirical evidence suggests a very effective approach however, and should lead to further work combining variational sequence encoding techniques for primitive extraction in RL. My decision would be to \"accept the paper if there is room\".\n\nReferences\n--------------\nCampos, Víctor, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i-Nieto, and Jordi Torres. 2020. “Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills.” *arXiv:2002.03647*.\n<http://arxiv.org/abs/2002.03647v4>.\n\nNachum, Ofir, Shixiang Gu, Honglak Lee, and Sergey Levine. 2018. “Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.” *arXiv:1810.01257*. <http://arxiv.org/abs/1810.01257v2>.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper with (potentially) practical values for real-world robotic tasks",
            "review": "## OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning\n### Summary\nThe authors present OPAL, an offline reinforcement learning approach that distills useful common behaviors from offline transition data. They leverage a variational GRU to encode trajectories into a latent space of primitive policies. These learned primitives can later on be transferred to other tasks by learning a high-level controller over the extracted primitive latent space. Their experimental results show that OPAL compares favorably against other SOTA methods in offline, online and few-shot reinforcement learning settings.\n\nOverall, the paper is clearly written, and the approach is potentially practical for real-world applications, due to its good performance on both offline and few-shot adaptation settings. \n\n### Strength\n- The proposed method can leverage large unlabeled data without pre-defined reward functions.\n- The property of OPAL is mathematically analyzed.\n- OPAL can potentially benefit from the advancement of offline RL algorithms by simply replacing CQL.\n- Can be adapted to a new task through few-shot imitation learning.\n\n### Weakness\n- Training details are missing for the OPAL encoder.\n- Not exactly a weakness, but it would be nice to see if the method works in real-world settings (real-world on/offline data). ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting research question but motivation of the method unclear",
            "review": "In the RL setting, this paper tackles the case where an agent may have access to large amounts of offline experience data. The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors. The paper provides results that show how performing offline primitive learning can be leveraged for improving few-shot imitation learning as well as exploration and transfer on a variety of benchmark domains.\n\nThe paper tackles an important question in reinforcement learning: learning temporally extended primitive behaviors from off-policy data is a very relevant question. However, I found the motivation for the approach quite vague as well as different elements that require clarification (see below) and because of this, I can't recommend acceptance.\n\nMotivation for the approach: \n- if the downstream policy has to stay close to the offline data distribution, it seems to me that if the offline data distribution is obtained with a bad policy, this can not lead to interesting decision-making.\n- can you explain why equation 2 \"motivates better generalization\"?\n\nOther concerns:\n- Why can gamma not have a value of 0 (first paragraph preliminaries)\n- What does it mean \"To capture multi-modality in a dataset\" ? (second paragraph preliminaries)\n- Figure 1:  What does the (81.1%) and (70.3%) refer to?\n- Table 4: why is SAC given without standard deviation?\n\nSome text improvements:\n- \"we focus on focus on\"\n\nOther comment:\n- The number of seeds (4) should ideally be increased.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}