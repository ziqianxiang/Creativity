{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a model for video action recognition.  The reviewers appreciated the development of a novel dynamic fusion method that examines channels from feature maps for use in temporal modeling.  After reading the authors' responses, the reviewers converged on an accept rating.  The solid empirical results and analysis, the fact that is is a plug-in method that could be used in other models, and the clear exposition were deemed to be positives.  As such, this paper is accepted to ICLR 2021."
    },
    "Reviews": [
        {
            "title": "This paper presents an approach to dynamically fuse channels from current and past feature maps for strong temporal modeling of human activities. The authors claim that using a skipping operation reduces the computational complexity of action recognition.",
            "review": "Authors assessed how their adaptive temporal fusion network performs on public datasets such as Something V1&2, Kinetics, etc.. The contribution of this paper is in proposing an approach to automatically determine which channels to keep, reuse, or skip per layer and per target instance that can result in efficient action recognition. \n\nSTRENGTHS: \nThe proposed method is model-agnostic, making it easy to use as a plugin operation for other network architectures. \nReusing history features when necessary to make the network capable for strong temporal modeling. \n\nCONCERNS:\nThe paper has examined the temporal fusion module on BN-Inception and ResNet models, while more recent models’ evaluation is missing. \nWhile the policy network is defined as two FC layers and a ReLU, it is not clear why the authors chose this architecture and how they have tuned it?\nIn section 3, Using 2D-CNN for Action Recognition, a citation to one of the recent works in modeling the temporal causality is missing: Asghari-Esfeden, Sadjad, Mario Sznaier, and Octavia Camps. \"Dynamic Motion Representation for Human Action Recognition.\" In The IEEE Winter Conference on Applications of Computer Vision, pp. 557-566. 2020.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A good paper that needs some work",
            "review": "#################################\n\nSummary:\n\nThe paper presented an adaptive inference model for efficient action recognition in videos. The core of the model is the dynamic gating of feature channels that controls the fusion between two frame features, whereby the gating is conditioned on the input video and helps to reduce the computational cost at runtime. The proposed model was evaluated on several video action datasets and compared against a number of existing deep models. The results demonstrated a good efficiency-accuracy trade-off for the proposed model.  \n\n#################################\n\nPros:\n* The paper has a novel idea (adaptive temporal feature fusion) and addresses an important problem in vision (efficient action recognition). \n* Solid experiments on multiple datasets. The analysis of the learned policy is quite interesting.\n* Well-written paper\n\n#################################\n\nCons:\n* Limited technical novelty\n\nThe idea of building adaptive inference models with a policy network for video classification has been previously explored by Wu et al., Meng et al. and others (e.g., skip part of the model, select a subset of frames, choose the input resolution to the model). The main technical component of the model is also very similar to the channel gating network (Hua et al.). The key innovation seems to be the perspective of modeling temporal feature fusion for adaptive inference. This is probably best considered as in parallel to previous approaches for adaptive video recognition. The technical components thus look less exciting.\n\n* Lack of comparison to other adaptive inference models / temporal fusion schemes   \n\nThere isn’t a real comparison between the proposed method and recent works on adaptive inference video recognition (e.g, Wu et al, Meng et al.). The benefit of model temporal feature fusion --- a main contribution of the paper, thus remains unclear with respect to other design choices (e.g., input resolution or frame selection). I’d suggest some experiments that compare to those work. Another important experiment is to contrast the proposed method with other temporal feature fusion schemes (e.g, LSTM, TSM). For example, TSM --- a hand-crafted feature fusion module, seems to have less number of parameters, slightly higher FLOPs and comparable accuracy (Table 3). If that is the case, the contribution of the proposed adaptive fusion scheme is much weakened. \n\n#################################\n\nMinor comments:\n\nIt is not totally clear to me how the FLOPs of the proposed model are computed. As the proposed model will have a different FLOP conditioned on the input video, were the reported FLOPs averaged across the dataset? I was not able to find a description in the paper. \n\nIt will be great if the authors can report some run-time performance (e.g., wall time). To achieve the theoretic FLOPs, the proposed model will rely on filter re-arrangement on the fly and sparse convolution kernels. Both can be less efficient on certain devices, e.g., GPUs.  \n \n\n#################################\n\nJustification for score:\n\nAll in all a good paper. My main concern is the missing link / comparison to previous works on adaptive video recognition. If this concern can be addressed, I am happy to raise my rating. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Weakness in the experiment to support the effectiveness of the proposed method and unclarity in some of the important details",
            "review": "#### General\nThis paper proposes an adaptive temporal fusion network called AdaFuse for action recognition, which adaptively removes temporal redundancy and reuses past features for accuracy and efficiency.\nI listed the Pros and Cons I found in the paper below as well as some questions to clarify some of the details.\n\n#### Pros\n1. The idea of learning a decision policy to dynamically determine whether channel-wise features at time $t$ are calculated normally, reused from $t-1$, or skipped, is interesting and reasonable.\n1. The experimental results show that the proposed method achieves good accuracy with reasonable computational budget.\n1. The ablation study in Table 4 reveals that the performance is greatly affected by the policy and it is important to fuse the futures from different frames to captures the temporal dependencies.\n\n#### Cons\n1. The propsoed method is not compared with some of the recent methods such as [1-3] ([4] is optional because the publication date is very close to the ICLR 2021 submission deadline). Especially for Jester and Mini-Kinetics dataset, the proposed method is compared with only TSN, which is old and weak as baseline as it does not incorporate the temporal information.\n1. In Table 3, it seems that the proposed method achieves good accuracy, but I am afraid that it is just because of the strong base network, TSM. Merely adding AdaFuse to TSM indeed saves some computation but degrades the performance as described in the paper. The proposed remedy indeed slightly improves the accuracy but it requires much more parameters compared to the vanilla TSM. Overall, I find it benefitial to use the proposed method on top of simple base networks such as TSN, but the benefit of using the proposed method on top of strong base networks such as TSM may be marginal. Combined with the point 1 above, I am not well convinced of the effectiveness of the proposed method.\n1. Some of the important details are not clear. I would appreciate if the authors could answer the questions I listed below.\n\n#### Questions\n1. Is it necessary to use Gumbel softmax? I think there are two kinds of tricks involved in Gumbel softmax. One is a trick for sampling from a categorical distribution, and the other is a trick for making the opperation differentiable. In my understanding, which may be wrong, the required characteristic for the present method is the latter one, and the sampling from the categorical distribution is not necessarily required. In this case, I think simply using $q$ instead of $\\log{r} + G$ in equation (7) is enough.\n1. Related to the point above, please clarify the type of output (hard or soft) of the policy net. The sentence after equation (2) says the output is integer values (0, 1, or 2), while the sentence before equation (7) says it is a real-valued vector.  \n1. Suppose $p_t^i = 1$ (reuse) and $p_{t-1}^i = 1$ (reuse again). In this case, is $y_t^i$ copied from $y_{t-2}^i$ ? Or is the feature map of $i$-th channel at time $t-1$ calculated on the fly for \"reusing\" at time $t$? In other words, if the policies for a channel is \"reuse\" $n$ consecutive times, does the method take the feature from $n$ frames before? \n\n\n#### Other comments\n1. Figure 1 may be incorrect or misleading. I think $p_t$, the output of the policy net, should go to the 2D Conv. block. Otherwise the block never knows which channel to compute at time $t$ and which channel to reuse or skip.\n\n[1] Sudhakaran+, Gate-Shift Networks for Video Action Recognition, CVPR 2020\n[2] Martinez+, Action recognition with spatial-temporal discriminative filter banks, ICCV 2019\n[3] Jiang+, STM: SpatioTemporal and Motion Encoding for Action RecognitionSTM: SpatioTemporal and Motion Encoding for Action Recognition, ICCV 2019\n[4] Kwon+, MotionSqueeze: Neural Motion Feature Learning for Video Understanding, ECCV 2020",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An Adaptive Temporal Fusion (AdaFuse) network has been proposed to balance efficiency and accuracy in video action recognition",
            "review": "In this work, the authors introduce an AdaFuse network for efficiency action recognition in videos. Specifically, they design a policy net to decide which channels should be kept, reused or skipped, according to the input features of two adjacent frames.\n\nStrength \n\n1 The paper is written well, and the organization is OK\n\n2 The idea of adaptive temporal fusion is somehow novel and interesting \n\nWeakness\n\n1 How to save computation. I understand the general idea of saving computation, if some channels are reused or skipped. However, in the training phase, the policy net would produce the real-value vector by Eq. (7), instead of the one-hot vector. In other words, the 'keep' entry for each channel is always used during training. Then, I guess computation saving is not claimed for training. It is for testing, right? How to do test? The policy net produces the real-value vector and then you make it as one-hot vector for saving computation?\n\n2 Missing SOTA. Compared with this paper, many recent approaches can achieve a competitive computation with better accuracy. It significantly reduces the potential value of this paper.\n\n*Jiang et al., STM: SpatioTemporal and Motion Encoding for Action Recognition, ICCV 2019\n\n*Li et al., TEA: Temporal Excitation and Aggregation for Action Recognition, CVPR 2020\n\n*Sudhakaran et al., Gate-Shift Networks for Video Action Recognition, CVPR2020\n\n*Liu et al., TEINet: Towards an efficient architecture for video recognition, AAAI 2020\n\n3 Please correct the abstract. The experiments are performed on mini-Kinetics, rather than Kinetics. I indeed suggest that, it would be better to perform the proposed method on Kinetics to further show the effectiveness.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}