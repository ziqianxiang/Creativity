{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes to do a fine-grained analysis of how shape and texture play a role in the decisions made by CNNs. Lots of recent evidence suggests that CNNs exhibit a texture bias, and there has been considerable effort in understanding where this comes from and how to overcome it. The paper focuses in particular on understanding: (a) what fraction of the neurons are devoted to shape-vs-texture (roughly speaking), and (b) per-pixel results using a convolutional readout function. The reviewers were divided at the time of submission and remained so at the end of discussion. At the end of discussion, the reviewers were split, with scores ranging from 4 (R1,R4), 7 (R2), and 8 (R3). The AC wants to thank and acknowledge the authors as well as all of the reviewers for their engagement in the discussion.\n\n- R2 and R3 are largely positive, driven by the extent of the experiments and the number of interesting results (e.g., how the fraction of the dimensions used for shape changes as a function of depth in the network). Both had smaller non-critical concerns that were addressed (as far as the AC can tell) in the discussion. \n- R4’s most important concern, in the AC's view, is the question: could these results / different conclusions have been obtained via linear probe methods like Hermann et al.? The authors argue that analyzing the fraction of neurons used and at a per-pixel is more fine-grained than linear probes. This boils down to an intangible question of contribution, on which the AC is inclined to agree with the authors and R2 and R3: analyzing the dimensions contribute provides, at least to the AC, a complementary view to the linear probe and that this will be of interest to the ICLR community (although see final comment). R4 also had a number of smaller concerns that seem to be largely addressed (e.g., about correctness).\n- R1 argues primarily that the paper does not have a clear point or methodological contribution, for instance pointing out that readout modules were used in Hermann et al. or (as an example) arguing the readout function design is too simple.  The AC is inclined to agree with the authors’ response that the other reviewers seem to largely agree on the contribution (especially contributions via experiments rather than method) but disagree on how to weigh these contributions. The AC would also add that readout modules are a core idea for understanding neural representations that long predate Hermann and are by design (as the authors note) almost always as simple as possible.\n\nAt the end of the day, the AC is agrees with R2 and R3 for the contribution of the work and is inclined to accept. Given the other reviews, the AC does not agree with R1’s arguments, but would suggest that the authors think about how to sharpen their claims further. The AC is sympathetic to the concerns of R4, and urges the authors to think about a more concise and clean argument for R4’s concerns --- many other readers will have similar concerns and as clean of an illustration will be helpful. Overall, the AC believes that the paper’s methods, experiments and analysis are of interest and value and is thus in favor of acceptance."
    },
    "Reviews": [
        {
            "title": "Interesting examination of shape vs texture in internal representations",
            "review": "Texture vs shape sensitivity is a basic and important question in understanding how deep convolutional nets work. This paper investigates this question by asking how CNNs represent shape versus texture information internally.  Using a (texture-vs-shape) stylized imagenet pioneered by Geirhos (2018), the paper applies a dimensionality estimation method of Esser (2020) and a segmentation-readout method for quantifying and visualizing the encoding of shape information in networks.  Several different network architectures and layers are compared; and results are compared to baselines from previous papers as well as natural lower- and upper-bounds.  The presentation is clear.\n\nThe paper makes several contributions, extending beyond Geirhos 2018 by investigating the internal representations of shape- vs texture rather than just the external performance characteristics.  It extends beyond Esser 2020 by more fully examining shape versus texture representations, and comparing a variety of networks and layers.  It extends beyond Bau 2017 by counting partially correlated neurons and explicitly training segmenters to explicitly read out local shape or class information.  The sharp increase in shape representation dimension in the last stage of resnet is an interesting finding, as is the rapid growth of shape representations early in training. The measurement of texture over shape representations in BagNet is a nice confirmation that the proposed measurements match observed behavior. The use of segmentation readout, tested several ways, provides helpful and new qualitative visualizations.\n\nThe paper could be strengthened if it proposed further testable hypotheses.  The paper does confirm that the measured shape dimensions is lower for BagNet architecture with reduced receptive field, but beyond observing correlations, the paper does not try to directly verify that the identified shape dimensions are responsible for the network’s shape-recognition abilities.  For example, would the shape segmentation readouts be more damaged by removal of the most shape-specific dimensions (vs other neurons)?  Or would the removal of these dimensions affect external shape or texture performance?  If an effect like this were verified, it would strengthen confidence in the proposed correlation-as-dimensionality measure of shape and texture content.\n\n**edit, after revisions**  As mentioned in discussion below, I think the addition of intervention experiments makes the effects very clear and strengthen the paper.  I revised my score from 7 to 8.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "New approach to understand the shape bias as a function of the visual hierarchy of CNNs",
            "review": "Pro’s: \n* The question that the authors are trying to understand is interesting and relevant in the field of object recognition, texture/shape bias, and learned representations in deep neural networks.\n* Authors provide a nice set of controlled experiments that suggest some of these effects, and authors present a scientific login in their approach (that although it is not perfect), it is quite relevant for the field. This is not “yet another paper on trying to overcome the texture bias without any intuition what-so-ever, to try to get better numbers (as has unfortunately been done in computer vision these days)”, on the contrary, this paper is about understanding the underlying mechanisms of the texture/shape bias that extend the final stages of computation in the visual hierarchy, and is why I am leaning towards accept.\nNearly all figures in the paper are clear and help convey what the authors are trying to express (although see somes clarification points)\n* The paper is clear and easy to read/understand.\n\nCon’s:\n* Some experimental evaluations/methods are not clear (see below Observeration/Clarifications that if clarified may convince me to raise my score).\n\n----\n\nObservations/Clarifications: \n\nFigure 3 shows an interesting plot. It would have been great to compare the result with that of Stylized ImageNet as well (that presumably will have a greater increase in shape-tuned neurons vs texture-tuned neurons).\n\nWhat is a “stage” in the ResNet? Is this a residual block, or a specific layer? Stages are mentioned and used throughout the paper but are never formally defined. I have an intuition that they are related to the depth in the hierarchy of the network, but not on the layer type or the computation (example: is it a conv layer, is it rectified?) That being said, why is Stage 1 not included at all? (presumably it is the input/image so it is redundant?) Authors should make this more clear.\n\nThe Binary and Semantic Segmentation tasks are not clear. The binary segmentation task’s goal is to purely create a binary mask of any object? Or more than one? Does the mask have to be a closed contour? Further what is confusing about the semantic segmentation task is if the pixel-wise labeling is done GIVEN the ground-truth binary mask or the predicted binary mask. This should be made more clear in section 3.1\n\nSection 3.2.1: Knowing an Object’s shape implies knowing its semantic class: I don’t think the results in Table 6 actually show what the authors claim. If by default the output of the binary masks are different, these effects will naturally carry over into the semantic segmentation procedure. The only way to truly account for the proposed question of “if shapes implies better semantic class”, is if all systems are equalized and given with the SAME binary mask (assume ground truth), and must use this oracle to then perform semantic segmentation. If the disparity is still great, then the before-mentioned claim has stronger computational support. Otherwise, I can see that there the confounding variable is that the nature of one system knowing how to binarize the image better a priori, will lead to differences in the final semantic segmentation score/maps.\n\n----\n\nAltogether the paper is well written and there is a good body of results/evaluations that support some of the claims the authors are doing. The problem they are working on is interesting and relevant to the ICLR and greater CV/ML/Vision Science community. I am leaning towards acceptance, and would appreciate it if authors clarify my doubts above to raise my score.\n\n----\n*Update as of December 5th. I would like to thank the authors for clearing up my concerns and would like to raise my score from 6 to 7. I think this paper would be a good poster that can provide complimentary insights through different experiments to the recent paper of Hermann et al. NeurIPS 2020.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Comprehensive analysis but limited novelty",
            "review": "The method provides two measures for assessing the degree at which shape is represented in CNNs. The first measure attempts to asses, within a given representation layer, the dimentionality required to encoder shape information. The second, evaluates the per-pixel shape representation, by attempting to generate the input's segmentation mask from the given representation (activation of the image at that layer). \n\n*****\n\nPros:\n\nThe method provides a comprehensive review of the encoded shape information, both on the representation level and pixel level. \nIt answers questions in a variety of settings:\n1. The shape encoded in different stages, or representation layers, for standard networks, and for networks trained to aleviate the shape bias (SIN, SIN + IN)\n2. The degree of shape representation as a function of epochs or time trained. \n3. The per pixel representation of shape, i.e to what degree can the semantic map of the input image be recovered from a given representation layer. \n\nThe paper is also very clear and results are presented well and well explained. \n\nCons:\n\nRegarding the first of the two methods (A in Fig 2), I am not convinced of its correctness. Eq. 1 upper bounds the mutual information using variable p_i, which can be calculated. However, with a lack of a lower bound, this doesn't really mean much. The actual mutual information can be much higher. Why is this bound tight?\n\nRegarding the second of the algorithm (B in Fig 2), existing algorithms in literature, presented for example in [1], learn a classifier, that given the representation of an image, attempts to classify it shape. As such novelty of Alg.2 is very limited: Instead of classifying the shape, the method is trained to produce a segmentation map. \n\nIn addition, the use of a classifier as in [1], could also be used to quantify the use of shape in different representation layers. Why is it better to use the existing formulation instead of a classifier? Can't the use of a classifier also replace the need of Alg.1 in the sense of capturing the amount of shape information used? Does the existing measure reveal something more interesting in Tables 1-4 and Figure 3?\n\n********\n\nOverall, while the paper provides a comprehensive analysis of the use of shape in CNNs, both on pixel level and the representation level, I am not sure of the correctness of Alg.1 and the originality and relevance of Alg.2 given existing work. \n\n[1] The Origins and Prevalence of Texture Bias in Convolutional Neural Networks. Hermann et al., ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Contribution limited and results not interesting",
            "review": "This paper performs a detailed analysis of shape vs texture tradeoff in deep neural networks. The authors use two methods to analyze the learned shape features in CNNs: one based on estimation of concept dimensionality, one based on reconstructing shape from latent representations.\n\n1. The contributions of this work is limited. First of all, the methods of this work is adopted or based on previous methods. The dimensionality estimation method is based on (Esser et al, 2020). The idea of using “read-out module” to segment images is similar to the idea in (Hermann et al, 2019) where a linear classifier is trained to predict the shape label. \n\n    Moreover, the results are not interesting. The analysis conclusions made in this paper are all expected and not surprising, though i have to acknowledge that this is a more fine-grained study. I would suggest the authors to stress a few key messages that you are trying to make in this paper, rather than the lots of analysis results presented in the current one without a central conclusion.\n\n2. Overall, I am not satisfied with the structure of this paper. It seems that the authors   struggle to put two not so relevant ideas together. I would like the authors to spend some time to discuss the motivation for using the two analysis methods and the dependencies between them. This will make this work coherent and consistent.\n\n3. The design of readout module is not clear to me. Specifically, why did the authors choose the readout module to be “either one or three convolution layers with 3×3 kernels”? The current one seems to be too simple, can their be other designs for the read out module? \n\n4. What hidden layer is analyzed for CNN in Table 1? \n\n5. What exactly is shown in Figure 1, i understand the subfigure “stylized image” and “semantic”, but what are the two figures in the middle “GT” and “Shape”? What are these two images? What does GT means? I didn’t find the authors rigorously define these for figure 1 in the paper.\n\nAfter rebuttal:  \nI appreciate the authors for the response. However, I feel that the conclusions drawn from this paper are not focused, there is no central message that i can get as a clear take-away for understanding CNNs after reading the paper. I do agree with each of the findings the authors make, it is just that the presentations and writings of the paper make me confused about what the authors are trying to convey through this paper. Therefore, i am keeping my score.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}