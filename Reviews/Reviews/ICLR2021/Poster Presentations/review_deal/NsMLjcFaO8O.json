{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviewers and myself agree that the contribution is clear, significant, and has enough originality. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.\n\nPros:\n- Solid technical contribution, specially the use of continuous noise levels.\n- Clever application of diffusion/score-matching models to a new domain and task, with conditioning.\n- Good empirical results, both objective and subjective.\n- Listening samples provided.\n\nCons:\n- Lack of formal comparison with flow-based vocoders.\n- Potentially limited novelty.\n- No official code available.\n\nNote: Readers may also be interested in concurrent work https://openreview.net/forum?id=a-xFK8Ymz5J (\"DiffWave: A Versatile Diffusion Model for Audio Synthesis\")."
    },
    "Reviews": [
        {
            "title": "Official Review ",
            "review": "This work proposes a neural vocoder based on denoising diffusion probabilistic model. It matches autoregressive neural vocoder in terms audio fidelity, but only requires constant number of sequential steps at synthesis.\n\nDetailed comments:\n\n1, The first key contribution is not convincing. The proposed method is a direct application of Ho et al. (2020) for waveform synthesis. The combination/connection between score matching and diffusion probabilistic model is a contribution in Ho et al. (2020).\n\n2, In general, flow-based models (e.g., WaveGlow, WaveFlow) can also match autoregressive model in terms of audio fidelity, while being much faster at synthesis (e.g., >20 times faster than real-time). What is the advantage of using WaveGrad as a neural vocoder? It provides comparable audio quality as flow-based models, but it is slower at synthesis (e.g., up to 5x faster than real-time). BTW, the authors may discuss WaveFlow (a SOTA flow-based model), which also introduces a trade-off between autoregressive and non-autoregressive models. It also needs constant number of sequential steps at synthesis. \n\n3, Section 2 is not well written and self-contained. For example, the authors simply list Equation (2-4) and references without further explanation. Indeed, they are not closely related to the technical contribution of this paper. One may either introduce in more detail, or skip some content as this work focuses on a particular parameterization of diffusion probabilistic model introduced by Ho et al. (2020).\n\nPros:\n- Solid technical contribution.\n- Good empirical results.\n\nCons:\n- It's unclear to me whether WaveGrad is a promising neural vocoder (see my comment 2).\n- Paper writing can be improved.\n\nI would like to raise my rating if my concerns are properly addressed.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A very good paper studying diffusion probabilistic models for conditional audio synthesis task",
            "review": "The work uses diffusion probabilistic models for conditional speech synthesis tasks, specifically to convert mel-spectrogram to the raw audio waveform. Results from the proposed approach match the state-of-the-art WaveRNN model. The paper is very well-written and it is quite easy to follow. The study of the total number of diffusion steps and two different ways (continuous and discrete) ways to feed it in the network is very interesting. It is quite relevant and important for speech synthesis tasks. Using this, authors are able to find a 6-step inference procedure that yields very competitive performance to WaveRNN while still being computationally feasible.\n\nPros:\n1. Great results for the neural vocoding task\n2. Exhaustive study of the diffusion steps and how to feed them to the network is valuable and original.\n\nCons:\n1. Study of the width/depth of the network can be more exhaustive.\n\n*Further comments*: Authors mention in the conclusion ````Wavegrad is simple to train`. What makes the authors say so? It would be great to substantiate it with evidence/comparison?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good paper, incline to accept",
            "review": "Motivated by recent works on score matching and diffusion probabilistic models, the authors presented a conditional, non-autoregressive model for waveform generation. The paper is well motivated and easy to follow. \nThe model starts from Gaussian white noise signal and iteratively refines the signal via Langevin dynamics-like sampler. The model is capable to generate high fidelity audio, outperforming a few adversarial non-autoregressive baselines even using only 6 iterations. Though the fundamental approaches of this paper have been applied in other domains, making the approach work well for waveform generation is relatively new and not easy.\nThe authors also had a deep dive on noise schedule, and proposed a variant of WaveGrad which conditioned on continuous noise level. The authors showed clear benefits of this variant over conditioning on discrete index, including using few iterations to generate high fidelity audio, being able to tradeoff between inference computation and output quality, fast hyper-parameter searching. However, one minor concern is that, if inference computation is not a bottleneck, when using linear schedule, for both 50/1000 iteration scenarios, WaveGrad conditioned on continuous noise level does not show clear benefits over conditioning on discrete index.\n\nThe other minor concern is regarding the model size. When comparing with adversarial counterparts, most models are with fewer parameters than the proposed WaveGrad. Though the authors claimed that model size has little effect on the performance of WaveGrad (according to Table 1), but WaveGrad does seem to clearly benefit from larger model according to Table C.1, the MOS for LJ speech datasets. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThe authors propose a conditional waveform synthesis model: WaveGrad, which combines recent methods from score matching and diffusion probabilistic models. \n\nPros:\n\n(1) The proposed diffusion probabilistic model-based approach achieves comparable results as the autoregressive WaveRNN and outperformed the non-autoregressive baselines.\n\n(2) The WaveGrad Base model using six iterations is much faster than the WaveRNN.\n\nCons:\n\n(1) The authors borrow ideas from Ho et al. (2020) for image generation to address the conditional waveform synthesis problem.  There are really limited innovations on the model side since the proposed method just combines some techniques from recent score matching and diffusion probabilistic models.  \n\n(2) Why the proposed WaveGrad Vocoder can synthesize high-quality speech comparing to LSTMs in WaveRNN and WaveNet in the original Tacotron 2? The authors should give more insights and explanations to better illustrate the proposed method. \n\n(3) The original Tacotron 2 with WaveNet can achieve 4.526 Â± 0.066, which is better than the WaveGrad. The authors should also include the results of the WaveNet Vocoder. \n\n(4) The authors should also provide results from the compared methods in https://wavegrad-iclr2021.github.io/ to validate the superiority of the proposed method. \n\n*** Post-Rebuttal ***\n\nThank the authors for responding to my concerns in the rebuttal. \n\nFor the contribution part,  the major technical contribution is the continuous noise schedule. However, it is very obscure in the original paper. As also suggested by R1, the authors should carefully revise the paper to make it more clear. In addition, I found the paper largely borrowed content from Ho et al. (2020). From the framework figure and Algorithms to texts and equations, only limited modifications are made. After reading the paper, my first impression is that the paper just uses a new model on image synthesis to address speech synthesis. The work is not well motivated and largely copying another paper in the method section is not a professional way.  \n\nHope the authors can modify the paper by adding the new clarifications and explanations in the rebuttal to improve the paper.\n\n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}