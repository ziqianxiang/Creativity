{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors present a Bayesian approach for context aggregation in neural processes based models. The article is well written, and provides a nice and comprehensive framework. The reviewers raised some issues regarding the lack of comparisons to proper baselines. The authors provided additional comparisons in the revised version. The comparisons were found satisfactory by some some reviewers, who increased their scores. Based on the revised version, I recommend acceptance."
    },
    "Reviews": [
        {
            "title": "A new Bayesian regression model as a multi-task learning problem",
            "review": "The paper builds upon previous lines of research on multi-task learning problem, such as conditional latent variable models including the Neural Process. As shown by the extensive Related Work section, this seems to be an active research direction. This makes it difficult for me to judge originality and significance, but it is well-written and clear.\n\nSpecific comments\n- the approximate posterior distribution q_\\phi is often referred to as \"the posterior distribution\". I would keep \"approximate\" here. \n- p2: \"correspondence of GPs with infinite Bayesian NNs (BNNs)\", what is meant by \"infinite BNN\"? is it infinite-width BNN? please specify.\n- p2: \"adaptive BLR\" please describe the acronym.\n- p6: the Gaussian approximation of the posterior predictive likelihood (10) is said to be \"inspired by GPs which also define a Gaussian likelihood\". This is also essentially what is done by Synthetic Likelihood (a Nature paper by SN Wood, 2010) which is I think more closely related to the proposed approach than GPs.\n- p6, one line below: define PB acronym the first time it is used (not three lines after).\n\nTypos\n- top p2: \"does not introducing noticeable computational overhead\".\n- p6: \"conditional model with an decoder operating on\".\n- The references list could be tidied: some first names are abbreviated, some not.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "A solid improvement for neural process inference, but I'd like to see actual regression tasks",
            "review": "In this paper, the authors make two contributions to neural process-like CLV models. First, they replace the somewhat adhoc variational-like approach to learning the amortized latent variable distribution with a monte carlo based approximation. Second, they replace the step of context aggregation with direct latent variable inference over z. \n\nOverall, in my opinion these modifications make the neural process model significantly cleaner from a Bayesian perspective and is quite nice. In the context of neural processes, I have very little criticism for the authors' methods. Everything makes sense, and the MC approach in equation (3) seems cleaner to me than the somewhat ad hoc \"VI like\" approach in equation (2).\n\nThe biggest difficulty I have is determining how to evaluate the authors' clear improvements to neural processes in the broader context of scalable probabilistic regression, an area to which the authors claim membership. To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). In this respect, it's difficult to fault the experimental evaluation.\n\nHowever, this paper and many neural process papers are written in the context of \"Formulating scalable probabilistic regression models with reliable uncertainty estimates.\" Surely, at some point, this should involve a comparison of these approaches to existing techniques for probabilistic regression, whether that be deep Gaussian processes, dropout based approaches, Bayesian neural networks, or other approaches. I don't mean to imply here that the authors are unaware of this large body of literature -- indeed, the authors have a decent if incomplete overview of techniques in this area (notably missing work on deep GPs).\n\nRather, it just seems surprising to me that the discussion of the relevant probabilistic regression literature ends at \"well, it exists.\" What I would like to see is a discussion of where the authors' impressive improvements to neural processes leave the model family in this broader context. How close or far off is the family on performance for standard benchmark regression tasks? Are there settings in which we can leverage the fully NN based nature of neural processes to achieve probabilistic regression in settings where the inductive biases of kernel methods are poor, like in computer vision or natural language processing? The relatively toy nature and limited dimensionality of the problems considered suggests that there is still significant progress to be made before such a comparison would be reasonable or even possible.\n\nTo summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. Within the very narrow context of neural process papers, I therefore have very little to complain about. However, from a broader scientific perspective I would feel that the paper would be significantly strengthened by a fair evaluation to the rest of this literature, whether empirical or simply in discussion, regardless of how the authors' approach fares in comparison.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting Paper Proposing a Sound solution",
            "review": "Summary of the Paper:\n  \n        This paper describes Bayesian context aggregation for neural processes. These models are useful to address regression problems in which a set of related tasks are available for inference with associated context information in the form of extra data. These models assume that there is a task-specific global latent variable and task-independent latent variable. They are learned via approximate maximum posterior likelihood, in which the latent variables specific for each tasks are marginalzied out. For this, an approximation to the posterior distribution of these variables is need. This requires conditioning to the context dataset which is challenging. In the past, a latent representation is used and the context data set is aggregated as the mean of the latent representation. In this paper a Bayesian way of aggregating context information is proposed. This is based on using Bayes rule and a Gaussian generative model for the latent representations. The proposed method also leads to a new way of training CLV models which is based on moment matching. The method is validated on several synthetic an real-world experiments showing improvements over mean aggregation.\n\nDetailed Comments:\n\nI believe that this is a relevant paper. Context aggregation is a difficult problem that is required to address the learning tasks described in the paper. Previous solution look limited and the proposed method seems natural and a more effective method of aggregating this information. The paper is well written and the proposed method is sound. The experiments are also convincing and exhaustive. I believe that this is a relevant paper for the conference.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple and effective idea, but missing an important baseline that can also address the problems of mean-aggregation in NPs",
            "review": "The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality.\n\nStrengths:\n1. The idea is simple and leads to a notable improvement compared to MA in terms of likelihood\n2. The background and method is presented very clearly.\n3. The evaluation is done on a wide variety of tasks, ranging from standard 1D regression of GP samples to pendulum trajectory prediction tasks.\n\nWeaknesses:\n1. The evaluation is missing an important baseline model, which are (A)NP models that have self-attention in the encoder for processing the contexts (c.f. model figure in ANP paper (Kim et al., 2019b)). Contrary to the NP/CNP baselines that are compared against in the paper, the ANP with self-attention in the encoder does not give uniform weights to each context point - the self-attention allows the model to assign varying importance to the different context points (despite using mean-aggregation after the self-attention), which is presented as a key motivation for the BA mechanism introduced in the paper. Hence for the experiments, I strongly suggest comparing against CNP/NP/ANP with self-attention in the deterministic/latent/latent path of the encoder. For completeness, if would be nice to also compare against models that have both deterministic and latent paths, since BA can also be applied to these models. At the same time, I understand that BA would be more interpretable for showing which observations have little/high effect on z compared to the approach of using self-attention in the encoder, but it would still be very informative for the reader to be able to compare the two approaches. Also these two approaches can be combined to have self-attention in the encoder + BA, which might also yield improved performance.\n2. The claim that “BA includes MA as a special case” doesn’t seem to be true. Using a “non-informative prior and uniform observation variances” leads to constant sigma_z and mu_z being linearly proportional to mean(r_n) (i.e. sum_n r_n / N), which is not quite the same as MA - MA allows sigma_z and mu_z to be non-linear functions of mean(r_n), hence is strictly more expressive than this special case.\n3. In Equation (7), it seems as though the context points (x_n,y_n) only affects r_n via the variance, which seems unnecessarily limiting. Why not have the mean also depend on r_n? e.g. p(r_n|z) = N(r_n| z + mu_{r_n}, diag(sigma_{r_n}^2) where mu_{r_n} is also computed as a function of (x_n,y_n)? This will still give a closed-form posterior p(z|r_{1:N}) since the mean of p(r_n|z) is still linear in z, creating a model that’s strictly more expressive with very similar efficiency. It would be informative to see how this changes the experimental results.\n4. I’m guessing the VI objective was used to train the ANP. Given the clear advantage of training with the MC objective, shouldn’t the ANP also be trained with MC?\n5. The latent variable models were not evaluated on 2D image completion tasks because “architectures without deterministic paths were not able to solve this task”. Why not then add a deterministic path to these latent variable models to allow them to train?\n\nOther points\n- In the text, it says that the model is also compared against ANP to show that BA can compete with SOTA. This is arguably incorrect since ConvCNP models are SOTA among models of the NP family, showing a significant improvement over ANP. Hence to achieve the goal mentioned in the text, it would make sense to compare with ConvCNP models as part of the evaluation against other deterministic NPs.\n\nOverall the paper is presented very clearly with a simple yet effective idea tested on a wide variety of tasks. However it’s missing an important baseline that uses self-attention in the encoder, along with several other baselines that would be informative to compare against. I am willing to increase my score should these results be included in the revised version of the paper.\n\n=================\n\nScore raised to 6 after inclusion of MA + SA results in rebuttal.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}