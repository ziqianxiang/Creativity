{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a new algorithm, BOIL, on the importance of representation change vs reuse in MAML. All reviewers found the paper insightful, with some proposing a few changes to make the paper even stronger. Like them, I recommend accepting the paper."
    },
    "Reviews": [
        {
            "title": "A well written paper which might benefit from more discussion.",
            "review": "Summary:\nPrevious work studied MAML and showed that representation reuse is the main contributing factor in performance and not representation change. This paper first asks the question of whether representation reuse is enough for meta-learning? The paper hypothesizes and empirically evaluates the need/benefit for representation change in meta-learning tasks especially for cross-domain transfer. For this, they propose BOIL, a variant of MAML where the head of the network is not updated during inner loop updates.\n\nStrengths:\n1. The paper is very well written.\n2. They do a simple, clearly described empirical study to understand what changes in representation happen in MAML during its training/testing and how to modify the MAML to encourage representation change.\n3. The proposed change to MAML, BOIL shows improved performance in several of the benchmark datasets especially in cross-domain tasks (compared to MAML)\n4. Their overall study is beneficial for the gradient based meta-learning models similar to MAML to better understand what is happening and directions to modification to training that could lead to potential improvement in generalization performance. \n\nWeakness/Comments:\n1. While the paper focuses on MAML, it is not clear what and how their findings and design modifications apply for other meta-learning methods such as CNP and also to gradient based meta-learning in Reinforcement Learning. Some discussion on the scope of the finidings/methods would be very helpful.\n2. I am concerned if what is happening in BOIL is that the problem of not have representation change is just being passed on to the body. i.e the top layer of body is like the head which is reusing and the everything below is changing. i.e I am wondering for deep networks the problem is just passed on to the lower part and not really handled. At some point we probably want the layers to change less and be reused. Authors comments on this would be appreciated.\n3. Even in a perfect representation change setting, there will be limitation of generalization to test time tasks depending on the distribution of the training tasks. One would hope that this covers a much larger set of domain transfers than with the representation reuse case. Some brief discussion on this would be good. i.e BOIL may not be able to transfer to any cross domain, if not, what class of domains can it successfully transfer to?\n4. My guess is that the speed of adaptation might decrease in BOIL compared to MAML. This might not be a problem in SL, but might be of interest in RL. What do you think?\n5. Do we lose anything by not allowing the head to change during inner loop training? Would it be possible to alternate between lr of inner loop being zero and non-zero for the head?\n6. One can imagine if we had a lot of training tasks with a wide distribution and a model that cannot memorize all that the training of , and good optimizer, MAML would be forced to perform representation change as the training loss cannot be successfully reduced using just training reuse as the the model's capacity is limited. Would this be an alternative at the cost of more data? What are your thoughts?\n\nOther Comments:\n1. I think if we can achieve representation change, then it can also help make MAML less prone to memorization overfitting (\"Meta-Learning without memorization, ICLR 2020, Meta-Learning requires meta-augmentation, NeurIPS 2020\"). That might also a be something to explore. \n\nQuestions to authors:\nPlease reply to the above comments/weakness mentioned. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting exploration, but I have a concern about whether BOIL is really working through representation change.",
            "review": "This paper performs an investigation (with an associated new algorithm, BOIL), into the relative importance of representation change vs representation reuse in few-shot learning with MAML. It also compares to related prior work, 'Rapid Learning or Feature Reuse, (Raghu et al 2020)', that explored this question, finding representation reuse to be the key component of MAML, and their associated algorithm, ANIL, which only updates the network head (not the body) in the inner loop of training.\n\nThe paper studies the important role played by representation change when performing few-shot learning across domains, and to explore this, proposes an algorithm BOIL (Body Only in the Inner Loop), which, in contrast, only updates the network *body* in the inner loop, and keeps the network head frozen. The results show that BOIL matches performance of MAML/ANIL in single domain few-shot learning (mini ImageNet, tiered MiniImagenet), and provides performance improvements in fine-grained and cross-domain few shot learning (Cars, CUB, cross-domain between these datasets). \n\nWhile the results are interesting, I have a key concern on how BOIL is working and some of the analysis results:\n\nBy freezing the head of the network during inner loop training, the head effectively acts as a fixed random projection layer, and  it is very possible that the penultimate layer now acts like a pseudo-head. This is what seems to be indicated by Figure 3, where the three first layers comprising of the body have extremely high representation reuse before/after adaptation, and the fourth layer, becomes much less similar, possibly acting as a head. It is important to determine whether this is what is happening, because it affects how much BOIL is actually reliant on representation change.\n\nBefore recommending accept, I would like to see more CKA comparisons (and any other representation analysis) of the BOIL representations across the different architectures/datasets, to confirm that BOIL is indeed working through representation change. \n\nMinor Comments:\n--- It would be helpful to know how the few shot learning tasks are setup on Cars and CUB\n--- The key detail of which representations are being compared in Figure 2 (cosine similarity plot) is very unclear. Also don't understand why representations before/after adaption are being plotted separately, instead of cosine similarity applied to before/after pair.\n\n--------------------------------------\n--------------------------------------\n--------------------------------------\n**After Rebuttal and Discussion**\nI appreciate the changes and additional experiments added by the authors, and am recommending accept. \n\nFor the final version, I would strongly encourage the authors to make the representation change/representation reuse argument clearer. Specifically, the authors might want to first present the BOIL algorithm, highlight key aspects of the algorithm (frozen head), present performance results, and then the ablation results in Appendix N to highlight the importance of freezing the head. After this, the paper could switch to a discussion on representation reuse/change and present the analysis results, making clear what is happening at a layer level vs at the algorithm level.\n\nRight now, I think it's still a little confusing that representation change often refers to the algorithm not the layers, and this actually reduces the impact of what is a very striking result!\n\nI hope the authors can make these changes, and with the clearer messaging, this should be a very interesting paper for the community, and provide many interesting directions for future work!\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official review",
            "review": "#### Summary:\nThis paper proposes a variant of MAML, called body only in the inner loop (BOIL), in which the output layer is not trained (learning rate is set to 0, always) but gradients are still backpropagated to the remaining layers, which are trained as in MAML. In essence, this is the inverse of the ANIL method of Raghu et al., which showed that MAML mainly adapts only the output layer for few-shot image classification. This paper shows that never training the output layer of the network (simply leaving it at its randomly-initialized values) can provide much better performance across a number of few-shot image classification tasks, including those requiring domain transfer.\n\nPros:\n- Consistently strong performance across a number of tasks\n- Simple method\n- Many experiments showing similar benefits\n- Side studies and ablations to try to better understand the reason for the improved performance provided by BOIL\n\nCons:\n- I am not convinced about the explanation of representation change versus representation reuse. Clearly, a lot of representation reuse is still occurring; i.e., all but one layer. The only difference is that instead of the final layer adapting and the remaining layers staying fixed, as in MAML, the final layer is fixed and the penultimate layer (now the final adapting layer) adapts. The preceding layers remain essentially fixed however (i.e., are reused). \n- The experiments are somewhat unfair. Instead of performing a hyperparameter search for each method to choose the best settings for each, a single setting is used for all methods. While there is a lot of overlap between the methods, it’s clear from Appendix E that MAML can perform better using those hyperparameters in some domains than is reported in the main text. While it’s likely that BOIL will still do well (as it still does better than the improved MAML scores), it would be more fair to report the higher score for MAML. In particular, it’s likely that the much lower learning rate (as used in Appendix E) prevents the severe overfitting commonly observed in MAML.\n \n#### Decision:\nOverall, I am on the fence about this paper. The results are strong and will be interesting to the community; however, the explanation does not seem valid to me and the evaluation could be more fair. I thus recommend a weak accept as readers will be interested in the results and can draw their own conclusions about the reasons for them.\n\n#### Questions:\n\n1. Why does adapting only the penultimate layer count as representation change but adapting only the output layer is representation reuse? This distinction seems arbitrary. Clearly, there is a lot of benefit to adapting the penultimate layer as shown by the experiments but this seems like an identical amount of representation reuse as MAML with a network that is missing a layer and instead has a random transform appended to the end. How is this different from that?\n\n2. What does Figure 2 look like if the cosine similarity of the output layer is plotted in each subplot along with that of the conv layers? Does BOIL then become a shifted version of MAML, as Figure 3 indicates it might?\n\n#### Comments:\n- A recent work [1] also examines fixing some layers, including the output layer, to obtain better performance than allowing all layers to adapt. It should be cited and discussed in the related work as well.\n- Please define $f$ in equation (1).\n- The final sentence of Section 3 is unintelligible to me. Please clarify it.\n- In Figure 2, the subfigures are far too small to read. Further, the subtitles say “adatation” instead of “adaptation”.\n- In general, the writing could use more editing.\n- The whitespace around the titles is overly compressed. Please move some text to the Appendix instead of cramming as much as possible into the main body.\n\n[1] Modular Meta-Learning with Shrinkage. Chen, Friesen, Behbani, Doucet, Budden, Hoffman, and de Freitas. \n\n\n***********\n\nAfter the discussion period, I have increased my score to 7 as the authors have provided a strong set of ablations and changes to address my concerns and those of the other reviewers.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}