{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper provides a study of the impact of preconditioning/second-order methods on generalization by giving a precise analysis in tractable regression settings.\nIt illustrates conditions under which preconditioning might be useful for better generalization. \nThe readability issues raised by the reviewers have been taken into account, as well as some missing references, except\n\nWu, D. and Xu, J. \"On the Optimal Weighted Regularization in Overparameterized Linear Regression\" NeurIPS 2020, raised by reviewer (though it is a really recent reference).\nOverall the contributions are significant enough to accept the paper for publication."
    },
    "Reviews": [
        {
            "title": "interesting study of the generalization properties of preconditioned gradient methods",
            "review": "The paper studies generalization properties of preconditioned gradient descent on linear/kernel regression problems. The main preconditioner that is studied in addition to vanilla GD is the (population) Fisher matrix (natural gradient descent or NGD), its empirical counterpart, and its interpolation with GD.\nThe authors first consider the \"ridgeless\" regression setup in high-dimension, where the estimator corresponds to the limiting gradient flow iterate, and show that NGD leads to a smaller (and optimal) variance term, and can improve the bias term compared to GD particularly in the presence of strong misspecification. Among others, the authors also consider early-stopping in a non-parametric RKHS setup, showing that an appropriate interpolation between NGD and GD achieves optimal rates with a much smaller number of steps compared to GD, a difference which becomes larger for \"difficult\" problems (which require more weight on the Fisher preconditioner). The findings are further illustrated with simple experiments on neural networks.\n\nOverall, the paper provides a comprehensive study of the impact of preconditioning/second-order methods/natural gradient on generalization by giving a precise analysis in tractable regression settings, which illustrate conditions under which preconditioning is or is not useful for better generalization. This makes the paper a strong contribution, and I am in favor of acceptance.\n\ncomments/typos:\n- section 3: 'population risk' -> should this be excess risk given the presence of noise? add a reference or some more details on the bias-variance decomposition?\n- the last sentence in section 3.2 \"in the analogy...\" could be clarified\n- end of p.5 \"lower bias compare to\" -> \"compared to\"\n- Prop. 6: first part with theta_P holds for any P? please specify\n- theorem 7: specify conditions on eta?\n- some comments on computational difficulties of the full preconditioner would be welcome. Would a diagonal preconditioner, as often used in deep learning, provide any (partial) benefits as in the full-matrix case presented here?\n\n\n### Update after rebuttal\nThank you for the clarifications.\nA couple minor comments:\n- regarding theorem 7, my comment was that it would be useful to include the conditions on eta in the theorem statement in the main text (though I do not feel strongly about it)\n- regarding \"misalignment\" and the relationship between the random effects model and the source condition, I appreciate the improved explanation of this analogy, but I still find that the last paragraph in section 3.2 could do a better job at providing the right intuition (skimming through the Richards et al. reference pointed out by R4 gave me a better intuition).",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper studies the effects of preconditioning on generalization properties in deep learning.",
            "review": "Summary:\n\nThe paper studies the effects of preconditioning on generalization properties in deep learning. By using a bias-variance decomposition of the expected risk, the paper determines optimal precondition matrix $P$ for bias and variance. Then the paper analyzes the generalization performance via the aspects: clean labels, well-specified model and aligned signal. Finally, it extends the analysis to the reproducing kernel Hilbert.\n\nPros:\n\nThe theoretical results provide guidelines of choosing precondition matrix for practical problems. In particular, by decomposing the risk into a sum of a bias and a variance, the paper addresses the following points:\n\n1. The asymptotic result on the variance (Theorem 1) implies that NGD achieves the minimal variance at stationary points, that suggests using NGD in the case where the variance term dominates.\n\n2. Theorem 2, on the other hand, provides the asymptotic result on the bias and the optimal precondition matrix for the bias to reach minimal value at stationary.\n\n3. Based on the results on the variance and the bias, Proposition 4 suggests an interpolating scheme between NGD and GD that aim at achieving better stationary risk than NGD or GD. The efficiency of this scheme is demonstrated in a least squares regression with the regular RKHS, where the interpolating scheme achieves the optimal convergence rate in fewer step than GD.\n\nCons:\n\n1. The paper contains a number of unclear / undefined terms such as well-specified and aligned signal, that make it difficult to read.\n\n2. The paper uses a lot of vague and unverified claims / statements which are usually the explanations after each theorem / proposition. For example, after theorem 1, it says that \"Theorem 1 implies that preconditioning with the inverse population Fisher results in the optimal stationary variance... In other words, when the labels are noisy so that the risk is dominated by the variance term... We emphasize that this advantage is only present when the population Fisher is used, but not its sample-based counterpart\". For me, it would be more clear if these statements could be explained in detail.\n\n3. The paper is not well-organized. For me, it is a collection of results that are unconnected. For example, after reading the analyses of bias and variance, I have no idea how they support the study of generalization or why section \"3.3 misspecification\" is placed along with bias and variance analyses, etc. I am not saying these results are irrelevant, however, there should be a better way of arranging / writing them so that they can support well the ideas of the paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insights for Natural Gradient Descent In Some Specific Cases",
            "review": "Summary: \n\nThe authors theoretically study the prediction performance of pre-conditioned gradient descent/flow with linear models and squared loss aligning in the setting of least squares regression and non-parametric regression. For parametric least squares, the predication performance of the limiting solution for preconditioned gradient flow i.e. time goes to infinity, is studied in an asymptotic regime where both the number of samples and dimension go to infinity in proportion to one another. Meanwhile for non-parametric regression, source and capacity assumptions are leveraged to achieve finite sample guarantees. Experiments are also conducted on neural networks in a student and teacher setup. \n\nSummary of main Contributions: \n\nA1) In the case of parametric least squares, an asymptotic characterisation of the test risk is utilised to study the limiting solution of preconditioned gradient flow. Preconditioning with the inverse Fisher information matrix (covariates population covariance) is shown to achieve the optimal variance among preconditioned updates (Theorem 1).  Meanwhile for the asymptotic bias, the optimal pre-conditioner depends upon the covariance of the ground truth parameter. In a mis-aligned case, where the ground truth covariance is equal to the inverse of the population covariates covariance, the optimal pre-conditioner for the bias aligns with the inverse Fisher information matrix (Theorem 2).   \n\nA2) In the case of an Isotropic covariance for the ground truth parameter, it is found that the Bias and Variance can be traded-off by interpolating between the two aforementioned pre-conditioners (Proposition 4).\n\nA3) For non-parametric regression, gradient descent pre-conditioned with the inverse regularised population covariates covariance is considered. Mini-max optimal statistical rates are achieved with a number of iterations that grows logarithmically in the data set size i.e. linear convergence (Theorem 7).\n\nA4) Experiments for neural networks are conducted in support of A1). Specifically, gradient descent pre-conditioned with the Fisher information matrix achieves better generalisation performance when the noise is large or the model is misaligned (Section 5). \n\nA5) For parametric least squares with a mis-aligned ground truth parameter, it is shown that early stopping with NGD achieves lower Bias than any other pre-conditioned gradient descent (Proposition 6).  \n\nPros: \nB1) I feel contribution A1) in conjunction with A5) is novel and offers a precise interpretation of when pre-conditioning with the inverse Fisher information matrix can yield an improvement in performance. \n\nB2) Contribution A2) is also interesting and can point towards understanding and controlling the implicit bias of gradient descent through the pre-conditioner i.e. taking a linear combination of two pre-conditioners. \n\nB3) Contribution A4) supports the findings in A1) in a setting beyond least squares.\n\n\n\nConcerns:\n\nC1) The authors do not compare their theoretical results for non-parametric regression (contribution A3) ) to prior work within the literature. Specifically, reference [1] where the generalisation performance of a pre-conditioned gradient method is considered. To remedy this, I feel the authors should discuss how their theoretical results and proof method differ from [1] as well as the novelty of their approach. \n\nC2) The theoretical results and discussion focus on a particular type of pre-conditioner: the inverse population covariates covariance and transforms thereof. This limits the applicability of the insights as this quantity is often not known in practice. Similarly, the experiments are in a setting where Fisher information is estimated accurately using 100,000 samples while training uses 1024 samples. In contrast, prior work for non-parametric regression considers pre-conditioners involving estimates of the population covariance [1]. To remedy this, I feel the authors should include a discussion on how their insights i.e. A1), A2) are impacted when the population covariance is swapped for an estimate (using unlabelled data).    \n\nC3) The manuscript can be difficult to read. For instance, the authors start with a time varying pre-conditioner while all pre-conditioners considered are constant in time. Tools from random matrix theory and regularity assumptions for non-parametric regression are introduced with little discussion. Section 3.3 \"Misspecficiation \\approx Label Noise\" considers misspecification that is independent and gets interpreted as additional noise. It is not clear what this brings to the manuscript in terms of insights and introduces another layer of complexity.   \n\n C4) For parametric least squares regression the results focus on three cases for the ground truth covariance: well-aligned (where it equals the covariates population covariance), mis-aligned (where it equals the inverse covariance population covariance) and Isotropic. Whereas the theoretical results allow for a more general ground truth covariance to be considered. It would be natural to follow the source conditions from non-parametric regression and investigate natural gradient descent when the ground truth covariance is not fully well- or mis-aligned.   \n\n\nGeneral Comments: \n\n-Remark on page 4 states \"we demonstrate generalisation properties only possed by the population Fisher\", clarify which properties are /only/ held the population Fisher versus Sample Fisher. \n\n-In Proposition 4 possibly change the description \"interpolating preconditioners\" as all the preconditioned methods are interpolating the data, and thus, can be confusing.\n\n-Proposition 4 states for pre-conditioners (ii) and (iii) the bias is monotone for $\\alpha$ in some range depending upon the covariates population covariance. What is the range of $\\alpha$ and is the risk increasing or decreasing? What conclusions are we to draw from this part of the result? \n\n-In Figure 6 and Figure 23 how is \"geometric\" and \"additive\" interpolation defined ?    \n \n-More discussion around Proposition 6 would be helpful. For instance, in the statement of the result what is choice of P ? The analysis is described as difficult, although no details are provided into how this result was obtained. Within the proof why is the ratio of Eigenvalues \\overline{\\lambda}_{min}/\\widehat{\\lambda}_{min} is bounded, and how many iterations are required until NGD is below, say, standard gradient descent?  \n\n-In Section 5, the misalignment experiment in Figure 7 is conducted for MNIST but not CIFAR-10, with no discussion in the main body of the manuscript for why this is. Although, paragraph \"Misalignment\" in Appendix C.3 states the phenomena of NGD outperforming GD in the misaligned case is \"... difficult to observe in practical neural network training on real-world data\". The authors then go on to state that, in short, this is due to (see Appendix A) NGD moving parameters further from initialisation, and thus, no longer well described by a linear model i.e. NTK.  Is there a link between this discussion within the Appendix and the experiments?   \n\n-A Summary at the start of Appendix A to describe contents of A1-A4 would improve readability.  Similarly, for Appendix C and D.\n\n-In the proof of Theorem 2 (Appendix D.2) some details on how to get from (ii) to (iii). \n\n\n[1] - Rudi, A., Carratino, L., and Rosasco, L. \"Falkon: An optimal large scale kernel method\",  Advances in Neural Information Processing System 2017.\n\n\nPOST REBUTTAL EDIT: \n\nI thank the authors for providing detailed answers regarding my concerns. I have updated my score in light these comments. Below are some additional comments in response. \n\nResponse to comments regarding C1) and C2): \nWhile early stopping with pre-conditioned updates differentiates this work from (A. Rudi et. al 2019), the analysis still requires the knowledge of the population covariance. Indeed, while the authors have included a section (Appendix A.3) showing that the operator norm of the population and the inverse regularised empirical covariance can be controlled, it would be insightful to discuss to what extent this allows the analysis for the pre-conditioned gradient descent to be extended to an approximated population covariance.  \n\nResponse to comment regarding C3): \nI am inclined to agree with reviewer 3, in that the manuscript is difficult to read due to the larger number of fragmented results. In this regard, I feel the authors should focus on a single phenomenon that is supported by both the parametric and non-parametric aspects of the paper, for instance, how pre-conditioning helps against misalignment.      \n\nResponse to comment regarding different prior on ground truth (point 4. third bullet point): \nNote that some concurrent works have studied the case of different priors on the ground truth [2,3], which are likely relevant in this case. \n\nMinor Comment: The pre-conditioned updates for non-parametric regression (4.1) use notation $\\alpha$ where as Appendix D.8.1 uses notation $\\lambda$, with the discussion then switching back to using $\\alpha$ and $\\lambda$ being used in reference to the regularisation used within FALKON. The switching of notation is possibly confusing here. \n\n[2] - D. Richards, J. Mourtada, L. Rosasco \"Asymptotics of Ridge (less) Regression under General Source Condition\", arXiv:2006.06386 (2020) \n\n[3] - Wu, D. and Xu, J. \"On the Optimal Weighted $\\ell_2 $ Regularization in Overparameterized Linear Regression\" NeurIPS 2020\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}