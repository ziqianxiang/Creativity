{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper received overall positive scores. All the reviewers agree that the approach presented in the paper is simple yet effective and the results are very impressive with >95% parameter reduction while maintaining the accuracy. The authors promptly revised the paper based on initial reviews. Therefore, I recommend accept and hope the authors incorporate the additional comments from Reviewer3 after the discussion. "
    },
    "Reviews": [
        {
            "title": "The paper discussed how to make embedding vector dimension adaptive to model performance and memory cost in recommendation scenarios.",
            "review": "This paper proposed a novel approach to reduce size of the embedding table while not to drop in accuracy and computational optimization. Fixed-size embedding table has two problems, high memory usage cost and overfitting problem for those features that do not require too large representation. This paper recast the problem of embedding-size selection into learning column-wise sparsity, constraint K (eq(7)) and then convert S(V,s) problem (eq(8)). Paper used three benchmark datasets and some classical methods to verify effect.\n\nPros.\n1.proposed a novel idea to learn embedding size  instead of fixed-size.\n2.Experiments baseline with classical datasets and models show promising results on recommendation ranking problem.\n3.The method has generalization ability across different recommendation models.\n\nCons\n1.In METHODOLOGY part,  paper didn’t explain how to convert constraint by K in eq(7) to S(V,s) in eq(8)  and prove them.\n2.Symbols in paper are confused such as V and v , and  N has different meanings in METHODOLOGY part.\n3.The method first trains a sparse embedding matrix and then do re-training. During the sparse embedding learning phase, the memory size can not be reduced to a sparse format leading to significant memory cost, this is not feasible for very large scale sparse features for CTR task.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting Application of Learnable Sparsity in Recommender Systems",
            "review": "In this paper, the authors apply the technique of 'learning' the sparsity structure [1] in the context of Neural Recommender Systems. \n \nIt's an interesting and very practical application of the technique in the context of RecSys. In designing large-scale RecSys models, where the number of distinct items and users can go in the range of millions, storing and retrieving can become a challenge. A parameter compression technique can indeed prove to be very effective in such situations. \n\nSome strong points of the paper:\n\n1) Provides a practical application of the technique described in the very recent ICML paper [1]. \n2) Paper is well-motivated and generally well-written. \n3) Results indicate a huge improvement in terms of the number of parameters, without sacrificing the performance in the downstream tasks. \n\nSome questions for the authors:\n1) Can you please recheck equation 8? In the original paper, the equation is defined for each element of the matrix, where the mod. operator makes sense, you this case, what is the significance of the mod. the operator on the entire matrix? \n\nOverall, given a very practical solution to the exploding users and items situation in a RecSys setting, I recommend an accept for the paper. \n\n\n\n\n\n\nRef:\n[1] Soft Threshold Weight Reparameterization for Learnable Sparsity, Kusupati et al, ICML'20.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple, yet effective technique PEP that proposes an LTH style iterative embedding pruning algorithm with learnable threshold for sparsity generation.",
            "review": "The paper proposes PEP (Plug-in Embedding Pruning) to reduce the size of embedding table while incurring insignificant drop in accuracy. The related work is well summarized into Embedding Parameter Sharing and Embedding Size Selection methods and the motivation for the current approach is well explained. The paper draws inspiration from Lottery Ticket Hypothesis. The problem formulation of Embedding pruning is done in a crisp way avoiding additional hyper parameter tuning that can be found in other methods. Similar to LTH, the paper shows that the initiation strategy can make the training process faster and stable. The results show an impressive 97-99% parameter pruning via PEP. As for the computation cost, PEP results show an additional 20-30% time cost compare with base models.\n\nOne of the main pros of this techniques is the learnable threshold(s) for pruning that can automatically jointly train with other parameters via gradient descent. As L0 based sparsity constraint on loss function is non-differentiable, the paper uses a neat re-parameterization technique to directly optimize the projection of sparsity matrix. The paper utilized the automatic differentiation framework to avoid complex back propagation and gradient calculation process. The main code of PEP is easy to read and understand. It’s only a few lines of code and well commented. \n\nThe results are well summarized and analyzed. Several baseline models (FM, DeepFM and AutoInt) were chosen that represented feature based models which was a fair strategy to follow. The one additional result that should have been added was running time comparison with MGQE, MDE, and DartsEmb pruning algorithms. The run time comparison of PEP was only shown with DartsEmb where the paper claims only 20-30% additional computation cost.\n\nPost Pruning analysis on MovieLens Dataset was interesting and show that PEP assigns high- frequent features larger sizes and fewer parameters are enough for low-frequent feature embeddings. A missing portion which when added can make the paper wholesome is analysis of PEP across models for the same dataset. The results show that PEP performed best compared to other methods for FM model whereas for AutoInt model, AUC of PEP method was lower than MGDE for instance. Any hypothesis, intuition of why that would happen will add value to the paper. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great results",
            "review": "The paper investigates the embedding compression problem for recommendation via pruning. The paper is written very clearly with very strong experiments (multiple datasets/recsys models/recent baselines), showing ~99% parameter reduction while maintaining the same or even better performance.\n\nI have a few questions:\n\n- It's a bit surprising to me this method works. Like NIS or AutoEmb, they choose to use AutoML or validation data to select a suitable dim, as merely relying on a training set would lead to zero compression (using all parameters will have the lowest training loss). The key eq 8. actually doesn't encourage the model for sparsity, right? It's not clear to me why the model will increase `s`  during training, as using all parameters should decrease the training loss faster. Any explanation?\n\n- Ablation study: it'd be good to see the effect of the sparsity constraint. That is to say, we may train a model without any modification, prune small values, and then re-train the model, basically following the Lottery Ticket paper. Via showing the performance of this baseline, we'd know the effect of the eq 8. It's not clear to me the better performance is from eq8 or the lottery ticket.\n\n- What's the performance without re-training the model? Is it necessary to retrain the model for a good performance? What's the possible reason behind this?\n\n- It'd be better to mention that one potential drawback of this method against the baselines is the sparse embedding table may not be very easily used in practice. If we store the full dense embedding matrix, we won't save the space. If we use sparse representation, it'd be slower for matrix operations. In contrast, baselines like NIS, are free of such an issue, as they have the same, smaller dimension for all feature values.\n\nIn summary, the results seem to be very promising, which is the most important reason for me to give a high rating. However, it's not clear to me why such a simple method works (doesn't rely on validation set, automatically prunes itself with training set only) very well, and a few ablation study experiments might be added. It's likely that I'd change my score based on the author's reply, e.g. I'd be happy to raise the score if the authors provide a clear explanation/intuition for the idea.\n\n[updated after discussion]\nThank the authors for their efforts to add ablation study and make the manuscript more clear in presentation, it greatly resolves my questions and thus I raised the score. Overall the paper is in a good shape now. However I do want to point out a couple of things: 1) the performance of UE in base model FM/DeepFM/AutoInt seems to be a bit weired, as in previous CTR paper, deep models should outperform FM significantly. It's not clear to me is this due to different experimental settings or training schemes. 2) if the target is better performance (instead of compression), there is no clever way to choose s, other than mannual picking for each dataset/model.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}