{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Quality: the paper takes an important question and analyzes it well from a theoretical angle; it also provides empirical evidence to back up its main message in more complex models. The proofs are non-trivial. The paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\nClarity: The motivation of studying the double-descent phenomenon with optimal regularization is well-explained in the introduction. Connections and comparisons with existing related works are discussed clearly. The paper is clearly written, and exposes the results in a clear and accessible fashion. \n\nOriginality: The presented theoretical results on the linear regression model are non-asymptotic, which is new and different from existing works. \n\nSignificance: The proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings, which affects its significance.\n\nMain Pros:\n- the paper takes an important question and analyzes it well from a theoretical angle. The proofs are non-trivial; the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\nMain Cons:\n- Generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground-truth parameters \\beta^*. \n- The experiments need to be more extensive and better-explained, especially for the CIFAR-100 experiments. It is important to discuss this difference clearly at the beginning. "
    },
    "Reviews": [
        {
            "title": "Nice paper addressing the role of regularization in mitigating double descent",
            "review": "he paper studies the surprising phenomenon of “double descent” in machine learning models which has recently come into light through many prior works. The phenomenon is used to describe the behavior of test performance of an estimator as the model parameters (complexity) or the number of samples are increased. It is observed that in many cases, the test error first decreases then increases attaining a peak and then starts to decrease again as the number of model parameters (or samples) are increased. This pervades many different models including neural networks, decision trees and linear regression.\nPrior works have noted that this occurs primarily for unregularized or under-regularized models and that leads to the motivating question for this work: can optimal regularization remove double-descent? The paper studies this question from a theoretical and empirical perspective. \nTheoretical: For the setting of linear regression the authors show that an optimal amount of l2 regularizer added to the objective completely removes the double descent phenomenon with respect to both number of samples and number of model parameters. \nEmpirical: For random feature classifiers and convolutional neural networks, the authors empirically show that an optimal amount of l2 regularization removes the appearance of double descent curves.\n\nIn terms of the theoretical analysis provided in this paper, a highlight is that the results are non-asymptotic and the Theorems identify precise values of the different parameters when the double descent phenomenon disappears. \nThe theoretical results have the caveat that they apply only when the input data is Gaussian. If the data is not Gaussian, l2 regularization might not be able to remove double descent as the authors demonstrate via the means of a counterexample. Moreover, if the covariates are not isotropic, then the authors conjecture but cant prove that optimal l2 regularization suffices.\n\nIn my view, the paper takes an important question and analyzes it well from a theoretical angle and also provides empirical evidence to back up its main message in more complex models. The proofs are non-trivial and I think the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non-asymptotic regime.\n\n\nQuestions:\n1. If I recall prior works correctly, many of them point out that double descent continues to hold even under different regularization schemes both theoretically and empirically. What changed here or what were the prior works missing? I am asking this because based on prior work your result sends a conflicting message that we can get rid of double descent and also not sacrifice performance at the same time.\n2. Curious to hear what happens if we look at deeper neural networks? Could the optimal regularization amount be so high that it starts to hurt performance?\n3. Equation (9) in the Appendix would benefit from more explanation. In particular, the calculation showing how we arrive at the singular values of the matrix (X^TX + lambdaI)^-1X^T.\n\nMinor typos:\n1. Abstract: “quantities such as the …”\n2. Extra bracket in definition of lambda_n^opt on page 4\n\n\n-------------\n\nThank you to the authors for their response. It has helped clear some questions I had in mind. I am keeping my rating.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Fundamental research question, insightful theoretical and experimental results",
            "review": "This paper studies the double descent phenomena -- for increasing sample/model size -- in linear ridge regression. When the model is well-specified and the features are drawn from an isotropic Gaussian distribution, for the optimal (or larger than optimal) ridge regularization parameter, the authors show that there will be no double descent with respect to the sample size, i.e. the test error monotonically decreases as a function of the sample size. Furthermore, under a (randomly) projected feature space model, they showed that the test performance is also monotone with respect to projection size. The authors verify some of the the theoretical findings in practice, and show that the same insights carry over for more complicated models.\n\n++:\nThe paper studies a fundamental question in theoretical machine learning.\nThe claims are supported both by clean theoretical results as well as empirical evaluations.\n\n--:\nThe proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings.\n\nQuestion / Minor Comment:\nThe paper puts forward the idea that double descent is an artifact of underregularization. To be more precise, the main takeaway from the paper is that optimal \\ell_2-regularization can mitigate double descent, provably in certain linear ridge regression problems; and in practice, in certain deep learning problems. A natural question is if authors have observed similar phenomena under different regularization techniques, including other norm-based penalties, or the more exotic ones from the deep learning literature.\nI suggest that in the experiments section, you clearly state how you obtain the optimal regularization parameter in each of the subsections.\nWhy does the experiments section only cover the “sample monotonicity” part of the theoretical results (Theorems 1 and 2)? I think it helps if you verify Theorem 3 as well.\nCan you comment on the requirement d <= p in Theorem 3? It’s true that d > p will not give a subspace of the p-dimensional ambient space, but one can perhaps extend the setting to d > p requiring that P^T P = I_p. \n\nOverall, I enjoyed reading the paper and recommend it for acceptance in ICLR. The paper is well-written for the most part. I found the theoretical results insightful, and well-supported by experiments.\n\n\n\n#####################################\nI have considered the rebuttal as well as other comments in my final recommendation.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Marginally above acceptance threshold",
            "review": "This paper studies the double-descent phenomenon from a theoretical perspective. It proves that for certain linear regression tasks, optimally-tuned L-2 regularized model is able to achieve monotonic test performance as the sample size or the model size increases. Empirical results on the effect of L-2 regularization on the double-descent phenomenon are provided for more general models and tasks.\n\n\nPros:\n\n+ The motivation of studying the double-descent phenomenon with optimal regularization is well-explained in the introduction. Connections and comparisons with existing related works are discussed clearly.\n\n+ The presented theoretical results on the linear regression model are non-asymptotic, which is new and different from existing works. The theoretical results are discussed with enough details such that I can easily follow the Lemmas and Theorems.\n\n\nCons:\n\n- My main concern is the generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground-truth parameters \\beta^*. One question is whether the results apply to the agnostic settings (the relationship between x and y is unknown)? Moreover, going beyond regression tasks, can we draw the same conclusion for classification tasks? \n\n- The experiments need to be more extensive and better-explained, especially for the CIFAR-100 experiments. This is an image classification task, which is different from previously introduced regression setting. It is important to discuss this difference clearly at the beginning. In addition, the author uses a 5-layer CNN architecture, whereas the state-of-the-art CIFAR-100 results adopts a much larger architecture. Varying architecture size and even considering more image benchmarks would strengthen the experiments.\n\n\nMinor Comments:\n\n1. For Figure 3, it seems that for certain model size, the optimally-regularized test error is much lower than any other one. How do you compute the optimally regularized curve in Figure 3?\n\n===== Post-Discussion Update =====\n\nI thank the authors' efforts for responding my questions. Overall, I find the results presented in the paper interesting and worth publishing. It would be nice to extend the results to more general settings.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting paper with neat results but potentially limited impact",
            "review": "The authors consider the problem of double descent in regularized linear regression estimators, and show that, with optimal regularization, such problems can be totally mitigated when the covariates are isotropic gaussian. Additionally, the authors show that there exist cases where ridge regression is provably non-monotonic in its average risk. These theoretical results are complemented with some empirical results on ridge regression and small scale neural networks.\n\nThe phenomenon of double descent has gathered substantial interest in the past couple of years, and this paper presents an interesting contribution towards cementing our understanding in the context of double descent in linear models. Although the obtained results are perhaps not overly surprising given the strong connection between the ridge estimator and linear regression with isotropic gaussian variables and homoskedastic gaussian noise, the finite-sample proofs characterize the phenomenon in a concise manner. The paper is clearly written, and exposes the results in a clear and accessible fashion. However, the impact of this paper is limited by the fact that the general phenomenon of regularization avoiding double descent is well-known (if not necessarily in such an explicit setup, at least from considerations in the Bayes-risk or minimax-risk framework, and penalized regression in general), and that interest in double descent mostly stems from the existence of this behavior in unregularized settings.\n\nOther notes: there are some typos (especially in the appendix):\nP.16 regularzier -> regularizer\nP. 17 homeostatic -> homoskedastic\n\n=====================\n\nUpdate after author response: I thank the authors for their responses. I broadly agree with the points brought up by the other reviewers, and despite some weaknesses brought up by various reviewers, this paper is a good contribution to the community. I have brought my score from 6 to 7.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}