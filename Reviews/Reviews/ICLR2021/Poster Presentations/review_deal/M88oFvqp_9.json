{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper swaps characteristics of an object in one image onto those of another object in another image--for example, adding fur to a car.  The authors give some examples where the task could be useful.  Further, they successfully argue  that this task is an illustration that the disentanglement task has been done well.\n\nTwo reviewers argued for acceptance, two for just-below-the-bar rejection.  The 2nd of those in favor of rejection engaged thoughtfully with the authors and raised the score by 1 after that engagement.\n\nWe have decided to accept the submission as a poster."
    },
    "Reviews": [
        {
            "title": "Nice extension of shape/appearance disentanglement to images from different domains",
            "review": "### Summary\nThis paper proposes a generative model as an extension of FineGAN that aims to learn a disentangled representation for image shape and appearance across different domains rather than \"intra-domain\" disentanglement. To this end, the authors adopt the prior that features that correspond to an object's appearance should preserve frequency histograms. In order to incorporate this prior into the differential learning procedure, they learn a library of convolutional filters using a contrastive learning framework. They provide many convincing baselines and comparisons to related work and are able to attain reasonable results for style/content transfer between unrelated domains.\n\n### Explanation of rating\nI think this paper is a good steps towards truly being able to learn generic disentangled representations. Although the kind of data used for experiments is relatively simple, the results that are achieved go beyond existing state-of-the art generative models.\n\n### Pros\n- This provides some new insight into the kinds of disentanglements that previous generative models are (and are not) able to discover.\n- The frequency histogram assumption is a nice prior that is general enough to apply to different domains\n- The evaluation and comparisons are quite extensive and convincing.\n\n### Cons/questions\n- It might help to clarify and emphasize the novelty of the proposed method vs. the parts of FineGAN that it builds upon. For instance, the authors claim that that their method supports intra-domain disentanglement. While this is true, it seems like this is a feature of the base model and not really a contribution.\n- All of the results shown involve images with a single subject that takes up most of the canvas? How does this behave on less obvious images, e.g. with less prominent or multiple subjects?\n- How are $N_x$, $N_y$. $N_b$ chosen?\n- How is sim in eq. 1 defined?\n- What is temperature $\\tau$ in eq. 1? How is it chosen / is the method stable to choice of temperature?\n- Page 1: \"it's appearance\" -> \"its appearance\"\n- Page 2: \"acros\" -> \"across\"\n\n---\n\nThanks to the authors for the clarifications. I have read the other reviews and responses and still believe that the paper is a good contribution. Therefore, I am keeping my score.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good results, but paper could benefit from clearer explanation",
            "review": "The authors build upon prior work (FineGAN) in intra-domain disentanglement to extend to inter-domain transfer of separate attributes. Since no ground truth data exists for inter-domain transfer, they use contrastive losses to enforce similar statistics of low-level filter activations (averaged over the image) as a proxy for appearance similarity. \n\nAppearance transfer experiments on performed on a good selection of datasets and the results of the proposed method represent a qualitative step forward in unsupervised conditional generation across domains. Quantitative metrics support this point.\n\nThat said, the paper could be significantly improved by spending less space motivating and defining the problem, and more space describing the actual method used. The authors mention FineGAN in passing as their base model, but it is essential to the proposed method and could use further elaboration. As is, the relevant details of the losses and architecture choices are not contained within the paper itself. For instance, the loss terms in L_{base} are not defined and no diagrams are given to help understand the workings of the base model.  Similarly, the training procedure is a bit unclear from the text. If the content of Figure 3 were expanded, or a training algorithm table provided, even in the supplemental it would significantly improve the paper by not relying on a reference to provide the description of the core technique.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Slightly novel approach, but need to improve experiment results",
            "review": "--Summary:\nThe paper proposed a method to learn disentangled representation of shape and appearance for cross-domain (different object categories) data. Build upon FineGAN, the method uses contrastive learning combined with normalized temperature-scaled cross-entropy loss to further disentangle the shape and appearance information.\n\n--Strongness:\n1. The model is slightly novel. They combine contrastive learning with normalized temperature-scaled cross-entropy loss to learn the filter bank to construct the appearance feature histogram.\n2. They perform many experiments including comparisons with baselines and ablation studies on the proposed loss terms. They demonstrate the effectiveness of their approach to generating hybrid images.\n3. The paper is well-organized.\n\n--Weakness:\n1. The motivation is still unclear. I still don't get the point for the usefulness of appearance transfer across two different types of objects (e.g. car and animal) which they claim as their contribution. For example, I don't see the application for applying car appearance to animals.\n2. The comparison baselines are too old. For the appearance transfer comparisons as shown in Figure 4 are the papers before 2018. For example, why don't you compare your model with StarGANv2[1] which also demonstrates appearance transfer to different shapes (e.g. Figure 10)?\n\n--Questions:\n1. I'm curious about the results if you replace the histogram method by just using CNN to extract features on the masked output from FineGAN?\n\n--Recommendation:\nAlthough the authors demonstrate the effectiveness of the proposed method, there are some concerns to be addressed: \n1) Motivation is not intuitive. \n2) There are many more recent papers for transferring appearance to another shape, e.g. StarGANv2[1], which is not included in the experiment.\n\nI currently vote negatively but the authors are strongly encouraged to address these concerns.\n\n[1] StarGAN v2: Diverse Image Synthesis for Multiple Domains, CVPR'20",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting direction, limited contribution",
            "review": "The submission describes a method to disentangle shape and appearance of images across two domains such that new images can be generated that have appearance and shape from either of these domains while still being visually convincing. Starting from an established method (FineGAN) to disentangle shape, appearance, background identity as well as a set of \"nuisance\" factors such as pose in one domain, the paper proposes to add a loss term that aims at retaining appearance when moving from one domain to another. This additional loss term essentials tries to keep the low-level image statistics between two images when both of them are generated with the same appearance, but possibly different shapes. It is trained such that it is invariant to the nuisance parameters (same object under differing views has same statistics), but discriminative towards the object appearance (different objects from the same view have differing statistics). The low level features are expressed as histograms of responses of convolution filters over the masked foreground pattern. The paper provides empirical evidences in the form of example images where appearance and shape are combined from two different domains (out of cars, birds, dogs, animals) as well as proxy measurements for the quality of the transfer: (a) how much do the low-level statistics differ in terms of $\\chi^2$ distance, (b) how well is shape disentangled under changing appearance by measuring the foreground overlap between samples, (c) a user preference study (which method transfers shape and appearance better?). The results are compared to some relevant baselines (FineGAN, CycleGan, AdaIn, MUNIT), and show moderate improvements over those.\n\n### Strengths\n**[S1]** The paper is written well and it seems that one could reproduce the method reasonably.\n\n**[S2]** The authors lay out their claims clearly.\n\n**[S3]** Based on the given motivation, retaining the low level statistics of the image, the authors derive how to model a solution, optimize and evaluate it with respect to this motivation in a structured manner.\n\n**[S4]** The empirical evaluation seems to demonstrate the effectiveness of the proposed solution towards the posed objective.\n\n**[S5]** The authors allude to the method being able to translate appearance under shape change when there is either part-part correspondence between the domains (dogs$\\leftrightarrow$cats) or none (dogs$\\leftrightarrow$cars). However, see [W2,W3].\n\n### Weaknesses\n**[W1]** While the overall motivation is fairly clear: How can I retain more appearance between domains where I do not have access to actual labeled samples for training?, the particular heuristical choice of approach, retaining the frequency statistics of low-level filter bank responses, is not well motivated. Low level filter bank response statistics are known to encode texture-like properties in the sense of repeatable patterns such as a cheetah's fur texture (Figure 5, rightmost panel). Are they sufficient to capture other properties of appearance that are less readily encoded in low-level frequency statistics? What other choices of embodying appearance, including texture, are there, and why is the choice of the presented low-level statistic favorable? I feel that the paper is lacking in setting the heuristic in context so that the reader can be confident in the choice of heuristic.\n\n**[W2]** The definition of intra- vs. inter-domain seems somewhat vague. It would be helpful to characterize this distinction either more theoretically or empirically: When are two domains close enough so that I do not need the additional term, when are they too far apart for even this method to work? Cats and dogs compared to cats and cars seem qualitatively different concept relations. The authors do imply hierarchies of closeness of domains (e.g. Section 2: having part level or no part level correspondences), but this is not used in the paper to clearly design or evaluate the method with respect to differing inter/intra-domain distributions or levels of domain proximity.\n\n**[W3]** Weakly supported claim: Section 2 claims \"Moreover, when part-level correspondences do exist (e.g., dogs â†” tiger), it combines appearance and shape in a way which preserves them.\" This indeed could be a strong point, see W2. As I read the paper however, this claim seems only anecdotally supported, e.g. by individual samples in figures 5/6. I feel that more thorough investigation and evidencing of this claim would strengthen the paper.\n\n**[W4]** Reference missing: I feel that [B1] below does indeed discuss and investigate related concepts, albeit from the perspective of style rather than object appearance. Nevertheless, it discusses disentanglement under disjoint domains, and it would be interesting to include this in the discussion of related work.\n\n**[W6]** I read the submission as more of an empirical paper than theoretical paper. From this perspective, the presented empirical evidence seems limited. In order to gauge the effectiveness and benefit of the method better, I wished for a wider range of data, where I can see the relation of shape/appearance transfer and the \"distance\" of the domains better. The authors do mention, but do not show for instance furniture. Can I transfer from cups to people or vice versa?\n\n### Further comments\n**[C1]** There is a flipped $\\pm$ in table 2 (\"Ours vs. FineGAN / dogs $\\leftrightarrow$ birds\")\n\n### Summary\nI feel that there is benefit to the direction that the submission is taking. Although at this point the weaknesses outweigh the strengths, a revision could make a strong contribution if for instance the choice of heuristic is motivated and evidenced more strongly in the context of potential alternatives, and the intra-/inter-domain distinction is worked out more clearly theoretically and/or empirically. As it is I feel that the paper would need significant revision for acceptance at ICLR.\n\n**[B1]** @InProceedings{Lee_2018_ECCV,\nauthor = {Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},\ntitle = {Diverse Image-to-Image Translation via Disentangled Representations},\nbooktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\nmonth = {September},\nyear = {2018}\n}",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}