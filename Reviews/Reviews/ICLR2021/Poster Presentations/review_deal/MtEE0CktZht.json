{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode-level exploratory behaviors.  The innovation is in the scoring and learning from these episode-level behaviors rather than trying to come up with shorter timescale proxies of exploration.  In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (i.e. good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states.  To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level.\n\nA weakness is that the paper uses low-level state in grid worlds and there is some ambiguity in applying this to settings with continuous states.  The authors discuss general strategies for dealing with these limitations as potential future work.\n\nThe reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective.  The authors put in solid effort to address reviewer concerns.  The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces.\n\nOverall, this paper had a consensus \"accept\" rating (7,7,7,6), and I endorse this as my decision."
    },
    "Reviews": [
        {
            "title": "Simple and effective approach, with strong empirical evaluation",
            "review": "Summary:\nThis paper tackles the problem of improving exploration in deep RL for procedurally-generated environments, where state-of-the-art exploration techniques typically fail. In the proposed approach, called RAPID, each agent-generated episode is evaluated with respect to its local exploration score (for the given episode), global exploration score (across all previous episodes), and extrinsic reward obtained. Episodes with high scores are stored in a replay buffer, and a policy is trained via behavioral cloning on batches of state-action pairs from this buffer. This policy is also used to produce the agent-generated episodes.\n\nRecommendation:\nThe approach is simple and intuitive, and empirically improves exploration in procedurally-generated environments. Training on procedurally-generated environments is becoming more common and useful, for instance in domain randomization for sim-to-real transfer in robotics, so this approach would be relevant for the ICLR audience. I have some concerns and questions (detailed below), but overall I recommend acceptance.\n\nPros:\n* The approach itself is simple and easy to implement.\n* The paper is clearly writtten and well-motivated.\n* The empirical evaluation is thorough: RAPID is compared against a suite of state-of-the-art baselines for exploration bonuses, exploration in procedurally-generated environments, and a self-imitation approach; there are also ablation studies and hyperparameter sensitivity studies. The ablation study comparing behavioral cloning versus exploration bonuses (where the per-episode exploration scores are given as part of the reward to the agent) is particularly interesting, given that existing approaches typically rely on the latter.\n* The empirical evaluation is on a variety of domains, including both discrete-action and continuous control tasks.\n\nCons:\n* I would like to see an empirical comparison against Never Give Up (NGU; Badia et al. 2020), which also uses episodic novelty and global novelty to guide exploration. Although NGU uses the same environment for training and testing, because it takes into account how controllable a state is, it wouldn't suffer from the limitation highlighted in Figure 1 (where a state is randomly generated, regardless of the agent's action), and may do well in procedurally-generated environments.\n* It seems strange to me that single state-action pairs are stored in the replay buffer, rather than keeping all state-action pairs from an entire episode together. It's possible that a particular state-action pair may only be good in terms of exploration, _in the context of_ the rest of the agent's trajectory in that episode.\n* The continuous control experiments for MuJoCo locomotion tasks are contrived, since the extrinsic reward for forward progress is summed and given at the end of the episode, which just doesn't make sense for locomotion tasks, and unnecessarily hampers the baseline approaches. Using a goal-reaching continuous control task would be more relevant: e.g., Swimmer in a procedurally-generated maze.\n* I would like to see a discussion of limitations and failure cases for RAPID.\n* There are minor grammatical errors and typos throughout the paper.\n\nQuestions:\n* Can the local score and global score be applied to observations (e.g., image observations) directly, instead of states?\n* Are the scores associated with state-action pairs in the replay buffer taken into account during sampling of batches, or ignored?\n* How were the weights chosen for the total score, that is a weighted sum of the extrinsic reward, local score, and global score? The weight for the global score is very small compared to the others (i.e., 0.001 versus 0.1 and 1). Why is this the case? What happens when this weight is higher (e.g., is there some type of suboptimal behavior that emerges)?\n* Why is it necessary to anneal the imitation learning to zero for some environments, but not others?\n* What is the state space for each of the tasks, that's used to compute the local and global scores? It would be useful to have a table in the Appendix that summarizes this.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review 1",
            "review": "Summary: This paper focuses on building reinforcement learning agents for procedurally generated environments. In particular it presents a method RAPID to intrinsically reward previously seen *good* exploration behaviors by attempting to imitate them in the future. Strong results are presented across multiple procedurally generated environments.\n\nPros:\n1. Procedurally generated environments provide a rigorous test of an agent's ability to generalize as exploration strategies learned cannot be reused as a whole. To counter this, they operate on the hypothesis that high rewarding trajectories are more likely to contain more generalizable exploration behaviors. The break down of this reward into local (per episode) and global extrinsic rewards gives the agent some idea of areas of the game it historically performs poorly on.   \n2. Given that the idea itself is simple (yet effective!), I appreciate that most of the main paper is dedicated to experiments - with each set of experiments designed to answer a particular research question. This make it easily accessible and digestible - answering many of the doubts I had when reading through the intro and methodology sections.\n\nCons: I have a couple of primary concerns mostly dealing with the underlying assumptions made.\n1. This method rewards good exploration behavior historically and as such depends on the ability of the agent to discover such trajectories in the first place.\n2. High extrinsic reward implies good exploration behavior.\nBoth of these assumptions likely do not hold in environments with sparse/ill-placed/deceptive rewards, challenges that environments such as Montezuma's Revenge/Net Hack/Interactive Fiction games are notorious for. In such environments, it is not only difficult to discover good exploration behaviors but also the overall extrinsic rewards themselves often lead to dead ends and globally suboptimal trajectories. The global score, being effectively count-based, alleviates some of this, but it is likely that this form of intrinsic that is based on extrinsic reward will likely fail in such scenarios. It would strengthen the paper to see positive results in such an environment.\n\nMinor: A few of citations to the ProcGen benchmark (Cobbe at al. 2020 https://arxiv.org/abs/1912.01588) and TextWorld (Cote et al. 2018 https://arxiv.org/abs/1806.11532), and the NetHack Learning Environment (Kuttler et al. https://arxiv.org/abs/2006.13760) are missing, being frameworks for looking RL in procedurally generated settings (and also good candidates for future environments to try RAPID in!!).\n\nRegardless, this paper provides a valuable contribution is studying exploration in procedurally generated environments - with the core idea of rewarding historically good behavior being one well proven in the past. The motivation is clear and the experiments well designed - proving that there are indeed environments where this idea works on, within the limits of the assumptions made. I would see this work accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good approach to exploration in procedurally generated environments, may not extend pas tabular environments.",
            "review": "This paper presents RAPID, an exploration algorithm for procedurally generated environments. The paper introduces an exploration scores composed of a local and global score. The local score is computed per-episode, it is the fraction of distinct states visited during an episode, the global score keeps track of the exploratory effort of the agent over the whole training procedure.\nThese two scores are then added to the environment reward and used to improve the agent's policy with imitation learning. RAPID is shown have a faster convergence and lead to a higher performance than previous exploration algorithms for procedurally generated environments.\n\nI found the paper well written and easy to follow. The authors provided many explanations and figures to illustrate the behaviour of their algorithm.\nThe experiment section was great, it included many experiments on different environments as well as an extensive ablation study to showcase the benefits of RAPID. I think the key insight of this paper is using the local score as a reward for exploration and using it with imitation learning. It seems to be a great proxy for counts in the setting of procedurally generated games in the tabular case. I was actually surprised by the performance of the local score, as it is a per-episode metric it seemed to be a weak signal and that would be hard to learn from it. I'm guessing that the ranking of episodes is key here to make the algorithm work.\n\nOn the other hand I was not convinced by the utility of the global score. First of all it is not clear if the global score is state dependent or not, I assume it is not. The authors say: \"the basic building blocks (e.g., walls, doors, etc.), the number of rooms, and the size of each room will be similar. From a global view, the agent should explore the regions that are not well explored in the past\", unfortunately while similar states may be encountered they will likely not be equal, a metric defined using raw counts will often not be useful. \n\"From a global view, the agent should explore the regions that are not well explored in the past\" This sentence hints at the notion that should be more clearly discussed: generalization. While the local score only quantifies exploration within a single episode, the global score tries to leverage past experience to improve exploration, however to do so as the environment changes constantly some kind of generalization is necessary. I am not sure how a metric that is not aware of the generalization can be helpful. This is also shown in the experiment section, the global score that does not appear to significantly contribute to performance except on KeyCorridor-S4-R3 (any reason why?) and it is even disabled for Mujoco experiments.\nAnother issue is that contrary to some other algorithm like RIDE, RAPID does not easily extend to environments with large or continuous state action spaces because obtaining meaningful counts in these environments is difficult, to be fair this an issue still faced in non procedurally generated environments. It would also be nice to know how RAPID was applied to Mujoco experiments, was the observation space discretized?\n\nOverall I lean towards acceptance as I think that the paper presented a good idea with some solid experiments even though I have doubts regarding how applicable it can be to more complex environments.\n\nFew typos:\nthe methods based on intrinsic rewards -> methods based on intrinsic rewards\nmay take extremely long time -> an extremely long time\nexploration in sparse environments. -> environments with sparse rewards\ncould vary in diffident environments -> different environments\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper presents an exploration method for procedurally-generated environments, RAPID, which imitates the past episodes that have a good exploration behavior. First, authors introduce exploration scores, local score for per-episode view of the exploration behavior, and global score for long-term and historical view of exploration. The authors use the weighted sum of these two exploration scores and extrinsic reward as a final episodic exploration score. They rank the state-action pairs based on episodic exploration score and train the agent to imitate behaviors with high score. In experiments, they show the results by comparing state-of-the-art algorithms in several procedurally-generated environments.\n\nThis paper is well written, but I have following concerns and questions about the paper.\n- MiniGrid seems to be a partially observable problem in which only 7x7 observation is given to the agent. What does the state used when calculating the exploration score in the paper mean?\n- Moreover, the paper also includes experiments on problems with continuous state space. In that case, I wonder the details of how the number of states and distinct states are handled.\n- When calculating the episodic exploration score (Equation (3)), it is thought that the results vary greatly depending on the weight $w_0$ - $w_2$ values. In the paper, it seems necessary to show the criteria for determining the weights and the difference in results according to the weights.\n- In order to show the effectiveness of the ranking buffer, in addition to show with/without results as shown in Table 1, the results when using the replay buffer without considering the ranking should be compared together.\n- It would be better if the MuJoCo tasks in Figure 7 could be explained more clearly. (ex. The existing MuJoCo would be a singleton, but what was changed to make it a procedurally-generated environment?)\n- In the experiment, the explanation is mainly based on KeyCorridor, but there is no explanation for what task it is. It would be better if a brief explanation of the KeyCorridor task is included in the paper (with the reason why this task is hard).\n- The author claims that the proposed method is effective in very sparse environments, but I am not sure exactly what makes it effective in very sparse environments. The justification for this claim needs to be explained more intuitively.\n\nThis paper contains experimental results for various tasks, but It is unclear the contribution of this paper and the reason why the proposed method is effective in very sparse and procedurally-generated environments. It seems necessary to explain intuitive motivation more clearly in the paper. Moreover, for the ablation study, It also needs precise verification that can show the effect of each component more intuitively, not ablation that simply separates and compares each component.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}