{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work proposes to train EBMs using multi-stage sampling. The EBMs are then used for generating high dimensional images, performing image to image translation, and out-of-distribution detection. The reviewers are impressed with the results, but indicate that the novelty is limited. While I agree that the work can be seen as a combination of previously proposed techniques, demonstrating that this combination can be made to work well is still a significant contribution to the field. In addition, the paper demonstrates strong results in using Langevin dynamics to translate between images, which I do think is novel. I therefore recommend accepting the paper for a poster presentation."
    },
    "Reviews": [
        {
            "title": "Nice work on up-scaling EBMs and using them for unsupervised image translation",
            "review": "Much like progressive growing of GANs two years ago, this paper adopts a similar coarse-to-fine procedure for scaling EBMs to higher resolutions. In particular, the approach starts from learning EBMs on low-resolution images and then smoothly transitions to higher resolution by carefully designing an expand layer and a smooth sampling procedure. Authors were able to obtain competitive FID scores on CIFAR-10, and demonstrate the first set of 256x256 image samples from EBMs. In addition, authors demonstrate successful application of EBMs to unpaired image-to-image translation.\n\n#### Pros\n* Strong experimental results. The FID scores obtained are among the best achievable results with maximum likelihood training. Scaling to 256x256 images is a great advancement of the field. Various experiments on denoising and inpainting demonstrates the versatile applications of EBMs.\n\n* Applications in unpaired image-to-image translation demonstrate strong potential of the approach in image editing\n\n* Sampling typically takes 50 Langevin steps. This is much faster than denoising score matching with Langevin dynamics or denoising diffusion probabilistic models, which typically need thousands of Langevin steps.\n\n#### Cons\n\n* FID computation is done using a PyTorch implementation. Although the results should be close to the original FID implementation based on TensorFlow, they are not exactly comparable to previous results. The numbers in tables are therefore not rigorous.\n\n* There is no widely agreed protocol on how to compute FIDs on CelebA 64. The processing methods can be quite different in different papers, and the number of samples used for FID computation can also be different. Please include your settings explicitly in the paper, otherwise the comparison in the table is not that meaningful.\n\n* The progressive growing procedure requires multiple stages of separate training and also need careful tuning of model architecture and sampling to make transition smooth.\n\n* There is a factual error in Appendix A.3.1.The work of Song & Ermon (2020) does not apply a sequence of decayed perturbation from 1 to 0.01. Quite the reverse, starting from noise scales different from 1 is one main point in that paper.\n\n-------------\nPost-rebuttal\n\nReporting PyTorch-computed FIDs risk the fairness when comparing against previous work. The repo quoted by the authors had a well-known issue, leading to much lower numbers compared to those computed by the original TensorFlow repo. Although this issue has been alleviated following some code update this year, the numbers still won't exactly match those of TensorFlow. Therefore, from a scientific perspective, the FID numbers have to be recomputed with the original TensorFlow code for a rigorous publication.\n\nIn addition, the FID number reported by the authors for their model is computed between 10k samples and the test dataset, while most other FID numbers in Table 1 are computed between 50k samples and the training set (following the original settings of [1]). This also makes the comparison not fair.\n\nI completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality. However, since the authors choose to report FIDs and compare against those directly ported from previous work, they have to follow the convention and ensure a fair comparison. Since I didn't get a satisfying response from the authors (authors claimed to \"have corrected it\", they just changed phrasing of their response but didn't re-compute scores), I decide to lower my scores from 7 to 6. I am still marginally inclined to acceptance, but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation.\n\n[1] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. and Hochreiter, S., 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems (pp. 6626-6637).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This work proposes a method to improve generation with energy based models. The work further shows how energy based models can also be extended to cross class image translation.\n\nStrong Points:\nGenerated samples appear to look good\n\nWeak Points:\nMy most major concern is that the overall technical novelty of the work is rather limited, and I do not think passes the bar of acceptance for ICLR. Past work [2] has shown that EBMs can composed with multi-scale sampling, while [1] notes that the smooth activation function Swish significantly improves EBM generation. Similarly, cross domain generation with EBMs have been previously demonstrated such as in [1]. While [2] uses a separate hierarchy of multi-scale EBMs, this work uses the sampling scheme in [3].\n\nMy second most major concern is other the theoretical framing of the proposed approach and well is issues with statements made in the paper. W\n\nThe proposed sampler linearly decays the noise schedule to 0, but what are the theoretical implication of such a behavior? For Langevin sampling to valid, the step size of both the model and the noise must decay to 0.\n\nThe proposed approach generates samples by running a fixed number of Langevin steps to generate a sample. But also the approach claims to train a maximum likelihood objective, where Langevin steps are used to approximate the energy landscape. A good energy landscape should be able to support an arbitrary number of MCMC sampling steps. What happens when a larger of number of steps of Langevin is run?\n\nThe paper states that past approaches such as [1] have relied on adding white noise to samples. I do not think this is the case. Can the authors elaborate?\n\nMy next major concern is that overall text of the paper is somewhat confusing to read, with some awkward sentences in the text. For example:\n\nSection 3.2:\nAssuming the total number of stages is S and the training starts from the min => Given a total of S stages of training \nIn what follows, be aware of the only difference => The only difference between\nIn this way, both stability and time efficiency in training EBMs are benefited. => In this way, both stability and time efficiency in training EBMs benefit.\n\nSection 3.3:\nWe name the style that can differentiate different domains discriminative saliency. => We call this approach discriminative saliency.\n\nConclusion:\nWe owe the success to the newly proposed network architecture, smoothing activations, the powerful interpretability of EBM =>\nThis success is due to our newly proposed architecture, smoothing activations, and the flexibility of EBM.\n\nTo improve the clarity of the text, it would be good to perhaps add an algorithm block on the precise sampling procedure used to train EBMs. For example, the term short-run samples was never defined and I had to look at referenced papers to figure out the meaning.\n\nThe related work seems to incomplete and misses a lot of recent work towards training EBMs. Some examples of additional approaches towards training EBMs including score matching and minimum probability flow learning.\n\nThere are also some issues with the empirical evaluation of the proposed approach:\n\nTo showcase mode coverage of the EBM, it's important to show diverse inpaintings, as opposed to a single inpainting of an image.\n\nFurthermore, as deep likelihood based model it is important to show some density/likelihood evaluation -- using some variant of AIS.\n\nMinor Comments:\n\nWhy is the proposed approach much more efficient than past approaches for image to image translation? It seems Langevin sampling steps would take more computational time.\n\n \n[1] Yilun Du and Igor Mordatch, Implicit Generation and Generalization with EBMs\n\n[2] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative convnets\nvia multi-grid modeling and sampling. \n\n[3] Tero Karras et al. Progressive Growing of GANs for Improved Quality, Stability, and Variation\n\n\n===== Post Rebuttal Updates\n\nI thank the authors for responding to my comments. The work shows improved generative performance by using a multi-scale architecture, however the approach is the same that used in previous GAN works. Furthermore, the generation quality is not as good those of recent GAN works and I don't believe the added EBM benefits are significant.  In addition, other contributions, such as the use of the Swish activation and domain transfer has also been noted as used in previous work [1]. I also have additional empirical concerns over the experiments.\n\nI list additional comments below:\n\n1) The diverse inpaintings (Figure 11) do not really look very diverse to me and seems to suggest that the model is not learning a likelihood. To evaluate diverse inpaintings, it would be good to follow past work and evaluated on ImageNet images where only the top half of an image is conditioned. [3]\n\n2) The likelihood evaluation are hard to interpret in A.5. An issue with evaluating AIS based likelihood sampling on MNIST for upper and lower bounds of likelihood depends heavily on the large number of steps of sampling required.  Upper and lower bounds depend heavily on the number of steps of sampling run (with unrealistically high likelihoods obtained when running only a few steps of AIS). It seems unlikely to me that the proposed model obtains a significant boost to log-likelihood compared to past approaches, and it would be good to report both the number of AIS transition distribution (and ensure that it is same used in [1]). In particular, I believe that this approach is likely to perform poorly with a large number of gradient steps (as the rebuttal response noted), which is required for proper evaluation of likelihood. \n\n3) The related work is still missing older work in the area of EBM training. Instead of adding additional references to recent work on score based generative modeling, I think the others should cite past works that have used score matching to training energy models. For examples, such works include [4, 5, 6].  \n\n4) I didn't find the out-of-distribution results to be a particularly compelling application of the model (although its good that it performs similarly to past approaches). The only results that appear to be better are uniform (which in my experience performance across all models fluctuates) and interpolation. Furthermore, the model from [1] used in section 4.2 is not conditional. \n\n5) I wouldn't say this paper is the first to generate 512x512 samples with EBMs. For example see [3].\n\n6) It's difficult to evaluate an open source code release, since the code is not provided at the time.\n\n7) Regarding the approach in [1], when doing source translation images are initialized from ground truth images from a seperate class.\n\n\n[1] Yilun Du and Igor Mordatch, Implicit Generation and Generalization with EBMs\n\n[2] Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, Pixel Recurrent Neural Networks\n\n[3] Tian Han et al. Divergence Triangle for Joint Training of Generator Model, Energy-based Model, and Inferential Model\n\n[4] Jascha Sohl-Dickstein et al. Minimum Probability Flow Learning \n\n[5] Saeed Saremi. Deep Energy Estimator Networks\n\n[6] HyvÃ¤rinen, Aapo. Estimation of non-normalized statistical models by score matching.\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Energy-based generative models and progressive training",
            "review": "The authors propose an energy-based generative model for image-to-image generation that differs from previous methods by incorporating a progressive learning scheme that gradually increases the resolution of images being trained on.\nStrengths:\n-- The presence of a saliency map in image-to-image translation is a benefit that is harder to get out of other methods.\n-- The ability to infill missing parts of an image with the same model that does unsupervised translation is a nice benefit.\n-- The smoothing along resolutions during training seems like a good way to incorporate the progressive training technique into this model.\nWeaknesses:\n-- The novelty here is very limited. The difference between this and previous energy-based models is the scheme of progressively generating at 8x8, 16x16, 32x32, etc... which itself is a well-established technique from the Progressive GAN. Given neither of these things is new, the novelty lies in just using the one with the other.\n-- The authors claim the saliency map is a benefit of their method but offer no comparisons with other models, e.g. the marginal gradient methods for any CNN model including CycleGANs.\n-- The quantitative scores for the GAN methods are significantly worse than models can actually achieve now. The Big-GAN, which is several years old now and not even SOTA on CIFAR has a better score than the one proposed here.\n-- The qualitative results from the GANs are much poorer than can actually be achieved. For example, the orange->apple not changing shape is a result of a poor architecture with too high of a cycle-consistency coefficient.\n-- The characterization of GANs in the previous work section is very poor. The authors claim, without evidence or citation, that the invertibility of the two directions in a CycleGAN \"may intensify long-standing instability issues\" when it very likely does the *opposite*. They also criticize GANs for requiring an \"elegant design\" like instance normalization, when this is a standard and easy-to-implement part and is no more problematic than any aspects of the authors' proposed framework. Also masquerading as a \"critique\" is that GANs have a \"fancy architecture\": I don't think the authors could devise a definition of fancy that excludes their model. If this section is designed to motivate their energy-based approach, it can be done without using arbitrary, subjective, inaccurate insults of other methods.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple approach, strong results, some details need clarification ",
            "review": "Summary:\n\nThis paper presents a number of methods to scale up training and sampling of EBMs on image data. The main contribution consists of an approach for progressively growing the model by increasing the image resolution as training progresses. This approach echos similar approaches used for scaling up GAN training. The approach involves slowly annealing in new blocks to the model during training which processes the image at increasing resolutions. This allows training of image EBMs at notably larger resolutions than published in prior work.\n\nThe authors also present a few additional architectural improvements such as using smooth nonlinearities over the currently-popular leaky ReLU. They find that this change allows us to remove the commonly-used Gaussian noise added to the input, improving sample quality. The authors demonstrate the performance of their approach, focusing on image generation. They present results with FID and compare to other generative models and EBMs. They also examine the quality of the learned energy functions with inpainting and denoising experiments. \n\nBeyond unconditional image generation, the authors also propose a simple method for image-translation based on EBMs. In this approach, unconditional EBMs are trained for each domain and an image is translated by taking a sample from 1 domain and running a Langevin MCMC sampler in the target domain EBM starting from the sample from the source domain. The resulting image retains many of the high level characteristics of the source image but adds low level characteristics from the target domain. Surprisingly this simple approach outperforms more involved methods based on adversarial training. \n\nStrong areas:\n\nThe empirical results in this paper are impressive, made more impressive by the simple nature of the approach. It appears that the proposed method gives a notable improvement in image generation quality over prior EBM models while also giving an improvement in run-time and parameter-efficiency. Particularly interesting are the very strong results in the image-translation task. I am not familiar with this space so I cannot say how much of an improvement is reported here over the adversarial methods, but it is quite shocking to me that this very simple approach would outperform two recent state-of-the-art methods for this task while also having no cycle consistency objective. This should certainly bring attention to EBMs for this task.\n\nWeaknesses:\n\nI felt that many of the experimental details in this paper could have been made more clear. I found myself having to re-read the paper to find out that the authors used short-run MCMC instead of PCD. As these training procedures are quite different, this should be made more clear.\n\nI also found the proposed architecture hard to understand. I could not tell if the full energy function continues to take in lower-resolution versions of the image as we add levels or if those inputs are replaced by the outputs of the added higher-resolution layers. The figures are not very clear on this. When sampling high-resolution images, do we first generate lower resolution samples, and use these to seed the high-resolution samples? Figure 2a makes it look like this is what is happening. But then algorithm 1 makes it seem like we generate samples directly from noise for the current training scale. This is important for understanding the method and it could be made much more clear. I would recommend the authors split apart algorithm 1 into an algorithm for training and one for sampling. I think that would make the method much more clear. \n\nWhile the experimental results were strong, I am curious why the authors did not present any results on out-of-distribution detection (aka anomaly detection). These are common nowadays and I am curious as to how this approach will perform given its improved scalability over many EBM models which we find perform very well at this task. \n\n\nMy recommendation:\n\nThis is an interesting work that presents a simple method that allows image EBMs to scale up considerably. The proposed approach notably improves image generation and allows models to be trained on much larger images than has been possible by EBMs in the past. Further, the image-translation application is intriguing and the strong results should make EBMs a standard baseline for that application. \n\nBecause of these strong empirical results, I am recommending to accept this paper, but I am not advocating for a strong-accept.  While the results are strong, the work does not improve our understanding of these models, or introduce any particularly novel techniques. \n\nI think the contributions of this work could be made greater with some experiments on some other applications such as OOD. I also think the work could be greatly improved by some rewrites to section 3.2. This is the main contribution of the paper and the authors should take more care to ensure that their effort is easily understood. \n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}