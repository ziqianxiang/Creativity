{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Shapley values are an important approach in extracting meaning from trained deep neural networks, and the paper proposes an innovative approach to address inefficiencies in post-processing to compute Shapley values, by instead incorporating their computation into training.  There was a robust discussion of this paper, and the authors' comments and changes substantially strengthened the paper and the reviewers' view of it, to the point that all reviewers now recommend acceptance.  Some lingering concerns remain that the authors should continue to work to address.  Is the method of computing Shapley values used as the baseline in the paper really state-of-the-art, or artificially weak?  The empirical results were methodologically sound but not as strong as one might expect or hope.  These concerns detract somewhat from enthusiasm, but nevertheless the paper yields an innovation to a widely-used approach to one of the most pressing current research problems.  The reviewers had a number of smaller suggestions that should also be incorporated including more significance testing and reporting of resulting p-values."
    },
    "Reviews": [
        {
            "title": "Interesting new models for generating fast SHAP explanations",
            "review": "This work proposes a new model class designed to make SHAP value calculations more efficient. The proposed method exploits sparsity and additivity among intermediate values to provide fast exact SHAP values for shallow ShapNets, and fast approximate SHAP values for Deep ShapNets. This approach enables SHAP-based regularization during training, layer-wise explanations, and faster SHAP-based explanations with minimal loss in quality.\n\nThe approach seems promising, but I have several questions and concerns about the method and presentation. I'll start with questions about the method.\n\n- The authors write that SHAP values provide the unique explanation that satisfies a number of desirable properties; however, as pointed out by many works at this point [1, 2, 3], there are several proposed approaches for handling the held out features. Why do the authors choose to use single reference values rather than the interventional or conditional distribution? To what extent is the ShapNet approach amenable to other definitions of missingness?\n- A couple questions about the shallow ShapNet. (i) When performing a forward pass, is it actually necessary to calculate the SHAP values (the Shapley representation)? The sum of the SHAP values $\\phi_i(f^{(j)}, x)$ for $i = 1, \\ldots, d$ is equal to $f^{(j)}(x)$, so it seems that SHAP value computation (represented by $g$ or $\\Omega$) is not required unless one requires the explanation (e.g., for regularization). If that's the case, then a follow-up question is (ii) could any function whose output is the linear combination of intermediate values $f^{(j)}(x)$ be called a shallow ShapNet? And does your version of a shallow ShapNet yield faster explanations only because of the sparsity in the intermediate value functions?\n- Is the ShapNet method unable to work on models that are not the linear combination of intermediate values (e.g., the classification probability after softmax activation rather than logit)? If so, that would be worth mentioning briefly.\n- It seems that the recommendation is to determine the sparsity pattern in the value functions $f^{(j)}$ using the butterfly trick inspired by FFT; are there any other approaches worth considering that could improve either performance or compute time? Do the authors think that it would be worth learning the pairs that achieve the best performance?\n- Perhaps the most important question about Deep ShapNet explanations is how similar they are to SHAP explanations (e.g., from KernelSHAP). If possible, the metrics from Table 4 would ideally be shown in the main text, and perhaps extended to include results from the vision models. These metrics, along with Table 3, seem very promising.\n\nSome concerns regarding the presentation and experiments:\n\n- Strumbelj and Kononenko [4] proposed a sampling algorithm for SHAP values that should be cited in the second paragraph of the introduction, along with the other estimation algorithms.\n- Figure 2 is helpful but a bit complicated due to all the lines and bounding boxes, and the legend does not exactly correspond to the colors used. This figure may be worth revising and clarifying.\n- There's a lot of verbosity used to describe some straightforward ideas involving SHAP values. The \"Shapley transform,\" the \"Shapley representation\" and the \"Shapley module\" are all based on the simple idea of concatenating SHAP values into a matrix/tensor. The authors might consider presenting these ideas in a more straightforward manner.\n- The sum operator $\\text{sum}^{[\\alpha_i]}$ uses ambiguous notation. Shouldn't the superscript indicate the dimension to sum across rather than the number of entries along that dimension? It would be unclear how to apply the operator $\\text{sum}^{[\\alpha_i]}$ if there was ever $\\alpha_i = \\alpha_j$ for some $j$.\n- The generalized Shapley transform $\\mathcal{S}$ is not clearly defined in Definition 13. Perhaps the authors could provide a definition in terms of $\\Omega$? The use of this notation in the ShapNet composition (Eq. 6) seems to mask an underlying dependence on functions like $f^{(1)}, \\ldots, f^{(c)}$ at each layer. Would it be possible to use notation that shows these functions?\n- In Table 1, perhaps baseline model performance should be provided for the image datasets as a reference.\n- The text and caption for Figure 3 do not seem to indicate which dataset is used. The results look promising; could KernelSHAP be provided as an additional comparison method?\n- The authors point out the ShapNets enable (i) explanation regularizations and (ii) layer-wise explanations. Table 2, Figure 4 and Figure 6 show that the regularization works as intended, but this is a lot of space to prove this point; other results might be more important to include in the main text (mentioned earlier). Figure 5 shows that layer-wise explanations are possible, but the text does not explain how this is useful or when users would want this.\n\nDespite some concerns, I think this paper contributes some novel ideas that move us towards tractable + principled explanations for expressive models. \n\n[1] Aas et al., \"Explaining individual predictions when features are dependent: More accurate approximations to Shapley values\" (2019)\n\n[2] Kumar et al., \"Problems with Shapley-value-based explanations as feature importance measures\" (2020)\n\n[3] Merrick and Taly, \"The Explanation Game: Explaining Machine Learning Models Using Shapley Values\" (2019)\n\n[4] Strumbelj and Kononenko, \"An Efficient Explanation of Individual Classifications using Game Theory\" (2010)\n\n##########\nUpdate\n##########\n\nI'm pleased with many of the changes the authors have made in response to the reviewers' concerns. However, I'll mention a couple lingering concerns.\n\n- The authors have updated their paper regarding use-cases for layer-wise explanations, but it remains unclear whether this feature is impactful. While it is possible to perform instance-level pruning with ShapNets, is there any reason to do so? Finding features with low SHAP values requires actually calculating the SHAP values, which takes longer than just evaluating the function. Therefore, it seems that this technique would not lead to saving either time or memory. If the authors disagree, they may consider improving this aspect of the paper.\n- The authors have clarified that they use the \"reference values\" approach to holding features primarily because it is convenient. To my knowledge, no existing research actually advocates for this approach. Most research advocates for either the \"interventional\" [2] or \"observational conditional\" approach [1]. The authors cite a paper that mentions the \"reference values\" idea (Baseline Shapley) [3], but not even this work truly advocates for this approach. In my view, this is a rather severe limitation of the proposed approach, and the authors did not suggest that they see a way to overcome it. To call these \"SHAP values\" is almost misleading because it silently changes one of the core aspects of SHAP. As an example of the consequences of this limitation, any feature that is equal to its reference value will have a SHAP value equal to zero, but it is easy to image how such features can be informative (e.g., black regions in an MNIST digit, such as missing arcs on the left-hand side of a \"3\" that distinguish it from an \"8\").\n\nWhile it is very helpful to calculate SHAP values faster, this is a flawed version of SHAP that is not supported by existing research that considers the question of how to model missing features. Unfortunately, the proposed approach apparently lacks the flexibility to work with different notions of missingness (e.g., the interventional or observational approach). For that reason, I'm lowering my score by one point (6).\n\n[1] Frye et al., \"Shapley-based explainability on the data manifold\" (2020)\n\n[2] Janzing et al., \"Feature relevance quantification in explainable AI: A causal problem\" (2019)\n\n[3] Sundararajan and Najmi, \"The many Shapley values for model explanation\" (2019)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clever idea, but empirical results could be stronger",
            "review": "(I am raising my score by 2 points after the author response)\n\n--\n\nSUMMARY\n\nThe authors present a new type of network architecture where the each intermediate layer outputs the shapely contributions of the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ (with respect to some reference $\\boldsymbol{r} \\in \\mathbb{R}^d$)  to $\\sum_{j=1}^c f^{(j)}(\\boldsymbol{x})$, where $f^{(j)}$ are functions that are parameterized by a neural network.  The functions $f^{(j)}$ can have vector-valued outputs (called \"meta-channels\" and denoted by the letter $n$ in the text), in which case the layer would have output dimensions $\\mathbb{R}^{n \\times d}$ (one entry for each meta-channel and input feature). Typically, computing shapely contributions to a function requires a number of operations that increases exponentially with the input size; the core trick that the authors use to make their networks scalable is to aggressively limit the subset of features (which they call the \"active set\") that are provided to each individual function $f^{(j)}$ - specifically, if only two inputs $(x_1, x_2)$, with reference values $(r_1, r_2)$, are provided to $f^{(0)}$, then the shapely contributions $\\phi(f^{(0)}, x_1)$ and  $\\phi(f^{(0)}, x_2)$ can be computed exactly from $f^{(0)}(x_1, x_2), f^{(0)}(x_1, r_2), f^{(0)}(r_1, x_2)$ and $f^{(0)}(r_1, r_2)$. In the case of deeper networks, the \"active sets\" (i.e. inputs) of the functions $f^{(j)}$ are disjoint pairs of size 2 that cover the full set of inputs, and from one layer to the next the pairs are permuted, which can allow the network to learn complex dependencies. When computing shapely contributions to subsequent layers, the \"meta-channels\" for a given input feature are grouped together - thus, every layer outputs estimated Shapely contributions for the original $d$ input features. To obtain the network output, the shapely contributions after the final network layer are simply summed over the $d$ input features. The authors also extend this setup to work on computer vision tasks, where parameters are shared between different input patches.\n\nThe authors benchmark their networks on two real-world tabular datasets and several computer vision tasks, and claim that their proposed Shapely Explanation Networks, or ShapNets, can achieve competitive performance while retaining explainability. On the tabular datasets, the authors demonstrate that their Shapely Explanation Networks provide good estimates of the true Shapely contributions for a fraction of the computational cost compared to methods like KernelSHAP. The authors also demonstrate how the explanations provided by ShapNets can be used for attribution regularization.\n\nSTRENGHTS\n\n- I think the idea of these ShapNets is very interesting, and the authors have clearly put considerable thought into how to design the ShapNets to make them efficient and scalable. The authors do well to draw a comparison to generalized additive models, which are easy to interpret but do not learn interactions; these ShapNets could potentially enable good explainability while also allowing the network to learn interactions\n- Estimating Shapely values through ShapNets is far more computationally efficient that KernelSHAP (table 3 of the supplement) and more accurate (with respect to the true Shapely values) than DeepSHAP, as shown in Table 4 of the supplement (however, agreement with the true Shapely values is not synonymous with the explanation being the most useful in a particular practical situation, as I discuss below)\n- Because the Shapely values are output by the network, they can also be regularized during model training\n\nWEAKNESSES\n\n(1)  The authors mention that they use an FFT-like butterfly permutation in order to allow the network to learn complex interactions between pairs of inputs, and they set the number of layers to be log_2 [number of features]. As diagrammed on the right of Figure 2, it does not look like this permutation is sufficient to allow every input to interact meaningfully with every other input - in particular, $x_1$ does not seem to be interacting (strongly) with $x_4$, and $x_2$ does not seem to be interacting (strongly) with $x_3$. The reason I put \"strongly\" in parentheses is because the only way $x_1$ can interact with $x_4$ is by modifying the Shapely contribution $g_2^{(1)}(x)$ of the input $x_2$ - but in the case where $x_2$ does not vary much (e.g. imagine it is always set to its reference value), that would altogether prevent the network from learning an interaction between $x_1$ and $x_4$. Also, it is counter-intuitive for a network to learn an interaction between two features by modifying the Shapely contribution of a third feature. It seems like the core issue is the absence of a hierarchical structure? The use of log_2 [number of features] layers implies a binary-tree-like hierarchical structure that seems to be missing in this setup.\n\n(2) One thing that can limit the applicability of ShapNet is that, for tabular data, TreeSHAP already provides a way to learn a powerful predictive model while also providing an efficient way to compute the SHAP values. Thus, in order to make a case for ShapNet on tabular inputs, the authors would need to show that ShapNet outperforms tree-based models. Table 1 shows the results of benchmarking on the Yeast and Breast Cancer datasets, but it looks like Generalized Additive Models achieve the best accuracy on these datasets (outperforming the DNNs as well as ShapNet)...thus, it seems that these datasets don't have complex interactions in them, and thus there wouldn't be a reason to prefer ShapNets over GAMs on these datasets (let alone prefer ShapNets over tree-based models). \n\n(3) Of course, tree-based models are not suitable for computer vision data, and ShapNets definitely have an edge-here - however, the benchmarking of explanation quality for the computer vision data seems to be limited to Figure 3, which used the \"drop in accuracy as features are removed\" as measure of explanation quality. This particular metric has issues in that the modified image could be out-of-distribution (see the ROAR paper by Hooker et al. - https://arxiv.org/abs/1806.10758). I would recommend that the authors instead either explore: (a)  the \"remove least k salient pixels\" method from the FullGrad paper: https://arxiv.org/pdf/1905.00780.pdf (b)  the \"digit flipping\" experiments used in the DeepSHAP/DeepLIFT papers, or (c) (best) train their models on the BAM dataset and quantify interpretation quality accordingly: https://github.com/google-research-datasets/bam. Also, I would be interested in seeing the timing benchmarks for these computer vision models (it seems that Table 3 in the supplement is for an untrained randomly initialized model with 16 features). I would also be curious to see how the contributions estimated by ShapNets compare to the true Shapely values for the computer vision datasets (Table 4 in the supplement seems to report this for tabular data only). However, I understand if it is too computationally expensive to accurately estimate the true Shapely values on vision data.\n\n(4) A note regarding Table 4 in the supplement: while it is good that ShapNets produce explanations that are similar to the true Shapely values (as this is their stated goal), we should not mistake \"agreement with true Shapely values\" as meaning the explanation is useful in a particular practical application - for that, different benchmarks would be required (such as the ones mentioned in the previous point).\n\n(5) The authors present \"Layer-wise explanations\" as a \"new capability\", but this seems a bit misleading; any post-hoc explanation method can be applied to give explanations for intermediate layers by simply treating the intermediate layer as the first layer of a truncated model, and using the activations of the intermediate layer under the reference input as the reference value of the intermediate layer (e.g. see https://captum.ai/api/neuron.html).\n\n(6) In a similar vein, the authors present \"Explanation regularization during training\" as a \"New capability\" but this is a bit misleading; explanation regularization during training has been performed using post-hoc attribution methods (as the authors acknowledge under \"Extended Literature Review\" in Appendix C). The authors don't seem to have compared their proposed explanation regularization during training to these earlier works.\n\n(7) The paper was very hard to read - it was only thanks to staring at Figure 2 that I managed to form a sense of what was going on, and I think most readers would similarly struggle. One point of confusion is that there are two kinds of outputs of each layer - the function output, denoted by the capital letter $G$ in Definition 11, as well as the shapely contributions, denoted by the lowercase $g_i$. The authors don't always clearly distinguish which \"output\" is being considered (e.g. the authors write \"we construct deep ShapNets by cascading the output of Shallow ShapNets from Def. 11\", but Def. 11 is for the function output denoted by the capital letter $G(x)$, whereas from Figure 2 it is clear that what is provided to subsequent layers is not $G(x)$, but rather the Shapely contributions $g_i(x)$.  Similarly, the discussion of \"meta channels\" is very confusing; when\"meta channels\" are first introduced in Remark 9, the authors describe meta-channels as the result of applying a Shapely Transform to a tuple of vector-valued functions - i.e. it is implied that meta-channels arise as part of the outputs of the ShapNet layers; however, the example of meta-channels that is given is the RGB channel of pixels, which is in the input. I eventually gathered that meta-channels are analogous to what one would normally think of as the \"channels/neurons\" in a neural network, because the actual \"channel dimension\" as it is described in this paper disappears due to the summation over the channels $c$ in Definition 11. I think the flow of the paper would be greatly improved if the authors stated their big-picture strategy up-front: that they can compute Shapely values efficiently because every parameterized function only operates on two features, and that they sum over many such functions in order to obtain the layer output (the analogy to a generalized additive model was helpful).\n\nMINOR\n\n- The caption of Figure 3 should specify which model/task the lines are being plotted for\n- The authors do not appear to have trained their models using early stopping; based on the methods description in the supplement (section G), it seems that the models were trained for fixed numbers of epochs. This does not, as far as I can tell, change the core results of the paper, but it is important context (e.g. helpful to know when evaluating  the change in accuracy due to regularization in Table 2).\n- In table 4, the authors wrote 1.42e+03 for KernelShap & Deep SHAPNet for the untrained models - I'm guess that was meant to be 1.42e-03?\n- The authors write \"DeepLIFT (scaled to get DeepSHAP)\" - can they clarify what they mean by this scaling? Are they referring to the Rescale rule of DeepLIFT? The Rescale rule doesn't refer to a scaling of the explanations; it's a particular type of modified backpropagation rule that was described in the DeepLIFT paper.\n- A suggestion regarding explanations that are computed with respect to the softmax logits, in case the authors are not doing this already: for each feature, it may be helpful to subtract the average contribution score across all classes from the contribution score to each individual class; that way, you can account for the softmax normalization, in that if a feature contributes equally to the logits of all classes then it effectively contributes to none of the classes (due to cancellation in the softmax). This point is discussed in the DeepLIFT paper in the section \"Adjustments for Softmax Layers\".\n\nEXPLANATION OF RATING\n\nWhile I think the core idea of ShapNets is very interesting and promising, I think the empirical results are currently somewhat lacking - in particular, I hope to see a clear use-case where one would prefer ShapNets over alternative methods. For this reason, I'm rating the paper at marginally below the acceptance threshold.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice idea, but limiting the cardinality of active sets of the Shapley modules seems to be restrictive ",
            "review": "##########################################################################\n\nSummary:\n\nThe paper proposes to incorporate Shapley values as latent representations in deep models. Specifically, the paper constructs Shallow SHAPNETs that computes the exact Shapley values. The paper also constructs Deep SHAPNETs that maintain the missingness and accuracy properties of Shapley values. The effectiveness of the proposed SHAPNETs is demonstrated through experiments on synthetic and real-world data. \n\nOverall, it seems to be a good idea to incorporate Shapley values into deep models and the proposed method seems to be reasonable. The empirical results have demonstrated the usefulness of the proposed method. The paper is also well-written and technically sound. I have some comments as detailed below.\n\n##########################################################################\n\nComments: \n\n- The main challenge for Shapley values is its computational complexity. The paper overcomes this challenge by forcing the active set of all Shapley modules to be size=2. While mitigating the computational challenge, this would limit the representation power of the model. The authors showed that this is not a big issue by providing a comparison in Table 1 on three datasets (Synthetic, Yeast, Breast Cancer). However, all three datasets are low dimensional and do not require a high representation power of the model. Therefore, I am not quite convinced that the proposed SHAPNETs have satisfactory representation power. A comparison of Deep SHAPNETs and the DNN models on the three high-dimensional image datasets (MNIST, FashionMNIST, Cifar-10) would better answer this question. \n\n- In principle, we can tradeoff between the representation power and the computational efficiency by varying the size of the active sets of the Shapley modules. Have the authors considered a comparison of SHAPNETs with different active set sizes? \n\n- In Table 1, Shallow SHAPNET has a better performance than the Deep SHAPNET. Why is that? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}