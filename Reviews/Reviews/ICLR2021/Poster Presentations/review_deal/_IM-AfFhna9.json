{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Three of four reviewers are in favour of accepting the paper. Some reviewers raised valid criticism regarding the derivations, interpretation of the mathematical analysis and experimental results. So clearly some aspects of the paper could and should be clarified in accordance with the points raised by the reviewers. However, all in all the paper contains enough contributions to warrant publication.   "
    },
    "Reviews": [
        {
            "title": "Interesting take on Unifying VCL and EWC",
            "review": "The authors propose Generalized VCL in this paper, which consists of multiple ideas: first, the authors introduce a beta-Elbo, which facilitates downweighting the KL-term of VCL. If beta taken to the limit towards zero, the authors show that the beta-elbo recovers the online EWC learning criterion, which draws an interesting link between VCL and EWC.\nThe authors also discuss reweighting terms to introduce a parameter lambda as in EWC, which they incorporate via a lambda-kl divergence term.\nFinally, furnished with this learning objective that interpolates between VCL and EWC, the authors propose to combine the learning objective with the architectural choice of Film layers, which they show facilitate overcoming the pruning behavior that their method inherits from VCL by offering ways to prune nodes without injecting noise into the network.\n\nExperiments are broad on multiple interesting datasets and quite clearly show that their proposed combined model performs best.\n\nPositives:\nThe paper draws an interesting unification between EWC and VCL, and in fact also other related works, as subtle modifications in a regularizer. This by itself is an interesting contribution. The fact that the authors study the interplay of their learning arlgorithm with architectural biases, i.e. overcoming early pruning via film layers, is also a valuable idea that I find not just interesting in itself, but also stylistically valuable as an approach to studying  deep learning. While the Film layers per se also appear somewhat ad hoc, their empirical benefits -particuarly when paired with the lambda-elbo, are impressive and well put together.\n\nCriticisms:\nWhile I really enjoy the derivation of the beta-elbo in the zero limit, I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified. It feels as if it is reverse engineered to match the desired criterion from EWC. I think the authors should dig deeper here for better justifications for such choices, as they did a good job having a mathematically interesting framework to derive earlier.\n\nAdditionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well? This is empirically confusing, it would be great to get some more help to understand the relative merits of each components here and clarify more how these pieces fit together empirically. I do enjoy the appendix discussing this qualitatively, but I would like to understand it quantitatively better, as theoretically film layers plus VCL (without this paper's innovations) should also benefit similarly.\n\nOne additional criticism is that the title is somewhat misleading, as it does not generalize VCL to broader settings, but rather collapses it towards the limit beta towards zero. The title raised hopes for a richer variational treatment rather than a unification to EWC and an architecture change. The authors might want to consider tweaking the title to sth that is closer to the paper's actual contributions.\n\n\nOverall:\nThis paper takes an interesting approach towards adding to the EWC and VCL literature by unifying them and offering an architectural fix for a key problem in these scenarios. While the contributions are mixed and not consistently derived from clear modeling assumptions, their interplay is well studied and highly relevant to the understanding and improvement of practical continual learning. I also want to again applaud the authors for studying and explaining the interplay of pruning and film layers, I enjoyed reading the supplementary information on this. I wish more papers that discover methods that perform well empirically would study the interplays of algorithm and architecture similarly to expose interesting effects.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This work considers online variational Bayesian approaches to continual learning. The authors propose a beta-ELBO objective which they claim interpolates between Gaussian variational inference (beta = 1) and Laplace’s approximation (beta = 0).\nFurthermore, the authors propose task-specific, non-probabilistic (point estimation) FiLM layers that apply an element-wise transformation to the activations.\n\nTheory / Contribution:\nThe two contributions seem quite orthogonal to each other and each of them is rather minor in novelty. \nIt is obvious that using beta=0 leads to MAP estimates from which Laplace’s approximation can be computed. However, I am quite confused what exactly the authors do here and there could be a major mistake:\nFrom the paper, I am not sure if the authors a) compute Laplace’s approximation in the end, at the resulting mean of q, for any beta value? As far as I understand, the authors instead b) only optimise the variance through the beta-ELBO. \nHowever, in this case, the resulting approximation would *not* identical to Laplace’s approximation!\nI need clarification what the authors are doing here.\nConsider the case of beta=0, the covariance will be the dirac distribution as the authors note in Sec. 2.2 or the supplementary material. The authors then go on and write the optimal covariance matrix for which the derivative of the beta-ELBO is zero.\nYou have first postulated that the covariance is zero, in order to be able to pull out the expectation, and then you again allow for a non-zero beta-elbo-minimizing covariance. This would be a contraction. This makes me guess you do compute Laplace’s approximation instead. But then it is not discussed how you deal with beta>0. \n\n\nRelated work:\nThe related work section is rather short mentioning only very few related approaches. More effort is required here.\n\n\nExperiments:\nThe experimental evaluation is thorough and seems promising. Although I am wondering why e.g. Fig. 2 does not include VCL and EWC. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong? For Fig. a), the largest beta=10 seems to be a good approximation and also the most local. In case of Fig. B) and C) it is unclear / subjective (from visually inspecting the likelihood function) which is the best approximation. In A), beta=0.1 is the least local approximation, in B) beta=10 and in C) beta=1. I cannot follow the intuition provided here.\n\n\nSummary:\nI am sceptical about the correctness regarding the equivalence between VI and Laplace’s approximation; the exact approach proposed in the paper is unclear and may be based on a contradiction. In case I have a misunderstanding here, I hope the authors will point this out and update the manuscript. \n\n\nUpdate after Rebuttal:\nThe authors provided clarifications and improved the manuscript. \nIn particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. \nI am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. \nBased on this, I changed my evaluation and now suggest acceptance. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "GVCL",
            "review": "This paper proposes Generalized Variational Continual Learning (GVCL). It is shown that Online EWC and VCL are special cases of GVCL, along with other theoretical contributions. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL. GVCL and GVCL-F are applied to a number of continual learning tasks and demonstrate competitive performance. \n\nAlthough GVCL and GVCL-F do not outperform baselines, particularly in hard settings (split-mnist and mixed vision), GVCL is an original and excellent contribution. The paper is clear and well-written, the proposed algorithm is theoretically motivated and analysed, experiments are comprehensive, demonstrating the empirical performance of GVCL. \n\nI have the following comments:\n- It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n- Why is GVFL significantly worse than baselines for split-mnist (Figure 2c)?\n- Why is split-mnist omitted from Figure 3?\n- The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nMinor:\n- \"the node is *effective* shut off\" -> effectively",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting perspective but lack of preciseness and kind of unclear.",
            "review": "This paper proposed a generalized variational continual learning (GVCL) framework using the \\beta - ELBO, and then combined with FiLM layers. The idea is interesting but there is a lack of preciseness. The pros and cons are as follows. \n\nPros: \n1. The proposed GVCL proposed a different and interesting perspective on the online EWC, viewed as a special case of \\beta \\to 0;\n2. FiLM layers are introduced to combine with GVCL, which lead to significant improvement in the performance;\n3. Various experiments are performed, showing some level of advantages. \n\nCons:\n1. The new perspective that online EWC could be viewed as a special case of the GVCL framework is lacking preciseness. First of all, as described in Sec. 2.3, the result of the \\beta-ELBO, even with \\beta \\to 0, does not lead to the key hyper parameter \\lambda in online EWC. To compensate this, the authors introduce a modified KL divergence to make them similar. However, it is not justified, from a unified Bayesian or some other theoretical perspective , why the previous \\beta-ELBO needs to be modified. It is kind of wired to start from the Bayesian  framework and then go back to the non-Bayesian perspective to design a Bayesian algorithm to improve the performance, and then claim that the previous non-Bayesian algorithm is a special case of the unified Bayesian framework. Moreover, as described in Sec. 2.3, the  resultant GVCL when \\beta \\to 0 is actually different from the previous online EWC algorithm. As a result, strictly speaking, it is not approperiate to claim that the online EWC could be recovered as a limiting case. \n\n2. If it is true that the proposed GVCL is a generalization of VCL and Online EWC, which allows interpolation between the two, then it is expected and reasonable that the GVCL alone (without additional FiLM layers) should perform at least the same as VCL and online EWC. Otherwise, the statement is not true and there is no advantage of the proposed GVCL framework . However, as shown in experimental results, e.g., Table 1, GVCL alone performs worse than Online EWC in large datasets, which is really wired. The authors also acknowledged this point and claimed that this is due to the difficulty in optimizing GVCL with small \\beta. It would be better to make such statement more precise because this is really important point for this paper. Otherwise, it implies that the so-called interpolation between VCL and online EWC has no additional advantage. \n\n3. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers, rather than the GVCL framework itself.  To make this more clear and for a more fair comparison, it is highly suggested to compare other methods (online EWC, VCL, HAT, etc) with FiLM layers. Otherwise, the current improvement of the performance is unclear. In addition, the improvement of GVCL-F over the baseline is not consistent. \n\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}