{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper considers a new and practical setting of meta-learning for out-of-domain task adaptation where a pretrained model exists but the original meta-training data is not available. The authors incorporate several ideas including deep ensembles, adversarial training and uncertainty-based step sizes, and achieve competitive performance under this particular setting.\n\nThe combination of various methods appears complicated, but the authors provide detailed ablation study to show the effectiveness of each component empirically. During rebuttal and discussion, they addressed many of the concerns from the reviewers. As pointed out by a reviewer, their proposed method would have a value in the domain adaptation area beyond meta-learning.\n\nThe remaining concern is on the somewhat ad-hoc combination of multiple methods and lack of a clear single solution for addressing the OOD few-shot learning problem. Nonetheless, the proposed methods show a convincing empirical improvement on the vanilla MAML baseline in the experiments."
    },
    "Reviews": [
        {
            "title": "submission 831 review",
            "review": "The paper proposes to reutilize pretrained MAML checkpoints for out-of-domain few-shot learning, combining with uncertainty-based adversarial training and deep ensembles.\n\nPros:\n\n1. The idea of combining meta-learning, uncertainty learning and adversarial training is well-structured. In particular, the related work part provides a clear introduction of background work.\n\n2. It is quite novel to leverage adversarial learning as data augmentation for meta-testing in MAML.\n3. The paper provides extensive and convincing experiment results over evaluating the proposed model’s robustness to the choice of base stepsizes.\n\nCons:\n\n1. It would be better if an optimization equation is provided, especially if there are generated adversarial examples.\n\n2. For the ablation study, the authors mention that the best absolute performance (Top-1) is always obtained through some use of adversarial training. Actually, it would be more convincing if they can discuss more choices of \\lambda_{AT} and \\lambda_{AUG} and \\lambda_{a} to present the sensitivity analysis of the hyper-parameters.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Blind Review #4",
            "review": "[Summary]\nMAML has two stages: (1) In the meta-training stage, given various tasks (but each say only has a few labeled data), we want to arrive at a representation where it can quickly adapt to any test task later. Let us denote this as $\\theta_0$ (2) In the meta-testing stage, we go from $\\theta_0$, given the few-shot test data, via an SGD process (typical scenario), to get a final model that has good performance on the test data of the test task.\n\nIn MAML, we assume that tasks are (i.i.d.) sampled from a distribution of tasks P(T). This paper considers a setting (as I understand) where the test task is out-of-distribution of P(T). In this setting, the authors argued further that one cannot run the meta training, so the focus of the paper is to improve stage (2) (gradient update process at the test time).\n\nThe proposed method contains two main parts: (A) We use an ensemble method to help understand uncertainty of the network, and finally use different step sizes for different layers for updating the network parameters at test adaptation phase, (B) adversarial training as data augmentation to help the test-time adaptation process. Experiments show that this method can bring some performance improvements\n\n[Assessment]\n1. I am not sure I entirely accept the authors' statement about testing tasks being out-of-distribution compared to the training tasks. In the original MAML setting, the test tasks can be entirely different from the training tasks (for example, we pick test tasks from different classes). Further, it has always been the case in MAML, that one can store a checkpoint $\\theta_0$ trained from some tasks, and  when a new task (maybe entirely different) comes, we start from $\\theta_0$ to adapt. This renders the motivations of this work less meaningful. And indeed, if we look at the performance numbers, the improvement is somewhat marginal.\n\n2. More specifically, at the very least, I think the precision of the paper can be improved. If MAML's purpose is already to be able to adapt to new, unseen, tasks, what is the precise meaning of out-of-distribution tasks here? (for example, one interpretation of this, stated in a formal way, is what I described above as saying that the new task is out-of-distribution of P(T))\n\n3. I do like the use of ensemble and also adversarial examples, and intuitively they can improve the test-time adaptation. However, the arguments for why they are useful are quite ad-hoc (to a degree that I feel superficial, no offense). Can we better justify their usage?\n\n4. What is the computational overhead in order to gain the additional accuracy? The use of ensemble together with adversarial training seems to bring a lot of computation cost, which seems not a good deal in view of the accuracy gain.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good first attempt to solve a new practical problem. However, why can’t the problem be solved by existing domain transfer or domain adaptation methods?",
            "review": "The paper tries to solve a novel practical problem: adapting pretrained meta-learning checkpoints to out-of-domain test tasks. It is an important problem in industry since there are some industrial tasks, especially meta-learning tasks, that have very limited data. Therefore, the meta-training process which requires a relatively large data set is not practical. \n\nTowards this end, the paper proposes to run ensembling fine-tuning with the support set of the test task, and use the weight variances from this process to guide the training.  The paper also proposes to add task adversarial examples to the training set to help the meta fine-tuning process. Experimental results seem promising.\n\nOne main concern I have in mind is, existing domain adaptation or domain transfer methods are trying to solve very similar problems, i.e., the model is pre-trained with large data set from other domains and you need to adapt it to the new domain. What are the differences between the current problem and domain transfer/adaptation problems? Why can’t we apply existing domain adaptation methods to solve the current problem?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice paper and ideas",
            "review": "### Summary \nThis paper considers the problem of adapting a pre-trained model for few-shot learning in case there is a shift of distribution from the meta-training set. If the new tasks significantly differ from the meta-training distribution the model might need to be retrained from scratch but this is not always possible, so the authors propose to \"repurpose\" the model under the assumption that the support set can be used to re-calibrate the pre-trained model to the new shifted distribution.\nFollowing two intuitions/hypotheses: 1) if the model uncertainty of specific parameters is high, then the step size should be small and 2) high uncertainty on input gradients require more adversarial training to improve robustness.\nIn practice, the uncertainty of the parameters is computed by using deep ensembles with perturbed MAML checkpoints rather than random initialization. High variance components are moved with lower step sizes. Moreover, If slightly perturbed models from the deep ensemble disagree on parts of the input gradient, then it means that they might be more prone to adversarial attacks, hence they need to be robustified with stronger adversarial training.\n\n### Questions\n- Q0a: maybe I'm missing the point, but am I wrong or the procedure that you propose can be also applied to any other supervised learning model, not necessarily few-shot? Let's consider you are not only observing new classes, but also new domains. Does the method apply to that situation? \n- Q0b: also the use of deep ensemble and the proposed UFGSM maybe can be useful in the supervised learning scenario to improve robustness. I would suggest to try some experiments in that direction.\n- Q1: from plots in figure 1 it seems that batchnorm parameters are the most uncertain. This is expected since BN capture the statistics of the activations from the meta-distribution and it is the most sensitive to the domain shift. Are you using batch-norm in a transductive setting, i.e. you don't use the running stats, but you always use the batch statistics also at test time? \n    - If no, and you use running stats, then a simple baseline would be to use test time statistics, so I would add this comparison to your experiments.\n    - If yes, I would also try not to update the BN scale and shift parameters in the inner loop since they seem very sensitive. Another alternative would be to have per-step running stats and per-step bn weights as proposed in Antoniou 2019.\n- Q2: the improvement wrt to the SGD baseline can be more appreciated with 5 shots, rather than only 1 sample. I would have expected the opposite because of the limited number of samples. How do you interpret this? \n- Q3: what happens if you try PGD instead of simply using FGSM? \n\n### Considerations\n- I think the paper is well written and motivated. The idea of repurposing a FSL model without retraining from scratch is timely and interesting. The experimental campaign is carefully performed as well as the ablation. I recommend acceptance, but I want to first to hear the author's response to my doubts.\n\n### Bib\nAntoniou et al 2019 How to train your maml\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}