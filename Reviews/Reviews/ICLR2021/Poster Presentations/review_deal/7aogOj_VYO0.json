{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces a method for differentially private deep learning, which the authors term Gradient Embedding Perturbation. This is similar to several (roughly) concurrent works, which project gradients to a subspace based on some auxiliary public data. However, a crucial difference involves the use of the residual gradients, which allows the method to achieve the first significant accuracy gains using subspace projection. The reviewers believe this method will be important for the practice of DP deep learning."
    },
    "Reviews": [
        {
            "title": "Review for \"Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning\"",
            "review": "Summary: The paper focuses on training deep learning models under differential privacy. The main contribution is a technique based on  projecting the gradient into a non-sensitive anchor subspace and then perturbing the projected gradient (and the residual).\n\nComments:\n\nIt is well-known that a straightforward way of perturbing the gradients for achieving differential privacy requires that the noise magnitude has to scale with the dimensionality of the gradients.\n\nNow to reduce the dependence on dimension the authors use a simple idea: reduce the dimension of the gradients using an embedding and then add noise into the low-dimensional gradients. They assume the presence of non-sensitive data for this purpose. The rough outline is as follows: 1) use the non-sensitive data to compute a subspace using a principal components approach, 2) project the private gradient into this subspace to obtain a low-dimensional gradient embedding and the residual, 3) perturb the gradient embedding and residual gradient separately for DP purposes, d) use the perturbed gradients in a DP-GD iteration.\n\nStrengths:\n1)\tThe problem of reducing noise in differentially private ML is an important practical problem.\n2)\tThe experiments shows benefit of this approach over Abadi et al.\nI think the most interesting part of the paper is the experimental evaluation. There is no strong theoretical component in the paper. \n\nHere are some questions/suggestions:\n1) Can you provide convergence bounds achieved with using gradients produced by GEP (Algorithm 1) in standard convex/nonconvex optimization settings.\n2)  Are there any good bounds on S_1 and S_2?\n3) I did not understand the role of Lemma 3.1? Donâ€™t you require the anchor subspace to be non-sensitive (not depend on sensitive gradients)?\n4) How crucial is the choice of picking the right auxiliary dataset. Providing experimental results with different choices of the auxiliary dataset might help.\n5) Why does not the random projection using JL work? The authors allude to an empirical observation in the text which I did not find.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Paper proposes gradient embedding as a potential solution to boost the privacy-utility trade-off in differentially private machine learning via gradient perturbation. Main idea is to project the gradients into a lower-dimensional subspace before adding noise, so that the curse of dimensionality in DPSGD can be somewhat dealt with. \n\nI like the paper and the motivation as high-dimensionality can be considered as a major driver behind the privacy-utility trade-off in DPSGD. Also, the requirement for a public data is much more acceptable in the provided scenario, where it is only required to get anchor gradients. Overall empirical evaluation is adequate, however, it would have been nice to see PATE's numbers on all comparisons, especially given that the implementation is publicly available. I would have also liked the source-code as these type of claims are best verified empirically, especially when we are using gradient projection that can can lead to projection errors and the page limit in the submission is nearly not enough for a thorough evaluation.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Excellent paper with nice insight on using gradient embeddings to improve private learning",
            "review": "This paper proposes a simple but highly effective approach to improve differentially private deep learning, by projecting gradients to a small subspace spanned by public data.\nThis idea has been proposed before, but this paper seems to have the unique (and important) insight to release noisy residual gradients, so as to ensure that the final gradient estimation remains unbiased.\nExperiments on MNIST, SVHN and CIFAR-10 show that the proposed approach significantly improves the privacy-utility tradeoff of DP-SGD.\n\nThis paper is excellent. It is well motivated, as the assumption of access to a small amount of public data seems very reasonable in practice. The proposed algorithm is quite natural. The key insight is to release private noisy estimates of both the low-dimensional gradient embedding *and* the residual (low-norm) gradient. This is easily shown to produce an unbiased estimate of the gradient, which is crucial for performance.\nThe experiments convincingly show that this approach leads to strong performance improvements, and I predict that this simple technique will be applied in many subsequent works.\n\nI have a number of (mostly minor) comments and suggestions for improvement: \n\n- In Algorithm 1, and in the main text, it would be good to clarify the clipping applied to both the embedded and residual gradients. It is also worth noting that in practice, the estimated gradient is probably *not* quite unbiased because of this clipping (the same holds for standard DP-SGD of course).\n\n- You mention that you use \"privacy amplification by subsampling\" to compute the DP guarantees. It would be good to clarify this, since some of these theorems apply specifically to a \"subsample + gaussian mechanism\" approach, which your algorithm doesn't quite match (GEP applies two independent Gaussian mechanisms to functions of a subsampled gradient).\nThis seems like it could be resolved as follows:\n- you have embedded gradients Wi clipped to norm C1\n- you have residual gradients Ri clipped to norm C2\n- You can \"stack\" these as gradients gi = [Wi/C1 ri/C2] of norm S=sqrt(2), and then apply a single Gaussian mechanism with sensitivity S to this stacked gradient. After that, you can rescale the two components by C1 and C2 respectively. This seems identical to your proposed approach of applying two Gaussian mechanisms with sensitivities scaled by a factor \\sqrt(2).\n \n\n- Only in Section 4 do you mention that you split the gradient into different groups to fit them in memory. It would be nice to see this discussed earlier on in the paper. E.g., the abstract claims that GEP has \"low computational cost\", which then seems strange when Section 3.1 says that the cost is O(m*k*p), which would be huge even for moderate DNNs. Section 3.2. mentions using a ResNet-20 with k=2000 basis vectors, which wouldn't fit in any GPU memory if it weren't for the grouping. While reading these sections, I was confused about how this could possibly be implemented at a low cost, as described.\nFollowing on this, it would be nice to have some results on the additional computational and memory costs of GEP in the evaluation. \n\n- Papernot et al. show that the choice of architecture can have a large influence on DP-SGD (https://arxiv.org/abs/2007.14191). The results they obtain are sometimes better than your GP baseline. It would be interesting to see if you can get even better results with GEP with such architectures.\n\n- This is somewhat orthogonal to your approach, but do you have a sense of whether 2000 public ImageNet samples could be leveraged in other ways than as gradient basis vectors? E.g., on CIFAR-10, it is well-known that unsupervised dictionaries of data patches can achieve >80% accuracy. So one could also consider using the public data to learn a patch dictionary, and then simply train a small private classifier on top of these features. Maybe this would perform better than GEP?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper studied how to use the information that the gradient lies in a low dimension space to design more accurate differentially private SGD algorithm. Specifically, the authors proposed a new method which is called Gradient Embedding perturbation method. And they showed that compared with the previous DP-SDG method, their method has higher accuracy. \nActually, I tend to weakly reject the paper due to the following reasons. \n1. As I know and also the authors mentioned in the related work part, there are also two other works study how to use the low dimensional gradient to get higher utilities for the Empirical Risk Minimization problem. However, I can see that in the other two works, they all have theoretical results for the utility, while in this paper there is just a variance analysis, so I wish to see the authors use Theorem 3.2 to get the excess error for their method when the loss functions are convex so that we can see the difference between the other two methods clearly. \n2. In the experimental part, I think it will be better to have a comparison with the above two papers to see the superiority of the method in this paper. just comparing with the base-line is not enough.  Moreover, the authors set $\\delta=10^{-5}$ or $10^{-6}$, I think it will be better to set $\\delta=\\frac{1}{n}$. And I want to see more comments about the number of public data to get the anchor subspace.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}