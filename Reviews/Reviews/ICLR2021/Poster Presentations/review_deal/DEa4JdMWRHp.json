{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The proposed approach is interesting and is differentiated enough from the recent body of work on Neural Network Granger causal modeling as it offers a mechanism for detecting signs of causality. \n\nThe authors have satisfactorily addressed the points raised in the reviews. In particular relationship with prior work and novelty of the contributions are now clearly articulated. The added discussion on the superiority of TCDF on simulated fMRI experiments is insightful. Though prediction error is only a proxy for the task at hand, the readers will appreciate the added evaluation. \n\nThe proposed approach to stability evaluation leveraging the time-reversal trick is novel and particularly pertinent, and could motivate some interesting follow-up work on this topic. It is also important that the authors have characterized the computational advantage of the approach. "
    },
    "Reviews": [
        {
            "title": "Marginally above acceptance threshold",
            "review": "Summary: \nThe paper proposes an interpretable method to detect Granger causality (GC) under nonlinear dynamics which can detect signs of Granger-causal effects (positive and negative) and inspect their variability over time. The novelty of this paper is 1 and 2 in the Pros below, and the methods utilized the existing stable frameworks such as a heuristic stability-based procedure that relies on time-reversed Granger causality (TRGC) (Winkler et al., 2016). \n\nReasons for score: \nAlthough the motivation and the experiments were clear and the code was reproducible, the presentations such as in the results were sometimes unclear (below).\nI think this would be a valuable paper in this community, the presentation and investigation may be required to obtain a higher rating. \n\nPros:\n1. The method called generalized vector autoregression (GVAR) generalizes the VAR which is the basis of linear GC methods. The method is interpretable based on a self-explaining neural network (SENN) framework and allows exploring signs of Granger-causal effects and their variability through time.\n2. The method utilizes a framework for inferring nonlinear multivariate GC that relies on a GVAR model with sparsity-inducing and time-smoothing penalties.\n3. The method outperformed the baseline methods on a range of synthetic Lorenz 96 and Lotka–Volterra datasets in inferring the ground truth GC structure and effect signs.\n\nCons:\n1. The detailed explanation and the difference from the previous TRGC work were unclear (below). \n2. There was little discussion about the reason why TCDF outperformed the proposed methods on the simulated fMRI dataset. \n3. There were no results in cost function (e.g., prediction error) even in the appendix. This detecting Granger causality would be unsupervised learning, thus the learning results based on the cost function are unknown. The information might help us understand the reason for 2. \n\nOther comments:\n\nIntroduction: \n“varying causal structures (Lowe et al., 2020)”: Did this paper focus on the varying causal structures?\n\nMethod:\nThe alpha in Alg.1 (threshold) is confusing with an elastic net regularization parameter.  \n\nThere is no explanation of Q of Alg. 1 in the main text. Is this a variable threshold in the analogous way of the ROC curve? How was the Q determined?\nIs the Alg. 1 the proposed method? The title “Stability-based thresholding” may be one of the components of the proposed method.\n\nExperiments:\n\nThe number of sequences to evaluate the methods in each dataset was unknown. In my understanding, the proposed methods learn each model for each sequence like the methods of Tank et al. (2018) and Khanna & Tan (2020). In the shared code, for example, Lorenz 96 used 5 sequences (in the code, called datasets). This may be critical information to evaluate the method.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Successful application of SENNs  to nonlinear dynamical systems infers GC relationships including sign in multi-variate time series  ",
            "review": "Interpretable ML approaches are very important to advance a wide variety of fields in healthcare and science. This work makes a significant contribution in this direction by successfully applying SENNs to multi-variate non-linear time series data. The method which the authors named GVAR is generic enough and can be applied to many different problems involving multi-variate time series data thereby making the paper quite impactful and significant. The authors look into GC relationships between variables including the sign of the relationship which they can successfully infer using their approach. The authors provide an array of favorable results across multiple simulated non-linear dynamical systems which corroborate their findings convincingly and also compare their results to a comprehensive set of related and SOTA approaches. There are no obvious inconsistencies or errors that I can see in the paper to the best of my knowledge. The paper is well written and conveys the information clearly enough. An exciting next step is to apply this approach to real data, could be a great addition to this paper but not necessary for publication.  ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Paper is  well presented but does not offer a fundamentally unique point of view as it combines existing ideas.",
            "review": "This paper primarily deals with learning Granger-causal relationships in multivariate time series in the nonlinear dynamics setting. The core method uses vector autoregressive modeling with sparsity inducing regularizers (elastic net and smoothness based fused lasso) along with the recently proposed with self-explaining neural networks (for interpretability). The authors also augment the framework by learning Granger-causal structures that are stable on original and time-reversed data. Exhaustive empirical analysis is done with recent GC baselines.  Some of my concerns with the paper are the following\n\na) Paper does seem a bit incremental in terms of using elastic-net and fused lasso based smoothness regularization within VAR framework. The idea of bringing SENN within this is useful but not enough to claim significant novelty.\n\nb) Paper does not have scope of developing any useful theory ? Authors cite stability selection related work which has some decent guarantees on false discovery rate control. Can such theory be developed for the method proposed here ?\n\nc) On the positive side the empirical analysis and coverage of related work is very exhaustive. Of particular significance is the result presented in Table 3. However, given the double computation and higher number of parameters involved in the method, I was expecting some discussion on time complexity and how to improve scalability of the method in practice.",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The authors present GVAR which is a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of SENNs..",
            "review": "# Introduction\n\nThe introduction is solid. My comment here is that there ought to be more discussion on sign detection. You leave even the definition until section 2.1, where a lay reader may struggle to understand what you are talking about if they are merely skimming your paper (as many do). Hence, for readability and for people to actually use this work, I suggest you focus more on sign detection early on and why it is important. You say that it is \"important to differentiate between the two types of interactions\" - well why is that?\n\n# Background and related work\n\n## 2.1\n\nStay clear of using $\\mathbb{V}$ and $\\mathbb{E}$ for vertices and edges. These type-settings are usually reserved for expectations and variances respectively.\n\n# Method\n\nPlease give the form of SENN, in the same form as you've given your GVAR model, i.e. by way equation 4. They are difficult to compare otherwise. Right now what you claim to be an extension of SENN is not easily perceptible by reading section 2.2.3 and looking at equation 4.\n\nThe noise/innovation term $\\boldsymbol{\\epsilon}_t \\in \\mathcal{\\mathbf{0},\\mathbf{1}}$? As standard? Please specify.\n\nAt the bottom of this section you say \"signs of Granger causal effects and their variability in time can be assessed as well by interpreting matrices\" -- it would be helpful if you could provide a toy-example, of matrices, at this point to show us how precisely we are to interpret Psi to get to GC.\n\n## 3.1 \n\nIt is not clear _why_ you \"aggregate the obtained generalised coefficients into matrix $\\mathbf{S}\"?\n\n# Algorithm 1\n\nWhat is $\\boldsymbol \\alpha$?\n\nOn line 7 you introduce the balance accuracy score. What is that? Why is it used in GVAR? Why did you choose that? What are the alternatives? My impression was that BA is the average of recall obtained on each class, in a classification problem. But here you are using it in a much more novel way. It would be very interesting to understand your reasoning and logic here.\n\nOtherwise, well-written algorithm. Easy to follow and understand. My main pointer, as noted, is that you introduce far too many new things in the algorithm rather in the body itself. The algorithm is meant to summarise your method exposition of your method in the body, not introduce new items such as $\\boldsymbol \\alpha$ and BA.\n\n# Experiments\n\nWhat you are doing, from a purely ML point of view, is structure learning. This is very interesting of course, but I note that you have mainly compared to methods specifically focused on causality. It would be helpful/interesting if you also included more vanilla ML methods used for structure learning. \n\nSpelling in sec 4.1 \"releveant\" should be 'relevant'.\n\nIf you can move some of your adjacency matrices into the main paper, from appendix F, that would help the paper a lot. Especially for the Lorenz-96 model. They visually demonstrate what you are trying to achieve, you barely need any legend to go with it. Right, for space issues presumably, they are wasted in the appendices. Please try to move at least one into the body as they a good opticals.\n\nWhy is the inference problem more difficult? F=8 is known to cause deterministic chaos but what about F=10 and 40? Would it not make more sense to evaluate a small ablation study on F from say 0 to 50 in increments of 10? It is not clear to me how computationally heavy GVAR is, hence this may be asking a lot. But since you have not added any such metrics, it is hard to know (computational complexity that is).\n\nYour experiments shadow those of Khanna and Tan pretty well, yet you did not deploy GVAR on the DREAM-3IN SILICO NETWORK INFERENCE CHALLENGE? Why is that? That is the experiment where their model performs the worst, and since they do quite well (at least in Lorenz 96 as shown your paper) I would have thought that a good comparison too. Naturally, you only have so much space, but a comment would be helpful.\n\nPlease correct me if I am wrong here, but not one of your experiments contains an identfiable noise model, except for the model in appendix I? Why is the innovation N(0,0.16) that seems oddly specific? Why not standard white noise N(0,1)? And under this regime your models performs as well as VAR and CMLP by the looks of it.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}