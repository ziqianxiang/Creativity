{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a learning based approach for solving combinatorial optimization problems such as routine using continuous optimizers. The key idea is to learn a continuous latent space via conditional VAE to represent solutions and perform search in this latent space for new problems at the test-time. The approach is novel and experiments showed good results including ablation analysis. \n\nReviewer comments are adequately addressed during the response phase and I find the changes satisfactory. Overall, this is a good paper and I recommend accepting it.\n\nOne last comment: It would be a great addition if the paper could add discussion about the applicability of this approach to arbitrary combinatorial optimization problems and what design choices are critical to come up with an effective instantiation."
    },
    "Reviews": [
        {
            "title": "Borderline paper",
            "review": "The authors propose an algorithm for routing problems by (1) using conditional variational autoencoder to learn a latent space for solutions, and (2) performing black-box continuous optimization such as evolutionary algorithms in the space. The proposed method outperforms existing neural-based methods on a benchmark of the traveling salesmen, and the capacitated vehicle routing problem.\n\nThe presentation and writing of the paper are excellent. I really like the intro and related work section where the authors have done a comprehensive survey and position their work among existing ones.\n\nAlthough the method is very neat and general, the idea of learning a latent space for solutions to combinatorial search problems is not new. It has been investigated in both continuous control\\cite{ichter2018learning} and drug design\\cite{gomez2018automatic}. In particular, \\cite{ichter2018learning} also uses a conditional variational autoencoder to address the issue brought by different problem instances. The authors should discuss more on the challenges of the current problem, why previous approaches cannot apply here, and focus on how to deal with the challenges, otherwise, the contribution will be weak.\n\nAlso in the experiment section, it would help the readers understand the approach better by including ablation studies such as applying evolutionary and random search methods directly in the solution space. There is no need to design sophisticated mutation strategies that involve domain knowledge. Simple heuristics will suffice here.\n\nIn general, I think this is a borderline paper mostly due to its weak contributions. I will consider raising the score if the authors address the issues well.\n\nAdditional question: what is the problem distribution? how do you generate them?\n\n\n\n@inproceedings{ichter2018learning,\n  title={Learning sampling distributions for robot motion planning},\n  author={Ichter, Brian and Harrison, James and Pavone, Marco},\n  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={7087--7094},\n  year={2018},\n  organization={IEEE}\n}\n\n\n@article{gomez2018automatic,\n  title={Automatic chemical design using a data-driven continuous representation of molecules},\n  author={G{\\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\\'a}ndez-Lobato, Jos{\\'e} Miguel and S{\\'a}nchez-Lengeling, Benjam{\\'\\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\\'a}n},\n  journal={ACS central science},\n  volume={4},\n  number={2},\n  pages={268--276},\n  year={2018},\n  publisher={ACS Publications}\n}\n\n\nUpdate after rebuttal:\nThanks for the response! I think it resolves my concerns on novelty and evaluation. Hence I raise my score to 6.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Solid approach, empirical evaluations can be strengthened ",
            "review": "This paper proposes a method to learn a continuous latent space via CVAE to represent solutions to routing problems. Combined with differentiable evolution search algorithms, one can search in the learned latent space for solutions to new problem instances at test time. The proposed method is evaluated on two classes of routing problems: TSP and CVRP. Results show better performance in terms of objective values and runtime. They are also competitive with established expert-designed algorithms such as LKH3.\n\nPositives:\n1. The proposed method is novel. The specific algorithm to realize the idea of learning continuous latent spaces is sensible and easy to understand, thanks to clear writing.\n\n2. A continuous latent space enables one to use existing tools in continuous optimization, as this paper demonstrates via two examples: differentiable evolution and random search.\n\n3. The evaluations on two practically relevant routing problems demonstrate the advantage of the proposed method. The experimental section includes useful insight into hyperparameter selection and interpretations of learned latent spaces.\n\nNegatives:\n1. Comparison with Joshi et al. (2019) should be included for TSP as well, especially their beam search variants. Given a major benefit of the proposed method is to enable search, comparing with beam search can better evaluate the practical advantage of the proposed method. The sampling procedure used in AM by Wool et al. (2019) improves solution quality but is not a direct search procedure.\n\n2. The problem size is relatively small with the largest problems having 100 nodes. I expect the proposed method, with the resulting continuous latent space, achieves larger improvements for larger problem instances. Can the authors consider larger scale experiments?\n\nQuestion:\n1. For symmetric breaking, does the symmetrical solution to each input problem change between epochs?\n\n2. Is it possible to employ gradient-based optimization algorithms in the learned latent space?\n\n=====================\nPost-rebuttal comments:\n\nThank the authors for the detailed responses and revised submission. My concerns have been adequately addressed and I raised my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great work!",
            "review": "Overall great work and highly readable, as reviewer I am leaning to accept\n\n##########################################################################\n\nSummary:\n\n \nThe paper presents a way of solving routing problems (which are discrete and highly combinatorial)\nby optimising the input to an instance-specific mapping from a continuous vector space to the space of solutions for that particular instance (desiring high objective value solutions).\nThe mapping is obtained from masked autoregressive sampling with a decoder trained as part of a conditional VAE.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for accepting. The idea presented is neat and seems to work well in the instance size range presented . I have one major concern about generalisation in problem-instance sizes (expanded below in Cons 1). The paper is readable. The experiments could be improved and I make suggestions for that. Hopefully the authors can address my concern in the rebuttal period. \n\n \n##########################################################################Pros: \n\nPros:\n\n1. The paper leverages a re-parametrization of the search space to make search better and in this case to even allow unconstrained continuous optimization methods for the problem of routing in a way that successfully generalizes to unseen instances (the best solutions to routing problems are problem-specific). \n\n \n2. The paper is clear and readable. \n\n \n##########################################################################\n\nCons: \n\n\nI have not taken into account points 2-5 for making review score decisions, but I believe they should be addressed in a camera-ready worthy version of the paper\n\n\n1. It isn't clear to me if the model is trained (and tested) on instances of varying sizes or fixed size. Generalising/extrapolating to unseen problem-instance sizes is really important.\n  + If the approach is size-specific then any unseen problem-instance size at test time will require generating new data and training, which would limit the impact of the approach.\n  + If the approach isn't size specific (from what I understood in the Appendix A it isn't) what is the training instance size distribution? From the section 4 wording I understood that the sizes are fixed, but I could have misunderstood.\n    - One possibility for showing size-generalisation/extrapolation could be training on one size (e.g. n=50) and testing on the other two sizes (e.g. n=20 and n=100). (Edit: the authors have done this)\n\n2\u0010. There is the implicit claim that learning helps, but there is no ablation/comparison to a randomly initialised model, I expect it to do really badly, but even in that case it would only help to show that it is the learning/training component (and not the architecture's inductive biases alone) that make the approach successful. Running CVAE-Opt-DE with a randomly initialised model and adding that to Table 1 would be great. (Edit: the authors have addressed this)\n3. The authors show their approach can be used by any unconstrained continuous optimization algorithm by evaluating one and a variation of it. There is the possibly implicit claim of \"any optimization algorithm can be used and still get great results\" and it might merit having optimization method different-enough from the other two. (Edit: the authors have explained this)\n4. The current decoding takes O(solution_size * problem_size) time, as the problem-instance is passed at each time-step, this will limit the scalability to large input sizes. (Edit: the authors have clarified this is fine in the subdomain they are targeting)\n5. It is not clear to me if Figure 5 shows one representative per equivalence class, or possibly more than one. (Edit: the authors have clarified)\n\n\n##########################################################################\n\nRandom thoughts\n\nBeing able to function-approximate the decoding plus measuring objective value, would allow gradient-based planning, and make the approach even better, Figure 5 suggest we might be able to do a linear/quadratic function of the latent search space points to the objective values (but even then variance could be too high). Maybe a cheaper alternative to know if this is worthwhile is plotting for each pair of solutions in Figure 5 the distance in x-axis and the cost-diff in y-axis (a per-solution view instead of being histogram aggregated) \n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting combination of ideas, but lacks motivation and novelty",
            "review": "$\\textbf{Summary.}$ This paper proposes a new approach to solve routing problems based on (1) training a variational autoencoder (VAE) to model the optimal solution of problems (i.e., supervised learning) and (2) applying continuous search methods for decoding at test time. While I think the idea of this paper is promising, it is limited in the sense that (1) the method is not particularly novel, (2) the VAE model relies on supervised learning for training, (3) the experiments are limited to small-scale settings, and (4) the proposed method does not show any promise over the \"traditional\" solvers, e.g., Concorde or LKH3. \n\n$\\textbf{Pros.}$ \n- I think the idea of transforming the combinatorial search problem into a continuous search problem is interesting. This direction has not been considered in the literature of using DNNs for combinatorial optimization.\n- The proposed framework can easily be generalized to combinatorial optimization problems other then routing problems, e.g., minimum vertex cover. \n\n$\\textbf{Cons.}$\n- The proposed framework is not particularly novel when compared to the existing works on de novo drug design using DNNs (which is also a combinatorial optimization). The paper mentions how routing problems are different from de novo drug design tasks since they require the DNN-solver to encode context of the given problem. However, this does not make the problem much more challenging and the paper simply uses conditional VAE to take care of the context encoding. \n- Importantly, the proposed framework use supervised learning for training the model and requires extracting optimal solutions from the training graphs using existing solvers. This severely limits applicability of the proposed framework. In comparison, (Kouter et al. 2019) trained the AM model using reinforcement learning to motivate DNNs for solving combinatorial problems where exact solvers do not exist. \n- For small scale instances (considered in the experiments), exact solvers can usually find solution better than the heuristics. I encourage the authors to provide more results on large-scale experiments, since this is where non-exact solvers are needed. This is especially significant since it is computationally intractable to extract optimal solutions from large-scale problems. How would the proposed method handle this case?\n- I think the proposed framework lacks motivation, since the proposed method does not show much promise over the traditional solvers such as Concorde or LKH3. This is especially significant since the proposed framework requires using traditional solvers to extract optimal solutions on the training graphs. In what situations would users be encouraged to use the proposed framework?\n- In the experiments, I think the authors should add a baseline proposed by (Lu et al. 2020) since they also considered using neural networks to solve the routing problems. While they use certain domain-specific components, I think the proposed framework also relies on domain knowledge (since they require exact solvers for training).\n- I suggest adding a reference to (Winter et al. 2019), who proposed to use swarm optimization on the latent space of VAE for molecular optimization.\n\n$\\textbf{References.}$\n\n(Winter et al. 2019) Efficient Multi-Objective Molecular Optimization in a Continuous Latent Space, Chemical science 2019\n\n(Lu et al. 2020) A Learning-based Iterative Method for Solving Vehicle Routing Problems, ICLR 2020\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}