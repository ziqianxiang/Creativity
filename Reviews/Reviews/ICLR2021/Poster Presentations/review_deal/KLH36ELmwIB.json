{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposed an interesting method to improve the robustness of DARTS and hence to alleviate the mode collapse. The idea consists of adding an auxiliary skip connection branch that complements the output of the cell function together with a depth analysis about the effect of the auxiliary branch. The proposed approach is validated on a few benchmarks showing the effectiveness of the proposed approach. All reviewers agreed that the idea is simple, efficient and interesting. author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score accepting the paper. Therefore, I recommend acceptance."
    },
    "Reviews": [
        {
            "title": "Addressing an important problem in neural architecture search",
            "review": "The paper focuses on improving the robustness of differentiable architecture search models. I am not an expert in the topic, so please see my review as an outsider's perspective. Although the problem is clearly formulated and motivated, I don't really understand the actual contribution of the paper. There is no intuitive explanation of the approach and why it should work. I hope the other reviewers are expert in the topic and can comment on this aspect, but as an outsider, I think the paper is poorly written (the English is clear, it's just the way they explain the solution and contribution) and the contributions are not clear.\n\nI tried to look at the prior work (mostly those mentioned in the paper and frequently cited) and based on this quick research, it seems that the approach is novel. However, again, I do believe that at least an expert view on this aspect is necessary. I am not sure if I know all the prior work.\n\nThe experimental results are encouraging. They show robustness across datasets. I think this is a strong paper from an empirical perspective. \n\nSince I do not fully understand the contributions and the authors didn't convince me on why this is a novel approach, I'd vote for marginally above acceptance threshold (mainly because of strong evaluation). ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "interesting idea to separate the two roles of skip connection to prevent model collapse",
            "review": "The paper reveals two roles of skip connection to prevent model collapse: stabilize the supernet training, and as a candidate operation to build the final network. Intuitively, the skip connection playing the first role should only be there during the training phase. Therefore, the authors propose to add an auxiliary skip connection to play that role. This auxiliary skip connection is decayed gradually during training. The paper provides an interesting theoretical analysis that this can help prevent the gradient vanishing problem. \nCompared to other approaches, the proposed one has the advantage that it does not rely on any heuristic indicator. Indeed, it is found that the existing indicator based on Hessian eigenvalue can discard good models.\nIn the experiments, this method - DART-, is compared with DART and several other approaches, and show that the proposed method can outperform the others, however, by small margins. \n\npros:\nThe assumption that skip connections play two different roles is very interesting and inspiring. It is indeed the case that such skip connections not only can stabilize the training, but are part of the model. The idea to separate the two roles is well motivated.\nThe paper provides some interesting theoretical analysis to show the potential impact of the auxiliary skip connection on the gradient vanishing problem.\nThe experiments are extensive, using several datasets and comparing several existing approaches. The experimental results provide some evidence that the idea works.\n\ncons:\nWhile the basic idea is well motivated, one could question about the specific architecture to add the auxiliary skip connection. The latter is intended to play the role of stabilizing the training. It is unclear why this specific architecture is appropriate for it. The paper does not provide a strong explanation for it.\nWhile DART- outperforms many of the baselines, it is only marginally better than some recent methods (namely P-DART). For example, in Table 2, DART- is equivalent to P-DART on CIFAR-10.  The experimental evident is moderately strong to show that DART- is better than the existing methods.\n\nRecommendation to the authors: It would be useful to justify the choice of the architecture of DART-, namely to explain why the auxiliary skip connection added can truly play the role of stabilizer. Would it be possible to use a different architecture as well?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "DARTS- review",
            "review": "This paper aims to improve the robustness of DARTS, and proposes to add an auxiliary skip connection branch to the “mixOp” in cells. The authors also analyze the effect of auxiliary branch on residual block from the view of gradient flow. Additionally, this paper refers to the theory of the work [1] and demonstrate that the auxiliary branch is able to reduce the dependence of the convergence of network weight on $\\beta_{skip}$ so as to disentangle the “two-fold role” of skip connection: “as an auxiliary connection to stabilize the supernet training, and as a candidate operation to build the final network”. Moreover, extensive experiments on multiple search spaces and datasets are conducted, showing the effectiveness of this method. They also illustrate the accuracy landscape w.r.t the architecture parameters to demonstrate that the auxiliary branch is able to smoothen the landscape, making DARTS- less sensitive to the perturbation than DARTS\n\nI have two questions: \n1). I notice that the authors adopt decay strategy on the weight of auxiliary branch ($\\beta$), and claim that their method is insensitive to the type of decay policy. I think the authors find an interesting phenomenon of skip connection, and I wonder how $\\beta$ affects the searching procedure? And can the effect of auxiliary branch holds or how long can the effect holds after $\\beta$ decays to 0. Specifically, suppose we search for 50 epochs with auxiliary branch, then we remove the auxiliary branch and search for another 50 epochs, will DARTS collapse?\n2). I agree with the authors’ point that skip connection “plays two-fold roles”. However, I wonder with the auxiliary skip connection suppress the architecture parameter of skip connection ($\\beta_{skip}$), so that no skip connections are chosen in the final model?\n\nIn general, I think this paper propose a simple but efficient method to alleviate the performance collapse of DARTS, the strengths and weaknesses are listed as follows:\n\n\nStrengths:\n1). The authors propose a simple but efficient indicator-free method to prevent skip connection from dominating the superNet. They also demonstrate the effectiveness of auxiliary branch from the view of gradient flow and the convergence of network weight. \n2). Extensive experiments on multiple search spaces and datasets show the effectiveness of the method.\n3). The method can combine with DARTS variants to further improve the performance.\n\n\nWeaknesses:\n1). I am sort of concerned that the auxiliary skip connection may suppress the weight of original skip connection. And I hope the authors will have further analysis.\n2). I wonder what is the meaning of “DARTS-”since the method actually add an auxiliary skip connection stead of removing any connections or operations. I strongly suggest the authors change the name into “regDARTS” (regularized), “gradDARTS” (graduated) etc.\n\n\n[1] Zhou, Pan, et al. \"Theory-inspired path-regularized differential network architecture search.\" arXiv preprint arXiv:2006.16537 (2020).\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting observation, but more experiments needed",
            "review": "This paper presents an interesting method to alleviate the mode collapse of DARTS (all operations degenerate to skip-connect). This is done by simply adding a skip-connect operation to complement the output of the cell function and making the coefficient of the auxiliary operation decay with time. The method is tested on a few benchmarks and settings.\n\nOverall, I think the idea is neat and interesting, but more experiments need to be done to *support the claim of this paper*.\n\n1. The statement that \"skip-connect has two roles, one for auxiliary connection and one for candidate operation\" is interesting, but not well justified. According to the authors' claim, the skip-connect operator start to dominate the cell structure after a long training procedure *because* the role of auxiliary connection becomes more and more important along with the training procedure. This is not supported by either [Zela, ICLR'20] or [Bi, 1910.11831], both of which claimed that the dominance of skip-connect comes from the incorrectly computed gradients of DARTS. *It is highly suggested to discuss this point.* Moreover, to justify the statement, an experiment is easily done: keep the auxiliary skip-connect throughout the search procedure (\\beta==1), and observe if mode collapse still happens. If not, please provide the results and the guess will become more convincing.\n\n2. The claim of \"preventing mode collapse\" is not well justified using experiments. For example, if the search procedure (on CIFAR10) is extended to 200 epochs or longer, is it possible to avoid mode collapse using the proposed method? It would be interesting to list a few results with varying lengths of the training procedure.\n\n3. Some experimental results need explanation.\n\n(1) In ImageNet experiments, the authors reported a 76.2% accuracy with 467M parameters. This really surprised me, before I checked the searched architecture and noticed that this architecture was obtained from the MobileNet space. Provided the name of DARTS-, this is VERY confusing, seemingly to guide the readers to believe that this is obtained in the DARTS space. I urge the authors to try their best to avoid such misunderstanding.\n\n(2) (minor) In the ablation study, the comparison against P-DARTS and PC-DARTS is weird. Even without the constraint of 2 skip-connect operations, P-DARTS does not report 96.48% on CIFAR10, even lower than the random search baseline.\n\n(3) (minor) Since a \"bad\" hyper-parameter of PC-DARTS (K=2) is used, I am wondering how the results are like if K=4 is used. BTW, the PC-DARTS paper reported 97.3% accuracy using K=2, but this paper reports 97.1%.\n\n\nI am now neutral on this paper, slightly leaning towards rejection. Provided more complete results, I will consider changing my score.\n\nAFTER READING THE REBUTTAL, I changed my score from 5 to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}