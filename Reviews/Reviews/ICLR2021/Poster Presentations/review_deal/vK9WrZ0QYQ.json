{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper closes an important gap in our understanding of neural tangent kernels. \nIn addition, the used techniques are novel. \nMy low confidence is mainly based on the fact, that the review process at conference is not perfectly suited to deal with such papers, since their review would actually require both expert reviewers and substantially longer reviewing periods."
    },
    "Reviews": [
        {
            "title": "A clearly written paper but I still have some doubts",
            "review": "The goal of this paper is to complete the theoretical subset inclusion relationships given in (Geifman et al., 2020).\nThe authors proved that the RKHS of the the neural tangent kernel (NTK) with any number of layers\nis a subset of the RKHS of the Laplace kernel.\nCombined with the results in (Geifman et al., 2020),\nif the input domain is restricted to the sphere of $(d-1)$-dimensional real space,\nthe RKHS of NTK with any number of layers\nis equal to the RKHS of the Laplace kernel.\n\nThe writing of this paper is very clear.\nI can easily get what the authors intend to express.\nHowever, I still have some doubts about the details.\n\nIn Section 2.1,\nthe original Laplace kernels and exponential power kernels are all classic shift-invariant kernels.\nHere the authors provided the dot-product versions of these two kernels.\nWhy do the authors need to do these changes?\nFor the sphere $\\mathbb{S}^{d-1}$, the shift-invariant versions are equivalent to the dot-product kernels?\n\nUnder Lemma 3,\nif $\\Sigma_k(x, x) = 1$,\nwe can only get $\\Sigma_k(u) = \\Sigma_{k-1}(u)$.\nWhat does the notation $\\kappa_1^{(k)}$ means here?\n\nTheorem 1 in (Aronszajn, 1950, p. 354) just stated that if $K_1 \\preccurlyeq K_2$, then $\\mathcal{K}_1 \\subseteq \\mathcal{K}_2$.\nI am very curious and doubting about Lemma 4.\nFor any positive constant $\\gamma > 0$,\ndoes Lemma 4 always exist?\n\nUnder Lemma 4, the authors stated that:\n1. \"Then the Maclaurin series of $K_{\\mathrm{Lap}}(u)$ and $N_k(u)$ have all non-negative coefficients by the classical approximation theory.\"\n2. \"If the Maclaurin series of $K(u)$ have all non-negative coefficients,\n$K$ is a positive definite kernel on the unit sphere.\n\nI cannot find the original documents of (Schoenberg, 1942) and (Cheney and Light, 2009).\nCould you please provide these results in the Appendix such that we can check the correctness of these results?\nThis is because, in the following part, all the proofs will be focused on $\\gamma^{2}\\left[z^{n}\\right] K_{\\text {Lap }}(z) \\geq\\left[z^{n}\\right] N_{k}(z)$.\n\nIn Section 3,\nI have two doubting points.\n1. The first one is under the definition of $\\Delta$-domain.\nThe authors used $z$ to replace $u$ and $u=x^{\\top} y$ as shown Section 2.1.\nIf we put $u=x^{\\top} y = 1$,\nthe Laplace kernels and exponential power kernels given in Section 2.1 will all become constant.\nCould the authors explain more about this assumption?\n2. The second one is under Lemma 5.\nThe authors stated that \"Careful singularity analysis then gives ...\".\nIt is very difficult for me to easily get these two results about Maclaurin coefficients.\nCould you give the detailed steps for the derivation of these results?\n\nIn Theorem 2,\nI am very curious about the seemingly contrary results:\n$\\mathcal{H}_{K_{\\text {exp }}^{\\gamma_1, \\sigma_1}} \\left(\\mathbb{S}^{d-1}\\right) \\subseteq \\mathcal{H}_{K_{\\text {exp }}^{\\gamma_2, \\sigma_2}}\\left(\\mathbb{S}^{d-1}\\right)$, and \n$\\mathcal{H}_{K_{\\mathrm{exp}}^{\\gamma_2, \\sigma_2}}\\left(\\mathbb{R}^{d}\\right) \\subseteq \\mathcal{H}_{K_{\\mathrm{exp}}^{\\gamma_1, \\sigma_1}}\\left(\\mathbb{R}^{d}\\right).$\nCould you provide more explanations about this point in Section 4.\nThe superficial explanations cannot help us understand this point.\nWe need to go deeper into the proof sketch to figure out what makes this phenomenon happen.\n\nI didn't go much deeper into the Appendix,\nso it is hard for me to check the correctness of the whole paper.\nSince ICLR is a highly selective conference,\nI will give a score 5 at present.\nAfter the authors solved my concerns and other reviewers verifies the correctness of all proofs,\nI may raise my score to 6 or 7.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "This paper compares the reproducing kernel Hilbert spaces (RKHSes) generated by a deep neural tangent kernel (NTK), a Laplacian kernel, and an exponential power kernel. The main result is twofold: 1) the RKHSes associated with a NTK and a Laplacian kernel  contain the same set of functions when restricted to the sphere S^{d-1}; and 2) the RKHS generated by the exponential kernel with a smaller power contains more functions when restricted to the sphere S^{d-1} or R^d.\n\nThe key proof idea involves an analysis  the asymptotic rates of the Maclaurin coefficients of the kernels. Aronszajnâ€™s lemma is then used to establish the inclusion of the RKHS. A notable contribution in the proof is the application of analytic combinatorics to obtain the asymptotic rate O(n^{-3/2}) of the Maclaurin coefficients of the NTK which is intractable due to its recursive definition.  The authors also provide numerical results showing the empirical rate matches the theoretical asymptotic rate of the Maclaurin coefficients of the Laplace kernel and NTKs.\n\nOverall, I think the paper is well-written, and the proof is solid.  The proof technique using analytic combinatorics seems novel, and the main results are useful for theoretical understanding of neural networks.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review on the theoretical analysis of the relationship between Laplace kernel and NTK",
            "review": "This paper proves that the reproducing kernel Hilbert spaces of a deep neural tangent kernel and the Laplace kernel have the same set of functions when they restricted to the sphere $S^{d-1}$, which improves the results established in Geifman et al., 2020. Moreover, the paper proves that more non-smooth of the exponential power kernel leads to a larger RKHS with restriction on the sphere $S^{d-1}$ and the entire $R^d$. Furthermore, the authors conduct numerical experiments to verify the asymptotics of the Maclaurin coefficients of the Laplace kernel and NTKs kernel. In summary, the paper is well-written and organized logically. The proof of theoretical results of this paper seems to be correct and reasonable, resulting from the full details of the proof provided in the appendices. \nThe contribution of this paper includes two parts. Firstly, it aims to explain why the Laplace kernel and NTK have similar performance in experiments from theoretical point of view by showing that the space of the Laplace kernel and NTK are the same when limited to a sphere. On the other hand, the author reveals the relationship between the smoothness of the exponential power kernel and the corresponding RKHS to explain the better performance of the exponential kernel with a smaller power in the experiments. \nNow I would like to give some comments, 1. Do Laplace kernel and NTK have a similar learning dynamic when we perform kernelized gradient decent in real-world dataset? 2. It is necessary to study the behavior of the NTK and the Laplace kernel outside of $S^{d-1}$. I wonder whether the theoretical results proposed are helpful to improve the performance of Laplace kernel or NTK. 3. The author demonstrates that a non-smooth exponential power kernel leads to a larger RKHS, but whether this indicates that the model obtained by adopting a non-smooth kernel has greater generalization capability. I think the author needs to further theoretically illustrate the relationship between them.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "strong article: RKHS of NTK in deep ReLU nets = Laplace kernel on the unit sphere ",
            "review": "This paper uses singularity analysis developed in the context of analytic combinatorics to study the relationship between the reproducing kernel Hilbert spaces of the NTK in a deep fully connected ReLU network, the Laplace kernel, and exponential power kernels. The main results are when these kernels are restricted to the unit sphere. In particular, the authors show that, as vector spaces, the RKHS on the unit sphere of the NTK for a ReLU network of any fixed depth are the same and in fact coincides with that of the Laplace kernel. The authors also compare the RKHS for exponential power kernels, demonstrating that larger powers lead to smaller Hilbert spaces. \n\nStrong Points: \n1. Understanding the NTK of a deep ReLU networks is a popular and important topic. \n2. The NTK is defined via non-linear recursions, meaning that obtaining quantitative results for deep networks is both technically and conceptually challenging. \n3. Proving that the RKHS of the Laplace and ReLU kernels coincides as vectors spaces is a substantial result.  \n4. The authors are the first, to my knowledge, to apply the rather elegant singularity analysis of Flajolet-Sedgewick to study the NTK. Perhaps the closes prior work used free probability to study the spectrum of the NTK. \n5. The paper is clearly written.\n\nWeak Points:\n1. As the authors themselves allude to in the discussion, showing that two RKHS are the same as vector spaces ignores the Hilbert spaces structure and can still lead to different inductive biases in kernel regression.\n2. The present work applies only to ReLU networks, while the recursive definition of the NTK is valid for rather general non-linearities. \n\nOverall, this article is a solid theoretical contribution to our understanding of the NTK. In addition to proving concrete results comparing the Laplace, NTK, and exponential power kernels, it is serves as a proof-of-concept for potentially using the tools of singularity analysis to understand neural networks. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}