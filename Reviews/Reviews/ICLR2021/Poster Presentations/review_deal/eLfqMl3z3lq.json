{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces an alternative to Langevin sampling and also the idea of adversarial score sampling. \nThe reviewers are generally supportive of the paper.\n\nPros:\n- The idea behind improving Langevin sampling is theoretically justified and leads to a simple algorithm. \n- The idea behind adversarial score matching is also shown to be effective \n- Improvement over baseline\n\nCons: \n- Two ideas packed  into one paper, which is reflected by the title as well. \n-  From the narrative it could be thought that using EDS on the last step of CAS is the contribution of the paper. "
    },
    "Reviews": [
        {
            "title": "Improvement of Langevin dynamics to sample from score matching functions adversarially trained.",
            "review": "This paper tackles the problem of generative modeling by using Langevin dynamics to sample from the denoising score function. Recently, this family of approaches (Song and Ermon 2019, Song and Ermon 2020) has shown promising and competitive results being positioned as a potential alternative to GANs. \n\nThe paper introduces different improvements over Song and Ermon (2020). A different sampling dynamic (Consistent Annealed Sampling) that produces a more stable training that the traditional annealing scheme by carefully scaling the injected noise. Second, it is empirically shown that running a denoising step on the generated sample leads to an improvement of the FID score. Based on this observation, the paper proposes to use a denoiser trained in an adversarial fashion to synthesize more realistic images.\n\nThe work addresses the very relevant problem of how to synthesize images in a realistic way, introducing some modifications to existing works that lead to an improvement on the quality of the generated image.  The paper is well written, presents a nice introduction to the method, which allows to motivate the different modifications in a natural way. The proposed modifications are analyzed in low-dimensional toy experiments and in small-scale images (CIFAR, LSUN-churchers, Stacked-MNIST).\n\nIn what follows I list a few questions: \n\n1. Would it be possible to analyze the strategy of sampling presented in Kadkhodaei and Simoncelli 2020 (concurrent work), and compare to the one proposed in the paper? Both strategies seem to improve the stabilization of the procedure by scaling the noise.\n\n2. Regarding the step of applying the denoiser to the generated sample. I wonder what happens if the denoiser is re-applied? Also, is this connected to the fact that the denoiser may have a fixed point and this fixed point might lead to a better sample? \n\n3. Regarding using an adversarial denoising. In the denoising literature, there are a few works connecting score matching and state-of-the-art image denoisers. I would like to see a better discussion of this. For example, see,\n\nRomano, Y., Elad, M. and Milanfar, P., 2017. The little engine that could: Regularization by denoising (RED). SIAM Journal on Imaging Sciences, 10(4), pp.1804-1844.\n\nReehorst, E.T. and Schniter, P., 2018. Regularization by denoising: Clarifications and new interpretations. IEEE transactions on computational imaging, 5(1), pp.52-67.\n\n---\nAfter Discussion:\nI think this is a good paper and I would like to see it presented at ICLR2021. \n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting finding about ALS, other contributions are less convincing",
            "review": "The submission presents three contributions. First, the authors show the inconsistencies in the existing annealed Langevin sampling used in score-matching generative models and propose to correct it with the newly proposed Consistent Annealed Sampling (CAS) algorithm. The second contribution claimed is in providing evidence of the benefits of Expected Denoised Sample (EDS). Furthermore, the submission introduces a hybrid adversarial score-matching model that demonstrates improvements in terms of FID on simpler architectures.\n\nThe proposed CAS algorithm is theoretically well-motivated based on the observation that ALS is inconsistent with the scaling of the noise during sampling process (although the question whether noise should follow none other than geometric progression is still an open question). The paper is well-written, and the ablation study is carried out well.\n\nHowever, it is a bit confusing as to whether the EDS (although under a different name — denoising jump) is a contribution of this paper or is something proposed prior to this work. I understand that this denoising procedure has already been presented as a necessary technique in score matching models. Nevertheless, I believe the authors contributed by showing that both ALS and CAS move samples towards the EDS (Proposition 3) and show additional empirical evidence of its benefits on synthetic and real datasets.\n\nTaking EDS on the last Langevin step diminishes the impact of CAS (doesn't bring unambiguous improvement in FID scores in the experiments), otherwise very interesting finding both theoretically and algorithmically, and substitute for ALS.\n\nThe effect of the hybrid model is also not persistent and depends on the architecture used. For an incremental improvement (a combination of two models), the improvement is not consistent across architectures. The paper does not explain whether there is a good rationale for such a combination; therefore I remain sceptical about the results.\n\nGiven all the above, I am still leaning a bit towards accepting the paper as it covers an interesting finding relating to the ALS. Although the CAS effect on performance is limited by the EDS, score-matching models are of broad interest for the ICLR community.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation to Accept",
            "review": "The paper presents a novel approach for denoising score matching, where the Annealed Langevin Sampling has been substituted by Consistent Annealed Sampling, which adds more stability to the process.\n\nThe paper is in general clear and well-written. The contributions are clearly highlighted and the proposed approach is conveniently compared with other state of the art methods, demonstrating its superiority.\n\nPositive aspects:\n- The Consistent Annealed Sampling proposed in this paper is more stable than the Annealed Langevin Sampling\n- The combination between GAN and score matching improves the quality/diversity of the generated sample\n\nNegative aspects: \n- The limitation of the method to Gaussian noise\n- The presentation of a real scenario for your approach would have been a plus\n\nHowever, I have some questions:\n1. Who is n_sigma parameter in Algorithm 1? \n2. Algorithm 1, line 4: there is no iteration over 't' in the loop?\n3. How does your denoising scheme work? Do you create noisy samples from your real data and try to denoise them using the proposed approach? Because taking a sample affected by random noise (in the test phase) I guess it won't work. \n4. The denoising scheme is used in a GAN framework, the denoised samples being perceived as real by the discriminator. Is the system trained end-to-end or first you denoise the image and afterwards you train the GAN?\n5. Could you please indicate an application scenario which could benefit from this approach, e.g. image-to-image translation, domain adaptation, etc.?\n6. Your method is assuming Gaussian noise. Can it be extended to the case of general noise (a noise model which could be also learnt)?\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review round 0 for “ Adversarial score matching and improved sampling for image generation”",
            "review": "The article deals with generative models based on “Annealed Langevin Sampling“ rather than a GAN.\nTheses models suffer from worse FID than GAN.\nAuthors proposed to denoise the last Langevin samples to reduce the gap in performance with Adversarial Network.\nThe paper is really easy to read with good illustrations and supporting experiments.\n\nIn order to gain in comprehension, especially for people new to ALS, it would have been great if authors have proposed an illustration (and comparison) of the samples evolution along Alg 1 and Alg 2 .\n\nAuthors are honest in their revised results comments but I don’t known if they will be able to include the erratum  in a final version\n\nAs I was not aware before this review of “Annealed Langevin Sampling” my rating may not be confident.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}