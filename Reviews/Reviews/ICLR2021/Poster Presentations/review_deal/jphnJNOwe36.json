{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques. All reviewers were positive about the paper, though R5 questioned the novelty of the paper which built heavily on a few previous papers (in particular, it builds heavily on Sagawa et al. 2020a,b). The AC is satisfied with the authors`' response clarifying the novelty. Given that this topic is quite timely and of interest to the ICLR community, and that this paper presented a clean investigation on it, the AC recommends acceptance."
    },
    "Reviews": [
        {
            "title": "Well-written paper, although a bit limited in overall contributions",
            "review": "Summary:\nThe paper builds upon prior work that shows that overparameterized networks learned by ERM can have poor worst-case performance over pre-defined groups. Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis-calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re-training the classification layer. They show improvements over ERM in worst-case subgroup error.  \n\nStrengths:  \n1. The paper is very well-written and easy to follow.\n2. The paper provides a better understanding of worst-case generalization in overparamaterized models by isolating the issue to the classification layer, which can help machine learning practitioners better understanding how they can address the issue of poor worst-group performance. \n\n\nWeaknesses:  \n1. The scope of the work seems largely limited to the set-up from Sagawa a, b. Given that all three datasets are simplified/synthetic (only one attribute, max. 4 subgroups), it would have been great to see how this paper's analysis applied to more complex settings.   \n2. It is not immediately clear why we would not expect classifier retraining/threshold correction (which as the author notes, are standard techniques) to work for the overparameterized setting? The richness of learned representations is well known, so in some sense the findings are not too surprising, especially given the simple (e.g. binary label) settings that make post-hoc corrections less complex. Could the authors better explain why overparameterization reduces our expectations on the effectiveness of these post-hoc procedures? \n3. The requirement of knowing the groups a priori seems rather significant. While one of their main cited works (Sagawa a) seems to have touched on group attribute mis-specification, this was not explored here -- how does having imprecise knowledge of the groups effect performance when using threshold correction or classifier retraining? \n4. I have trouble fully understanding Figure 2 without a sense of what \"insufficient\" or poor representations would look like in a tSNE visualization.  \n\n\nRecommendation:  \nI recommend acceptance. While I remain concerned about the limited scope of the experiments, I believe the paper adds valuable insights to the overall important topic of robustness / worst-case generalization.  \n\nQuestions:\n1. Does the nature of the attribute (land vs water, or hair color) have any effect on the observed poor worst-group performance, or are the results are mainly due to the fact that some groups are rarer than others? For example, would the authors expect similar results if y=male,female and A=blond,dark for celebA?   \n2. Did the authors visualize the embeddings of models trained with DRO, to see whether there is any improvement in the learned representations ability to distinguish subgroups? ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A demonstration of a few methods for post-hoc improvements for bias in overparametrized models. Interesting, but not sure if there is enough new information.",
            "review": "This paper is concerned with potential improvements to the worst-case\n(mainly minority class/group) generalizations in over-parametrized\nneural networks through post-hoc corrections. The authors demonstrate\nthe problem and the suggested corrections (that were used in previous\nliterature) on one artificial classification task as well as two\nimage classification tasks. The paper shows that post-hoc corrections\nmay improve the worst-subgroup scores similar to an earlier\nstate-of-the-art system that modifies the learning objective.\n\nThis topic is interesting. The paper is in general written well, and\ndemonstrates the problem convincingly. The post-hoc fix solution\nsuggested also seem to be performing reasonably well on the problems /\ndata sets used in the study.\n\nThat being said, it feels the study/paper builds on a few earlier\nstudies heavily, and I am not fully convinced that there is enough new\nfindings in the present paper to warrant publication in ICLR.\n\nI also have a few minor notes/suggestions:\n\n- The findings presented in Figure 4 is interesting (mainly the fact\n  that the overparametrization seem to be improving the worst-group\n  performance with threshold tuning). It would be interesting to see\n  more investigation/discussion of what could be the underlying\n  reason.\n\n- Page 2 (middle): \"also common the fairness literature\" ->   \"also common in the fairness literature\" \n- Figure 2 is not very readable. If increasing size/scale is not an\n  option due to limited space, taking legend out the figures may also\n  improve it. Colorblind friendly colors may also be a good idea.\n- There are case (normalization) issues in the references:\n  \"ml\", \"t-sne\" (not exhaustive, a through check is\n  recommended).\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Nice study of simple methods for improving worst-case subgroup error in overparameterized models",
            "review": "This paper studies how to improve the worst-case subgroup error in overparameterized models using two simple post-hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per-group threshold on the logits. The efficacy of these techniques is evaluated on three synthetic datasets.\n\nPros:\n- The paper studies a timely topic--generalization in overparameterized models-- with some applications to fairness.\n- The algorithms presented are very simple and avoid the overhead of more complicated algorithms like DRO or subsampling strategies.\n- In the experiments presented, both the thresholding method and the \"learn a new classification layer\" method significantly improve worst-case group error over ERM and are competitive with DRO.\n- The paper is well-written and pleasant to read.\n\nCons:\n- The experimental evaluation is somewhat limited, focusing on three synthetic datasets and two models (ResNet/Logistic Regression). It remains an open question how well the presented technique work more generally both on different model families and on \"real\" distributions.\n\nMinor:\n- There appears to be a discrepancy between the text and the main results table, Table 1. The text says \"For example, on celebA, THR reduces the worst-subgroup error from 56.94% to 31.11%\", but the table shows THR gets a worst-subgroup error of 12.10. What's going on there?\n\n==============\n\nUpdate after rebuttal:\nThank you for clarifying the numbers in Table 1 should match the main text. I enjoyed this paper, and I'm keeping my score unchanged.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}