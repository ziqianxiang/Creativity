{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper studies knowledge distillation through the lens of semi-parametric inference. There has been a lot of work on knowledge distillation in the past, but, as the paper points out, most of it is heuristic or empirical in nature, and thus theoretical understanding on the subject is lacking. The reviewers generally agree that this paper makes a useful theoretical contribution towards a better understanding of knowledge distillation. However, the reviewers also raised some concerns, especially regarding the clarity of the text, and they felt that the paper might be overstating its contribution. Still, all reviewers recommend acceptance, and on balance the merits of the paper seem to outweigh the weaknesses, so I'd be happy to recommend acceptance."
    },
    "Reviews": [
        {
            "title": "Interesting approach for guarantees for KD. Final message muddied by lack of clarity and upselling of results.",
            "review": "### Summary\nThis paper gives generalization guarantees for vanilla knowledge distillation, where it identifies a large variance issue due to the teacher’s complexity in this guarantee, and then proposes alternative approaches to eliminate this issue. The framework it uses is inspired by semi-parametric methods. The main resulting algorithm has two key components: cross-fitting and a corrective surrogate loss. An SGD variant is also presented. Experiments illustrate some of the benefits of the cross-fitting approach.\n\n### Strengths\n+ The semi-parametric perspective and the attempt to have a bias-variance tradeoff as part of the knowledge distillation procedure are well-motivated and insightful.\n+  Lemma 2 is a neat technical building block and it has the potential of being useful beyond the scope of the paper.\n\n### Weaknesses\n- The paper lacks clarity in notation and presentation. In terms of notation, many expressions are overloaded (multiple forms of $\\gamma$ and completely different meanings for symbols $\\zeta$ and $\\mu$). In terms of clarity: important definitions are buried in the text instead of being highlighted (e.g. the corrective surrogate loss or the choice of, the definition of $q_p$ given only in an example), key steps are not always spelled out (e.g. the Bregman divergence simplifications, the choice of $\\hat \\gamma= q_{\\hat p}$ in the cross-fitting approach, or the use of the teacher in the SGD approach)\n- The motivation of the intermediate choices are not well-presented. For example, in transitioning to the corrective surrogate loss, a Taylor expansion of the ideal loss is used, but then the $\\gamma$-corrected loss is given without explanation. To me it seems to be motivated only by quadratic losses, but I’m sure more can be said by the authors.\n- Some of the arguments in favor of the paper are a bit underhanded. ~~For example, when discussing the drawbacks of vanilla distillation, the paper calls them *inherent*, though the only evidence is the upper bound provided. To make such a claim, a lower bound is needed.~~ [Edit: the author's lower bound is good.]\n- More importantly, the key contribution of the approach, the claim of eliminating the teacher complexity from the upper bounds is not quite crisp. Namely, Theorem 4 requires a upper bound on the critical radii of *uniformly over all* classes that could be empirically selected by the teacher. It’s true that the complexity of the union of these classes (as in Theorem 1) can still be larger than this uniform bound. But this is not the comparison that the paper makes when it expounds the advantage of the new bound. Instead, it compares the complexity of any *one* of these classes (paragraph at the end of Section 4.2, page 6) to the union, which is disingenuous.\n- Lastly, even though the corrective surrogate loss is touted as the key component of the approach, the experiments clearly show that it’s the cross-fitting that is ultimately regularizing the problem and reducing the dependence on the teacher’s complexity. But this isn’t analyzed in isolation in the paper. In general, the corrective loss only contributes modestly. The only exception seems in the Adult Dataset, but only when the teacher has *low* complexity, which is also not part of the message of the paper.\n\n### Overall\nI found that the paper takes an interesting approach to giving guarantees to KD methods, with several interesting technical contributions. However, the final message of the paper is muddied by the lack of clarity and an attempt to upsell the actual technical results. This is why at this point I would place the paper as borderline.\n\n_[Edit: I read the response of the authors. The lower bound for vanilla KD seems good. But the author still fail to adequately explain the gain of Thm 4 (now Thm 3). The requisite upper bound on the critical radius uniformly over all potential functions generated by the teacher means that the result of the theorem still has an implicit dependence on the teacher's complexity. For this reason, I will leave my recommendation the same.]_\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"KNOWLEDGE DISTILLATION AS SEMIPARAMETRIC INFERENCE\"",
            "review": "The paper under review presents a new approach to the theoretical understanding of konowledge distillation, an efficient technique that makes possible the training of small models given the predictions of a more complex model. The problem is recast as a problem in semi-supervised learning and several theorems are then drawn from this new viewpoint. \n\nBased on this re-interpretation of the problem, a nice analysis of vanilla knowledge distillation is given and a correction is derived from a Taylor expansion of the loss which is then carefully investigated as well. Finally a stochastic gradient method is studied using relevant bias and variance parameters.\n\nAll the proofs seem correct. The simulations results provide convincing illustrations of the findings.\n\nThe exposition follows a fast pace and is not very easy to comprehend for the non-expert. In particular a more detailed presentation of the semi-supervised learning background should be given in order to better substantiate the subsequent findings of this paper. \n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Review of Paper 2515",
            "review": "The knowledge distillation (KD) approach is a two-step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model. A key theoretical question about KD is whether and how much this two-step approach can improve on the one-step approach that trains the student model directly on the labeled data. This paper casts KD as a semiparametric inference problem by treating the optimal student model as the parameter of primary interest and the true class probabilities as the nuisance parameter. Building on the semiparametric framework, the paper makes two contributions: 1) develops theoretical guarantees for the vanilla KD algorithm; 2) proposes improved KD by using a first-order bias-corrected loss and a sample splitting procedure.\n\nOverall, I find the paper novel, well-written, and thought-provoking. It bridges the two directions of KD and semiparametric inference, allowing for the possibility of borrowing theoretical and methodological tools from semiparametric inference to analyze and improve the KD approach.\n\nOn the other hand, I think the paper somewhat oversells the semiparametric inference idea, since the theory presented in the paper (Theorem 1) does not have much semiparametric flavor. The central questions in semiparametric inference are the information bounds and semiparametric efficiency for the target parameter. Although Theorem 1 is useful, it does not directly address these key questions. For example, what is the best possible performance for the student model? In many classical semiparametric inference problems, we are able to construct a semiparametrically efficient estimator. Would this be possible for KD?\n\nMore comments:\n1) The paper seems to treat $f_0$ as infinite dimensional (page 3, 3rd paragraph of Section 3). However, classical semiparametric inference largely exploits the finite or low dimensionality of the target parameter to answer the questions mentioned above. In the KD framework, I still think the finite-dimensional case is more interesting and would provide more fundamental insights.\n\n2) There is a notable difference between the settings of KD and classical semiparametric inference: the target parameter $f_0$ in KD is ancillary in the sense that the data generating model depends only on the nuisance parameter $p_0$. Would this affect the applicability of semiparametric inference techniques?\n\n3) Page 4, line 11, “student” should be “teacher”.\n\nUpdate: I appreciate the authors’ response. The answer to my semiparametric efficiency question, however, is too terse and unclear. Theorems 1 and 3 provide only error bounds; I don’t see how they, together with the semiparametric efficiency of OLS, directly imply semiparametric efficiency for the student model. Semiparametric efficiency involves the optimal asymptotic variance. The authors might have confused the concept with rate optimality. Nevertheless, the paper is a nice contribution, and I will keep my rating.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Although viewing knowledge distillation as a semi-parametric inference problem is interesting, the related work of this paper is not sufficient and the key contribution of balancing bias and variance requires more effects.",
            "review": "This paper formulates knowledge distillation as a semi-parametric inference problem. Then, the paper adapts techniques from semi-parametric inference to analyze the error of a student model and improve the performance of knowledge distillation. The presentation is overall of good quality and it is relatively easy for me to follow the logic flow. That being said, there is still some room of improving the clarity of the paper.\n\nP1: Analyzing and improving knowledge distillation from a point view of semi-parametric inference is very interesting.  The adaptation of semi-parametric inference techniques such as cross-fitting and orthogonal machine learning is novel.\n\nC1: The related work is not sufficient. After reviewing some pioneer studies in knowledge distillation before 2015, this paper jumps to very recent studies in 2020.  As knowledge distillation is a hot research topic, there are a large number of related studies from 2016 to 2019. It is highly recommended for this paper to present the advance of knowledge distillation techniques from 2015 until now in a more comprehensive and organized way.\n\nC2: Some contributions of this paper, cross-fitting and orthogonal machine learning, are largely adapted from semi-parametric inference literature. The contribution that is not adapted from semi-parametric inference literature is the proposed correction that balances the bias and variance. The correction is a linear combination of a bias term and a variance term. It is not clear how the bias and variance terms are related to the theoretical analysis in previous sections. It would be better if this paper could provide a clearer justification of the bias and variance terms by the theoretical analysis.\n\nC3: The hyper-parameter \\alpha is introduced to balance the bias and variance of knowledge distillation in the experiment section. Since one of the key ideas of the paper is to balance the bias and variance, the hyper-parameter should be important in determining the accuracy of the student model. This paper should do some ablation studies to investigate how the hyper-parameter affects the performance of the proposed knowledge distillation.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}