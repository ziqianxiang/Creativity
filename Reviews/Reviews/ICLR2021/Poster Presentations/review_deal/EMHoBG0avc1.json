{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces improving passage retrieval for multi-hop QA datasets by recursively retrieving passages, adding previously retrieved passages to the input (in addition to a query). This simple method shows gains on multiple QA benchmark datasets, and the evaluation presented in the paper on multiple competitive benchmark datasets (HotpotQA, FEVER) is very thorough (R1, R3, R4). \n\nWhile the application is pretty narrow, the performance gain (considering both efficiency and accuracy) is fairly significant, and the paper presents a simple model with less assumption (e.g., inter-document hyperlinks), that could be useful for future research. \n\n[1] also seems like a relevant line of work. \n\n[1] Generation-Augmented Retrieval for Open-domain Question Answering\nhttps://arxiv.org/pdf/2009.08553.pdf\n\n"
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "This paper extends the recently proposed dense retrieval methods to the multi-hop open-domain questions, so as to handle complex multi-hop queries. The overall idea is simple, direct but effective. The authors conduct extensive experiments on two multi-hop datasets, HotpotQA and multi-evidence FEVER, and evaluation results demonstrate that the proposed model achieves impressive results on both the knowledge retrieval task and multi-hop QA. \n\nMy only concern is about the novelty of the paper. The contribution of this paper seems to be limited as it just combines the recently proposed dense retrieval methods with multi-hop QA. Besides, compared with [1], this paper seems to just replace the RNN-based encoder in the knowledge retriever with the BERT-based encoder.\n\n[1] Das R, Dhuliawala S, Zaheer M, et al. Multi-step retriever-reader interaction for scalable open-domain question answering[C].ICLR, 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A simple and effective pseudo relevance feedback approach to multi-hop QA",
            "review": "Summary; The paper proposes a simple, clever, and as far as I can tell novel, combination of dense retrieval techniques and pseudo relevance feedback for multi-hop (complex) open-domain QA. The basic idea is to concatenate the passages returned for the first query to the original question, to form a new query to be encoded and used in combination with the retrieval system. Thus, the paper does not present a radically new idea but combines successfully recent popular methods (dense retrieval, featuring in most SOTA work in QA) with old techniques well known and studied for decades in the IR community.\n\nIn general, I enjoyed reading this paper and find this is a useful technique that the community should learn about to explore further in the same and other related problems; e.g., OpenQA. The approach is somewhat incremental, but competitive and conceptually simple. The lack of awareness for the IR context is my main reason for the score not being higher.\n\nPros:\n- The paper shows that the idea is effective in terms of performance, yielding state-of-the-art results on two multi-hop datasets, HotpotQA and multi-evidence FEVER. \n- Experiments seem generally rigorous and reproducible following standard popular datasets and procedures.\n- Experiments include very recent work on reader’s architectures, including generative ones (RAG, FID).\n- The architecture’s simplicity does not make assumptions on the nature of the data and associated meta information (e.g., link graphs) and produces also a more efficient system.\n\nCons:\n- The paper could be organized better in its final version. In particular, providing more context and motivation for the problem; first of all, why are such “complex” questions important? How key is the multi-hopness aspect? I would particularly recommend anchoring this discussion to the findings of (Min et al, 2019, https://www.aclweb.org/anthology/P19-1416/). In particular the fact that many such questions can be solved in one hop. How natural/artificial is the task? How does this aspects affects this specific study? \n- The related work section is insufficient and the absence of an adequate discussion of the (pseudo) relevance feedback work in IR is a major weakness. This is an foundational line of work going back at least to the research of Rocchio in the 1970s. I would suggest (Ruthven and Lalmas, 2003, A survey on the use of relevance feedback for information access systems) as a starting point.\n\nDetailed feedback/comments:\n- How does the use of tfidf as a source of hard negatives relate to the argument about IR baselines being poor?\n- What is the motivation for, and conclusion for, evaluating additional linked docs as negatives if they only yield minor gains? \n- Could you discuss more the nature of the supervised information available with respect to the fact that the number of hops is known, and also the order of the passage sequences (or can be inferred heuristically); e.g., wrt to the claim “training in an order-agnostic manner hardly works at all, and underperforms even the single-hop baseline”.\n- Have you tried using more than 2 steps? It may be valuable to run an experiment.\n- It would be also valuable to experiment with this approach on other OpenQA tasks such as Natural Question etc. Also to provide more evidence for the generality of the approach.\n- How does this approach deal with the limited encoder capacity, in terms of number of tokens? How many passages can you append to form the 2nd query, and how does this affect performance?\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Reasonable Idea, Solid Experiment, Strong Results, Lack of Novelty",
            "review": "Summary:\nThis paper proposes multi-hop dense retrieval for open-domain multi-hop question answering. It extends previous dense passage retrieval into the corresponding multi-hop version by using retrieved passages to latently reformulate the query representation after each retrieval pass. In the end, it can significantly improve the performance on HotpotQA and multi-evidence FEVER dataset. The analyses are very comprehensive and extensive from almost every relevant perspective.\n\nPros:\n1.\tExtending dense passage retriever into its multi-hop version is a reasonable direction. This is the first work in this direction.\n2.\tThe experimental results are strong, and the analyses are comprehensive.\nCons:\n1.\tThis paper mainly focuses on the experimental and analysis part. Although it is good to know these lessons in multi-hop dense retrieval, the proposed method itself is limited in terms of novelty.\n2.\tSince the idea mainly comes from dense passage retrieval, it is not very clear to me whether the improvement comes from the dense passage retrieval or the proposed latent query reformulation for DPR. Since these are the major contribution of this paper, I think it is necessary to have a separate section for discussion.\n\nQuestion:\n1.\tIn Table 3, what is the training detail of the Single-hop ablation? Is it also trained through a similar negative sampling process described in section 2.2? If so, why the improvement of multi-hop version DPR is so significant?\n2.\tIn section 1, it is said that “The main problem in answering multi-hop open-domain questions is that the search space grows exponentially with each retrieval hop.” In my understanding, the proposed multi-hop DPR still suffers from this problem, right? The only difference between the proposed multi-hop DPR and previous approaches is that it does not use any structured knowledge within the documents for retrieval.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper provides several significant findings that are expected to be referred to by many other studies. Their method is simple and outperforms other multi-hop QA models. Also, it is computationally efficient.",
            "review": "1. Summary of this paper\n    - The topic of this paper is multi-hop QA, which studies answering complex natural language questions. Complex questions require an information aggregation process across multiple documents and recent multi-hop QA models design this process by sequentially retrieving relevant documents (Asai 2020 et al.). This paper alleviates two problems in recent multi-hop QA models. One is that recent multi-hop QA models require external knowledge such as Wikipedia hyperlinks. This problem results in the models' low generalization ability on new domains that the external knowledge is no longer available. The other problem is computational efficiency. The authors propose a novel multi-hop QA model named MDR that does not require external knowledge and is ten times faster than the recent models. MDR uses question reformulation and MIPS. Question reformulation design the information aggregation process by iteratively generating a query vector related to the documents that should be accompanied to answer the original question. MDR generates such query vectors by comparing the given question and previously retrieved documents. MDR encodes passages in a large corpus(indexing) with the same encoder used in the question reformulation process and uses MIPS to find relevant documents with the generated query vectors. In experiments, the authors show that MDR outperforms recent multi-hop QA models, and also they show the computational efficiency of MDR.\n\n2. Strong and weak points of this paper\n    - Strong points\n        - This paper provides a detailed analysis of their method. Experimental results show the validity of the proposed method, and some strong findings described below.\n            - Table 2 confirms that MDR outperforms \"Graph Rec Retriever (Asai et al.)\". This result shows the feasibility of a more accurate multi-hop QA model without external knowledge such as Wikipedia hyperlinks.\n            - Table 3 shows a detailed analysis of each component in MDR. This table indicates several vital features for multi-hop QA models that can be easily ignored in the model design process. The experimental results on \"w/o order\" and \"w/o linked negatives\" show significant findings in multi-hop QA.\n            - Table 4 shows that the question reformulation method (MDR) has similar performance to the question decomposition method with human-annotated sub-questions.\n            - Table 5 shows the end-to-end performance of multi-hop QA models. MDR outperforms existing state-of-the-art multi-hop QA models.\n        - The proposed method is computationally efficient.\n        - The proposed method is simple. Many follow-up studies based on the proposed method are expected.\n        - The experimental results support their claim.\n    - Weak points\n        - This paper does not mention the publicly available code of their method. It would be nice if the authors provide implementations after the decision process.\n        - In the section \"Question Decomposition for Retrieval,\" the authors conclude that question decomposition is unnecessary in the context of dense retrieval with a strong pretrained encoder. However, Table 4 shows that question decomposition with a simple open-domain QA model has a similar performance to MDR. These results indicate that question decomposition is an effective method to make simple single-hop open-domain QA models used in multi-hop QA. Please provide more evidence for the conclusion, \"unnecessity of question decomposition.\"\n\n3. Recommendation\n    - Accept\n    - This paper provides several significant findings that are expected to be referred to by many other studies. Their method is simple and outperforms other multi-hop QA models. Also, it is computationally efficient.\n\n4. Questions\n    - In Table 4, the Decomp method is based on DPR (dense passage retriever). What will be the results if MDR uses the gold sub-questions? Does using sub-questions in MDR increase retriever performance?\n    - Please provide the number of hard negative samples for a question.\n    - In section 2.2, what is the start token in the sentence \" Specifically, we apply layer normalization over the start token’s representations from RoBERTa to get the final dense query/passage vectors.\" Is the start token pooled_output of the CLS token or hidden representation of the CLS token?",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}