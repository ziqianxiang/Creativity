{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a user-interaction framework where users choose a subset of LFs from a family of LFs generated using some template (e.g. keywords for text classification).   The proposed criteria is not very surprising, but the authors present a practical and useful system that is well demonstrated both in the paper and the very careful author feedback.  These enhancements have also been incorporated in the revised version. \n\n Apart from the literature pointed by the reviewers, here are some more papers that are related to this paper:\n1. Gregory Druck, Burr Settles, Andrew McCallum:\nActive Learning by Labeling Features. EMNLP 2009: 81-90\n\n2. \tGregory Druck, Gideon S. Mann, Andrew McCallum:\nLearning from labeled features using generalized expectation criteria. SIGIR 2008: 595-602\n\n 3. Data Programming using Continuous and Quality-Guided Labeling Functions. In AAAI, 2020."
    },
    "Reviews": [
        {
            "title": "Interesting problem and potentially impactful solution",
            "review": "The paper argues that identifying weak supervision signals may require domain expert knowledge and creativity. Therefore, the authors propose an interactive weak supervision solution, in which the annotators assess the quality of automatically identified labelling functions. The proposed solution shows superior performance on a number of classification benchmarks after some minimum number of iterations.\n\nStrengths:\n1. The paper studies an important and novel problem in the domain of weak supervision\n2. The proposed solution outperforms the baselines\n3. The authors show some promising results by running experiments with real human annotators.\n4. The paper is well written and easy to follow.\n\nWeaknesses:\n1. A large body of work on weak supervision has been completely ignored by the authors. These are some relevant papers:\n- Using weak supervision beyond classification (for ranking): https://dl.acm.org/doi/10.1145/3077136.3080832\n- Learning from multiple weak supervision signals: https://dl.acm.org/doi/10.1145/3209978.3210041\n- Some theoretical justifications for weak supervision training (for ranking): https://dl.acm.org/doi/10.1145/3234944.3234968\n\n2. Fidelity-weighted learning (ICLR 2018) was proposed to automatically identify the quality of weak supervision signals. It is different from the proposed solution in the sense that a small set of labeled instances are used to assess the quality of weak supervision signals as opposed to labeling the weak supervision functions themselves. However, the goal is the same and some comparison (or at least some discussions) are necessary for the paper, I believe.\n\n3. There is no discussion on going beyond classification tasks and even some extreme classification scenarios.\n\n4. I think there should be some difficulty estimation associated with each weak supervision annotation function. Some functions may not be easily assessed by human annotators and this may influence the model. I think this can be addressed in future work.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper proposes a new framework for interactively selecting labeling heuristics in a weakly supervised setting. The main idea of the proposed approach is to combine weak supervision and active learning. Compared to the previous work which relies on human manually create labeling functions (the abstraction of the weak supervision), this work defines a family of labeling function and uses an active learning method to interactively identify a set of labeling functions that maximizes the utility based on the usefulness by the users. The experiment results showed that the proposed approach outperforms other baseline methods.\n\nMain comments:\nThe idea of using an active learning approach to select useful labeling functions from a large pool of labeling functions based on usefulness is interesting and the paper presents compelling solutions to a practical problem. However, the paper lacks clarity to help understand the technical contributions. Here are detailed comments:\n\n1. For labeling function family, the authors defined the labeling function family as sets of expert-interpretable weak supervision sources and shared several examples in different domains. However, there is lack of description of how to free the users to generate such labeling function family or how to adapt the existing labeling families to users' applications, which makes it hard for the readers to understand the exact user interaction in the framework. It's crucial to provide more details here.\n\n2. For the final subset of labeling functions, the authors defined the three scenarios. The distinctions of these different scenarios are not clear. What's the relation between the size of labeling functions selected in the framework and the # of iterations to select the labeling functions? There are some other unclear parts:\n  -- For scenario (A), if there are no restrictions about labeling functions, it seems like Eq (4) is not necessary since the framework only needs to find all useful labeling functions.\n  -- For scenario (B), it seems that this is the only useful case.\n  -- For scenario (C), it seems that this is only a special case.\n  -- Can you show the algorithmic procedure of LWS-AS? It's a little confusing about relations between the usefullness of LF and LF validated by experts. What's the difference there?\n  -- Overall, Section 3 should be clarified to help reader to understand the model.\n\n3. For the experimental evaluation, the proposed method outperforms the baseline on different tasks. There are several unclear parts:\n  -- The comparison with Snuba, it's not fair to compare # of iterations for proposed methods with the # of labeled data since in each iteration the proposed method will apply one more labeling function which likely to label more than 1 sample.\n  -- For the IWS-LSE top 100, ac, the authors didn't define what top 100 is. If it's $m$, it's not clear why the performance is improving when the # of iterations is greater than 100.\n  -- The approach only evaluated on the text classification problem, it would strengthen the paper if the authors can evaluate in another domain, such as image classification. Snuba has some image applications.\n\nOverall, I like the direction and idea, and I think this paper can be a great paper if the authors can clarify how real users interact with the framework and provide a more thorough experimental analysis.\n\n------ Post Rebuttal------\n\nThanks to the authors for the new experiments and feedback!\n\nThe rebuttal addressed most of my comments and increased my score by one point.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "The paper studies a very interesting problem in weak supervision. One of the main challenges with weak supervision approaches is how to generate the weakly labeled data. The paper suggests an interactive framework for generating and validating labeling functions to generate weakly supervised data.\n\nThe user experiments are interesting and  provide very good insights about the method but there are several open questions about the experiments:\n\n- The paper defines an LF family based on the existence of unigrams and exhaustively generate LFs from that family. I understand this would work for sentiment classification (the tow tasks the experiments consider). However, it is not clear to me that this would generalized to other text classification tasks (not to mention language tasks beyond text classification)\n\n- It is interesting to measure and show the effort of validating labeling functions vs. labeling samples. However, I am not sure if that results would hold beyond sentiment analysis too. Looking at the appendix, it seems most LFs are positive words such as \"wonderful\" and \"perfect\" or negative words such as \"poor\" or \"bad\". These will be significantly easier to judge by a human than other text classification tasks (e.g. the \"Salman Rushdie\" example in the introduction)\n\n- It would be interesting to compare the proposed approach to a setting where a user is asked to generate LFs based on a description of the tasks and a few examples.\n\n- Regarding the comparison with Snuba, Suba uses a small labeled dataset  to generate heuristics. How was that implemented in the experiments?\n\n----------------------------------\nEdited after authors responses: I would like to thank the authors for the detailed response and the changes they have made to the paper. While I still have some concerns and questions about generalization to more complex tasks, models and labeling functions, I think the additional experiments demonstrate the value of the proposed framework and opens the door for future work to explore these questions.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Experiments may be over simplistic",
            "review": "This paper proposes a new approach for active learning by interactively discovering weak supervision. Instead of asking human to annotate data points, the method collects feedback about candidate label functions, from which a model learns to identify promising label functions. With the final set of label functions, they train a classifier with the estimated labels on unlabeled data. They conduct experiments on text classification datasets with both oracle and human feedback, and show a large improvement compared with traditional active learning.\n\nThe paper is well-motivated and the proposed active learning strategy is derived based on theoretical guarantees. I have a few questions about the framework: \n\n-- Does the system start training without any knowledge of the classification task (i.e. no label functions)? If so, how can the model distinguish between the candidates in the beginning? \n\n-- Is the expert-feedback model updated after each human feedback? How is the model updated?\n\nI suggest to add a step-by-step description of the system to help the readers understand the pipeline.\n\nThe experiments have been performed on several text classification tasks, where the label function family contains only uni-gram indicators. The end classifier is a small MLP on bag-of-words features. I have concerns about its applicability to real-world NLP tasks. Can the proposed method be extended to more complicated (and useful) label functions such as regular expressions? If we use more advanced text classifiers like BERT, can the proposed strategy still help? Since here the feedback from human is just whether a uni-gram is indicative to the label, which can be easily learned by a neural model.\n\nMissing reference:\nSrivastava, Shashank, Igor Labutov, and Tom Mitchell. \"Learning to Ask for Conversational Machine Learning.\" EMNLP 2019.\nIt also seeks human supervision beyond the instance level.\n\n------\nAfter author response:\n\nThe authors presented new experiments on image datasets during the rebuttal, which demonstrate the flexibility of the proposed framework. However, all the experiments are conducted on simple tasks and models. It is still unclear whether and how this method can help more practical problems.\n\nOverall, I think this paper presents a nice exploration towards interactive weak supervision. I hope the authors can release their code and experiment details to encourage future work.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}