{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces \"Concept Embeddings\"  to  Prototypical Network, which are part-based representations and are learnt by a set of independent networks (which can share weights).  The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross-organ cell type classification.  The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few-shot learning methods.\n\nAlthough some reviewers raised concerns about not using other few-shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the “part-based concepts” to reasonably span the space of all images which is a characteristic of fine-grained image classification problem.   Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient.\n\n"
    },
    "Reviews": [
        {
            "title": "Review for Concept Learners for Generalizable Few-Shot Learning",
            "review": "**Overview** \nInspired by DeepEMD's obersvation that compositional representations generalize better for few-shot image classification, this paper (COMET) introduces \"Concepts Embeddings\" components to the Prototypical Networks. \"Concepts Embeddings\" are part-based representations and are learnt by a set of independent networks $ \\\\{ f_{\\theta_i} \\\\} $ (can also share weights). \n\nFor each novel class, its $N$ concept prototypes are computed using the support set, and a query example is classified by measuring its distances to each novel class's $N$ concept prototypes. \n\nEvaluations and experiments are carried out on 3 datasets: CUB, Tabula Muris and Reuters. The proposed COMET approach obtains favorable performances on all 3 datasets.\n\n**Pros** \nThis work explore ways to learn compositional representations in a semi-supervised fashion.  It takes Prototypical Networks as baseline and adds a novel \"Concept Embedding\" component.  The authors show the proposed approach can significantly improve ProtoNet baseline on 3 datasets. It also outperforms the recent approach DeepEMD. The following observation on the CUB dataset is very interesting: \"Strikingly, by adding just one most frequent concept corresponding to a bird’s beak on top of the whole image concept, we improve ProtoNet’s performance on CUB by 10% and 5% in 1-shot and 5-shot tasks, respectively.\"\n\n**Cons** \nUsing multiple prototypes per class has been explored in \"Infinite Mixture Prototypes for Few-shot Learning\" (ICML'19), maybe the authors should mention it and highlight differences.\n\nIt's unclear where does $C$ comes from by just reading section 2.2.  How to compute $C$ is later elaborated in experiments section 3.1. Essentially $C$ are hand-crafted for each dataset. Maybe the authors should hint this in section 2.2.\n\nThe math symbols are a bit confusing in section 2.2. For instance, inconsistent notation $f_{\\theta}^{(j)}$ in section 2.2 and $f_{\\theta_j}$ in Figure 1, also $D$'s definition is not given (is it equal to the number of pixels $\\times$ number of input channel?).\n\nI appreciate the authors provinding many interesting ablation studies, but the main evaluation (Table 1) misses 2 important image few-shot classification dataset/benchmarks: miniImageNet, and MetaDataset.  How does the COMET with \"Unsupervised concept annotation\" performs on the above two datasets?\n\nWhen compared with ensemble, why specifically 5 ProtoNets? How many concepts are used in COMET and COMET (shared weight)? This ablation should be included in main paper, as network capacity can improve accuracy significantly, see: \"A Closer Look at Few-shot Classification\"(ICLR'19).\n\n\n**Reason for the decision** \nThis paper explores using compositional representations in Prototypical Networks. The authors introduce a novel concept embeddings idea, and show that it can significantly improve ProtoNet baseline on 3 datasets. However using multiple prototypes per class is not a new idea and the authors should highlight the differences with prior works (what's the strength of this particular formulation?). For the evaluation, I highly suggest the authors to include standard mini-ImageNet and Meta-Dataset FSL image classification benchmarks and report performances on them. \n\nI find the following observation on the CUB dataset  very interesting: \"Strikingly, by adding just one most frequent concept corresponding to a bird’s beak on top of the whole image concept, we improve ProtoNet’s performance on CUB by 10% and 5% in 1-shot and 5-shot tasks, respectively.\" That means human-validate concepts are complimentary to the representations learnt by a 4-layer network, but how about using a deep/stronger backbone, would it narrow the margin? I suggest the authors add this ablation study: How much does adding a concept help w.r.t to the depth of the backbone.  \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper need more clarification.",
            "review": "This paper proposes concept learners to effectively combines the outputs of independent concept learners. The model is evaluated on several datasets from different domains. \n\nFirst of all, why the authors define it as generalizable few-shot learning, the settings targeted in this paper seem to do no different from traditional few-shot learning. Why is it called generalizable few-shot learning?\n\nThe other two main concerns are:\n\nThe idea of learning to attend different segments of an image or learning to the segment has been proposed in previous literature [a,b,c,d]. Even if they are not specifically targeted on a few-shot image classification, the proposed concept learners are still pretty similar to previous works and are not specifically designed for few-shot image classification tasks. Thus, I believe the novelty is somewhat limited for this submission.\n\nIn experiments, even if the authors choose three datasets for comparisons. I am more interested in results on standard benchmarks, such as miniImageNet, tieredImageNet. The results on the current datasets are not a convincing performance in my point of view. It is expected to see experiments on more standard and large-scaled datasets.\n\nA minor point I am curious about is that by simple data augmentation method: crop, is it possible that multiple random cropping can generate different concepts and achieve similar effects by simply cropping multiple times on one image?\n\n\n[a] Linsley, D., Shiebler, D., Eberhardt, S. and Serre, T., 2018. Learning what and where to attend. arXiv preprint arXiv:1805.08819.\n\n[b] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n\n[c] Zhu Y, Liu C, Jiang S. Multi-attention meta learning for few-shot fine-grained image recognition[C]//Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence. 2020: 1090-1096.\n\n[d] Hou R, Chang H, Bingpeng M A, et al. Cross attention network for few-shot classification[C]//Advances in Neural Information Processing Systems. 2019: 4003-4014.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting idea for few-shot learning and some concerns on the experimental evaluation",
            "review": "The paper presents a knowledge-driven prototypical learning strategy for few-shot classification tasks. The main idea of this work is to introduce a set of concepts defined in the subspaces of inputs and represent each class as a group of concept prototypes for few-shot learning. Following the prototypical networks, the method first computes the concept embeddings of an input, and then takes the summation of the distances between those embeddings and their corresponding concept prototypes in each class to estimate the class probability.  The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross-organ cell type classification. \n\nStrengths:\n-  The idea of introducing concept knowledge into few-shot learning in a domain-agnostic manner seems interesting. While prior work has explored the compositionality for few-shot learning in vision domain, which shares the similar idea, the specific way that this paper introduces concepts as class representation is different. \n\n-  The paper is well written and easy to follow. \n\n- The experiments show the proposed method outperforms baseline prototypical network and other prior work on all four benchmarks across three domains.\n\nConcerns:\n-  Additional supervision from concepts: Using concept-based prototypes essentially introduces a form of part-level supervision, and therefore, it is not surprising to see the improvement in few-shot learning.  In addition, while such representations make sense for transferring prior knowledge on classes and provide interpretability, it is non-trivial to obtain such concepts in some domains. For example, it is difficult to learn the keypoint concepts for general object classes in vision domain. \n\n- Eq. 4: The distance from an input to each class representation sums over individual distances from each concepts. It is unclear how such aggregation is able to handle noisy concepts and outliers. The summation would be dominated by concepts with large distances, which may not relevant to a specific input. More clarification would be helpful for this point. \n\n- The experimental evaluation seems a bit lacking. For the vision domain, the paper only evaluated this method on the fine-grained classification tasks, which is uncommon in few-shot classification literature. It would be more convincing to show the results on the miniImageNet, which is a general object classification task, and see how the concept would be generalized to such cases.  \n\n- The method use multiple prototypes for each class, which has a higher model complexity than the standard prototypical network. Instead of using ensemble strategy (as shown in the suppl), what if the ablative study evaluates the setting in which the baseline learns multiple prototypes for each class? \n\n- The effect of predefined concepts. Do the human-defined concepts play an critical role in the proposed method? What if the method uses randomly selected subsets of features for concept definition?   \n  \nUpdates: Thanks for the authors' response. The newly added experimental results address my concerns. I think this is an interesting work and recommend this paper to be accepted. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "No obvious issues",
            "review": "Summary\n\nThis paper introduces potential use of intermediate structured representation of input space called “concepts” which are most likely human-interpretable. This intermediate space is then used for few-shot learning instead of using only the input space. This leads to better classification performance on the task, and it shows that injecting human-interpretable structured representation into task correlates with better performance (as one would hope). The paper uses datasets from different domains and shows improvement over approaches that don’t use the above defined “concepts”.\n\n\nStrengths\n- Paper is well-written\n- Approach is tested on different dataset domains\n- Experiments, ablation and qualitative results verify the claims of the paper\n\n\nWeaknesses\n\n- Figure 3 (Right)\n    - In CUB you mentioned that not all 15 part-based annotations are available for every image (ref. Section 3.1 - Datasets). Does that mean there is a non-uniform distribution of part annotations per class? If this is the case, should it be taken into account for Figure 3 (Right)? It could be that beak is important for most of the classes simply because it was present more often that the other parts.\n\n- Fine-grained classification only?\n    - If I understood correctly, you need concepts (or part-based annotations) to be consistently present in all input images and all classes, because if they are not and the distribution of concepts is heavily skewed towards a subset of classes, then your approach won’t be effective. Does that mean that your approach is only effective for fine-grained classification datasets? Can mini-ImageNet or Omniglot be candidates for testing?\n\n\nMinor concerns (suggestions, typos, etc.)\n- Table 1\n    - Are the methods reimplemented or you cited results from papers? Can you appropriately mention these facts and/or cite the papers where you picked numbers from in Table 1?\n    - It was a bit surprising to see MAML underperforming in comparison to all the baselines (including Finetune)\n\n\nPreliminary Rating and its justification\n\nI see no obvious issues with the paper. I recommend accept.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}