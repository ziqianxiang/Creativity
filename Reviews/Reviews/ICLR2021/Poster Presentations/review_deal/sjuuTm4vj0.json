{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The scores here are bimodal. \nThe low-scoring reviewers have problems with the evaluation, and I agree it could be improved. \nThe high scoring reviewers seem to mostly agree with those complaints, but think that the paper is interesting enough \nto be accepted anyway. \nOne of the low-scoring reviewers has some complaints about novelty that I don't find super convincing.\nThe other low-scoring reviewer has suggested that they'd be OK with a decision of Accept. \n\nPart of me thinks that I should reject this paper with a message of \"come back later with the experiments improved\", and that\nthat would be the best thing for the field, because the paper can already be publicized on arXiv anyway. \nBut the other part of me thinks: what if they do that and get unlucky with a bad batch of reviews the next time (the current reviewers were great and had a really thorough discussion)?\nWith some amount of trepidation, I'm recommending accept, but *please* reward my faith in you (the authors) and make an effort to fix the things reviewers complained about before the camera ready."
    },
    "Reviews": [
        {
            "title": "Lack of novelty, weak evaluation",
            "review": "In this paper, the authors propose a latent space regression method for analyzing and manipulating the latent space of pre-trained GAN models. Unlike existing optimization-based methods, an explicit latent code regressor is learned to map the input to the latent space. The authors apply this approach to several applications: image composition, attribute modification, image completion, and multimodal editing. They also present some analysis on the independence of semantic parts of an image.\n\nThis paper addresses an interesting problem of manipulating the latent space of pre-trained GANs. However, learning an explicit latent regressor (encoder) is already explored in [1]. According to [1], the biggest problem of learning such an encoder is the encoded latent code cannot fully reconstruct the original content. As can be seen in image completion in Fig. 1, the contents in the inverted image are perceptually different from the input. This is due to the limitation of expressiveness of the latent space, which is discussed in recent works including [1]. Based on it, technical novelty of this paper is significantly limited and I could not find the discussion about it in this paper.\n\nThe authors discuss about image inversion methods (GAN prior methods such as [1]), but did not compare them. State-of-the-art works need to be compared to verify the effectiveness of the proposed method. It looks like existing methods are also applicable to applications addressed in this paper.\n\n[1] Gu et al., Image Processing Using Multi-Code GAN Prior. In CVPR, 2020\n\nAfter rebuttal, the technical novelty is still not convincing. The masked encoder is widely used in image inpainting works. Combining multiple existing techniques may work better than existing methods, but it has little impact on the community. Thus, I do not change my rating.",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Impressive Image Editing Results",
            "review": "**PAPER SUMMARY**\n\nThe paper trains feedforward networks to project input images into the latent space of a pretrained GAN generator, and shows how this can be used both for various image editing tasks as well as to probe the internals of the trained generator. Experiments demonstrate that this can be used for tasks such as image composition, image completion, attribute modification, and multimodal image editing.\n\n**STRENGTHS**\n\n- The paper is well-written and easy to follow\n- The proposed method is both simple and effective for a variety of tasks\n- Experimental results (both qualitative and quantitative) are impressive and thorough\n- Extensive supplementary material providing additional details to aid reproduction\n\n**WEAKNESSES**\n\n- Some missing references to BiGANs / ALI\n\n**MISSING REFERENCES**\n\nOne missing line of related work is that of BiGANs [1, 2] / Adversarially Learned Inference [3] in which an inference network is jointly trained with the generator and discriminator to project samples into the latent space. However the overall goal of these papers is often some kind of unsupervised feature learning, which is very different from the image editing applications presented in this submission.\n\n[1] Donahue et al, “Adversarial Feature Learning”, ICLR 2017\n\n[2] Donahue and Simonyan, “Large Scale Adversarial Representation Learning”, NeurIPS 2019\n\n[3] Dumoulin et al, “Adversarially Learned Inference”, ICLR 2017\n\n**OVERALL**\n\nOn the whole this is a strong paper. The method is simple and effective, the results are impressive, the experiments are thorough, and the paper is very well-written and easy to follow. This is a clear accept in my view.\n\n**AFTER REBUTTAL**\n\nAfter reading the other review's and the author responses, my opinion is unchanged: This is a well-written paper with a simple and effective method, and a clear accept.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Thin evidence despite large number of experiments",
            "review": "The paper investigates controllable image synthesis using generative adversarial networks. The key component is a masked encoder that finds an approximate latent code given an image and a mask, so that the generated image matches the given image at the unmasked parts.\n\nAs a point of clarity, it is not always clear which experiments use generated images and which use real images. The use of Z loss when training the encoder of course requires the use of generated images, but using the encoder does not. The necessity of Z loss is left somewhat unclear even with the appendix, as is the need for the complex three-component loss in general. In the projection method presented in StyleGAN2 paper, only LPIPS loss is used and it appears to finds latent codes for generated images quite well.\n\nI'm worried about regressing to W+ latent space with the StyleGAN variants. As is evident from, e.g., the bottom two rows in Figure 4 of Abdal et al., this latent space is so large that any generator can produce pretty much any image from this kind of latent code. In this view, the regressor network can be seen as just a conveniently weak bias towards natural-looking images - with a hypothetical perfect latent regression network, it should be possible to regress both generated and real images into W+ with fairly small resulting image error. This would presumably make the composites also retain all their artifacts instead of falling on the manifold of generated images. The imperfect nature of the regression network seems to be necessary for the image manipulation to succeed, but this is not addressed or analyzed in the paper.\n\nThe increase in FID shown in Figure 3 is quite remarkable. Compared to StyleGAN, the FFHQ and LSUN Car FIDs are over 10x worse than in the original. The reason for this would need to explored in more detail. Table 3 in A.2.4 reports some StyleGAN FIDs that do not match previously published numbers. FID of LSUN Car is 3.27 in the StyleGAN paper (2.32 in StyleGAN2), but Table 3 reports it as 22.06, almost 10x worse than state of the art. Reported FFHQ FID of 3.28 is between the StyleGAN papers but does not match either of them. The ProGAN FIDs are also consistently worse than reported in StyleGAN paper for the same datasets. These inconsistencies are puzzling, especially given that the paper claims to use pre-trained networks.\n\nI'm also somewhat concerned that the greater realism in terms of density (Figure 5) may be explained simply by the regressor network's incapability to create latent codes that are not at the densest regions, and the much higher FIDs are in line with this. If so, the method requires a tricky balancing act between image quality and retaining of individual components in the composite. In one direction the image quality falls, and in the other direction the amount of control falls. It is unclear if there is a sweet spot where both are good, and the examples in the paper do not convince me that there is.\n\nOverall the retaining of context does not seem to be very good. In Figure 10, the church image changes nearly completely except for the location of horizon, even though only the tower is supposed to change. The amount of control seems to be lacking as well. There is only one example where different modifications are done on the same base image (Figure 10 in Appendix A.2.1) and even there the laughing face acquires a mustache even though there is none in the composite. Experiments in Figure 12 and 13 also show that the composite is very different from the context image, and both are badly corrupted in all cases.\n\nTo summarize, when neither image quality nor control are properly demonstrated, I cannot see this method providing any advantages compared to previous work on editable GAN synthesis.\n\nPros: important topic, paper is well written.\n\nCons: quality of results is dubious, analysis of regressor network remains thin despite the large number of experiments, retaining of context with different modifications is not demonstrated",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Masked encoder seems like a great idea",
            "review": "I like this paper and I think it represents a very through-provoking and promising idea. The key aspect of this work is a (screen-space) masked encoder that learns to complete the loop with a previously trained generator. This allows for image completion, editing, collages, and essentially creates a prior for the generator’s training distribution. This allows the application to “snap” the (possibly partial) input to the manifold. While there are many encoders for GANs, I have never seen them formulated quite this way. The idea is also thoroughly compared and ablated.\n\nOkay, with that out of the way, I can specify that these comments relate to the 22-page version including the supplement. The 8-page version is too terse and I found myself longing for details and discussions that were nowhere to be found (except in the supplement). For example, Eq. 4 is the beef, but it’s not really discussed in any meaningful way. Yeah, ok so you added a mask? But what does that _mean_? Given that now the encoder has only a random subset of pixels at its disposal and yet it needs to produce the full latent code, it would seem that it needs to learn very flexible ways of coming up with a plausible latent. Thus I think it starts to participate in generative modeling in a much more real sense than a typical full-image encoder does. As a reader I would like to see this kind of discussions on top of the mechanical descriptions. \n\nNow, the real issue of course is the 8 page limit combined with a verbose layout, which suits many papers very well, but is a poor fit to multi-application works like this one. ICLR (and other conferences) should consider going to 10 pages with some “length must be proportional to the contribution” clause, because papers like this would be faster to write AND read if they were a bit longer. They would also be genuinely better. As an official reviewer I will read the supplement, but most others will not. Getting back to this paper: Obviously the authors have chosen the subset of material they felt was most compelling, and I won’t argue with that, but personally I didn’t like e.g. Sec 4.3 nearly as much as the discussion in A.2.1 (minus dataset rebalancing). I felt the former was unsurprising while the latter more clearly illustrated the key contribution, but that’s just a personal opinion. Sec 4.4 / Fig. 7 were very interesting for me, so please let those stay :) \n\nAs a summary, I find the idea clearly described, interesting, and likely to have many applications in future. Thus I recommend acceptance. \n\nEDIT: Lowered score by one point after reviewer discussions. The evaluation could certainly be better and the difference to earlier mask-based methods more clear.\n\nMinor:\n- Fig 16: Are you sure your Poisson solver isn’t broken? I would have expected much better results in that column.\n- Sec 4.4: Most of the time “c” is called component, but at least once it’s a “segment” instead. Should be component.\n- Sec 4.3: “to obtained the blended image” —> “to obtain the blended image”",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}