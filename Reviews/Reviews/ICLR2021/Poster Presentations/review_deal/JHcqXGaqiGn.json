{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper addresses a very important issue in GNN, the definition of a well-defined pooling function for node aggregation. The proposed Graph Multiset Transformer, although not entirely new, seems to be useful in practice. Issues related to experimental results, as well as problems with presentation, have been solved by the authors's rebuttal, that presented solid experimental results and analysis.\nConcerns about the real expressivity of the proposed approach when compared to Weisfeiler-Lehman graph isomorphism test do not affect the contribution delivered by the paper, that seems, at this point, significant. "
    },
    "Reviews": [
        {
            "title": "The Review",
            "review": "This paper proposes a Transformer-like model: Graph Multiset Transformer to perform the graph pool/aggregation. Overall, the technique part is concrete and clear and the experimental evaluations are comprehensive. The authors also prove the expressive power regarding WL-test. However, several points need to be clarified or addressed.\n1.\tThe idea to utilize the Transformer-like architecture to model the graph neural network (GNN) is not new. Some existing works[1,2] have employed the transformer to enhance the expressive power of GNN. It’s better to add more discussions between GTM and the existing works to highlight its contribution. Meanwhile, several studies [3,4,5,6,7] about graph pooling / self-attention are missing. It’s better to make discussions in the related works. Moreover, if possible, I suggest making the comparison with these methods, especially the recent studies to make the whole experimental results more convincing, e.g. HaarPool, EigenPool, etc.\n \n1.\tAbout the experimental settings. 1) In Section 4.1, the authors state that the 4 molecule datasets are obtained from OGB dataset. However, in OGB dataset, it only contains HIV, while Tox21, Toxcast, BBBP is not included. Maybe there is a mistake. 2) For the molecular dataset, the data splitting is very curial for the final results. Meanwhile, the atom/bond feature extraction process for the molecular datasets is unclear. The authors need to clarify the data splitting (random/scaffold) and feature extraction process to ensure the reproducibility of experiments.\n \nMinor:\n\nIn Equation 6, what is the QW_i^Q ?\n\nIn Equation 8, why we need MH(H, H, H)? How about directly applying H into SelfAtt block, i.e., Z=H?\n\nIn the experiments, this paper only evaluates the memory efficiency of GMT. I would like to see the evaluation about the time efficiency of GMT with other baselines.\n \nOverall, this paper is well written and the experiments look solid. Considering the novelty issue, I think this is a borderline paper. I recommend “Marginally below acceptance threshold ” and would like to see the author's response.\n\n[1] Rong, Yu, et al. \"GROVER: Self-supervised Message Passing Transformer on Large-scale Molecular Data.\" arXiv preprint arXiv:2007.02835 (2020).\n\n[2] Chithrananda, Seyone, Gabe Grand, and Bharath Ramsundar. \"ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction.\" arXiv preprint arXiv:2010.09885 (2020).\n\n[3] Wang, Yu Guang, et al. \"Haar graph pooling.\" arXiv (2019): arXiv-1909.\n\n[4] Ma, Yao, et al. \"Graph convolutional networks with eigenpooling.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019.\n\n[5] Bianchi, Filippo Maria, Daniele Grattarola, and Cesare Alippi. \"Spectral clustering with graph neural networks for graph pooling.\" (2020).\n\n[6] Ranjan, Ekagra, Soumya Sanyal, and Partha P. Talukdar. \"ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations.\" AAAI. 2020.\n\n[7] Li, Jia, et al. \"Semi-supervised graph classification: A hierarchical graph perspective.\" The World Wide Web Conference. 2019.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This work proposes a multi-head attention-based approach to capture node interactions in improving graph pooling. While it contributes a few novel ideas, several parts of the manuscript need more explanations. Additional experimental results are needed.",
            "review": "This work proposes a Graph Multiset Transformer (GMT) that uses a multi-head attention-based approach to capture potential interactions between nodes when pooling nodes to produce a graph representation. Multi-head attention mechanism is used to group nodes into clusters, each of which produces a representation. Self-attention is then used to pool representations of clusters into the representation of a graph.\n\nPros: \nThe proposed pooling is more reasonable than a simple sum or average pooling as the multi-head attention mechanism can potentially capture dependencies between nodes. \n\nCons: Several parts of the manuscript need more explanations. Additional experimental results (see below for details) are needed.\n(1) It is not clear what multi-head attention achieve semantically (as claimed: better capture structure information). \n(2) Figures are neither self-explained nor well explained in the main text.\n(3) The std of cross validation in each experiment should be reported. The mean performance alone is not enough to say a method performs better or not. It will be better to provide a p-value (e.g., t-test) to show if a method is statistically significantly better.\n(4) Four datasets from the Open Graph Benchmark were used. The authors should refer to the leaderboard for the performance of some baseline methods. For example, the leaderboard of HIV dataset reports that GIN has 0.7654 rather than 0.7595 listed in this manuscript. \n(5) The abstract points out that without considering task relevance is a weakness of the previous pooling method. However, it is not clear how the proposed approach make improvement(s) on this. \n(6) The proposed method does not necessary pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes.\n(7) In appendix, examples in Figure 10 are confusing. More explanations are needed. \n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2 ",
            "review": "This work studies the graph pooling operation for graph neural networks. It proposes the Graph Multiset Pooling which treats the graph pooling as a multiset encoding problem and can capture the graph structural information. It first employs multi-head attention to learn node features, where the query Q is a learnable matrix contains k vectors. Then a GMPool operation is performed and finally, the self-attention is used for learning inter-node relationships. Experimental results show the effectiveness of the proposed method.\n\nStrengths:\n+ This work studies an important problem, graph pooling. Graph pooling can learn high-level graph representations but is still less explored.\n+ The proposed method is interesting. By using a learnable query matrix, the method can reduce the n-node input to k-node output. The self-attention used after GMPool can learn the relationships between high-level embeddings.\n+ The experimental results are promising. The proposed method outperforms other compared methods.\n\nWeaknesses:\n- Even though the method is called Multiset Pooling, its method is not related to Multiset. The proposed method is mainly based on attention and self-attention mechanism. Then claiming the proposed method as Multiset Pooling is not convincing.\n- The experimental settings are not fair enough. The pooling operation is defined as reducing n-node input to k-node output. For all other methods, the pooling layer is connected with the global sum/average. However, in the proposed GMT, the GMPool is connected with a self-attention layer. It is not clear whether the proposed GMPool or the self-attention layer leads to the performance gain? A careful ablation study is needed.\n- I think the proposed method can be regarded as using SAGPool in the clustering-based pooling. The main difference is the multi-head attention and the learnable matrix S. Please comment if I missed something. \n- The use of graph structures is not very convincing. The GNN(H, A) is the simple message passing of GNNs. Then the graph structural information A is already incorporated in H since H is obtained by GNNs.\n- Several baselines are missing, such as Structpool and Edge Pooling. They should be discussed and compared.\n\nQuestions:\n1. If the query Q is obtained from the learnable matrix S, which is k*d dimensions. Then the output of  Att(Q; K; V ) should also have k*d’ dimension, which means the output is already reduced from n vectors to k vectors. Why do we still need the GMPool operation? The GMPool does not “compress the n nodes into the k typical nodes”.\n\n\n=====Update after rebuttal=====\n\nI have read the authors' rebuttal. Most of my concerns are addressed properly, and hence I am willing to increase my score from 4 to 6. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A method for multi-head attention pooling on graphs",
            "review": "The work extends the set transformer to obtain a method for multi-head attention pooling on multisets with connectivity (graphs). The authors show that the approach is as expressive as the WL isomorphism test and has better space complexity than existing node clustering networks. The method achieves state-of-the-art results on graph classification and strong results on graph reconstruction and generation.\n\nStrengths:\n- The paper is very well written and polished. \n- The figures complement the text well. \n- The work is technically and mathematically sound.\n- The method shows good results and is scalable, making it a valuable addition to the set of existing GNN operators.\n- The authors make an effort to substantiate their statements about expressivity and scalability with proofs.\n- The experiments are well chosen and show where improvements come from.\n\nWeaknesses:\n- The proven expressiveness is not a very strong statement, since most pooling approaches adhere to this property. It is nice to have the theoretical analysis though.\n- The method itself is an incremental variation of set transformers (Lee et al.), although adapted for a different type of input data.\n- Using the identity matrix as adjacency (as described in Appendix B, to work around the scalability issue of node clustering methods) seems to make the approach identical to the set transformer in all layers except the first, which dampens the contribution.\n\nThere is some potential for improvement in clarity of presentation:\n- In abstract: \"may yield representations that do not pass the graph isomorphism test\", in introduction, page 2: \"accurate representations of given graphs that can pass the WL-test\". Those sentences are confusing as it is not about representations passing the WL test, is it? It is about two graphs which are distinguished by WL get different representations (as correctly stated elsewhere in the work).\n- Regarding page 4, paragraph \"Graph Multi-head Attention\" and the following ones:\n\t- On the one hand, they are extremely close to Vaswani et al. and Lee et al. (sometimes even nearly the same sentences).\n\t- On the other hand, some things are left out, which are crucial for understanding, such as definitions of symbols for dimensionalities (n_q, d_k, d_v, d_model) and the origin of some matrices, see next point. \n\t- Where do the seeds S come from (for the non self attention operator)? It probably is a parameter matrix that is directly optimized but it is not completely clear (\"learnable\" is ambiguous, can also be the output of a network)\n\t- Suggestion: maybe the description of the pooling method becomes more clear when described in a top-down manner: Eq 7 -> Eq 6 -> Eq 5 -> Att\n\n\nExperiments:\n- Variances of graph classification results over the cross validation would be greatly appreciated (since there seems to be a space issue, they can go into the appendix)\n- The reconstruction architecture does not reconstruct adjacency. It might be interesting to see how well the method can do that (for the synthetic graphs for example)\n\nRelated work:\n- The work [1] should be mentioned and discussed in related work and compared against in experiments. It is also a pooling method with attention but seems to follow a different approach.\n\nTypos:\n- Figure 2 caption: \"to compress the all nodes\" -all\n- Proof of Theorem 4: \"but it is highly efficient than node clustering methods\"\n- Proof of Proposition 5: \"then the first them inherently generates\"\n\nAll in all, I think this paper has a valuable contribution, even if the method is incremental. Therefore, I tend to vote for accepting the paper but encourage the authors to improve on the mentioned issues in experiments, related work and presentation.\n\n[1] Ranjan et al.: ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations, AAAI 2020",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}