{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a method for future trajectory generation. The main contribution is in proposing a technique for data augmentation in the latent space which encourages prediction of trajectories that are both plausible, but also different from the training set. The results clearly show superior performance on standard benchmarks. The evaluation is thorough and ablations show that the proposed innovation matters. \n\nR2, R3, R4 recommend that the paper be accepted with scores 6, 8, and 6 respectively. R1 recommends the paper be rejected with a score of 5. The main concern of reviewers are: \n\nR1: \" In summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results.\" The authors addressed this concern in their rebuttal.  \n\nR2: \"Some other points remain still open such as the limited focus on Trajectron in evaluations.\" Since trajectron is a recent SOTA, I think this is not a big concern. Authors compare against other baseline methods too. \n\nR4: Comparison to Mercat, Jean, et al., ICRA 2020 is missing. The authors mention that their code is unavailable and therefore cannot compare.  \n\nR4: \"underlying reasons for the success of different components (classification of latent intent and hallucinative latent intent) are hard to explain\". I agree with this and this is also my major concern which I detail below. \n\nThe paper proposes to find diverse trajectories by generating two latent vectors: z, z'. The first h time steps are generated by latent vector z and the remainder using z'. The generated trajectory is evaluated by a discriminator that ensures plausibility. The latent vectors are chosen to be discrete and a classifier is trained to recognize z from ground truth trajectories. To encourage diverse trajectories, authors use a loss that encourages mis-classification of the latent variable inferred from the generated trajectory. Since the generated trajectory cannot be classified well, it is assumed to be different from the training set. \n\nThis formulation is rather adhoc. If the trajectory is indeed different from the training distribution, then it will also fool the discriminator. If it doesnot, then it's not very different. The mis-classification, is akin to encouraging high entropy in the z space inferred from predicted trajectories. With this view, it is possible that there is no need to generate two latent vectors z, z', but simply generate one and use the entropy penalty. I would love to see this experiment and see the authors demystify their method. It would also lead to significant changes in writing. Even now, writing needs improvement. Due to the proposed method being a adhoc trick, that is not well justified, I would normally not recommend acceptance. However, the empirical results are strong, tilting the recommendation to acceptance."
    },
    "Reviews": [
        {
            "title": "Review [Updated]",
            "review": "**SUMMARY**\n\nThe present work considers the problem of multi-agent trajectory prediction. Its main contribution is incorporating generative augmentation losses for improving the quality of a trajectory predictor. This is achieved by allowing trajetcory predictors to model intent as an unobserved latent variable in the model and using this to generate trajectories corresponding to different intentions. The work also proposes to use a descriminative loss encouraging diversity of the intents and an additional \"hallucination\" loss that allows for modelling mixed intents.\n\nThe idea is integrated with Trajectron++ and it is shown how the newly approach improves upon the Final Displacement Error, a comon trajectory prediction metric. In human experiments, it is also shown that the trajectories predicted by the new approach are considered more realistic by humans in comparison to the baseline. \n\n**STRENGTHS**\n- The idea of a structured generative loss for creating a richer diversity of predictions has a lot of potential to improve the trajectory prediction tasks.\n- Human studies seem to be a very interesting idea for studying trajectory prediction.\n\n**WEAKNESSES**\n- Although the authors claim somewhat broad applicability of their approach, the idea is ultimately only demonstrated on Trajectron++\n- The evaluations require some more data to be better interpretable.\n\n**CLARITY**\n\nI found this work mostly clear to read. It sometimes makes an assumption about the reader's familiarity with more specialized techniques such as LSGAN and spectral normalization. While there is probably no space to introduce everything used by the authors and I do not hold it against them, maybe it is worth briefly motivating these and other non-obvious design choices without going much into detail. Further smaller clarity questions/remarks are:\n- What is the motivation behind the discreteness of the latent variable?\n- What do the authors mean by \"unsupervised discrete random variables\"? As I understand it, a random variable is a mathematical object that typically does not entail properties such as supervised or unsupervised.  Maybe simply write \"discrete latent variable\"?\n\n**REPRODUCIBILITY**\n\nI believe the work to be mostly reproducible. It is not possible to judge the usefulness of the code without it being available during the review process. I still applaud the author's intent to release it.\n\n**EVALUATION**\n- Focusing the evaluation on one predictor (Trajectron++) seems insufficient given that the authors make a much broader claim about the general usefulness of their approach.\n- Similarly, the claim that the proposed approach outperforms multiple state-of-the-art predictors seems misleading. In Table 1, Trajectron would have been already the clearly best model even without the addition proposed by the authors.  In Table 6, the proposed approach does not always outperform the baseline and does not even always outperform Trajectron++. While I do not believe that every paper has to outperform all previous papers always on all metrics, it would be interesting to see a discussion/analysis on why this happens. \n- Also why does Table 6 only contain UCY? Is it because ETH is already shown in Table 4? The comparisons seem different to me, so maybe worth also integrating ETH in table 6?\n- Because the proposed method's performance is still somewhat close to Trajectron++'s, it would be interesting to see several runs of the same experiments also reporting the standard deviation of the obtained metrics. At least for the comparison with Trajectron++.\n\n**SUMMARY**\n\nIn summary, I think the authors propose an interesting idea and I believe using humans to judge the quality of trajectories is a cool evaluation methodology. The paper's weak point is a much broader claim while it is only evaluated with Trajectron++. And, in the comparison with this baseline, it is not clear to what extent the work will constantly outperform it. That being said, I want to encourage the authors in following this line of work and providing further evaluations.\n\n**POST-DISCUSSION UPDATE**\n\nI believe the authors to have addressed some of my concerns and I appreciate the additional experiments demonstrating that even the close results were more than a mere statistical artifact. Some other points remain still open such as the limited focus on Trajectron in evaluations.  In summary, I believe that the paper has now surpassed the acceptance threshold and am happy to recommend its publication.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Solid paper for consideration of accept",
            "review": "Summary:\nThe paper designed a framework for motion forecasting (trajectory prediction), with emphasis on multimodal distribution modeling and generalization. Specifically, they use latent code to model agent's intents. This latent code combined with historical trajectories and map were used to generate future trajectories, which were further judged by a discriminator. Besides, they added latent code classification and hallucinative data augmentation for performance boosting. \n\nReasons for score:\nThe major reason for the voting of accept is that this paper is solid and generally easy to understand. The interplay of latent code classification and hallucinative is interesting. \n\nPros:\n1. Solid method development, fluent method introduction, and systematic ablation studies. \n2. The proposed method seems to be interesting, especially the adding of latent code z to generate trajectories and then classify them.\n\nCons:\n1. The proposed methods seems to be combination of existing components, which limits its theoretical contributions. \n2. The underlying reasons for the success of different components (classification of latent intent and hallucinative latent intent) are hard to explain. \n\nPlease address the following questions:\n1. It would be interesting to see how this method compare with \"Mercat, Jean, et al. \"Multi-head attention for multi-modal joint vehicle motion forecasting.\" 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020.\" which seems to be a state of art.\n2. During inference, how do you generate and select multiple trajectory candidates?\n3. It seems like only when both classification latent and hallucinative latent are combined can they improve the performance. I notice the authors discussed about this, but I think this interplay needs more discussion. \n4. Did you try to see what is the contribution of map?\n5. For equation (2), it is better to display it as multi lines for easy understanding. Also, P(z,|x,m),e = EncÎ¸(x,m), the \",\" after z seems to be a typo.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "R1 updated review after author feedback",
            "review": "Summary: The authors tackle the problem of trajectory prediction in autonomous vehicles.  They propose a data augmentation scheme where they enrich the raw trajectories with synthetically generated trajectories to reduce spurious modes in the predictions. This enriched dataset is hoped to improve  accuracy in trajectory prediction problems.\n\nStrengths:\n\n1. The paper belongs to an active area of research. Reliable and accurate Trajectory prediction is one of the central problems in autonomous driving.\n\nWeaknesses:\n\n1. The paper is very hard to read. For example, the contributions are confusing and not clearly defined. In the first paragraph of the Introduction, a data augmentation scheme is presented. But then on Pg.2, the authors present two notions of intents as their contributions. What is the relation? From my understanding, the authors use $\\hat z \\sim P(z)$ to generate new data  (I'm looking at  Eqn 4). And this eqn 4 is termed as the \"classified intent\". There are several things to unpack here: \nFirst, these contributions are not well-defined. What is a \"classified intent\" and \"hallucinative intent\". These are not standard terms in the trajectory prediction literature. So these terms need to be defined and explained. I would like to see a mathematical definition and references to relevant citations in the traffic psychology literature.\nOnce it is formally defined what a \"classified intent\" and \"hallucinative intent\" is, the next thing would be to formally derive equations 4 and 5 from those definitions. Because eqns 4/5 are not derived with supporting derivations and justifications, the main concern here is that I don't find the motivation or relevance of these equations convincing.\nMy suggestion here is that most of the current material on Pg.4 can be moved to an implementation section. Instead, use that space to include the motivations, definitions, justifications, and derivations for eqns 4/5 as explained above.\n\n2. The questionable nature of the proposed equations (4, 5) directly relates to my next point, that is, the lack of any useful improvement over SOTA. In Tab. 1, the max FDE improvement is 18cm. In Tab 2. it is 5cm. Furthermore, the authors report gains in terms of percentages, which is misleading and even dangerous, since in the real world, it is important to know absolute errors and not percentage errors relative to another benchmark.\n\n3. Additionally, the authors primarily compare using the FDE (a weaker and insufficient metric than the ADE) on most occasions, most critical of which are Tab.1 which contains the comparison with other methods and Tab.5 which contains the ablation experiments that highlight the benefits of the proposed equations 4 and 5. For fair evaluation, it is necessary to also present comparisons using ADE.\n\nIn summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting hallucination technique for data augmentation",
            "review": "This paper presents an interesting technique to generate multimodal trajectory GAN and a carefully designed latent intent space. This latent intent space allows an operation termed as hallucination, which switches agent intents to enrich the latent spaces' coverage. \n\nThe paper is overall clear and well-written. However, there is one point regarding the hallucinative learning probably can benefit from more elaboration: the time step \"t\" is only introduced in the Hallucinative Learning paragraph. How is the time step being used in the network? Is the network only predicting y_{gt} as a simple waypoint (t=1), or a sequence of waypoints (a trajectory, in that case, what is the horizon)?  If I understand correctly, the main hallucination idea is to extract as many intentions in the latent space as possible, from the training data, and then the hallucination is implemented as randomly switching between different learned intents to generate augmented data? This point needs to be carefully clarified. \n\nRegarding the idea of using \"hallucination\" to generate more training data, many recent works have used the idea of hallucination for data augmentation, or even generating training data from scratch. For example, https://arxiv.org/pdf/2010.08098.pdf and https://arxiv.org/pdf/2007.14479.pdf generate training data using hallucination based on geometric feasibility, instead of latent intent, https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Few-Shot_Learning_via_Saliency-Guided_Hallucination_of_Samples_CVPR_2019_paper.pdf uses saliency to guide the generation of hallucination, https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf also used a very similar adversarial approach to generate feature hallucination. These works can help to set stage in the related work section. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}