{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a hyper-net method for multi-objective optimization, which trains a neural network that maps preference vector to the corresponding Pareto solution. The proposed idea is interesting and useful, although the evaluation of the work is not overwhelming convincing. The writing of the work can be further improved. \n\nAlso, the basic idea of the work is the almost the same as a concurrent work \"Lin et al 2020. controllable pareto multi-task learning\" which is also submitted to this conference. The paper cited that paper briefly, \"... The proposed method is conceptually similar to our approach...\",  which is too vague and brief. We urge the author to provide a through discussion on the detailed difference and similarity of the works, including empirical comparisons when necessary. "
    },
    "Reviews": [
        {
            "title": "Learning the Pareto Front",
            "review": "The paper proposes a method for multi-objective optimization. The key idea is to learn the entire Pareto front at once by training a hypernetwork that takes preference vector as an inputs and outputs network parameters, which corresponds to a point on the Pareto set with the desired trade-off specified by the preference vector. Specifically, the hypernetwork is a multi-head network where each head outputs a weight tensor of a module in the target network. The method improves HV from the baselines, in several multi-task learning problems, including image classification, regression and, mixed classification and regression.\n\n+) The main contribution of this work is to learn a continuous function that maps a preference vector to network parameters that corresponds to the desired trade-off. The trained hypernetwork generalizes to preference vectors unseen during training so that the required training time for getting models of an arbitrary trade-off is reduced.\n\n+) Compared to prior work, CPMTL, which starts from a Pareto optimal point and extends Pareto front locally around the point, the proposed method attempts to train a single hypernetwork that represents the entire Pareto front.\n\n+) Pareto front provides insights on the trade-off relation between tasks but it usually requires repetitive training of models under different preference settings. I think this method can be useful when understanding the relations among tasks by significantly reducing the training time to obtain the Pareto front.\n\n-) One concern is the scalability to the number of tasks. The amount of reduce in the training time of the proposed method over the base methods depends on how much the method generalize to the preference vectors unseen during training. For example, ideally, the model trained using the preference vectors [0,1] and [1,0] generalizes to [0.5,0.5]. In other words, it is desired that a hypernetwork is trained using less number of samples while generalizes well to arbitrary trade-offs, relying on the smoothness of the hypernetwork. Otherwise, the training requires more number of iterations to sample sufficient number of preference vectors that covers the whole preference vector space to match the original performance, where its size grows exponentially to the number of tasks. Current manuscript lacks experimental or theoretical analysis on this generalization performance.\n\n-) Relating to the above point, I think evaluation measure HV and uniformity are not sufficient to evaluate the overall behavior. HV measures the quality of exploration, but higher HV does not necessarily mean that every model of method A dominates method B. For example, in the left figure of Figure 2, some points of EPO dominates PHN-EPO though HV is higher for PHN-EPO (Table 1). This implies that the proposed method reduces training time at cost of possible degrading movement of Pareto front. In other words, depending on the hyperparameter alpha used for the Dirichlet distribution, under the comparable training time to the baseline, the baseline method may dominate its PHN counter part at certain preferences.\n\n-) And therefore, I think one needs to investigate the accuracy plot (as Figure 2) together with HV and uniformity to better understand the behavior of the method. Some are missing in the current manuscript (e.g. NYUv2)\n\n-) Another concern is the scalability to the target model capacity. In the current manuscript, all the experiments are performed using architectures with small number of parameters (up to \\~0.37M of ENNet), which is much smaller than popular architectures such as ResNet50 (\\~26M). In addition, the required size of the hypernetwork grows at least proportional to the target model capacity,\nand the amount of training data and the training time likely increase accordingly.\n\n-) HyperNetwork require some overhead of memory and computation cost during the test time.\n\n-) Comparison with important prior work CPMTL is necessary.\n\nI would recommend 'accept'. Though I have concerns about the scalability and lack of analysis, I think this work has some advantages stated above.\n\nQuestions:\n1. Discussion on the generalization performance to unseen preference vectors\n2. Comparison with CPMTL\n3. Accuracy plot for NYUv2\n\n___\nThanks for the response. After reading the authors' response and other reviews, I would like to keep my recommendation.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The idea of learning the entire Pareto front is interesting. However, the organization and experiments need more elaboration.",
            "review": "Thanks for the efforts of the authors. After reading the response and other reviews,  I raise my score from 5 to 6. However, the proposed algorithm lacks analysis. So I cannot improve my score further. \n___________________________________________________\n\nSummary: \nThe paper proposes using a hypernetwork to learn the entire Pareto front of a multi-objective optimization.  It develops two approaches using linear scalarization and exact Pareto optimal (EPO), respectively. Experiments on several multi-task learning tasks show its advantage in terms of hypervolume and uniformity.\n\nStrengths: \n+ It is very interesting and useful that the paper tries to learn the entire Pareto front directly.\n+ The paper proposes using hypernetworks to learn the Pareto front for multi-task learning. \n\nWeaknesses: \n- Algorithm 1 is the main contribution. The pseudocode and its descriptions are not clear. For example, it is better to use two algorithms to descript PHN-LS and PHN-EPO, respectively. r is the weights for LS, while it is the ray for EPO. However, r should be pre-given. How and why PHN could optimize the multi-objective optimization for any r? What does “Dir(\\alpha)” mean?\n-PHN is a solving method for MOO problems. So the paper should verify that it could learn the entire Pareto front. The paper may test with MOO problems with known Pareto fronts. The results in Figure 2 cannot testify to this claim. This is more important than the experiments on the multi-task learning.\n\nMinor comments:\n1.\tThe legend in Figure2 should be colored.\n2.\tThe captions of the subfigures in Figure 4 should be under them. \n3.\tTypos:\nsampled “form” the m-dimensional-> sampled “from” the m-dimensional, “minimas”->”minima”, longer “then”-> longer “than”, “follwoing\" ->“following\"\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper with minor issues",
            "review": "The paper proposed a novel algorithm for MOO, which combines preference-based methods with hypernetworks, in order to encapsulate the preferences in the network input.\nThe paper is well written, the proposed method is clear, and the experiments are sufficient. \nMy concerns are the following.\n\nFirst, in the abstract you say that\n\"Recent optimization algorithms can target a specific desired ray in loss space,\nbut still face two grave limitations: (i) A separate model has to be learned for\neach point on the front; and (ii) The exact trade-off must be known prior to the\noptimization process. \"\n\nThis is not entirely true. In Reinforcement Learning (RL) manifold-based approach such as\n\nParisi et al, \"Multi-objective Reinforcement Learning through Continuous Pareto Manifold Approximation\"\nYang et al, \"A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation\"\n\ndo not need neither a separate model, nor to know the trade-off a priori. The work of Parisi et al learns a parametrization producing infinitely many solutions at the same time using a specific loss, while Yang et al train a neural network which takes preferences over the objectives as input, and generalizes over them. \nDespite being tested only on MORL, these algorithms can easily be extended to MOO.\nTherefore, I suggest you to rephrase your abstract and introduction to mention these methods.\n\nIn particular, the work of Yang et al is very reminiscent of your algorithm, since both include the preference vector in the network input. \n\nNonetheless, I feel the paper has enough novely thanks to the use of hypernetworks, EPO, and its application to MOO.\nThe evaluation is sufficient, but I would suggest to move the evaluation of evolutionary algorithms to the main section. Evolutionary algorithms are extremely popular in MOO and thanks to a wide variety of fitness functions can learn any frontier, even though they may require longer run time (as your experiments clearly show).\n\nFinally, a bit more analysis on concave frontier would be beneficial. In Figure 1 you show that LS fails in finding concave frontiers, but it seems to me that all experiments have convex frontier. To my understanding, EPO should address this limitation (\"The recent EPO algorithm can guarantee convergence to a local Pareto optimal point on a specified ray r, addressing the theoretical limitations of LS\"). Am I correct? A bit more discussion and experiments with concave frontiers would be a nice addition to the paper.\n\nOverall, I am leaning to accept it but the authors should address the above issues.\n\n**EDIT **\nThe authors have addressed my concerns, and I have increased my score.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Learning the Pareto Front with Hypernetworks",
            "review": "This paper tracks the problem of learning the entire Pareto front to allow the user to select a desired Pareto optimal solution by one inference procedure without retraining the model. The high-level idea is to learn the entire Pareto front simultaneously using a single hyper network, which receives as input the desired preference vector and returns a Pareto-optimal solution whose loss vector is in the desired direction. The paper gives an early trial to build a toolbox to allow users to get a desired solution by a single inference procedure.\n\nStrength:\n1.\tThis paper is an early trial to use a hyper network to directly approximate the Pareto Optimal front, allowing practitioners to flexibly choose Pareto solutions conditioned on different preference vectors.\n2.\tThe paper is well-written and a substantial number of experiments are conducted, promising good results of the proposed method. \n\nFeedbacks:\n1.\tMy major concern is that there is a similar work [1] that shares the same spirit with the work.  The authors might want to clarify the differences or conduct some experimental comparisons.\n2.\tIt is better to attach more details about the EPO algorithm.\n3.\tThere are some typos or grammar issues, such as 1) Page 6, the title of the subsection ‘Hyperparamter tuning’ should be ‘Hyperparameter tuning’. 2) Page 6, ‘We therefor’ should be ‘We therefore’. 3) Page 6, ‘as follow’ should be ‘as follows’. 4) Page 7, ‘are train and evaluate’ should be ‘are trained and evaluated’.\n\n[1] X. Lin, Z. Yang, Q. Zhang, and S. Kwong, “Controllable Pareto multi-task learning,” arXiv preprint arXiv: 2010.06313, 2020.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}