{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the effect of using unlabelled out-of-distribution (OOD) data in the training procedure to improve robust (and standard) accuracies. The main algorithmic contribution is a data-augmentation based robust training algorithm to train a loss which is carefully designed to benefit from the additional OOD data. What's also interesting is that the OOD data is fed with random labels to the training procedure. As demonstrated in the theoretical results, this way of feeding OOD data helps to remove the dependency to non-robust features and hence improves robustness.  \n\nAs pointed out by all the reviewers (which I agree with), the idea of using unlabelled OOD data at training is novel/interesting, and the paper also shows how this can be done algorithmically. The numerical results also confirm the effectiveness of the proposed methods. "
    },
    "Reviews": [
        {
            "title": "Official Blind Review #2",
            "review": "In this work, the authors propose to use out-of-distribution (OOD) data to improve the generalization of deep neural networks, especially against adversarial attacks. Theoretic analysis and experimental results demonstrate the effectiveness of such method. The idea is interesting and the paper is easy to follow. The experiments are also thorough and support their theoretic analysis.\n\nHowever, I still have some concerns below:\n\n1.\tUnlabeled data [1,2,3] seems to be related to your work more than you claimed. And you missed two related works of unlabeled data. IPlease explain more on the difference between your method and previous works on unlabeled data and provide a more detailed comparison.\n\n2.\tYou adopt OOD data and assign each data with uniform distribution label. I am curious on what would happen if your OOD data contain some data that is overlapped with the data in training set. Would it still work well?\n\n[1] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp. 11190–11201, 2019.\n\n[2] J. Uesato, J. Alayrac, P. Huang, R. Stanforth, A. Fawzi, and P. Kohli. Are labels required for improving adversarial robustness? In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\n[3] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John E. Hopcroft, Liwei Wang. Adversarially Robust Generalization Just Requires More Unlabeled Data. arXiv Preprint arXiv: 1906.00555.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Marginally below acceptance threshold",
            "review": "This paper proposes a new data augmentation method that utilizes out-of-distribution data for enhancing generalizability for both supervised and adversarial learning. While most of existing data augmentation methods explore auxiliary unlabeled in-distribution data, this paper tries to leverage out-of-distribution data for enhancing performance. Theoretical analysis is first presented and explains why out-of-distribution data can help. Then, a simple method motivated from the analysis is proposed and later verified by extensive experiments including both supervised and adversarial learning experiments. \n\nThe proposed method is technically sound and extensive experiments are conducted to verify the effectiveness of the proposed method in different datasets for both the supervised learning and adversarial learning tasks. The paper is well motivated and well written. \n\nI am not familiar with the topic. My main concern is the technical contribution of this paper. It is straightforward that labeled data, though are out-of-domain, can enhance the capability of the feature extractor as the low layers of deep neural networks extract low-level semantics that are shared across images. There is no doubt about this. The paper provides some analysis on this and proposes a simple method which combines in-distribution and out-of-distribution data to train the model. The combination approach is very straightforward as well. I am not an expert in this area, but from an educated guess. I do not think this paper is good enough for acceptance.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Neat idea! But still has a concern. ",
            "review": "**1. Summary and contributions: Briefly summarize the paper and its contributions**\n In this work, authors looked at Out of Distribution (OOD) data from the data augmentation and regularization perspective and introduced Out-of-distribution data Augmented Training (OAT) based on their theoretical analysis which demonstrated that training with OOD data can remove undesirable feature contributions in a simple Gaussian model. The authors conducted experiments on both standard learning and adversarial learning and showed the effectiveness of OAT with strong results. Experimental results also imply that a common undesirable feature space exists among diverse datasets.  \n\n##########################################################################\n\n**2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.**\n \nVery strong experimental results, it is clear from the results that OAT is effective for both standard learning and adversarial learning. \n\nA sound theoretical analysis demonstrating that training with OOD data can remove undesirable feature contributions in a simple Gaussian model. \n\nNeat randomization test that analyzed the effect of OAT for standard learning. It verified the claim that OAT regularizes the model to learn only features with a strong correlation with class labels, even though the generalization gap is rather small.  \n\n##########################################################################\n\n**3. Weaknesses: Explain the limitations of this work along the same axes as above.**\n \nIn the middle of page 6: “In particular, from the results against AA it can be seen that the effectiveness of OAT does not rely on obfuscated gradients (Athalye et al., 2018).” It would be helpful to explain this a bit more.\n\nAdversarial training on OOD data could be more clearly described before the theoretical analysis. How exactly are you using the OOD data? Maybe switch 3.1 and 3.2? Then it will be easier for the readers to follow the theoretical analysis. I think it will improve readability if Equation 9 is introduced earlier in the paper. Why not introduce OAT earlier?\n\nFor the rest of the writing issues, see the next section. \n ##########################################################################\n\n**4. Clarity: Is the paper well written?**\nTypos: \\\nPage 2: a high success rates\\\nPage 2: a human-an ability thought to be\\\nAmbiguity: \\\nTitle: The word “remove” first appeared on page 4, the middle of the paper. Even though the paper conveyed the idea in the title, consider either change the title or more explicitly introducing the ideas in page 4 in the introduction section to improve readability. \\\nPage 3, The space X was never explicitly defined. \\\nPage 3, it’d be good to remind readers what is u.a.r\\\nPage 8, “can lead to higher performance by implementing existing data augmentation methods.” Did you mean when mixed with existing data augmentation methods?\\\n##########################################################################\n\n**5. Reasons for score**\nIn conclusion, the ideas are very interesting and the strengths outweigh weakness by a large margin, so I would recommend accept. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary: This paper seeks to remove \"non-robust\" features by first hypothesizing that out-of-distribution image datasets share the same spurious features as the target data distribution, then performing adversarial training on the out-of-distribution data (against uninformative labels) to remove these spurious features. They apply the method to PGD and TRADES adversarial training and show empirically that it improves the clean and robust accuracies. When unlabeled in-distribution data is available, the method can be combined with robust self-training to get further gains.\n\nStrengths:\n- The method seems to be reasonably robust to the choice of OOD dataset, which makes the method quite flexible.  \n- The method gives consistant and substaintial gains beyond PGD and TRADES adversarial training, as well as robust self-training, when OOD data is used. This is useful particularly since robust self-training (RST) doesn't seem to work well with OOD data (filtering unlabeled TinyImages data to be more like CIFAR was crucial to RST's success).\n- They test against a variety of attacks, including PGD with 100 steps, CW, and AA attacks. \n\nWeaknesses: Given the generality of using OOD data, there should be some things to check to make sure the given intuitions are correct.\n\n- To perhaps give a bit more supporting evidence that the OOD datasets help due to sharing the same non-robust features, some ablations could be done - for example, what if you use a synthetically generated OOD set (like Gaussian noise)? I think that random OOD examples could also help because it can provide a regularizing effect - recent works [1,2] show that generic regularization such as dropout or early stopping improve adversarial training methods. It's unclear to me that the benefit is necessarily from removing non-robust features. \n- The toy theoretical model seems to rely on the fact that the OOD dataset specifically only contains nonzero mean features for non-robust components, essentially allowing the algorithm to differentiate the non-robust features and robust features. However, it's unlikely that real-world OOD datasets have \\emph{only} the non-robust features. \n- How much does the choice of OOD dataset matter - why was SVHN, Fashion etc used for CIFAR while Places, Visda only used for ImageNet? Does the method work for all pairs of datasets? \n- If you combine all the OOD datasets, do we get even more gains? Why do we choose a particular OOD dataset rather than just using whatever images we can find?\n- From Table 1 and Table 2, it seems that OAT tends to further degrade the clean accuracy of PGD, although it slightly improves the clean accuracy of TRADES. \n\nOther: \n- It's pretty unclear throughout what is mean by the notion of \"undesirable features\", which also seems to be used synonymously with \"non-robust features\".\n- Can you use in-distribution unlabeled data with the OAT loss, and if so do we get a similar benefit?  \n\n\n[1] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, Kamalika Chaudhuri. A Closer Look at Accuracy vs. Robustness, 2020.\n[2] Leslie Rice, Eric Wong, J. Zico Kolter. Overfitting in adversarially robust deep learning, 2020.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}