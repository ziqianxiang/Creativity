{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work combines normalizing flows with conditional sampling. While there are connections to other works, the paper seems novel and applicable, and has nice experimental results. The authors did a good job clarifying the reviewers questions, and have addressed their major concerns. We appreciate the additional analyses added to the paper."
    },
    "Reviews": [
        {
            "title": "A slight variation on standard stochastic EM algorithms",
            "review": "Summary\n\nThis paper proposes a stochastic expectation--maximisation (EM) algorithm. The main idea is that the target distribution is specified as a deterministic mapping, a.k.a. a normalising flow, from some simple \"base\" distribution.\n\n\nStrengths\n\nThe algorithm appears to be formally correct (in the sense that it is a standard stochastic EM algorithm). The method is demonstrated on a large number of examples.\n\nWeaknesses\n\nThe proposed algorithm is just a standard Metropolis--Hastings (MH) update interspersed with a stochastic EM update for the parameters of the target distribution. This does not seem novel; such algorithms have been around for decades.\n\nThe authors should explain why they need for extending the space to include $y_O$ instead of just mapping $\\xi$ to $y_M$.\n\n\nMinor comments\n\n- There are a number of typos in the bibliography mostly related to inconsistent use of capital letters in article titles and journal/conference names.\n\n- What does the semi-colon in $p_{f,\\theta}(x_M; x_O)$ mean? Why not use a comma if this is meant to be a joint density?\n\n- In Section 3, it would be helpful to write the (extended) target distribution of the Metropolis--Hastings algorithm down explicitly and formally.\n\n- Within LaTeX's maths mode, you cannot just write operators $min$ and $Uniform$ (LaTeX treats this, e.g., as multiplying $m$ by $i$ by $n$ which leads to the wrong spacing).",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper on missing data completion with normalizing flows, some questions",
            "review": "I enjoyed reading this paper and the idea of combining normalizing flow density models with conditional sampling seems natural, useful and has interesting potential applications to missing data problems.\n\nThings that were not clear to me are the following:\n\n1. I don't understand what this \"pre-trained\" model being referred to is. Is this a generative model for the entire data set? Do you mean that you get the model $p_{f, \\theta}$ from somewhere already and not learn it yourself?\n\n2. The choice of $q$ is one of the more interesting parts of the proposed algorithm that deserves to be studied in greater detail! If we don't use $q$, wouldn't the algorithm still work? You should try a version of your algorithm without assuming any $q$ on $y_O$, then checking how much does it affect performance.\n\n3. I am not convinced that it is fair to set \"q to be an independent normal distribution centered on the conditioning values $x_{O}$\". Clearly, with sufficiently low variance for $q$ it is possible to force the chain to sample values of $y_{O}$ that are very close to $x_{O}$. Since the dependence between $y_{O}$ and $y_{M}$ is generally strong, this will have you perform good reconstruction of $x_{M}$. If you do use a $q$ you should try variances other than the (1e-03)^2 which you have used as mentioned in the Appendix.\n\n4. Is it expensive to compute the probability density for the Metropolis correction? If you don't do a Metropolis correction, do the conditional samples still look reasonably good?\n\n5. Somewhat related to 2). In section 6.3 figure 7 the acceptance rate for PL-MCMC looks really low and is continuing to drop. You should probably run the chain for even longer until the acceptance rate plateaus completely and see what the samples at this point look like. Or perhaps you should decrease your proposal scaling.\n\nI am willing to upgrade my review if particularly point 2,3 is addressed.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Review 4",
            "review": "Summary:\nThe submission introduces an approach for conditional sampling by running a Random walk Metropolis algorithm on the latent space as a pullback of normalizing flows. The normalizing flows are learnt via a MC-EM approach for incomplete training data. The method is illustrated on numerous experiments sampling missing parts for image and UCI datasets. \n\nPositives:\nThe combination of normalizing flows and MCMC sampling for conditional sampling is new as far as I am aware and is an interesting approach. Qualitative experimental results on different data sets seem promising with quantitative results indicating that it performs competitively (better reconstruction RMSE compared to MisGAN, closely matching the performance of MIWAE). More advanced MCMC samplers (such as gradient-based ones) could be used in the proposed framework, potentially yielding better experimental results.\n\nNegatives:\nThe approach relies on hyperparameters such as the choice of the auxiliary data density q and the MCMC proposal density g, and I feel that choices for these should be analyzed in greater detail in the paper. For example, the choice of both q and g affect the exploration of the target and so do they affect the sampling quality in practice for a reasonable number MCMC steps? Likewise, it is well documented that adaptive MCMC methods can converge to a different target and I am not yet convinced that the MC-EM approach approximately maximizes the complete training data likelihood without some restrictions on the MCMC adaptation strategy.\n\nRecommendation:\nI am recommending a weak accept, since the approach is interesting and is supported by many experiments, however, the hyperparameter choices for the adaptation should be better analyzed.\n\nComments:\nHow do you choose the perturbation scales for the random walk proposals?\nCould it be that the target has different scales across different dimensions, so that the MCMC kernel explores only some dimensions well and does this affect the learning process?\nDo the acceptance rates have a large influence on the MC-EM training? My intuition would be that for high acceptance rates, the learning process would be more volatile (the training samples Xâ€™_train would have many same entries and some very different ones)?\nWhy do the acceptance rate go to zero in Figure 7?\nWhy do you make the approximation of the Metropolis-Hastings ratio in the experiments? Is the proposal not just a simple 2-mixture density which should not incur a much higher computational cost?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}