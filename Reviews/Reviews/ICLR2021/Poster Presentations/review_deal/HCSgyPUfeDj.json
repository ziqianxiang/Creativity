{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper investigates the one-class classification problem, proposing to learn a self-supervised representation and a distribution-augmented contrastive learning method; thorough results and analysis show that the method is effective and backs up their claims in terms of the underlying mechanism for why it works. In general, reviewers thought the paper was well-written, well-motivated/argued, and presents a thorough related work comparison and experimentation, though the novelty was found to be somewhat low. Several reviewers brought up some possible weaknesses in terms of demonstrating uniformity of the representations as well as suggesting additional datasets. Through an interesting discussion, the authors provided additional visualizations and results on the Mvtec dataset. This further bolstered the arguments in the paper. \n\nOverall, this is a strong paper with a clear argument and contribution, and so I recommend acceptance. "
    },
    "Reviews": [
        {
            "title": "Official Blind Review",
            "review": "Summary: \nThey investigated the effectiveness of self-supervised learning (SSL) for one class classification (OCC).  \nHere is what I think are contributions relative to existing literature - \nEmpirically improved AUC for multiple OCC datasets, here are the techniques that were useful - \nUsed “distribution augmentation” [DistAug] for learning representation for OCC, and in ablation studies show DistAug leads to  improvement over standard augmentation\nUsed KDE and OCSVM on top of learned representation, and showed improvement over using the classification head training during SSL\nUsed a smaller batch size (=32) \nUsed a MLP head during SSL\nThe authors also included a section of visualizing explanation using existing techniques to illustrate how their method leads to more reasonable decisions.  \n\nStrength:\nThe paper is well written. I appreciate the clarity, and good coverage of the current literature.\nThe ablation studies are thorough, which make the empirical improvement solid. \n\nConcerns:\nThe uniformity argument is weak. The authors state the empirical improvement on OCC using their method hinges on the DistAug technique, which is motivated to reduce the uniformity of the learned representation. When achieved the inliers will live in the dense regions on the hypersphere, and outliers will live on the non-occupied region.  This assumes all the test inputs are projected onto the hypersphere, including the outlier.  From my understanding, the authors used f() for OCC, not \\phi() which is the normalized (i.e. hypersphere) output. In this case, there are many ways that OCC can be achieved even if \\phi() of the training inputs are uniform on the hypersphere.  Suppose  both the inliers and outliers after f() live on hyperspheres, just with a different radius, then  after normalization they can both be uniformly distributed  on  the  same hypersphere.  \nOne question is if there is a difference in using f()  or \\phi() for OCC. \nFurthermore, the authors try to back this claim up using Figure 4, but I cannot  seem to connect the dots here. \nThey authors used MMD to a uniform distribution to measure how uniform the representations are.  The less uniform (i.e. higher MMD), the better it should be for OCC.  The  correlation between MMD and AUC does not  seem  to be very strong.  E.g., for the (DA) gf variant, the 2 metrics actually  seem negatively correlated.  \nThis again, makes me wonder if “less uniformity” really is why their technique led to an improvement in OCC. \nIf this is not why, then we should find another explanation for why there was  an improvement. \nThere is always the concern that the improvement comes from extra hyperparameter tuning. Did the author also tune for good hyperparameters for the non DistAug version as described in A.3?  \n\n\nOverall, a fairly thorough empirical investigation into better techniques for using SSL for OCC.  It can be a decent contribution along the lines  of one of the  “improve techniques …”  papers if the above concerns can be addressed.  In fact, I think not focusing on selling DistAug, but really identifying what contributes to  the gain empirically makes this  paper stronger. \n\n\nReferences:\n[DistAug] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Distribution augmentation for generative modeling. In Proceedings ofMachine Learning and Systems 2020, pages 10563–10576, 2020.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of \"Learning and Evaluating Representations for Deep One-Class Classification\"",
            "review": "This paper presents a two-stage representation learning approach to deep one-class classification.\n\nIn the first stage, a mapping f to a versatile high-level latent representation is learned using self-supervised learning for a contrastive learning proxy task. In the second stage, the same mapping f is used to map the data to the latent space, whereafter a traditional one-class classifier such as OC-SVM or KDE, is applied. \nIt is shown that the one-class task puts somewhat different requirements on the representation than with a multi-class classification task, both 1) in terms of uniformity of the data points in the representation, which is desired for multi-class tasks but not fully beneficial for one-class tasks, and 2) in terms of minimizing or maximizing the distance between different instances of the negative class - for multi-class tasks you want the distances maximized, while for one-class tasks you want the negative (inlier) examples close together. 1) is addressed by using smaller batch sizes in training while 2) is addressed by distribution augmentation that will render a compact inlier distribution in the representation.\n\nThis paper is overall a good paper that will be interesting to a certain audience at ICLR. \n+ It is well written, well motivated, with a clear argument and as far as I can see, technically correct. \n+ The experiments are well designed, valid and exhaustive, with comparison to a range of baselines as well as an ablation study. \n+ Moreover, the visual explanation of what the different representations have focused on is highly interesting.\n+ I appreciate the comprehensive grounding of the contribution in both new and old related work. The reference list contains all the relevant state of the art, as well as references to more classical work such as [13,14,29,47,53].\n\nThe paper is not highly seminal, but more incremental in nature, putting together and modifying existing methodology. However, since it is very well done, the work is absolutely worth acceptance. \n\nA criticism is that there are some repetition in the line of argument, for example between 2.1.2 second paragraph and 2.1.3 first paragraph. A more compact, e.g., section 2.1 would render more space for results which now have been pushed to the appendix to a large degree. Another suggestion for improvement could be to indicate more clearly in figure 1(b) that f is kept fixed in this step. This could be done e.g. with a different color of the f box in figure 1(b).\n\n ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "state-of-the-art one class classification with representation learning and classical 1-class models",
            "review": "This paper proposes a framework for deep one-class classification (an example application being anomaly detection).  The basic idea is to combine self-supervised representation learning (eg through a proxy task such as rotation prediction or contrastive learning), with a classical approach to one-class classification, such as one-class SVM or KDE.  This is in contrast to existing methods for deep one-class classification that use simulated outliers to form a surrogate classification loss and then train end-to-end.  The paper further improves on the first stage of representation learning, by introducing modifications to contrastive learning to make it more appropriate for one-class classification.  The main insight is to introduce distribution augmentation, where geometric transformations of images, such as rotation, are treated as separate instances, to be separated from the original view.  This is motivated from the perspective of reducing uniformity of the inliers across the unit hypersphere, to allow for better separation from outliers.\n\nPositives:\n+ strong empirical results, with improved performance over existing methods for one-class classification\n+ validation of two stage framework, by showing improved performance with RotNet representation with KDE detector versus RotNet end-to-end [20]\n+ validation of improvements to contrastive learning for one-class classification, such as distribution augmentation, batch size selection, use of MLP project head\n\nMinor negatives:\n- I think the paper would flow a little better if the related work section was moved earlier in the paper, rather than coming only after the detailed description of the method.\n- In describing distribution augmentation and contrasting it with standard data augmentation for contrastive learning, it is clarified that the two sets of augmentations are disjoint.  I would it have found it helpful if the paper was explicit about which data augmentations were used for the contrastive learning, as this did not seem to be stated in the paper.\n\nOverall I found this to be a nice paper with strong empirical results.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Data-augmented self-supervision with contrastive learning for anomaly detection ",
            "review": "This paper proposes an anomaly detection approach that has two stages: a first stage for learning a feature representation and a second stage to train either a one-class classifier based on OC-SVM or KDE.  The main contribution of the paper is the feature representation learning that relies on contrastive learning to optimise a self-supervised loss function which minimises the distance of the samples from the same image augmented with different data augmentation functions and maximises the distance of samples from different images augmented with the same augmentation functions.  The data augmentation functions used were horizontal flip and rotation (0,90,180,270).  Results on the public datasets CIFAR-10, CIFAR-100, Fashion MNIST, and Cat-vs-Dog show that the proposed method has better anomaly detection (measured with AUC) than the state of the art.  The paper also displays qualitative anomaly detection results and an ablation study that shows: a) how close to uniform distribution (on hypersphere) the feature representations are as a function of batch size, and b) how AUC is affected with batch size and depth of MLP project heads.\n\nThis paper has outstanding results on the datasets CIFAR-10, CIFAR-100, Fashion MNIST, and Cat-vs-Dog, but it is missing result on a challenging dataset, such as Mvtec [51].  It is also missing results on anomaly localisation (e.g., Venkataramanan, Shashanka, et al. \"Attention Guided Anomaly Detection and Localization in Images.\" arXiv preprint arXiv:1911.08616 (2019)), so it scores slightly below acceptance for results given that it is hard to assess how the method would perform in a more realistic anomaly detection problem.  In terms of the proposed method, it is quite similar to [20,21], with the difference that it uses more data augmentation functions and rely on contrastive loss.  Therefore, it scores slightly below acceptance on novelty as well.  \n\nOne argument that seems contradictory is the one for class collision and uniformity.  In particular, if pre-training forces all inlier samples to be on a hyper-sphere, wouldn't it be advantageous to have a uniform distribution given that outliers could be easily detected as not lying on the hyper-sphere?  Of course, this would probably require a change in the OC-SVM classifier.  Can the authors comment on that?\n\nAlso, the argument on Sec. 2.1.3, on the effect of projection heads, says that \"I(g(f(x));x) <= I(f(x);x), so f can retain more information than g, thus more suitable for downstream tasks that are not necessarily correlated with the proxy tasks\".  If we push this argument, then I(f(x);x) <= I(x;x), so we should use x for downstream tasks.  Can the authors comment on that?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}