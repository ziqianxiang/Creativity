{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the problem of unsupervised domain translation. Here translation does not refer to language translation. Instead, it refers to the idea of transferring high-level semantic features. Specifically, the authors look at digit style transfer (between MNIST/postal address numbers and SVHN/street view house numbers) and Sketches to Reals. The visuals look very convincing and the empirical results are strong, too. There is one weaker review but the authors address the concerns in their response and the reviewer did unfortunately not respond despite promting."
    },
    "Reviews": [
        {
            "title": "Nice Results using Domain Invariant Categorical Semantics to Improve UDT",
            "review": "Summary: The authors use Domain Invariant Categorical Semantics to improve unsupervised domain translation (UDT). They learn these semantics in an unsupervised manner. They show how this can improve results on Semantic Preserving Unsupervised Domain Translation and Style Heterogeneous Domain Translation by doing experiments on MNIST<->SVHN (features traditionally learned are very different but digit identity could be the same) and Sketches->Reals (distinct styles) respectively.\n\n---\n\nStrengths:\n- The visual results on both tasks/datasets are striking.\n- The paper is simple to read and the idea is intuitive.\n- Experiments are extensive, including an ablation on the losses and comparison against baselines.\n\nWeaknesses: \n- More examples of Sketches->Real could have been shown;\n- When does this method fail? Some failure cases would be good\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting and original idea",
            "review": "The paper addresses the domain translation problem and proposes a novel approach to translate images between domains in an unsupervised manner, by integrating unsupervised learning of domain-invariant semantic features between the two domains. The paper is well-written with a clear standing point and motivation, along with well-described contributions. The paper has an inclusive and sound theoretical comparison to related work. Evaluation is well-designed and includes previous work in the same context.\nIn overall, it is a good paper with an original and potentially inspiring idea and a convincing application of conditional GANs, would be an interesting read for many in the conference.\n\nTypos\nPage 1: \nshowing that such a translation\ntwo situations where unsupervised domain translation\nPage 7: \nthe representations of the real images\n\nComments\n1. 'unsupervised domain translation methods work due to an inductive bias toward identity' in sections 1 and 3.2: need citation to support this statement\n2. Considering clustering is one of the important parts of the contributions it may need more emphasis in the paper, it would be nice to restructure this part, extend the discussion of alternatives and an analysis to compare different approaches (also by moving some part of the discussion to the paper from the appendices).",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting work but lacking promising results ",
            "review": "Summary: This paper proposes to learn the categorical semantic features in an unsupervised manner to enhance the ability of image translation at preserving high-level features, and obtains some good results on Semantic-Preserving Unsupervised Domain Translation and Style-Heterogeneous Domain Translation problems. \n\n\nMajor issues: \n- The proposed method seems to be a combination of current works. The main contribution of this work may be leveraging the unsupervised representation learning for semantic features extraction.  \n\n- The quality of generated images is still not satisfactory with such rapid development of GANs.\n\n- The experiment and qualitative evaluation are too limited. Only two image translation tasks are conducted for comparison, and little visual results are given. It will be preferred if some common I2I tasks results are given. Only FID is used, adding other metrics, such as LPIPS, NDB, and JSD, will be more convincing. \n\n\nMinor issues\n- What are the essential differences between SPUDT and SHDT problem? How does the model solve the two problems according to their differences?\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "This paper presents unsupervised domain translation (UDT), considering two scenarios: Semantic Preserving Unsupervised Domain Translation (SPUDT) and is Style-Heterogeneous Domain Translation (SHDT). This study uses MNIST and SVHN datasets for demonstrating SPUDT and Sketches and Reals samples from the DomainNet dataset for demonstrating SHDT. Although the method uses different components depending on the scenario (or dataset), the presented architecture is essentially the same.\n\nThe method consists of the framework for learning invariant categorical semantics across domains (Section 3.1) and semantic style modulation to make SHDT generations consistent (Section 3.2). Section 3.1 consists of unsupervised representation learning, clustering, and unsupervised domain adaptation. However, this section did not describe the first two components in detail probably because the method uses different components depending on the datasets (described in Section 4). Unsupervised domain adaptation is realized by the minimization problem described in P4. Section 3.2 describes content encoders, semantics encoders (obtained in Section 3.1), mapping networks, style encoders, and generator. The section presents the loss functions (adversarial loss, cycle-consistency loss, style-consistency loss, style diversity loss, and semantic loss).\n\nSection 4 compares the proposed method with UDT baselines under SPUDT and SHDT scenarios. The experiments on MNIST and SVHN show that the proposed method achieved high accuracy in domain translation and high quality in the generation (measured by FID). The experimental results on Sketches and Reals show that the proposed method yields high-quality generations. Overall, the experiments demonstrate the importance of incorporating the semantics category in UDT.\n\nPROS\n\nIt was interesting to see that the performance of UDT was drastically improved by incorporating semantic categories induced by the clustering algorithms.\n\nThe presented method was reasonable and well designed.\n\nCONS\n\nThe main part of this paper is dense. I had to go back and forth between the main body and the appendix a lot.\n\nThis paper does not seem to emphasize the technical novelty of this work. Although Section 3.1 was designed to present a novel framework, it does not explain the detail of the components. Methods for unsupervised representation learning and clustering are explained in Section 4. The component of unsupervised domain adaptation may be a new proposal in this work. However, this paper did not explain this component in the main body; I had to read Appendix B.5 to understand the formula presented in P4.\n\nThe model architecture described in Section 3.2 is mostly based on previous work (e.g., adversarial loss, cycle-consistency loss, etc.). The semantic loss presented in this section may be a new addition to UDT. But again, this is not emphasized in Section 3.2.\n\nQUESTIONS\n\nP4: \"We describe those regularizers in more detail in Appendix B.5.\"\nIf this part is a proposal in this paper, it should be explained as the main content of this paper. If I do not read Appendix B.5., I have no idea how domain 2 is involved in the minimization problem.\n\nMINOR COMMENTS:\n\nP1: \"That are mappings biased toward the identity.\"\n\"That\" -> \"Those\"\n\nIt may be convenient to have equation numbers so that reviewers/readers can locate a formula directly.",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}