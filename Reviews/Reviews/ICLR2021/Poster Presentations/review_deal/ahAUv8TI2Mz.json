{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The submission combines meta-learning and attention mechanism for generalised zero-shot learning. The image-guided attention on the semantic space helps to adapt the better class specific semantic information while separate experts operate on the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta-learning based training adapts the model to few-shot learning scenario. The submission has received two accept, two weak accept and one weak reject reviews. All reviewers found the methodology interesting but they found it moderately novel. The experimental evaluation has been found strong. The rebuttal addressed all the reviewers' concerns and during the discussion phase all reviewers recommended acceptance. The meta reviewer follows the consensus of all the reviewers and recommends acceptance."
    },
    "Reviews": [
        {
            "title": "Adaptive and Generative Zero-Shot Learning",
            "review": "The paper proposes a framework for the GZSL using the meta-learning and attention mechanism. The image-guided attention on the semantic space helps to adapt the better class specific semantic information. The modified semantic space projected to the visual space and in the visual space, cosine similarity is measured. The paper learns separate expert for the seen and unseen classes. The unseen class expert is trained with the pseudo negative samples with pseudo negative labels. Meta-learning based training helps to learn the model when only a few examples per class are available.\n\nComment: \n1: The adaptive modification of the semantic space is novel in the ZSL setup with clear intuition. Also, the mixup idea in the ZSL setup seems to work well, it helps to separate the seen and unseen class data hence choosing the seen/unseen expert is easy. \n2: The paper shows a strong result on the standard ZSL dataset. The ablation over the various component shows the efficacy of the proposed component. \n\n3: How meta-learning is used in the paper is not clear also in the implementation details it's not mentioned. What is the task definition? How meta-train and meta-test are defined? The paper mention they use meta-learning, but nothing is clear about this. Therefore reproducibility is the main issue, and I think using the provided information the reported result can not be reproduced. Please update the experimental setup and provide the detail so that result can be easily reproduced.\n\n4: There are a few recent works [a,b,c,d] that explore the meta-learning framework for the ZSL setup. The author should mention these paper and add the advantage of the proposed model over the other meta-learning approach also these result should be compared. Note: [d] is mentioned in the paper. Also please add an ablation if the model has advantage using the meta-learning, i.e. what is the result of the proposed component if we use the simple network without meta-learning.\n\n[a] Meta-Learning for Generalized Zero-Shot Learning, AAAI-20\n[d] Episode-based prototype generating network for zero-shot learning, CVPR-20\n[b] Learning to Compare: Relation Network for Few-Shot Learning, CVPR-18\n[c] Correction Networks: Meta-Learning for Zero-Shot Learning, ArXiv\n\n5: Reproducibility is the main challenge here I request the author, please provide the code for the CUB dataset (since its look too high, I suspect the result in the inductive setting).\n\n6: In the table-1, what is T for the ZSL result? Is it transductive setting?\n\nOverall I like the idea, and paper shows a good result. Reproducibility is the main challenge, and CUB results look too high, I like to verify the result, therefore requesting the author to submit the code for the CUB dataset. Also, please discuss the suggested paper that is also using meta-learning based training. I will update my score on the successful verification of the code and result and the updated paper quality.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "This paper describes a method for GZSL using a combination of adaptive and generative techniques to create virtual classes that help account for unseen classes.",
            "review": "Due to time shortage this will be a short review. I have gone through the paper carefully.\nMotivation\nThe authors motivate their work well and have a thorough literature survey.\nMethod\nThe authors essentially create clusters that can anticipate unseen classes. That idea is not new in and of itself but the authors' realization of the idea through mix-ups and image-adaptive semantics is new and interesting. The overall approach is technically sound and each component is well motivated and described.\n\nResults\nThe results are convincing. The authors get an across the board improvement over the state of the art.\nClarity\nThe paper is clearly written and has a good logical flow. I would recommend not using adjectives like elegant and insightful for one's own work. Perhaps such assessments are best left to the reviewers.\nQuality, originality and significance\nThe paper presents moderate innovation in my view. It is thorough and hence does deserve consideration. In short, good quality, reasonable originality and decent significance.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Review of \"Adaptive and Generative Zero-Shot Learning\"",
            "review": "I. Summary\n\nThe authors consider (generative) zero-shot classification. Their approach, combines two main aspects: (1) generating \"virtual\" classes by mixup interpolation and (2) they introduce an attention mechanism, dubbed \"image-attentive attention\" to allow their approach to model both intra- and extra- class variations.\n\nII. Strong and weak points.\n\nPositive: \n- Strong results: overall the results are good. Yes, the paper may not account for the latest crop of papers from e.g. CVPR but I feel that is besides the point as these methods are structurally different.\n- Relatively extensive evaluation: there is an ablation study, and a distinction over whether the image encoder is fine-tuned.\n- Writing is clear, paper is reasonably easy to follow.\n\nNegative:\n- A large part of the improvement stems from the virtual classes, which is essentially mixup applied to ZSL. This is somewhat derivative.\n\nIII. Rating and questions to authors\nDespite the negative comment I mentioned, I feel the paper should be accepted. How does the paper work with actual generated samples as in the GAN/VAE-based models? Do these extra samples work well with the virtual samples? Is the information redundant/complementary?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Please find below for the detailed comments",
            "review": "This paper solves ZSL and GZSL problem by an elegant fusion of adaptive and generative learning. Different from previous generative models that synthesize unseen samples for training the model, authors create virtual classes as unseen classes in the training phase. In addition, four standard GZSL datasets demonstrate the effectiveness of the proposed method. \nStrengths:\n1. The motivation and the contribution are clearly presented in this manuscript. In general, this paper is well-written.\n2. Instead of generating synthetic data of unseen classes, authors propose to yield virtual classes and data by mixup interpolations.\n3. The architectures of IAS and S2V are simple and effective in terms of experimental results.\nWeaknesses:\n1. Parameter settings for the network are unclear.\n2. More generative ZSL methods from CVPR’20 or ECCV’20 should be compared.\n3. More experimental results such as a tsne illustration for generated virtual classes should be given.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An interesting approach for zero-shot learning leveraging the instance attention and synthetic data to enhance the unseen classifier",
            "review": "Summary: The authors proposed an interesting method for zero-shot learning. In particular, the authors adopted an attention mechanism from the input feature in the semantic to visual mapping, to introduce intra-class variations in the visual space. They also propose a process to synthesize \"fake\" class representations such that a classifier for unseen classes can be trained.  Combining these two the authors demonstrated significant results on benchmark zero-shot learning datasets.\n\nReason for score: I think this paper is marginally below the acceptance threshold. It proposed an elegant method which produces state-of-the-art results on benchmark datasets. But more details need to be revealed to help readers understand the method (discussed below), and I hope the authors can address my concerns during the rebuttal.\n\nPros:\n1. The paper proposed an interesting attention-based method to introduce intra-class variations to the representations for each class. \n\n2. The authors proposed to synthesize virtual classes to enrich the data for training an unseen classifier.\n\n3. The extensive experiments on the benchmark datasets illustrate the superiority of the proposed method.\n\nCons:\n1. The overall training scheme is not very clear and I hope the authors can clarify them. For example, what is the order of training the seen/unseen experts? Do they share any component, like the IAS or S2V module?\n\n2. Baseline explanation is missing. In Sec. 4.3, the authors demonstrate the ablation study where the baseline model does not have IAS or the virtual classes training scheme. How is this baseline implemented?  Without the virtual classes how would you train the unseen expert? My understanding is that without the IAS the model reduces to pure S2V and without the virtual classes the model can only train on seen classes, in which the entire model reduce to a single cosine similarity classifier in the visual feature space (given the seen features and semantics). Then how do you leverage the seen/unseen expert inference strategy? I am curious about it because this baseline reaches considerably high performance in Table 2, in fact already outperforms many state-of-the-art methods in Table 1. It is kind of shocking to me that such a naive baseline contributes this level of accuracy improvement, if I didn't misunderstand it. I would be appreciated if the authors can deliberate this part, so readers can understand the contribution of each module in the proposed method.\n\n3. Why is seen expert detecting the abnormal? In Sec. 3.3, the authors claim that \"Such optimization (the cross-entropy) can be thought of as one-category anomaly detection over only seen classes\". During training, since the seen expert has no data for unseen classes, the classifier is easily biased to the seen classes, even the new data is from an unseen class in the inference time. That's one of the reasons why many embedding based zsl methods have very poor generalized zero-shot learning performance [Xian et al, cvpr 2017]. Could authors deliberate more on why this specific optimization (the cross-entropy on the seen) can address this issue, such that the model can work as a anomaly detector and predict higher score for unseen classes when unseen example is input? It would also be helpful if the authors can demonstrate the predicted scores of the unseen examples empirically.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}