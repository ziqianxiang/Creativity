{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a new, large-scale, open-domain dataset for on-screen audio-visual separation, and provides an initial solution to this task. As the setting is quite specialized, the authors proposed a neural architecture based on spatial-temporal attentions (while using existing learning objective for audio separation). The reviewers were initially concerned that, while reasonably motivated, the architecture seemed some arbitrary. The authors then provided extensive ablation studies to evaluate the significance of each component with existing datasets, and these efforts are appreciated by reviewers. The authors may consider re-organizing the paper and moving some ablation studies to the main text. On the other hand, the reviewers believe that the dataset will be very useful for the community due to its diversity in content and label quality."
    },
    "Reviews": [
        {
            "title": "Some thoughts",
            "review": "**Pros**\n\nAudio-visual sound source separation is an impotant task. The paper pushes the boundary from specific domains (e.g. speakers, musics, etc) to generalized open-domain, which is crucial and far from trivial.\n\nThe authors introduced a new, large-scale, open-domain dataset for on-screen audio-visual separation. The videos span 2500 hours, 55 of which are verified by human labelers. The dataset will definitely be very useful for the community as it is way more diverse than before.\n\n---\n**Cons**\n\n*Related work*\n\nTo my understanding, Owens and Efros (2018) did not assume fixed number of speakers. While they validate their method under such setting, there is actually no limitation in their model that prevents them to have multiple on-screen sources. Therefore, I'm not sure about the first contribution, except the \"open-domain\" part. \n\n*Model*\n\nIn terms model architecture, (maybe I have missed something but) I didn't see much novelty in the current state. To me, the proposed model is simply a composition of multiple exisiting modules from previous work. Please note that I'm not saying building upon the sucess of previous efforts are wrong by any means. I just had the feeling that the authors are piecing various building blocks together w/o providing much intuition. Maybe there is some novelty lying within the current design. For instance, the authors may have developed a novel routing/module drawing inspiration from a certain observation; the combination of xxx and yyy is based on deliberate choice. It is, however, not clear to me at this point, at least the writing does not reflect it.\n\nFurthermore, if the network is the key player in this paper, the authors shall provide more evidence. While the authors do show conduct some ablation studies on the losses and data, there aren't any analysis regarding the importance of each module (e.g. how critical is the attention design?). It is thus difficult for readers to understand which part of the network is crucial for the success, and what is the major novelty within the architecture. The current form provides very litte intuition and take away.\n\n*Objectives*\n\nEq. 4 and Eq. 6 looks very similar to me. Aren't they equivalent if the **A** in Eq. 6 is the same as **A** that minimizes Eq. 2, since the assignment in Eq. 4 comes from Eq. 2? On the other side, if the two **A** are different, what's the intuition of exploiting different A for different loss?\n\n\n*Writing/Presentation*\n\nThe flow of the model architecture section can be improved. The authors did not provide any high-level context. Instead, they simply dig into the \" details\" of each module. I'm not aware of the connections among while reading the text. Instead, I need to constantly check the figure and infer these.\n\nI also don't know what is the input/output of the model and what representations they are using. Shoudn't these be explained at the very beginning? These are not explicitly defined and I need to infer them myself. For instance, is the output of masking network a M x T soft mask with values lying within [0, 1]? do they exploit waveform (fig. 2) or spectrogram (fig. 1) for audio? I figured/inferred a lot these out after I moved to the experiment section. But from two cents, these are related to the model.\n\n*Experiments*\n\nCurrently the authors only evaluate the model on their own dataset. How does the model work on existing datasets? For instance, AudioSet, MUSIC, FAIR-Play, etc. It seems that there is nothing preventing them from applying their model to those datasets. Without these results, it would be hard to justify if the proposed \"open-domain model\" can generalize to \"a specific domain.\" I think at least  direct inference (generalization) or train from scratch is required. \n\nFurthremore, the authors did not compare with any baseline. It seems to me that quite a few prior art [Owens and Efros (2018), Hang et al (2018), etc] can serve as baselines with minor modification. Take Owens and Efros (2018) for example. While they may not be able to decompose each sound source within the on-screen mixture, one can still leverage it to evaluate the on/off-screen separation. The authors thus shall be able to report SI-SNR too. Otherwise its very difficult for people to do an apple-to-apple comparison of this work and prior work.\n\nThe authors should report more performance at more percentiles. The most illustrative way is to show the cumulative plot - how many % of data have error less than x.\n\nIs there an intuition of why only pre-training part of the model? Why not pre-train the separation network too?\n\n---\n**Minor comments**\n\nHow do the authors define the diversity (Sec. 5.1) of the videos? Do they make use of the tags provided by YFCC100M? Also, whats the statistics of those filtered data? Would be great to provide more details so that we know its indeed covering a wide range of semantic categories.\n\nSome relevant literatures are missing. For instance, [1] also associates the visual information with speeching signal. The subjectives (eg person, dog, birds) in the paper can be viewed as on-screen audio, while prepositions can be seen as off-screen voice. There are definitely a lot more on this direction, but this paper pop out my head right away.\n\n[1] Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input. ECCV 2018.\n\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "This paper proposed an unsupervised method for open-domain, audio-visual separation system. The proposed model was optimized using the newly suggested mixture invariant training (MixIT) together with a cross entropy loss function. The authors suggest to separately process the audio and video, and next align them with a spatiotemporal attention module.\n\nUnsupervised source separation, especially for open-domain is an interesting and important research direction. However, there are several concerns with this submission that need to be addressed first.\n\nMy main concern is the contribution of this paper. The authors presented a fairly complicated system comprised of several modules. I would expect the authors to run an ablation study / analysis to better understand their contribution to the final model performance. For instance, why do we need attentional pooling? do we need it in both audio and video? When does the model fail? can we learn something from it? \n\nSecond, I know the authors said this is the first system to do so, however, I would still expect the authors to compare to some baseline. Maybe a fully supervised one? Otherwise it is hard to interpret the numbers presented in Table 1. It is hard to understand how good is this system and how much room for improvement do we have. \n\nRegarding the samples, it is a bit hard to interpret this results. For every file there are 5 videos and 5 separated samples, some of them sound almost identical. Again, there is no baseline to compare against, so it is hard to understand how good the quality of the separations is. I suggest the authors to improve the samples page to better present emphasis their results. \n\nA question for the authors, since you treated this task at an unsupervised task, did you try to run some subjective evaluations? maybe let users annotate the sound files and compare variance?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Good solution to on- vs off-screen sound classification, additional evaluation is necessary to support the full title",
            "review": "This paper describes a system for separating \"on-screen\" sounds from \"off-screen\" sounds in an audio-visual task, meaning sounds that are associated with objects that are visible in a video versus not. It is trained to do this using mixture invariant training to separate synthetic mixtures of mixtures. It is evaluated on a subset of the YFCC100m that is annotated by human raters as to whether the clips have on-screen, off-screen, or both types of sounds, with the predictions of a previously described model  (Jansen et al, 2020) helping to reduce the number with only off-screen sounds. The predictions are evaluated in terms of how well they can estimate the true on-screen sound (in terms of SI-SNR) and how well they can reject off-screen sound (in terms of a metric called off-screen suppression ratio, OSR). The results show that the system can successfully distinguish between on- and off-screen sound, but that different training regimens lead to different tradeoffs in these two metrics. The system with the best SI-SNR (8.0 dB) is trained using just data from the previous model along with the mixture invariant training criterion.\n\nThe paper presents an interesting approach to solving the on-screen vs off-screen sound problem in audio-visual source separation. While other approaches have solved similar problems for more specific source types (speech, music), this one does appear more \"universal\", with few assumptions tying it to a specific sound type. While this novelty is one of the strengths of the paper, it makes it more difficult to evaluate the system in comparison to established baselines. Such baselines would make it easier to understand how well the system is doing and which parts of it are the most useful and important. Perhaps it could be compared to one of the more source-specific systems on test data suited to such a system.  Another useful benchmark would be using the audio-visual coincidence prediction system of Jansen et al (2020) to assign the entire soundtrack to on or off screen and measuring the various evaluation metrics on the predictions (although infinities in the metrics make this tricky). Yet another baseline might be using an audio-only mixture of mixtures separation system, perhaps with an oracle assignment system. There is a baseline that uses oracle mixtures to compute an estimate of the relevant signals, but the opposite would also be informative as to the utility of the visual component of the system and problem. \n\nWhile these additional baselines would be nice, I do not think that they are necessary for publication presently. The experiments reported are conducted thoroughly and carefully. The results, while leaving certain aspects of the optimal training program underspecified, demonstrate that the system is useful. \n\nOne additional aspect of the system that could be quantified more fully is the quality of the individual source separations. While the on- vs off-screen task is evaluated thoroughly, there is no quantitative evaluation of the source separation performance for individual sources in each mixture. It would be possible to evaluate on completely synthetic mixtures, although this is perhaps outside of the main contribution of the paper, the audio-visual combination.\n\nOverall, this is an interesting approach that is well described. The evaluation is sufficient for the on- vs off-screen task, although not sufficient to judge whether the system has learned a completely unsupervised source separator, making the scope of the contribution somewhat more limited than it has the potential to be.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A new method on multi-modal source separation",
            "review": "Summary of the paper:\nThis paper proposes a multi-modal sound source separation framework in which they aim to separate on-screen sound. The proposed method extends the recent unsupervised source separation framework MixIT by conditioning video input. Although there have been numerous multi-modal sound source separation papers, this work goes one step further by using the sound data “in-the-wild”. The experiment results show reasonable performance on separating mixture from mixtures.\n\n\n\nStrength: \n\nThis paper extends the existing source separation approaches to enable multi-modal source separation.\n\n\n\nWeakness:\n\n-----Novelty-----\n\nThe proposed method is a combination of existing approaches, which shows mere novelty.\n\n-----Writings-----\n\nSome unexplained notations and typos.\nIf the notation appears for the first time in the paper the authors must explain what it is.\nFor example,  \n\n1. In equation (6) please explain the notation \"P(?)\" in the following sentence: \"minimum loss over all settings P(R) of the labels\"\n\n2. In section 4.1 what does Ms stand for? \n\"We use a MixIT separation loss (Wisdom et al., 2020b), which optimizes the assignment of M\nestimated sources ^s = Ms (x1 + x2) to two reference mixtures x1, x2 as follows\"\n\nSome sections (especially methods) are poorly explained.\nFor example,\n“following the same procedure as Tzinis et al. (2020)” -> following how?\n“The concatenated visual embedding is resampled, fed through a dense layer, and concatenated together with all convolutional blocks.” -> resampled how? What resampling method did you use?\n\nFurthermore, I’d like to ask the authors to specify exact neural architectures at least in Appendix.\n“We also use local features extracted from an intermediate level in the visual convolutional network, that has 8 × 8 spatial locations” -> from which layer did you take the local video embedding that has the size of 8x8?\n\n-----Experiments-----\n\nI understand that it is hard to obtain single source on-screen clips, but it could have been better if the authors had collected some small samples and test the single mixture separation performance.\n\n\nOverall:\n\nThe unsupervised separation framework is an important research direction to deal with the real-world sound sources. Although the novelty is mild, therefore, I think this work shows a promising research direction, hence recommend a weak accept.\n\n\nQuestions:\n\n1.\t5 video frames for 5-second of video seems too small to me. Are there any reasons for choosing such small number of frames for conditioning?\n\n2.\tWhy does Global video embedding have to be an input for On-screen classifier?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}