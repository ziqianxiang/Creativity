{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. "
    },
    "Reviews": [
        {
            "title": "Several major weaknesses",
            "review": "# Summary\n\nThis paper claims to have 3 main contributions. \n\nC1: Understanding/Theory. It explains why the two tricks work in zero-shot learning (ZSL): (i) normalization + scaling in the compatibility function of the class features and the attributes, and (ii) attribute unit normalization.\n\nC2: Method. It proposes a “class normalization” scheme (Eq. 9 and 10) and Fig. 3. \nC2.1 From a “theoretical explanation” of C1 (ii), this fixes (ii) in a “deep” ZSL model.\nC2.2 It improves “smoothness” of a “irregular” loss landscape in ZSL.\n\nC3: Experiments. It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL. It also considers continual ZSL (Sect. 4), in which the proposed method is evaluated via mean accuracy (over timesteps) accuracy metrics and a forgetting metric.\n\n###\n\n# Strengths\n\nS1. Simple method. This is a simple feature-attribute scoring function via scaled cosine similarity (with normalization).\n\nS2. Strong empirical results (on both accuracy and training speed). See Table 2.\n\n# Weaknesses\n\nW1. Clarity\n\nThe organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the “theoretical” results which are not rigorous and at times unsupported. Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing. \n\nW2. Soundness\n\nI have a lot of concerns and questions here as I read through Sect. 3. At a high-level, I don’t see a clear connection between “improved variance control of prediction y^ or the smoothness of loss landscape” and “zero-shot learning effectiveness.” Details below. This is in part due to poor clarity.\n\nW3. Experiments\n\nIMO, if the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2). \n\n- If baseline methods already adopt some of these tricks, it should be made clear and see if removing these tricks lead to inferior performance.\n- If baseline methods do not adopt some of these tricks, these tricks, especially class normalization, could be applied to show improved performance. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.) \n\nThis is done to some degree in the continual setting.\n\nW4. Related work\n\nAs I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.\n\n###\n\n# Detailed comments (mainly to clarify my points about weaknesses)\n\n\n## Statement 1\n\nThe main claim for this part is that this statement provides “a theoretical understanding of the trick” and “allows to speed up the search [of the optimal value fo \\gamma].”\n\nHowever, I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, “better stability” and “the training would not stale”) and the zero-shot learning accuracy for this to be the “why normalization + scaling works.” My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.\n\nMoreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it? \n\nFinally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?\n\n\n## Statement 2 and 3\n\nWhy wouldn’t the following statement in Sect. 3.3 invalidate Statement 1? \n“This may create an impression that it does not matter how we initialize the weights — normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit value”\n\nIt is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.\n\nSimilar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? In particular, the authors again claim that the main bottleneck in improving zero-shot learning is “variance control” (the end of Sect. 3.3).\n\nI also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: “And these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).”\nH1: Would this statement still be true after we transform a_c with an MLP?\nH2: Why is it not “a sensible thing to do” if we just want zero mean and unit variance? \nH3: Why is “such an approach far from being scalable”? \nH4: What if these are things like word embeddings?\nH5: Fig. 12 and Fig. 13 are not explained.\nH6: Histograms in Fig. 13 look quite normal.\n\nHow useful is Statement 2? Why is the connection with Xavier initialization important?\n\nWhy is “preserving the variance between z and y~” in Statement 3  important for zero-shot learning?\n\n\n## Improved smoothness\n\nThe claim “improved smoothness” at the end of Sect. 3 and Appendix F is really hard to understand.\nF1: How do the authors define “irregular loss surface”?\nF2: “Santurkar et al. (2018) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model, which suggests that our class-wise standardization will provide the same impact.” This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear.\n\nSimilarly to my comments to Statement 1-3, how is improved smoothness related to zero-shot learning effectiveness?  \n\n\n## Other more minor comments\n1. Abstract: Are the authors the one to “generalize ZSL to a broader problem”? Please tone down the claim if not.\n2. After Eq. (2): Why does attribute normalization look “inconsiderable” (possibly this is not the right word?) or why is it “surprising” that this is preferred in practice? Don’t most zero-shot learning methods use this (see for example Table 4 in [A])?\n3. Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).\n4. Under Table 1 “These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.”: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a “linear” or “multi-layer” model.\n5. The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.\n6. The second paragraph of Sect. 3: What exactly limits “the tools” for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].\n7. What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected? \n\n[A] Synthesized Classifiers for Zero-Shot Learning\n\n[B] Zero-Shot Learning by Convex Combination of Semantic Embeddings\n\n[C] Class-Balanced Loss Based on Effective Number of Samples\n\n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Simple and effective approach",
            "review": "The paper shows that normalization is critical for zero-shot learning (ZSL). In the ZSL randomization is coming from the two sources, attribute and feature.  Normalization of the two source helps to reduce the variance. The paper uses an embedding based model where normalize visual feature are projected to the attribute space and in the attribute space, cosine similarity is measured to predict the class label. Paper also extend ZSL framework to the continual learning ZSL (CZSL) setup where whole data are not present at a time; instead, data comes in the form of a task, sequentially. The author shows that normalization helps to improve the CZSL result.\n\nPositive:\n1: The normalization is important in the NN/CNN model, in the ZSL, there are two sources of information, and proper normalization is important on both source for the better result. The paper gives a theoretical justification of why normalization is important and how we can do the same for the performance gain.\n\n2: The proposed normalization shows the significant performance gain on the standard dataset for the GZSL setup (provided evaluation and code is correct). \n\n3: Recently generative model shows the SOTA result for the GZSL setting since they can synthesize the unseen class samples and easily can handle the data biasness. It is nice to see that the non-generative model shows a significant improvement using the simple method, and it is much faster to train. \n\nComment:\n1: The main concern is the result, I am unable to understand from where the exact gain is coming. Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario; it helps the generalization ability and smooth training and resulting in a performance gain. The performance gain using eq:[9] and [10] is expected, but [9]+[10] (CN) shows the much better result, I am unable to understand why this happens? I request the author; please explain the same.\n\n2: The proposed approach is an embedding based model, it does not generate the samples from the unseen classes, then how model overcome the data biasness towards the seen class? Generally, it observes that seen-class shows the high accuracy and unseen-class show the low accuracy; hence H-mean is very poor. The generative model can handle this scenario since they can generate data from the unseen class. The normalization technique does not help to overcome the data biasness towards the seen-class then how model handle the data biases?\n\n3: I appreciate the theoretical justification and identifying the problem in the deep model and providing the solution for that (section 3.4)\n\n4: The comparison with the few recent meta-learning based approach [a] [b] [c] for the ZSL are missing, can you show the result compared with these approaches? Also, I request the author please provide the result if the same normalization is applied with the approach [c] (it is also embedding based model in the meta-learning framework). \n\n[a] Episode-based Prototype Generating Network for Zero-Shot Learning, CVPR-20\n[b] Meta-Learning for Generalized Zero-Shot Learning\n[c] Learning to Compare: Relation Network for Few-Shot Learning, CVPR=18\n\nOverall I like the idea and contribution, but I suspect the provided result for the GZSL result in table-2, I request the author please provide the code for the AWA2 and AWA1 dataset. I will further increase the score on the successful verification of the result.\n\n4: If you don't use cosine similarity, then what is the dependency relation between the variance and weight W. I mean in the statement-1 if we don't use normalization then how variance depends on weight W? Also Var[\\hat(y)_c] is independent of W, but learning is not independent of the initialization of W, in this case, variance does not matter, proper learning and generalization are more important.\n\n5: I agree with the author that the provided evaluation metric for the CZSL is more generic and realistic. Here the model and CL procedure is not clear. What is Multi-task? What is sequential? How you ensure to overcome the catastrophic forgetting over the previous task while training the current task, The proper description is not provided, I request the author please provide the same.\n\n6: In continual learning scenario with the increase of task, the model performance is degraded. It is shown in the figure-9, and 10 (supplementary) with the increase of the task model's performance is increasing, it means that model does not forget anything and also you have backward transfer how this is possible? Maybe I misunderstood something please explain.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An excellent paper investigating the normalization effect in the zero-shot learning context",
            "review": "================\nSummary: \n\nThis paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research: normalize+scale and attribute normalization. It also demonstrates these tricks are not enough w.r.t. normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue. Both theoretical and empirical analysis are provided and results look convincing. Finally the authors propose a continual zero-shot learning problem scheme and illustrate some pioneering experimental results.\n\n================\nReason for my score:\n\nThere is rare work on the normalization trick in the context of zero-shot learning, although techniques like attribute normalization are widely used in practice. This paper investigates the normalization effect extensively for zero-shot learning, and provides many insightful thoughts for utilizing these tricks. The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results. Such work makes good contributions to the related community and hence I give my score.\n\n================\nPros:\n\n1. The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning, in the perspective of data variances.\n2. The paper proposes a class normalization trick to alleviate the variance inflation/diminish in the non-linear model, and demonstrates its effectiveness on benchmark datasets.\n3. The empirical analysis in the paper are extensive and convincing.\n4. The paper also proposes a new framework of continual zero-shot learning.\n\n================\nCons:\n\n1. The paper didn't evaluate on another widely used benchmark dataset aPY, can author explain the reason?\n2. On CUB dataset, the proposed method has a considerably large margin to the state-of-the-art methods, in contrast to other datasets. Is there any explanation on why it is the case? Have you tried to explain this failure especially from the perspective of the proposed normalization trick?",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "CLASS NORMALIZATION FOR ZERO-SHOT LEARNING",
            "review": "Summary: This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time. It proposes two normalization tricks: normalize + scale trick and attributes normalization trick and apply in the zero-shot image classification task. This paper also shows that two normalization tricks are not enough to variance control in a deep architecture. To address this problem, a new initialization scheme is introduced. Apart from theoretical analysis and a new initialization scheme for normalization, it extends the zero-short learning approach in a continual learning framework. This new framework is called continual zero-shot learning (CZSL) and provides corresponding evaluation metrics. The experiments for CZSL are performed in two datasets, CUB and SUN.  This paper experimentally shows the effectiveness of the initialization, normalization, and scaling trick.\n\nStrong Points:  1-  Paper is well organized and easy to follow.\n\n2-  This paper took an interesting problem and developed a compelling investigation for how normalization affects performance. The theoretical justification for the normalization tricks sounds interesting and makes sense. \n\n3-  It introduced two new techniques for normalization and shows by the theoretical justification that only normalization techniques are not sufficient for proper model training. For good model training apart from normalization tricks, it introduced an initialization scheme.\n\n4- Using normalization tricks and a new initialization scheme reduces a significant model training time compared to previous approaches. It presents training speed results for several baseline approaches.\n \n5- Innovative attempts in introducing a new ZSL problem, and several evaluation metrics are proposed for continual ZSL.\n\n\nWeaknesses:  1- This paper presents a robust analysis of normalization, initialization, and scaling trick for ZSL. It also extends ZSL in continual learning.  I appreciate the author's effort for this solid analysis.  I expect a  new proposed model from authors to make the paper more strong. \n2-  Missing comparison:  I recommend including paper [a] in the comparison table for CZSL.  Approach [a] is the first proposed baseline for continual zero-shot learning. Therefore it must be included in the comparison table. \n3- Some recent state-of-the approaches are missing in the comparison table for ZSL. Please compare it with [b],[c] models.\n\n4- Why have aPY dataset is not included in the experiments? Does this model not perform well in aPY dataset?\n5- Is it possible for this normalization and scaling tricks for other applications such as object detection, action recognition, and image retrieval? \n6-  I wonder by the training time you reported. I understand you have used quite a small neural network. Still, to have a clear view or fair comparison, you should have compared the timings with other initialization and default normalization and scaling tricks as well.\n\n[a]- Lifelong Zero-Shot Learning, by Kun Wei et al. IJCAI 2020.\n[b]- Episode-Based Prototype Generating Network for Zero-Shot Learning,  by  Yu et al. CVPR2020.\n[c]- Meta-Learning for Generalized Zero-Shot Learning, by Verma et al. AAAI 2020.\n\nRating Reason: This paper has included a well-detailed analysis and mathematical formulations for normalization, initialization about model training. But it does not propose a novel model, which limits the novelty of the model.  CZSL formulation is also already explored in [a].",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}