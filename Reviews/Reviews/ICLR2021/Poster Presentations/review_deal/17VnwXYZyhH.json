{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces a new method to probe contextualized word embeddings for syntax and sentiment properties using hyperbolic geometry. The paper is written well and relevant to the ICLR community. Reviewers highlight that the proposed Poincaré probe offers solid results, extensive experiments that support the benefits of the approach, and proposes a new approach to analyze the geometry of BERT models. The revised version clarified various concerns of the initial reviews and improved the manuscript (comparison to Euclidean probes, low dimensional examples, new results on edge length distributions etc.). Overall, the paper makes valuable contributions to probing contextualized word embeddings and the majority of reviewers and the AC support acceptance for its contributions. Please revise your paper to take feedback from reviewers after rebuttal into account (especially to further improve clarity and discussion of the method)."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "This paper proposes probing BERT representations by projecting them into a Poincare subspace. The proposed approach is used to probe ELMO and BERT for both syntax and sentiment in comparison with the conventional Euclidean probes.\n\nI am ambivalent about this paper. On the positive side, I think that it is a quite solid work, with extensive experimentation, additional supporting results in the appendix, and an accompanying code that can be used to reproduce results and obtain additional visualizations. The paper is also well written and the authors are rigorous when discussing their results rather than trying to oversell. \n\nOn the negative side, I have some reservations about the relevance of this study. What do we learn from it? It is true that the Poincare probes obtain generally higher scores than the Euclidean probes, but it doesn't look like they lead to any new insight about how BERT works. If the message here is that Poincare probes are more appropriate than their Euclidean counterparts, I would have liked to see instances were Euclidean probes lead to erroneous or at least different conclusions when compared to Poincare probes. In the absence of that, we can expect that practitioners will stick with Euclidean probes given that they are simply easier to use.\n\nMoreover, I am not sure if the comparison with Euclidean probes is entirely fair. If my understanding is correct, the Poincare probes learn two linear transformations (P and Q), whereas Euclidean probes learn a single one. Unless I am missing something, it could be that the Poincare probes obtain higher scores simply because the transformation they are learning is more expressive, and not because of the underlying geometric space. In order to test this hypothesis, I think that the authors should try learning two linear transformations for Euclidean probes, with a non-linearity like ReLU in between.\n\nFinally, I feel that some of the analyses did not follow a systematic methodology and some of the interpretations seem subjective and possibly questionable. For instance, I don't see any clear difference between the Poincare and Euclidean probes when it comes to the sentence length (Figure 2C), except for very short sentences. For sentence length > 12 the curves look very similar to me, except that the absolute values for Poincare are higher. Similarly, the visualizations, although interesting, provide a rather anecdotal evidence, in particular since the examples seem to be cherry-picked.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple, interesting, could stand more rigor ",
            "review": "#### Summary:\n\nIn the same vein as Hewitt & Manning 2019, the authors present an extremely lightly parametrized “probe” model to determine the presence of syntactic structure in the embedding space of BERT models. While Hewitt & Manning examine the Euclidean distance between linearly transformed token embeddings and its correlation with parse tree distance and depth, this work examines a different distance function based on distances in hyperbolic space. They find that this distance measure, for an equivalent or lesser number of parameters, better reproduces the syntactic properties. This suggests that the BERT model may operate simply, but on a non-Euclidean manifold, in order to work with syntactic information.\n\n\n#### Reasons for score:\n\n\nThe paper presents a simple method to examine syntactic structure in contextual embedding models, as a nice extension of the seminal work on Euclidean probing, and should be directly of interest to the ICLR audience. The paper seems to be well-executed experimentally, with a variety of analyses.\n\nHowever, I would like to see the authors give more geometric rigor to their model and argument, and clarify the hypothesis which they are testing. There is a lot of appeal to intuition about hyperbolic space and its ability to encode trees, but it is not clear what that has to do with their proposed probe in a concrete way. \n\nThe paper would be greatly improved by a simple 1 to 3 dimensional example of their proposed probe and how it could discover an embedded submanifold with hyperbolic structure.\n\n#### Positives:\n\n- This is a mathematically (and superficially conceptually), simple generalization of the work of Hewitt & Manning, which offers improvements to and new avenues for exploration of these probing ideas.\n- The work convincingly shows that their probe offers better performance, especially in UUAS, compared to Euclidean probes, which is quite interesting.\n- Generally, I believe that there is a good idea here, but it would be much improved by additional mathematical rigor.\n\n#### Comments/Concerns:\n\n- It is hard to understand this paper without having read Hewitt & Manning, it could do with being a bit more self-contained. For example, the metrics like “DSpr”, etc, should be explained.\n\n- It is not clear what hypothesis is being tested here. What does it mean for the embedding space to have a hyperbolic structure in this sense? The Nash embedding theorem ensures the existence of an isometric embedding of d-1 dimensional hyperbolic space in d dimensional Euclidean space, but that embedding can hardly be accomplished in general by the simple linear transform + exponential map + moebius multiplication described here.\n\n- In this vein, there should be much more rigor as to describing what exactly the Poincaré probe is doing. The paper talks about mapping to “the tangent space”. What tangent space is this? Is it the tangent space of a Poincaré ball at the origin? Why is this a reasonable thing to do if the goal is to discover hyperbolic structure? When should this be expected to work?\n\n- An example in low dimensions with visuals of a hyperbolic submanifold, and a Poincaré probe interacting with it, should feature prominently.\n\n- Along these same lines, it would be very helpful for the paper to give an example of how the neural network might access this information on a hyperbolic submanifold, though this may be beyond the scope of the work.\n\n- In the introduction, the authors note the ability of hyperbolic space to encode exponentially larger volumes vs Euclidean space, but of course the Poincaré ball they are examining is embedded in Euclidean space.  What does this mean concretely in the context of the BERT transformer network?\n\n#### Minor comments:\n\n- middle of page 7, “RiemannianAdam” is missing the space ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A good argument for taking the curvature of the space into account for probes, but the comparison might not be entirely fair and the experimental setting description needs to be more complete.",
            "review": "## Summary\n\nThis work examines some of the syntactic and semantic information present in contextual word embeddings by training probes for dependency parsing and sentiment classification. The probes take the form of a low-dimensional projection of the word embeddings, and obtain the dependency parse and sentiment by considering the distance between the pairs of embeddings or between the embeddings and a class embedding respectively.\n\nThis is closely related to the approach of (Hewitt and Manning, 2019), whose experimental setting this work reproduces, but the innovation here consists in using hyperbolic rather than Euclidean distances: the embedding space is identified with the tangent space at the origin and projected to the Poincare ball using its exponential map, allowing the probe to use the hyperbolic distance.\n\nThe hyperbolic version of the probes consistently out-perform the Euclidean one, which is encouraging, and the authors provide some useful visualizations of the learned projections. However, the experiments fail to account for one possibly relevant difference between their Hyperbolic and Euclidean setting. The paper is also at times difficult to follow on its own, as it relies a bit too much on cited work.\n\n## Clarity\n\nEven if the experimental setting is the same as (Hewitt and Manning, 2019), a quick summary would be welcome here: currently, the name of the parsing dataset is not even mentioned, nor are the meanings of the Spearman correlation metrics (the abbreviations are not self-explanatory).\n\nThere are also some open questions: for parsing, do you train a single model with both objectives? From reading the code, it looks like you alternate doing an epoch of each. Is that right? How does that compare to learning both individually? \n\n## Correctness\n\nMy main issue with the comparison lies in the second projection which the hyperbolic (but not the Euclidean) method uses, as defined in Equation (3)\n\nFirst, doing the projection in the hyperbolic space is redundant. Remember that the Mobius matrix-vector simply applies the logarithmic map at the origin followed by a linear transformation in the tangent space followed by the exponential map at the origin. Hence, Equations (2) and (3) could simply be summarized as:\n$$q_i = \\exp_0 (Q P h_i) $$\n\n(Same remark for the GRU in the appendix.)\n\nWhile this would be equivalent to having a single projection matrix if we could do global optimization, given the geometry of the loss function the hyperbolic setting with its additional parameter might be easier to optimize than the Euclidean one. In particular, this probably accounts for the difference in sentiment classification with fixed label embeddings.\n\nThankfully, the difference in the same task with learned embedding seems to indicate that there is more to the result, and I would consider increasing my score if all the results hold with either an additional linear transformation in the Euclidean setting or a single projection in the hyperbolic.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3 - edited after author response",
            "review": "This paper proposes probes based on hyperbolic embedding spaces, and compares them to the behaviour of Euclidean probes from recent work. The main result is that these probes allow for better recovery of syntactic properties of sentences from contextualized word embeddings compared to context-independent ones, when comparing them to euclidean probes. Similar results are presented  on sentiment analysis, even though no results are presented for context-independent word embeddings.\n\nOn the whole the paper is well-written with great visualizations. My main concern is that the findings are not strong enough at this point. As per the introduction, the first main finding is that the results indicate the possibility that hyperbolic models help us construct more sensitive probes. But how do we know a probe is not too sensitive or not sensitive enough? E.g. looking at the results in table 1, how do we know that the differences in the scores when using contextualised embeddings are for a good reason? Some of the differences are small, especially in the case of the depth probe. Similarly, for the second finding about BERT might be encoding information in non-Euclidean way, how do we know this is the case? One way would be to somehow modify BERT and make it \"more Euclidean\", and then this would help strengthen the hypothesis. But now as it stands, it is interesting but rather speculative.\n\nSome other points:\n- Statistical significance would help in reporting the results as many differences are small in Table 1. Also confidence intervals in the sentence length experiment would be nice.\n- It would be better to explain the scores used in Table 1, instead of referring to Hewitt and Manning\n- For syntactic parsing it is argued that the probe shouldn't be a parser, but in the sentiment analysis the probe is a better parser than the baseline model used. Shouldn't the probe not be a good sentiment analysis model in itself?\n- In the ends of section 5, the referring labels for subfigures in figure 6 are off.\n\nPost-author response: I appreciate the extra experiment on GLoVe vs Linear on sentiment analysis, it is what I was asking for. I have raised my score in response to that, as well as the additional reporting on the results. The discussion in Bayesian terms was interesting, I think it would help. Nevertheless, I was thinking is it that it should be possible to construct embeddings that have known syntactic vs semantic properties. E.g. one could increase/decrease the context size, perhaps to extreme values in the case of models such as GLoVe. And then we could actually have a much stronger prior. If the paper is accepted, I think such an experiment would be very informative.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}