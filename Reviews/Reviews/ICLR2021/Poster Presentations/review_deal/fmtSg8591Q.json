{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper establishes the currently sharpest regret bounds for reinforcement learning in episodic factored MDP. The result improve the result by Osband and Van Roy 2014. The proposed FMDP-BF is a model-based algorithm that construct confidence sets of the transition distributions using Bernstein  and adapt policies by optimistic planning. The regret bounds holds with high probability. They also provide a lower bound for this class of problems. Reviewers all see merit in the theoretical results of the paper and reach a consensus that this is a good paper. We'd still like to request that the authors make all corrections and clarifications following the reviewers's suggestions, especially to improve the clarity of the formulation and proof sketches.\n\nA separate suggestion: Model-based RL is a long existing approaches. For MDP belongs to a specific family, there exist regret bounds that depend on Eluder dimension of the the MDP family, see eg. https://arxiv.org/abs/1406.1853 and https://arxiv.org/abs/2006.01107. Can these results be applied to the factored MDP family and yield similar regret bounds? It would be necessary to add discussions about these papers, and explain why or why not these general regret bounds can apply to analyze HMDP."
    },
    "Reviews": [
        {
            "title": "Review of Paper 718",
            "review": "This paper studies RL in episodic factored MDPs (FMDPs) in the regret setting. The paper introduces an algorithm called FMDP-BF, which is a model-based algorithm implementing the optimistic principle by maintaining upper and lower confidence bounds derived using empirical Bernstein-type confidence sets.\nThe paper presents a high-probability regret bound for FMDP-BF appearing to be superior than existing results for episodic FMDPs. It also reports a regret lower bound for FMDPs. \n\nThe paper further studies a variant of \"RL under constraints\" in episodic MDPs and the regret setting, and shows that the corresponding problem can be cast as an instance of RL in an associated FMDP. Applying FMDP-BF to this (originally non-factored) RL problem, the paper derives tighter regret bounds than what would be obtained otherwise. \n\n\nMain Comments:\n\nThe paper studies an interesting and timely RL problem, which I believe to be significant in practice. In particular, the studied application to constrained RL is interesting and relevant. While the presented algorithm builds on well-known constructions that are already present in a series of recent works on episodic tabular MDPs, deriving sharp regret bounds like those in the present paper poses additional challenges making the problem non-trivial. To address such challenges, the paper introduces a couple of technical lemmas that sound interesting beyond the scope of this paper. These include in particular, Lemma 4.1 and Theorem 1 stating a Bellman-type result for the variance of factored Markov chains.\n\nThe paper is well-organized and states the results and the setup clearly. While the general writing quality is acceptable, I believe it still requires some polishing. In particular, there are many typos (see a list of them below), and in some cases there are unclear statements. \nThe review of the relevant literature seems adequate. In particular, the authors responsibly mention the parallel work of (Tian et al., 2020), though I urge the authors to provide a deeper technical comparison of their results to those of the latter paper. \n\nBesides the writing quality that could be improved by clarifying some statements and fixing typos, I have the following comments, which I would like to be addressed in the rebuttal:\n\n- While (Tian et al., 2020) is cited in the paper, the authors fail to adequately compare their results to those in (Tian et al., 2020)  -- except for a short discussion in page 3. Specifically, I believe they need to compare Theorem 2 to the regret bounds of (Tian et al., 2020) on page 6. I believe this is the right place for making such a comparison. Currently, there is only comparison against (Osband and Van Roy, 2014). The paper also lacks a comparison between the lower bound in Theorem 3 and those in (Tian et al., 2020). A precise and specific comparison is necessary here. \n\n- The idea of reducing the constrained RL problem to an FMDP is very nice. However, I think Assumption 2 makes the application of such a reduction rather restrictive. Could you please explain whether this can be improved or not?\n\nMinor Comments:\n\n- In Section 2.1, as far as I know (Lu & Van Roy, 2019) is not relevant to factored MDPs.\n\n- In page 7, the authors stated that introducing an $\\epsilon$-net to relax Assumption 2 \"will not influence the regret\". This is not true. This will penalize the regret, but perhaps with the choice of $\\epsilon=1/T$, the penalty does not influence the \\emph{order} of regret bounds ignoring polylogarithmic factors. Please explain.\n\n===== Post Rebuttal =====\n\nThanks for your response and for revising the paper.  I have increased my score accordingly. \n\n\nSome Typos/Mistakes:\n\np. 1: In paragraph 3, introduce $T$.\n\np. 1: Introduce $\\mathcal X[Z_i]$.\n\np. 2: there exists functions -> exist\n\np. 3 (and elsewhere): for factored Markov chain -> … chains\n\np. 3: Knapsack setting -> the knapsack setting\n\np. 3: … is more concise -> are \n\np. 3: We use … estimation of $V^\\star_h$ -> …  estimation of $V^\\star_h$, respectively.\n\np. 4: do not harm to the … -> remove \"to\"\n\np. 4 (and elsewhere): omits higher order factors -> I believe you mean here that $\\eta$ collects some higher order terms in the expression.\n\np. 5: all possible value of -> values \n\np. 6: $X[Z_i]$ -> Did you mean $\\mathcal X[Z_i]$\n\np. 7: for a $\\sqrt{n}$ -> for a factor of $\\sqrt{n}$\n\np. 7: focus -> focuses \n\np. 7: … but counts the cumulative cost … -> \"counts\" has made the statement rather unclear.\n\np. 8: from state to … -> from states and budget to actions (or state-space … to action-space)\n\np. 8: With prob. at least -> shortening probability to \"prob.\" does not seem to provide any gain in the space. \n\np. 8: To be more formally -> formal\n\np. 8: in interesting future work -> an interesting …\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Tight regret bounds for episodic Factored MDPs",
            "review": "This paper proposes a reinforcement learning algorithm FMDP-BF for episodic Factored MDPs. Similar to previous works, FMDP-BF follows the principle of \"optimism in the face of uncertainty\" to efficiently explore to achieve low regret. Compared to algorithms for general MDPs, FMDP-BF leverages the factorization structure of FMDP and results in exponential regret reduction in the size of the sate space. Compared to previous results for FMDP, the proposed algorithm utilizes Bernstein-type confidence bounds and new bounds on the factored transitions to reduce the regret order. The paper also connects a class of constrained RL to FMDP and  provide a sample efficient for the constrained RL problem.\n\nThe proposed method improves the regret order for FMDP by using novel bounds for the estimation error of one-step values (Lemma 4.1) and their variances (Corollary 1.1). A regret lower bound is also provided which matches the upper bound except for a $\\sqrt{n}$ and logarithmic factors. The paper is generally well written, and the proofs in the appendix are basically correct except some minor issues listed below. \n\nSome suggestions:\n\n- Corollary 1.1, likely typo: $V^{\\pi}_{h}$ instead of $V^{\\pi}_{h+1}$.\n- Algorithm 1, line 13, likely typo: $\\hat{R}_k(s, \\pi_{k,h})$.\n- Bottom of page 11, definition of $\\phi_{k,i}$, likely typo in $N_{k-1}(s,a)$ .\n- Through out the proofs, many terms could be replaced by proper notations like $\\phi_{k,i}$ or $CB^R_{k, Z^R_i}(s, a)$. For example, the RHS of (9) is exactly $\\phi_{k,i}$. By properly using the notations, the proofs would be much easier to read and the connection between lemmas would be much clearer.\n- $\\eta_{k, h, i}(s, a)$ is mentioned in Algorithm 1, but the notation is never explicitly defined. Also, it's a bit confusion to have two different sets of definitions for $CB^R, CB^P$ in (1)-(2) and in (3)-(6) for different algorithms. Maybe to use different notations and explicitly define the extra term $\\eta$.\n- Lemma D.1, missing $[Z^R_i]$ in (7). (9) seems like a direct application of (10).\n- Lemma D.2, the failure probability of $F^N$ in Dann et al. is bounded by $SAH\\delta$, which seems to suggests the log term in (15) is not correct.\n- In the proof of Lemma D.2, $V$ should be $V_{h+1}$, and the sentence after the set of equations should be \"The last inequality...\"\n- In (19), $\\mathbb{P}_n$ instead of $\\mathbb{P}_{k, n}$, and there seems to require an additional inequality for the last term to continue the recursion.\n- For Lemma E.2, missing $|S_i|$ in (21), and missing $1/m$ for the first equation in the proof, and $\\hat{\\mathbb{P}}_{i, k}$ in after the first inequality of the second equation.\n- Typo $\\mathbb{P}_h$ in the proof of Lemma E.3.\n- In the proof of Theorem 5, typo $\\hat{\\mathbb{P}}_{k, h}$ in the definition of $\\delta^3_{k, h}$, missing $[Z^P_i]$ in the third paragraph of page 19, and extra a's in the value functions in the last equation of page 19. In page 20, $\\delta^1_{k,h}$ and $\\delta^3_{k,h}$ should be bounded by Lemma E.2. At the end of the proof, the doesn't the big-O notation also hides all lower order terms in $T$?\n- In the proof of Lemma F.2, the constant in the log term is not consistent with Lemma D.2.\n- In the proof of Lemma F.4, the first equality incorrectly swaps the two summation. The result seems to be correct by first applying the inequality.\n- In the proof of Lemma F.5, the second inequality in the first equation seems to miss the last term which might be bounded by $log(T)$.\n- In the proof of Lemma F.10, the two inequalities bounded by $CB^R$ and $CB^P$ terms may be better stated as a lemma.\n- In the proof of Theorem 2, extra actions in the value functions in (44).\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Concerns about technical novelty over Osband & Van Roy (2014), Azar et al. 2017",
            "review": "The authors study the factor MDP problem in an online and episodic setting. They provide two main contributions on this question. First, they propose an OFU type algorithm which enjoys a better regret bound than Osband & Van Roy (2014) by a factor of  $\\sqrt{nH \\Gamma}$. The improvement is brought about by a refined consideration on the confidence radius' dependence on the variances of the rewards, which carries a similar idea to the design of UCBVI-CH by Azar et al. 2017 for the tabular case.  The second contribution is on the generalization to an episodic FMDP with knapsacks problem, where the authors generalize the approach in the first contribution to provide a regret bound. \n\nMy overall concern about the paper is about the technical novelty of the paper. While I understand that there are quite a lot of technical calculations involved, the improved regret bound compared to Osband & Van Roy (2014) appears to be a direct outcome of replacing the confidence bounds used in Osband & Van Roy (2014)  by the confidence bounds proposed in Azar et al. 2017 on each of the factors in the factored model. In other words, on top of the technical contributions from Osband & Van Roy (2014)  and Azar et al. 2017, the proposed paper's first technical contributions appear to be on how to merge those two papers together.  \n\n### Post Rebuttal ### Thanks for the clarification, I have adjusted my score accordingly\n\nWhile the paper's second contributions allow the incorporation of a more refined resource constraints, it appears to me that the the proposed approach is unlikely to yield a computationally tractable algorithm even in the tabular setting, when the original state space is small. Indeed, the vector of remaining resource levels are embedded into the state, so that it will results in a curse in dimensionality even when the original space is of a manageable size. Overall, based on my evaluation on the theoretical and practical impact of the paper, I find that the contributions fall marginally below the acceptance threshold.  Finally, after the statement of the regret bound in Theorem 2, it is helpful to compare with the regret bound in the recent work by Tian et al. 2020, similarly to how the proposed algorithm is compared to Osband & Van Roy (2014).\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper focuses on episodic, factored Markov decision processes and proposes the FMDP-BF algorithm, which is computation efficient and improves the previous result of the FMDP algorithm. The author also provides a theoretical lower bound for the FMDP-BF algorithm and shows that the FMDP-BF algorithm's regret is near-optimal. Besides, this paper applies the FMDP-BF algorithm in RLwK and provides theoretic analyses of regret. However, I still have some suggestions about this paper.\nFirstly, the definition of factor MDPs is complex and challenging to understand. It is better to add some examples for the factor MDP setting.\nSecondly, there is no experimental support for the FMDP algorithm, and it is better to perform some experiments with FMDP-BF, FMDP-CH algorithm.\nFinally, I have a concern about the FMDP algorithm. The UCBVI-CH algorithm, UCBVI-BF algorithm, and FMDP-CH algorithm only maintain one value function V in each episode. The only difference is the form of a bonus term. However, the FMDP-BF algorithm needs to keep two value function V to compute the bonus term, and I wondered whether it is necessary to keep two value functions in each episode.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}