{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper performs an empirical comparison of similarity-based attribution methods, which aim to \"explain\" model predictions via training samples. To this end, the authors propose a handful of metrics intended to measure the acceptability of such methods. While one reviewer took issue with the proposed criteria, the general consensus amongst reviewers is that this provides at least a start for measuring and comparing instance-attribution methods. \n\nIn sum, this is a worthwhile contribution to the interpretability literature that provides measures for comparing and contrasting explanation-by-training-example methods. "
    },
    "Reviews": [
        {
            "title": "Interesting motivation but the correctness of the evaluation criteria is unclear. More discussion needed",
            "review": "#### Summary:\n This work provides an empirical evaluation of similarity metrics used in example-based explanations methods, where the goal is to provide decision support examples in the training set for a black-box model's prediction. The paper evaluates gradient based metrics popular in the literature such as Influence functions, fisher kernels but also simpler naive approaches that relies on l2, cosine distances and dot product on different embedding spaces. The authors introduce 2 new tasks for assessing the reliability of the different methods: an identical class test and an identical subclass test.\n\n#### Strengths\n - The paper is clearly written and easy to follow. The idea to empirically evaluate different similarity metrics for example-based explanation methods is interesting and could be useful to practitioners. The motivation for the work is clear and well introduced. \n\n#### Weaknesses\n- Identical class test: the requirement that the returned explanatory examples in the training set should belong to the same class as the example is ambiguous and raises some concerns.  1) Is is not clear how this applies in case where the test example is misclassified? Why does the identity requirement applied only on the predicted label and not the true label as well? 2) Training examples that are either predicted differently or belong to a different true class as the test example can be useful in understanding failure modes of the model, misclassified examples, or inhibitory examples. This validity of this test need to be discussed and justified.\n- Same applies for the Identity subclass test. The subclass test has even stronger requirements than the identity class test. The validity of this test should be justified beyond a simple definition.\n\n-Strong statements are made on the 'usefulness' of the explanations but these are mainly intuitive assessment of individual cases.  For example, in section 3.2, the authors state \"The violation of this requirement leads to nonsense explanations such as “I think this image is cat because a similar image I saw in the past was dog.” which do not make sense to the users at all. When faced with such an explanation, the users will find the validity of the model’s prediction questionable, and will ignore the prediction even if the underlying model is valid\". \nThe usefulness of the explanations provided by these methods to human users should be assessed through a human evaluation.\n\n#### Recommendation with reasons\nWhile I like to motivation behind this work and it's potential usefulness, the validity of the proposed tests is not clearly addressed. For that reason, I can not recommend acceptance for this work. \n  \n#### Questions\n- Justify the correctness of the identity and subclass identity test\n\n#### Additional feedback\nAdditional reference: Yeh, C. K., Kim, J., Yen, I. E. H., & Ravikumar, P. K. (2018). Representer point selection for explaining deep neural networks. In Advances in Neural Information Processing Systems (pp. 9291-9301)",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Empirical comparative study of metrics for explanations with simple requirements.",
            "review": "The authors investigate which relevance metrics are desirable for explanations that are based on extracting similar instances as evidence to support a model prediction. They establish three minimal requirements for those metrics:  one adapted from the state-of the art: (1) model randomization to ensure the measure is model-specific, and two proposed requirements,  (2) identical class test (instance pointed out should be of the same class as the test instance) and (3) identical subclass test (instance pointed out should be of the same subclass as test instance). Several metrics are compared and the authors conclude that many of the state-of-the-art metrics do not even satisfy the first requirement and that the cosine similarity of the gradients of the  loss perform best. \nThe authors conduct an empirical evaluation on two image datasets, two text datasets and one tabular dataset. \nThe paper is well-written, the method and the two requirements straightforward and the empirical comparative approach acceptable. \nIt is not clear from the paper how the noise in the labels is handled (what if the nearest neighbor used in the second requirement is mislabeled?). More extensive experiments on tabular data would have been good. \nFurthermore, how to ensure these three requirements are sufficient?  While the approach presented and the two additional principles are quite simple, it is nice to see a comparative study on explanation metrics. The work could benefit from deeper insights on the evaluation metrics from a theoretical perspective.\n\nMinor comments:\n- Figure 1 and 2 are not necessary in my opinion\n- Figure 3: sub-captions. missing. \n- concrete examples on the vehicle data would have been desirable. Why this data specifically? there are many benchmark datasets from the UCI repository and others.\n- \"do\" --> does in section 3\n- \"the the\" in section 3.1\n- \"do\" -> does on section 3.2\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The study reported an systematic evaluation of existing metrics used in generating similarity-based explanations. Overall, the paper is technically sound and easy to understand.",
            "review": "The presented study tackled the problem of evaluating different metrics for generating similarity-based explanations. To this end, the authors used three tests to evaluate four types of metrics on several different datasets. The experiment results revealed that the cosine similarity of the gradients of the loss performed the best. Overall, the methods adopted by the study are technically sound and the paper is well written (though with a few typo errors). In particular, the presented experiments are extensive and the results are somewhat interesting. Still, there are a few comments that can be considered to further improve the paper:\n\n1. It would be good to provide a more comprehensive description of relevant studies in generating explanations (e.g., what are those application scenarios, what datasets were used).\n2. Why only logistic regression and deep neural networks like CNN and Bi-LSTM were used for experiments? Any justification for this choice?\n3. It would be good to provide some examples (especially those by using textual data) to provide a more in-depth explanation the actual value behind those reported numbers, e.g., what specific explanations can be produced by applying the best-performing metric vs. the worst-performing metric?\n\n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Evaluation of similarity-based explanations",
            "review": "Summary:\n\n \nThis paper is on the evaluation of XAI for supervised models. XAI is a trending topic since several years now but evaluating the quality of an explanation is still challenging. This works focuses on similarity-based explanations, which means providing similar training examples along with the predicted class. Another assumption is that the chosen examples must be of the same class.\n\nThe main contribution is to benchmark several similarity measures against various datasets, using three tests. One test is taken from the literature and the two others are proposed in the paper. The experiments show that some similarity measures are better than others. I really like that the authors find an explanation for those results, based on geometrical properties of the measured objects. Besides, they use this finding to correct existing measures.\n\n\n##########################################################################\n\nReasons for score: \n\nAs a whole, I like the main aim of the paper for I think XAI deserves better evaluation metrics. However, I have some concerns that explains my vote.\n\nFirst of all, I am not fully convinced that the assumption of restricting to similar examples with the *same* class is really good for giving a relevant explanation. I believe that a good explanation will need both positive and negative examples, in particular near the decision frontier. Therefore this tempers the interest of the proposal made in this paper.\n\nSecond, I had a hard time figuring out how and when the third test (with subclasses) can be used, and how it has been implemented in the presented experiments. From my understanding, this test is useful when you access a class taxonomy (e.g., animal -> mammal -> cat). But in Section 4.2 the authors explain that they randomly assign existing classes to on of the two (subclass) A and B. What does it mean? Besides I'm not sure to be convinced by the argument raised in 3.3 (\"...the violation of this third requirement again leads to nonsense explanation\"). We can find similarities between two animals (here, car and frog) because they share common properties (e.g., breathing), in particular if you compare them to a truck.\n\nThird, the randomization test relies on a unique random model that is confronted to the learned model. However it is possible that the random model is close to the learned model and I expected the random model to be generated several times in order to get more robust results.\n\nFinally, I was puzzled by Fig.6. Why don't the authors use the same examples for the different measures? It seems unfair as the examples given to GC looks easier to classify/explain.\n\n \n##########################################################################Pros: \n\n \n1. address the difficult task of evaluating explanations in XAI\n2. paper easy to read (except for the third test)\n3. really interesting discussion on the failure of (some) existing measures\n \n##########################################################################\n\nCons: \n\n \n1. restricted to a narrow form of XAI\n2. subclass test not well explained \n3. Fig.6 is misleading (see above)\n\n \n##########################################################################\n\nQuestions during rebuttal period: \n \nPlease address and clarify the cons above.\n\n \n#########################################################################\n\nAdditionnal comments:\n\n\"two models with different reasoning process\": this expression is misleading, it's not really a \"reasoning process\". I would write \"inference process\" or \"inductive process\".\n\nThe accuracy of the learned models is never taken into account. I think evaluating the explanations should be done at the light of classification ability. The authors may discuss this point.\n\nSome typos: \n\n\"two instance\"\n\"the the\" (twice)\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}