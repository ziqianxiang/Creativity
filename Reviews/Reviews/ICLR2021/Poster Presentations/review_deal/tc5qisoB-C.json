{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "**Overview**: This paper provides a new clustering-based method to predict future probability density of a policy. It provides comparable performance to prior Q-learning-based methods, but without careful hyper-parameter tuning.\n\n**Pro**: The method of using clustering to estimate future density is novel. Both theory and experiments appear solid. In the rebuttal phase, the authors convinced all the reviewers by addressing their concerns. The reviewers unanimous tend to acceptance.\n\n**Con**: The reviewers had many concerns before the rebuttal. But these were addressed by the authors. \n\n**Recommendation**: The C-learning method proposed in this paper is novel and can be potentially useful in practice. Both theory and experiments are solid and convincing. Hence the recommendation is accept.     "
    },
    "Reviews": [
        {
            "title": "Official Blind Review #5",
            "review": "[summary]\nThis paper studies to predict future state density function by using an indirectly method via classification. The main idea is to sample the future state from two sources: 1) from replay buffer, 2) the actual next state in the trajectory (in off policy setting we only need the next state) and then use a classifier to distinguish them. By Bayesian rule we can recalculate the conditional density function by the ratio of the classifier. The paper compare this method with several baseline and find that they can predict the conditional density function very close to reality.\n\n[originality]\nI really like the idea and the method seems very interesting and novel to me. However, the main concern is the motivation of the classifier. It seems to me that we can classify the true $s_{t,+}$ with any source of distribution of $p(s_{+})$. For example, in your Algorithm 1 or 2, we can replace $p(s+)$ with arbitrary distribution (even the distribution we create), then the whole derivation still hold. I believe it is reasonable to try to distinguish $p(s+)$ with $p(s+|s_t, a_t)$, but did you compare with any other prior distribution of $p(s+)$?\n\n[clarity and theoretical soundness]\nMy major concern of the paper is the clarity which I will explain in the sequel.\n- Notation.\n The main notation of the future state density function is abused the notation of $p$. I think in Definition 1 the right hand side you are defining a new condition density function, not a new future state. So I would recommend to write it as:\n$$p_{+}(s'|s,a) = (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}p_{t}(s'|s,a),$$\nwhere $p_t(s'|s,a)$ is the distribution if $s'$ occur $t$ step after the occurrence of the $s,a$ pair.\nMany notations in the main text has this kind of problem where it is hard to distinguish the future distribution with the original distribution.\n- Main message in section 5 is not clear.\nAlgorithm box 1 is confusing. It seems to me that we have double terms of $s_{t,+}$, and they are all used in the loss function (F=1 and F=0). If I understand correctly, we should put $s_{t,+,0} \\sim p(s_{t,+}$ (again I don't like this notation, p can be any other meaning) and $s_{t,+,1} \\gets s_{t+\\delta}$, and the loss function for $F=0$ uses $s_{t,+,0}$ and $F=1$ uses $s_{t,+,1}$. \nAnd without looking at the paragraph in details, it is hard to tell what is $F$ at the first glance. I feel like you can rewrite this section a little bit by motivating the reader why we consider using a classifier.\n-Minor.\n  1. Remark 1 should add some assumption, otherwise we can always think tabular case is a special case of in the continuous state space, where the probability mass function is not 0.\n  2. Eq.(1) should be $p(F=1|s_t, a_t, s_{t,+}) = $XXX, missing a $s_{t,+}$ in left hand side. Similar for the next line.\n  3.The sentence after Eq.(2), why we can get rid of estimating the marginal density? Not clear and no explanation.\n  4. Eq.(4) missing an expectation for $E_{s_{t+1}, a_{t+1}}$.\n  5. Eq.(5), we should put $F(\\theta, \\pi) = $ in the equation not a line above.\n  6. Eq.(5), the subscript of expectation is not standard (compared to Eq.(3)).\n  7. Unclear reason why we want to stop gradient in Eq.(7) and Algorithm 3. Is that stabilized the learning process?\n\n[related work]\nIn the first line of the page 2, I think \"contrastive approach\" is not accurate. I didn't see any contrastive objective in the paper.\nI feel like the density based methods in off-policy evaluation(e.g. Liu et. al. 2018 and DICE family (e.g. Nachum et. al. 2019)) is more relevant to value based method, where we can set the reward function as the indicator function and estimate the average reward of the policy $\\pi$. Their methods are also based on a recursive objective, I think a comparison between that will be beneficial to further clarify your contribution.\n\n[reference]\n1. Qiang Liu, Lihong Li, Ziyang Tang, Dengyong Zhou. Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation\n2. Ofir Nachum, Yinlam Chow, Bo Dai, Lihong Li. DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Building probability density of reaching a future observation via contrastive classification",
            "review": "This paper explores learning a classifier to predict if a given state/observation will be reached in the future from the current state and action pair. Using this classifier, the paper is able to create a probability density function that could be conditioned on reaching a goal state. This idea is interesting and could leverage large scale self-supervised learning to build the future state probability density function. I have some concerns which are listed below.\n\n1. How does the method work on low dimensional action spaces where the probability of reaching a future state from current state and action is usually higher?\n\n2. How does the method work on high dimensional tasks that have discrete-continuous dynamics due to multiple contacts, such as the pen task. The probability of reaching a future state from the current state + action is very low and the experiments show only training on a human demonstrated dataset. Can the policy be used to control the hand to reach a target pen position very different from the human demonstration?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting and relevant to UOM paper but relevance and connection is not discussed. ",
            "review": "This paper studies a problem of predicting future state distribution in an MDP. The approach taken is an indirect approach which first predicts whether an observation comes from the future and transfers this binary prediction via Bayes rule to predictions over future states. \n\nConceptually the paper's appears novel but a previous similar work was very relevant. In UOM paper, Definition 1 is almost exactly the same as the discounted state occupancy function defined the UOM paper, just an extra normalization constant (1-\\gamma). Apparently at this core the idea is not novel. However, the paper's development of casting the prediction problem into two stages (first binary prediction) and using Bayesian to transfer into the future state distribution is novel. The connection of UOM paper is interesting but not discussed unfortunately. In the UOM paper, the authors there appear to focus on reward-less MDPs, where you can generate/compute the value function given a reward function on the fly. Here the paper focuses on more on the estimation of the the discounted state occupancy function, -- this is my interpretation.   \n\nParagraph \"In discrete state spaces\":\nintroducing this reward: again this is just another interpretation of the discounted state occupancy function in UOM (equation 2). \n\nRemark 1:\nI don't understand the point of Remark 1. do you mean for a continuous-state problems, the prob of reaching some particular state is zero? Isn't that that obvious? What is the point here?\n\nHypothesis 1 and 2: I don't know whether they make sense. The paper relies on the experiments to try to make sense of it. I'm not sure this is a sound approach. Why testing these hypotheses is interesting at the first place? At least this isn't clear from the paper. \n\n\nUOM paper (not cited):\nhttps://papers.nips.cc/paper/5590-universal-option-models\n\nI've read the authors' feedbacks and other reviewers' comments. R5's main concerns are the clarity and the motivation of Bayessian classifier and off-policy learning. That should have been resolved from authors' feedbacks. \n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "well written and clear",
            "review": "Summary:\nThis work presents a goal-conditioned RL, which estimates probability density using a classifier. \n\nStrengths:\n+ The problem is well explained, the logical structure seems adequate. \n+ The paper is well written and clear. \n+ The approach technically sounds and mathematically well-formulated.\n\nWeaknesses:\n- Although the reported evaluation results are competitive to baselines, it would have been even stronger if the performance is substantially improved. Do you have any insight on how better results can be achieved?\n\n ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good idea, but confusing manuscripts.",
            "review": "The authors propose a new algorithm, called C-learning, which tackles goal-conditioned reinforcement learning problems. Specifically, the algorithm converts the future density estimation problem, which goal-conditioned Q learning is inherently performing, to a classification problem. The experiments showed that the modification allows a more precise density estimation than Q-learning, and in turn, a good final policy.\n\nOverall, I like the general idea to use classification as a tool for estimating the future density function. Especially, the idea is valuable in that it allows a better understanding of prior Q-learning based approaches in choosing a sensitive hyperparameter. However, the manuscript can be enhanced much by adapting more precise notations and adding more explanations on equations:\n* $p$ is highly overloaded; it is used to represent future conditional state density function and marginal state density function for both on-policy and off-policy, and transition dynamics.\n* Also, it would be important to notate $\\pi$ in most of the parts, including $p$, $Q$, and $C$ (unless it is very obvious). Especially, the current notation is very confusing when the off-policy algorithm is introduced.\n* Related to this concern, I am not fully convinced of the off-line algorithm due to the marginal future state distribution $p(s_{t+})$. Doesnâ€™t it supposed to be $p^{\\pi_{\\text{eval}}}(s_{t+})$, and therefore, does the marginal distribution also need to be adjusted as $p(s_{t+}|s_{t+1},a_{t+1})$?\n* It would be also helpful if the full derivation for Equation (6) is included in the main manuscript.\n\n-- After rebuttal\n\nI've read the authors' feedbacks and other reviewers' comments. My major concern was the clarity of the manuscript as other reviewers mentioned, and I believe the concern has been resolved during the rebuttal period. I adjusted my ratings representing that.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}