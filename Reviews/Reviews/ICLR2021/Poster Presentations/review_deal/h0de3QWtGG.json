{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a counterfactual approach to interpret aspects within a  sequential decision-making setup. The reviewers have reacted to each others' comments as well as the authors' response to their views. I am recommending acceptance of this paper, as it targets an interesting problem and presents an intriguing approach. I think the community would appreciate further discussing this paper at the conference."
    },
    "Reviews": [
        {
            "title": "This paper studies interpretable policy learning in batch IRL setup. Interesting problem but the paper doesn't completely live up to the expectations.",
            "review": "This paper considers the problem of Inverse Reinforcement Learning in a batch setting. In this problem, we are given several trajectories from the expert policy e.g. electronic health records, and our goal is to recover the particular reward function the expert is maximizing without any possibility to solve the forward RL problem. One of the main motivations of this paper is to provide explanations behind the expert’s decision-making.\n\nIn order to solve the batch IRL, first the paper assumes a linear reward function i.e. $R(h_t, a_t) = <w, \\phi(h_t, a_t)>$. This in turn implies that the expected reward under a policy $\\pi$ is the inner product between weight vector $w$ and feature expectation $\\mu^{\\pi}$. With this setup, the goal is to come up with a candidate policy $\\pi$ which is close to the expert's policy $\\pi_E$ in terms of feature expectation.\n\nThe authors propose a new algorithm to learn a policy given a dataset of trajectories from the expert policy. The main component of the algorithm is to learn the feature expectations given a candidate policy, which the authors show can be obtained through a TD learning-based algorithm. With this procedure, the main algorithm is an iterative algorithm, which chooses the current weight vector to be the residual of the observed feature expectation and the convex hull of the feature expectation of the policies found so far. The final policy is the best convex combination of the feature expectations of the candidate policies found in the algorithm.\n\nI think the paper makes interesting contributions to the literature on IRL in the batch setting. However, I do have several questions about the experiments and some of the claims made in the paper.\n\n1.  I don't understand what value the counterfactuals contribute to the paper. I understand that asking about outcomes for a different chosen action leads to thinking about counterfactuals, but the paper isn't using any counterfactual estimation algorithm explicitly. Furthermore, the authors should discuss when the counterfactuals $E[Y_{t+1}(a_t) | h_t]$ are well-defined. Notice that $Y_{t+1}(\\cdot)$ depend on counterfactuals $Y_{t'}(\\cdot)$ through history $h_t$. Are they well-defined? What if $h_t$ is continuous because some $x_{t'}$ is a continuous random variable?\n2.  The authors claim that this work can handle batch IRL setting with history-dependent reward functions. But that comes with a lot of intrinsic assumptions. First of all, $w$ does not change with time, and $\\phi(\\cdot)$ is always $d$-dimensional. This avoids the curse of dimensionality problem as the complexity of the problem does not increase with the length of the history. This also shows up in the experiments section where the reward just depends on the history of length $5$, \n3. The assumption of linear rewards and time-invariant $w$ is very critical in this paper. This should be discussed in more detail as it lets the expected reward to be written in the form of the dot product between w and feature expectation. \n4.  The first simulation uses linear features and also assumes that reward just depends on the current x and v. This makes the feature expectations computations really simple. Ideally, it would have been nice to see high dimensional w, non-linear feature, and dependence of reward on the history of length > 1. \n5. Finally, the authors argue that the use of counterfactuals gives an interpretable parametrization of the expert policy. I don't agree with this statement completely. Isn't it completely driven by the low-dimensional parameter $w$? The use of counterfactuals here is also different than the counterfactuals introduced by Hérnan, and Robins, 2000. Why not first pose counterfactuals for the reward functions $R(h_t,a_t)$ and then consider some restricted low-dimensional marginal models for interpretation?\n\nIn summary, I think the paper chose a great problem as finding interpretable policies in a batch IRL setting would be really useful. However, I do think the ideas are not carried out fully, and some parts of the paper can benefit significantly from better design choices.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review for Learning What If Explanations for Sequential Decision Making",
            "review": "## Summary \n\nThe authors address the problem of comparing the effects of different treatments in medical decision making.  They formulate this as learning a reward function via inverse reinforcement learning in a batch setting, where the only admissible data is in the form of medical records.  Their goal is to develop a framework where they estimate the reward function as well as an optimal policy, and subsequently to compare alternatives (different treatments or abstaining from treatment) for a given history.  They propose to integrate counterfactual reasoning into the process of learning optimal policies and reward functions, which they argue is critical for off-policy evaluation in this setting, the results of which is their method dubbed CIRL.  They present results of two simulation studies, as well as an experiment on the MIMIC III data set.  \n\n## Strong points\n\n1. The authors frame the value of the ability to interrogate preferences of different policies in terms of the value it would bring to comparing data-driven clinical guidelines of when & how to treat. (cf. page 1, Introduction : *Moreover, modeling the reward function of different clinical practitioners can be revealing as to their tendencies to treat various diseases more/less aggressively (Rysavy et al., 2015), which —in combination with patient outcomes—has the potential to inform and update clinical guidelines.*\n\n2. By re-framing batch IRL as counterfactual reasoning, they cast the feature embedding component of the reward function to be learned as a function of possible actions, which yields interpretable results (cf. page 2, Contributions).\n\n\n## Weak points\n\n1. Reading over the paper, it is not clear they make their case for interpretable decision making.  The one strong piece of evidence they rely upon is a simulation study for recovering the parameters of the data generating process in a tumour volume vs harmful side-effects (cf. Figure 3), which shows that CIRL recovers the generating weights of the reward function with lower variance than comparable methods, but this may be due to the agreement of the assumptions CIRL makes on the reward function with that of the function used in their simulation.  It would be more convincing had they also tried more complex reward functions.  \n\n2. I grant that real data (cf. the MIMIC III case study of section 5.2) does not provide reward weights for comparison, but would like to see more in-depth evaluation than the scenario provided.  In this example, the highest proportional weight is given to Temperature and WBC (White Blood-Cell count, a marker of lymphocytic activity), which is consistent with medical guidelines.  But analgesics are also indicated to reduce temperature, and WBC may be high due to viral rather than bacterial infection.  To be more convincing, I would want to see more experiments for other interventions, to see how frequently CIRL is able to recover the desired outcome(s).\n\n\n## Recommendation\n\nIRL is a bit beyond my expertise, so I'll speak mostly to the advances in interpretability.\n\nThe premise in the introduction, that different reward functions of different clinical practitioners (or even different institutions) is not tested, and so it is difficult to assay how useful this would be in a real setting.  To the extent that CIRL can recover accurate estimates of the reward function weights, these are interpretable in the same manner that weights in a GLM are interpretable.  This seems sufficient for comparing the relative frequency of actions chosen by experts given the same history, but I am not convinced it provides an explanation for why experts make their decision (cf. arxiv.org/abs/1606.03490)\n\nThe ability of the method's claim as explainable AI notwithstanding, the blending of counterfactual reasoning with batch IRL is promising, and I recommend the paper be accepted.\n\n\n## Questions for the authors\n\n- The problem formulation in section 3 is a bit light on details with respect to how counterfactual reasoning is present in the definition of the feature representation (cf. Equation 1).  \n\n- I felt as if there was some switches in nomenclature that might help people from adjacent fields better read and understand the paper.  For example, the $\\mu^{\\pi}(h,a)$ are described as \"history action feature expectation\".  However, a friend of mine who read the paper with me recognized the definition of successor representations.  Are these two names for the same quantity? \n\n- Section 4.1 equation (5) describes how the parameters of the recurrent network are used to approximate the feature expectations, but I wonder how efficient this is as a size of the action space A?  The experiments run in the paper feature very small (binary?) actions.\n\n- In the experiments described in Section 5 the weights in the experiments are constrained to have 1-norm < 1, but when the space of possible actions given a history is large, I wonder how can you characterize if a difference in weights between two potential actions is meaningful (i.e statistically significant)?  \n\n## Suggestions for improving the paper\n\n- The definition for feature expectations at the end of section 4.1 are sufficiently similar to the on-policy roll-out estimation of $\\mu$ hat that it bears taking some care here to explain the difference.\n\n- The introduction to Section 4 of the paper lays out the core of Algorithm 1 very clearly, but it would help readability of the paper if the subsection titles for section 4 followed the description of the numbered list in the introduction (e.g 4.1 is entitled \"Counterfactual mu-learning\" rather than \"Estimating feature expectations\" as in the list)",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Learning explanable \"reward\" from the expert demonstration while considering the counterfactual questions",
            "review": "Summary:\nThis paper proposed a way to learn explanations of expert decisions by modeling their reward function. Different from standard IRL, this main focus is on modeling the preference in the logged decision to answer the \"what-if\" (aka counterfactual reasoning) question. The proposed method considered the expert can take decisions based the history instead of one observation. The proposed approach is run on a simulated and a real medical dataset, showing the effectiveness of the approach.\n\nDetailed comments:\nPros:\n1. This paper is studied an important question that is related to many key applications. Learning explainable reward signals is very useful in practice, for downstream tasks such as learning an explainable AI system and for better incorporating the system with the human expert. Partial observability or the non-Markov decision policy in the data-set is very common in real-world applications but the mainstream of work in RL/IRL focuses on the MDP settings.\n2. Counterfactuals are part of the nature of RL/IRL from a fixed dataset. This paper leverage the idea from counterfactual reasoning and tried to ask and answer the counterfactual in IRL. I think it is a good attempt in an important direction and also help to bring the \nthe gap between causal inference and the RL community.\n\nCons:\n1. Eq 4 gives an updating rule of counterfactual $\\mu$: $\\hat{\\mu}$. As $\\mu$ and $Q$ function can be defined as an expected feature/rewards. Can the definition of $\\hat{\\mu}$ first be written in the expected form and then in a recursive form which gives the (DP) updating rule? In general, it's better to give a more clear definition and explanation of $\\hat{\\mu}$ here.\n2. The empirical study in the MIMIC III dataset is interesting but it seems unclear what the conclusion can be drawn from there. Except for the accuracy of matching expert actions, is there any other further explanation of the learned reward by CIRL, and how does that compare with other IRL methods?\n\nMinor comments:\nIt might worth mentioning that [1] also tried to learn the model to explain the decision in (expert) data and considered partial observability. Maybe this work is more IRL-like and [1] is more like model-based RL.\n\n[1] POPCORN: Partially Observed Prediction Constrained Reinforcement Learning, AISTATS 2020",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "An interesting approach to learn from decision making",
            "review": "The paper describes how to learn cost benefit tradeoffs associated with the expert’s actions. The proposed approach integrates counterfactual reasoning into batch inverse reinforcement learning and offers a sensble framework for defining reward fuctions and tentatively explain how domain expert think and act. The framework is developed for those cases where active interaction with the system under study, i.e., experimentation, is not possible, which is very often the case in healtcare.\nThe paper estimates the effects of different decisions by exploiting the concept of counterfactual to accommodate settings where the policy applied by the expert depends on histories of observations rather than just current states of the system under consideration.\n----------------------------------------------------------------------------------------------------------------------\nReason for Score:\nOverall, I vote for accepting. I like the methodological framework, it is well structured and convincing, even if the basic assumprtions required to make the proposed approach working are strict. Indeed, in many situations it is not clear whether these assumptions are satisfied or not. Furthermore, assumptions can not be tested. The problem tackled is extremely challenging from the theorectical point of view and to the best of my knowledge this is the first paper which tries to explain sequential decisions through counterfactual reasoning and to tackle the batch IRL problem in partially-observable environments.\n----------------------------------------------------------------------------------------------------------------------\nPros.\n1) tackles a very relevant problem, both theoretically and practically.\n2) well structured and written.\n3) methods sound and convincing.\n3) numerial experiments are well designed and results seem to confirm the effectiveness of the proposed appproach.\n----------------------------------------------------------------------------------------------------------------------\nCons.\n1) at page 3 when introducing the value function of a policy, the choice of V is not that clever because early in the paper you let V to be the volume of tumor.\n2) please provide quantitative description of how accuracy is computed in numerical experiments\n3) the explanation components of the proposed approach should be further developed, I see potential there but at the current stage it limits to the weights and not for example tries to investigate whether these weights change during time\n4) I would like to see how the proposed approach scales with the number of covariates of patients.\n----------------------------------------------------------------------------------------------------------------------\nQuestions during rebuttal period: \nPlease address and clarify the cons above \n----------------------------------------------------------------------------------------------------------------------\nI found no typos.\n----------------------------------------------------------------------------------------------------------------------",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}