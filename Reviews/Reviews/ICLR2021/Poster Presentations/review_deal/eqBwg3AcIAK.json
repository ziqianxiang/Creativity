{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents an approach to domain adaptation in reinforcement learning. The main idea behind this approach, DARC, is to modify the reward function in the source domain so that the learned policy is optimal in the target domain. This is achieved by learning a classifier that learns to discriminate between the data from the source domain and those from the target domain. \n\nOverall, reviewers appreciated the intuitiveness of the approach as well as its formal analysis. They had some concerns with respect to experiments, which was sorted out in the author response period. Given the overall positive reviews, I recommend accepting the paper. \n"
    },
    "Reviews": [
        {
            "title": "A simple and effective domain adaptation reward modifier for transfer in RL",
            "review": "## Summary\nThis paper proposes a method for domain adaptation in RL where the source and target domains differ only in the transition distriubtions. A theoretical derivation based on RL as probabilistic inference is presented that starts with the objective of matching the desired distribution of trajectories in the target domain with the distribution achieved by the policy in the source domain. The final objective appears as a modification to the reward function while training in the source domain and is implemented easily with just two binary classifiers that predict the domain given either state-action or state-action-next-state tuples. Theorem 4.1 provides a theoretical guarantee on the performance of a policy trained on such a modified reward in the source domain by giving a bound on the performance in the target domain, under a very mild assumption that the optimal policy on the target domain achieves similar rewards when put in the source domain. Experiments are presented that show improved performance in terms of rewards vs experience on target domain on environments such as broken reacher, broken ant, etc (where the target domain has some \"broken\" component). Further, it is also shown that the reward modification on source visually matches the reward expected in target (Fig 4), that without the reward modification the policy usually exploits the source domain's transitions which cannot be exploited in the target domain, and finally, that safety emerges from the proposed objective.\n\n## Strengths\n- The theoretical bound presented in Theorem 4.1 is strong given that Assumption 1 is quite mild. In fact, Assumption 1 is trivially met in most of the environments used in Fig 6, e.g.: for half cheetah obstacle, the target policy does not run in to the obstacle and hence will get the same reward in the source and target environment.\n- The final form of the objective as a modification to the reward is a simple and easy to implement objective with the two binary classifiers. Further, the max-entropy derivation makes it applicable to any maximum-entropy algorithm.\n- The empirical evidence shows that the proposed method has very similar performance to the RL on target baselines while improving over other domain adaptation baselines in four continuous control environments.\n\n## Weaknesses\n- The variety in the source-target domain shift seems limited in the experiments shown in Figure 6. Three tasks have the crippled-limb setting and one task has an obstacle in the target domains. The introduction started off by giving some great examples such as aggressive driving failing to work on target domains with icy roads. It would have been great to see application of this method on more subtle but dangerous changes to the target domain, similar to the car driving example. For example, reducing friction on the HalfCheetah or Ant environments -- it would be quite interesting to see what strategies develop in the source domain; does the agent take shorter steps to staty in contact with the floor?\n- The assumption that the rewards for the target and source domains match is suitable for the choice of environments in Section 6 but seems quite limiting to a more general setting where certain rewards may never be observed in the source domain. Further, there is an implicit assumption that the source domain is \"free\" and the target domain is \"restrictive\" (e.g. broken ant, obstacle halfcheetah). It would be good to discuss the rare but still possible case where the opposite is true, the target domain allows for more \"free\" movement.\n\n## Other issues/comments\n- Theorem 4.1 has a crucial typo: $p_{\\text{source}}$ is mentioned on the right hand side but it should actually be $p_{\\text{target}}$. The appendix (Theorem B.3) has the correct version of this theorem statement.\n- In Theorem 4.1, the reward-maximizing (entropy-regularized) policy in the target domain is said to satisfy Assumption 1. To confim, does this mean that, in the target domain, the policy that maximizes the entropy-regularized reward objective is the same as the policy that maximizes rewards without the maximum-entropy constraint?\n- Minor typo in Section 5, Algorithm Summary: $q_{\\theta_{SAS}}$ should be $q_{\\theta_{SA}}$ in the second line.\n- I don't understand the transparent colored lines in Figure 6, are they supposed to represent error bars or are they the non-smooth version of the darker colored lines?\n- Typo/stray sentence in the last para, last line of Section 1.\n\n## Feedback to authors\n- To further study the usefulness of this approach (as future work perhaps), it would be good to apply it to an offline RL setting where a finite set of \"good\" trajectories from the target domain are provided and no further trajectories can be collected during training. Only during test time is the agent then deployed in the target domain to measure performance (and possibly safety).\n\n\n--------------------------\n## Post-rebuttal Update\n\nThe authors have shown new experiments on icy environments that show good results for the proposed method (DARC). This directly addresses my point (and recommendation) about trying out experiments similar to the aggressive driving on icy road example that was mentioned in the introduction. Having read through the other reviews and responses by the authors, I feel that most major concerns have been addressed. As such I am inclined to increase my score from 7 -> 8, recommending acceptance of the paper and entrusting the authors to include the new experiments in the main paper.\n\nA minor note: My second point under \"Other issues/comments\" section was not answered in the rebuttal. I hope the authors can clarify this in the future either in the main paper or appendix.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Well-written and well-motivated, but needs comparisons to stronger baselines, more evidence of robustness to dynamics, and discussion of limitations.",
            "review": "Summary: This paper introduces DARC, an RL approach that aims to transfer from a source environment to a target environment with different dynamics. DARC utilizes an adversarial-like reward that distinguishes between source and target domains. This reward is used to augment the environmental reward. DARC is evaluated against several baselines in mujoco tasks. \n\n\nOverall, the paper is well-written and the motivation is clear. The approach seems reproducible, as pseudocode is provided along with training details and hyperparameters. The technique seems simple and easy to implement. The paper also includes some theoretical guarantees for the approach. \n\n\nHowever, I believe this paper is borderline and I lean towards reject for a few reasons. The quick summary is that stronger baselines could have been included, it is unclear how well the approach can handle large (or even medium) differences in dynamics, and the approach does not seem robust to choices of source and target domains.\n\n\nWhile the method was evaluated against several baselines, I believe there are stronger methods it could have been compared to. In particular, there are quite a few imitation learning approaches that aim to learn in environments with different dynamics than the demonstrator, which is similar to the idea of transfer [1-3]. Each of these approaches has been shown to work in mujoco environments with disabled joints and hence would be good baselines. While MATL is a reasonable comparison, it was not evaluated in such environments in the original paper.\n\n\nFurthermore, using adversarial-based rewards to classify whether a trajectory is from the agent or source is not a new concept. It originated from GAIL [4] and has since seen several variants, including works that learn across different variants of the domain [5-7]. The paper should be more clear in expressing that domain classification in reinforcement learning is not a new concept.\n\n\nThe requirement that $P_{target}(s_{t+1}|s_t, a_t) > 0$ ⇒ $P_{source}(s_{t+1}|s_t, a_t) > 0$ is a weak point of the approach. It is unclear if the lower limit is 0 or some larger number in practice. This is important given the claim that DARC is for transfer in environments with different dynamics.  If the dynamics are very different, then I would expect the discriminative reward to do poorly because it would be easy to distinguish between source and target transitions. An interesting experiment would be to see how the discriminator behaves as the discrepancies in dynamics increases. \n\n\nFinally, it seems that one needs to get a bit lucky in the source and target choice. The example shown in fig 3. shows that if there were an obstacle in the target domain and no obstacle in the source domain, then the agent would need to go around this obstacle. But if the obstacle were in the source domain, I would expect the approach to be suboptimal in the target domain since the reward function would encourage it to go around an invisible object. \n\n\nSome concrete steps for improving the paper would be to include more discussion as well as comparisons to approaches that seem equipped to handling different dynamics, demonstrating the impact of differences in dynamics on the approach, and making the limitations of the approach more clear.\n \n\nOther comments/thoughts:\n\n\nThe acronym DARC is introduced in the theorem. It may be better to introduce beforehand\n\n\nWhat does the environment reward in Fig. 7 look like? It would be interesting to see a similar jump in return\n\n\nThe introduction mentions a 111-dimensional ant task. Was this actually used?\n\n\n“Our method performs 10× more gradient updates per environment step in the source domain” -- I don’t think this is explained in the paper?\n\n\nWhy does DARC sometimes have higher asymptotic reward than training from scratch?\n \nTypos\n\n\nhigher-dimensional tasks. broadly-applicable approach for learning from inaccurate models.\n\n\nthat most prior work -> than most prior work\n\n\nthe perform quite poorly -> they perform quite poorly\n\n\nso the agent is penalized -> the agent is penalized \n\n\nThis result that\n\n\nsafegaurds -> safeguards\n\n \n[1] Adversarial Imitation via Variational Inverse Reinforcement Learning. Qureshi et al.\n\n\n[2] Learning Robust Rewards with Adversarial Inverse Reinforcement Learning. Fu et al. \n\n\n[3] State Alignment-Based Reinforcement Learning. Liu et al. \n\n\n[4] Generative Adversarial Imitation Learning. Ho et al. \n\n\n[5] Third-Person Imitation Learning. Stadie et al.\n\n\n[6] Generative Adversarial Imitation from Observation. Torabi et al.\n\n\n[7] Cross-domain Imitation Learning. Kim et al.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "DARC is cool and the motivation is solid, but the assumption 1 is quite restrictive",
            "review": "\n## Summary\n\nThe paper introduces DARC, a domain transfer algorithm motivated by maximum entropy RL. By introducing classifiers for the target and source domain, the reward function in the source domain can be modified such that it restricts the behavior of the optimized policy to transitions that reflect the target domain. In this way, the method achieves good domain transfer without having an explicit model.\n\nAdditionally, the authors provide a lower bound on the performance of such a policy in the target domain. \n\n## Overall Recommendation\n\nAlthough I believe the method to be quite restricted by the assumptions that it makes, overall, I recommend acceptance, but it could be stronger after clearing up a few things.\n\n## Writeup\n\nI really like the writeup, the connection to the related literature is clear and it is clear what is the goal of the method. The method is explained well and I didn't have any problems following the text.\n\n## Pros\n\nThe method is very intuitive and theoretically well justified. The authors provide experiments and compare against different model-free settings and a model-based baseline. I also like the connection to safety, namely that this method ensures safety by enforcing behavior that is far away from violating the safety constraints.\n\n## Cons\n\nThe experiments are still a bit dissatisfying, because I would like to see an experiment where transfer happens to the real system. Also, I can see a benefit in improving the simulators or the source domain to reflect the target domain, I don't think that taking the source domain as-is is the way to go, because this is also a waste of data from the target domain. \n\nI take note that data collection from the target domain is nevertheless necessary, in order to fit the classifiers. Might as well improve the simulator(model) in the source domain.\n\nAssumption 1 is quite restrictive.\n\n## Comments/Questions\n\nThe assumption on the reward maximizing policy from the target domain is quite strong, this also translates to an assumption about how much the dynamics are allowed to change. It would seem more satisfying to me to take into account the transition dynamics errors in deriving the bound, such as done in MBPO[1]. \n\nI believe that the way you chose to construct your experiments reflect your assumptions nicely. For the broken cheetah/ant/reacher, a policy that works with less degrees of freedom would more-or-less obviously work with more (satisfying assumption  1). I wonder how would your method perform with not so obvious changes in dynamics - changes in mass, friction etc, perhaps even adversarial changes in dynamics? \n\nNormally, when doing sim2real, the dynamics change in a different way than what is shown in the experiments. \n\nThrowing away data from the target domain and not improving the source domain seems like the wrong thing to do,  ideally you would reuse the data from the target domain to improve the source domain.\n\nFurthermore, as far as I know,  MBPO  doesn't work on transfer, but actually learns the simulator for the environment (the model). Obviously if the data for fitting the model only comes from the source domain, the model would overfit to the source domain but  stick to the regions that it's certain about. The data from the target domain should be reused  for MBPO, or it should be given the same chance as RL trained on target.  Was this done in this way? The results for MBPO seem a bit too disappointing to be true.\n\nA further separating thing is that, you assume that you basically have a simulator that \"encompasses\" the target domain, i.e. policies good in the target domain are still valid in the source domain, this is not needed by MBPO or the model-free variants, and it obviously helps the method a lot. In the broken environments case, if you would switch the target and source domains and then evaluate against the baselines, I believe that the baselines would be better or at least on-par.\n\n-------\n\n[1] Janner, Michael, et al. \"When to trust your model: Model-based policy optimization.\" Advances in Neural Information Processing Systems. 2019.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "review2",
            "review": "This paper presents a method to do domain adaptation. The idea is to modify the reward function in the source domain so the learned policy can be optimal in the target domain. This is accomplished by learning classifiers that distinguish transition in the source domain from transition in the target domain.  \n\nThe idea is introduced intuitively and justified via formal analysis and it seems to effectively solve the tasks presented, but I have a few questions that need clarification before I make a final decision.\n\n(1) The assumption that the target transition will have non-zero probability in the source transition seems to be violated in some of the examples. For example, if the action for the particular broken joint is non-zero, the transition in the target domain will be entirely different from the source domain since the simulator is deterministic. I don't see how this will satisfy the assumption.  \n\n(2)Further comment on this assumption. For the motivating example of driving on roads with different frictions. Again transition in the target domain (icy road) has almost zero probability in the source domain.\n\n(3) About Assumption 1 in the paper. What is $\\epsilon$? What is $R_{max}$? I see this is defined in the appendix, but please define it in the main text.  And this assumption seems to become lemmaB2 (or look very similar to it), this is quite confusing. And for LemmaB2, there seems to be a typo in the first inequality, I assume the second p_source should be p_target, otherwise I can't see the difference between the two expectation. And what is the $p$ and $q$ in the first line of the proof? \n\n(4) Further question on the assumption, since R_max is the maximum total reward possible for a trajectory, this can be a large number (for example, for half cheetah it is something like 8000). I don't see why this assumption is meaningful.\n\n(5) For the half cheetah obstacle examples, the problem is not a MDP anymore since the obstacles is placed at some specific position and the policy doesn't have access to the position information as far as I know.\n\n(6) The experiment for humanoid standing, while interesting, seems unrealistic. In real world scenario, the robot should at least be rewarded an alive bonus for avoiding the termination condition.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}